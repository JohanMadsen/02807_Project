<doc id="26994" url="https://en.wikipedia.org/wiki?curid=26994" title="Scotland">
Scotland

Scotland ( ) is a country that is part of the United Kingdom and covers the northern third of the island of Great Britain. It shares a border with England to the south, and is otherwise surrounded by the Atlantic Ocean, with the North Sea to the east and the North Channel and Irish Sea to the south-west. In addition to the mainland, the country is made up of more than 790 islands, including the Northern Isles and the Hebrides.

The Kingdom of Scotland emerged as an independent sovereign state in the Early Middle Ages and continued to exist until 1707. By inheritance in 1603, James VI, King of Scots, became King of England and King of Ireland, thus forming a personal union of the three kingdoms. Scotland subsequently entered into a political union with the Kingdom of England on 1 May 1707 to create the new Kingdom of Great Britain. The union also created a new Parliament of Great Britain, which succeeded both the Parliament of Scotland and the Parliament of England. In 1801, Great Britain itself entered into a political union with the Kingdom of Ireland to create the United Kingdom of Great Britain and Ireland.

Within Scotland, the monarchy of the United Kingdom has continued to use a variety of styles, titles and other royal symbols of statehood specific to the pre-union Kingdom of Scotland. The legal system within Scotland has also remained separate from those of England and Wales and Northern Ireland; Scotland constitutes a distinct jurisdiction in both public and private law. The continued existence of legal, educational, religious and other institutions distinct from those in the remainder of the UK have all contributed to the continuation of Scottish culture and national identity since the 1707 union with England.

In 1997, a Scottish Parliament was re-established, in the form of a devolved unicameral legislature comprising 129 members, having authority over many areas of domestic policy. Scotland is represented in the United Kingdom Parliament by 59 MPs and in the European Parliament by 6 MEPs. Scotland is also a member of the British–Irish Council, and sends five members of the Scottish Parliament to the British–Irish Parliamentary Assembly.

"Scotland" comes from "Scoti", the Latin name for the Gaels. The Late Latin word "Scotia" ("land of the Gaels") was initially used to refer to Ireland. By the 11th century at the latest, "Scotia" was being used to refer to (Gaelic-speaking) Scotland north of the River Forth, alongside "Albania" or "Albany", both derived from the Gaelic "Alba". The use of the words "Scots" and "Scotland" to encompass all of what is now Scotland became common in the Late Middle Ages.

Repeated glaciations, which covered the entire land mass of modern Scotland, destroyed any traces of human habitation that may have existed before the Mesolithic period. It is believed the first post-glacial groups of hunter-gatherers arrived in Scotland around 12,800 years ago, as the ice sheet retreated after the last glaciation.

The groups of settlers began building the first known permanent houses on Scottish soil around 9,500 years ago, and the first villages around 6,000 years ago. The well-preserved village of Skara Brae on the mainland of Orkney dates from this period. Neolithic habitation, burial, and ritual sites are particularly common and well preserved in the Northern Isles and Western Isles, where a lack of trees led to most structures being built of local stone.

The 2009 discovery in Scotland of a 4000-year-old tomb with burial treasures at Forteviot, near Perth, the capital of a Pictish Kingdom in the 8th and 9th centuries AD, is unrivalled anywhere in Britain. It contains the remains of an early Bronze Age ruler laid out on white quartz pebbles and birch bark. It was also discovered for the first time that early Bronze Age people placed flowers in their graves.

Scotland may have been part of a Late Bronze Age maritime trading culture called the Atlantic Bronze Age, which included other Celtic nations, and the areas that became England, France, Spain, and Portugal.

In the winter of 1850, a severe storm hit Scotland, causing widespread damage and over 200 deaths. In the Bay of Skaill, the storm stripped the earth from a large irregular knoll, known as "Skerrabra". When the storm cleared, local villagers found the outline of a village, consisting of a number of small houses without roofs. William Watt of Skaill, the local laird, began an amateur excavation of the site, but after uncovering four houses, the work was abandoned in 1868. The site remained undisturbed until 1913, when during a single weekend the site was plundered by a party with shovels who took away an unknown quantity of artefacts. In 1924, another storm swept away part of one of the houses and it was determined the site should be made secure and more seriously investigated. The job was given to University of Edinburgh's Professor Vere Gordon Childe who travelled to Skara Brae for the first time in mid-1927.

The written protohistory of Scotland began with the arrival of the Roman Empire in southern and central Great Britain, when the Romans occupied what is now England and Wales, administering it as a province called "Britannia". Roman invasions and occupations of southern Scotland were a series of brief interludes.

According to the Roman historian Tacitus, the Caledonians "turned to armed resistance on a large scale", attacking Roman forts and skirmishing with their legions. In a surprise night-attack, the Caledonians very nearly wiped out the whole 9th Legion until it was saved by Agricola's cavalry.

In AD 83–84, the General Gnaeus Julius Agricola defeated the Caledonians at the Battle of Mons Graupius. Tacitus wrote that, before the battle, the Caledonian leader, Calgacus, gave a rousing speech in which he called his people the "last of the free" and accused the Romans of "making the world a desert and calling it peace" (freely translated). After the Roman victory, Roman forts were briefly set along the Gask Ridge close to the Highland line (only Cawdor near Inverness is known to have been constructed beyond that line). Three years after the battle, the Roman armies had withdrawn to the Southern Uplands.

The Romans erected Hadrian's Wall to control tribes on both sides of the wall so the "Limes Britannicus" became the northern border of the Roman Empire; although the army held the Antonine Wall in the Central Lowlands for two short periods – the last during the reign of Emperor Septimius Severus from 208 until 210.

The Roman military occupation of a significant part of what is now northern Scotland lasted only about 40 years; although their influence on the southern section of the country, occupied by Brythonic tribes such as the Votadini and Damnonii, would still have been considerable between the first and fifth centuries. The Welsh term Hen Ogledd ("Old North") is used by scholars to describe what is now the North of England and the South of Scotland during its habitation by Brittonic-speaking people around AD 500 to 800. According to writings from the 9th and 10th centuries, the Gaelic kingdom of Dál Riata was founded in the 6th century in western Scotland. The 'traditional' view is that settlers from Ireland founded the kingdom, bringing Gaelic language and culture with them. However, some archaeologists have argued against this view, saying there is no archaeological or placename evidence for a migration or a takeover by a small group of elites.

The Kingdom of the Picts (based in Fortriu by the 6th century) was the state that eventually became known as "Alba" or "Scotland". The development of "Pictland", according to the historical model developed by Peter Heather, was a natural response to Roman imperialism. Another view places emphasis on the Battle of Dun Nechtain, and the reign of Bridei m. Beli (671–693), with another period of consolidation in the reign of Óengus mac Fergusa (732–761).

The Kingdom of the Picts as it was in the early 8th century, when Bede was writing, was largely the same as the kingdom of the Scots in the reign of Alexander I (1107–1124). However, by the tenth century, the Pictish kingdom was dominated by what we can recognise as Gaelic culture, and had developed a traditional story of an Irish conquest around the ancestor of the contemporary royal dynasty, Cináed mac Ailpín (Kenneth MacAlpin).

From a base of territory in eastern Scotland north of the River Forth and south of the River Oykel, the kingdom acquired control of the lands lying to the north and south. By the 12th century, the kings of Alba had added to their territories the English-speaking land in the south-east and attained overlordship of Gaelic-speaking Galloway and Norse-speaking Caithness; by the end of the 13th century, the kingdom had assumed approximately its modern borders. However, processes of cultural and economic change beginning in the 12th century ensured Scotland looked very different in the later Middle Ages.

The push for this change was the reign of David I and the Davidian Revolution. Feudalism, government reorganisation and the first legally recognised towns (called burghs) began in this period. These institutions and the immigration of French and Anglo-French knights and churchmen facilitated cultural osmosis, whereby the culture and language of the low-lying and coastal parts of the kingdom's original territory in the east became, like the newly acquired south-east, English-speaking, while the rest of the country retained the Gaelic language, apart from the Northern Isles of Orkney and Shetland, which remained under Norse rule until 1468. The Scottish state entered a largely successful and stable period between the 12th and 14th centuries, there was relative peace with England, trade and educational links were well developed with the Continent and at the height of this cultural flowering John Duns Scotus was one of Europe's most important and influential philosophers.
The death of Alexander III in March 1286, followed by that of his granddaughter Margaret, Maid of Norway, broke the centuries-old succession line of Scotland's kings and shattered the 200-year golden age that began with David I. Edward I of England was asked to arbitrate between claimants for the Scottish crown, and he organised a process known as the Great Cause to identify the most legitimate claimant. John Balliol was pronounced king in the Great Hall of Berwick Castle on 17 November 1292 and inaugurated at Scone on 30 November, St. Andrew's Day. Edward I, who had coerced recognition as Lord Paramount of Scotland, the feudal superior of the realm, steadily undermined John's authority. In 1294, Balliol and other Scottish lords refused Edward's demands to serve in his army against the French. Instead, the Scottish parliament sent envoys to France to negotiate an alliance. Scotland and France sealed a treaty on 23 October 1295, known as the Auld Alliance (1295–1560). War ensued and King John was deposed by Edward who took personal control of Scotland. Andrew Moray and William Wallace initially emerged as the principal leaders of the resistance to English rule in what became known as the Wars of Scottish Independence (1296–1328).
The nature of the struggle changed significantly when Robert the Bruce, Earl of Carrick, killed his rival John Comyn on 10 February 1306 at Greyfriars Kirk in Dumfries. He was crowned king (as Robert I) less than seven weeks later. Robert I battled to restore Scottish Independence as King for over 20 years, beginning by winning Scotland back from the Norman English invaders piece by piece. Victory at the Battle of Bannockburn in 1314 proved the Scots had regained control of their kingdom. In 1315, Edward Bruce, brother of the King, was briefly appointed High King of Ireland during an ultimately unsuccessful Scottish invasion of Ireland aimed at strengthening Scotland's position in its wars against England. In 1320 the world's first documented declaration of independence, the Declaration of Arbroath, won the support of Pope John XXII, leading to the legal recognition of Scottish sovereignty by the English Crown.

However, war with England continued for several decades after the death of Bruce. A civil war between the Bruce dynasty and their long-term Comyn-Balliol rivals lasted until the middle of the 14th century. Although the Bruce dynasty was successful, David II's lack of an heir allowed his half-nephew Robert II to come to the throne and establish the Stewart Dynasty. The Stewarts ruled Scotland for the remainder of the Middle Ages. The country they ruled experienced greater prosperity from the end of the 14th century through the Scottish Renaissance to the Reformation. This was despite continual warfare with England, the increasing division between Highlands and Lowlands, and a large number of royal minorities.

This period was the height of the Franco-Scottish alliance. The Scots Guard – la Garde Écossaise – was founded in 1418 by Charles VII of France. The Scots soldiers of the Garde Écossaise fought alongside Joan of Arc against England during the Hundred Years' War. In March 1421, a Franco-Scots force under John Stewart, 2nd Earl of Buchan, and Gilbert de Lafayette, defeated a larger English army at the Battle of Baugé. Three years later, at the Battle of Verneuil, the French and Scots lost around 7000 men. The Scottish intervention contributed to France's victory in the war.

In 1502, James IV of Scotland signed the Treaty of Perpetual Peace with Henry VII of England. He also married Henry's daughter, Margaret Tudor, setting the stage for the Union of the Crowns. For Henry, the marriage into one of Europe's most established monarchies gave legitimacy to the new Tudor royal line. A decade later, James made the fateful decision to invade England in support of France under the terms of the Auld Alliance. He was the last British monarch to die in battle, at the Battle of Flodden. Within a generation the Auld Alliance was ended by the Treaty of Edinburgh. France agreed to withdraw all land and naval forces. In the same year, 1560, John Knox realised his goal of seeing Scotland become a Protestant nation and the Scottish parliament revoke papal authority in Scotland. Mary, Queen of Scots, a Catholic and former queen of France, was forced to abdicate in 1567.

In 1603, James VI, King of Scots inherited the thrones of the Kingdom of England and the Kingdom of Ireland, and became King James I of England and Ireland, and left Edinburgh for London. With the exception of a short period under the Protectorate, Scotland remained a separate state, but there was considerable conflict between the crown and the Covenanters over the form of church government. The Glorious Revolution of 1688–89 saw the overthrow of King James VII of Scotland and II of England by the English Parliament in favour of William III and Mary II.

The Battle of Altimarlach in 1680 was the last significant clan battle fought between highland clans. In common with countries such as France, Norway, Sweden and Finland, Scotland experienced famines during the 1690s. Mortality, reduced childbirths and increased emigration reduced the population of parts of the country by between 10 and 15 percent.

In 1698, the Company of Scotland attempted a project to secure a trading colony on the Isthmus of Panama. Almost every Scottish landowner who had money to spare is said to have invested in the Darien scheme. Its failure bankrupted these landowners, but not the burghs. Nevertheless, the nobles' bankruptcy, along with the threat of an English invasion, played a leading role in convincing the Scots elite to back a union with England.

On 22 July 1706, the Treaty of Union was agreed between representatives of the Scots Parliament and the Parliament of England and the following year twin Acts of Union were passed by both parliaments to create the united Kingdom of Great Britain with effect from 1 May 1707; there was popular opposition and anti-union riots in Edinburgh, Glasgow, and elsewhere.

With trade tariffs with England now abolished, trade blossomed, especially with Colonial America. The clippers belonging to the Glasgow Tobacco Lords were the fastest ships on the route to Virginia. Until the American War of Independence in 1776, Glasgow was the world's premier tobacco port, dominating world trade. The disparity between the wealth of the merchant classes of the Scottish Lowlands and the ancient clans of the Scottish Highlands grew, amplifying centuries of division.

The deposed Jacobite Stuart claimants had remained popular in the Highlands and north-east, particularly amongst non-Presbyterians, including Roman Catholics and Episcopalian Protestants. However, two major Jacobite risings launched in 1715 and 1745 failed to remove the House of Hanover from the British throne. The threat of the Jacobite movement to the United Kingdom and its monarchs effectively ended at the Battle of Culloden, Great Britain's last pitched battle.

The Scottish Enlightenment and the Industrial Revolution made Scotland into an intellectual, commercial and industrial powerhouse–so much so Voltaire said "We look to Scotland for all our ideas of civilisation." With the demise of Jacobitism and the advent of the Union, thousands of Scots, mainly Lowlanders, took up numerous positions of power in politics, civil service, the army and navy, trade, economics, colonial enterprises and other areas across the nascent British Empire. Historian Neil Davidson notes "after 1746 there was an entirely new level of participation by Scots in political life, particularly outside Scotland." Davidson also states "far from being 'peripheral' to the British economy, Scotland – or more precisely, the Lowlands – lay at its core."

In the Highlands, clan chiefs gradually started to think of themselves more as commercial landlords than leaders of their people. These social and economic changes included the first phase of the Highland Clearances and, ultimately, the demise of clanship.

The Scottish Reform Act 1832 increased the number of Scottish MPs and widened the franchise to include more of the middle classes. From the mid-century, there were increasing calls for Home Rule for Scotland and the post of Secretary of State for Scotland was revived. Towards the end of the century Prime Ministers of Scottish descent included William Gladstone, and the Earl of Rosebery. In the later 19th century the growing importance of the working classes was marked by Keir Hardie's success in the Mid Lanarkshire by-election, 1888, leading to the foundation of the Scottish Labour Party, which was absorbed into the Independent Labour Party in 1895, with Hardie as its first leader.

Glasgow became one of the largest cities in the world and known as "the Second City of the Empire" after London. After 1860 the Clydeside shipyards specialised in steamships made of iron (after 1870, made of steel), which rapidly replaced the wooden sailing vessels of both the merchant fleets and the battle fleets of the world. It became the world's pre-eminent shipbuilding centre. The industrial developments, while they brought work and wealth, were so rapid that housing, town-planning, and provision for public health did not keep pace with them, and for a time living conditions in some of the towns and cities were notoriously bad, with overcrowding, high infant mortality, and growing rates of tuberculosis.
While the Scottish Enlightenment is traditionally considered to have concluded toward the end of the 18th century, disproportionately large Scottish contributions to British science and letters continued for another 50 years or more, thanks to such figures as the physicists James Clerk Maxwell and Lord Kelvin, and the engineers and inventors James Watt and William Murdoch, whose work was critical to the technological developments of the Industrial Revolution throughout Britain. In literature, the most successful figure of the mid-19th century was Walter Scott. His first prose work, "Waverley" in 1814, is often called the first historical novel. It launched a highly successful career that probably more than any other helped define and popularise Scottish cultural identity. In the late 19th century, a number of Scottish-born authors achieved international reputations, such as Robert Louis Stevenson, Arthur Conan Doyle, J. M. Barrie and George MacDonald. Scotland also played a major part in the development of art and architecture. The Glasgow School, which developed in the late 19th century, and flourished in the early 20th century, produced a distinctive blend of influences including the Celtic Revival the Arts and Crafts movement, and Japonism, which found favour throughout the modern art world of continental Europe and helped define the Art Nouveau style. Proponents included architect and artist Charles Rennie Mackintosh.

This period saw a process of rehabilitation for Highland culture. In the 1820s, as part of the Romantic revival, tartan and the kilt were adopted by members of the social elite, not just in Scotland, but across Europe, prompted by the popularity of Macpherson's Ossian cycle and then Walter Scott's Waverley novels. However, the Highlands remained poor, the only part of mainland Britain to continue to experience recurrent famine, with a limited range of products exported out of the region, negligible industrial production, but a continued population growth that tested the subsistence agriculture. These problems, and the desire to improve agriculture and profits were the driving forces of the ongoing Highland Clearances, in which many of the population of the Highlands suffered eviction as lands were enclosed, principally so that they could be used for sheep farming. The first phase of the clearances followed patterns of agricultural change throughout Britain. The second phase was driven by overpopulation, the Highland Potato Famine and the collapse of industries that had relied on the wartime economy of the Napoleonic Wars. The behaviour of tenants and landlords varied, but overall the clearances were notorious as a result of the late timing, the lack of legal protection for year-by-year tenants under Scots law, the abruptness of the change from the traditional clan system, and the brutality of some evictions. The population of Scotland grew steadily in the 19th century, from 1,608,000 in the census of 1801 to 2,889,000 in 1851 and 4,472,000 in 1901. Even with the development of industry, there were not enough good jobs. As a result, during the period 1841–1931, about 2 million Scots migrated to North America and Australia, and another 750,000 Scots relocated to England.
After prolonged years of struggle in the Kirk, in 1834 the Evangelicals gained control of the General Assembly and passed the Veto Act, which allowed congregations to reject unwanted "intrusive" presentations to livings by patrons. The following "Ten Years' Conflict" of legal and political wrangling ended in defeat for the non-intrusionists in the civil courts. The result was a schism from the church by some of the non-intrusionists led by Dr Thomas Chalmers, known as the Great Disruption of 1843. Roughly a third of the clergy, mainly from the North and Highlands, formed the separate Free Church of Scotland. In the late 19th century growing divisions between fundamentalist Calvinists and theological liberals resulted in a further split in the Free Church as the rigid Calvinists broke away to form the Free Presbyterian Church in 1893. Catholic emancipation in 1829 and the influx of large numbers of Irish immigrants, particularly after the famine years of the late 1840s, mainly to the growing lowland centres like Glasgow, led to a transformation in the fortunes of Catholicism. In 1878, despite opposition, a Roman Catholic ecclesiastical hierarchy was restored to the country, and Catholicism became a significant denomination within Scotland.

Industrialisation, urbanisation and the Disruption of 1843 all undermined the tradition of parish schools. From 1830 the state began to fund buildings with grants; then from 1846 it was funding schools by direct sponsorship; and in 1872 Scotland moved to a system like that in England of state-sponsored largely free schools, run by local school boards. The historic University of Glasgow became a leader in British higher education by providing the educational needs of youth from the urban and commercial classes, as opposed to the upper class. The University of St Andrews pioneered the admission of women to Scottish universities. From 1892 Scottish universities could admit and graduate women and the numbers of women at Scottish universities steadily increased until the early 20th century.

Scotland played a major role in the British effort in the First World War. It especially provided manpower, ships, machinery, fish and money. With a population of 4.8 million in 1911, Scotland sent over half a million men to the war, of whom over a quarter died in combat or from disease, and 150,000 were seriously wounded. Field Marshal Sir Douglas Haig was Britain's commander on the Western Front.

The war saw the emergence of a radical movement called "Red Clydeside" led by militant trades unionists. Formerly a Liberal stronghold, the industrial districts switched to Labour by 1922, with a base among the Irish Catholic working-class districts. Women were especially active in building neighbourhood solidarity on housing issues. However, the "Reds" operated within the Labour Party and had little influence in Parliament and the mood changed to passive despair by the late 1920s.

The shipbuilding industry expanded by a third and expected renewed prosperity, but instead, a serious depression hit the economy by 1922 and it did not fully recover until 1939. The interwar years were marked by economic stagnation in rural and urban areas, and high unemployment. Indeed, the war brought with it deep social, cultural, economic, and political dislocations. Thoughtful Scots pondered their declension, as the main social indicators such as poor health, bad housing, and long-term mass unemployment, pointed to terminal social and economic stagnation at best, or even a downward spiral. Service abroad on behalf of the Empire lost its allure to ambitious young people, who left Scotland permanently. The heavy dependence on obsolescent heavy industry and mining was a central problem, and no one offered workable solutions. The despair reflected what Finlay (1994) describes as a widespread sense of hopelessness that prepared local business and political leaders to accept a new orthodoxy of centralised government economic planning when it arrived during the Second World War.

During the Second World War, Scotland was targeted by Nazi Germany largely due to its factories, shipyards, and coal mines. Cities such as Glasgow and Edinburgh were targeted by German bombers, as were smaller towns mostly located in the central belt of the country. Perhaps the most significant air-raid in Scotland was the Clydebank Blitz of March 1941, which intended to destroy naval shipbuilding in the area. 528 people were killed and 4,000 homes totally destroyed.
Perhaps Scotland's most unusual wartime episode occurred in 1941 when Rudolf Hess flew to Renfrewshire, possibly intending to broker a peace deal through the Duke of Hamilton. Before his departure from Germany, Hess had given his adjutant, Karlheinz Pintsch, a letter addressed to Hitler that detailed his intentions to open peace negotiations with the British. Pintsch delivered the letter to Hitler at the Berghof around noon on 11 May. Albert Speer later said Hitler described Hess's departure as one of the worst personal blows of his life, as he considered it a personal betrayal. Hitler worried that his allies, Italy and Japan, would perceive Hess's act as an attempt by Hitler to secretly open peace negotiations with the British.

As in World War I, Scapa Flow in Orkney served as an important Royal Navy base. Attacks on Scapa Flow and Rosyth gave RAF fighters their first successes downing bombers in the Firth of Forth and East Lothian. The shipyards and heavy engineering factories in Glasgow and Clydeside played a key part in the war effort, and suffered attacks from the Luftwaffe, enduring great destruction and loss of life. As transatlantic voyages involved negotiating north-west Britain, Scotland played a key part in the battle of the North Atlantic. Shetland's relative proximity to occupied Norway resulted in the Shetland bus by which fishing boats helped Norwegians flee the Nazis, and expeditions across the North Sea to assist resistance.

Scottish industry came out of the depression slump by a dramatic expansion of its industrial activity, absorbing unemployed men and many women as well. The shipyards were the centre of more activity, but many smaller industries produced the machinery needed by the British bombers, tanks and warships. Agriculture prospered, as did all sectors except for coal mining, which was operating mines near exhaustion. Real wages, adjusted for inflation, rose 25 percent, and unemployment temporarily vanished. Increased income, and the more equal distribution of food, obtained through a tight rationing system, dramatically improved the health and nutrition; the average height of 13-year-olds in Glasgow increased by 2 inches.

After 1945, Scotland's economic situation worsened due to overseas competition, inefficient industry, and industrial disputes. Only in recent decades has the country enjoyed something of a cultural and economic renaissance. Economic factors contributing to this recovery included a resurgent financial services industry, electronics manufacturing, (see Silicon Glen), and the North Sea oil and gas industry. The introduction in 1989 by Margaret Thatcher's government of the Community Charge (widely known as the Poll Tax) one year before the rest of Great Britain, contributed to a growing movement for Scottish control over domestic affairs. Following a referendum on devolution proposals in 1997, the Scotland Act 1998 was passed by the UK Parliament, which established a devolved Scottish Parliament and Scottish Government with responsibility for most laws specific to Scotland. The Scottish Parliament was reconvened in Edinburgh on 4 July 1999. The first First Minister of Scotland was Donald Dewar, who served until his sudden death in 2000.

The Scottish Parliament Building at Holyrood itself did not open until October 2004, after lengthy construction delays and running over budget. The Scottish Parliament has a form of proportional representation (the additional member system), which normally results in no one party having an overall majority. The pro-independence Scottish National Party led by Alex Salmond achieved this in the 2011 election, winning 69 of the 129 seats available. The success of the SNP in achieving a majority in the Scottish Parliament paved the way for the September 2014 referendum on Scottish independence. The majority voted against the proposition, with 55% voting no to independence. More powers, particularly in relation to taxation, were devolved to the Scottish Parliament after the referendum, following cross-party talks in the Smith Commission.

The mainland of Scotland comprises the northern third of the land mass of the island of Great Britain, which lies off the north-west coast of Continental Europe. The total area is , comparable to the size of the Czech Republic. Scotland's only land border is with England, and runs for between the basin of the River Tweed on the east coast and the Solway Firth in the west. The Atlantic Ocean borders the west coast and the North Sea is to the east. The island of Ireland lies only from the south-western peninsula of Kintyre; Norway is to the east and the Faroes, to the north.

The territorial extent of Scotland is generally that established by the 1237 Treaty of York between Scotland and the Kingdom of England and the 1266 Treaty of Perth between Scotland and Norway. Important exceptions include the Isle of Man, which having been lost to England in the 14th century is now a crown dependency outside of the United Kingdom; the island groups Orkney and Shetland, which were acquired from Norway in 1472; and Berwick-upon-Tweed, lost to England in 1482.

The geographical centre of Scotland lies a few miles from the village of Newtonmore in Badenoch. Rising to above sea level, Scotland's highest point is the summit of Ben Nevis, in Lochaber, while Scotland's longest river, the River Tay, flows for a distance of .

The whole of Scotland was covered by ice sheets during the Pleistocene ice ages and the landscape is much affected by glaciation. From a geological perspective, the country has three main sub-divisions.

The Highlands and Islands lie to the north and west of the Highland Boundary Fault, which runs from Arran to Stonehaven. This part of Scotland largely comprises ancient rocks from the Cambrian and Precambrian, which were uplifted during the later Caledonian orogeny. It is interspersed with igneous intrusions of a more recent age, remnants of which formed mountain massifs such as the Cairngorms and Skye Cuillins.

A significant exception to the above are the fossil-bearing beds of Old Red Sandstones found principally along the Moray Firth coast. The Highlands are generally mountainous and the highest elevations in the British Isles are found here. Scotland has over 790 islands divided into four main groups: Shetland, Orkney, and the Inner Hebrides and Outer Hebrides. There are numerous bodies of freshwater including Loch Lomond and Loch Ness. Some parts of the coastline consist of machair, a low-lying dune pasture land.

The Central Lowlands is a rift valley mainly comprising Paleozoic formations. Many of these sediments have economic significance for it is here that the coal and iron bearing rocks that fuelled Scotland's industrial revolution are found. This area has also experienced intense volcanism, Arthur's Seat in Edinburgh being the remnant of a once much larger volcano. This area is relatively low-lying, although even here hills such as the Ochils and Campsie Fells are rarely far from view.

The Southern Uplands are a range of hills almost long, interspersed with broad valleys. They lie south of a second fault line (the Southern Uplands fault) that runs from Girvan to Dunbar. The geological foundations largely comprise Silurian deposits laid down some 400–500 million years ago. The high point of the Southern Uplands is Merrick with an elevation of . The Southern Uplands is home to the UK's highest village, Wanlockhead ( above sea level).

The climate of Scotland is temperate and oceanic, and tends to be very changeable. As it is warmed by the Gulf Stream from the Atlantic, it has much milder winters (but cooler, wetter summers) than areas on similar latitudes, such as Labrador, southern Scandinavia, the Moscow region in Russia, and the Kamchatka Peninsula on the opposite side of Eurasia. However, temperatures are generally lower than in the rest of the UK, with the coldest ever UK temperature of recorded at Braemar in the Grampian Mountains, on 11 February 1895. Winter maxima average in the Lowlands, with summer maxima averaging . The highest temperature recorded was at Greycrook, Scottish Borders on 9 August 2003.

The west of Scotland is usually warmer than the east, owing to the influence of Atlantic ocean currents and the colder surface temperatures of the North Sea. Tiree, in the Inner Hebrides, is one of the sunniest places in the country: it had more than 300 hours of sunshine in May 1975. Rainfall varies widely across Scotland. The western highlands of Scotland are the wettest, with annual rainfall in a few places exceeding . In comparison, much of lowland Scotland receives less than annually. Heavy snowfall is not common in the lowlands, but becomes more common with altitude. Braemar has an average of 59 snow days per year, while many coastal areas average fewer than 10 days of lying snow per year.

Scotland's wildlife is typical of the north-west of Europe, although several of the larger mammals such as the lynx, brown bear, wolf, elk and walrus were hunted to extinction in historic times. There are important populations of seals and internationally significant nesting grounds for a variety of seabirds such as gannets. The golden eagle is something of a national icon.

On the high mountain tops, species including ptarmigan, mountain hare and stoat can be seen in their white colour phase during winter months. Remnants of the native Scots pine forest exist and within these areas the Scottish crossbill, the UK's only endemic bird species and vertebrate, can be found alongside capercaillie, Scottish wildcat, red squirrel and pine marten. Various animals have been re-introduced, including the white-tailed sea eagle in 1975, the red kite in the 1980s, and there have been experimental projects involving the beaver and wild boar. Today, much of the remaining native Caledonian Forest lies within the Cairngorms National Park and remnants of the forest remain at 84 locations across Scotland. On the west coast, remnants of ancient Celtic Rainforest still remain, particularly on the Taynish peninsula in Argyll, these forests are particularly rare due to high rates of deforestation throughout Scottish history.

The flora of the country is varied incorporating both deciduous and coniferous woodland and moorland and tundra species. However, large scale commercial tree planting and the management of upland moorland habitat for the grazing of sheep and commercial field sport activities impacts upon the distribution of indigenous plants and animals. The UK's tallest tree is a grand fir planted beside Loch Fyne, Argyll in the 1870s, and the Fortingall Yew may be 5,000 years old and is probably the oldest living thing in Europe. Although the number of native vascular plants is low by world standards, Scotland's substantial bryophyte flora is of global importance.

The population of Scotland at the 2001 Census was 5,062,011. This rose to 5,295,400, the highest ever, at the 2011 Census. The most recent ONS estimate, for mid-2017, was 5,424,800.

In the 2011 Census, 62% of Scotland's population stated their national identity as 'Scottish only', 18% as 'Scottish and British', 8% as 'British only', and 4% chose 'other identity only'.

Although Edinburgh is the capital of Scotland, the largest city is Glasgow, which has just over 584,000 inhabitants. The Greater Glasgow conurbation, with a population of almost 1.2 million, is home to nearly a quarter of Scotland's population. The Central Belt is where most of the main towns and cities are located, including Glasgow, Edinburgh, Dundee, and Perth. Scotland's only major city outside the Central Belt is Aberdeen.

In general, only the more accessible and larger islands remain inhabited. Currently, fewer than 90 remain inhabited. The Southern Uplands are essentially rural in nature and dominated by agriculture and forestry. Because of housing problems in Glasgow and Edinburgh, five new towns were designated between 1947 and 1966. They are East Kilbride, Glenrothes, Cumbernauld, Livingston, and Irvine.

Immigration since World War II has given Glasgow, Edinburgh, and Dundee small South Asian communities. In 2011, there were an estimated 49,000 ethnically Pakistani people living in Scotland, making them the largest non-White ethnic group. Since the Enlargement of the European Union more people from Central and Eastern Europe have moved to Scotland, and the 2011 census indicated that 61,000 Poles live there.
Scotland has three officially recognised languages: English, Scots, and Scottish Gaelic. Scottish Standard English, a variety of English as spoken in Scotland, is at one end of a bipolar linguistic continuum, with broad Scots at the other. Scottish Standard English may have been influenced to varying degrees by Scots. The 2011 census indicated that 63% of the population had "no skills in Scots". Others speak Highland English. Gaelic is mostly spoken in the Western Isles, where a large proportion of people still speak it; however, nationally its use is confined to just 1% of the population. The number of Gaelic speakers in Scotland dropped from 250,000 in 1881 to 60,000 in 2008.

There are many more people with Scottish ancestry living abroad than the total population of Scotland. In the 2000 Census, 9.2 million Americans self-reported some degree of Scottish descent. Ulster's Protestant population is mainly of lowland Scottish descent, and it is estimated that there are more than 27 million descendants of the Scots-Irish migration now living in the US. In Canada, the Scottish-Canadian community accounts for 4.7 million people. About 20% of the original European settler population of New Zealand came from Scotland.

In August 2012, the Scottish population reached an all-time high of 5.25 million people. The reasons given were that, in Scotland, births were outnumbering the number of deaths, and immigrants were moving to Scotland from overseas. In 2011, 43,700 people moved from Wales, Northern Ireland or England to live in Scotland.

The total fertility rate (TFR) in Scotland is below the replacement rate of 2.1 (the TFR was 1.73 in 2011). The majority of births are to unmarried women (51.3% of births were outside of marriage in 2012).
Life expectancy for those born in Scotland between 2012 and 2014 is 77.1 years for males and 81.1 years for females. This is the lowest of any of the four countries of the UK.

Just over half (54%) of the Scottish population reported being a Christian while nearly 37% reported not having a religion in a 2011 census.
Since the Scottish Reformation of 1560, the national church (the Church of Scotland, also known as The Kirk) has been Protestant in classification and Reformed in theology. Since 1689 it has had a Presbyterian system of church government and enjoys independence from the state. Its membership is 398,389, about 7.5% of the total population, though according to the 2014 Scottish Annual Household Survey, 27.8%, or 1.5 million adherents, identified Church of Scotland as their religion. The Church operates a territorial parish structure, with every community in Scotland having a local congregation.

Scotland also has a significant Roman Catholic population, 19% professing that faith, particularly in Greater Glasgow and the north-west. After the Reformation, Roman Catholicism in Scotland continued in the Highlands and some western islands like Uist and Barra, and it was strengthened during the 19th century by immigration from Ireland. Other Christian denominations in Scotland include the Free Church of Scotland, and various other Presbyterian offshoots. Scotland's third largest church is the Scottish Episcopal Church.

Islam is the largest non-Christian religion (estimated at around 75,000, which is about 1.4% of the population), and there are also significant Jewish, Hindu and Sikh communities, especially in Glasgow. The Samyé Ling monastery near Eskdalemuir, which celebrated its 40th anniversary in 2007, is the first Buddhist monastery in western Europe.

The head of state of the United Kingdom is the monarch, currently Queen Elizabeth II (since 1952). The regnal numbering ("Elizabeth II") caused controversy around the time of her coronation because there had never been an Elizabeth I in Scotland. The British government stated in April 1953 that future British monarchs would be numbered according to either their English or their Scottish predecessors, whichever number would be higher. For instance, any future King James would be styled James VIIIsince the last Scottish King James was James VII (also James II of England, etc.)while the next King Henry would be King Henry IX throughout the UK even though there have been no Scottish kings of that name. A legal action, MacCormick v Lord Advocate (1953 SC 396), was brought in Scotland to contest the right of the Queen to entitle herself "Elizabeth II" within Scotland, but the Crown won the case.

The monarchy of the United Kingdom continues to use a variety of styles, titles and other royal symbols of statehood specific to pre-union Scotland, including: the Royal Standard of Scotland, the Royal coat of arms used in Scotland together with its associated Royal Standard, royal titles including that of Duke of Rothesay, certain Great Officers of State, the chivalric Order of the Thistle and, since 1999, reinstating a ceremonial role for the Crown of Scotland after a 292-year hiatus.

Scotland has limited self-government within the United Kingdom, as well as representation in the UK Parliament. Executive and legislative powers respectively have been devolved to the Scottish Government and the Scottish Parliament at Holyrood in Edinburgh since 1999. The UK Parliament retains control over reserved matters specified in the Scotland Act 1998, including UK taxes, social security, defence, international relations and broadcasting. The Scottish Parliament has legislative authority for all other areas relating to Scotland. It initially had only a limited power to vary income tax, but powers over taxation and social security were significantly expanded by the Scotland Acts of 2012 and 2016.

The Scottish Parliament can give legislative consent over devolved matters back to the UK Parliament by passing a Legislative Consent Motion if United Kingdom-wide legislation is considered more appropriate for a certain issue. The programmes of legislation enacted by the Scottish Parliament have seen a divergence in the provision of public services compared to the rest of the UK. For instance, university education and care services for the elderly are free at point of use in Scotland, while fees are paid in the rest of the UK. Scotland was the first country in the UK to ban smoking in enclosed public places.

The Scottish Parliament is a unicameral legislature with 129 members (MSPs): 73 of them represent individual constituencies and are elected on a first-past-the-post system; the other 56 are elected in eight different electoral regions by the additional member system. MSPs serve for a four-year period (exceptionally five years from 2011–16). The Parliament nominates one of its Members, who is then appointed by the Monarch to serve as First Minister. Other ministers are appointed by the First Minister and serve at his/her discretion. Together they make up the Scottish Government, the executive arm of the devolved government. The Scottish Government is headed by the First Minister, who is accountable to the Scottish Parliament and is the minister of charge of the Scottish Government. The First Minister is also the political leader of Scotland. The Scottish Government also comprises the Deputy First Minister, currently John Swinney MSP, who deputises for the First Minister during a period of absence of overseas visits. Alongside the Deputy First Minister's requirements as Deputy, the minister also has a cabinet ministerial responsibility. Swinney is also currently Cabinet Secretary for Education and Skills. The Scottish Government's cabinet comprises nine cabinet secretaries, who form the Cabinet of Scotland. There are also twelve other ministers, who work alongside the cabinet secretaries in their appointed areas. As a result, junior ministers do not attend cabinet meetings.

In the 2016 election, the Scottish National Party (SNP) won 63 of the 129 seats available. Nicola Sturgeon, the leader of the SNP, has been the First Minister since November 2014. The Conservative Party became the largest opposition party in the 2016 elections, with the Labour Party, Liberal Democrats and the Green Party also represented in the Parliament. The next Scottish Parliament election is due to be held on 6 May 2021.

Scotland is represented in the British House of Commons by 59 MPs elected from territory-based Scottish constituencies. In the 2017 general election, the SNP won 35 of the 59 seats. This represented a significant decline from the 2015 general election, when the SNP won 56 seats. Conservative, Labour and Liberal Democrat parties also represent Scottish constituencies in the House of Commons. The next United Kingdom general election is scheduled for 5 May 2022. The Scotland Office represents the UK government in Scotland on reserved matters and represents Scottish interests within the UK government. The Scotland Office is led by the Secretary of State for Scotland, who sits in the Cabinet of the United Kingdom; the incumbent is Conservative MP David Mundell.

The relationships between the central UK Government and devolved governments of Scotland, Wales and Northern Ireland are based on the extra-statutory principles and agreements with the main elements are set out in a "Memorandum of Understanding" between the UK government and the devolved governments of Scotland, Wales and Northern Ireland. The MOU lays emphasis on the principles of good communication, consultation and co-operation.

Since devolution in 1999, Scotland has devolved stronger working relations across the two other devolved governments, the Welsh Government and Northern Ireland Executive. Whilst there are no formal concordats between the Scottish Government, Welsh Government and Northern Ireland Executive, ministers from each devolved government meet at various points throughout the year at various events such as the British-Irish Council and also meet to discuss matters and issues that is devolved to each government. Scotland, along with the Welsh Government, British Government as well as the Northern Ireland executive, participate in the Joint Ministerial Committee (JMC) which allows each government to discuss policy issues together and work together across each government to find solutions. The Scottish Government considers the successful re-establishment of the Plenary, and establishment of the Domestic fora to be important facets of the relationship with the UK Government and the other devolved administrations.

In the aftermath of the United Kingdom's decision to withdraw from the European Union in 2016, the Scottish Government has called for there to be a joint approach from each of the devolved governments. In early 2017, the devolved governments met to discuss Brexit and agree on Brexit strategies from each devolved government which lead for Theresa May to issue a statement that claims that the devolved governments will not have a central role or decision making process in the Brexit process, but that the UK Government plans to "fully engage" Scotland in talks alongside the governments of Wales and Northern Ireland.

Whilst foreign policy remains a reserved matter, the Scottish Government still has the power and ability to strengthen and develop Scotland, the economy and Scottish interests on the world stage and encourage foreign businesses, international devolved, regional and central governments to invest in Scotland. Whilst the First Minister usually undertakes a number of foreign and international visits to promote Scotland, international relations, European and Commonwealth relations are also included within the portfolios of both the Cabinet Secretary for Culture, Tourism and External Affairs (responsible for international development) and the Minister for International Development and Europe (responsible for European Union relations and international relations).

During the G8 Summit in 2005, then First Minister Jack McConnell welcomed each head of government of the G8 nations to the countries Glasgow Prestwick Airport on behalf of then UK Prime Minister Tony Blair. At the same time, McConnell and the then Scottish Executive pioneered the way forward to launch what would become the Scotland Malawi Partnership which co-ordinates Scottish activities to strengthen existing links with Malawi. During McConnell's time as First Minister, several relations with Scotland, including Scottish and Russian relations strengthened following a visit by President of Russia Vladimir Putin to Edinburgh. McConnell, speaking at the end, highlighted that the visit by Putin was a "post-devolution" step towards "Scotland regaining it's international identity".

Under the Salmond administration, Scotland's trade and investment deals with countries such as China and Canada, where Salmond established the Canada Plan 2010–2015 which aimed to strengthen "the important historical, cultural and economic links" between both Canada and Scotland. To promote Scotland's interests and Scottish businesses in North America, there is a Scottish Affairs Office located in Washington, D.C. with the aim to promoting Scotland in both the United States and Canada.

During a 2017 visit to the United States, First Minister Nicola Sturgeon met with Jerry Brown, Governor of California, where both signed an agreement committing both the Government of California and the Scottish Government to work together to tackle climate change, as well as Sturgeon signing a £6.3 million deal for Scottish investment from American businesses and firms promoting trade, tourism and innovation. During an official visit to the Republic of Ireland in 2016, Sturgeon claimed that is it "important for Ireland and Scotland and the whole of the British Isles that Ireland has a strong ally in Scotland". During the same engagement, Sturgeon became the first head of government to address the Seanad Éireann, the Upper House of the Irish Parliament.

A policy of devolution had been advocated by the three main UK parties with varying enthusiasm during recent history. A previous Labour leader. John Smith, described the revival of a Scottish parliament as the "settled will of the Scottish people". The devolved Scottish Parliament was created after a referendum in 1997 found majority support for both creating the Parliament and granting it limited powers to vary income tax. The constitutional status of Scotland is nonetheless subject to ongoing debate.

The Scottish National Party (SNP), which supports Scottish independence, was first elected to form the Scottish Government in 2007. The new government established a "National Conversation" on constitutional issues, proposing a number of options such as increasing the powers of the Scottish Parliament, federalism, or a referendum on Scottish independence from the United Kingdom. In rejecting the last option, the three main opposition parties in the Scottish Parliament created a commission to investigate the distribution of powers between devolved Scottish and UK-wide bodies. The Scotland Act 2012, based on proposals by the commission, was subsequently enacted devolving additional powers to the Scottish Parliament.

In August 2009 the SNP proposed a bill to hold a referendum on independence in November 2010. Opposition from all other major parties led to an expected defeat. After the 2011 elections gave the SNP an overall majority in the Scottish Parliament, a referendum on independence for Scotland was held on 18 September 2014. The referendum resulted in a rejection of independence, by 55.3% to 44.7%. During the campaign, the three main parties in the UK Parliament pledged to extend the powers of the Scottish Parliament. An all-party commission chaired by Lord Smith of Kelvin was formed, which led to a further devolution of powers through the Scotland Act 2016.

Following a referendum on the UK's membership of the European Union on 23 June 2016, where a UK-wide majority voted to withdraw from the EU whilst a majority within Scotland voted to remain, Scotland's First Minister, Nicola Sturgeon, announced that as a result a new independence referendum was "highly likely".

Historical subdivisions of Scotland included the mormaerdom, stewartry, earldom, burgh, parish, county and regions and districts. Some of these names are still sometimes used as geographical descriptors.

Modern Scotland is subdivided in various ways depending on the purpose. In local government, there have been 32 single-tier council areas since 1996, whose councils are responsible for the provision of all local government services. Community councils are informal organisations that represent specific sub-divisions of a council area.

In the Scottish Parliament, there are 73 constituencies and eight regions. For the Parliament of the United Kingdom, there are 59 constituencies. Until 2013, the Scottish fire brigades and police forces were based on a system of regions introduced in 1975. For healthcare and postal districts, and a number of other governmental and non-governmental organisations such as the churches, there are other long-standing methods of subdividing Scotland for the purposes of administration.

City status in the United Kingdom is conferred by letters patent. There are seven cities in Scotland: Aberdeen, Dundee, Edinburgh, Glasgow, Inverness, Stirling and Perth.

Scots law has a basis derived from Roman law, combining features of both uncodified civil law, dating back to the "Corpus Juris Civilis", and common law with medieval sources. The terms of the Treaty of Union with England in 1707 guaranteed the continued existence of a separate legal system in Scotland from that of England and Wales. Prior to 1611, there were several regional law systems in Scotland, most notably Udal law in Orkney and Shetland, based on old Norse law. Various other systems derived from common Celtic or Brehon laws survived in the Highlands until the 1800s.

Scots law provides for three types of courts responsible for the administration of justice: civil, criminal and heraldic. The supreme civil court is the Court of Session, although civil appeals can be taken to the Supreme Court of the United Kingdom (or before 1 October 2009, the House of Lords). The High Court of Justiciary is the supreme criminal court in Scotland. The Court of Session is housed at Parliament House, in Edinburgh, which was the home of the pre-Union Parliament of Scotland with the High Court of Justiciary and the Supreme Court of Appeal currently located at the Lawnmarket. The sheriff court is the main criminal and civil court, hearing most cases. There are 49 sheriff courts throughout the country. District courts were introduced in 1975 for minor offences and small claims. These were gradually replaced by Justice of the Peace Courts from 2008 to 2010. The Court of the Lord Lyon regulates heraldry.

For many decades the Scots legal system was unique for being the only legal system without a parliament. This ended with the advent of the Scottish Parliament, which legislates for Scotland. Many features within the system have been preserved. Within criminal law, the Scots legal system is unique in having three possible verdicts: "guilty", "not guilty" and ""not proven"". Both "not guilty" and "not proven" result in an acquittal, typically with no possibility of retrial in accordance with the rule of double jeopardy. There is, however, the possibility of a retrial where new evidence emerges at a later date that might have proven conclusive in the earlier trial at first instance, where the person acquitted subsequently admits the offence or where it can be proved that the acquittal was tainted by an attempt to pervert the course of justice – see the provisions of the Double Jeopardy (Scotland) Act 2011. Many laws differ between Scotland and the other parts of the United Kingdom, and many terms differ for certain legal concepts. Manslaughter, in England and Wales, is broadly similar to culpable homicide in Scotland, and arson is called wilful fire raising. Indeed, some acts considered crimes in England and Wales, such as forgery, are not so in Scotland. Procedure also differs. Scots juries, sitting in criminal cases, consist of fifteen jurors, which is three more than is typical in many countries.

The Scottish Prison Service (SPS) manages the prisons in Scotland, which collectively house over 8,500 prisoners. The Cabinet Secretary for Justice is responsible for the Scottish Prison Service within the Scottish Government.

Health care in Scotland is mainly provided by NHS Scotland, Scotland's public health care system. This was founded by the National Health Service (Scotland) Act 1947 (later repealed by the National Health Service (Scotland) Act 1978) that took effect on 5 July 1948 to coincide with the launch of the NHS in England and Wales. However, even prior to 1948, half of Scotland's landmass was already covered by state-funded health care, provided by the Highlands and Islands Medical Service. Healthcare policy and funding is the responsibility of the Scottish Government's Health Directorates. The current Cabinet Secretary for Health and Sport is Shona Robison and the Director-General (DG) Health and chief executive, NHS Scotland is Paul Gray.

In 2008, the NHS in Scotland had around 158,000 staff including more than 47,500 nurses, midwives and health visitors and over 3,800 consultants. There are also more than 12,000 doctors, family practitioners and allied health professionals, including dentists, opticians and community pharmacists, who operate as independent contractors providing a range of services within the NHS in return for fees and allowances. These fees and allowances were removed in May 2010, and prescriptions are entirely free, although dentists and opticians may charge if the patient's household earns over a certain amount, about £30,000 per annum.

The Economy of Scotland had an estimated nominal gross domestic product (GDP) of up to £152 billion in 2015. In 2014, Scotland's per capita GDP was one of the highest in the EU. Scotland has a Western-style open mixed economy closely linked with the rest of the UK and the wider world. Traditionally, the Scottish economy has been dominated by heavy industry underpinned by shipbuilding in Glasgow, coal mining and steel industries. Petroleum related industries associated with the extraction of North Sea oil have also been important employers from the 1970s, especially in the north-east of Scotland.

In February 2012, the Centre for Economics and Business Research concluded that "Scotland receives no net subsidy" from the UK, as greater per capita tax generation in Scotland balanced out greater per capita public spending. More recent data, from 2012–13, show that Scotland generated 9.1% (£53.1bn; this included a geographical share of North Sea oil revenue – without it, the figures were 8.2% and £47.6bn) of the UK's tax revenues and received 9.3% (£65.2bn) of spending. Scotland's public spending deficit in 2012–13 was £12bn, a £3.5bn increase on the previous year; over the same period, the UK's deficit decreased by £2.6bn. Over the past thirty years, Scotland contributed a relative budget surplus of almost £20billion to the UK economy.

In the final quarter of 2016, the Scottish economy contracted by 0.2%; the UK as a whole grew by 0.7% in the same period. As of September 2015, the Scottish unemployment rate of 5.9% was above the UK rate of 5.5%, while the Scottish employment rate of 74.0% was higher than the UK figure of 73.5%. De-industrialisation during the 1970s and 1980s saw a shift from a manufacturing focus towards a more service-oriented economy.

Edinburgh is the financial services centre of Scotland, with many large finance firms based there, including: Lloyds Banking Group (owners of HBOS); the Government-owned Royal Bank of Scotland and Standard Life. Edinburgh was ranked 15th in the list of world financial centres in 2007, but fell to 37th in 2012, following damage to its reputation, and in 2016 was ranked 56th out of 86.

In 2014, total Scottish exports (excluding intra-UK trade) were estimated to be £27.5 billion. Scotland's primary exports include whisky, electronics and financial services. The United States, Netherlands, Germany, France, and Norway constitute the country's major export markets. Scotland's Gross Domestic Product (GDP), including oil and gas produced in Scottish waters, was estimated at £150 billion for the calendar year 2012. If Scotland became independent, it would hold 95% of the UK's current oil and gas reserves if they were split geographically using a median line from the English-Scottish border. If the reserves were split by population, that figure would be reduced to 9%.

Whisky is one of Scotland's more known goods of economic activity. Exports increased by 87% in the decade to 2012 and were valued at £4.3 billion in 2013, which was 85% of Scotland's food and drink exports. It supports around 10,000 jobs directly and 25,000 indirectly. It may contribute £400–682 million to Scotland, rather than several billion pounds, as more than 80% of whisky produced is owned by non-Scottish companies.

A briefing published in 2002 by the Scottish Parliament Information Centre (SPICe) for the Scottish Parliament's Enterprise and Life Long Learning Committee stated that tourism accounted for up to 5% of GDP and 7.5% of employment.

Although the Bank of England is the central bank for the UK, three Scottish clearing banks issue Sterling banknotes: the Bank of Scotland; the Royal Bank of Scotland; and the Clydesdale Bank. The value of the Scottish banknotes in circulation in 2013 was £3.8 billion; underwritten by the Bank of England using funds deposited by each clearing bank, under the Banking Act, (2009), in order to cover the total value of such notes in circulation.

Of the money spent on UK defence, about £3.3 billion can be attributed to Scotland as of 2013. Although Scotland has a long military tradition predating the Treaty of Union with England, its armed forces now form part of the British Armed Forces, with the exception of the Atholl Highlanders, Europe's only legal private army. In 2006, the infantry regiments of the Scottish Division were amalgamated to form the Royal Regiment of Scotland. Other distinctively Scottish regiments in the British Army include the Scots Guards, the Royal Scots Dragoon Guards and the 154 (Scottish) Regiment RLC, an Army Reserve Regiment of the Royal Logistic Corps.

Because of their topography and perceived remoteness, parts of Scotland have housed many sensitive defence establishments. Between 1960 and 1991, the Holy Loch was a base for the US fleet of Polaris ballistic missile submarines. Today, Her Majesty's Naval Base Clyde, north-west of Glasgow, is the base for the four Trident-armed "Vanguard" class ballistic missile submarines that comprise the UK's nuclear deterrent. Scapa Flow was the major Fleet base for the Royal Navy until 1956.

A single front-line Royal Air Force base is located in Scotland. RAF Lossiemouth, located in Moray, is the most northerly air defence fighter base in the United Kingdom and is home to three fast-jet squadrons equipped with the Eurofighter Typhoon.

The Scottish education system has always been distinct from the rest of the United Kingdom, with a characteristic emphasis on a broad education. In the 15th century, the Humanist emphasis on education cumulated with the passing of the Education Act 1496, which decreed that all sons of barons and freeholders of substance should attend grammar schools to learn "perfyct Latyne", resulting in an increase in literacy among a male and wealthy elite. In the Reformation, the 1560 "First Book of Discipline" set out a plan for a school in every parish, but this proved financially impossible. In 1616 an act in Privy council commanded every parish to establish a school. By the late seventeenth century there was a largely complete network of parish schools in the lowlands, but in the Highlands basic education was still lacking in many areas. Education remained a matter for the church rather than the state until the Education Act (1872).

The "Curriculum for Excellence", Scotland's national school curriculum, presently provides the curricular framework for children and young people from age 3 to 18. All 3- and 4-year-old children in Scotland are entitled to a free nursery place. Formal primary education begins at approximately 5 years old and lasts for 7 years (P1–P7); children in Scotland study Standard Grades, or Intermediate qualifications between the ages of 14 and 16. These are being phased out and replaced by the National Qualifications of the Curriculum for Excellence. The school leaving age is 16, after which students may choose to remain at school and study for Access, Intermediate or Higher Grade and Advanced Higher qualifications. A small number of students at certain private, independent schools may follow the English system and study towards GCSEs and A and AS-Levels instead.

There are fifteen Scottish universities, some of which are amongst the oldest in the world. These include the University of St Andrews, the University of Glasgow, the University of Aberdeen and the University of Edinburgh—many of which are ranked amongst the best in the UK. Proportionally, Scotland had more universities in QS' World University Rankings' top 100 in 2012 than any other nation. The country produces 1% of the world's published research with less than 0.1% of the world's population, and higher education institutions account for 9% of Scotland's service sector exports. Scotland's University Courts are the only bodies in Scotland authorised to award degrees.

Tuition is handled by the Student Awards Agency Scotland (SAAS), which does not charge fees to what it defines as "Young Students". Young Students are defined as those under 25, without children, marriage, civil partnership or cohabiting partner, who have not been outside of full-time education for more than three years. Fees exist for those outside the young student definition, typically from £1,200 to £1,800 for undergraduate courses, dependent on year of application and type of qualification. Postgraduate fees can be up to £3,400. The system has been in place since 2007 when graduate endowments were abolished.<ref name="www.scotland.gov.uk/News/Releases/2008/02/28172530"></ref> Labour's education spokesperson Rhona Brankin criticised the Scottish system for failing to address student poverty. Scotland has fewer disadvantaged students than England, Wales or Northern Ireland and disadvantaged students receive around £560 a year less in financial support than their counterparts in England do.

Scotland's universities are complemented in the provision of Further and Higher Education by 43 colleges. Colleges offer National Certificates, Higher National Certificates, and Higher National Diplomas. These Group Awards, alongside Scottish Vocational Qualifications, aim to ensure Scotland's population has the appropriate skills and knowledge to meet workplace needs. In 2014, research reported by the Office for National Statistics found that Scotland was the most highly educated country in Europe and among the most well-educated in the world in terms of tertiary education attainment, with roughly 40% of people in Scotland aged 16–64 educated to NVQ level 4 and above. Based on the original data for EU statistical regions, all four Scottish regions ranked significantly above the European average for completion of tertiary-level education by 25- to 64-year-olds.

Scottish music is a significant aspect of the nation's culture, with both traditional and modern influences. A famous traditional Scottish instrument is the Great Highland bagpipe, a wind instrument consisting of three drones and a melody pipe (called the chanter), which are fed continuously by a reservoir of air in a bag. Bagpipe bands, featuring bagpipes and various types of drums, and showcasing Scottish music styles while creating new ones, have spread throughout the world. The clàrsach (harp), fiddle and accordion are also traditional Scottish instruments, the latter two heavily featured in Scottish country dance bands. There are many successful Scottish bands and individual artists in varying styles including Annie Lennox, Amy Macdonald, Runrig, Belle and Sebastian, Boards of Canada, Camera Obscura, Cocteau Twins, Deacon Blue, Franz Ferdinand, Susan Boyle, Emeli Sandé, Texas, The View, The Fratellis, Twin Atlantic and Biffy Clyro. Other Scottish musicians include Shirley Manson, Paolo Nutini, Andy Stewart and Calvin Harris.

Scotland has a literary heritage dating back to the early Middle Ages. The earliest extant literature composed in what is now Scotland was in Brythonic speech in the 6th century, but is preserved as part of Welsh literature. Later medieval literature included works in Latin, Gaelic, Old English and French. The first surviving major text in Early Scots is the 14th-century poet John Barbour's epic "Brus", focusing on the life of Robert I, and was soon followed by a series of vernacular romances and prose works. In the 16th century, the crown's patronage helped the development of Scots drama and poetry, but the accession of James VI to the English throne removed a major centre of literary patronage and Scots was sidelined as a literary language. Interest in Scots literature was revived in the 18th century by figures including James Macpherson, whose Ossian Cycle made him the first Scottish poet to gain an international reputation and was a major influence on the European Enlightenment. It was also a major influence on Robert Burns, whom many consider the national poet, and Walter Scott, whose Waverley Novels did much to define Scottish identity in the 19th century. Towards the end of the Victorian era a number of Scottish-born authors achieved international reputations as writers in English, including Robert Louis Stevenson, Arthur Conan Doyle, J. M. Barrie and George MacDonald. In the 20th century the Scottish Renaissance saw a surge of literary activity and attempts to reclaim the Scots language as a medium for serious literature. Members of the movement were followed by a new generation of post-war poets including Edwin Morgan, who would be appointed the first Scots Makar by the inaugural Scottish government in 2004. From the 1980s Scottish literature enjoyed another major revival, particularly associated with a group of writers including Irvine Welsh. Scottish poets who emerged in the same period included Carol Ann Duffy, who, in May 2009, was the first Scot named UK Poet Laureate.

As one of the Celtic nations, Scotland and Scottish culture is represented at interceltic events at home and over the world. Scotland hosts several music festivals including Celtic Connections (Glasgow), and the Hebridean Celtic Festival (Stornoway). Festivals celebrating Celtic culture, such as Festival Interceltique de Lorient (Brittany), the Pan Celtic Festival (Ireland), and the National Celtic Festival (Portarlington, Australia), feature elements of Scottish culture such as language, music and dance.

The image of St. Andrew, martyred while bound to an X-shaped cross, first appeared in the Kingdom of Scotland during the reign of William I. Following the death of King Alexander III in 1286 an image of Andrew was used on the seal of the Guardians of Scotland who assumed control of the kingdom during the subsequent interregnum. Use of a simplified symbol associated with Saint Andrew, the saltire, has its origins in the late 14th century; the Parliament of Scotland decreeing in 1385 that Scottish soldiers should wear a white Saint Andrew's Cross on the front and back of their tunics. Use of a blue background for the Saint Andrew's Cross is said to date from at least the 15th century. Since 1606 the saltire has also formed part of the design of the Union Flag. There are numerous other symbols and symbolic artefacts, both official and unofficial, including the thistle, the nation's floral emblem (celebrated in the song, The Thistle o' Scotland), the Declaration of Arbroath, incorporating a statement of political independence made on 6 April 1320, the textile pattern tartan that often signifies a particular Scottish clan and the royal Lion Rampant flag. Highlanders can thank James Graham, 3rd Duke of Montrose, for the repeal in 1782 of the Act of 1747 prohibiting the wearing of tartans.

Although there is no official national anthem of Scotland, "Flower of Scotland" is played on special occasions and sporting events such as football and rugby matches involving the Scotland national teams and since 2010 is also played at the Commonwealth Games after it was voted the overwhelming favourite by participating Scottish athletes. Other currently less popular candidates for the National Anthem of Scotland include "Scotland the Brave", "Highland Cathedral", "Scots Wha Hae" and "A Man's A Man for A' That".

St Andrew's Day, 30 November, is the national day, although Burns' Night tends to be more widely observed, particularly outside Scotland. In 2006, the Scottish Parliament passed the St Andrew's Day Bank Holiday (Scotland) Act 2007, designating the day an official bank holiday. Tartan Day is a recent innovation from Canada.

The national animal of Scotland is the unicorn, which has been a Scottish heraldic symbol since the 12th century.

Scottish cuisine has distinctive attributes and recipes of its own but shares much with wider British and European cuisine as a result of local and foreign influences, both ancient and modern. Traditional Scottish dishes exist alongside international foodstuffs brought about by migration. Scotland's natural larder of game, dairy products, fish, fruit, and vegetables is the chief factor in traditional Scots cooking, with a high reliance on simplicity and a lack of spices from abroad, as these were historically rare and expensive. Irn-Bru is the most common Scottish carbonated soft drink, often described as "Scotland's other national drink" (after whisky). During the Late Middle Ages and early modern era, French cuisine played a role in Scottish cookery due to cultural exchanges brought about by the "Auld Alliance", especially during the reign of Mary, Queen of Scots. Mary, on her return to Scotland, brought an entourage of French staff who are considered responsible for revolutionising Scots cooking and for some of Scotland's unique food terminology.

National newspapers such as the "Daily Record", "The Herald", "The Scotsman" and "The National" are all produced in Scotland. Important regional dailies include the Evening News in Edinburgh "The Courier" in Dundee in the east, and "The Press and Journal" serving Aberdeen and the north. Scotland is represented at the Celtic Media Festival, which showcases film and television from the Celtic countries. Scottish entrants have won many awards since the festival began in 1980.

Television in Scotland is largely the same as UK-wide broadcasts, however, the national broadcaster is BBC Scotland, a constituent part of the British Broadcasting Corporation, the publicly funded broadcaster of the United Kingdom. It runs three national television stations, and the national radio stations, "BBC Radio Scotland" and "BBC Radio nan Gàidheal", amongst others. Scotland also has some programming in the Gaelic language. BBC Alba is the national Gaelic-language channel. The main Scottish commercial television station is STV.

Scotland hosts its own national sporting competitions and has independent representation at several international sporting events, including the FIFA World Cup, the Rugby Union World Cup, the Rugby League World Cup, the Cricket World Cup, the Netball World Cup and the Commonwealth Games. Scotland has its own national governing bodies, such as the Scottish Football Association (the second oldest national football association in the world) and the Scottish Rugby Union. Variations of football have been played in Scotland for centuries, with the earliest reference dating back to 1424.

Association football is the most popular sport and the Scottish Cup is the world's oldest national trophy.
Scotland contested the first ever international football game in 1872 against England. The match took place at Hamilton Crescent, Glasgow, home of the West of Scotland Cricket Club. Scottish clubs have been successful in European competitions with Celtic winning the European Cup in 1967, Rangers and Aberdeen winning the UEFA Cup Winners' Cup in 1972 and 1983 respectively, and Aberdeen also winning the UEFA Super Cup in 1983.

With the modern game of golf originating in 15th-century Scotland, the country is promoted as the home of golf. To many golfers the Old Course in the Fife town of St Andrews, an ancient links course dating to before 1552, is considered a site of pilgrimage. In 1764, the standard 18-hole golf course was created at St Andrews when members modified the course from 22 to 18 holes. The world's oldest golf tournament, and golf's first major, is The Open Championship, which was first played on 17 October 1860 at Prestwick Golf Club, in Ayrshire, Scotland, with Scottish golfers winning the earliest majors. There are many other famous golf courses in Scotland, including Carnoustie, Gleneagles, Muirfield, and Royal Troon. Other distinctive features of the national sporting culture include the Highland games, curling and shinty. In boxing, Scotland has had 13 world champions, including Ken Buchanan, Benny Lynch and Jim Watt.

Scotland has competed at every Commonwealth Games since 1930 and has won 356 medals in total—91 Gold, 104 Silver and 161 Bronze. Edinburgh played host to the Commonwealth Games in 1970 and 1986, and most recently Glasgow in 2014.

The Scottish motorways and major trunk roads are managed by Transport Scotland. The remainder of the road network is managed by the Scottish local authorities in each of their areas.

Scotland has five main international airports (Glasgow, Edinburgh, Aberdeen, Glasgow Prestwick and Inverness), which together serve 150 international destinations with a wide variety of scheduled and chartered flights. GIP operates Edinburgh airport and AGS operates Aberdeen and Glasgow International, while Highlands and Islands Airports operates 11 regional airports, including Inverness, which serve the more remote locations. The Scottish Government owns Glasgow Prestwick, having purchased the airport from Infratil for a nominal sum.

Over the period of history, Scotland has had several national airlines that has acted as the countries flag carrier, however, most of which are now defunct. Airline companies such as Air Scotland, Caledonian Airways, Scottish Airlines and Highland Airways (founded as Air Alba), all at one stage was seen to be Scotland's national airline and flag carrier. Loganair, still in operation and mostly operations in the Scottish highlands and serving the outer islands of Scotland, is largely considered to be the modern day flag carrier of Scotland and in 2017 to honour this title, Loganair revamped and introduced new and current airlines with their updated Tartan Aircraft livery to help bring "a new Scottish identify to the skies".

Network Rail owns and operates the fixed infrastructure assets of the railway system in Scotland, while the Scottish Government retains overall responsibility for rail strategy and funding in Scotland. Scotland's rail network has around 350 railway stations and of track. Over 89.3million passenger journeys are made each year.

The East Coast and West Coast main railway lines connect the major cities and towns of Scotland with each other and with the rail network in England. London North Eastern Railway provides inter-city rail journeys between Glasgow, Edinburgh, Aberdeen and Inverness to London. Domestic rail services within Scotland are operated by Abellio ScotRail. During the time of British Rail, the West Coast Main Line from London Euston to Glasgow Central was electrified in the early 1970s, followed by the East Coast Main Line in the late 1980s. British Rail created the ScotRail brand. When British Rail existed, many railway lines in Strathclyde were electrified. Strathclyde Passenger Transport Executive was at the forefront with the acclaimed "largest electrified rail network outside London". Some parts of the network are electrified, but there are no electrified lines in the Highlands, Angus, Aberdeenshire, the cities of Dundee or Aberdeen, or Perth & Kinross, and none of the islands has a rail link (although the railheads at Kyle of Lochalsh and Mallaig principally serve the islands).

The East Coast Main Line crosses the Firth of Forth by the Forth Bridge. Completed in 1890, this cantilever bridge has been described as "the one internationally recognised Scottish landmark". Scotland's rail network is managed by Transport Scotland.

Regular ferry services operate between the Scottish mainland and outlying islands. Ferries serving both the inner and outer Hebrides are principally operated by the state-owned enterprise Caledonian MacBrayne.

Services to the Northern Isles are operated by Serco. Other routes, served by multiple companies, connect southwest Scotland to Northern Ireland. DFDS Seaways operate a freight-only service from Rosyth, near Edinburgh, to Zeebrugge, Belgium.

Additional routes are operated by local authorities.

Increasing amounts of Scotland's electricity are generated through solar power and wind power, a sizable proportion of Scotland's electricity is generated that way.




</doc>
<doc id="26995" url="https://en.wikipedia.org/wiki?curid=26995" title="Shire">
Shire

A shire is a traditional term for a division of land, found in Great Britain and some other English speaking countries. It was first used in Wessex from the beginning of Anglo-Saxon settlement, and spread to most of the rest of England in the tenth century. In some rural parts of Australia, a shire is a local government area; however, in Australia it is not synonymous with a "county", which is a lands administrative division.

The word derives from the Old English "scir", itself a derivative of the Proto-Germanic "skizo" (cf. Old High German "scira"), meaning care or official charge. In the UK, "shire" is the original term for what is usually known now as a "county"; the word "county" having been introduced at the Norman Conquest of England. The two are nearly synonymous. Although in modern British usage counties are referred to as "shires" mainly in poetic contexts, terms such as Shire Hall remain common. Shire also remains a common part of many county names. 

In regions with so-called rhotic pronunciation such as Scotland, the word "shire" is pronounced . In non-rhotic areas the final R is silent unless the next word begins in a vowel. When "shire" is a suffix as part of a placename in England, the vowel is unstressed and thus usually shortened and/or monophthongised: pronunciations include , or sometimes , with the pronunciation of the final R again depending on rhoticity. In many words, the vowel is normally reduced all the way to a single schwa, as in for instance "Leicestershire" or "Berkshire" . Outside England, and especially in Scotland and the US, it is more common for "shire" as part of a placename to be pronounced identically to the full word, as a result of spelling pronunciation.

The system was first used in Wessex from the beginning of Anglo-Saxon settlement, and spread to most of the rest of England in the tenth century, along with West Saxon political control. In Domesday (1086) the city of York was divided into shires. The first shires of Scotland were created in English-settled areas such as Lothian and the Borders, in the ninth century. King David I more consistently created shires and appointed sheriffs across lowland "shores" of Scotland.

The shire in early days was governed by an "Ealdorman" and in the later Anglo-Saxon period by royal official known as a "shire reeve" or sheriff. The shires were divided into hundreds or wapentakes, although other less common sub-divisions existed. An alternative name for a shire was a "sheriffdom" until sheriff court reforms separated the two concepts. The phrase "shire county" applies, unofficially, to non-metropolitan counties in England, specifically those that are not local Unitary authority areas. In Scotland the word "county" was not adopted for the shires. Although "county" appears in some texts, "shire" was the normal name until counties for statutory purposes were created in the nineteenth century.

"Shire" also refers, in a narrower sense, to ancient counties with names that ended in "shire". These counties are typically (though not always) named after their county town. The suffix "-shire" is attached to most of the names of English, Scottish and Welsh counties. It tends not to be found in the names of shires that were pre-existing divisions. Essex, Kent, and Sussex, for example, have never borne a "-shire", as each represents a former Anglo-Saxon kingdom. Similarly Cornwall was a British kingdom before it became an English county. The term 'shire' is not used in the names of the six traditional counties of Northern Ireland.

Counties in England bearing the "-shire" suffix include:
Bedfordshire, Berkshire, Buckinghamshire, Cambridgeshire, Cheshire, Derbyshire, Gloucestershire, Hampshire, Herefordshire, Hertfordshire, Huntingdonshire, Lancashire, Lincolnshire, Leicestershire, Northamptonshire, Nottinghamshire, Oxfordshire, Shropshire, Staffordshire, Warwickshire, Wiltshire, Worcestershire and Yorkshire. These counties, on their historical boundaries, cover a little more than half the area of England. The counties that do not use "-shire" are mainly in three areas, in the south-east, south-west and far north of England.
Several of these counties no longer exist as administrative units, or have had their administrative boundaries reduced by local government reforms. Several of the successor authorities retain the "-shire" county names, such as West Yorkshire and South Gloucestershire.

The county of Devon was historically known as Devonshire, although this is no longer the official name. Similarly, Dorset, Rutland and Somerset were formerly known as Dorsetshire, Rutlandshire and Somersetshire, but these terms are no longer official, and are rarely used outside the local populations.

Hexhamshire was a county in the north-east of England from the early 12th century until 1572, when it was incorporated into Northumberland.

In Scotland, barely affected by the Norman conquest of England, the word "shire" prevailed over "county" until the 19th century. Earliest sources have the same usage of the "-shire" suffix as in England (though in Scots this was oftenmost "schyr"). Later the "Shire" appears as a separate word.

"Shire" names in Scotland include Aberdeenshire, Ayrshire, Banffshire, Berwickshire, Clackmannanshire, Cromartyshire, Dumfriesshire, Dunbartonshire, Inverness-shire, Kincardineshire, Kinross-shire, Kirkcudbrightshire, Lanarkshire, Morayshire, Nairnshire, Peeblesshire, Perthshire, Renfrewshire, Ross-shire, Roxburghshire, Selkirkshire, Stirlingshire, and Wigtownshire. 

In Scotland four shires have alternative names with the "-shire" suffix: Angus (Forfarshire), East Lothian (Haddingtonshire), Midlothian (Edinburghshire) and West Lothian (Linlithgowshire).

Sutherland is occasionally still referred to as Sutherlandshire. Similarly, Argyllshire, Buteshire, Caithness-shire and Fifeshire are sometimes found. Also, Morayshire was previously called Elginshire. There is currently much debate about whether Argyllshire was ever really used.

Shires in Wales bearing the "-shire" suffix include: Brecknockshire (or Breconshire), Caernarfonshire, Cardiganshire (now Ceredigion), Carmarthenshire, Denbighshire, Flintshire, Monmouthshire, Montgomeryshire, Pembrokeshire, and Radnorshire. In Wales, the counties of Merioneth and Glamorgan are occasionally referred to with the "shire" suffix. The only traditional Welsh county that never takes "shire" is Anglesey—in English: in Welsh it is referred to as 'Sir Fon'.

The suffix –"shire" could be a generalised term referring to a district. It did not acquire the strong association with county until later. Other than these, the term was used for several other districts. Bedlingtonshire, Craikshire, Norhamshire and Islandshire were exclaves of County Durham, which were incorporated into Northumberland or Yorkshire in 1844. The suffix was also used for many hundreds, wapentakes and liberties such as Allertonshire, Blackburnshire, Halfshire, Howdenshire, Leylandshire, Powdershire, Pydarshire, Richmondshire, Riponshire, Salfordshire, Triggshire, Tynemouthshire, West Derbyshire and Wivelshire, counties corporate such as Hullshire, and other districts such as Applebyshire, Bamburghshire, Bunkleshire, Carlisleshire, Coldinghamshire, Coxwoldshire, Cravenshire, Hallamshire, Mashamshire and Yetholmshire. Richmondshire is today the name of a local government district of North Yorkshire.

Non-county shires were very common in Scotland. Kinross-shire and Clackmannanshire are arguably survivals from such districts. Non-county "shires" in Scotland include Bunkleshire, Coldinghamshire and Yetholmshire.

"Shire" is the most common word in Australia for rural local government areas (LGAs). New South Wales, Victoria, Queensland, Western Australia and the Northern Territory use the term "Shire" for this unit; the territories of Christmas Island and Cocos Island are also shires. In contrast, South Australia uses district and region for its rural LGA units, while Tasmania uses municipality. Shires are generally functionally indistinguishable from towns, borough, municipalities, or cities.

Three LGAs in outer metropolitan Sydney and four in outer metropolitan Melbourne have populations exceeding that of towns or municipalities, but retain significant bushlands and/or semi-rural areas, and most have continued to use "Shire" in their titles whilst others have dropped "Shire" from their titles. These "city-shires" are:

Sydney:

Melbourne:

In 1634, eight "shires" were created in the Virginia Colony by order of Charles I, King of England. They were renamed as counties only a few years later. They were:


As of 2013 six of the original eight Shires of Virginia are considered to be still extant whilst two have consolidated with a neighbouring city. Most of their boundaries have changed in the intervening centuries.

Before the Province of New York was granted county subdivisions and a greater royal presence in 1683, the early ducal colony consisted of York Shire, as well as Albany and Ulster, after the three titles held by Prince James: Duke of York, Duke of Albany, Earl of Ulster. While these were basically renamed Dutch core settlements, they were quickly converted to English purposes, while the Dutch remained within the colony, as opposed to later practice of the Acadian Expulsion. Further Anglo-Dutch synthesis occurred when James enacted the Dominion of New England and later when William III of England took over through the Glorious Revolution.

A few New England states and commonwealths (namely Vermont, Massachusetts, and Maine), still use the term "shire town" for their county seats, although they use the term county, rather than shire.

The word also survives in the name of the state of New Hampshire, whose co-founder, John Mason, named his Province of New Hampshire after the English county.



</doc>
<doc id="26997" url="https://en.wikipedia.org/wiki?curid=26997" title="Scientist">
Scientist

A scientist is a person engaging in a systematic activity to acquire knowledge that describes and predicts the natural world. In a more restricted sense, a scientist may refer to an individual who uses the scientific method. The person may be an expert in one or more areas of science. The term scientist was coined by the theologian, philosopher, and historian of science William Whewell in 1833. This article focuses on the more restricted use of the word. Scientists perform research toward a more comprehensive understanding of nature, including physical, mathematical and social realms.

Philosophy is today typically regarded as a distinct activity from science, though the activities were not always distinguished in this fashion, with science considered a "branch" of philosophy rather than opposed to it, prior to modernity. Philosophers aim to provide a comprehensive understanding of fundamental aspects of reality and experience, often pursuing inquiries with conceptual, rather than empirical, methods. Natural scientific research is usually also distinguished from inquiry in the humanities more generally, and often with inquiry in the social sciences and mathematics on various grounds, although these distinctions may be controversial.

When science is done with a goal toward practical utility, it is called applied science. An applied scientist may not be designing something in particular, but rather is conducting research with the aim of developing new technologies and practical methods. When science seeks to answer questions about fundamental aspects of reality it is sometimes called natural philosophy, as it was generally known before the 19th century.

Science and technology have continually modified human existence through the engineering process. As a profession the scientist of today is widely recognized. Scientists include theoreticians who mainly develop new models to explain existing data and predict new results, and experimentalists who mainly test models by making measurements — though in practice the division between these activities is not clear-cut, and many scientists perform both tasks.

There is a continuum from the most theoretical to the most empirical scientists with no distinct boundaries. In terms of personality, interests, training and professional activity, there is little difference between applied mathematicians and theoretical physicists.

Scientists can be motivated in several ways. Many have a desire to understand why the world is as we see it and how it came to be. They exhibit a strong curiosity about reality. Other motivations are recognition by their peers and prestige, or the desire to apply scientific knowledge for the benefit of people's health, the nations, the world, nature or industries (academic scientist and industrial scientist). Scientists tend to be less motivated by direct financial reward for their work than other careers. As a result, scientific researchers often accept lower average salaries when compared with many other professions which require a similar amount of training and qualification.

The number of scientists is vastly different from country to country. For instance, there are only 4 full-time scientists per 10,000 workers in India while this number is 79 for the United Kingdom and the United States.
According to the United States National Science Foundation 4.7 million people with science degrees worked in the United States in 2015, across all disciplines and employment sectors. The figure included twice as many men as women. Of that total, 17% worked in academia, that is, at universities and undergraduate institutions, and men held 53% of those positions. 5% of scientists worked for the federal government and about 3.5% were self-employed. Of the latter two groups, two-thirds were men. 59% of US scientists were employed in industry or business, and another 6% worked in non-profit positions.

Scientist and engineering statistics are usually intertwined, but they indicate that women enter the field far less than men, though this gap is narrowing. The number of science and engineering doctorates awarded to women rose from a mere 7 percent in 1970 to 34 percent in 1985 and in engineering alone the numbers of bachelor's degrees awarded to women rose from only 385 in 1975 to more than 11000 in 1985. 

Until the late 19th or early 20th century, scientists were called "natural philosophers" or "men of science".

English philosopher and historian of science William Whewell coined the term "scientist" in 1833, and it first appeared in print in Whewell's anonymous 1834 review of Mary Somerville's "On the Connexion of the Physical Sciences" published in the "Quarterly Review". Whewell's suggestion of the term was partly satirical, a response to changing conceptions of science itself in which natural knowledge was increasingly seen as distinct from other forms of knowledge. Whewell wrote of "an increasing proclivity of separation and dismemberment" in the sciences; while highly specific terms proliferated—chemist, mathematician, naturalist—the broad term "philosopher" was no longer satisfactory to group together those who pursued science, without the caveats of "natural" or "experimental" philosopher. Members of the British Association for the Advancement of Science had been complaining about the lack of a good term at recent meetings, Whewell reported in his review; alluding to himself, he noted that "some ingenious gentleman proposed that, by analogy with "artist", they might form [the word] "scientist", and added that there could be no scruple in making free with this term since we already have such words as "economist", and "atheist"—but this was not generally palatable".

Whewell proposed the word again more seriously (and not anonymously) in his 1840 "The Philosophy of the Inductive Sciences":

He also proposed the term "physicist" at the same time, as a counterpart to the French word "physicien". Neither term gained wide acceptance until decades later; "scientist" became a common term in the late 19th century in the United States and around the turn of the 20th century in Great Britain. By the twentieth century, the modern notion of science as a special brand of information about the world, practiced by a distinct group and pursued through a unique method, was essentially in place.

The social roles of "scientists", and their predecessors before the emergence of modern scientific disciplines, have evolved considerably over time. Scientists of different eras (and before them, natural philosophers, mathematicians, natural historians, natural theologians, engineers, and others who contributed to the development of science) have had widely different places in society, and the social norms, ethical values, and epistemic virtues associated with scientists—and expected of them—have changed over time as well. Accordingly, many different historical figures can be identified as early scientists, depending on which elements of modern science are taken to be essential.

Some historians point to the 17th century as the period when science in a recognizably modern form developed (what is popularly called the Scientific Revolution). It wasn't until the 19th century that sufficient socioeconomic changes occurred for scientists to emerge as a major profession.

Knowledge about nature in Classical Antiquity was pursued by many kinds of scholars. Greek contributions to science—including works of geometry and mathematical astronomy, early accounts of biological processes and catalogs of plants and animals, and theories of knowledge and learning—were produced by philosophers and physicians, as well as practitioners of various trades. These roles, and their associations with scientific knowledge, spread with the Roman Empire and, with the spread of Christianity, became closely linked to religious institutions in most of European countries. Astrology and astronomy became an important area of knowledge, and the role of astronomer/astrologer developed with the support of political and religious patronage. By the time of the medieval university system, knowledge was divided into the "trivium"—philosophy, including natural philosophy—and the "quadrivium"—mathematics, including astronomy. Hence, the medieval analogs of scientists were often either philosophers or mathematicians. Knowledge of plants and animals was broadly the province of physicians.

Science in medieval Islam generated some new modes of developing natural knowledge, although still within the bounds of existing social roles such as philosopher and mathematician. Many proto-scientists from the Islamic Golden Age are considered polymaths, in part because of the lack of anything corresponding to modern scientific disciplines. Many of these early polymaths were also religious priests and theologians: for example, Alhazen and al-Biruni were mutakallimiin; the physician Avicenna was a hafiz; the physician Ibn al-Nafis was a hafiz, muhaddith and ulema; the botanist Otto Brunfels was a theologian and historian of Protestantism; the astronomer and physician Nicolaus Copernicus was a priest. During the Italian Renaissance scientists like Leonardo Da Vinci, Michelangelo, Galileo Galilei and Gerolamo Cardano have been considered as the most recognizable polymaths.

During the Renaissance, Italians made substantial contributions in science. Leonardo Da Vinci made significant discoveries in paleontology and anatomy. The Father of modern Science,
Galileo Galilei, made key improvements on the thermometer and telescope which allowed him to observe and clearly describe the solar system. Descartes was not only a pioneer of analytic geometry but formulated a theory of mechanics and advanced ideas about the origins of animal movement and perception. Vision interested the physicists Young and Helmholtz, who also studied optics, hearing and music. Newton extended Descartes' mathematics by inventing calculus (contemporaneously with Leibniz). He provided a comprehensive formulation of classical mechanics and investigated light and optics. Fourier founded a new branch of mathematics — infinite, periodic series — studied heat flow and infrared radiation, and discovered the greenhouse effect. Girolamo Cardano, Blaise Pascal Pierre de Fermat, Von Neumann, Turing, Khinchin, Markov and Wiener, all mathematicians, made major contributions to science and probability theory, including the ideas behind computers, and some of the foundations of statistical mechanics and quantum mechanics. Many mathematically inclined scientists, including Galileo, were also musicians.

Luigi Galvani, the pioneer of the bioelectromagnetics, discovered the animal electricity. He discovered that a charge applied to the spinal cord of a frog could generate muscular spasms throughout its body. Charges could make frog legs jump even if the legs were no longer attached to a frog. While cutting a frog leg, Galvani's steel scalpel touched a brass hook that was holding the leg in place. The leg twitched. Further experiments confirmed this effect, and Galvani was convinced that he was seeing the effects of what he called animal electricity, the life force within the muscles of the frog. At the University of Pavia, Galvani's colleague Alessandro Volta was able to reproduce the results, but was sceptical of Galvani's explanation.

During the age of Enlightenment, Francesco Redi, discovered that microorganisms can cause disease. This was later explained by Louis Pasteur. There are many compelling stories in medicine and biology, such as the development of ideas about the circulation of blood from Galen to Harvey. The flowering of genetics and molecular biology in the 20th century is replete with famous names. Ramón y Cajal won the Nobel Prize in 1906 for his remarkable observations in neuroanatomy.

Marie Curie became the first female to win the Nobel Prize and the first person to win it twice. Her efforts led to the development of nuclear energy and Radio therapy for the treatment of cancer. In 1922, she was appointed a member of the International Commission on Intellectual Co-operation by the Council of the League of Nations. She campaigned for scientist's right to patent their discoveries and inventions. She also campaigned for free access to international scientific literature and for internationally recognized scientific symbols.

Lazzaro Spallanzani is one of the most influential figures in experimental physiology and the natural sciences. His investigations have exerted a lasting influence on the medical sciences. He made important contributions to the experimental study of bodily functions and animal reproduction.

Some see a dichotomy between experimental sciences and purely "observational" sciences such as astronomy, meteorology, oceanography and seismology. But astronomers have done basic research in optics, developed charge-coupled devices, and in recent decades have sent space probes to study other planets in addition to using the Hubble Telescope to probe the origins of the Universe some 14 billion years ago. Microwave spectroscopy has now identified dozens of organic molecules in interstellar space, requiring laboratory experimentation and computer simulation to confirm the observational data and starting a new branch of chemistry. Computer modeling and numerical methods are techniques required of students in every field of quantitative science.

Those considering science as a career often look to the frontiers. These include cosmology and biology, especially molecular biology and the human genome project. Other areas of active research include the exploration of matter at the scale of elementary particles as described by high-energy physics, and materials science, which seeks to discover and design new materials. Although there have been remarkable discoveries with regard to brain function and neurotransmitters, the nature of the mind and human thought still remains unknown.








</doc>
<doc id="27000" url="https://en.wikipedia.org/wiki?curid=27000" title="Smog">
Smog

Smog is a type of air pollutant. The word "smog" was coined in the early 20th century as a blend of the words smoke and fog to refer to smoky fog, its opacity, and odour. The word was then intended to refer to what was sometimes known as pea soup fog, a familiar and serious problem in London from the 19th century to the mid-20th century. This kind of visible air pollution is composed of nitrogen oxides, sulphur oxides, ozone, smoke or dirt particles and also less visible particles such as CFC's. Human-made smog is derived from coal emissions, vehicular emissions, industrial emissions, forest and agricultural fires and photochemical reactions of these emissions.

Modern smog, as found for example in Los Angeles, is a type of air pollution derived from vehicular emission from internal combustion engines and industrial fumes that react in the atmosphere with sunlight to form secondary pollutants that also combine with the primary emissions to form photochemical smog. In certain other cities, such as Delhi, smog severity is often aggravated by stubble burning in neighboring agricultural areas. The atmospheric pollution levels of Los Angeles, Beijing, Delhi, Lahore, Mexico City, Tehran and other cities are increased by inversion that traps pollution close to the ground. It is usually very highly toxic to humans and can cause severe sickness, shortened life or death.

Coinage of the term "smog" is generally attributed to Dr. Henry Antoine Des Voeux in his 1905 paper, "Fog and Smoke" for a meeting of the Public Health Congress. The 26 July 1905 edition of the London newspaper "Daily Graphic" quoted Des Voeux, "He said it required no science to see that there was something produced in great cities which was not found in the country, and that was smoky fog, or what was known as 'smog'." The following day the newspaper stated that "Dr. Des Voeux did a public service in coining a new word for the London fog." However, this is predated by a "Los Angeles Times" article of January 19, 1893, in which the word is attributed to "a witty English writer."

Coal fires, used to heat individual buildings or in a power-producing plant, can emit significant clouds of smoke that contributes to smog. Air pollution from this source has been reported in England since the Middle Ages. London, in particular, was notorious up through the mid-20th century for its coal-caused smogs, which were nicknamed 'pea-soupers.' Air pollution of this type is still a problem in areas that generate significant smoke from burning coal. The emissions from coal combustions are one of the main causes of air pollution in China. Especially during autumn and winter when coal-fired heating ramps up, the amount of produced smoke forces some Chinese cities to close down roads, schools or airports. One prominent example for this was China's Northeastern city Harbin in 2013.

Traffic emissions – such as from trucks, buses, and automobiles – also contribute. Airborne by-products from vehicle exhaust systems cause air pollution and are a major ingredient in the creation of smog in some large cities.

The major culprits from transportation sources are carbon monoxide (CO), nitrogen oxides (NO and NO), volatile organic compounds, sulphur dioxide, and hydrocarbons. (Hydrocarbons are the main components of petroleum fuels such as gasoline and diesel fuel.) These molecules react with sunlight, heat, ammonia, moisture, and other compounds to form the noxious vapors, ground level ozone, and particles that comprise smog.

Photochemical smog is the chemical reaction of sunlight, nitrogen oxides and volatile organic compounds in the atmosphere, which leaves airborne particles and ground-level ozone. This noxious mixture of air pollutants may include the following:

A primary pollutant is an air pollutant emitted directly from a source.
A secondary pollutant is not directly emitted as such, but forms when other pollutants (primary pollutants) react in the atmosphere.
Examples of a secondary pollutant include ozone, which is formed when hydrocarbons (HC) and nitrogen oxides (NO) combine in the presence of sunlight; nitrogen dioxide (NO), which is formed as nitric oxide (NO) combines with oxygen in the air; and acid rain, which is formed when sulfur dioxide or nitrogen oxides react with water.
All of these harsh chemicals are usually highly reactive and oxidizing. Photochemical smog is therefore considered to be a problem of modern industrialization. It is present in all modern cities, but it is more common in cities with sunny, warm, dry climates and a large number of motor vehicles. Because it travels with the wind, it can affect sparsely populated areas as well.

The composition and chemical reactions involved in photochemical smog were not understood until the 1950s. In 1948, flavor chemist Arie Haagen-Smit adapted some of his equipment to collect chemicals from polluted air, and identified ozone as a component of Los Angeles smog. Haagen-Smit went on to discover that nitrous oxides from automotive exhausts and gaseous hydrocarbons from cars and oil refineries, exposed to sunlight, were key ingredients in the formation of ozone and photochemical smog. Haagen-Smit worked with Arnold Beckman, who developed various equipment for detecting smog, ranging from an "Apparatus for recording gas concentrations in the atmosphere" patented on October 7, 1952, to "air quality monitoring vans" for use by government and industry.

An erupting volcano can also emit high levels of sulfur dioxide along with a large quantity of particulate matter; two key components to the creation of smog. However, the smog created as a result of a volcanic eruption is often known as vog to distinguish it as a natural occurrence.

The radiocarbon content of some plant life has been linked to the distribution of smog in some areas. For example, the creosote bush in the Los Angeles area has been shown to have an effect on smog distribution that is more than fossil fuel combustion alone.

Smog is a serious problem in many cities and continues to harm human health. Ground-level ozone, sulphur dioxide, nitrogen dioxide and carbon monoxide are especially harmful for senior citizens, children, and people with heart and lung conditions such as emphysema, bronchitis, and asthma. It can inflame breathing passages, decrease the lungs' working capacity, cause shortness of breath, pain when inhaling deeply, wheezing, and coughing. It can cause eye and nose irritation and it dries out the protective membranes of the nose and throat and interferes with the body's ability to fight infection, increasing susceptibility to illness. Hospital admissions and respiratory deaths often increase during periods when ozone levels are high. There is a lack of knowledge on the long-term effects of air pollution exposure and the origin of asthma. An experiment was carried out using intense air pollution similar to that of the 1952 Great Smog of London. The results from this experiment concluded that there is a link between early-life pollution exposure that leads to the development of asthma, Proposing the ongoing effect of the Great Smog.
Modern studies continue to find links between mortality and the presence of smog. One study, published in Nature magazine, found that smog episodes in the city of Jinan, a large city in eastern China, during 2011–15, were associated with a 5.87% (95% CI 0.16–11.58%) increase in the rate of overall mortality. This study highlights the effect of exposure to air pollution on the rate of mortality in China.

The U.S. EPA has developed an Air Quality Index to help explain air pollution levels to the general public. 8 hour average ozone concentrations of 85 to 104 ppbv are described as "Unhealthy for Sensitive Groups", 105 ppbv to 124 ppbv as "unhealthy" and 125 ppb to 404 ppb as "very unhealthy". The "very unhealthy" range for some other pollutants are: 355 μg m – 424 μg m for PM10; 15.5 ppm – 30.4ppm for CO and 0.65 ppm – 1.24 ppm for NO.

In 2016, the Ontario Medical Association announced that smog is responsible for an estimated 9,500 premature deaths in the province each year.

A 20-year American Cancer Society study found that cumulative exposure also increases the likelihood of premature death from a respiratory disease, implying the 8-hour standard may be insufficient.

Tiny magnetic particles from air pollution have for the first time been discovered to be lodged in human brains– and researchers think they could be a possible cause of Alzheimer’s disease.
Researchers at Lancaster University found abundant magnetite nanoparticles in the brain tissue from 37 individuals aged three to 92-years-old who lived in Mexico City and Manchester. This strongly magnetic mineral is toxic and has been implicated in the production of reactive oxygen species (free radicals) in the human brain, which are associated with neurodegenerative diseases including Alzheimer’s disease.

A study examining 806 women who had babies with birth defects between 1997 and 2006, and 849 women who had healthy babies, found that smog in the San Joaquin Valley area of California was linked to two types of neural tube defects: spina bifida (a condition involving, among other manifestations, certain malformations of the spinal column), and anencephaly (the underdevelopment or absence of part or all of the brain, which if not fatal usually results in profound impairment).

According to a study published in The Lancet, even a very small (5 μg) change in PM2.5 exposure was associated with an increase (18%) in risk of a low birth weight at delivery, and this relationship held even below the current accepted safe levels.

Smog can form in almost any climate where industries or cities release large amounts of air pollution, such as smoke or gases. However, it is worse during periods of warmer, sunnier weather when the upper air is warm enough to inhibit vertical circulation. It is especially prevalent in geologic basins encircled by hills or mountains. It often stays for an extended period of time over densely populated cities or urban areas, and can build up to dangerous levels.

According to the Canadian Science Smog Assessment published in 2012, smog is responsible for detrimental effects on human and ecosystem health, as well as socioeconomic well-being across the country. It was estimated that the province of Ontario sustains $201 million in damages annually for selected crops, and an estimated tourism revenue degradation of $7.5 million in Vancouver and $1.32 million in The Fraser Valley due to decreased visibility. Air pollution in British Columbia is of particular concern, especially in the Fraser Valley, because of a meteorological effect called inversion which decreases air dispersion and leads to smog concentration.

For the past few years, cities in northern India have been covered in a thick layer of winter smog. The situation has turned quite drastic in the National Capital, Delhi. This smog is caused by the collection of Particulate Matter (a very fine type of dust and toxic gases) in the air due to stagnant movement of air during winters.

Delhi is the most polluted city in the world and according to one estimate, air pollution causes the death of about 10,500 people in Delhi every year. During 2013–14, peak levels of fine particulate matter (PM) in Delhi increased by about 44%, primarily due to high vehicular and industrial emissions, construction work and crop burning in adjoining states. Delhi has the highest level of the airborne particulate matter, PM2.5 considered most harmful to health, with 153 micrograms. Rising air pollution level has significantly increased lung-related ailments (especially asthma and lung cancer) among Delhi's children and women. The dense smog in Delhi during winter season results in major air and rail traffic disruptions every year. According to Indian meteorologists, the average maximum temperature in Delhi during winters has declined notably since 1998 due to rising air pollution.
Environmentalists have criticised the Delhi government for not doing enough to curb air pollution and to inform people about air quality issues. Most of Delhi's residents are unaware of alarming levels of air pollution in the city and the health risks associated with it. Since the mid-1990s, Delhi has undertaken some measures to curb air pollution – Delhi has the third highest quantity of trees among Indian cities and the Delhi Transport Corporation operates the world's largest fleet of environmentally friendly compressed natural gas (CNG) buses. In 1996, the Centre for Science and Environment (CSE) started a public interest litigation in the Supreme Court of India that ordered the conversion of Delhi's fleet of buses and taxis to run on CNG and banned the use of leaded petrol in 1998. In 2003, Delhi won the United States Department of Energy's first 'Clean Cities International Partner of the Year' award for its "bold efforts to curb air pollution and support alternative fuel initiatives". The Delhi Metro has also been credited for significantly reducing air pollutants in the city.

However, according to several authors, most of these gains have been lost, especially due to stubble burning, rise in market share of diesel cars and a considerable decline in bus ridership. According to CUE and System of Air Quality Weather Forecasting and Research (SAFER), burning of agricultural waste in nearby Punjab, Haryana and Uttar Pradesh regions results in severe intensification of smog over Delhi. The state government of adjoining Uttar Pradesh is considering imposing a ban on crop burning to reduce pollution in Delhi NCR and an environmental panel has appealed to India's Supreme Court to impose a 30% cess on diesel cars.

Joint research between American and Chinese researchers in 2006 concluded that much of the city's pollution comes from surrounding cities and provinces. On average 35–60% of the ozone can be traced to sources outside the city. Shandong Province and Tianjin Municipality have a "significant influence on Beijing's air quality", partly due to the prevailing south/southeasterly flow during the summer and the mountains to the north and northwest.

In 1306, concerns over air pollution were sufficient for Edward I to (briefly) ban coal fires in London. In 1661, John Evelyn's "Fumifugium" suggested burning fragrant wood instead of mineral coal, which he believed would reduce coughing. The "" the same year describes how the smoke "does our lungs and spirits choke, Our hanging spoil, and rust our iron."

Severe episodes of smog continued in the 19th and 20th centuries, mainly in the winter, and were nicknamed "pea-soupers," from the phrase "as thick as pea soup." The Great Smog of 1952 darkened the streets of London and killed approximately 4,000 people in the short time of 4 days (a further 8,000 died from its effects in the following weeks and months). Initially a flu epidemic was blamed for the loss of life.

In 1956 the Clean Air Act started legally enforcing smokeless zones in the capital. There were areas where no soft coal was allowed to be burned in homes or in businesses, only coke, which produces no smoke. Because of the smokeless zones, reduced levels of sooty particulates eliminated the intense and persistent London smog.

It was after this that the great clean-up of London began. One by one, historical buildings which, during the previous two centuries had gradually completely blackened externally, had their stone facades cleaned and restored to their original appearance. Victorian buildings whose appearance changed dramatically after cleaning included the British Museum of Natural History. A more recent example was the Palace of Westminster, which was cleaned in the 1980s. A notable exception to the restoration trend was 10 Downing Street, whose bricks upon cleaning in the late 1950s proved to be naturally "yellow"; the smog-derived black colour of the façade was considered so iconic that the bricks were painted black to preserve the image. Smog caused by traffic pollution, however, does still occur in modern London.

Other areas of the United Kingdom were affected by smog, especially heavily industrialised areas.

The cities of Glasgow and Edinburgh, in Scotland, suffered smoke-laden fogs in 1909. Des Voeux, commonly credited with creating the "smog" moniker, presented a paper in 1911 to the Manchester Conference of the Smoke Abatement League of Great Britain about the fogs and resulting deaths.

One Birmingham resident described near black-out conditions in the 1900s before the Clean Air Act, with visibility so poor that cyclists had to dismount and walk in order to stay on the road.

On 29 April 2015, the UK Supreme Court ruled that the government must take immediate action to cut air pollution, following a case brought by environmental lawyers at ClientEarth.

Due to its location in a highland "bowl", cold air sinks down onto the urban area of Mexico City, trapping industrial and vehicle pollution underneath, and turning it into the most infamously smog-plagued city of Latin America. Within one generation, the city has changed from being known for some of the cleanest air of the world into one with some of the worst pollution, with pollutants like nitrogen dioxide being double or even triple international standards.
Similar to Mexico City, the air pollution of Santiago valley, located between the Andes and the Chilean Coast Range, turn it into the most infamously smog-plagued city of South America. Other aggravates of the situation reside in its high latitude (31 degrees South) and dry weather during most of the year.

In December 2005, schools and public offices had to close in Tehran and 1600 people were taken to hospital, in a severe smog blamed largely on unfiltered car exhaust.

Smog was brought to the attention of the general US public in 1933 with the publication of the book "Stop That Smoke", by Henry Obermeyer, a New York public utility official, in which he pointed out the effect on human life and even the destruction of of a farmer's spinach crop. Since then, the United States Environmental Protection Agency has designated over 300 U.S. counties to be non-attainment areas for one or more pollutants tracked as part of the National Ambient Air Quality Standards. These areas are largely clustered around large metropolitan areas, with the largest contiguous non-attainment zones in California and the Northeast. Various U.S. and Canadian government agencies collaborate to produce real-time air quality maps and forecasts. To combat smog conditions, localities may declare "smog alert" days, such as in the Spare the Air program in the San Francisco Bay Area.

Because of their locations in low basins surrounded by mountains, Los Angeles and the San Joaquin Valley are notorious for their smog. The over-reliance on vehicles for transportation in these regions combined with the additional effects of the San Francisco Bay and Los Angeles/Long Beach port complexes frequently contribute to further air pollution.

Los Angeles in particular is strongly predisposed to accumulation of smog, because of peculiarities of its geography and weather patterns. Los Angeles is situated in a flat basin with ocean on one side and mountain ranges on three sides. A nearby cold ocean current depresses surface air temperatures in the area, resulting in an inversion layer: a phenomenon where air temperature increases, instead of decreasing, with altitude, suppressing thermals and restricting vertical convection. All taken together, this results in a relatively thin, enclosed layer of air above the city that cannot easily escape out of the basin and tends to accumulate pollution.

Though Los Angeles was one of the best known cities suffering from transportation smog for much of the 20th century, so much so that it was sometimes said that "Los Angeles" was a synonym for "smog", strict regulations by government agencies overseeing this problem, including tight restrictions on allowed emissions levels for all new cars sold in California and mandatory regular emission tests of older vehicles, resulted in significant improvements in air quality. For example, air concentrations of volatile organic compounds declined by a factor of 50 between 1962 and 2012. Concentrations of air pollutants such as nitrous oxides and ozone declined by 70% to 80% over the same period of time.


In the late 1990s, massive immigration to Ulaanbaatar from the countryside began. An estimated 150,000 households, mainly living in traditional Mongolian gers on the outskirts of Ulaanbaatar, burn wood and coal (some poor families burn even car tires and trash) to heat themselves during the harsh winter, which lasts from October to April, since these outskirts are not connected to the city's central heating system. A temporary solution to decrease smog was proposed in the form of stoves with improved efficiency, although with no visible results. 
Coal-fired ger stoves release high levels of ash and other particulate matter (PM). When inhaled, these particles can settle in the lungs and respiratory tract and cause health problems. At two to 10 times above Mongolian and international air quality standards, Ulaanbaatar's PM rates are among the worst in the world, according to a December 2009 World Bank report. The Asian Development Bank (ADB) estimates that health costs related to this air pollution account for as much as 4 percent of Mongolia's GDP.

Smog is a regular problem in Southeast Asia caused by land and forest fires in Indonesia, especially Sumatra and Kalimantan, although the term haze is preferred in describing the problem. Farmers and plantation owners are usually responsible for the fires, which they use to clear tracts of land for further plantings. Those fires mainly affect Brunei, Indonesia, Philippines, Malaysia, Singapore and Thailand, and occasionally Guam and Saipan. The economic losses of the fires in 1997 have been estimated at more than US$9 billion. This includes damages in agriculture production, destruction of forest lands, health, transportation, tourism, and other economic endeavours. Not included are social, environmental, and psychological problems and long-term health effects. The second-latest bout of haze to occur in Malaysia, Singapore and the Malacca Straits is in October 2006, and was caused by smoke from fires in Indonesia being blown across the Straits of Malacca by south-westerly winds. A similar haze has occurred in June 2013, with the PSI setting a new record in Singapore on June 21 at 12pm with a reading of 401, which is in the "Hazardous" range.

The Association of Southeast Asian Nations (ASEAN) reacted. In 2002, the Agreement on Transboundary Haze Pollution was signed between all ASEAN nations. ASEAN formed a Regional Haze Action Plan (RHAP) and established a co-ordination and support unit (CSU). RHAP, with the help of Canada, established a monitoring and warning system for forest/vegetation fires and implemented a Fire Danger Rating System (FDRS). The Malaysian Meteorological Department (MMD) has issued a daily rating of fire danger since September 2003. Indonesia has been ineffective at enforcing legal policies on errant farmers.

Since start of winter season heavy smog loaded with pollutants covered major part of Punjab especially the city of Lahore, causing breathing problems and disrupting normal traffic.

Doctors advised residents to stay indoors and wear facemasks outside.

The severity of smog is often measured using automated optical instruments such as Nephelometers, as haze is associated with visibility and traffic control in ports. Haze however can also be an indication of poor air quality though this is often better reflected using accurate purpose built air indexes such as the American Air Quality Index, the Malaysian API (Air Pollution Index) and the Singaporean Pollutant Standards Index.

In hazy conditions, it is likely that the index will report the suspended particulate level. The disclosure of the responsible pollutant is mandated in some jurisdictions.

The Malaysian API does not have a capped value; hence its most hazardous readings can go above 500. Above 500, a state of emergency is declared in the affected area. Usually, this means that non-essential government services are suspended, and all ports in the affected area are closed. There may also be prohibitions on private sector commercial and industrial activities in the affected area excluding the food sector. So far, state of emergency rulings due to hazardous API levels were applied to the Malaysian towns of Port Klang, Kuala Selangor and the state of Sarawak during the 2005 Malaysian haze and the 1997 Southeast Asian haze.







</doc>
<doc id="27001" url="https://en.wikipedia.org/wiki?curid=27001" title="Smoke">
Smoke

Smoke is a collection of airborne solid and liquid particulates and gases emitted when a material undergoes combustion or pyrolysis, together with the quantity of air that is entrained or otherwise mixed into the mass. It is commonly an unwanted by-product of fires (including stoves, candles, oil lamps, and fireplaces), but may also be used for pest control (fumigation), communication (smoke signals), defensive and offensive capabilities in the military (smoke screen), cooking, or smoking (tobacco, cannabis, etc.). Used in rituals where incense, sage, or resin is burned to produce a smell for spiritual purposes. Smoke is sometimes used as a flavoring agent, and preservative for various foodstuffs. Smoke is also a component of internal combustion engine exhaust gas, particularly diesel exhaust.

Smoke inhalation is the primary cause of death in victims of indoor fires. The smoke kills by a combination of thermal damage, poisoning and pulmonary irritation caused by carbon monoxide, hydrogen cyanide and other combustion products.

Smoke is an aerosol (or mist) of solid particles and liquid droplets that are close to the ideal range of sizes for Mie scattering of visible light. This effect has been likened to three-dimensional textured privacy glass — a smoke cloud does not obstruct an image, but thoroughly scrambles it.

The composition of smoke depends on the nature of the burning fuel and the conditions of combustion.

Fires with high availability of oxygen burn at a high temperature and with small amount of smoke produced; the particles are mostly composed of ash, or with large temperature differences, of condensed aerosol of water. High temperature also leads to production of nitrogen oxides. Sulfur content yields sulfur dioxide, or in case of incomplete combustion, hydrogen sulfide. Carbon and hydrogen are almost completely oxidized to carbon dioxide and water. Fires burning with lack of oxygen produce a significantly wider palette of compounds, many of them toxic. Partial oxidation of carbon produces carbon monoxide, nitrogen-containing materials can yield hydrogen cyanide, ammonia, and nitrogen oxides. Hydrogen gas can be produced instead of water. Content of halogens such as chlorine (e.g. in polyvinyl chloride or brominated flame retardants) may lead to production of e.g. hydrogen chloride, phosgene, dioxin, and chloromethane, bromomethane and other halocarbons. Hydrogen fluoride can be formed from fluorocarbons, whether fluoropolymers subjected to fire or halocarbon fire suppression agents. Phosphorus and antimony oxides and their reaction products can be formed from some fire retardant additives, increasing smoke toxicity and corrosivity. Pyrolysis of polychlorinated biphenyls (PCB), e.g. from burning older transformer oil, and to lower degree also of other chlorine-containing materials, can produce 2,3,7,8-tetrachlorodibenzodioxin, a potent carcinogen, and other polychlorinated dibenzodioxins. Pyrolysis of fluoropolymers, e.g. teflon, in presence of oxygen yields carbonyl fluoride (which hydrolyzes readily to HF and CO); other compounds may be formed as well, e.g. carbon tetrafluoride, hexafluoropropylene, and highly toxic perfluoroisobutene (PFIB).

Pyrolysis of burning material, especially incomplete combustion or smoldering without adequate oxygen supply, also results in production of a large amount of hydrocarbons, both aliphatic (methane, ethane, ethylene, acetylene) and aromatic (benzene and its derivates, polycyclic aromatic hydrocarbons; e.g. benzo[a]pyrene, studied as a carcinogen, or retene), terpenes. Heterocyclic compounds may be also present. Heavier hydrocarbons may condense as tar; smoke with significant tar content is yellow to brown. Presence of such smoke, soot, and/or brown oily deposits during a fire indicates a possible hazardous situation, as the atmosphere may be saturated with combustible pyrolysis products with concentration above the upper flammability limit, and sudden inrush of air can cause flashover or backdraft.

Presence of sulfur can lead to formation of e.g. hydrogen sulfide, carbonyl sulfide, sulfur dioxide, carbon disulfide, and thiols; especially thiols tend to get adsorbed on surfaces and produce a lingering odor even long after the fire. Partial oxidation of the released hydrocarbons yields in a wide palette of other compounds: aldehydes (e.g. formaldehyde, acrolein, and furfural), ketones, alcohols (often aromatic, e.g. phenol, guaiacol, syringol, catechol, and cresols), carboxylic acids (formic acid, acetic acid, etc.).

The visible particulate matter in such smokes is most commonly composed of carbon (soot). Other particulates may be composed of drops of condensed tar, or solid particles of ash. The presence of metals in the fuel yields particles of metal oxides. Particles of inorganic salts may also be formed, e.g. ammonium sulfate, ammonium nitrate, or sodium chloride. Inorganic salts present on the surface of the soot particles may make them hydrophilic. Many organic compounds, typically the aromatic hydrocarbons, may be also adsorbed on the surface of the solid particles. Metal oxides can be present when metal-containing fuels are burned, e.g. solid rocket fuels containing aluminium. Depleted uranium projectiles after impacting the target ignite, producing particles of uranium oxides. Magnetic particles, spherules of magnetite-like ferrous ferric oxide, are present in coal smoke; their increase in deposits after 1860 marks the beginning of the Industrial Revolution. (Magnetic iron oxide nanoparticles can be also produced in the smoke from meteorites burning in the atmosphere.) Magnetic remanence, recorded in the iron oxide particles, indicates the strength of Earth's magnetic field when they were cooled beyond their Curie temperature; this can be used to distinguish magnetic particles of terrestrial and meteoric origin. Fly ash is composed mainly of silica and calcium oxide. Cenospheres are present in smoke from liquid hydrocarbon fuels. Minute metal particles produced by abrasion can be present in engine smokes. Amorphous silica particles are present in smokes from burning silicones; small proportion of silicon nitride particles can be formed in fires with insufficient oxygen. The silica particles have about 10 nm size, clumped to 70-100 nm aggregates and further agglomerated to chains. Radioactive particles may be present due to traces of uranium, thorium, or other radionuclides in the fuel; hot particles can be present in case of fires during nuclear accidents (e.g. Chernobyl disaster) or nuclear war.

Smoke particulates, like other aerosols, are categorized into three modes based on particle size:
Most of the smoke material is primarily in coarse particles. Those undergo rapid dry precipitation, and the smoke damage in more distant areas outside of the room where the fire occurs is therefore primarily mediated by the smaller particles.

Aerosol of particles beyond visible size is an early indicator of materials in a preignition stage of a fire.

Burning of hydrogen-rich fuel produces water; this results in smoke containing droplets of water vapor. In absence of other color sources (nitrogen oxides, particulates...), such smoke is white and cloud-like.

Smoke emissions may contain characteristic trace elements. Vanadium is present in emissions from oil fired power plants and refineries; oil plants also emit some nickel. Coal combustion produces emissions containing aluminium, arsenic, chromium, cobalt, copper, iron, mercury, selenium, and uranium.

Traces of vanadium in high-temperature combustion products form droplets of molten vanadates. These attack the passivation layers on metals and cause high temperature corrosion, which is a concern especially for internal combustion engines. Molten sulfate and lead particulates also have such effect.

Some components of smoke are characteristic of the combustion source. Guaiacol and its derivatives are products of pyrolysis of lignin and are characteristic of wood smoke; other markers are syringol and derivates, and other methoxy phenols. Retene, a product of pyrolysis of conifer trees, is an indicator of forest fires. Levoglucosan is a pyrolysis product of cellulose. Hardwood vs softwood smokes differ in the ratio of guaiacols/syringols. Markers for vehicle exhaust include polycyclic aromatic hydrocarbons, hopanes, steranes, and specific nitroarenes (e.g. 1-nitropyrene). The ratio of hopanes and steranes to elemental carbon can be used to distinguish between emissions of gasoline and diesel engines.

Many compounds can be associated with particulates; whether by being adsorbed on their surfaces, or by being dissolved in liquid droplets. Hydrogen chloride is well absorbed in the soot particles.

Inert particulate matter can be disturbed and entrained into the smoke. Of particular concern are particles of asbestos.

Deposited hot particles of radioactive fallout and bioaccumulated radioisotopes can be reintroduced into the atmosphere by wildfires and forest fires; this is a concern in e.g. the Zone of alienation containing contaminants from the Chernobyl disaster.

Polymers are a significant source of smoke. Aromatic side groups, e.g. in polystyrene, enhance generation of smoke. Aromatic groups integrated in the polymer backbone produce less smoke, likely due to significant charring. Aliphatic polymers tend to generate the least smoke, and are non-self-extinguishing. However presence of additives can significantly increase smoke formation. Phosphorus-based and halogen-based flame retardants decrease production of smoke. Higher degree of cross-linking between the polymer chains has such effect too.

The naked eye detects particle sizes greater than 7 µm (micrometres). Visible particles emitted from a fire are referred to as smoke. Invisible particles are generally referred to as gas or fumes. This is best illustrated when toasting bread in a toaster. As the bread heats up, the products of combustion increase in size. The fumes initially produced are invisible but become visible if the toast is burnt.

An ionization chamber type smoke detector is technically a product of combustion detector, not a smoke detector. Ionization chamber type smoke detectors detect particles of combustion that are invisible to the naked eye. This explains why they may frequently false alarm from the fumes emitted from the red-hot heating elements of a toaster, before the presence of visible smoke, yet they may fail to activate in the early, low-heat smoldering stage of a fire.

Smoke from a typical house fire contains hundreds of different chemicals and fumes. As a result, the damage caused by the smoke can often exceed that caused by the actual heat of the fire. In addition to the physical damage caused by the smoke of a fire – which manifests itself in the form of stains – is the often even harder to eliminate problem of a smoky odor. Just as there are contractors that specialize in rebuilding/repairing homes that have been damaged by fire and smoke, fabric restoration companies specialize in restoring fabrics that have been damaged in a fire.

Smoke from oxygen-deprived fires contains a significant concentration of compounds that are flammable. A cloud of smoke, in contact with atmospheric oxygen, therefore has the potential of being ignited – either by another open flame in the area, or by its own temperature. This leads to effects like backdraft and flashover. Smoke inhalation is also a danger of smoke that can cause serious injury and death.

Many compounds of smoke from fires are highly toxic and/or irritating. The most dangerous is carbon monoxide leading to carbon monoxide poisoning, sometimes with the additive effects of hydrogen cyanide and phosgene. Smoke inhalation can therefore quickly lead to incapacitation and loss of consciousness. Sulfur oxides, hydrogen chloride and hydrogen fluoride in contact with moisture form sulfuric, hydrochloric and hydrofluoric acid, which are corrosive to both lungs and materials. When asleep the nose does not sense smoke nor does the brain, but the body will wake up if the lungs become enveloped in smoke and the brain will be stimulated and the person will be awoken. This does not work if the person is incapacitated or under the influence of drugs and/or alcohol.

Cigarette smoke is a major modifiable risk factor for lung disease, heart disease, and many cancers. Smoke can also be a component of ambient air pollution due to the burning of coal in power plants, forest fires or other sources, although the concentration of pollutants in ambient air is typically much less than that in cigarette smoke. One day of exposure to PM2.5 at a concentration of 880 μg/m3, such as occurs in Beijing, China, is the equivalent of smoking one or two cigarettes in terms of particulate inhalation by weight. The analysis is complicated, however, by the fact that the organic compounds present in various ambient particulates may have a higher carcinogenicity than the compounds in cigarette smoke particulates. Secondhand tobacco smoke is the combination of both sidestream and mainstream smoke emissions from a burning tobacco product. These emissions contain more than 50 carcinogenic chemicals. According to the Surgeon General's latest report on the subject, "Short exposures to secondhand [tobacco] smoke can cause blood platelets to become stickier, damage the lining of blood vessels, decrease coronary flow velocity reserves, and reduce heart variability, potentially increasing the risk of a heart attack". The American Cancer Society lists "heart disease, lung infections, increased asthma attacks, middle ear infections, and low birth weight" as ramifications of smoker's emission.
Smoke can obscure visibility, impeding occupant exiting from fire areas. In fact, the poor visibility due to the smoke that was in the Worcester Cold Storage Warehouse fire in Worcester, Massachusetts was the exact reason why the trapped rescue firefighters couldn't evacuate the building in time. Because of the striking similarity that each floor shared, the dense smoke caused the firefighters to become disoriented.

Smoke contains a wide variety of chemicals, many of them aggressive in nature. Examples are hydrochloric acid and hydrobromic acid, produced from halogen-containing plastics and fire retardants, hydrofluoric acid released by pyrolysis of fluorocarbon fire suppression agents, sulfuric acid from burning of sulfur-containing materials, nitric acid from high-temperature fires where nitrous oxide gets formed, phosphoric acid and antimony compounds from P and Sb based fire retardants, and many others. Such corrosion is not significant for structural materials, but delicate structures, especially microelectronics, are strongly affected. Corrosion of circuit board traces, penetration of aggressive chemicals through the casings of parts, and other effects can cause an immediate or gradual deterioration of parameters or even premature (and often delayed, as the corrosion can progress over long time) failure of equipment subjected to smoke. Many smoke components are also electrically conductive; deposition of a conductive layer on the circuits can cause crosstalks and other deteriorations of the operating parameters or even cause short circuits and total failures. Electrical contacts can be affected by corrosion of surfaces, and by deposition of soot and other conductive particles or nonconductive layers on or across the contacts. Deposited particles may adversely affect the performance of optoelectronics by absorbing or scattering the light beams.

Corrosivity of smoke produced by materials is characterized by the corrosion index (CI), defined as material loss rate (angstrom/minute) per amount of material gasified products (grams) per volume of air (m). It is measured by exposing strips of metal to flow of combustion products in a test tunnel. Polymers containing halogen and hydrogen (polyvinyl chloride, polyolefins with halogenated additives, etc.) have the highest CI as the corrosive acids are formed directly with water produced by the combustion, polymers containing halogen only (e.g. polytetrafluoroethylene) have lower CI as the formation of acid is limited to reactions with airborne humidity, and halogen-free materials (polyolefins, wood) have the lowest CI. However, some halogen-free materials can also release significant amount of corrosive products.

Smoke damage to electronic equipment can be significantly more extensive than the fire itself. Cable fires are of special concern; low smoke zero halogen materials are preferable for cable insulation.

When smoke comes into contact with the surface of any substance or structure, the chemicals contained in it are transferred to it. The corrosive properties of the chemicals cause the substance or structure to decompose at a rapid rate. Certain materials or structures absorb these chemicals, which is why clothing, unsealed surfaces, potable water, piping, wood, etc., are replaced in most cases of structural fires.

As early as the 15th century Leonardo da Vinci commented at length on the difficulty of assessing smoke, and distinguished between black smoke (carbonized particles) and white 'smoke' which is not a smoke at all but merely a suspension of harmless water particulates.

Smoke from heating appliances is commonly measured in one of the following ways:

In-line capture. A smoke sample is simply sucked through a filter which is weighed before and after the test and the mass of smoke found. This is the simplest and probably the most accurate method, but can only be used where the smoke concentration is slight, as the filter can quickly become blocked.

The ASTM smoke pump is a simple and widely used method of in-line capture where a measured volume of smoke is pulled through a filter paper and the dark spot so formed is compared with a standard.

Filter/dilution tunnel. A smoke sample is drawn through a tube where it is diluted with air, the resulting smoke/air mixture is then pulled through a filter and weighed. This is the internationally recognized method of measuring smoke from combustion.

Electrostatic precipitation. The smoke is passed through an array of metal tubes which contain suspended wires. A (huge) electrical potential is applied across the tubes and wires so that the smoke particles become charged and are attracted to the sides of the tubes. This method can over-read by capturing harmless condensates, or under-read due to the insulating effect of the smoke. However, it is the necessary method for assessing volumes of smoke too great to be forced through a filter, i.e., from bituminous coal.

Ringelmann scale. A measure of smoke color. Invented by Professor Maximilian Ringelmann in Paris in 1888, it is essentially a card with squares of black, white and shades of gray which is held up and the comparative grayness of the smoke judged. Highly dependent on light conditions and the skill of the observer it allocates a grayness number from 0 (white) to 5 (black) which has only a passing relationship to the actual quantity of smoke. Nonetheless, the simplicity of the Ringelmann scale means that it has been adopted as a standard in many countries.

Optical scattering. A light beam is passed through the smoke. A light detector is situated at an angle to the light source, typically at 90°, so that it receives only light reflected from passing particles. A measurement is made of the light received which will be higher as the concentration of smoke particles becomes higher.

Optical obscuration. A light beam is passed through the smoke and a detector opposite measures the light. The more smoke particles are present between the two, the less light will be measured.

Combined optical methods. There are various proprietary optical smoke measurement devices such as the 'nephelometer' or the 'aethalometer' which use several different optical methods, including more than one wavelength of light, inside a single instrument and apply an algorithm to give a good estimate of smoke. It has been claimed that these devices can differentiate types of smoke and so their probable source can be inferred, though this is disputed.

Inference from carbon monoxide. Smoke is incompletely burned fuel, carbon monoxide is incompletely burned carbon, therefore it has long been assumed that measurement of CO in flue gas (a cheap, simple and very accurate procedure) will provide a good indication of the levels of smoke. Indeed, several jurisdictions use CO measurement as the basis of smoke control. However it is far from clear how accurate the correspondence is.

Throughout recorded history, humans have used the smoke of medicinal plants to cure illness. A sculpture from Persepolis shows Darius the Great (522–486 BC), the king of Persia, with two censers in front of him for burning Peganum harmala and/or sandalwood Santalum album, which was believed to protect the king from evil and disease. More than 300 plant species in 5 continents are used in smoke form for different diseases. As a method of drug administration, smoking is important as it is a simple, inexpensive, but very effective method of extracting particles containing active agents. More importantly, generating smoke reduces the particle size to a microscopic scale thereby increasing the absorption of its active chemical principles.



</doc>
<doc id="27002" url="https://en.wikipedia.org/wiki?curid=27002" title="Tobacco pipe">
Tobacco pipe

A tobacco pipe, often called simply a pipe, is a device specifically made to smoke tobacco. It comprises a chamber (the bowl) for the tobacco from which a thin hollow stem (shank) emerges, ending in a mouthpiece (the bit). Pipes can range from very simple machine-made briar models to highly prized hand-made artisanal implements made by renowned pipemakers, which are often very expensive collector's items. Pipe smoking is the oldest known traditional form of tobacco smoking.

Smoking pipes of various types have been used since ancient times. Herodotus described Scythians inhaling the fumes of burning leaves in 500 B.C.

Some Native American cultures smoke tobacco in ceremonial pipes, and have done so since long before the arrival of Europeans. For instance the Lakota people use a ceremonial pipe called čhaŋnúŋpa. Other American Indian cultures smoke tobacco socially. The tobacco plant is native to South America but spread into North America long before Europeans arrived. Tobacco was introduced to Europe from the Americas in the 16th century and spread around the world rapidly.

As tobacco was not introduced to the Old World until the 16th century, the older pipes outside of the Americas were usually used to smoke hashish, a rare and expensive substance outside areas of the Middle East, Central Asia and India, where it was then produced. 

A pipe's fundamental function is to provide a relatively safe, manipulable volume in which to incompletely combust tobacco (and/or other smokable substances) while allowing the smoke drawn from this combustion to cool sufficiently for consumption by the smoker. Typically this is accomplished by connecting a refractory 'bowl' to some sort of 'stem' which extends and cools the smoke mixture drawn through the combusting organic mass (see below).

The broad anatomy of a pipe typically comprises mainly the bowl and the stem. The "bowl" (1) which is the cup-like outer shell, the part hand-held while packing, holding and smoking a pipe, is also the part "knocked" top-down to loosen and release impacted spent tobacco. On being sucked, the general stem delivers the smoke from the bowl to the user's mouth.

Inside the bowl is an inner "chamber" (2) space holding tobacco pressed into it. This "draught hole" (3), is for air flow where air has travelled through the tobacco in the chamber, taking the smoke with it, up the "shank" (4). At the end of the shank, the pipe's "mortise" (5) and "tenon" (6) join is an air-tight, simple connection of two detachable parts where the mortise is a hole met by the tenon, a tight-fitting "tongue" at the start of the "stem" (7). Known as the "bore" (10), the inner shaft of this second section stays uniform throughout while the outer stem tapers down to the mouthpiece or "bit" (8) held in the smoker's teeth, and finally ends in the "lip" (9), attenuated for comfort.

The bowls of tobacco pipes are commonly made of briar wood, meerschaum, corncob, pear-wood or clay. Less common are other dense-grained woods such as cherry, olive, maple, mesquite, oak, and bog-wood. Minerals such as catlinite and soapstone have also been used. Pipe bowls are sometimes decorated by carving, and moulded clay pipes often had simple decoration in the mould.

Unusual, but still noteworthy pipe materials include gourds, as in the famous calabash pipe, and pyrolytic graphite. Metal and glass are uncommon materials for tobacco pipes, but are common for pipes intended for other substances, such as cannabis.

The stem needs a long channel of constant position and diameter running through it for a proper draw, although filter pipes have varying diameters and can be successfully smoked even without filters or adapters. Because it is molded rather than carved, clay may make up the entire pipe or just the bowl, but most other materials have stems made separately and detachable. Stems and bits of tobacco pipes are usually made of moldable materials like vulcanite, lucite, Bakelite, and soft plastic. Less common are stems made of reeds, bamboo, or hollowed out pieces of wood. Expensive pipes once had stems made of amber, though this is rare now.

The majority of pipes sold today, whether handmade or machine-made, are fashioned from briar (). Briar is a particularly well suited wood for pipe making for a number of reasons. The first and most important characteristic is its natural resistance to fire. The second is its inherent ability to absorb moisture. The burl absorbs water in nature to supply the tree in the dry times and likewise will absorb the moisture that is a byproduct of combustion. Briar is cut from the root burl of the tree heath ("Erica arborea"), which is native to the rocky and sandy soils of the Mediterranean region. Briar burls are cut into two types of blocks; ebauchon and plateaux. Ebauchon is taken from the heart of the burl while plateaux is taken from the outer part of the burl. While both types of blocks can produce pipes of the highest quality, most artisan pipemakers prefer to use plateaux because of its superior graining.

Meerschaum (hydrated magnesium silicate), a mineral found in small shallow deposits mainly around the city of Eskişehir in central Turkey, is prized for the properties which allow it to be carved into finely detailed decorative and figural shapes. It has been used since the 17th century and, with clay pipes, represented the most common medium for pipes before the introduction of briar as the material of choice in the 19th century. The word "meerschaum" means "sea foam" in German, alluding to its natural white color and its surprisingly low weight. Meerschaum is a very porous mineral that absorbs elements of the tobacco during the smoking process, and gradually changes color to a golden brown. Old, well-smoked meerschaum pipes are valued by collectors for their distinctive coloring.

Meerschaum pipes can either be carved from a block of meerschaum, or made from meerschaum dust collected after carving and mixed with a binder then pressed into a pipe shape. The latter are far less absorbent, color in blotches, and lack the smoking quality of the block carved pipe.

Ceramic pipes, made of moulded and then fired clay, were used almost universally by Europeans between the introduction of tobacco in the 16th century, and the introduction of cheap cigarettes at the end of the nineteenth.

The material is not very strong and the early varieties had long thin stems, so they frequently broke, but were cheap to replace. This fragility was somewhat intentional as it was utilized by Colonial American tavern keepers, for example, in renting the clay pipes to patrons. When the patron was done smoking the pipe and returned it to the keeper, the end of the stem was simply broken off so as to be ready for the next patron. The snapping off of the end mouthpiece was done until the stem was too short, rendering the pipe unusable. 

Forming the pipe involved making them in moulds with the bore created by pushing an oiled wire inside the stem. The preferred material was pipeclay or "tobacco pipe clay", which fires to a white colour and is found in only certain locations. In North America, many clay pipes were historically made from more typical terracotta-coloured clays. According to one British writer in 1869, the French preferred old pipes and the English new, the middle class preferred long stems and the working class preferred short. Short stemmed pipes, sometimes called "cuttys" or "nose warmers" in England, were preferred by those doing manual work as they could be gripped between the teeth, leaving both hands free. 

Later low-quality clay pipes were made by slip casting in a mould. Higher quality pipes are made in a labour-intensive hand shaping process. Traditionally, clay pipes are un-glazed. Clays burn "hot" in comparison to other types of pipes, so they are often difficult for most pipe-smokers to use. Their proponents claim that, unlike other materials, a well-made clay pipe gives a "pure" smoke with no flavour addition from the pipe bowl. In addition to aficionados, reproductions of historical clay styles are used by some historical re-enactors. Clay pipes were once very popular in Ireland, where they were called
"dúidín"s.
Broken fragments of clay pipe can be useful as dating evidence for archaeologists as the form varied over time. 
In the 1950's, the American archaeologist J. C. Harrington noted that the bore of pipe stems decreased, so a late sixteenth or early seventeenth centuries pipe would have a stem bore diameter of around , but a late eighteenth century pipe would have a diameter of around . The size of bowls also increased over time as tobacco became a cheaper commodity, and later pipes tended to be more decorated.

Calabash gourds (usually with meerschaum or porcelain bowls set inside them) have long made prized pipes, but they are labour-intensive and, today, quite expensive. Because of this expense, pipes with bodies made of wood (usually mahogany) instead of gourd, but with the same classic shape, are sold as calabashes. Both wood and gourd pipes are functionally the same (with the important exception that the dried gourd, usually being noticeably lighter, sits more comfortably in the mouth). They consist of a downward curve that ends with an upcurve where the bowl sits. Beneath the bowl is an air chamber which serves to cool, dry, and mellow the smoke. There are also briar pipes being sold as calabashes. These typically do not have an air chamber and are so named only because of their external shape.

A calabash pipe is rather large and easy to recognize as a pipe when used on a stage in dramatic productions. Early portrayers of the character Sherlock Holmes, particularly William Gillette and Basil Rathbone, took advantage of this fact when it was required to portray Holmes smoking. This is why Holmes is stereotypically depicted as favouring a calabash. In fact, most stories, particularly "The Adventure of the Copper Beeches", described him as preferring a long-stemmed cherry-wood churchwarden pipe or a clay pipe.

The specifically American style of pipes made from corncobs are cheap and effective, even if some regard them as inelegant. The cobs are first dried for two years. Then they are hollowed out to make a bowl shape. The bowls are dipped in a plaster-based mixture and varnished or lacquered on the outside. Shanks made from pine wood are then inserted into the bowls. The first and largest manufacturer of corncob pipes is Missouri Meerschaum, located in Washington, Missouri, in the United States. Missouri Meerschaum has produced the pipes since 1869. General Douglas MacArthur and Mark Twain were perhaps the most famous smokers of this type of pipe, along with the cartoon characters Popeye and Frosty the Snowman.

Corncob pipes remain popular today because they are inexpensive and require no "break-in" period like briar pipes. For these two reasons, corncob pipes are often recommended as a "beginner's pipe." However, corncob pipes are equally valued by both learners and experienced smokers who simply desire a cool, clean smoke. Pipesmokers who wish to sample a wide variety of different tobaccos and blends also might keep a stock of corncobs on hand to permit them to try new flavors without "carryover" from an already-used pipe, or to keep a potentially bad-tasting tobacco from adding its flavor to a more expensive or favored pipe.

A churchwarden pipe is a tobacco pipe with a long stem.

A variety of other materials may also be used for pipes. The Redmanol corporation manufactured pipes with translucent stems in the 1920s and a series of pipes were manufactured and distributed by the Tar Gard (later Venturi) Corporation of San Francisco from 1965-1975. Marketed under names such as "the pipe," "THE SMOKE" and "Venturi," they used materials such as pyrolytic graphite, phenolic resin, nylon, Bakelite and other synthetics, allowing for higher temperatures in the bowl, reduced tar, and aesthetic variations of color and style.
After Venturi stopped making pipes, several companies continue to make pipes from Brylon, a composite of nylon and wood flour, as a cheaper substitute for briar.

Metal is an uncommon material for making tobacco pipes, but they are not unknown. The most common form of this is a pipe with a shank made of aluminium, which serves as a heat sink. Mouthpieces are made of vulcanite or lucite. The bowls are removable, though not interchangeable between manufacturers. They are made of varying materials to allow the smoker to try different characteristics or to dedicate particular bowls for particular tobaccos.

Other metal tobacco pipes include the very small Japanese kiseru and Arabian midwakh. Hookahs also may have metal stems, but fall into the general category of water pipes.

A "hookah", "ghelyan", or "narghile", is a Middle Eastern water pipe that cools the smoke by filtering it through a water chamber. Often ice, cough-drops, milk, or fruit juice is added to the water. Traditionally, the tobacco is mixed with a sweetener, such as honey or molasses. Fruit flavors have also become popular. Modern hookah smokers, especially in the US, smoke "me'assel" "moassel" "molasses" or "shisha" all names for the same wet mixture of tobacco, molasses/honey, glycerine, and often, flavoring. This style of tobacco is smoked in a bowl with foil or a screen (metal or glass) on top of the bowl. More traditional tobaccos are "tombiek" (a dry unflavored tobacco, which the user moistens in water, squeezes out the extra liquid, and places coals directly on top) or "jarak" (more of a paste of tobacco with fruit to flavor the smoke).

Smoking a pipe requires more apparatus and technique than cigarette or even cigar smoking. In addition to the pipe itself and matches or a pipe lighter, smokers usually require a pipe tool for packing, adjusting, and emptying the tobacco in the bowl, and a regular supply of pipe cleaners.

Tobaccos for smoking in pipes are often carefully treated and blended to achieve flavour nuances not available in other tobacco products. Many of these are blends using staple ingredients of variously cured Burley and Virginia tobaccos which are enhanced by spice tobaccos, among them many Oriental or Balkan varietals, Latakia (a fire-cured spice tobacco of Syrian origin), Perique (uniquely grown in St. James Parish, Louisiana) which is also an old method of fermentation, or blends of Virginia and Burley tobaccos of African, Indian, or South American origins. Traditionally, many U.S. blends are made of American Burley with sweeteners and flavorings added to create an "aromatic" flavor, whereas "English" blends are based on natural Virginia tobaccos enhanced with Oriental and other natural tobaccos. There is a growing tendency towards "natural" tobaccos which derive their aromas from artful blending with selected spice tobaccos only and careful, often historically-based, curing processes.

Pipe tobacco can be purchased in several forms, which vary both in flavour (leading to many blends and opportunities for smokers to blend their own tobaccos) and in the physical shape and size to which the tobacco has been reduced. Most pipe tobaccos are less mild than cigarette tobacco, substantially more moist and cut much more coarsely. Too finely cut tobacco does not allow enough air to flow through the pipe, and overly dry tobacco burns too quickly with little flavour. Pipe tobacco must be kept in an airtight container, such as a canning jar or sealed tin, to keep from drying out.

Some pipe tobaccos are cut into long narrow ribbons. Some are pressed into flat plugs which are sliced into flakes. Others are tightly wound into long ropes, then sliced into discs. Plug tobacco is maintained in its pressed block form and sold in small blocks. The plug will be sliced into thin flakes by the smoker and then prepared in a similar fashion to flake tobacco. It is considered that plug tobacco holds its flavor better than rubbed or flake tobacco. Flake tobacco (sliced cakes or ropes) may be prepared in several ways. Generally it is rubbed out with the fingers and palms until it is loose enough to pack. It can also be crumbled or simply folded and stuffed into a pipe. Some people also prefer to dice up very coarse tobaccos before using them, making them easier to pack.

In the most common method of packing, tobacco is added to the bowl of the pipe in several batches, each one pressed down until the mixture has a uniform density that optimizes airflow (something that it is difficult to gauge without practice). This can be done with a finger or thumb, but if the tobacco needs to be repacked later, while it is burning, the tamper on a pipe tool is sometimes used. If it needs to be loosened, the reamer, or any similar long pin can be used. A traditional way of packing the pipe is to fill the bowl and then pack gently to about full, fill again and pack slightly more firmly to about full, and then pack more firmly still to the top.

An alternative packing technique called the Frank method involves lightly dropping tobacco in the pipe, after which a large plug is gingerly pushed into the bowl all at once.

Matches, or separately lit slivers of wood are often considered preferable to lighters because of lower burning temperature. Butane lighters made specifically for pipes 
emit flame sideways or at an angle to make it easier to direct flame into the bowl. Torch-style lighters should never be used to light a pipe because their flames are too hot and can char the rim of the pipe bowl. Matches should be allowed to burn for several seconds to allow the sulfur from the tip to burn away and the match to produce a full flame. A naphtha fueled lighter should also be allowed to burn a few seconds to get rid of stray naphtha vapors that could give a foul taste to the smoke. When a flame has been produced, it is then moved in circles above the rim of the bowl while the smoker puffs to draw the flame down and light the tobacco. Packing method and humidity can affect how often a pipe must be relit.

With care, a briar pipe can last a very long time without burning out. However, due to aggressive (hot) smoking, imperfections in the wood, a hole can be burned in the tobacco chamber of the pipe. There are several methods used to help prevent a wood pipe from burning out. These generally involve coating the chamber with any of a variety of substances, or by gently smoking a new pipe to build up a cake (a mixture of ash, unburned tobacco, oils, sugars, and other residue) on the walls.

These coatings may include honey and water; powdered sugar and water; cigar ash and water; and sour cream, buttermilk, and activated charcoal among many others.

Many modern briar pipes are pre-treated by the manufacturer to resist burning. If smoked correctly, the cake will build up properly on its own. Another technique is to alternate a half-bowl and a full-bowl the first several times the pipe is used to build an even cake. Burley is often recommended to help a new pipe build cake.

The effectiveness of these methods is by no means universally agreed upon.

The caked layer that helps prevent burning through the bottom or sides of a briar wood pipe may damage other pipes, such as meerschaum or clay. As the cake layer heats up, it expands and may cause cracks or breaks in non-briar pipes.

Pipe smoke, like cigar smoke, is usually not inhaled. It is merely brought into the mouth, pumped around oral and nasal cavities to permit absorption of nicotine toward the brain through the mucous membranes, and released. It is normal to have to relight a pipe periodically. If it is smoked too slowly, this will happen more often. If it is smoked too quickly, it can produce excess moisture causing a gurgling sound in the pipe and an uncomfortable sensation on the tongue (referred to as "pipe tongue", or more commonly, "tongue bite").

A pipe cleaner can be used to dry out the bowl and, wetted with alcohol, the inner channel. The bowl of the pipe can also become uncomfortably hot, depending on the material and the rate of smoking. For this reason, clay pipes in particular are often held by the stem. Meerschaum pipes are held in a square of chamois leather, with gloves, or else by the stem in order to prevent uneven coloring of the material.
The ash and the last bits of unburned tobacco, known as dottle, should be cleaned out with a suitable pipe tool. A soft or bristle pipe cleaner, which may be moistened with strong spirits is then run through the airways of the stem and shank to remove any moisture, ash, and other residue before the pipe is allowed to dry. A pipe should be allowed to cool before removing the stem to avoid the possibility of warping it.

A cake of ash eventually develops inside the bowl. This is generally considered desirable for controlling overall heat. However, if it becomes too thick, it may expand faster than the bowl of the pipe itself when heated, cracking the bowl. Before reaching this point, it needs to be scraped down with a reamer. It is generally recommended to keep the cake at approximately the thickness of a U.S. dime (about 1/20th of an inch or 1.5 mm), though sometimes the cake is removed entirely as part of efforts to eliminate flavors or aromas.

Cake is considered undesirable in meerschaum pipes because it can easily crack the bowl and/or interfere with the mineral's natural porosity. Meerschaum also softens when heated so it is recommended to allow meerschaum pipes to cool before cleaning as people have been known to push pipe cleaners through the walls of heated pipes.

Regardless if a pipe is cleaned after every smoke, over time there is a buildup of cake in the bowl and tars in the internals of a smoking pipe. The cake can be controlled by gentle reaming, but a buildup of tars in the shank and airway of a pipe is more difficult to deal with. This may require the services of a professional pipe restorer to properly clean and sanitize the pipe.

When tobacco is burned, oils from adjoining not yet ignited particles vaporize and condense into the existing cake on the walls of the bowl and shank. Over time, these oils can oxidize and turn rancid, causing the pipe to give a sour or bitter smoke. A purported countermeasure involves filling the bowl with kosher salt and carefully wetting it with strong spirits. It is important to not use iodized salt, as the iodine and other additives may impart an unpleasant flavor. Regularly wiping out the bowl with spirits such as vodka or rum is helpful in preventing souring. Commercial pipe-sweetening products are also available.





</doc>
<doc id="27003" url="https://en.wikipedia.org/wiki?curid=27003" title="Swiss cheese">
Swiss cheese

Swiss cheese is a generic name in North America for several related varieties of cheese, mainly of North American manufacture, which resemble Emmental cheese, a yellow, medium-hard cheese that originated in the area around Emmental, in Switzerland. Some types of Swiss cheese have a distinctive appearance, as the blocks of the cheese are riddled with holes known as "eyes". Swiss cheese without eyes is known as "blind". (The term is applied to cheeses of this style made outside Switzerland, such as Jarlsberg cheese, which originates in Norway).

Three types of bacteria are used in the production of Emmental cheese: "Streptococcus salivarius" subspecies "thermophilus", "Lactobacillus" ("Lactobacillus helveticus" or "Lactobacillus delbrueckii" subspecies "bulgaricus"), and "Propionibacterium" ("Propionibacterium freudenreichii" subspecies "shermani"). In a late stage of cheese production, the propionibacteria consume the lactic acid excreted by the other bacteria and release acetate, propionic acid, and carbon dioxide gas. The carbon dioxide slowly forms the bubbles that develop the "eyes". The acetate and propionic acid give Swiss its nutty and sweet flavor. A hypothesis proposed by Swiss researchers in 2015 notes that particulate matter may also play a role in the holes' development and that modern sanitation eliminated debris such as hay dust in the milk played a role in reduced hole size in Swiss cheeses, or even "blind cheese". Historically, the holes were seen as a sign of imperfection and cheese makers originally tried to avoid them by pressing during production. In modern times, the holes have become an identifier of the cheese.

In general, the larger the eyes in a Swiss cheese, the more pronounced its flavor because a longer fermentation period gives the bacteria more time to act. This poses a problem, however, because cheese with large eyes does not slice well and comes apart in mechanical slicers. As a result, industry regulators have limited the eye size by which Swiss cheese receives the Grade A stamp.

In 2014, 297.8 million pounds of Swiss cheese was reportedly produced in the United States.

Baby Swiss and Lacy Swiss are two varieties of American Swiss cheeses. Both have small holes and a mild flavor. Baby Swiss is made from whole milk, and Lacy Swiss is made from low fat milk. Baby Swiss was developed in the mid-1960s outside of Charm, Ohio, by the Guggisberg Cheese Company, owned by Alfred Guggisberg.


</doc>
<doc id="27004" url="https://en.wikipedia.org/wiki?curid=27004" title="Spontaneous combustion (disambiguation)">
Spontaneous combustion (disambiguation)

Spontaneous combustion is the self-ignition of a mass, for example, a pile of oily rags. Allegedly, humans can also ignite and burn without an obvious cause; this phenomenon is known as spontaneous human combustion.

Spontaneous Combustion is also the name of:



</doc>
<doc id="27005" url="https://en.wikipedia.org/wiki?curid=27005" title="Smoke signal">
Smoke signal

The smoke signal is one of the oldest forms of long-distance communication. It is a form of visual communication used over long distance. In general smoke signals are used to transmit news, signal danger, or gather people to a common area.

In ancient China, soldiers stationed along the Great Wall would alert each other of impending enemy attack by signaling from tower to tower. In this way, they were able to transmit a message as far away as in just a few hours.

Abuse of the smoke signal is known to have contributed to the fall of the Western Zhou Dynasty in the 8th century BCE. King You of Zhou had a habit of fooling his warlords with false warning beacons in order to amuse Bao Si, his concubine who was known as one of four ancient beauties of China. When an actual rebellion occurred, no one came to the aid of the king.

Polybius, a Greek historian, devised a more complex system of alphabetical smoke signals around 150 BCE, which converted Greek alphabetic characters into numeric characters. It enabled messages to be easily signaled by holding sets of torches in pairs. This idea, known as the "Polybius square", also lends itself to cryptography and steganography. This cryptographic concept has been used with Japanese Hiragana and the Germans in the later years of the First World War.

The North American indigenous peoples also communicated via smoke signal. Each tribe had its own signaling system and understanding. A signaler started a fire on an elevation typically using damp grass, which would cause a column of smoke to rise. The grass would be taken off as it dried and another bundle would be placed on the fire. Reputedly the location of the smoke along the incline conveyed a meaning. If it came from halfway up the hill, this would signify all was well, but from the top of the hill it would signify danger.

Smoke signals remain in use today. In Rome, the College of Cardinals uses smoke signals to indicate the selection of a new Pope during a papal conclave. Eligible cardinals conduct a secret ballot until someone receives a vote of two-thirds plus one. The ballots are burned after each vote. Black smoke indicates a failed ballot, while white smoke means a new Pope has been elected.

Smoke signals may also refer to smoke-producing devices used to send distress signals.

Lewis and Clark's journals cite several occasions when they adopted the Native American method of setting the plains on fire to communicate the presence of their party or their desire to meet with local tribes.

Yámanas of South America used fire to send messages by smoke signals, for instance if a whale drifted ashore. The large amount of meat required notification of many people, so that it would not decay. They might also have used smoke signals on other occasions, thus it is possible that Magellan saw such fires (which inspired him to name the landscape Tierra del Fuego) but he may have seen the smoke or lights of natural phenomena.

Noon Gun time signalling was used to set marine chronometers in Table Bay.

Aboriginal Australians throughout Australia would send up smoke signals for various purposes. Sometimes to notify others of their presence, particularly when entering lands which were not their own. Sometimes used to describe visiting whites, smoke signals were the fastest way to send messages. Smoke signals were sometimes to notify of incursions by hostile tribes, or to arrange meetings between hunting parties of the same tribe. This signal could be from a fixed lookout on a ridge of from a mobile band of tribesman. "Putting up a smoke" would often result in nearby individuals or groups replying with their own signals. To carry information, the colour of the smoke was varied, sometimes black, white or blue depending on whether the material being burnt was wet grass, dry grass, reeds or other, and the shape of the smoke could be a column, ball or smoke ring. This message could include the names of individual tribesmen. Like other means of communication, signals could be misinterpreted. In one recorded instance, a smoke signal reply translated as "we are coming" was misinterpreted as joining a war party for protection of the tribe when it was actually hunting parties coming together after a successful hunt.

Modern avionics has made skywriting possible.



</doc>
<doc id="27006" url="https://en.wikipedia.org/wiki?curid=27006" title="Serendipity">
Serendipity

Serendipity means an unplanned, fortuitous discovery. 

The term was coined by Horace Walpole in 1754. In a letter he wrote to his friend Horace Mann, Walpole explained an unexpected discovery he had made about a (lost) painting of Bianca Cappello by Giorgio Vasari by reference to a Persian fairy tale, "The Three Princes of Serendip". The princes, he told his correspondent, were "always making discoveries, by accidents and sagacity, of things which they were not in quest of".

The notion of serendipity is a common occurrence throughout the history of scientific innovation. Examples are Alexander Fleming's accidental discovery of penicillin in 1928, the invention of the microwave oven by Percy Spencer in 1945, and the invention of the Post-it note by Spencer Silver in 1968.

In June 2004, a British translation company voted the word to be one of the ten English words hardest to translate. However, due to its sociological use, the word has since been exported into many other languages.

The first noted use of "serendipity" in the English language was by Horace Walpole (1717–1797). In a letter to Horace Mann (dated 28 January 1754) he said he formed it from the Persian fairy tale "The Three Princes of Serendip", whose heroes "were always making discoveries, by accidents and sagacity, of things they were not in quest of". The name comes from "Serendip", an old name for Sri Lanka (aka Ceylon), hence "Sarandib" by Arab traders. It is derived from the Sanskrit "Siṃhaladvīpaḥ" (Siṃhalaḥ, Sri Lanka + dvīpaḥ, island).

M. E. Graebner describes serendipitous value in the context of the acquisition of a business as "windfalls that were not anticipated by the buyer prior to the deal": i.e., unexpected advantages or benefits incurred due to positive synergy effects of the merger.

Ikujiro Nonaka points out that the serendipitous quality of innovation is highly recognized by managers and links the success of Japanese enterprises to their ability to create knowledge not by processing information but rather by "tapping the tacit and often highly subjective insights, intuitions, and hunches of individual employees and making those insights available for testing and use by the company as a whole".

Serendipity is postulated by Napier and Vuong (2013) as a 'strategic advantage' with which a firm can tap its potential creativity.

Serendipity is a key concept in competitive intelligence because it is one of the tools for avoiding blind spots (see Blindspots analysis).

Serendipity is used as a sociological method in Anselm L. Strauss' and Barney G. Glaser's "grounded theory", building on ideas by sociologist Robert K. Merton, who in "Social Theory and Social Structure" (1949) referred to the "serendipity pattern" as the fairly common experience of observing an unanticipated, anomalous and strategic datum which becomes the occasion for developing a new theory or for extending an existing theory. Robert K. Merton also coauthored (with Elinor Barber) "The Travels and Adventures of Serendipity" which traces the origins and uses of the word "serendipity" since it was coined. The book is "a study in sociological semantics and the sociology of science", as the subtitle of the book declares. It further develops the idea of serendipity as scientific "method" (as juxtaposed with purposeful discovery by experiment or retrospective prophecy).

William Boyd coined the term zemblanity in the late twentieth century to mean somewhat the opposite of serendipity: "making unhappy, unlucky and unexpected discoveries occurring by design". A zemblanity is, effectively, an "unpleasant unsurprise". It derives from Novaya Zemlya (or Nova Zembla), a cold, barren land with many features opposite to the lush Sri Lanka (Serendip). On this island Willem Barents and his crew were stranded while searching for a new route to the east.

Bahramdipity is derived directly from Bahram Gur as characterized in "The Three Princes of Serendip". It describes the "suppression" of serendipitous discoveries or research results by powerful individuals.





</doc>
<doc id="27007" url="https://en.wikipedia.org/wiki?curid=27007" title="Samuel Morse">
Samuel Morse

Samuel Finley Breese Morse (April 27, 1791 – April 2, 1872) was an American painter and inventor. After having established his reputation as a portrait painter, in his middle age Morse contributed to the invention of a single-wire telegraph system based on European telegraphs. He was a co-developer of the Morse code and helped to develop the commercial use of telegraphy.

Samuel F. B. Morse was born in Charlestown, Massachusetts, the first child of the pastor Jedidiah Morse (1761–1826), who was also a geographer, and his wife Elizabeth Ann Finley Breese (1766–1828). His father was a great preacher of the Calvinist faith and supporter of the American Federalist party. He thought it helped preserve Puritan traditions (strict observance of Sabbath, among other things), and believed in the Federalist support of an alliance with Britain and a strong central government. Morse strongly believed in education within a Federalist framework, alongside the instillation of Calvinist virtues, morals, and prayers for his first son.

After attending Phillips Academy in Andover, Massachusetts, Samuel Morse went on to Yale College to receive instruction in the subjects of religious philosophy, mathematics, and science of horses. While at Yale, he attended lectures on electricity from Benjamin Silliman and Jeremiah Day and was a member of the Society of Brothers in Unity. He supported himself by painting. In 1810, he graduated from Yale with Phi Beta Kappa honors.

Morse expressed some of his Calvinist beliefs in his painting, "Landing of the Pilgrims", through the depiction of simple clothing as well as the people's austere facial features. His image captured the psychology of the Federalists; Calvinists from England brought to North America ideas of religion and government, thus linking the two countries. This work attracted the attention of the notable artist, Washington Allston. Allston wanted Morse to accompany him to England to meet the artist Benjamin West. Allston arranged—with Morse's father—a three-year stay for painting study in England. The two men set sail aboard the "Libya" on July 15, 1811.

In England, Morse perfected his painting techniques under Allston's watchful eye; by the end of 1811, he gained admittance to the Royal Academy. At the Academy, he was moved by the art of the Renaissance and paid close attention to the works of Michelangelo and Raphael. After observing and practicing life drawing and absorbing its anatomical demands, the young artist produced his masterpiece, the "Dying Hercules". (He first made a sculpture as a study for the painting.)

To some, the "Dying Hercules" seemed to represent a political statement against the British and also the American Federalists. The muscles symbolized the strength of the young and vibrant United States versus the British and British-American supporters. During Morse's time in Britain, the Americans and British were engaged in the War of 1812. Both societies were conflicted over loyalties. Anti-Federalist Americans aligned themselves with the French, abhorred the British, and believed a strong central government to be inherently dangerous to democracy.

As the war raged on, Morse's letters to his parents became more anti-Federalist in tone. In one such letter, Morse wrote:

I assert...that the Federalists in the Northern States have done more injury to their country by their violent opposition measures than a French alliance could. Their proceedings are copied into the English papers, read before Parliament, and circulated through their country, and what do they say of them... they call them [Federalists] cowards, a base set, say they are traitors to their country and ought to be hanged like traitors.

Although Jedidiah Morse did not change Samuel's political views, he continued as an influence. Critics believe that the elder Morse's Calvinist ideas are integral to Morse's "Judgment of Jupiter," another significant work completed in England. Jupiter is shown in a cloud, accompanied by his eagle, with his hand spread above the parties and he is pronouncing judgment. Marpessa, with an expression of compunction and shame, is throwing herself into the arms of her husband. Idas, who tenderly loved Marpessa, is eagerly rushing forward to receive her while Apollo stares with surprise.

Critics have suggested that Jupiter represents God's omnipotence—watching every move that is made. Some call the portrait a moral teaching by Morse on infidelity. Although Marpessa fell victim, she realized that her eternal salvation was important and desisted from her wicked ways. Apollo shows no remorse for what he did but stands with a puzzled look. Many American paintings throughout the early nineteenth century had religious themes, and Morse was an early exemplar of this. "Judgment of Jupiter" allowed Morse to express his support of Anti-Federalism while maintaining his strong spiritual convictions. Benjamin West sought to present the "Jupiter" at another Royal Academy exhibition, but Morse's time had run out. He left England on August 21, 1815, to return to the United States and begin his full-time career as a painter.

The decade 1815–1825 marked significant growth in Morse's work, as he sought to capture the essence of America's culture and life. He painted the Federalist former President John Adams (1816). The Federalists and Anti-Federalists clashed over Dartmouth College. Morse painted portraits of Francis Brown—the college's president—and Judge Woodward (1817), who was involved in bringing the Dartmouth case before the U.S. Supreme Court.

Morse also sought commissions among the elite of Charleston, South Carolina. Morse's 1818 painting of Mrs. Emma Quash symbolized the opulence of Charleston. The young artist was doing well for himself. Between 1819 and 1821, Morse went through great changes in his life, including a decline in commissions due to the Panic of 1819. Unable to stop the rift within Calvinism, his father was forced to resign from his ministerial position, which he had held for three decades. The new branch that formed was the Congregational Unitarians, Morse considered them to be anti-Federalists, as their beliefs were related to religious salvation.

Although Samuel Morse respected his father's religious opinions, he sympathized with the Unitarians. Among the converts to Unitarianism were the prominent Pickerings of Portsmouth, New Hampshire, whom Morse had painted. Some critics thought his sympathies represented his own anti-Federalism. Morse was commissioned to paint President James Monroe in 1820. He embodied Jeffersonian democracy by favoring the common man over the aristocrat.

Morse had moved to New Haven. His commissions for the "Hall of Congress" (1821) and a portrait of the Marquis de Lafayette (1825) engaged his sense of democratic nationalism. The "Hall of Congress" was designed to capitalize on the success of François-Marius Granet's "The Capuchin Chapel in Rome," which toured the United States extensively throughout the 1820s, attracting audiences willing to pay the 25-cent admission fee.

The artist chose to paint the House of Representatives, in a similar way, with careful attention to architecture and dramatic lighting. He also wished to select a uniquely American topic that would bring glory to the young nation. His subject did just that, showing American democracy in action. He traveled to Washington D.C. to draw the architecture of the new Capitol and placed eighty individuals within the painting. He chose to portray a night scene, balancing the architecture of the Rotunda with the figures, and using lamplight to highlight the work. Pairs of people, those who stood alone, individuals bent over their desks working, were each painted simply but with faces of character. Morse chose nighttime to convey that Congress' dedication to the principles of democracy transcended day.

"The Hall of Congress" failed to draw a crowd when exhibited in New York City in 1821. By contrast, John Trumbull's "Declaration of Independence" had won popular acclaim the previous year. Viewers may have felt that the architecture of "The Hall of Congress" overshadows the individuals, making it hard to appreciate the drama of what was happening.

Morse was honored to paint the Marquis de Lafayette, the leading French supporter of the American Revolution. He felt compelled to paint a grand portrait of the man who helped to establish a free and independent America. He features Lafayette against a magnificent sunset. He has positioned Lafayette to the right of three pedestals: one has a bust of Benjamin Franklin, another of George Washington, and the third seems reserved for Lafayette. A peaceful woodland landscape below him symbolized American tranquility and prosperity as it approached the age of fifty. The developing friendship between Morse and Lafayette and their discussions of the Revolutionary War affected the artist after his return to New York City.

In 1826, he helped found the National Academy of Design in New York City. He served as the Academy's President from 1826 to 1845 and again from 1861 to 1862.

From 1830 to 1832, Morse traveled and studied in Europe to improve his painting skills, visiting Italy, Switzerland, and France. During his time in Paris, he developed a friendship with the writer James Fennimore Cooper. As a project, he painted miniature copies of 38 of the Louvre's famous paintings on a single canvas (6 ft. x 9 ft), which he entitled "The Gallery of the Louvre." He completed the work upon his return to the United States.

On a subsequent visit to Paris in 1839, Morse met Louis Daguerre. He became interested in the latter's daguerreotype—the first practical means of photography. Morse wrote a letter to the "New York Observer" describing the invention, which was published widely in the American press and provided a broad awareness of the new technology.

Some of Morse's paintings and sculptures are on display at his Locust Grove estate in Poughkeepsie, New York.

As noted, in 1825 New York City had commissioned Morse to paint a portrait of Lafayette in Washington, DC. While Morse was painting, a horse messenger delivered a letter from his father that read, "Your dear wife is convalescent". The next day he received a letter from his father detailing his wife's sudden death. Morse immediately left Washington for his home at New Haven, leaving the portrait of Lafayette unfinished. By the time he arrived, his wife had already been buried. Heartbroken that for days he was unaware of his wife's failing health and her death, he decided to explore a means of rapid long distance communication.

While returning by ship from Europe in 1832, Morse encountered Charles Thomas Jackson of Boston, a man who was well schooled in electromagnetism. Witnessing various experiments with Jackson's electromagnet, Morse developed the concept of a single-wire telegraph. He set aside his painting, "The Gallery of the Louvre". The original Morse telegraph, submitted with his patent application, is part of the collections of the National Museum of American History at the Smithsonian Institution. In time the Morse code, which he developed, would become the primary language of telegraphy in the world. It is still the standard for rhythmic transmission of data.

Meanwhile, William Cooke and Professor Charles Wheatstone had learned of the Wilhelm Weber and Carl Gauss electromagnetic telegraph in 1833. They had reached the stage of launching a commercial telegraph prior to Morse, despite starting later. In England, Cooke became fascinated by electrical telegraphy in 1836, four years after Morse. Aided by his greater financial resources, Cooke abandoned his primary subject of anatomy and built a small electrical telegraph within three weeks. Wheatstone also was experimenting with telegraphy and (most importantly) understood that a single large battery would not carry a telegraphic signal over long distances. He theorized that numerous small batteries were far more successful and efficient in this task. (Wheatstone was building on the primary research of Joseph Henry, an American physicist). Cooke and Wheatstone formed a partnership and patented the electrical telegraph in May 1837, and within a short time had provided the Great Western Railway with a stretch of telegraph. However, within a few years, Cooke and Wheatstone's multiple-wire signaling method would be overtaken by Morse's cheaper method.

In an 1848 letter to a friend, Morse describes how vigorously he fought to be called the sole inventor of the electromagnetic telegraph despite the previous inventions.

Morse encountered the problem of getting a telegraphic signal to carry over more than a few hundred yards of wire. His breakthrough came from the insights of Professor Leonard Gale, who taught chemistry at New York University (he was a personal friend of Joseph Henry). With Gale's help, Morse introduced extra circuits or relays at frequent intervals and was soon able to send a message through of wire. This was the great breakthrough he had been seeking. Morse and Gale were soon joined by Alfred Vail, an enthusiastic young man with excellent skills, insights, and money.

At the Speedwell Ironworks in Morristown, New Jersey on January 11, 1838, Morse and Vail made the first public demonstration of the electric telegraph. Although Morse and Alfred Vail had done most of the research and development in the ironworks facilities, they chose a nearby factory house as the demonstration site. Without the repeater, the range of the telegraph was limited to , and the inventors had pulled of wires inside the factory house through an elaborate scheme. The first public transmission, with the message, "A patient waiter is no loser", was witnessed by a mostly local crowd.

Morse traveled to Washington, D.C. in 1838 seeking federal sponsorship for a telegraph line but was not successful. He went to Europe, seeking both sponsorship and patents, but in London discovered that Cooke and Wheatstone had already established priority. After his return to the US, Morse finally gained financial backing by Maine congressman Francis Ormand Jonathan Smith. This funding may be the first instance of government support to a private researcher, especially funding for applied (as opposed to basic or theoretical) research.

Morse made his last trip to Washington, D.C., in December 1842, stringing "wires between two committee rooms in the Capitol, and sent messages back and forth" to demonstrate his telegraph system. Congress appropriated $30,000 in 1843 for construction of an experimental telegraph line between Washington, D.C., and Baltimore along the right-of-way of the Baltimore and Ohio Railroad. An impressive demonstration occurred on May 1, 1844, when news of the Whig Party's nomination of Henry Clay for U.S. President was telegraphed from the party's convention in Baltimore to the Capitol Building in Washington.

On May 24, 1844, the line was officially opened as Morse sent the now-famous words, "What hath God wrought," from the Supreme Court chamber in the basement of the U.S. Capitol building in Washington, D.C., to the B&O's Mount Clare Station in Baltimore. Annie Ellsworth chose these words from the Bible (Numbers 23:23); her father, U.S. Patent Commissioner Henry Leavitt Ellsworth, had championed Morse's invention and secured early funding for it. His telegraph could transmit thirty characters per minute.

In May 1845, the Magnetic Telegraph Company was formed in order to build telegraph lines from New York City toward Philadelphia; Boston; Buffalo, New York; and the Mississippi. Telegraphic lines rapidly spread throughout the United States in the next few years, with 12,000 miles of wire laid by 1850.

Morse at one time adopted Wheatstone and Carl August von Steinheil's idea of broadcasting an electrical telegraph signal through a body of water or down steel railroad tracks or anything conductive. He went to great lengths to win a lawsuit for the right to be called "inventor of the telegraph" and promoted himself as being an inventor. But Alfred Vail also played an important role in the development of the Morse code, which was based on earlier codes for the electromagnetic telegraph.

Morse received a patent for the telegraph in 1847, at the old Beylerbeyi Palace (the present Beylerbeyi Palace was built in 1861–1865 on the same location) in Istanbul, which was issued by Sultan Abdülmecid, who personally tested the new invention. He was elected an Associate Fellow of the American Academy of Arts and Sciences in 1849. The original patent went to the Breese side of the family after the death of Samuel Morse.

In the 1850s, Morse went to Copenhagen and visited the Thorvaldsens Museum, where the sculptor's grave is in the inner courtyard. He was received by King Frederick VII, who decorated him with the Order of the Dannebrog. Morse expressed his wish to donate his portrait from 1830 to the king. The Thorvaldsen portrait today belongs to Margrethe II of Denmark.

The Morse telegraphic apparatus was officially adopted as the standard for European telegraphy in 1851. Only the United Kingdom (with its extensive overseas empire) kept the needle telegraph of Cooke and Wheatstone.

In 1858, Morse introduced wired communication to Latin America when he established a telegraph system in Puerto Rico, then a Spanish Colony. Morse's oldest daughter, Susan Walker Morse (1819–1885), would often visit her uncle Charles Pickering Walker, who owned the Hacienda Concordia in the town of Guayama. During one of her visits, she met Edward Lind, a Danish merchant who worked in his brother-in-law's Hacienda La Henriqueta in the town of Arroyo. They later married. Lind purchased the Hacienda from his sister when she became a widow. Morse, who often spent his winters at the Hacienda with his daughter and son-in-law, set a two-mile telegraph line connecting his son-in-law's Hacienda to their house in Arroyo. The line was inaugurated on March 1, 1859, in a ceremony flanked by the Spanish and American flags. The first words transmitted by Samuel Morse that day in Puerto Rico were:

Puerto Rico, beautiful jewel! When you are linked with the other jewels of the Antilles in the necklace of the world's telegraph, yours will not shine less brilliantly in the crown of your Queen!

There is an argument amongst historians that Morse may have received the idea of a plausible telegraph from Harrison Gray Dyar some eighteen years earlier than his patent.

Morse was a leader in the anti-Catholic and anti-immigration movement of the mid-19th century. In 1836, he ran unsuccessfully for mayor of New York under the anti-immigrant Nativist Party's banner, receiving only 1,496 votes. When Morse visited Rome, he allegedly refused to take his hat off in the presence of the Pope.

Morse worked to unite Protestants against Catholic institutions (including schools), wanted to forbid Catholics from holding public office, and promoted changing immigration laws to limit immigration from Catholic countries. On this topic, he wrote, "We must first stop the leak in the ship through which muddy waters from without threaten to sink us."

He wrote numerous letters to the New York "Observer" (his brother Sidney was the editor at the time) urging people to fight the perceived Catholic menace. These were widely reprinted in other newspapers. Among other claims, he believed that the Austrian government and Catholic aid organizations were subsidizing Catholic immigration to the United States in order to gain control of the country.

In his "Foreign Conspiracy Against the Liberties of the United States", Morse wrote:
Surely American Protestants, freemen, have discernment enough to discover beneath them the cloven foot of this subtle foreign heresy. They will see that Popery is now, what it has ever been, a system of the darkest political intrigue and despotism, cloaking itself to avoid attack under the sacred name of religion. They will be deeply impressed with the truth, that Popery is a political as well as a religious system; that in this respect it differs totally from all other sects, from all other forms of religion in the country.

In the 1850s, Morse became well known as a defender of slavery, considering it to be sanctioned by God. This was a position held by many Southerners and others. In his treatise "An Argument on the Ethical Position of Slavery," he wrote:

Morse married Lucretia Pickering Walker on September 29, 1818, in Concord, New Hampshire. She died on February 7, 1825, shortly after the birth of their third child (Susan b. 1819, Charles b. 1823, James b. 1825). He married his second wife, Sarah Elizabeth Griswold on August 10, 1848, in Utica, New York and had four children (Samuel b. 1849, Cornelia b. 1851, William b. 1853, Edward b. 1857).

In the United States, Morse held his telegraph patent for many years, but it was both ignored and contested. In 1853, "The Telegraph Patent case – O'Reilly v. Morse" came before the U.S. Supreme Court where, after very lengthy investigation, Chief Justice Roger B. Taney ruled that Morse had been the first to combine the battery, electromagnetism, the electromagnet, and the correct battery configuration into a workable practical telegraph. However, in spite of this clear ruling, Morse still received no official recognition from the United States government.

The Supreme Court did not accept all of Morse's claims. The "O'Reilly v. Morse" case has become widely known among patent lawyers because the Supreme Court explicitly denied Morse's claim 8 for any and all use of the electromagnetic force for purposes of transmitting intelligible signals to any distance.
The Supreme Court sustained, however, Morse's claim to such telecommunication when effectuated by means of Morse's inventive "repeater" apparatus. This was an electrical circuit in which a cascade of many sets comprising a relay and a battery were connected in series, so that when each relay closed, it closed a circuit to cause the next battery to power the succeeding relay, as suggested in the accompanying figure. This caused Morse's signal to pass along the cascade without degrading into noise as its amplitude decreased with the distance traveled. (Each time the amplitude of the signal approaches the noise level, the repeater [in effect, a nonlinear amplifier] boosts the signal amplitude well above the noise level.) This use of "repeaters" permitted a message to be sent to great distances, which was previously not feasible.

The Supreme Court thus held that Morse could properly claim a patent monopoly on the system or process of transmitting signals at any distance by means of the repeater circuitry indicated above, but he could not properly claim a monopoly over any and all uses of electromagnetic force to transmit signals. The apparatus limitation in the former type of claim limited the patent monopoly to what Morse taught and gave the world. The lack of that limitation in the latter type of claim (i.e., claim 8) both gave Morse more than was commensurate with what he had contributed to society and discouraged the inventive efforts of others who might come up with different and/or better ways to send signals at a distance using the electromagnetic force.

The problem that Morse faced and how he solved it is discussed in more detail in the article "O'Reilly v. Morse". In summary, the solution, as the Supreme Court stated, was the repeater apparatus described in the preceding paragraphs.

The importance of this legal precedent in patent law cannot be overstated, as it became the foundation of the law governing the eligibility of computer program implemented inventions (as well as inventions implementing natural laws) to be granted patents.

Assisted by the American ambassador in Paris, the governments of Europe were approached about their long neglect of Morse while their countries were using his invention. There was a widespread recognition that something must be done, and in 1858 Morse was awarded the sum of 400,000 French francs (equivalent to about $80,000 at the time) by the governments of France, Austria, Belgium, the Netherlands, Piedmont, Russia, Sweden, Tuscany, and Turkey, each of which contributed a share according to the number of Morse instruments in use in each country. In 1858, he was also elected a foreign member of the Royal Swedish Academy of Sciences.

Morse lent his support to Cyrus West Field's ambitious plan to construct the first transoceanic telegraph line. Morse had experimented with underwater telegraph circuits since 1842. He invested $10,000 in Field's Atlantic Telegraph Company, took a seat on its board of directors, and was appointed honorary "Electrician". In 1856, Morse traveled to London to help Charles Tilston Bright and Edward Whitehouse test a 2,000-mile-length of spooled cable.

After the first two cable-laying attempts failed, Field reorganized the project, removing Morse from direct involvement. Though the cable broke three times during the third attempt, it was successfully repaired, and the first transatlantic telegraph messages were sent in 1858. The cable failed after just three months of use. Though Field had to wait out the Civil War, the cable laid in 1866 proved more durable, and the era of reliable transatlantic telegraph service had begun.

In addition to the telegraph, Morse invented a marble-cutting machine that could carve three-dimensional sculptures in marble or stone. He could not patent it, however, because of an existing 1820 Thomas Blanchard design.

Samuel Morse gave large sums to charity. He also became interested in the relationship of science and religion and provided the funds to establish a lectureship on "the relation of the Bible to the Sciences". Though he was rarely awarded any royalties for the later uses and implementations of his inventions, he was able to live comfortably.

He died in New York City on April 2, 1872, and was interred at Green-Wood Cemetery in Brooklyn, New York. By the time of his death, his estate was valued at some $500,000 ($ today).

Morse was elected a member of the American Antiquarian Society in 1815.

Despite honors and financial awards received from foreign countries, there was no such recognition in the U.S. until he neared the end of his life when on June 10, 1871, a bronze statue of Samuel Morse was unveiled in Central Park, New York City. An engraved portrait of Morse appeared on the reverse side of the United States two-dollar bill silver certificate series of 1896. He was depicted along with Robert Fulton. An example can be seen on the website of the Federal Reserve Bank of San Francisco's website in their "American Currency Exhibit":

A blue plaque was erected to commemorate him at 141 Cleveland Street, London, where he lived from 1812 to 1815.

According to his "The New York Times" obituary published on April 3, 1872, Morse received respectively the decoration of the Atiq Nishan-i-Iftikhar (English: Order of Glory) [first medal on wearer's right depicted in photo of Morse with medals], set in diamonds, from Sultan Abdülmecid of Turkey (c.1847), a "golden snuff box containing the Prussian gold medal for scientific merit" from the King of Prussia (1851); the "Great Gold Medal of Arts and Sciences" from the King of Württemberg (1852); and the "Great Golden Medal of Science and Arts" from Emperor of Austria (1855); a cross of Chevalier in the Légion d'honneur from the Emperor of France; the "Cross of a Knight" of the Order of the Dannebrog from the King of Denmark (1856); the Cross of Knight Commander of the Order of Isabella the Catholic, from the Queen of Spain, besides being elected member of innumerable scientific and art societies in this [United States] and other countries. Other awards include Order of the Tower and Sword from the kingdom of Portugal (1860), and Italy conferred on him the insignia of chevalier of the Order of Saints Maurice and Lazarus in 1864. Morse's telegraph was recognized as an IEEE Milestone in 1988.

In 1975, Morse was inducted into the National Inventors Hall of Fame.

On April 1, 2012, Google announced the release of "Gmail Tap", an April Fools' Day joke that allowed users to use Morse Code to send text from their mobile phones. Morse's great-great-grandnephew Reed Morse—a Google engineer—was instrumental in the prank, which became a real product.







</doc>
<doc id="27008" url="https://en.wikipedia.org/wiki?curid=27008" title="Ship">
Ship

A ship is a large watercraft that travels the world's oceans and other sufficiently deep waterways, carrying passengers or goods, or in support of specialized missions, such as defense, research and fishing. Historically, a "ship" was a sailing vessel with at least three square-rigged masts and a full bowsprit. Ships are generally distinguished from boats, based on size, shape, load capacity, and tradition.

Ships have been important contributors to human migration and commerce. They have supported the spread of colonization and the slave trade, but have also served scientific, cultural, and humanitarian needs. After the 15th century, new crops that had come from and to the Americas via the European seafarers significantly contributed to the world population growth. Ship transport is responsible for the largest portion of world commerce.

As of 2016, there were more than 49,000 merchant ships, totaling almost 1.8 billion dead weight tons. Of these 28% were oil tankers, 43% were bulk carriers, and 13% were container ships. 

Ships are generally larger than boats, but there is no universally accepted distinction between the two. Ships generally can remain at sea for longer periods of time than boats. A legal definition of ship from Indian case law is a vessel that carries goods by sea. A common notion is that a ship can carry a boat, but not "vice versa". A US Navy rule of thumb is that ships heel towards the "outside" of a sharp turn, whereas boats heel towards the "inside" because of the relative location of the center of mass versus the center of buoyancy. American and British 19th Century maritime law distinguished "vessels" from other craft; ships and boats fall in one legal category, whereas open boats and rafts are not considered vessels.

In the Age of Sail, a full-rigged ship was a sailing vessel with at least three square-rigged masts and a full bowsprit; other types of vessel were also defined by their sailplan, e.g. barque, brigantine, etc.

A number of large vessels are usually referred to as boats. Submarines are a prime example. Other types of large vessel which are traditionally called boats are Great Lakes freighters, riverboats, and ferryboats. Though large enough to carry their own boats and heavy cargoes, these vessels are designed for operation on inland or protected coastal waters.

In most maritime traditions ships have individual names, and modern ships may belong to a ship class often named after its first ship. In the northern parts of Europe and America a ship is traditionally referred to with a female grammatical gender, represented in English with the pronoun "she", even if named after a man. This is not universal usage and some English language journalistic style guides advise using "it" as referring to ships with female pronouns can be seen as offensive and outdated. In many documents the ship name is introduced with a ship prefix being an abbreviation of the ship class, for example "MS" (motor ship) or "SV" (sailing vessel), making it easier to distinguish a ship name from other individual names in a text.

The first known vessels date back about 10,000 years ago, but could not be described as ships. The first navigators began to use animal skins or woven fabrics as sails. Affixed to the top of a pole set upright in a boat, these sails gave early ships range. This allowed men to explore widely, allowing for the settlement of Oceania for example (about 3,000 years ago).

By around 3000 BC, Ancient Egyptians knew how to assemble wooden planks into a hull. They used woven straps to lash the planks together, and reeds or grass stuffed between the planks helped to seal the seams. The Greek historian and geographer Agatharchides had documented ship-faring among the early Egyptians: ""During the prosperous period of the Old Kingdom, between the 30th and 25th centuries B. C., the river-routes were kept in order, and Egyptian ships sailed the Red Sea as far as the myrrh-country."" Sneferu's ancient cedar wood ship Praise of the Two Lands is the first reference recorded (2613 BC) to a ship being referred to by name.

The ancient Egyptians were perfectly at ease building sailboats. A remarkable example of their shipbuilding skills was the Khufu ship, a vessel in length entombed at the foot of the Great Pyramid of Giza around 2500 BC and found intact in 1954.

It is known that ancient Nubia/Axum traded with India, and there is evidence that ships from Northeast Africa may have sailed back and forth between India/Sri Lanka and Nubia trading goods and even to Persia, Himyar and Rome. Aksum was known by the Greeks for having seaports for ships from Greece and Yemen.

Elsewhere in Northeast Africa, the Periplus of the Red Sea reports that Somalis, through their northern ports such as Zeila and Berbera, were trading frankincense and other items with the inhabitants of the Arabian Peninsula well before the arrival of Islam as well as with then Roman-controlled Egypt.

A panel found at Mohenjodaro depicted a sailing craft. Vessels were of many types; their construction is vividly described in the Yukti Kalpa Taru, an ancient Indian text on shipbuilding. This treatise gives a technical exposition on the techniques of shipbuilding. It sets forth minute details about the various types of ships, their sizes, and the materials from which they were built. The Yukti Kalpa Taru sums up in a condensed form all the available information. The Yukti Kalpa Taru gives sufficient information and dates to prove that, in ancient times, Indian shipbuilders had a good knowledge of the materials which were used in building ships. In addition to describing the qualities of the different types of wood and their suitability for shipbuilding, the Yukti Kalpa Taru gives an elaborate classification of ships based on their size.

The oldest discovered sea faring hulled boat is the Late Bronze Age Uluburun shipwreck off the coast of Turkey, dating back to 1300 BC.

The Phoenicians, the first to sail completely around Africa, and Greeks gradually mastered navigation at sea aboard triremes, exploring and colonizing the Mediterranean via ship. Around 340 BC, the Greek navigator Pytheas of Massalia ventured from Greece to Western Europe and Great Britain. In the course of the 2nd century BC, Rome went on to destroy Carthage and subdue the Hellenistic kingdoms of the eastern Mediterranean, achieving complete mastery of the inland sea, that they called "Mare Nostrum". The monsoon wind system of the Indian Ocean was first sailed by Greek navigator Eudoxus of Cyzicus in 118 BC.

In China, miniature models of ships that feature steering oars have been dated to the Warring States period (c. 475–221 BC). By the Han dynasty, a well kept naval fleet was an integral part of the military. Sternpost-mounted rudders started to appear on Chinese ship models starting in the 1st century AD. Ship technology advanced to the point where by the medieval period, water tight compartments were developed.

In the 1st century A.D., the people from Nusantaran archipelago already made large ships over 50 m long and stood out 4-7 m out of the water. They could carry 700-1000 people and 260 ton cargo. These ships known as K'un-lun po (ships of Southern country) by the Chinese or kolandiaphonta by the Greeks. It has 4-7 masts and able to sail against the wind due to the usage of tanja sails. These ships reaching as far as Ghana.

The Swahili people had various extensive trading ports dotting the coast of medieval East Africa and Great Zimbabwe had extensive trading contacts with Central Africa, and likely also imported goods brought to Africa through the Southeast African shore trade of Kilwa in modern-day Tanzania.

It is known by historians that at its height the Mali Empire built a large naval fleet under Emperor Mansa Musa in the late 13th and early 14th century. Arabic sources describe what some consider to be visits to the New World by a Mali fleet in 1311.

Before the introduction of the compass, celestial navigation was the main method for navigation at sea. In China, early versions of the magnetic compass were being developed and used in navigation between 1040 and 1117. The true mariner's compass, using a pivoting needle in a dry box, was developed in Europe no later than 1300.

Until the Renaissance, navigational technology remained comparatively primitive. This absence of technology did not prevent some civilizations from becoming sea powers. Examples include the maritime republics of Genoa and Venice, Hanseatic League, and the Byzantine navy. The Vikings used their knarrs to explore North America, trade in the Baltic Sea and plunder many of the coastal regions of Western Europe.

Towards the end of the 14th century, ships like the carrack began to develop towers on the bow and stern. These towers decreased the vessel's stability, and in the 15th century, the caravel, designed by the Portuguese, based on the Arabic "qarib" which could sail closer to the wind, became more widely used. The towers were gradually replaced by the forecastle and sterncastle, as in the carrack "Santa María" of Christopher Columbus. This increased freeboard allowed another innovation: the freeing port, and the artillery associated with it.

In the 16th century, the use of freeboard and freeing ports became widespread on galleons.

At this time, ships were developing in Asia in much the same way as Europe. Japan used defensive naval techniques in the Mongol invasions of Japan in 1281. It is likely that the Mongols of the time took advantage of both European and Asian shipbuilding techniques. During the 15th century, China's Ming dynasty assembled one of the largest and most powerful naval fleets in the world for the diplomatic and power projection voyages of Zheng He. Elsewhere in Japan in the 15th century, one of the world's first iron-clads, "Tekkōsen" (), literally meaning "iron ships", was also developed. In Japan, during the Sengoku era from the fifteenth to 17th century, the great struggle for feudal supremacy was fought, in part, by coastal fleets of several hundred boats, including the atakebune. In Korea, in the early 15th century during the Joseon era, "Geobukseon"(거북선), was developed. The "turtle ship", as it was called is recognized as the first armored ship in the world.

During the Age of the Ajuran, the Somali sultanates and republics of Merca, Mogadishu, Barawa, Hobyo and their respective ports flourished, enjoying a lucrative foreign commerce with ships sailing to and coming from Arabia, India, Venetia, Persia, Egypt, Portugal and as far away as China. In the 16th century, Duarte Barbosa noted that many ships from the Kingdom of Cambaya in what is modern-day India sailed to Mogadishu with cloth and spices, for which they in return received gold, wax and ivory. Barbosa also highlighted the abundance of meat, wheat, barley, horses, and fruit on the coastal markets, which generated enormous wealth for the merchants.

Middle Age Swahili Kingdoms are known to have had trade port bullship and trade routes with the Islamic world and Asia and were described by Greek historians as "metropolises". Famous African trade ports such as Mombasa, Zanzibar, and Kilwa were known to Chinese sailors such as Zheng He and medieval Islamic historians such as the Berber Islamic voyager Abu Abdullah ibn Battua. In the 14th century AD, King Abubakari I, the brother of King Mansa Musa of the Mali Empire, is thought to have had a great armada of ships sitting on the coast of West Africa. This is corroborated by ibn Battuta himself who recalls several hundred Malian ships off the coast. This has led to great speculation, with historical evidence, that it is possible that Malian sailors may have reached the coast of Pre-Columbian America under the rule of Abubakari II, nearly two hundred years before Christopher Columbus and that black traders may have been in the Americas before Columbus.
Fifty years before Christopher Columbus, Chinese navigator Zheng He traveled the world at the head of what was for the time a huge armada. The largest of his ships had nine masts, were long and had a beam of . His fleet carried 30,000 men aboard 70 vessels, with the goal of bringing glory to the Chinese emperor.

At the same time Zheng He made his expedition, Portuguese explorer Gil Eanes sailed on a square-rigged caravel beyond Cape Bojador the end of what was then considered the known world opening the route to deep sea exploration, continental sea communication technology and the spherical earth principle.

The carrack and then the caravel were developed in Portugal. After Columbus, European exploration rapidly accelerated, and many new trade routes were established. In 1498, by reaching India, Vasco da Gama proved that the access to the Indian Ocean from the Atlantic was possible. These explorations in the Atlantic and Indian Oceans were soon followed by France, England and the Netherlands, who explored the Portuguese and Spanish trade routes into the Pacific Ocean, reaching Australia in 1606 and New Zealand in 1642. In the 17th century Dutch and Spanish explorers such as Abel Tasman and Luís Vaz de Torres explored the coasts of Australia, while in the 18th century it was British explorer James Cook who mapped much of Polynesia.

Parallel to the development of warships, ships in service of marine fishery and trade also developed in the period between antiquity and the Renaissance.

Maritime trade was driven by the development of shipping companies with significant financial resources. Canal barges, towed by draft animals on an adjacent towpath, contended with the railway up to and past the early days of the industrial revolution. Flat-bottomed and flexible scow boats also became widely used for transporting small cargoes. Mercantile trade went hand-in-hand with exploration, self-financed by the commercial benefits of exploration.

During the first half of the 18th century, the French Navy began to develop a new type of vessel known as a ship of the line, featuring seventy-four guns. This type of ship became the backbone of all European fighting fleets. These ships were long and their construction required 2,800 oak trees and of rope; they carried a crew of about 800 sailors and soldiers.

During the 19th century the Royal Navy enforced a ban on the slave trade, acted to suppress piracy, and continued to map the world. A clipper was a very fast sailing ship of the 19th century. The clipper routes fell into commercial disuse with the introduction of steam ships with better fuel efficiency, and the opening of the Suez and Panama Canals.

Ship designs stayed fairly unchanged until the late 19th century. The industrial revolution, new mechanical methods of propulsion, and the ability to construct ships from metal triggered an explosion in ship design. Factors including the quest for more efficient ships, the end of long running and wasteful maritime conflicts, and the increased financial capacity of industrial powers created an avalanche of more specialized boats and ships. Ships built for entirely new functions, such as firefighting, rescue, and research, also began to appear.

In light of this, classification of vessels by type or function can be difficult. Even using very broad functional classifications such as fishery, trade, military, and exploration fails to classify most of the old ships. This difficulty is increased by the fact that the terms such as sloop and frigate are used by old and new ships alike, and often the modern vessels sometimes have little in common with their predecessors.

In 2007, the world's fleet included 34,882 commercial vessels with gross tonnage of more than 1,000 tons, totaling 1.04 billion tons. These ships carried 7.4 billion tons of cargo in 2006, a sum that grew by 8% over the previous year. In terms of tonnage, 39% of these ships are tankers, 26% are bulk carriers, 17% container ships and 15% were other types.

In 2002, there were 1,240 warships operating in the world, not counting small vessels such as patrol boats. The United States accounted for 3 million tons worth of these vessels, Russia 1.35 million tons, the United Kingdom 504,660 tons and China 402,830 tons. The 20th century saw many naval engagements during the two world wars, the Cold War, and the rise to power of naval forces of the two blocs. The world's major powers have recently used their naval power in cases such as the United Kingdom in the Falkland Islands and the United States in Iraq.

The size of the world's fishing fleet is more difficult to estimate. The largest of these are counted as commercial vessels, but the smallest are legion. Fishing vessels can be found in most seaside villages in the world. As of 2004, the United Nations Food and Agriculture Organization estimated 4 million fishing vessels were operating worldwide. The same study estimated that the world's 29 million fishermen caught of fish and shellfish that year.

Because ships are constructed using the principles of naval architecture that require same structural components, their classification is based on their function such as that suggested by Paulet and Presles, which requires modification of the components. The categories accepted in general by naval architects are:

Some of these are discussed in the following sections.

Freshwater shipping may occur on lakes, rivers and canals. Ships designed for those venues may be specially adapted to the widths and depths of specific waterways. Examples of freshwater waterways that are navigable in part by large vessels include the Danube, Mississippi, Rhine, Yangtze and Amazon Rivers, and the Great Lakes.

Lake freighters, also called lakers, are cargo vessels that ply the Great Lakes. The most well-known is , the latest major vessel to be wrecked on the Lakes. These vessels are traditionally called boats, not ships. Visiting ocean-going vessels are called "salties." Because of their additional beam, very large salties are never seen inland of the Saint Lawrence Seaway. Because the smallest of the Soo Locks is larger than any Seaway lock, salties that can pass through the Seaway may travel anywhere in the Great Lakes. Because of their deeper draft, salties may accept partial loads on the Great Lakes, "topping off" when they have exited the Seaway. Similarly, the largest lakers are confined to the Upper Lakes (Superior, Michigan, Huron, Erie) because they are too large to use the Seaway locks, beginning at the Welland Canal that bypasses the Niagara River.

Since the freshwater lakes are less corrosive to ships than the salt water of the oceans, lakers tend to last much longer than ocean freighters. Lakers older than 50 years are not unusual, and as of 2005, all were over 20 years of age.

, built in 1906 as "William P Snyder", was the oldest laker still working on the Lakes until its conversion into a barge starting in 2013. Similarly, "E.M. Ford", built in 1898 as "Presque Isle", was sailing the lakes 98 years later in 1996. As of 2007 "E.M. Ford" was still afloat as a stationary transfer vessel at a riverside cement silo in Saginaw, Michigan.

Commercial vessels or merchant ships can be divided into four broad categories: fishing, cargo ships, passenger ships, and special-purpose ships. The UNCTAD review of maritime transport categorizes ships as: oil tankers, bulk (and combination) carriers, general cargo ships, container ships, and "other ships", which includes "liquefied petroleum gas carriers, liquefied natural gas carriers, parcel (chemical) tankers, specialized tankers, reefers, offshore supply, tugs, dredgers, cruise, ferries, other non-cargo". General cargo ships include "multi-purpose and project vessels and roll-on/roll-off cargo".

Modern commercial vessels are typically powered by a single propeller driven by a diesel or, less usually, gas turbine engine., but until the mid-19th century they were predominantly square sail rigged. The fastest vessels may use pump-jet engines. Most commercial vessels have full hull-forms to maximize cargo capacity. Hulls are usually made of steel, although aluminum can be used on faster craft, and fiberglass on the smallest service vessels. Commercial vessels generally have a crew headed by a captain, with deck officers and marine engineers on larger vessels. Special-purpose vessels often have specialized crew if necessary, for example scientists aboard research vessels.

Fishing boats are generally small, often little more than but up to for a large tuna or whaling ship. Aboard a fish processing vessel, the catch can be made ready for market and sold more quickly once the ship makes port. Special purpose vessels have special gear. For example, trawlers have winches and arms, stern-trawlers have a rear ramp, and tuna seiners have skiffs. In 2004, of fish were caught in the marine capture fishery. Anchoveta represented the largest single catch at . That year, the top ten marine capture species also included Alaska pollock, Blue whiting, Skipjack tuna, Atlantic herring, Chub mackerel, Japanese anchovy, Chilean jack mackerel, Largehead hairtail, and Yellowfin tuna. Other species including salmon, shrimp, lobster, clams, squid and crab, are also commercially fished. Modern commercial fishermen use many methods. One is fishing by nets, such as purse seine, beach seine, lift nets, gillnets, or entangling nets. Another is trawling, including bottom trawl. Hooks and lines are used in methods like long-line fishing and hand-line fishing. Another method is the use of fishing trap.

Cargo ships transport dry and liquid cargo. Dry cargo can be transported in bulk by bulk carriers, packed directly onto a general cargo ship in break-bulk, packed in intermodal containers as aboard a container ship, or driven aboard as in roll-on roll-off ships. Liquid cargo is generally carried in bulk aboard tankers, such as oil tankers which may include both crude and finished products of oil, chemical tankers which may also carry vegetable oils other than chemicals and LPG/LNG tankers, although smaller shipments may be carried on container ships in tank containers.

Passenger ships range in size from small river ferries to very large cruise ships. This type of vessel includes ferries, which move passengers and vehicles on short trips; ocean liners, which carry passengers from one place to another; and cruise ships, which carry passengers on voyages undertaken for pleasure, visiting several places and with leisure activities on board, often returning them to the port of embarkation. Riverboats and inland ferries are specially designed to carry passengers, cargo, or both in the challenging river environment. Rivers present special hazards to vessels. They usually have varying water flows that alternately lead to high speed water flows or protruding rock hazards. Changing siltation patterns may cause the sudden appearance of shoal waters, and often floating or sunken logs and trees (called snags) can endanger the hulls and propulsion of riverboats. Riverboats are generally of shallow draft, being broad of beam and rather square in plan, with a low freeboard and high topsides. Riverboats can survive with this type of configuration as they do not have to withstand the high winds or large waves that are seen on large lakes, seas, or oceans.
Fishing vessels are a subset of commercial vessels, but generally small in size and often subject to different regulations and classification. They can be categorized by several criteria: architecture, the type of fish they catch, the fishing method used, geographical origin, and technical features such as rigging. As of 2004, the world's fishing fleet consisted of some 4 million vessels. Of these, 1.3 million were decked vessels with enclosed areas and the rest were open vessels. Most decked vessels were mechanized, but two-thirds of the open vessels were traditional craft propelled by sails and oars. More than 60% of all existing large fishing vessels were built in Japan, Peru, the Russian Federation, Spain or the United States of America.

A weather ship was a ship stationed in the ocean as a platform for surface and upper air meteorological observations for use in marine weather forecasting. Surface weather observations were taken hourly, and four radiosonde releases occurred daily. It was also meant to aid in search and rescue operations and to support transatlantic flights. Proposed as early as 1927 by the aviation community, the establishment of weather ships proved to be so useful during World War II that the International Civil Aviation Organization (ICAO) established a global network of weather ships in 1948, with 13 to be supplied by the United States. This number was eventually negotiated down to nine.

The weather ship crews were normally at sea for three weeks at a time, returning to port for 10-day stretches. Weather ship observations proved to be helpful in wind and wave studies, as they did not avoid weather systems like other ships tended to for safety reasons. They were also helpful in monitoring storms at sea, such as tropical cyclones. The removal of a weather ship became a negative factor in forecasts leading up to the Great Storm of 1987. Beginning in the 1970s, their role became largely superseded by weather buoys due to the ships' significant cost. The agreement of the use of weather ships by the international community ended in 1990. The last weather ship was "Polarfront", known as weather station M ("Mike"), which was put out of operation on 1 January 2010. Weather observations from ships continue from a fleet of voluntary merchant vessels in routine commercial operation.

Naval vessels are those used by a navy for military purposes.
There have been many types of naval vessel. Modern naval vessels can be broken down into three categories: surface warships, submarines, and support and auxiliary vessels.

Modern warships are generally divided into seven main categories: aircraft carriers, cruisers, destroyers, frigates, corvettes, submarines and amphibious assault ships. The distinction between cruisers, destroyers, frigates, and corvettes is not rigorous; the same vessel may be described differently in different navies. Battleships were used during the Second World War and occasionally since then (the last battleships were removed from the U.S. Naval Vessel Register in March 2006), but were made obsolete by the use of carrier-borne aircraft and guided missiles.

Most military submarines are either attack submarines or ballistic missile submarines. Until the end of World War II the primary role of the diesel/electric submarine was anti-ship warfare, inserting and removing covert agents and military forces, and intelligence-gathering. With the development of the homing torpedo, better sonar systems, and nuclear propulsion, submarines also became able to effectively hunt each other. The development of submarine-launched nuclear and cruise missiles gave submarines a substantial and long-ranged ability to attack both land and sea targets with a variety of weapons ranging from cluster munitions to nuclear weapons.

Most navies also include many types of support and auxiliary vessel, such as minesweepers, patrol boats, offshore patrol vessels, replenishment ships, and hospital ships which are designated medical treatment facilities.

Fast combat vessels such as cruisers and destroyers usually have fine hulls to maximize speed and maneuverability. They also usually have advanced marine electronics and communication systems, as well as weapons.

Some components exist in vessels of any size and purpose. Every vessel has a hull of sorts. Every vessel has some sort of propulsion, whether it's a pole, an ox, or a nuclear reactor. Most vessels have some sort of steering system. Other characteristics are common, but not as universal, such as compartments, holds, a superstructure, and equipment such as anchors and winches.

For a ship to float, its weight must be less than that of the water displaced by the ship's hull. There are many types of hulls, from logs lashed together to form a raft to the advanced hulls of America's Cup sailboats. A vessel may have a single hull (called a monohull design), two in the case of catamarans, or three in the case of trimarans. Vessels with more than three hulls are rare, but some experiments have been conducted with designs such as pentamarans. Multiple hulls are generally parallel to each other and connected by rigid arms.

Hulls have several elements. The bow is the foremost part of the hull. Many ships feature a bulbous bow. The keel is at the very bottom of the hull, extending the entire length of the ship. The rear part of the hull is known as the stern, and many hulls have a flat back known as a transom. Common hull appendages include propellers for propulsion, rudders for steering, and stabilizers to quell a ship's rolling motion. Other hull features can be related to the vessel's work, such as fishing gear and sonar domes.

Hulls are subject to various hydrostatic and hydrodynamic constraints. The key hydrostatic constraint is that it must be able to support the entire weight of the boat, and maintain stability even with often unevenly distributed weight. Hydrodynamic constraints include the ability to withstand shock waves, weather collisions and groundings.

Older ships and pleasure craft often have or had wooden hulls. Steel is used for most commercial vessels. Aluminium is frequently used for fast vessels, and composite materials are often found in sailboats and pleasure craft. Some ships have been made with concrete hulls.

Propulsion systems for ships fall into three categories: human propulsion, sailing, and mechanical propulsion. Human propulsion includes rowing, which was used even on large galleys. Propulsion by sail generally consists of a sail hoisted on an erect mast, supported by stays and spars and controlled by ropes. Sail systems were the dominant form of propulsion until the 19th century. They are now generally used for recreation and competition, although experimental sail systems, such as the turbosails, rotorsails, and wingsails have been used on larger modern vessels for fuel savings.

Mechanical propulsion systems generally consist of a motor or engine turning a propeller, or less frequently, an impeller or wave propulsion fins. Steam engines were first used for this purpose, but have mostly been replaced by two-stroke or four-stroke diesel engines, outboard motors, and gas turbine engines on faster ships. Nuclear reactors producing steam are used to propel warships and icebreakers, and there have been attempts to utilize them to power commercial vessels (see NS "Savannah").

In addition to traditional fixed and controllable pitch propellers there are many specialized variations, such as contra-rotating and nozzle-style propellers. Most vessels have a single propeller, but some large vessels may have up to four propellers supplemented with transverse thrusters for maneuvring at ports. The propeller is connected to the main engine via a propeller shaft and, in case of medium- and high-speed engines, a reduction gearbox. Some modern vessels have a diesel-electric powertrain in which the propeller is turned by an electric motor powered by the ship's generators.

For ships with independent propulsion systems for each side, such as manual oars or some paddles, steering systems may not be necessary. In most designs, such as boats propelled by engines or sails, a steering system becomes necessary. The most common is a rudder, a submerged plane located at the rear of the hull. Rudders are rotated to generate a lateral force which turns the boat. Rudders can be rotated by a tiller, manual wheels, or electro-hydraulic systems. Autopilot systems combine mechanical rudders with navigation systems. Ducted propellers are sometimes used for steering.

Some propulsion systems are inherently steering systems. Examples include the outboard motor, the bow thruster, and the Z-drive.

Larger boats and ships generally have multiple decks and compartments. Separate berthings and heads are found on sailboats over about . Fishing boats and cargo ships typically have one or more cargo holds. Most larger vessels have an engine room, a galley, and various compartments for work. Tanks are used to store fuel, engine oil, and fresh water. Ballast tanks are equipped to change a ship's trim and modify its stability.

Superstructures are found above the main deck. On sailboats, these are usually very low. On modern cargo ships, they are almost always located near the ship's stern. On passenger ships and warships, the superstructure generally extends far forward.

Shipboard equipment varies from ship to ship depending on such factors as the ship's era, design, area of operation, and purpose. Some types of equipment that are widely found include:

Ships float in the water at a level where mass of the displaced water equals the mass of the vessel, such that the downwards force of gravity equals the upward force of buoyancy. As a vessel is lowered into the water its weight remains constant but the corresponding weight of water displaced by its hull increases. If the vessel's mass is evenly distributed throughout, it floats evenly along its length and across its beam (width). A vessel's stability is considered in both this hydrostatic sense as well as a hydrodynamic sense, when subjected to movement, rolling and pitching, and the action of waves and wind. Stability problems can lead to excessive pitching and rolling, and eventually capsizing and sinking.

The advance of a vessel through water is resisted by the water. This resistance can be broken down into several components, the main ones being the friction of the water on the hull and wave making resistance. To reduce resistance and therefore increase the speed for a given power, it is necessary to reduce the wetted surface and use submerged hull shapes that produce low amplitude waves. To do so, high-speed vessels are often more slender, with fewer or smaller appendages. The friction of the water is also reduced by regular maintenance of the hull to remove the sea creatures and algae that accumulate there. Antifouling paint is commonly used to assist in this. Advanced designs such as the bulbous bow assist in decreasing wave resistance.

A simple way of considering wave-making resistance is to look at the hull in relation to its wake. At speeds lower than the wave propagation speed, the wave rapidly dissipates to the sides. As the hull approaches the wave propagation speed, however, the wake at the bow begins to build up faster than it can dissipate, and so it grows in amplitude. Since the water is not able to "get out of the way of the hull fast enough", the hull, in essence, has to climb over or push through the bow wave. This results in an exponential increase in resistance with increasing speed.

This hull speed is found by the formula:

formula_1

or, in metric units:

formula_2

where "L" is the length of the waterline in feet or meters.

When the vessel exceeds a speed/length ratio of 0.94, it starts to outrun most of its bow wave, and the hull actually settles slightly in the water as it is now only supported by two wave peaks. As the vessel exceeds a speed/length ratio of 1.34, the hull speed, the wavelength is now longer than the hull, and the stern is no longer supported by the wake, causing the stern to squat, and the bow rise. The hull is now starting to climb its own bow wave, and resistance begins to increase at a very high rate. While it is possible to drive a displacement hull faster than a speed/length ratio of 1.34, it is prohibitively expensive to do so. Most large vessels operate at speed/length ratios well below that level, at speed/length ratios of under 1.0.

For large projects with adequate funding, hydrodynamic resistance can be tested experimentally in a hull testing pool or using tools of computational fluid dynamics.

Vessels are also subject to ocean surface waves and sea swell as well as effects of wind and weather. These movements can be stressful for passengers and equipment, and must be controlled if possible. The rolling movement can be controlled, to an extent, by ballasting or by devices such as fin stabilizers. Pitching movement is more difficult to limit and can be dangerous if the bow submerges in the waves, a phenomenon called pounding. Sometimes, ships must change course or speed to stop violent rolling or pitching.

How it has been convincingly shown in scientific studies of the 21st century, controllability of some vessels decreases dramatically in some cases that are conditioned by effects of the bifurcation memory. This class of vessels includes ships with high manoeuvring capabilities, aircraft and controlled underwater vehicles designed to be unstable in steady-state motion that are interesting in terms of applications. These features must be considered in designing ships and in their control in critical situations.

A ship will pass through several stages during its career. The first is usually an initial contract to build the ship, the details of which can vary widely based on relationships between the shipowners, operators, designers and the shipyard. Then, the design phase carried out by a naval architect. Then the ship is constructed in a shipyard. After construction, the vessel is launched and goes into service. Ships end their careers in a number of ways, ranging from shipwrecks to service as a museum ship to the scrapyard.

A vessel's design starts with a specification, which a naval architect uses to create a project outline, assess required dimensions, and create a basic layout of spaces and a rough displacement. After this initial rough draft, the architect can create an initial hull design, a general profile and an initial overview of the ship's propulsion. At this stage, the designer can iterate on the ship's design, adding detail and refining the design at each stage.

The designer will typically produce an overall plan, a general specification describing the peculiarities of the vessel, and construction blueprints to be used at the building site. Designs for larger or more complex vessels may also include sail plans, electrical schematics, and plumbing and ventilation plans.

As environmental laws are becoming more strict, ship designers need to create their design in such a way that the ship, when it nears its end-of-term, can be disassembled or disposed easily and that waste is reduced to a minimum.

Ship construction takes place in a shipyard, and can last from a few months for a unit produced in series, to several years to reconstruct a wooden boat like the frigate "Hermione", to more than 10 years for an aircraft carrier. During World War II, the need for cargo ships was so urgent that construction time for Liberty Ships went from initially eight months or longer, down to weeks or even days. Builders employed production line and prefabrication techniques such as those used in shipyards today.

Hull materials and vessel size play a large part in determining the method of construction. The hull of a mass-produced fiberglass sailboat is constructed from a mold, while the steel hull of a cargo ship is made from large sections welded together as they are built.

Generally, construction starts with the hull, and on vessels over about , by the laying of the keel. This is done in a drydock or on land. Once the hull is assembled and painted, it is launched. The last stages, such as raising the superstructure and adding equipment and accommodation, can be done after the vessel is afloat.

Once completed, the vessel is delivered to the customer. Ship launching is often a ceremony of some significance, and is usually when the vessel is formally named. A typical small rowboat can cost under US$100, $1,000 for a small speedboat, tens of thousands of dollars for a cruising sailboat, and about $2,000,000 for a Vendée Globe class sailboat. A trawler may cost $2.5 million, and a 1,000-person-capacity high-speed passenger ferry can cost in the neighborhood of $50 million. A ship's cost partly depends on its complexity: a small, general cargo ship will cost $20 million, a Panamax-sized bulk carrier around $35 million, a supertanker around $105 million and a large LNG carrier nearly $200 million. The most expensive ships generally are so because of the cost of embedded electronics: a costs around $2 billion, and an aircraft carrier goes for about $3.5 billion.

Ships undergo nearly constant maintenance during their career, whether they be underway, pierside, or in some cases, in periods of reduced operating status between charters or shipping seasons.

Most ships, however, require trips to special facilities such as a drydock at regular intervals. Tasks often done at drydock include removing biological growths on the hull, sandblasting and repainting the hull, and replacing sacrificial anodes used to protect submerged equipment from corrosion. Major repairs to the propulsion and steering systems as well as major electrical systems are also often performed at dry dock.

Some vessels that sustain major damage at sea may be repaired at a facility equipped for major repairs, such as a shipyard. Ships may also be converted for a new purpose: oil tankers are often converted into floating production storage and offloading units.

Most ocean-going cargo ships have a life expectancy of between 20 and 30 years. A sailboat made of plywood or fiberglass can last between 30 and 40 years. Solid wooden ships can last much longer but require regular maintenance. Carefully maintained steel-hulled yachts can have a lifespan of over 100 years.

As ships age, forces such as corrosion, osmosis, and rotting compromise hull strength, and a vessel becomes too dangerous to sail. At this point, it can be scuttled at sea or scrapped by shipbreakers. Ships can also be used as museum ships, or expended to construct breakwaters or artificial reefs.

Many ships do not make it to the scrapyard, and are lost in fires, collisions, grounding, or sinking at sea. The Allies lost some 5,150 ships during World War II.

One can measure ships in terms of overall length, length between perpendiculars, length of the ship at the waterline, beam (breadth), depth (distance between the crown of the weather deck and the top of the keelson), draft (distance between the highest waterline and the bottom of the ship) and tonnage. A number of different tonnage definitions exist and are used when describing merchant ships for the purpose of tolls, taxation, etc.

In Britain until Samuel Plimsoll's Merchant Shipping Act of 1876, ship-owners could load their vessels until their decks were almost awash, resulting in a dangerously unstable condition. Anyone who signed on to such a ship for a voyage and, upon realizing the danger, chose to leave the ship, could end up in jail. Plimsoll, a Member of Parliament, realised the problem and engaged some engineers to derive a fairly simple formula to determine the position of a line on the side of any specific ship's hull which, when it reached the surface of the water during loading of cargo, meant the ship had reached its maximum safe loading level. To this day, that mark, called the "Plimsoll Line", exists on ships' sides, and consists of a circle with a horizontal line through the centre. On the Great Lakes of North America the circle is replaced with a diamond. Because different types of water (summer, fresh, tropical fresh, winter north Atlantic) have different densities, subsequent regulations required painting a group of lines forward of the Plimsoll mark to indicate the safe depth (or freeboard above the surface) to which a specific ship could load in water of various densities. Hence the "ladder" of lines seen forward of the Plimsoll mark to this day. This is called the "freeboard mark" or "load line mark" in the marine industry.

Ship pollution is the pollution of air and water by shipping. It is a problem that has been accelerating as trade has become increasingly globalized, posing an increasing threat to the world’s oceans and waterways as globalization continues. It is expected that, “...shipping traffic to and from the United States is projected to double by 2020." Because of increased traffic in ocean ports, pollution from ships also directly affects coastal areas. The pollution produced affects biodiversity, climate, food, and human health. However, the degree to which humans are polluting and how it affects the world is highly debated and has been a hot international topic for the past 30 years.

Oil spills have devastating effects on the environment. Crude oil contains polycyclic aromatic hydrocarbons (PAHs) which are very difficult to clean up, and last for years in the sediment and marine environment. Marine species constantly exposed to PAHs can exhibit developmental problems, susceptibility to disease, and abnormal reproductive cycles.

By the sheer amount of oil carried, modern oil tankers must be considered something of a threat to the environment. An oil tanker can carry of crude oil, or . This is more than six times the amount spilled in the widely known "Exxon Valdez" incident. In this spill, the ship ran aground and dumped of oil into the ocean in March 1989. Despite efforts of scientists, managers, and volunteers, over 400,000 seabirds, about 1,000 sea otters, and immense numbers of fish were killed.

The International Tanker Owners Pollution Federation has researched 9,351 accidental spills since 1974. According to this study, most spills result from routine operations such as loading cargo, discharging cargo, and taking on fuel oil. 91% of the operational oil spills were small, resulting in less than 7 tons per spill. Spills resulting from accidents like collisions, groundings, hull failures, and explosions are much larger, with 84% of these involving losses of over 700 tons.

Following the "Exxon Valdez" spill, the United States passed the Oil Pollution Act of 1990 (OPA-90), which included a stipulation that all tankers entering its waters be double-hulled by 2015. Following the sinkings of "Erika" (1999) and "Prestige" (2002), the European Union passed its own stringent anti-pollution packages (known as Erika I, II, and III), which require all tankers entering its waters to be double-hulled by 2010. The Erika packages are controversial because they introduced the new legal concept of "serious negligence".

When a large vessel such as a container ship or an oil tanker unloads cargo, seawater is pumped into other compartments in the hull to help stabilize and balance the ship. During loading, this ballast water is pumped out from these compartments.

One of the problems with ballast water transfer is the transport of harmful organisms. Meinesz believes that one of the worst cases of a single invasive species causing harm to an ecosystem can be attributed to a seemingly harmless jellyfish. "Mnemiopsis leidyi", a species of comb jellyfish that inhabits estuaries from the United States to the Valdés peninsula in Argentina along the Atlantic coast, has caused notable damage in the Black Sea. It was first introduced in 1982, and thought to have been transported to the Black Sea in a ship’s ballast water. The population of the jellyfish shot up exponentially and, by 1988, it was wreaking havoc upon the local fishing industry. "The anchovy catch fell from in 1984 to in 1993; sprat from in 1984 to in 1993; horse mackerel from in 1984 to zero in 1993." Now that the jellyfish have exhausted the zooplankton, including fish larvae, their numbers have fallen dramatically, yet they continue to maintain a stranglehold on the ecosystem. Recently the jellyfish have been discovered in the Caspian Sea. Invasive species can take over once occupied areas, facilitate the spread of new diseases, introduce new genetic material, alter landscapes and jeopardize the ability of native species to obtain food. "On land and in the sea, invasive species are responsible for about 137 billion dollars in lost revenue and management costs in the U.S. each year."

Ballast and bilge discharge from ships can also spread human pathogens and other harmful diseases and toxins potentially causing health issues for humans and marine life alike. Discharges into coastal waters, along with other sources of marine pollution, have the potential to be toxic to marine plants, animals, and microorganisms, causing alterations such as changes in growth, disruption of hormone cycles, birth defects, suppression of the immune system, and disorders resulting in cancer, tumors, and genetic abnormalities or even death.

Exhaust emissions from ships are considered to be a significant source of air pollution. “Seagoing vessels are responsible for an estimated 14 percent of emissions of nitrogen from fossil fuels and 16 percent of the emissions of sulfur from petroleum uses into the atmosphere.” In Europe ships make up a large percentage of the sulfur introduced to the air, “...as much sulfur as all the cars, lorries and factories in Europe put together.” “By 2010, up to 40% of air pollution over land could come from ships.” Sulfur in the air creates acid rain which damages crops and buildings. When inhaled, sulfur is known to cause respiratory problems and increase the risk of a heart attack.

Ship breaking or ship demolition is a type of ship disposal involving the breaking up of ships for scrap recycling, with the hulls being discarded in ship graveyards. Most ships have a lifespan of a few decades before there is so much wear that refitting and repair becomes uneconomical. Ship breaking allows materials from the ship, especially steel, to be reused.
In addition to steel and other useful materials, however, ships (particularly older vessels) can contain many substances that are banned or considered dangerous in developed countries. Asbestos and polychlorinated biphenyls (PCBs) are typical examples. Asbestos was used heavily in ship construction until it was finally banned in most of the developed world in the mid 1980s. Currently, the costs associated with removing asbestos, along with the potentially expensive insurance and health risks, have meant that ship-breaking in most developed countries is no longer economically viable. Removing the metal for scrap can potentially cost more than the scrap value of the metal itself. In most of the developing world, however, shipyards can operate without the risk of personal injury lawsuits or workers' health claims, meaning many of these shipyards may operate with high health risks. Furthermore, workers are paid very low rates with no overtime or other allowances. Protective equipment is sometimes absent or inadequate. Dangerous vapors and fumes from burning materials can be inhaled, and dusty asbestos-laden areas around such breakdown locations are commonplace.

Aside from the health of the yard workers, in recent years, ship breaking has also become an issue of major environmental concern. Many developing nations, in which ship breaking yards are located, have lax or no environmental law, enabling large quantities of highly toxic materials to escape into the environment and causing serious health problems among ship breakers, the local population and wildlife. Environmental campaign groups such as Greenpeace have made the issue a high priority for their campaigns.


Model ships

Lists

Ship sizes




</doc>
<doc id="27009" url="https://en.wikipedia.org/wiki?curid=27009" title="Soap opera">
Soap opera

A soap opera is an ongoing drama serial on television or radio, featuring the lives of many characters and their emotional relationships. The term originated from radio dramas being sponsored by soap manufacturers.

BBC Radio's "The Archers", first broadcast in 1950, is the world’s longest-running radio soap opera; the world's longest-running television soap opera is "Coronation Street", first broadcast on ITV in 1960.

The first serial considered to be a "soap opera" was "Painted Dreams", which debuted on October 20, 1930 on Chicago radio station WGN. Early radio series such as "Painted Dreams" were broadcast in weekday daytime slots, usually five days a week, when most of the listeners would be housewives; thus, the shows were aimed at and consumed by a predominantly female audience. The first nationally broadcast radio soap opera was "Clara, Lu, and Em", which aired on the NBC Blue Network at 10:30 p.m. Eastern Time on January 27, 1931.

One of the defining features that makes a television program a soap opera, according to Albert Moran, is "that form of television that works with a continuous open narrative. While Spanish language telenovelas are sometimes called "soap operas," telenovelas have conflicts that get resolved and a definite ending after (more or less) a year of daily weekday airing. But with soap operas each episode ends with a promise that the storyline is to be continued in another episode". In 2012, "Los Angeles Times" columnist Robert Lloyd wrote of daily dramas, "Although melodramatically eventful, soap operas such as this also have a luxury of space that makes them seem more naturalistic; indeed, the economics of the form demand long scenes, and conversations that a 22-episodes-per-season weekly series might dispense with in half a dozen lines of dialogue may be drawn out, as here, for pages. You spend more time even with the minor characters; the apparent villains grow less apparently villainous."

Soap opera storylines run concurrently, intersect and lead into further developments. An individual episode of a soap opera will generally switch between several different concurrent narrative threads that may at times interconnect and affect one another or may run entirely independent to each other. Each episode may feature some of the show's current storylines, but not always all of them. Especially in daytime serials and those that are broadcast each weekday, there is some rotation of both storyline and actors so any given storyline or actor will appear in some but usually not all of a week's worth of episodes. Soap operas rarely bring all the current storylines to a conclusion at the same time. When one storyline ends, there are several other story threads at differing stages of development. Soap opera episodes typically end on some sort of cliffhanger, and the season finale (if a soap incorporates a break between seasons) ends in the same way, only to be resolved when the show returns for the start of a new yearly broadcast.

Evening soap operas and those that air at a rate of one episode per week are more likely to feature the entire cast in each episode, and to represent all current storylines in each episode. Evening soap operas and serials that run for only part of the year tend to bring things to a dramatic end-of-season cliffhanger.

In 1976, "Time" magazine described American daytime television as "TV's richest market," noting the loyalty of the soap opera fan base and the expansion of several half-hour series into hour-long broadcasts in order to maximize ad revenues. The article explained that at that time, many prime time series lost money, while daytime serials earned profits several times more than their production costs. The issue's cover notably featured its first daytime soap stars, Bill Hayes and Susan Seaforth Hayes of "Days of Our Lives", a married couple whose onscreen and real-life romance was widely covered by both the soap opera magazines and the mainstream press at large.

The main characteristics that define soap operas are "an emphasis on family life, personal relationships, sexual dramas, emotional and moral conflicts; some coverage of topical issues; set in familiar domestic interiors with only occasional excursions into new locations". Fitting in with these characteristics, most soap operas follow the lives of a group of characters who live or work in a particular place, or focus on a large extended family. The storylines follow the day-to-day activities and personal relationships of these characters. "Soap narratives, like those of film melodramas, are marked by what Steve Neale has described as 'chance happenings, coincidences, missed meetings, sudden conversions, last-minute rescues and revelations, deus ex machina endings.'" These elements may be found across the gamut of soap operas, from "EastEnders" to "Dallas". Due to the prominence of English-language television, most soap-operas are completely English (or in the case of a foreign soap opera, dubbed into English). However, several South African soap operas started incorporating a multi-language format, the most prominent being 7de Laan, which incorporates Afrikaans, English, Zulu and several other Bantu languages which make up the 11 Official Languages of South Africa (the subtitles are always in English).

In many soap operas, in particular daytime serials in the US, the characters are frequently attractive, seductive, glamorous and wealthy. Soap operas from the United Kingdom and Australia tend to focus on more everyday characters and situations, and are frequently set in working class environments. Many of the soaps produced in those two countries explore social realist storylines such as family discord, marriage breakdown or financial problems. Both UK and Australian soap operas feature comedic elements, often affectionate comic stereotypes such as the gossip or the grumpy old man, presented as a comic foil to the emotional turmoil that surrounds them. This diverges from US soap operas where such comedy is rare. UK soap operas frequently make a claim to presenting "reality" or purport to have a "realistic" style. UK soap operas also frequently foreground their geographic location as a key defining feature of the show while depicting and capitalising on the exotic appeal of the stereotypes connected to the location. As examples, "EastEnders" focuses on the tough and grim life in the east end of London; "Coronation Street" and its characters exhibit the stereotypical characteristic of "northern straight talking".

Romance, secret relationships, extramarital affairs, and genuine hate have been the basis for many soap opera storylines. In US daytime serials, the most popular soap opera characters, and the most popular storylines, often involved a romance of the sort presented in paperback romance novels. Soap opera storylines sometimes weave intricate, convoluted and sometimes confusing tales of characters who have affairs, meet mysterious strangers and fall in love, and who commit adultery, all of which keeps audiences hooked on the unfolding story. Crimes such as kidnapping, rape, and even murder may go unpunished if the perpetrator is to be retained in the ongoing story.

Australian and UK soap operas also feature a significant proportion of romance storylines. In Russia, most popular serials explore the "romantic quality" of criminal and/or oligarch life.

In soap opera storylines, previously unknown children, siblings and twins (including the evil variety) of established characters often emerge to upset and reinvigorate the set of relationships examined by the series. Unexpected calamities disrupt weddings, childbirths, and other major life events with unusual frequency.

As in comic books – another popular form of linear storytelling pioneered in the US during the 20th century – a character's death is not guaranteed to be permanent. On "The Bold and the Beautiful", Taylor Forrester (Hunter Tylo) was shown to flatline and have a funeral. When Tylo reprised the character in 2005, a retcon explained that Taylor had actually gone into a coma.

Stunts and complex physical action are largely absent, especially from daytime serials. Such story events often take place off screen and are referred to in dialogue instead of being shown. This is because stunts or action scenes are difficult to adequately depict visually without complex action, multiple takes, and post production editing. When episodes were broadcast live, post production work was impossible. Though all serials have long switched to being taped, extensive post production work and multiple takes, while possible, are not feasible due to the tight taping schedules and low budgets.

The first daytime TV soap opera in the United States was "These Are My Children" in 1949, though earlier melodramas had aired in the evenings as once-a-week programs. Soap operas quickly became a fixture of American daytime television in the early 1950s, joined by game shows, sitcom reruns and talk shows.

In 1988, H. Wesley Kenney, who at the time served as the executive producer of "General Hospital", said to "The New York Times":
Many long-running US soap operas established particular environments for their stories. "The Doctors" and "General Hospital", in the beginning, told stories almost exclusively from inside the confines of a hospital. "As the World Turns" dealt heavily with Chris Hughes' law practice and the travails of his wife Nancy who, tired of being "the loyal housewife" in the 1970s, became one of the first older women on the American serials to enter the workforce. "Guiding Light" dealt with Bert Bauer (Charita Bauer) and her alcoholic husband Bill, and their endless marital troubles. When Bert's status shifted to caring mother and town matriarch, her children's marital troubles were showcased. "Search for Tomorrow" mostly told its story through the eyes of Joanne Gardner (Mary Stuart). Even when stories revolved around other characters, Joanne was frequently a key player in their storylines. "Days of Our Lives" initially focused on Dr. Tom Horton and his steadfast wife Alice. The show later branched out to focus more on their five children. "The Edge of Night" featured as its central character Mike Karr, a police detective (later an attorney), and largely dealt with organized crime. "The Young and the Restless" first focused on two families, the prosperous Brooks family with four daughters, and the working class Foster family of a single working mother with three children. Its storylines explored realistic problems including cancer, mental illness, poverty and infidelity.

In contrast, "Dark Shadows" (1966–1971) and "Port Charles" (1997–2003) featured supernatural characters and dealt with fantasy and horror storylines. Their characters included vampires, witches, ghosts, goblins and angels.

The American soap opera "Guiding Light" (originally titled "The Guiding Light" until 1975) started as a radio drama in January 1937 and subsequently transferred to television in June 1952. With the exception of several years in the late 1940s, during which creator Irna Phillips was involved in a dispute with Procter & Gamble, "Guiding Light" was heard or seen nearly every weekday from 1937 until 2009, making it the longest story ever told in a broadcast medium.

Originally serials were broadcast as fifteen-minute installments each weekday in daytime slots. In 1956, "As the World Turns" and "The Edge of Night", both produced by Procter & Gamble Productions, debuted as the first half-hour soap operas on the CBS television network. All soap operas broadcast half-hour episodes by the end of the 1960s. With increased popularity in the 1970s, most soap operas had expanded to an hour in length by the end of the decade ("Another World" even expanded to 90 minutes for a short time). More than half of the serials had expanded to one-hour episodes by 1980. As of 2012, three of the four US serials air one hour episodes each weekday; only "The Bold and the Beautiful" airs 30-minute episodes.

Soap operas were originally broadcast live from the studio, creating what many at the time regarded as a feeling similar to that of a stage play. As nearly all soap operas were originated at that time in New York City, a number of soap actors were also accomplished stage actors who performed live theatre during breaks from their soap roles. In the 1960s and 1970s, new serials such as "General Hospital", "Days of our Lives" and "The Young and the Restless" were produced in Los Angeles. Their success made the West Coast a viable alternative to New York-produced soap operas, which were becoming more costly to perform. By the early 1970s, nearly all soap operas had transitioned to being taped. "As the World Turns" and "The Edge of Night" were the last to make the switch, in 1975.

"Port Charles" used the practice of running 13-week "story arcs," in which the main events of the arc are played out and wrapped up over the 13 weeks, although some storylines did continue over more than one arc. According to the 2006 Preview issue of "Soap Opera Digest", it was briefly discussed that all ABC shows might do telenovela arcs, but this was rejected.

Though U.S. daytime soap operas are not generally rerun by their networks, occasionally they are rebroadcast elsewhere; CBS and ABC have made exceptions to this, airing older episodes (either those aired earlier in the current season or those aired years prior) on major holidays when special event programming is not scheduled. Early episodes of "Dark Shadows" were rerun on PBS member stations in the early 1970s after the show's cancellation, and the entire series (except for a single missing episode) was rerun on the Sci-Fi Channel in the 1990s. After "The Edge of Night"s 1984 cancellation, reruns of the show's final five years were shown late nights on USA Network from 1985 to 1989. On January 20, 2000, a digital cable and satellite network dedicated to the genre, SOAPnet, began re-airing soaps that originally aired on ABC, NBC and CBS.

Newer broadcast networks since the late 1980s, such as Fox and cable television networks, have largely eschewed soap operas in their daytime schedules, instead running syndicated programming and reruns. No cable television outlet has produced its own daytime serial, although DirecTV's The 101 Network took over existing serial "Passions", continuing production for one season; while TBS and CBN Cable Network respectively aired their own soap operas, "The Catlins" (a primetime soap that utilized the daily episode format of its daytime counterparts) and "Another Life" (a soap that combined standard serial drama with religious overtones), during the 1980s. Fox, the fourth "major network," carried a short lived daytime soap "Tribes" in 1990. Yet other than this and a couple of pilot attempts, Fox mainly stayed away from daytime soaps, and has not attempted them since their ascension to major-network status in 1994 (it did later attempt a series of daily prime time soaps, which aired on newly created sister network MyNetworkTV, but the experiment was largely a failure).

Due to the masses of episodes produced for a series, release of soap operas to DVD (a popular venue for distribution of current and vintage television series) is considered impractical. With the exception of occasional specials, daytime soap operas are notable by their absence from DVD release schedules (an exception being the supernatural soap opera, "Dark Shadows", which did receive an essentially complete release on both VHS and DVD; the single lost episode #1219 is reconstructed by means of an off-the-air audio recording, still images, and recap material from adjacent episodes).

Due to the longevity of these shows, it is not uncommon for a single character to be played by multiple actors. The key character of Mike Karr on "The Edge of Night" was played by three different actors.

Conversely, several actors have remained playing the same character for many years, or decades even. Helen Wagner played Hughes family matriarch Nancy Hughes on American soap "As the World Turns" from its April 2, 1956 debut through her death in May 2010. She is listed in the Guinness Book of World Records as the actor with the longest uninterrupted performance in a single role. A number of performers played roles for twenty years or longer, occasionally on more than one show. Rachel Ames played Audrey Hardy on both "General Hospital" and "Port Charles" from 1964 until 2007, and returned in 2009. Susan Lucci played Erica Kane in "All My Children" from the show's debut in January 1970 until it ended its network television run on ABC on September 23, 2011. Erika Slezak played Victoria Lord #3 on "One Life to Live" from 1971 until the show ended its network television run on ABC on January 13, 2012 and resumed the role in its short-lived online revival on April 29, 2013.

Other actors have played several characters on different shows. Millette Alexander, Bernard Barrow, Doris Belack, David Canary, Judith Chapman, Jordan Charney, Joan Copeland, Nicolas Coster, Jacqueline Courtney, Louis Edmonds, Dan Hamilton, Don Hastings, Vincent Irizarry, Lenore Kasdorf, Teri Keane, Lois Kibbee, John Loprieno, Maeve McGuire, James Mitchell, Christopher Pennock, Gary Pillar, Antony Ponzini, William Prince, Louise Shaffer, and Diana van der Vlis, among many others, have all played multiple soap roles.

For several decades, most daytime soap operas concentrated on family and marital discord, legal drama and romance. The action rarely left interior settings, and many shows were set in fictional, medium-sized Midwestern towns.

Exterior shots were slowly incorporated into the series "The Edge of Night" and "Dark Shadows". Unlike many earlier serials that were set in fictional towns, "The Best of Everything" and "Ryan's Hope" were set in a real-world location, New York City.

The first exotic location shoot was made by "All My Children", to St. Croix in 1978. Many other soap operas planned lavish storylines after the success of the "All My Children" shoot. Soap operas "Another World" and "Guiding Light" both went to St. Croix in 1980, the former show culminating a long-running storyline between popular characters Mac, Rachel and Janice, and the latter to serve as an exotic setting for Alan Spaulding and Rita Bauer's torrid affair. "Search for Tomorrow" taped for two weeks in Hong Kong in 1981. Later that year, some of the cast and crew ventured to Jamaica to tape a love consummation storyline between the characters of Garth and Kathy.

During the 1980s, perhaps as a reaction to the evening drama series that were gaining high ratings, daytime serials began to incorporate action and adventure storylines, more big-business intrigue, and an increased emphasis on youthful romance.

One of the first and most popular couples was Luke Spencer and Laura Webber on "General Hospital". Luke and Laura helped to attract both male and female fans. Even actress Elizabeth Taylor was a fan and at her own request was given a guest role in Luke and Laura's wedding episode. Luke and Laura's popularity led to other soap producers striving to reproduce this success by attempting to create supercouples of their own.

With increasingly bizarre action storylines coming into vogue, Luke and Laura saved the world from being frozen, brought a mobster down by finding his black book in a Left-Handed Boy Statue, and helped a Princess find her Aztec Treasure in Mexico. Other soap operas attempted similar adventure storylines, often featuring footage shot on location – frequently in exotic locales.

During the 1990s, the mob, action and adventure stories fell out of favor with producers, due to generally declining ratings for daytime soap operas at the time, and the resultant budget cuts. In addition, soap operas were no longer able to go on expensive location shoots overseas as they were able to do in the 1980s. During that decade, soap operas increasingly focused on younger characters and social issues, such as Erica Kane's drug addiction on "All My Children", the re-emergence of Viki Lord's multiple personality disorder on "One Life to Live", and Stuart Chandler dealing with his wife Cindy dying of AIDS on "All My Children". Other social issues included cancer, rape, abortion, homophobia, and racism.

Some shows during the 2000s incorporated supernatural and science fiction elements into their storylines. One of the main characters on the earlier soap opera "Dark Shadows" was Barnabas Collins, a vampire, and "One Life to Live" featured an angel named Virgil. Both shows featured characters who traveled to and from the past.

Modern U.S. daytime soap operas largely stay true to the original soap opera format. The duration and format of storylines and the visual grammar employed by U.S. daytime serials set them apart from soap operas in other countries and from evening soap operas. Stylistically, UK and Australian soap operas, which are usually produced for early evening timeslots, fall somewhere in between U.S. daytime and evening soap operas. Similar to U.S. daytime soap operas, UK and Australian serials are shot on videotape, and the cast and storylines are rotated across the week's episodes so that each cast member will appear in some but not all episodes. UK and Australian soap operas move through storylines at a faster rate than daytime serials, making them closer to U.S. evening soap operas in this regard.

American daytime soap operas feature stylistic elements that set them apart from other shows:

Soap opera ratings have significantly fallen in the U.S. since the 2000s. No new major daytime soap opera has been created since "Passions" in 1999, while many have been cancelled. Since January 2012, four daytime soap operas – "General Hospital", "Days of Our Lives", "The Young and the Restless" and "The Bold and the Beautiful" – continue to air on the three major networks, down from a total of 12 during the 1990–91 season and a high of 19 in the 1969–70 season. This marks the first time since 1953 that there have been only four soap operas airing on broadcast television. "The Young and the Restless", the highest-rated soap opera from 1988 to the present, had fewer than 5 million daily viewers as of February 2012, a number exceeded by several non-scripted programs such as "Judge Judy". Circulations of soap opera magazines have decreased and some have even ceased publication. SOAPnet, which largely aired soap opera reruns, began to be phased out in 2012 and fully ceased operations the following year. The Daytime Emmy Awards, which honor soap operas and other daytime shows, moved from primetime network television to smaller cable channels in 2012, then failed to get any TV broadcast at all in 2014, 2016, and 2017.

Several of the U.S.’s most established soaps ended between 2009 and 2012. The longest-running drama in television and radio history, "Guiding Light", barely reached 2.1 million daily viewers in 2009 and ended on September 18 of that year, after a 72-year run. "As the World Turns" aired its final episode on September 17, 2010 after a 54-year run. "As the World Turns" was the last of 20 soap operas produced by Procter & Gamble, the soap and consumer goods company from which the genre got its name. "As The World Turns" and "Guiding Light" were also among the last of the soaps that originated from New York City. "All My Children", another New York-based soap, moved its production out to Los Angeles in an effort to reduce costs and raise sagging ratings; however, both it and "One Life to Live", each with a four-decade-plus run, were cancelled in 2011. "All My Children" aired its network finale in September 2011 with "One Life to Live" following suit in January 2012. Both "All My Children" and "One Life to Live" were briefly revived online in 2013, before being canceled again that same year.

As women increasingly worked outside of the home, daytime television viewing declined. New generations of potential viewers were not raised watching soap operas with their mothers, leaving the shows' long and complex storylines foreign to younger audiences. Now, as viewers age, ratings continue to drop among young adult women, the demographic group that soap opera advertisers pay the most for. Those who might watch in workplace breakrooms are not counted, as Nielsen does not track television viewing outside the home. The rise of cable and the internet has also provided new sources of entertainment during the day. The genre's decline has additionally been attributed to reality television displacing soap operas as TV's dominant form of melodrama. An early term for the reality TV genre was "docu-soap". A precursor to reality TV, the televised 1994–95 O. J. Simpson murder case, both preempted and competed with an entire season of soaps, transforming viewing habits and leaving soap operas with 10 percent fewer viewers after the trial ended.

Daytime programming alternatives such as talk shows, game shows, and court shows cost up to 50% less to produce than scripted dramas, making those formats more profitable and attractive to networks, even if they receive the same or slightly lower ratings than soap operas. A network may even prefer to return a timeslot to its local stations to keeping a soap opera with disappointing ratings on the air, as was the case with "Sunset Beach" and "Port Charles". Compounding the financial pressure on scripted programming in the 2007–2010 period was a decline in advertising during the Great Recession, which led shows to reduce their budgets and cast sizes. In addition to these external factors, a litany of production decisions has been cited by soap opera fans as contributing to the genre's decline, such as cliched plots, a lack of diversity that narrowed audience appeal, and the elimination of core families.

Serials produced for primetime slots have also found success. The first primetime soap opera was "Faraway Hill" (1946), which aired on October 2, 1946, on the now-defunct DuMont Television Network. "Faraway Hill" ran for 12 episodes and was primarily broadcast live, interspersed with short pre-recorded film clips and still photos to remind the audience of the previous week's episode.

The first long-running prime time soap opera was "Peyton Place" (1964–1969) on ABC. It was based in part on the eponymous 1957 film (which, in turn, was based on the 1956 novel).

The popularity of "Peyton Place" prompted the CBS network to spin-off popular "As the World Turns" character Lisa Miller into her own evening soap opera, "Our Private World" (originally titled "The Woman Lisa" in its planning stages). "Our Private World" was broadcast from May to September 1965. The character of Lisa (and her portrayer Eileen Fulton) returned to "As The World Turns" after the series ended.

The structure of "Peyton Place", with its episodic plots and long-running story arcs, set the mold for the primetime serials of the 1980s when the format reached its pinnacle.

The successful primetime serials of the 1980s included "Dallas", "Dynasty", "Knots Landing" and "Falcon Crest". These shows frequently dealt with wealthy families, and their personal and big-business travails. Common characteristics were sumptuous sets and costumes, complex storylines examining business schemes and intrigue, and spectacular disaster cliffhanger situations. Each of these series featured a wealthy, domineering, promiscuous, and passionate antagonist as a key character in the storyline – respectively, J. R. Ewing, Alexis Colby, Abby Cunningham and Angela Channing. These villainous schemers became immensely popular figures that audiences "loved to hate".

Unlike daytime serials, which are shot on video in a studio using the multi-camera setup, these evening series were shot on film using a single camera setup, and featured substantial location-shot footage, often in picturesque locales. "Dallas", its spin-off "Knots Landing", and "Falcon Crest" all initially featured episodes with self-contained stories and specific guest stars who appeared in just that episode. Each story was completely resolved by the end of the episode, and there were no end-of-episode cliffhangers. After the first couple of seasons, all three shows changed their story format to that of a pure soap opera, with interwoven ongoing narratives that ran over several episodes. "Dynasty" featured this format throughout its run.

The soap opera's distinctive open plot structure and complex continuity was increasingly incorporated into American primetime television programs of the period. The first significant drama series to do this was "Hill Street Blues". This series, produced by Steven Bochco, featured many elements borrowed from soap operas, such as an ensemble cast, multi-episode storylines, and extensive character development over the course of the series. It and the later "Cagney & Lacey" overlaid the police series formula with ongoing narratives exploring the personal lives and interpersonal relationships of the regular characters. The success of these series prompted other drama series, such as "St. Elsewhere" and situation comedy series, to incorporate serialized stories and story structure to varying degrees.

The primetime soap operas and drama series of the 1990s, such as "Beverly Hills, 90210", "Melrose Place", "Party of Five", "The OC", and "Dawson's Creek", focused more on younger characters. In the 2000s, ABC began to revitalize the primetime soap opera format with shows such as "Desperate Housewives", "Grey's Anatomy", "Brothers & Sisters", "Ugly Betty", "Private Practice", and more recently "Revenge", "Nashville", "Scandal", "Mistresses", and formerly "Ringer", which its sister production company ABC Studios co-produced with CBS Television Studios for The CW. While not soaps in the traditional sense, these shows managed to appeal to wide audiences with their high drama mixed with humor, and are soap operas by definition. These successes led to NBC's launching serials, including "Heroes" and "Friday Night Lights". The upstart MyNetworkTV, a sister network of Fox, launched a line of primetime telenovelas (a genre similar to soap operas in terms of content) upon its launch in September 2006, but discontinued its use of the format in 2007, after disappointing ratings.

On June 13, 2012, "Dallas", a continuation of the 1978 original series premiered on the cable network, TNT. The revived series, which was canceled after three seasons in 2014, delivered solid ratings for the channel, only losing viewership after the show's most established star, Larry Hagman, died midway through the series. In 2012, Nick at Nite debuted a primetime soap opera, "Hollywood Heights", which aired episodes five nights a week (on Monday through Fridays) in a manner similar to a daytime soap opera, instead of the once-a-week episode output common of other primetime soaps. The series, which was an adaptation of the Mexican telenovela "Alcanzar una estrella", suffered from low ratings (generally receiving less than one million viewers) and was later moved to sister cable channel TeenNick halfway through its run to burn off the remaining episodes.

In 2015, Fox debuted "Empire", a primetime musical serial centering on the power struggle between family members within the titular recording company. Created by Lee Daniels and Danny Strong and led by Academy Award nominees Terrence Howard and Taraji P. Henson, the drama premiered to high ratings. The show is strongly influenced by other works such as William Shakespeare's "King Lear", James Goldman's "The Lion in Winter" and the 1980s soap opera "Dynasty". Also in 2015, E! introduced "The Royals", a series following the life and drama of a fictional English Royal family, which was also inspired by "Dynasty" (even featuring Joan Collins as the Queen's mother). In addition, ABC debuted a primetime soap opera "Blood & Oil", following a young couple looking to make money off the modern-day Williston oil boom, premiering on September 27, 2015 during the 2015-16 TV season.

The telenovela, a shorter-form format of serial melodrama, shares some thematic and especially stylistic similarity to the soap opera, enough that the colloquialism "Spanish soap opera" has arisen to describe the format. The chief difference between the two is length of series; while soap operas usually have indefinite runs, telenovelas typically have a central story arc with a prescribed ending within a year or two of the show's launch, requiring more concise storytelling.

Spanish-language networks, chiefly Univision and Telemundo, have found success airing telenovelas for the growing U.S. Hispanic market. Both originally produced and imported Latin American dramas are popular features of the networks' daytime and primetime lineups, sometimes beating English-language networks in the ratings.

Some web series are soap operas, such as "" or "". In 2013, production company Prospect Park revived "All My Children" and "One Life to Live" for the web, retaining original creator Agnes Nixon as a consultant and keeping many of the same actors (Prospect Park purchased the rights to both series months after their cancellations by ABC in 2011, although it initially suspended plans to relaunch the soaps later that same year due to issues receiving approval from acting and production unions). Each show initially produced four half-hour episodes a week, but quickly cut back to two half-hour episodes each. In the midst of (though not directly related to) a lawsuit between Prospect Park and ABC, the experiment ended that same year, with both shows being canceled again.

Soap operas in the UK began on radio and consequently were associated with the BBC. It had resisted soaps as antithetical to its quality image, but began broadcasting "Front Line Family" in April 1941 on its North American shortwave service to encourage American intervention on Britain's behalf in World War II. The BBC continues to broadcast the world's longest-running radio soap, "The Archers", which first aired in May 1950, and has been running nationally since 1951. It is currently broadcast on BBC Radio 4 and continues to attract over five million listeners, or roughly 25% of the radio listening population of the UK at that time of the evening.

In the UK, soap operas are one of the most popular genres, with most being broadcast during prime time. Most UK soap operas focus on everyday, working-class communities, influenced by the conventions of the kitchen sink drama. The most popular British television programmes are "EastEnders", "Coronation Street", "Emmerdale", "Hollyoaks", "Doctors", and the Australian produced "Neighbours" and "Home and Away". The first three of these are consistently among the highest-rated shows on British television. Such is the magnitude of the popularity of the soap genre in the UK that all television serials in the country are reputedly enjoyed by members of the British Royal Family, including Elizabeth II herself. Major events in British culture are often mentioned in the storyline, such as England’s participation at the World Cup, and the death of Princess Diana.

The 1986 Christmas Day episode of "EastEnders" is often referred to as the highest-rated UK soap opera episode ever, with 30.15 million viewers (more than half the population at the time). The figure of 30.15 million was actually a combination of the original broadcast, which had just over 19 million viewers, and the Sunday omnibus edition with 10 million viewers. The combined 30.15 million audience figure makes the aforementioned Christmas Day 1986 episode of "EastEnders" the highest-rated single-channel broadcast in the history of UK television. Overall it ranks third behind the 1966 FIFA World Cup Final (32.3 million viewers) and Princess Diana's funeral in 1997 (32.1 million viewers) which were transmitted on both BBC One and ITV.

An early television serial was "The Grove Family" on the BBC, which produced 148 episodes from 1954 to 1957. The programme was broadcast live and only a handful of recordings were retained in the archives. The UK's first twice-weekly serial was ITV's "Emergency - Ward 10", running from 1957 until 1967.

In the 1960s "Coronation Street" revolutionised UK television and quickly became a British institution. On 17 September 2010, it became the world's longest-running television soap opera and was listed in "Guinness World Records". The BBC also produced several serials: "Compact" was about the staff of a women's magazine; "The Newcomers" was about the upheaval caused by a large firm setting up a plant in a small town; "United!" contained 147 episodes and focused on a football team; "199 Park Lane" (1965) was an upper class serial, which ran for only 18 episodes. None of these serials came close to making the same impact as "Coronation Street". Indeed, most of the 1960s BBC serials were largely wiped.

During the 1960s, "Coronation Street"s main rival was "Crossroads", a daily serial that began in 1964 and aired on ITV in the early evening. "Crossroads" was set in a Birmingham motel and, although the program was popular, its purported low technical standard and bad acting were much mocked. By the 1980s, its ratings had begun to decline. Several attempts to revamp the program through cast changes and, later, expanding the focus from the motel to the surrounding community were unsuccessful. "Crossroads" was cancelled in 1988 (a new version of "Crossroads" was later produced, running from 2001 until 2003).

A later rival to "Coronation Street" was ITV's "Emmerdale Farm" (later renamed "Emmerdale"), which began in 1972 in a daytime slot and was set in rural Yorkshire. Increased viewership resulted in "Emmerdale" being moved to a prime-time slot in the 1980s.

"Pobol y Cwm" ("People of the Valley") is a Welsh language serial that has been produced by the BBC since October 1974, and is the longest-running television soap opera produced by the broadcaster. "Pobol y Cwm" was originally broadcast on BBC Wales television from 1974 to 1982; it was then moved to the Welsh-language television station S4C when it opened in November 1982. The program was occasionally shown on BBC1 in London during periods of regional optout in the mid- to late 1970s. "Pobol y Cwm" was briefly shown in the rest of the UK in 1994 on BBC2, with English subtitles; it is consistently the most watched programme each week on S4C.

Daytime soap operas were non-existent until the 1970s because there was virtually no daytime television in the UK. ITV introduced "General Hospital", which later moved to a prime time slot. In 1980, Scottish Television debuted "Take the High Road", which lasted for over twenty years. Later, daytime slots were filled with an influx of Australian soap operas such as "The Sullivans" (aired on ITV from 1977), "The Young Doctors" (from 1982), "Sons and Daughters" (from 1983), "A Country Practice" (from 1982), "Richmond Hill" (from 1988 to 1989) and eventually, "Neighbours" was acquired by the BBC in 1986, and "Home and Away" aired on ITV beginning in 1989. These achieved significant levels of popularity; "Neighbours" and "Home and Away" were moved to early-evening slots, helping begin the UK soap opera boom in the late 1980s.

The day Channel 4 began operations in 1982 it launched its own soap, the Liverpool-based "Brookside", which would redefine soaps over the next decade. The focus of "Brookside" was different from earlier soap operas in the UK; it was set in a middle-class new-build cul-de-sac, unlike "Coronation Street" and "Emmerdale Farm", which were set in established working-class communities. The characters in "Brookside" were generally either people who had advanced themselves from inner-city council estates, or the upper middle-class who had fallen on hard times. Though "Brookside" was still broadcast in a pre-watershed slot (8.00 p.m. and 8.30 p.m. on weekdays, around 5.00 p.m. for the omnibus on Saturdays), it was more liberal than other soaps of the time: the dialogue regularly included expletives. This stemmed from the overall more liberal policy of the channel during that period. The soap was also heavily politicised. Bobby Grant (Ricky Tomlinson), a militant trade-unionist anti-hero, was the most overtly political character. Storylines were often more sensationalist than on other soaps (throughout the soap's history, there were two armed sieges on the street) and were staged with more violence (particularly, rape) often being featured.

In 1985, the BBC's "EastEnders" debuted and became a near instant success with viewers and critics alike, with the first episode attracting over 17 million viewers. The Christmas Day 1986 episode was watched by 30.15 million viewers and contained a scene in which divorce papers were served to Angie Watts (Anita Dobson) by her husband, Queen Vic landlord Den (Leslie Grantham). Critics talked about the downfall of "Coronation Street", but it continued to perform successfully. For the better part of ten years, "EastEnders" has shared the number one position with "Coronation Street".

A notable success in pioneering late-night broadcasting, in October 1984, Yorkshire Television began airing the cult Australian soap opera "Prisoner", which originally ran from 1979 to 1986. It was eventually broadcast on all regions of the UK in differing slots, usually around 23:00 (but never before 22:30 in any region), under the title "Prisoner: Cell Block H". It was probably most popular in the Midlands where Central Television consistently broadcast the serial three times a week from 1987 to 1991. Its airing in the UK was staggered, so different regions of the country saw it at a different pace. The program was immensely successful, regularly achieving 10 million viewers when all regions' ratings per episode were added together. Central bowed to fan pressure to repeat the soap, of which the first 95 episodes aired. Then, rival station Channel 5 also acquired rights to repeat the entire rerun of the program, starting in 1997. All 692 episodes have since been released on DVD in the UK.

In 1992, the BBC made "Eldorado" to daily alternate with "EastEnders". The programme was heavily criticised and only lasted one year. Nevertheless, soap operas gained increasing prominence on UK television schedules. In 1995, Channel 4 premiered "Hollyoaks", a soap with a youth focus. When Channel 5 launched in March 1997, it debuted the soap opera "Family Affairs", which was formatted as a week-daily soap, airing Monday through Fridays.

"Brookside"s premise evolved during the 1990s, phasing out the politicised stories of the 1980s and shifting the emphasis to controversial and sensationalist stories such as child rape, sibling incest, religious cults and drug addiction, including the infamous 'body under the patio' storyline that ran from 1993 to 1995, and gave the serial its highest ratings ever with 9 million viewers.

"Coronation Street" and "Brookside" began releasing straight-to-video features. The "Coronation Street" releases generally kept the pace and style of conventional programs episodes with the action set in foreign locations. The "Brookside" releases were set in the usual locations, but featured stories with adult content not allowed on television pre-watershed, with these releases given '18' certificates.

"Emmerdale Farm" was renamed "Emmerdale" in 1989. The series was revamped in 1993 with many changes executed via the crash of a passenger jet that partially destroyed the village and killed several characters. This attracted criticism as it was broadcast near the fifth anniversary of the Lockerbie bombing. The storyline drew the soap its highest ever audience of 18 million viewers. The revamp was a success and "Emmerdale" grew in popularity.

Throughout the 1990s, "Brookside", "Coronation Street", "EastEnders" and "Emmerdale" continued to flourish. Each increased the number of episodes that aired on a weekly basis by at least one, further defining soap operas as the leading genre in British television.

Since 2000, new soap operas have continued to be developed. Daytime drama "Doctors" began in March 2000, preceding "Neighbours" on BBC One. In 2002, as ratings for the Scottish serial "High Road" (formerly "Take The High Road") continued to decline, BBC Scotland launched "River City", which proved popular and effectively replaced "High Road" when it was cancelled in 2003. The long-running serial "Brookside" ended in November 2003 after 21 years on the air, leaving "Hollyoaks" as Channel 4's flagship serial.

A new version of "Crossroads" featuring a mostly new cast was produced by Carlton Television for ITV in 2001. It did not achieve high ratings and was cancelled in 2003. In 2001, ITV also launched a new early-evening serial entitled "Night and Day". This program too attracted low viewership and, after being shifted to a late night time slot, was cancelled in 2003. "Family Affairs", which was broadcast opposite the racier "Hollyoaks", never achieved significantly high ratings leading to several dramatic casting revamps and marked changes in style and even location over its run. By 2004, "Family Affairs" had a larger fan base and won its first awards, but was cancelled in late 2005.

In 2008, ITV premiered "The Royal Today", a daily spin-off of popular 1960s-based drama "The Royal", which had been running in a primetime slot since 2002. Just days later, soap opera parody programs "Echo Beach" premiered alongside its sister show, the comedy "Moving Wallpaper". Both "Echo Beach" and "The Royal Today" ended after just one series due to low ratings. Radio soap opera "Silver Street" debuted on the BBC Asian Network in 2004. Poor ratings and criticism of the program led to its cancellation in 2010.

UK soap operas for many years usually only aired two nights a week. The exception was the original "Crossroads", which began as a weekdaily soap opera in the 1960s, but later had its number of weekly broadcasts reduced. Things started to change in 1989 when "Coronation Street" began airing three times a week. In 1996, it expanded again, to air four episodes a week. "Brookside" premiered in 1982 with two episodes a week. In 1990, it expanded to three episodes a week; the trend was followed by "EastEnders" in 1994 and "Emmerdale" in 1997, In 2000, the soap went five episodes a week. "Family Affairs" debuted as a weekdaily soap in 1997, producing five episodes a week throughout its entire run. The imported "Neighbours" screens as five new episodes a week, which are shown once at 1:45 p.m. and repeated at 5:30 p.m. on Channel 5 each weekday.

Currently, "Coronation Street" (which began airing two episodes on Monday nights in 2002) and "Hollyoaks" both produce five episodes a week, while "EastEnders" produces four each week. In 2002, "Brookside" expanded from three half-hour episodes on different weeknights to airing one 90-minute episode each week. In 2004, "Emmerdale" began airing six episodes a week. "Doctors" airs five episodes a week, and is the only soap without a weekend omnibus repeat screening. It was announced in June 2016 that starting late 2017, Coronation Street would air six episodes a week.

Due to a January 2008 overhaul of the ITV network, the Sunday episodes of "Coronation Street" and "Emmerdale" were moved out of their slots. "Coronation Street" added a second episode on Friday evenings at 8:30 p.m. "Emmerdale"s Tuesday edition was extended to an hour, putting it in direct competition with rival "EastEnders".

In July 2009, the schedules of these serials were changed again. On 23 July 2009, "Coronation Street" moved from the Wednesday slot it held for 49 years, to Thursday evenings. "Emmerdale" reverted to running just one 30-minute episode on Tuesday evenings and the other 30-minute installment was moved to Thursday evenings. Coronation Street has since returned to a Wednesday slot. It now airs Mondays, Wednesdays and Fridays at 19:30 and 20:30. Emmerdale airs at 19:00 every weeknight, and 20:00 on Thursdays.

UK soap operas are shot on videotape in the studio using a multi-camera setup. Since the 1980s, these programs routinely feature scenes shot outdoors in each episode. This footage is shot on videotape on a purpose-built outdoor set that represents the community that the soap focuses on. During their early years, "Coronation Street" and "Emmerdale" used 16 mm film while scenes were shot on location. Later soap operas such as "Hollyoaks" and "Family Affairs" started filming on high-definition video, a more modern filming process, as opposed to standard videotape, which features better quality and appears to look more like film than videotape.

UK soap operas do not incorporate recap sequences at the beginning of each episode, which would be appropriate for the fact that when an episode ends, it picks up the story during the following episode. However, in 2012, "Hollyoaks" began airing a recap sequence at the beginning of each episode. With the exception of "Hollyoaks", soap operas in the UK also lack incidental music, although "EastEnders" would sometimes feature music that plays over an ending scene if it was dramatic, with an alternative "EastEnders" theme known as "Julia's theme".

Australia has had quite a number of well-known soap operas, some of which have gained cult followings in the UK, New Zealand and other countries. The majority of Australian television soap operas are produced for early evening or evening timeslots. They usually produce two or two-and-a-half hours of new material each week, either arranged as four or five half-hour episodes a week, or as two one-hour episodes. Stylistically, these series most closely resemble UK soap operas in that they are nearly always shot on videotape, are mainly recorded in a studio and use a multi-camera setup. The original Australian serials were shot entirely in-studio. During the 1970s occasional filmed inserts were used to incorporate sequences shot outdoors. Outdoor shooting later became commonplace and starting in the late 1970s, it became standard practice for some on-location footage to be featured in each episode of any Australian soap opera, often to capitalise on the attractiveness and exotic nature of these locations for international audiences. Most Australian soap operas focus on a mixed age range of middle-class characters and will regularly feature a range of locations where the various, disparate characters can meet and interact, such as the café, the surf club, the wine bar or the school.

The genre began in Australia on radio, as it had in the United States and the United Kingdom. One such radio serial, "Big Sister", featured actress Thelma Scott in the cast and aired nationally for five years beginning in 1942. Probably the best known Australian radio serial was the long-running soap opera "Blue Hills", which was created by Gwen Meredith and ran from 1949 to 1976. With the advent of Australian television in 1956, daytime television serials followed. The first Australian television soap opera was "Autumn Affair" (1958) featuring radio personality and "Blue Hills" star Queenie Ashton making the transition to television. Each episode of this serial ran for 15 minutes and aired each weekday on the Seven Network. "Autumn Affair" failed to secure a sponsor and ended in 1959 after 156 episodes. It was followed by "The Story of Peter Grey" (1961), another Seven Network weekday series aired in a daytime slot in 15-minute installments. "The Story of Peter Grey" ran for 164 episodes.

The first successful wave of Australian evening television soap operas started in 1967 with "Bellbird", produced by the Australian Broadcasting Corporation. This rural-based serial screened in an early evening slot in 15-minute installments as a lead-in to the evening news. "Bellbird" was a moderate success but built-up a consistent and loyal viewer base, especially in rural areas, and enjoyed a ten-year run. "Motel" (1968) was Australia's first half-hour soap opera; the daytime soap had a short run of 132 episodes.

The first major soap opera hit in Australia was the sex-melodrama "Number 96", a nighttime series produced by Cash Harmon Television for Network Ten, which debuted March 1972. The program dealt with such topics as homosexuality, adultery, drug use, rape-within-marriage and racism, which had rarely been explored on Australian television programs before. The series became famous for its sex scenes and nudity and for its comedic characters, many of whom became cult heroes in Australia. By 1973, "Number 96" had become Australia's highest-rated show. In 1974, the sexed-up antics of "Number 96" prompted the creation of "The Box", which rivaled it in terms of nudity and sexual situations and was scheduled in a nighttime slot. Produced by Crawford Productions, many critics considered "The Box" to be a more slickly produced and better written show than "Number 96". "The Box" also aired on the Ten Network, programmed to run right after "Number 96". For 1974 "Number 96" was again the highest rating show on Australian television, and that year "The Box" occupied the number two spot.

Also in 1974, the Reg Grundy Organisation created its first soap opera, and significantly Australia's first "teen" soap opera, "Class of '74". With its attempts to hint at the sex and sin shown more openly on "Number 96" and "The Box", its high school setting and early evening timeslot, "Class of '74" came under intense scrutiny from the Broadcasting Control Board, who vetted scripts and altered entire storylines.

By 1975, both "Number 96" and "The Box", perhaps as a reaction to declining ratings for both shows, de-emphasised the sex and nudity shifting more towards comedic plots. "Class of '74" was renamed "Class of '75" and also added more slapstick comedy for its second year, but the revamped show's ratings declined, resulting in its cancellation in mid-1975. That year Cash Harmon's newly launched second soap "The Unisexers" failed in its early evening slot and was cancelled after three weeks; the Reg Grundy Organisation's second soap "Until Tomorrow" ran in a daytime slot for 180 episodes.

A feature film version of "Bellbird" entitled "Country Town" was produced in 1971 by two of the show's stars, Gary Gray and Terry McDermott, without production involvement by the Australian Broadcasting Corporation. "Number 96" and "The Box" also released feature film versions, both of which had the same title as the series, released in 1974 and 1975 respectively. As Australian television had broadcast in black and white until 1975, these theatrical releases all had the novelty of being in colour. The film versions of "Number 96" and "The Box" also allowed more explicit nudity than could be shown on television at that time.

In November 1976 "The Young Doctors" debuted on the Nine Network. This Grundy Organization series eschewed the adult drama of "Number 96" and "The Box", focusing more on relationship drama and romance. It became a popular success but received few critical accolades. A week later "The Sullivans", a carefully produced period serial chronicling the effects of World War II on a Melbourne family, also debuted on Nine. Produced by Crawford Productions, "The Sullivans" became a ratings success, attracted many positive reviews, and won television awards. During this period "Number 96" re-introduced nudity into its episodes, with several much-publicised full-frontal nude scenes, a cast revamp and a new range of shock storylines designed to boost the show's declining ratings. "Bellbird" experienced changes to its broadcast pattern with episodes screening in 60 minute blocks, and later in 30 minute installments.
"Bellbird", "Number 96" and "The Box", which had been experiencing declining ratings, were cancelled in 1977. Various attempts to revamp each of the shows with cast reshuffles or spectacular disaster storylines had proved only temporarily successful. "The Young Doctors" and "The Sullivans" continued to be popular. November 1977 saw the launch of successful soap opera/police procedural series "Cop Shop" (1977–1984) produced by Crawford Productions for Channel Seven. In early December 1977 Channel Ten debuted the Reg Grundy Organisation produced "The Restless Years" (1977–1981), a more standard soap drama focusing on several young school leavers.

The Seven Network, achieving success with "Cop Shop" produced by Crawford Productions, had Crawfords produce "Skyways", a series with a similar format but set in an airport, to compete with the Nine Network's popular talk show "The Don Lane Show". "Skyways", which debuted in July 1979, emphasised adult situations including homosexuality, marriage problems, adultery, prostitution, drug use and smuggling, crime, suicide, political intrigue, and murder, and featured some nudity. Despite this, the program achieved only moderate ratings and was cancelled in mid-1981.

The Reg Grundy Organisation found major success with the women's-prison drama "Prisoner" (1979–1986) on Network Ten, and melodramatic family saga "Sons and Daughters" (1982–1987) on the Seven Network. Both shows achieved high ratings in their original runs, and unusually, found success in repeats after the programs ended.

Grundy soap "The Young Doctors" and Crawford Productions' "The Sullivans" continued on the Nine Network until late 1982. Thereafter Nine attempted many new replacement soap operas produced by the Reg Grundy Organisation: "Taurus Rising" (1982), "Waterloo Station" (1983), "Starting Out" (1983) and "Possession" (1985), along with "Prime Time" (1986) produced by Crawford Productions. None of these programs were successful and most were cancelled after only a few months. The Reg Grundy Organisation also created "Neighbours", a suburban-based daily serial devised as a sedate family drama with some comedic and lightweight situations, for the Seven Network in 1985.

Produced in Melbourne at the studios of HSV-7, "Neighbours" achieved high ratings in Melbourne, Brisbane and Adelaide, but not in Sydney, where it aired at 5.30 p.m. placing it against the hit dating game show "Perfect Match" on Channel 10. The Seven Network's Sydney station ATN-7 quickly lost interest in "Neighbours" as a result of the low ratings in Sydney. HSV-7 in Melbourne lobbied heavily to keep "Neighbours" on the air, but ATN-7 managed to convince the rest of the network to cancel the show and instead keep ATN-7's own Sydney-based dramas "A Country Practice" and "Sons and Daughters".

After the network cancelled "Neighbours", it was immediately picked up by Channel Ten, which revamped the cast and scripts slightly and aired the series in the 7.00 p.m. slot starting 20 January 1986. It initially attracted low audiences, however after a concerted publicity drive, Ten managed to transform the series into a major success, turning several of its actors into major international stars. The show's popularity eventually declined and it was moved to the 6.30 p.m. slot in 1992. As of 2015 the series airs on Eleven and is Australia's longest-running soap opera.

The success of "Neighbours" in the 1980s prompted the creation of somewhat similar suburban and family or teen-oriented soap operas such as "Home and Away" (1988–present) on Channel Seven and "Richmond Hill" (1988) on Channel Ten. Both proved popular, however "Richmond Hill" emerged as only a moderate success and was cancelled after one year to be replaced on Ten by "E Street" (1989–1993).

Nine continued trying to establish a successful new soap opera, without success. After the failure of family drama "Family and Friends" in 1990, it launched the raunchier and more extreme "Chances" in 1991, which resurrected the sex and melodrama of "Number 96" and "The Box" in an attempt to attract attention. "Chances" achieved only moderate ratings, and was moved to a late-night timeslot. It underwent several revamps that removed much of the original cast, and refocused the storylines to incorporate science-fiction and fantasy elements. The series continued in a late night slot until 1992, when it was cancelled due to low ratings despite the much-discussed fantasy storylines.

Several Australian soap operas have also found significant international success. In the UK, starting in the mid-1980s, daytime broadcasts of "The Young Doctors", "The Sullivans", "Sons and Daughters" and "Neighbours" (which itself was subsequently moved to an early-evening slot) achieved significant success. Grundy's "Prisoner" began airing in the United States in 1979 and achieved high ratings in many regions there, however the show ended its run in that country three years into its run. "Prisoner" also aired in late-night timeslots in the UK beginning in the late 1980s, achieving enduring cult success there. The show became so popular in that country that it prompted the creation of two stage plays and a stage musical based on the show, all of which toured the UK, among many other spin-offs. In the late 1990s, Channel 5 repeated "Prisoner" in the UK. Between 1998 and 2005, Channel 5 ran late-night repeats of "Sons and Daughters". During the 1980s, the Australian attempts to emulate big-budget U.S. soap operas such as "Dallas" and "Dynasty" had resulted in the debuts of "Taurus Rising" and "Return to Eden", two slick soap opera dramas with big budgets that were shot entirely on film. Though their middling Australian ratings resulted in the shows running only for a single season, both programs were successfully sold internationally.

Other shows to achieve varying levels of international success include "Richmond Hill", "E Street", "Paradise Beach" (1993–1994), and "Pacific Drive" (1995–1997). Indeed, these last two series were designed specifically for international distribution. Channel Seven's "Home and Away", a teen soap developed as a rival to "Neighbours", has also achieved significant and enduring success on UK television.

"Something in the Air", a serial examining a range of characters in a small country town ran on the ABC from 2000 to 2002.

Attempts to replicate the success of daily teen-oriented serials "Neighbours" and "Home and Away" saw the creation of "Echo Point" (1995) and "Breakers" (1999) on Network Ten. These programs foregrounded youthful attractive casts and appealing locations but the programs were not long-running successes and "Neighbours" and "Home and Away" remained the most visible and consistently successful Australian soap operas in production. In their home country, they both attracted respectable although not spectacular ratings in the early 2000s. By 2004, "Neighbours" was regularly attracting just under a million viewers per episode – considered at that time a low figure for Australian prime time television. By March 2007, the Australian audience for "Neighbours" had fallen to fewer than 700,000 a night. This prompted a revamp of the show's cast, its visual presentation, and a move away from the recently added action-oriented emphasis to refocus the show on the domestic storylines it is traditionally known for. During this period "Neighbours" and "Home and Away" continued to achieve significant ratings in the UK. This and other lucrative overseas markets, along with Australian broadcasting laws that enforce a minimum amount of domestic drama production on commercial television networks, help ensure that both programs remain in production. Both shows get higher total ratings in the UK than in Australia (the UK has three times the total population of Australia) and the UK channels make a major contribution to the production costs.

It has been suggested that with their emphasis on the younger, attractive and charismatic characters, "Neighbours" and "Home and Away" have found success in the middle ground between glamorous, fantastic U.S. soaps with their wealthy but tragic heroes and the more grim, naturalistic UK soap operas populated by older, unglamorous characters. The casts of "Neighbours" and "Home and Away" are predominantly younger and more attractive than the casts of UK soaps, and without excessive wealth and glamour of the U.S. daytime serial, a middle-ground in which they have found their lucrative niche.

"Neighbours" was carried in the United States on the Oxygen cable channel in March 2004; however it attracted few viewers, perhaps in part due to its scheduling opposite well-established and highly popular U.S. soap operas such as "All My Children" and "The Young and the Restless", and was dropped by the network shortly afterwards due to low ratings.

"headLand" made its debut on Channel Seven in November 2005, the series arose out of a proposed spinoff of "Home and Away" that was to have been produced in conjunction with "Home and Away"s UK broadcaster, Five. The idea for the spin-off was scuttled after Five pulled out of the deal, which meant that the show could potentially air on a rival channel in the UK; as such, Five requested that the new show be developed as a standalone series and not be spun off from a series that it owned a stake in. The series premiered in Australia on November 15, 2005, but was not a ratings success and was cancelled two months later on January 23, 2006. The series broadcast on E4 and Channel 4 in the UK. Nickelodeon's "" appeared on July 2006 on Network Ten. Since Connie considered this mention as a torrid soap opera, this was mentioned in the Steven Universe episode "Love Letters".

After losing the UK television rights to "Neighbours" to Five, the BBC commissioned a replacement serial "Out of the Blue", which was produced in Australia. It debuted as part of BBC One's weekday afternoon schedule on 28 April 2008 but low ratings prompted its move to BBC Two on 19 May 2008. The series was cancelled after its first season.

"Neighbours"' continued low ratings in Australia resulted in it being moved to Ten's new digital channel, Eleven on January 11, 2011. However, it continues to achieve reasonable ratings on Channel 5 in the United Kingdom, and as of March 2013 still reportedly achieved significant international sales.

Pioneering series "Pukemanu" aired over two years (1971–72) and was the NZBC's first continuing drama. It followed the goings-on of a North Island timber town.
"Close to Home" is a New Zealand television soap opera that ran on Television One (Formally NZBC, later becoming Television New Zealand) from 1975 to 1983. At its peak in 1977 nearly one million viewers tuned in twice weekly to watch the series co-created by Michael Noonan and Tony Isaac (who had initially only agreed to make the show on the condition they would get to make "The Governor"). "Gloss" is a television drama series that screened from 1987 to 1990. The series is about a fictional publishing empire run by the Redfern family. Gloss was NZ's answer to US soap "Dynasty", with the Carrington oil scions replaced by the wealthy Redferns and their Auckland magazine empire. It was a starting point for many actors who went on to many productions in New Zealand, Australia and around the world including Temuera Morrison, Miranda Harcourt, Peter Elliott, Lisa Chappell, Danielle Cormack and Kevin Smith. Many of them would go on to star in "Shortland Street", which has been New Zealand's most popular soap since its debut in 1992. It airs on TVNZ 2.

Radio New Zealand began airing its first radio soap "You Me Now" in September 2010. It is available for podcast on its website.

Relatively few daily soap operas have been produced on English Canadian television, with most Canadian stations and networks that carry soap operas airing those imported from the United States or the United Kingdom. Notable daily soaps that did exist included "Family Passions", "Scarlett Hill", "Strange Paradise", "Metropia", "Train 48" and the international co-production "Foreign Affairs". "Family Passions" was an hour-long program, as is typical of American daytime soaps; all of the others ran half-hour episodes. Unlike American or British soap operas, the most influential of which have run for years or even decades, even daily Canadian soap operas have run for a few seasons at most. Short-run soaps, including "49th & Main" and "North/South", have also aired. Many of these were produced in an effort to comply with Canadian content regulations, which require a percentage of programming on Canadian television to originate from Canada.

Notable prime time soap operas in Canada have included "Riverdale", "House of Pride", "Paradise Falls", "Lance et Compte" ("He Shoots, He Scores"), "Loving Friends and Perfect Couples" and "The City". The "Degrassi" franchise of youth dramas also incorporated some elements of the soap opera format.

On French-language television in Quebec, the "téléroman" has been a popular mainstay of network programming since the 1950s. Notable téléromans have included "Rue des Pignons", "Les Belles Histoires des pays d'en haut", "Diva", "La famille Plouffe", and the soap opera parody "Le Cœur a ses raisons".

Unlike the season based production in most countries, most of Indian television fiction tends to be regular-broadcasting soap opera. These started in the late 1990s, as more and more people began to purchase television sets. At the beginning of the 21st century, soap operas became an integral part of Indian culture. Indian soap operas mostly concentrate on the conflict between love and arranged marriages occurring in India, and many includes family melodrama. Indian soap operas have multilingual production.

Many soap operas produced in India are also broadcast overseas in the UK, Canada, the United States, and some parts of Europe, South Africa, Australia and South East Asia. They are often mass-produced under large production banners, with companies like Balaji Telefilms running different language versions of the same serial on different television networks or channels.

The Australian serial "The Restless Years" was remade in the Netherlands as "Goede tijden, slechte tijden" (which debuted in 1990) and in Germany as "Gute Zeiten, schlechte Zeiten" (which has aired since 1992): both titles translate to "good times, bad times". These remakes are still airing, but have long since diverged from the original Australian storylines. The two shows are the highest-rated soap operas in their respective countries.

A later Australian serial, "Sons and Daughters", has inspired five remakes produced under license from the original producers and based, initially, on original story and character outlines. These are "Verbotene Liebe" (Germany, 1995–2015); "Skilda världar" (Sweden, 1996–2002); "Apagorevmeni agapi" (Greece, 1998); "Cuori Rubati" (Italy, 2002–2003) and "Zabranjena ljubav" (Croatia, 2004–2008). Both "The Restless Years" and "Sons and Daughters" were created and produced in Australia by the Reg Grundy Organisation.

The Norwegian soap opera "Hotel Cæsar" has aired on TV 2 since 1998, and is the longest-running television drama in Scandinavia. Popular foreign soaps in the country include "Days of Our Lives" (broadcast on TV6), "The Bold and the Beautiful" (TNT) and "Home and Away" (TV 2), all of which are subtitled.

Serials have included "Goede tijden, slechte tijden" (1990–present), "Onderweg naar Morgen" (1994–2010) and "Goudkust" (1996–2001). In 2016 "Goede tijden, slechte tijden" spin-off "Nieuwe Tijden" started airing. U.S. daytime serials "As The World Turns" and "The Bold and the Beautiful" have aired in the Netherlands; "As the World Turns" began airing in the country in 1990, with Dutch subtitles.

In the 1980s, West German networks successfully added American daytime and primetime soap operas to their schedule before Das Erste introduced its first self-produced weekly soap with "Lindenstraße", which was seen as a German counterpart to "Coronation Street". Like in other countries, the soap opera met with negative reviews, but eventually proved critics wrong with nearly 13 million viewers tuning in each week. Even though the format proved successful, it was not until 1992 before "Gute Zeiten, schlechte Zeiten" became the first German daily soap opera. Early ratings were bad as were the reviews, but the RTL network was willing to give its first soap opera a chance; ratings would improve, climbing to 7 million viewers by 2002. Not long after "Gute Zeiten, schlechte Zeiten", Das Erste introduced "Marienhof", which aired twice a week.

After successfully creating the first German daily soap, production company Grundy Ufa wanted to produce another soap for RTL. Like "GZSZ", the format was based on an Australian soap opera from Reg Watson. But RTL did not like the plot idea about separated twins who meet each other for the first time after 20 years and fall in love without knowing that they are related. The project was then taken to Das Erste, which commissioned the program, titled "Verbotene Liebe", which premiered on January 2, 1995. With the premiere of "Verbotene Liebe", the network turned "Marienhof" into a daily soap as well. In the meanwhile, RTL debuted the Grundy Ufa-produced "Unter uns" in late 1994.

ZDF started a business venture with Canada and co-produced the short-lived series "Family Passions", starring actors such as Gordon Thomson, Roscoe Born, Dietmar Schönherr and a young Hayden Christensen. The daytime serial premiered on December 5, 1994, lasting 130 episodes. After its cancellation, the network debuted "Jede Menge Leben". Even after a crossover with three soaps, "Freunde fürs Leben", "Forsthaus Falkenau" and "Unser Lehrer Doktor Specht", the soap was canceled after 313 episodes. Sat.1 tried to get into the soap business as well, after successfully airing the Australian soap opera "Neighbours", which was dropped in 1995 due to the talk show phenomenon that took over most of the daytime schedules of German networks. The network first tried to tell a family saga with "So ist das Leben – die Wagenfelds", before failing with "Geliebte Schwestern". RTL II made its own short-lived attempt with "Alle zusammen - jeder für sich".

The teen soap opera "Schloss Einstein" debuted on September 4, 1998, focusing on the life of a group of teenagers at the fictional titular boarding school near Berlin. As of July 2014, the series has produced over 815 episodes during the course of 17 seasons, a milestone in German television programming, and was renewed for an 18th season to debut in 2015.

In 1999, after the lasting success of "Gute Zeiten, schlechte Zeiten", "Marienhof", "Unter uns" and "Verbotene Liebe", ProSieben aired "Mallorca – Suche nach dem Paradies", set on the Spanish island with the same name. After nine months, the network canceled the program due to low viewership and high production costs. Even though ratings had improved, the show ended its run in a morning timeslot. The soap opera became something of a cult classic, as its 200-episode run was repeated several times on free-to-air and pay television.

In 2006, "Alles was zählt" became the last successful daily soap to make its debut, airing as a lead-in to "Gute Zeiten, schlechte Zeiten" and also produced by Grundy Ufa. Since Germany started to produce its own telenovelas, all soap operas faced declines in ratings. "Unter uns" was in danger of cancellation in 2009, but escaped such a fate due to budget cuts imposed by the show's producers and the firing of original cast member Holger Franke, whose firing and the death of his character outraged fans, resulting in a ratings spike in early 2010. After "Unter uns" was saved, Das Erste planned to make changes to its soap lineup. "Marienhof" had to deal with multiple issues in its storytelling, as well as in producing a successful half-hour show. Several changes were made within months, however "Marienhof" was canceled in June 2011. "Verbotene Liebe" was in danger of being cancelled as well, but convinced the network to renew it with changes that it made in both 2010 and 2011; the soap was later expanded to 45 minutes after "Marienhof" was canceled, and the network tried to decide on whether to revamp its lineup.

While "Gute Zeiten, schlechte Zeiten", "Unter uns" and "Alles was zählt" are currently the only daily soaps on the air after "Verbotene Liebe" has been cancelled and aired its last episode in June, 2015 due to low ratings, the telenovelas "Sturm der Liebe" and "Rote Rosen" are considered soaps by the press as well, thanks to the changing protagonists every season.

In Belgium, the two major soap operas are "Thuis" ("Home") and "Familie" ("Family"). Soap operas have been very popular in Flanders, the Dutch-speaking part of Belgium. "Familie" debuted in late 1991, and with nearly 6,000 half-hour episodes, it has the highest episode total of any soap in Europe outside of the United Kingdom. The highest-rated soap opera is "Thuis", which has aired on "één" since late 1995. "Thuis" is often one of the five most-watched Belgian shows and regularly garners over one million viewers (with 6.3 million Flemmings in total).

During the 1990s, foreign soap operas such as "Neighbours" and "The Bold and the Beautiful" were extremely popular, the latter having achieved a cult status in Belgium and airing in the middle of the decade during prime time. Both soaps still air today, along with other foreign soaps such as "Days of Our Lives", Australia's "Home and Away" and Germany's "Sturm der Liebe". Vitaya unsuccessful attempted to air the Dutch soap opera "Goede Tijden, Slechte Tijden" in 2010. Other foreign soaps that previously aired on Belgian television include "The Young and the Restless", "EastEnders" (both on VTM), "Port Charles" (at één, then known as TV1) and "Coronation Street" (on Vitaya). "Santa Barbara" aired during the 1990s on VTM for its entire run.

The only teen soap opera on Belgian television was "Spring" ("Jump" in English), which aired on the youth-oriented Ketnet and produced over 600 15-minute episodes from late 2002 until 2009, when it was cancelled after a steady decline in ratings following the departures of many of its original characters.

The most successful soap opera in Italy is the evening series "Un posto al sole" ("A Place Under the Sun"), which had aired on Rai 3 since 1996 (whose format is based on the Australian soap opera "Neighbours"). Several other Italian soaps have been produced such as "Ricominciare" ("Starting Over"), "Cuori rubati" ("Stolen Hearts"), "Vivere" ("Living"), "Sottocasa" ("Downstairs"), "Agrodolce" ("Bittersweet") and "Centovetrine" ("Hundred Shop Windows").

The most popular Italian prime-time drama series, "Incantesimo" ("Enchantment"), which ran from 1998 to 2008, became a daytime soap opera for the final two years of its run, airing five days a week on Rai 1.

In the early years of RTÉ, the network produced several dramas but had not come close to launching a long-running serial. RTÉ's first television soap was "Tolka Row", which was set in urban Dublin. For several years, both "Tolka Row" and "The Riordans" were produced by RTÉ; however, the urban soap was soon dropped in favor of the more popular rural soap opera "The Riordans", which premiered in 1965. Executives from Yorkshire Television visited during on-location shoots for "The Riordans" in the early 1970s and in 1972, debuted "Emmerdale Farm" (now "Emmerdale"), based on the successful format of the Irish soap opera. In the late 1970s, "The Riordans" was controversially dropped. The creator of that series would then go on to produce the second of his "Agri-soap" trilogy "Bracken", starring Gabriel Byrne, whose character had appeared in the last few seasons of "The Riordans". Bracken was soon replaced by the third "Agri-soap" "Glenroe", which ran until 2001. As RTÉ wanted a drama series for its Sunday night lineup rather than a soap opera, "On Home Ground" (2001–2002), "The Clinic" (2002–2009) and "RAW" (2010–2013) replaced the agri-soaps of the previous decades.

In 1989, RTÉ decided to produce its first Dublin-based soap opera since the 1960s. "Fair City", which is set in the fictional city of Carrickstown, initially aired one night a week during the 1989-90 season, and similar to its rural soaps, much of the footage was filmed on location – in a suburb of Dublin City. In 1992, RTÉ made a major investment into the series by copying the houses used in the on-location shoots for an on-site set in RTÉ's Headquarters in Dublin 4. By the early 1990s, it was airing two nights a week for 35 weeks a year. With competition from the UK soap operas, RTÉ expanded "Fair City" to three nights a week for most of the year and one night a week during the summer in 1996, later expanding to four nights a week and two nights during the summer. Until the early 2000s, the series produced four episodes a week, airing all 52 weeks of the year. "Fair City" airs Sundays, Tuesdays and Thursdays at 8.00 p.m. GMT on RTÉ One; however, after rival network TV3 moved "Coronation Street" to Thursday night, the Wednesday night episode of "Fair City" began airing at 7:30 p.m. each week.

TG4 produce the Irish language soap "Ros na Rún" ("Headland of the Secrets" or "Headland of the Sweethearts"); set in the fictional village of "Ros Na Rún", located outside Galway and near Spiddal, it centres on the domestic and professional lives of the town's residents. It is modeled on an average village in the West of Ireland, but with its own distinct personality – with a diverse population that share secrets, romances and friendships among other things. While the core community has remained the same, the look and feel of "Ros Na Rún" has changed and evolved over the years to incorporate the changing face of rural Ireland. It has an established a place not only in the hearts and minds of the Irish speaking public, but also the wider Irish audience. The program has dealt with many topics, including domestic violence, infidelity, theft, arson, abortion, homosexuality, adoption, murder, rape, drugs, teen pregnancy and paedophilia. It runs twice a week for 35 weeks of the year, currently airing Tuesday and Thursday nights. "Ros na Rún" is the single largest independent production commissioned in the history of Irish broadcasting. Prior to TG4's launch, it originally aired on RTÉ One in the early 1990s.

Although Ireland has access to international soaps (such as "Coronation Street", "Emmerdale", "EastEnders", "Home and Away", "Hollyoaks" and "Neighbours"), "Fair City" continues to outperform them all, and is Ireland's most popular soap opera, with the show peaking at over 700,000 viewers.

January 2015 "Red Rock" has broadcast on TV3. Red Rock airs twice a week on Wednesday and Thursday nights. The series is base in a fishing village in Dublin. The soaps centres around the local Garda station but also includes stories from the village.

RTÉ Radio produced its first radio soap, "Kennedys of Castleross", which ran from April 13, 1955 to 1975. In 1979 RTÉ long running TV soap The Riordans moved to Radio until December 24, 1985. In the mid-1980s, RTÉ debuted a new radio soap, "Harbour Hotel", which ran until the mid-1990s. The network later ran two short-lived radio soaps, "Konvenience Korner" and "Riverrun", which were followed in 2004 by "Driftwood". RTÉ does not run any radio soaps, however RTÉ Radio 1 continues to air radio dramas as part of its nighttime schedule.


In Greece, there have been several soap operas.

An early serial was "Sti skia tou hrimatos" ("Money Shadows"), which ran from 1990 to 1991. September 1991 saw the debut of "Lampsi" ("the Shining"), from creator Nicos Foskolos. The series would become Greece's longest-running soap opera. After the success of "Lampsi" came the short lived "To galazio diamandi" ("Blue Diamond") and "Simphonia siopis" ("Omertà"). "Lampsi" was canceled in June 2005 due to declining ratings. It was replaced by "Erotas" ("Love"), a soap that ran from 2005 to 2008. After that series ended, ANT1 abandoned the soap opera genre and focused on comedy series and weekly dramas.

Greece's second longest-running soap is "Kalimera Zoi" ("Goodmorning Life"), which ran from September 1993 until its cancellation in June 2006 due to low ratings.

Mega Channel began producing soap operas in 1990 with the prime time serial "I Dipsa" ("The Thirst"), which ran for 102 episodes. Other daytime soaps have included "Paralliloi dromoi" (1992–1994) and its successor "Haravgi" ("Daylight", 1994–1995), both of which were cancelled due to low viewership; as well as the serials "Apagorevmeni Agapi" ("Forbidden Love"), which ran from 1998 to 2006; "Gia mia thesi ston Ilio" ("A Spot Under the Sun"), which ran from 1998 to 2002; "Filodoxies" ("Expectations"), which ran from 2002 to 2006; and "Vera Sto Deksi" ("Ring on the Right Hand"), which ran from 2004 to 2006 and proved to be a successful competitor to "Lampsi", causing that show's ratings to decline.

"Ta Mistika Tis Edem" ("Edem Secrets"), which was created by the producers of "Vera Sto Deksi", debuted in 2008 and has eclipsed that show's success. Its ratings place it consistently among the three highest-rated daytime programs.

IENED (which was renamed ERT2 in 1982) was responsible for the first Greek soap operas "I Kravgi Ton Likon" and "Megistanes". ERT also produced the long-running soap "O Simvoleografos". Since 2000 and with the introduction of private television, ERT produced additional daily soap operas, which included "Pathos" ("Passion"), "Erotika tis Edem" ("Loving in Eden") and "Ta ftera tou erota" ("The Wings of Love"). These failed to achieve high ratings and were canceled shortly after their premiere.

Alpha produced "Kato apo tin Acropoli" ("Under the Acropolis"), which ran for 2½ years.

The first daytime soap opera produced by a Cyprus channel was LOGOs TV's "Odos Den Ksehno" ("'Don't Forget' Street"), which ran from January to December 1996. It was followed by "To Serial", which also ran for one year from September 1997 to June 1998. CyBC created the third weekdaily soap, "Anemi Tou Pathous" ("Passion Winds"), running from January 2000 to June 2004, which was replaced by "I Platia" ("The Square") from September 2004 to July 2006. "Epikindini Zoni" ran from 2009 to 2010, and was cancelled after 120 episodes. "Vimata Stin Ammo" made its debut in September 2010.

Sigma TV first commissioned the weekdaily comedic soap "Sto Para Pente", which aired from September 1998 to June 2004, and first held the distinction of being the longest weekday show in Cyprus television history, before it was surpassed by "Se Fonto Kokkino", which ran from September 2008 to July 2012. Other Sigma TV weekday shows include "Akti Oniron" (which ran from 1999 to 2001), "Vourate Geitonoi" (which ran from 2001 to 2005, and was the most successful weekdaily series, achieving ratings shares of up to 70% of all television households in the country), "Oi Takkoi" (which ran from 2002 to 2005), "S' Agapo" (which ran from 2001 to 2002), "Vasiliki" (which ran from 2005 to 2006), "Vendetta" (which ran from September 2005 to December 2006), "30 kai Kati" (which ran from 2006 to 2007) and "Mila Mou" (which ran from September 2007 to January 2009).

ANT1 Cyprus aired the soap "I Goitia Tis Amartias" in 2002, which was soon canceled. "Dikse Mou To Filo Sou" followed from 2006 to 2009, along with "Gia Tin Agapi Sou", which ran from 2008 to 2009 and itself was followed by "Panselinos", which has aired since 2009.

The longest-running weekly show on Cyprus television is "Istories Tou Horkou" ("Villages Stories", which premiered on CyBC in March 1996 and ran until its cancellation in June 2006; it was revived in September 2010 but was cancelled again in March 2011 due to very low ratings), followed by "Manolis Ke Katina" ("Manolis and Katina", which ran from 1995 to 2004). The most controversial of these series was "To Kafenio" ("The Coffee Shop"), which premiered on CyBC on 1993 as a weekly series, before moving to MEGA Channel Cyprus six years later in 1999 as a weekdaily show and then moved to ANT1 Cyprus on 2000, which canceled the show one year later. There were plans to move the show back to CyBC as a weekly series in 2001, with the original cast, however this plan was never realised. The most successful weekly shows in Cyprus currently are ANT1's "Eleni I Porni" ("Eleni, The Whore"), which premiered in October 2010 and CyBC's "Stin Akri Tu Paradisou" ("At The Heaven's Edge"), which premiered in 2007. The most successful weekdaily soap was "Aigia Fuxia", which aired on ANT1 Cyprus from 2008 to 2010.

The only daily Finnish soap opera so far is "Salatut elämät" ("Secret Lives"), which has achieved popularity in Finland since its 1999 debut on MTV3. It focuses on the lives of people along the imaginary Pihlajakatu street in Helsinki. The show has also spawned several Internet spin-off series and a film based on the show that was released in 2012.

Other soap-like shows in Finland are YLE shows "Uusi päivä" (which has aired since 2009) and "Kotikatu" (which ran from 1995 to 2012), however these programs do not adhere to a five-episode-a-week schedule.

With the advent of internet television and mobile phones, several soap operas have also been produced specifically for these platforms, including "", a spin-off of the established "EastEnders". For those produced only for the mobile phone, episodes may generally consist of about six or seven pictures and accompanying text.

On September 13, 2011, TG4 launched a new 10-part online series titled, "Na Rúin" (an Internet spin-off of "Ros na Rún"). The miniseries took on the theme of a mystery; the viewer had to read Rachel and Lorcán's blogs as well as watch video diaries detailing each character's thoughts to solve the mystery of missing teenage character Ciara.

In motion pictures, the 1982 comedy "Tootsie" has the lead character impersonating a woman in order to gain acting work on a long running television soap opera. Several scenes parody the production of soaps, their outrageous storylines and idiosyncratic stylistic elements.

The 1991 comedy "Soapdish" stars Sally Field as an aging soap opera actress on fictional series "The Sun Also Sets" who pines over her own neuroses and misfortunes such as her live-in boyfriend who leaves her to go back to his wife, and the incidents of backstabbing and scheming behind the scenes, some of which are more interesting than the stories on the program.

Another 1991 comedy, Delirious, stars John Candy as a soap opera writer who, after a head injury, has a dream experience of being in his own creation. The dream experience is an increasingly outrageous exaggeration of soap opera plot elements.

On television, several soap opera parodies have been produced:



</doc>
<doc id="27010" url="https://en.wikipedia.org/wiki?curid=27010" title="Software engineering">
Software engineering

Software engineering is the application of engineering to the development of software in a systematic method.

Notable definitions of software engineering include: 

The term has also been used less formally:

When the first digital computers appeared in the early 1940s, the instructions to make them operate were wired into the machine. Practitioners quickly realized that this design was not flexible and came up with the "stored program architecture" or von Neumann architecture. Thus the division between "hardware" and "software" began with abstraction being used to deal with the complexity of computing.

Programming languages started to appear in the early 1950s and this was also another major step in abstraction. Major languages such as Fortran, ALGOL, and COBOL were released in the late 1950s to deal with scientific, algorithmic, and business problems respectively. David Parnas introduced the key concept of modularity and information hiding in 1972 to help programmers deal with the ever-increasing complexity of software systems.

The origins of the term "software engineering" have been attributed to various sources. The term "software engineering" appeared in a list of services offered by companies in the June 1965 issue of COMPUTERS and AUTOMATION and was used more formally in the August 1966 issue of Communications of the ACM (Volume 9, number 8) “letter to the ACM membership” by the ACM President Anthony A. Oettinger;, it is also associated with the title of a NATO conference in 1968 by Professor F.L. Bauer, the first conference on software engineering.. At the time there was perceived to be a "software crisis".

In 1984, the Software Engineering Institute (SEI) was established as a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States. Watts Humphrey founded the SEI Software Process Program, aimed at understanding and managing the software engineering process. The Process Maturity Levels introduced would become the Capability Maturity Model Integration for Development(CMMi-DEV), which has defined how the US Government evaluates the abilities of a software development team.

Modern, generally accepted best-practices for software engineering have been collected by the ISO/IEC JTC 1/SC 7 subcommittee and published as the Software Engineering Body of Knowledge (SWEBOK).

Software engineering can be divided into sub-disciplines. Some of them are:

Knowledge of computer programming is a prerequisite for becoming a software engineer. In 2004 the IEEE Computer Society produced the SWEBOK, which has been published as ISO/IEC Technical Report 1979:2004, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience.
Many software engineers enter the profession by obtaining a university degree or training at a vocational school. One standard international curriculum for undergraduate software engineering degrees was defined by the Joint Task Force on Computing Curricula of the IEEE Computer Society and the Association for Computing Machinery, and updated in 2014. A number of universities have Software Engineering degree programs; , there were 244 Campus Bachelor of Software Engineering programs, 70 Online programs, 230 Masters-level programs, 41 Doctorate-level programs, and 69 Certificate-level programs in the United States.

In addition to university education, many companies sponsor internships for students wishing to pursue careers in information technology. These internships can introduce the student to interesting real-world tasks that typical software engineers encounter every day. Similar experience can be gained through military service in software engineering.

Legal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer. In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Canada, there is a legal requirement to have P.Eng when one wants to use the title "engineer" or practice "software engineering". In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title.

The United States, since 2013, has offered an "NCEES" "Professional Engineer" exam for Software Engineering, thereby allowing Software Engineers to be licensed and recognized. Mandatory licensing is currently still largely debated, and perceived as controversial. In some parts of the US such as Texas, the use of the term Engineer is regulated by law and reserved only for use by individuals who have a Professional Engineer license.

The IEEE Computer Society and the ACM, the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE's "Guide to the Software Engineering Body of Knowledge - 2004 Version", or SWEBOK, defines the field and describes the knowledge the IEEE expects a practicing software engineer to have. The most current SWEBOK v3 is an updated version and was released in 2014. The IEEE also promulgates a "Software Engineering Code of Ethics".

In November 2004, the U. S. Bureau of Labor Statistics counted 760,840 software engineers holding jobs in the U.S.; in the same time period there were some 1.4 million practitioners employed in the U.S. in all other engineering disciplines combined. Due to its relative newness as a field of study, formal education in software engineering is often taught as part of a computer science curriculum, and many software engineers hold computer science degrees and have no engineering background whatsoever.

Many software engineers work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work for themselves as freelancers. Some organizations have specialists to perform each of the tasks in the software development process. Other organizations require software engineers to do many or all of them. In large projects, people may specialize in only one role. In small projects, people may fill several or all roles at the same time. Specializations include: in industry (analysts, architects, developers, testers, technical support, middleware analysts, managers) and in academia (educators, researchers).

Most software engineers and programmers work 40 hours a week, but about 15 percent of software engineers and 11 percent of programmers worked more than 50 hours a week in 2008. Injuries in these occupations are rare. However, like other workers who spend long periods in front of a computer terminal typing at a keyboard, engineers and programmers are susceptible to eyestrain, back discomfort, and hand and wrist problems such as carpal tunnel syndrome.

The Software Engineering Institute offers certifications on specific topics like security, process improvement and software architecture. IBM, Microsoft and other companies also sponsor their own certification examinations. Many IT certification programs are oriented toward specific technologies, and managed by the vendors of these technologies. These certification programs are tailored to the institutions that would employ people who use these technologies.

Broader certification of general software engineering skills is available through various professional societies. , the IEEE had certified over 575 software professionals as a Certified Software Development Professional (CSDP). In 2008 they added an entry-level certification known as the Certified Software Development Associate (CSDA). The ACM had a professional certification program in the early 1980s, which was discontinued due to lack of interest. The ACM examined the possibility of professional certification of software engineers in the late 1990s, but eventually decided that such certification was inappropriate for the professional industrial practice of software engineering.

In the U.K. the British Computer Society has developed a legally recognized professional certification called "Chartered IT Professional (CITP)", available to fully qualified members ("MBCS"). Software engineers may be eligible for membership of the Institution of Engineering and Technology and so qualify for Chartered Engineer status. In Canada the Canadian Information Processing Society has developed a legally recognized professional certification called "Information Systems Professional (ISP)". In Ontario, Canada, Software Engineers who graduate from a "Canadian Engineering Accreditation Board (CEAB)" accredited program, successfully complete PEO's ("Professional Engineers Ontario") Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the "Professional Engineers Ontario" and can become Professional Engineers P.Eng. The PEO does not recognize any online or distance education however; and does not consider Computer Science programs to be equivalent to software engineering programs despite the tremendous overlap between the two. This has sparked controversy and a certification war. It has also held the number of P.Eng holders for the profession exceptionally low. The vast majority of working professionals in the field hold a degree in CS, not SE. Given the difficult certification path for holders of non-SE degrees, most never bother to pursue the license.

The initial impact of outsourcing, and the relatively lower cost of international human resources in developing third world countries led to a massive migration of software development activities from corporations in North America and Europe to India and later: China, Russia, and other developing countries. This approach had some flaws, mainly the distance / timezone difference that prevented human interaction between clients and developers and the massive job transfer. This had a negative impact on many aspects of the software engineering profession. For example, some students in the developed world avoid education related to software engineering because of the fear of offshore outsourcing (importing software products or services from other countries) and of being displaced by foreign visa workers. Although statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear to have been affected. Nevertheless, the ability to smartly leverage offshore and near-shore resources via the follow-the-sun workflow has improved the overall operational capability of many organizations. When North Americans are leaving work, Asians are just arriving to work. When Asians are leaving work, Europeans are arriving to work. This provides a continuous ability to have human oversight on business-critical processes 24 hours per day, without paying overtime compensation or disrupting a key human resource, sleep patterns.

While global outsourcing has several advantages, global - and generally distributed - development can run into serious difficulties resulting from the distance between developers. This is due to the key elements of this type of distance that have been identified as geographical, temporal, cultural and communication (that includes the use of different languages and dialects of English in different locations). Research has been carried out in the area of global software development over the last 15 years and an extensive body of relevant work published that highlights the benefits and problems associated with the complex activity. As with other aspects of software engineering research is ongoing in this and related areas.

Software engineering is a direct sub-field of engineering and has an overlap with computer science and management science . It is also considered a part of overall systems engineering.

In general, software engineering focuses more on techniques for the application of software development in industry, while computer science focuses more on algorithms and theory.

Software engineering sees its practitioners as individuals who follow well-defined engineering approaches to problem-solving. These approaches are specified in various software engineering books and research papers, always with the connotations of predictability, precision, mitigated risk and professionalism. This perspective has led to calls for licensing, certification and codified bodies of knowledge as mechanisms for spreading the engineering knowledge and maturing the field.

Software craftsmanship has been proposed by a body of software developers as an alternative that emphasizes the coding skills and accountability of the software developers themselves without professionalism or any prescribed curriculum leading to ad-hoc problem-solving (craftmanship) without engineering (lack of predictability, precision, missing risk mitigation, methods are informal and poorly defined). The Software Craftsmanship Manifesto extends the Agile Software Manifesto and draws a metaphor between modern software development and the apprenticeship model of medieval Europe.

Software engineering extends engineering and draws on the engineering model, i.e. engineering process, engineering project management, engineering requirements, engineering design, engineering construction, and engineering validation. The concept is so new that it is rarely understood, and it is widely misinterpreted, including in software engineering textbooks, papers, and among the communities of programmers and crafters.

One of the core issues in software engineering is that its approaches are not empirical enough because a real-world validation of approaches is usually absent, or very limited and hence software engineering is often misinterpreted as feasible only in a "theoretical environment."

Edsger Dijkstra, the founder of many of the concepts used within software development today, refuted the idea of "software engineering" up until his death in 2002, arguing that those terms were poor analogies for what
he called the "radical novelty" of computer science:






</doc>
<doc id="27011" url="https://en.wikipedia.org/wiki?curid=27011" title="Software Engineering Institute">
Software Engineering Institute

The Software Engineering Institute (SEI) is an American research and development center headquartered in Pittsburgh, Pennsylvania. Its activities cover cybersecurity, software assurance, software engineering and acquisition, and component capabilities critical to the Department of Defense.

The Carnegie Mellon Software Engineering Institute is a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States. The SEI also has offices in Washington, DC and Los Angeles, California. The SEI operates with major funding from the U.S. Department of Defense. The SEI also works closely with industry and academia through research collaborations.

On November 14, 1984, the U.S. Department of Defense elected Carnegie Mellon University as the host site of the Software Engineering Institute. The institute was founded with an initial allocation of $6 million, with another $97 million to be allocated in the subsequent five years. The SEI's contract with the Department of Defense is subject to review and renewal every five years.

The SEI program of work is conducted in several principal areas: cybersecurity, software assurance, software engineering and acquisition, and component capabilities critical to the Department of Defense.

The SEI defines specific initiatives aimed at improving organizations' software engineering capabilities.

Organizations need to effectively manage the acquisition, development, and evolution (ADE) of software-intensive systems. Success in software engineering management practices helps organizations predict and control quality, schedule, cost, cycle time, and productivity. The best-known example of SEI in management practices is the SEI’s Capability Maturity Model (CMM) for Software (now Capability Maturity Model Integration (CMMI)). The CMMI approach consists of models, appraisal methods, and training courses that have been proven to improve process performance. In 2006, Version 1.2 of the CMMI Product Suite included the release of CMMI for Development. CMMI for Development was the first of three constellations defined in Version 1.2: the others include CMMI for Acquisition and CMMI for Services. The CMMI for Services constellation was released in February 2009. Another management practice developed by CERT, which is part of the SEI, is the Resilience Management Model (CERT-RMM). The CERT-RMM is a capability model for operational resilience management. Version 1.0 of the Resilience Management Model was released in May 2010.

SEI work in engineering practices increases the ability of software engineers to analyze, predict, and control selected
functional and non-functional properties of software systems. Key SEI tools and methods include the SEI Architecture Tradeoff Analysis Method (ATAM) method, the SEI Framework for Software Product Line Practice, and the SEI Service Migration and Reuse Technique (SMART).

The goal of SEI work is to improve organizations acquisition processes.

The SEI is also the home of the CERT/CC (CERT Coordination Center), a federally funded computer security organization. The SEI CERT Program's primary goals are to ensure that appropriate technology and systems-management practices are used to resist attacks on networked systems and to limit damage and ensure continuity of critical services in spite of successful attacks, accidents, or failures. The SEI CERT program is working with US-CERT to produce the Build Security In (BSI) website, which provides guidelines for building security into every phase of the software development lifecycle. The SEI has also conducted research on insider threats and computer forensics. Results of this research and other information now populate the CERT Virtual Training Environment.

Carnegie Mellon, Capability Maturity Model, CMM, CMMI, Architecture Tradeoff Analysis Method, ATAM, and CERT are registered in the U.S. Patent and Trademark Office by Carnegie Mellon University.

The SEI Partner Network helps the SEI disseminate software engineering best practices. Organizations and individuals in the SEI Partner Network are selected, trained, and licensed by the SEI to deliver authentic SEI services, which include courses, consulting methods, and management processes. The network currently consists of nearly 250 partner organizations worldwide.

The SEI sponsors national and international conferences, workshops, and user-group meetings. Other events cover subjects including acquisition of software-intensive systems, commercial off-the-shelf (COTS)-based systems, network security and survivability, software process research, software product lines, CMMI, and the SEI Team Software Process.

SEI courses help bring state-of-the-art technologies and practices from research and development into widespread use. SEI courses are currently offered at the SEI’s locations in the United States and Europe. In addition, using licensed course materials, SEI Partners train thousands of individuals annually.

The SEI Membership Program helps the software engineering community to network. SEI Members include small business owners, software and systems programmers, CEOs, directors, and managers from both Fortune 500 companies and government organizations

Through the SEI Affiliate Program, organizations place technical experts with the SEI for periods ranging from 12 months to four years. Affiliates currently are working on projects with the SEI to identify, develop, and demonstrate improved software engineering practices.

In order to recognize outstanding achievement in improving an organization's ability to create and evolve software-dependent systems, the SEI and IEEE Computer Society created the Software Process Achievement Award program. In addition to rewarding excellence, the purpose of this award is to foster continuous advancement in the practice of software engineering and to disseminate insights, experiences, and proven practices throughout the relevant research and practitioner communities.

The SEI publishes reports that offer new technical information about software engineering topics, whether theoretical or applied. The SEI also publishes books on software engineering for industry, government and military applications and practices.

In addition, the SEI offers public courses, workshops, and conferences in process improvement, software architecture and product lines, and security.

On 11 November 2015, the Software Engineering Institute was accused of aiding Federal Bureau of Investigation in uncovering the identities of users of the Tor network. Later prosecution showed the hack was paid for by the Department of Defense and subpoena by the FBI.

SEI has been an occasional site of anti-war movement and peace movement protests, many of which have been organized by Pittsburgh's Thomas Merton Center.



</doc>
<doc id="27012" url="https://en.wikipedia.org/wiki?curid=27012" title="Software crisis">
Software crisis

Software crisis is a term used in the early days of computing science for the difficulty of writing useful and efficient computer programs in the required time. The software crisis was due to the rapid increases in computer power and the complexity of the problems that could not be tackled. With the increase in the complexity of the software, many software problems arose because existing methods were insufficient.

The term "software crisis" was coined by some attendees at the first NATO Software Engineering Conference in 1968 at Garmisch, Germany. Edsger Dijkstra's 1972 ACM Turing Award Lecture makes reference to this same problem:
The causes of the software crisis were linked to the overall complexity of hardware and the software development process. The crisis manifested itself in several ways:

The main cause is that improvements in computing power had outpaced the ability of programmers to effectively utilize those capabilities. Various processes and methodologies have been developed over the last few decades to improve software quality management such as procedural programming and object-oriented programming. However software projects that are large, complicated, poorly specified, and involve unfamiliar aspects, are still vulnerable to large, unanticipated problems.




</doc>
<doc id="27013" url="https://en.wikipedia.org/wiki?curid=27013" title="Swedish Academy">
Swedish Academy

The Swedish Academy (), founded in 1786 by King Gustav III, is one of the Royal Academies of Sweden. It has 18 members, who are elected for life. The academy makes the annual decision on who will be the laureate for the Nobel Prize in Literature, awarded in memory of the donor Alfred Nobel.

The Swedish Academy was founded in 1786 by King Gustav III. Modelled after the Académie française, it has 18 members. The motto of the Academy is "Talent and Taste" (""Snille och Smak"" in Swedish). The primary purpose of the Academy is to further the "purity, strength, and sublimity of the Swedish language" (""Svenska Språkets renhet, styrka och höghet"") (Walshe, 1965). To that end the Academy publishes two dictionaries. The first is a one-volume glossary called "Svenska Akademiens ordlista" ("SAOL"). The second is a multi-volume dictionary, edited on principles similar to those of the "Oxford English Dictionary", entitled "Svenska Akademiens Ordbok" ("SAOB"). The "SAOL" has reached its 14th edition while the first volume of the "SAOB" was published in 1898 and, as of 2017, work has progressed to words beginning with the letter "V".

The building now known as the Stockholm Stock Exchange Building was built for the bourgeoisie. The bottom floor was used as a trading exchange (this later became the stock exchange) and the upper floor was used for balls, New Year's Eve parties, etc. When the academy was founded, the ballroom was the biggest room in Stockholm that could be heated and thus used in the winter, so the King asked if he could borrow it.

The academy has had its annual meeting there every year since, attended by members of the Swedish royal family. However, it was not until 1914 the academy gained the right to use the upper floor as their own for all eternity. It is here that the Academy meets and, amongst other business, announces the names of Nobel Prize laureates. The latter makes it arguably one of the most influential literary bodies in the world.

Dag Hammarskjöld's former farm at Backåkra, close to Ystad in southern Sweden, was bought in 1957 as a summer residence by Hammarskjöld, then Secretary-General of the United Nations (1953–1961). The south wing of the farm is reserved as a summer retreat for the 18 members of the Swedish Academy, of which Hammarskjöld was a member.

Prior to 2018 it was not possible for members of the Academy to resign; membership was for life, although the Academy could decide to exclude members – this happened twice to Gustaf Mauritz Armfelt who was excluded in 1794, re-elected in 1805, and excluded again in 1811. In 1989, Werner Aspenström, Kerstin Ekman and Lars Gyllensten chose to stop participating in the meetings of the Academy, over its refusal to express support for Salman Rushdie when Ayatollah Khomeini condemned him to death for "The Satanic Verses", and in 2005, Knut Ahnlund made the same decision, as a protest against the choice of Elfride Jelinek as the Nobel laureate for 2004. On 25 November 2017, Lotta Lotass said in an interview that she has not participated in the meetings of the Academy for more than two years and does not consider herself a member any more.

In April 2018, three members of the academy board resigned in response to a sexual-misconduct investigation involving author Jean-Claude Arnault, husband of board member Katarina Frostenson. Arnault was accused by at least 18 women of sexual assault and harassment; he denied all accusations. The three members resigned in protest over the decision by Sara Danius, the board secretary, to not take what they felt was appropriate legal action against Arnault. Two former permanent secretaries, Sture Allén and Horace Engdahl, called Danius a weak leader.

On 10 April, Danius resigned from her position by the Academy, bringing the number of empty seats to four. Frostenson voluntarily agreed to withdraw from participating in the academy, bringing the total of withdraws to five. Because two other seats were still vacant from the Rushdie affair, this left only 11 active members. The scandal was widely seen as damaging to the credibility of the Nobel prize in Literature and the authority of the academy. "With this scandal you cannot possibly say that this group of people has any kind of solid judgment," noted Swedish journalist Björn Wiman.

On 27 April 2018, the Swedish Economic Crime Authority opened a preliminary investigation regarding financial crime linked to the Academy.

On 2 May 2018, the Swedish King amended the rules of the academy and made it possible for members to resign. The new rules also states that a member who has been inactive in the work of the academy for more than two years, can be asked to resign. Following the new rules, the first members to formally be granted permission to leave the Academy and vacating their chairs were Kerstin Ekman, Klas Östergren, Sara Stridsberg and Lotta Lotass.

On 4 May 2018, the Swedish Academy announced that following the preceding internal struggles the Nobel laureate for literature selected in 2018 will be postponed until 2019, when two laureates will be selected.

Since 1901, the Academy has annually decided who will be the laureate for the Nobel Prize in Literature, awarded in memory of the donor Alfred Nobel.

The Swedish Academy annually awards nearly 50 different prizes and scholarships, most of them for domestic Swedish authors. Common to all is that they are awarded without competition and without application. The Dobloug Prize, the largest of these at $40,000, is a literature prize awarded for Swedish and Norwegian fiction.

Swedish: Stora Priset, literally the Big Prize, was instituted by King Gustav III. The prize, which consists of a single gold medal, is the most prestigious award that can be awarded by the Swedish Academy. It has been awarded to, among others, Selma Lagerlöf (1904 and 1909), Herbert Tingsten (1966), Astrid Lindgren (1971), Evert Taube (1972) and Tove Jansson (1994).

The Academy awards around 50 prizes each year. A person does not have to apply nor compete for the prizes.

Full list of awards (in Swedish)

The previous permanent secretary of the Academy was Sara Danius, who was preceded by Peter Englund. Danius stepped down in 2018 following a period of internal conflicts. The current members of the Swedish Academy listed by seat number:





</doc>
<doc id="27014" url="https://en.wikipedia.org/wiki?curid=27014" title="Svenska Dagbladet">
Svenska Dagbladet

Svenska Dagbladet (, "The Swedish Daily News"), abbreviated SvD, is a daily newspaper published in Stockholm, Sweden.

The first issue of "Svenska Dagbladet" appeared on 18 December 1884. Ivar Anderson is among its former editors-in-chief who assumed the post in 1940.

The paper is published in Stockholm and provides coverage of national and international news as well as local coverage of the Greater Stockholm region. Its subscribers are concentrated in the capital, but it is distributed in most of Sweden. During the beginning of the 1900s the paper was one of the right-wing publications in Stockholm.

"Svenska Dagbladet" is owned by Schibsted which purchased it in the late 1990s. The stated position of the editorial page is "independently moderate" ("oberoende moderat"), which means it is independent but adheres to the liberal conservatism of the Moderate Party. On the other hand, the paper is also regarded as conservative.

In November 2000 "Svenska Dagbladet" changed its format from broadsheet to tabloid. In 2005 the paper started a Web portal for business news as a joint venture with "Aftonbladet".

Since 1925 "Svenska Dagbladet" has awarded an individual sportsperson or a team the Svenska Dagbladet Gold Medal at the end of each year.

The circulation of "Svenska Dagbladet" was 185,000 copies in 2003. The paper had a circulation of 187,100 copies on weekdays in 2005. Among Swedish morning newspapers "Svenska Dagbladet" had the third largest circulation with 195,200 copies in 2007 after "Dagens Nyheter" and "Göteborgs-Posten". In 2008 "Svenska Dagbladet" had a circulation of 123,383 copies. The circulation of the paper was 185,600 copies in 2011. It was 159,600 copies in 2012 and 143,400 copies in 2013.






</doc>
<doc id="27016" url="https://en.wikipedia.org/wiki?curid=27016" title="Sture Allén">
Sture Allén

Sture Allén (born 31 December 1928 in Gothenburg) is a Swedish retired professor of computational linguistics at the University of Gothenburg, who was the permanent secretary of the Swedish Academy between 1986 and 1999. He was elected to chair 3 of the Swedish Academy in 1980. He is also a member of the Norwegian Academy of Science and Letters.


 


</doc>
<doc id="27018" url="https://en.wikipedia.org/wiki?curid=27018" title="Stress">
Stress

Stress may refer to:










</doc>
<doc id="27019" url="https://en.wikipedia.org/wiki?curid=27019" title="South Korea">
South Korea

South Korea, officially the Republic of Korea is a country in East Asia, constituting the southern part of the Korean Peninsula and lying east to the Asian mainland.The name "Korea" is derived from Goguryeo which was one of the great powers in East Asia during its time, ruling most of the Korean Peninsula, Manchuria, parts of the Russian Far East and Inner Mongolia, under Gwanggaeto the Great. South Korea lies in the north temperate zone and has a predominantly mountainous terrain. It comprises an estimated 51.4 million residents distributed over . The capital and largest city is Seoul, with a population of 10 million.

Archaeology indicates that the Korean Peninsula was inhabited by early humans starting from the Lower Paleolithic period (2.6 Ma–300 Ka). The history of Korea begins with the foundation of Gojoseon in 2333 BC by the legendary king Dangun. Following the unification of the Three Kingdoms of Korea under Unified Silla in AD 668, Korea was subsequently ruled by the Goryeo dynasty (918–1392) and the Joseon dynasty (1392–1910). It was annexed by the Empire of Japan in 1910. At the end of World War II, Korea was divided into Soviet and U.S. zones of occupations. A separate election was held in the U.S. zone in 1948 which led to the creation of the Republic of Korea (ROK), while the Democratic People's Republic of Korea (DPRK) was established in the Soviet zone. The United Nations at the time passed a resolution declaring the ROK to be the only lawful government in Korea.

The Korean War began in 1950 when forces from the North invaded the South. The war lasted three years and involved the U.S., China, the Soviet Union and several other nations. The border between the two nations remains the most heavily fortified in the world. Under long-time military leader Park Chung-hee, the South Korean economy grew significantly and the country was transformed into a G-20 major economy. Military rule ended in 1987, and the country is now a presidential republic consisting of 17 administrative divisions.

South Korea is a developed country and a high-income economy, with a "very high" Human Development Index, ranking 18th in the world. The country is considered a regional power and is the world's 11th largest economy by nominal GDP and the 12th largest by PPP as of 2010. South Korea is a global leader in the industrial and technological sectors, being the world's 5th largest exporter and 8th largest importer. Its export-driven economy primarily focuses production on electronics, automobiles, ships, machinery, petrochemicals and robotics. South Korea is a member of the ASEAN Plus mechanism, the United Nations, Uniting for Consensus, G20, the WTO and OECD and is a founding member of APEC and the East Asia Summit.

The name "Korea" derives from the name "Goryeo". The name "Goryeo" itself was first used by the ancient kingdom of Goguryeo in the 5th century as a shortened form of its name. The 10th-century kingdom of Goryeo succeeded Goguryeo, and thus inherited its name, which was pronounced by visiting Persian merchants as "Korea". The modern spelling of Korea first appeared in the late 17th century in the travel writings of the Dutch East India Company's Hendrick Hamel. Despite the coexistence of the spellings "Corea" and "Korea" in 19th century publications, some Koreans believe that Imperial Japan, around the time of the Japanese occupation, intentionally standardised the spelling on "Korea", making Japan appear first alphabetically.

After Goryeo was replaced by Joseon in 1392, Joseon became the official name for the entire territory, though it was not universally accepted. The new official name has its origin in the ancient country of Gojoseon (Old Joseon). In 1897, the Joseon dynasty changed the official name of the country from "Joseon" to "Daehan Jeguk" (Korean Empire). The name "Daehan", which means "Great Han" literally, derives from Samhan (Three Hans), referring to the Three Kingdoms of Korea, not the ancient confederacies in the southern Korean Peninsula. However, the name "Joseon" was still widely used by Koreans to refer to their country, though it was no longer the official name. Under Japanese rule, the two names "Han" and "Joseon" coexisted. There were several groups who fought for independence, the most notable being the "Provisional Government of the Republic of Korea" (/).

Following the surrender of Japan, in 1945, the "Republic of Korea" (/, IPA: , literally "Great Korean people's country"; ) was adopted as the legal English name for the new country. Since the government only controlled the southern part of the Korean Peninsula, the informal term "South Korea" was coined, becoming increasingly common in the Western world. While South Koreans use "Han" (or "Hanguk") to refer to the entire country, North Koreans and ethnic Koreans living in China and Japan use the term "Joseon" as the name of the country. The Korean name "Daehan Minguk" is sometimes used by South Koreans as a metonym to refer to the Korean ethnicity (or "race") as a whole, rather than just the South Korean state.

The history of Korea begins with the founding of Joseon (also known as "Gojoseon", or Old Joseon, to differentiate it with the 14th century dynasty) in 2333 BC by Dangun, according to Korea's foundation mythology. Gojoseon expanded until it controlled the northern Korean Peninsula and parts of Manchuria. Gija Joseon was purportedly founded in the 12th century BC, but its existence and role have been controversial in the modern era. In 108 BC, the Han dynasty defeated Wiman Joseon and installed four commanderies in the northern Korean peninsula. Three of the commanderies fell or retreated westward within a few decades. As Lelang commandery was destroyed and rebuilt around this time, the place gradually moved toward Liadong. Thus, its force was diminished and it only served as a trade center until it was conquered by Goguryeo in 313.

During the period known as the Proto–Three Kingdoms of Korea, the states of Buyeo, Okjeo, Dongye and Samhan occupied the whole Korean peninsula and southern Manchuria. From them, Goguryeo, Baekje and Silla emerged to control the peninsula as the Three Kingdoms of Korea. Goguryeo, the largest and most powerful among them, was a highly militaristic state, and competed with various Chinese dynasties during its 700 years of history. Goguryeo experienced a golden age under Gwanggaeto the Great and his son Jangsu, who both subdued Baekje and Silla during their times, achieving a brief unification of the Three Kingdoms of Korea and becoming the most dominant power on the Korean Peninsula. In addition to contesting for control of the Korean Peninsula, Goguryeo had many military conflicts with various Chinese dynasties, most notably the Goguryeo–Sui War, in which Goguryeo defeated a huge force said to number over a million men. Baekje was a great maritime power; its nautical skill, which made it the Phoenicia of East Asia, was instrumental in the dissemination of Buddhism throughout East Asia and continental culture to Japan. Baekje was once a great military power on the Korean Peninsula, especially during the time of Geunchogo, but was critically defeated by Gwanggaeto the Great and declined. Silla was the smallest and weakest of the three, but it used cunning diplomatic means to make opportunistic pacts and alliances with the more powerful Korean kingdoms, and eventually Tang China, to its great advantage.

The unification of the Three Kingdoms by Silla in 676 led to the North South States Period, in which much of the Korean Peninsula was controlled by Later Silla, while Balhae controlled the northern parts of Goguryeo. Balhae was founded by a Goguryeo general and formed as a successor state to Goguryeo. During its height, Balhae controlled most of Manchuria and parts of the Russian Far East, and was called the "Prosperous Country in the East". Later Silla was a golden age of art and culture, as evidenced by the Hwangnyongsa, Seokguram, and Emille Bell. Relationships between Korea and China remained relatively peaceful during this time. Later Silla carried on the maritime prowess of Baekje, which acted like the Phoenicia of medieval East Asia, and during the 8th and 9th centuries dominated the seas of East Asia and the trade between China, Korea and Japan, most notably during the time of Jang Bogo; in addition, Silla people made overseas communities in China on the Shandong Peninsula and the mouth of the Yangtze River. Later Silla was a prosperous and wealthy country, and its metropolitan capital of Gyeongju was the fourth largest city in the world. Buddhism flourished during this time, and many Korean Buddhists gained great fame among Chinese Buddhists and contributed to Chinese Buddhism, including: Woncheuk, Wonhyo, Uisang, Musang, and Kim Gyo-gak, a Silla prince whose influence made Mount Jiuhua one of the Four Sacred Mountains of Chinese Buddhism. However, Later Silla weakened under internal strife and the revival of Baekje and Goguryeo, which led to the Later Three Kingdoms period in the late 9th century.

In 936, the Later Three Kingdoms were united by Wang Geon, a descendant of Goguryeo nobility, who established Goryeo as the successor state of Goguryeo. Balhae had fallen to the Khitan Empire in 926, and a decade later the last crown prince of Balhae fled south to Goryeo, where he was warmly welcomed and included into the ruling family by Wang Geon, thus unifying the two successor nations of Goguryeo. Like Silla, Goryeo was a highly cultural state, and invented the metal movable type printing press. After defeating the Khitan Empire, which was the most powerful empire of its time, in the Goryeo–Khitan War, Goryeo experienced a golden age that lasted a century, during which the Tripitaka Koreana was completed and there were great developments in printing and publishing, promoting learning and dispersing knowledge on philosophy, literature, religion, and science; by 1100, there were 12 universities that produced famous scholars and scientists. However, the Mongol invasions in the 13th century greatly weakened the kingdom. Goryeo was never conquered by the Mongols, but exhausted after three decades of fighting, the Korean court sent its crown prince to the Yuan capital to swear allegiance to Kublai Khan, who accepted, and married one of his daughters to the Korean crown prince. Henceforth, Goryeo continued to rule Korea, though as a tributary ally to the Mongols for the next 86 years. During this period, the two nations became intertwined as all subsequent Korean kings married Mongol princesses, and the last empress of the Yuan dynasty was a Korean princess. In the mid-14th century, Goryeo drove out the Mongols to regain its northern territories, briefly conquered Liaoyang, and defeated invasions by the Red Turbans. However, in 1392, General Yi Seong-gye, who had been ordered to attack China, turned his army around and staged a coup.

Yi Seong-gye declared the new name of Korea as "Joseon" in reference to Gojoseon, and moved the capital to Hanseong (one of the old names of Seoul). The first 200 years of the Joseon dynasty were marked by peace, and saw great advancements in science and education, as well as the creation of Hangul by Sejong the Great to promote literacy among the common people. The prevailing ideology of the time was Neo-Confucianism, which was epitomized by the seonbi class: nobles who passed up positions of wealth and power to lead lives of study and integrity. Between 1592 and 1598, Toyotomi Hideyoshi launched invasions of Korea, but his advance was halted by Korean forces (most notably the Joseon Navy led by Admiral Yi Sun-sin and his renowned "turtle ship") with assistance from Righteous Army militias formed by Korean civilians, and Ming dynasty Chinese troops. Through a series of successful battles of attrition, the Japanese forces were eventually forced to withdraw, and relations between all parties became normalized. However, the Manchus took advantage of Joseon's war-weakened state and invaded in 1627 and 1637, and then went on to conquer the destabilized Ming dynasty. After normalizing relations with the new Qing dynasty, Joseon experienced a nearly 200-year period of peace. Kings Yeongjo and Jeongjo particularly led a new renaissance of the Joseon dynasty during the 18th century. In the 19th century, the royal in-law families gained control of the government, leading to mass corruption and weakening of the state, and severe poverty and peasant rebellions throughout the country. Furthermore, the Joseon government adopted a strict isolationist policy, earning the nickname "the hermit kingdom", but ultimately failed to protect itself against imperialism and was forced to open its borders. After the First Sino-Japanese War and the Russo-Japanese War, Korea was occupied by Japan (1910–45). At the end of World War II, the Japanese surrendered to Soviet and U.S. forces who occupied the northern and southern halves of Korea, respectively.

Despite the initial plan of a unified Korea in the 1943 Cairo Declaration, escalating Cold War antagonism between the Soviet Union and the United States eventually led to the establishment of separate governments, each with its own ideology, leading to the division of Korea into two political entities in 1948: North Korea and South Korea. In the South, Syngman Rhee, an opponent of communism, who had been backed and appointed by the United States as head of the provisional government, won the first presidential elections of the newly declared Republic of Korea in May. In the North, however, a former anti-Japanese guerrilla and communist activist, Kim Il-sung was appointed premier of the Democratic People's Republic of Korea in September.

In October the Soviet Union declared Kim Il-sung's government as sovereign over both parts. The UN declared Rhee's government as "a lawful government having effective control and jurisdiction over that part of Korea where the UN Temporary Commission on Korea was able to observe and consult" and the Government "based on elections which was observed by the Temporary Commission" in addition to a statement that "this is the only such government in Korea." Both leaders began an authoritarian repression of their political opponents inside their region, seeking for a unification of Korea under their control. While South Korea's request for military support was denied by the United States, North Korea's military was heavily reinforced by the Soviet Union.

On June 25, 1950, North Korea invaded South Korea, sparking the Korean War, the Cold War's first major conflict, which continued until 1953. At the time, the Soviet Union had boycotted the United Nations (UN), thus forfeiting their veto rights. This allowed the UN to intervene in a civil war when it became apparent that the superior North Korean forces would unify the entire country. The Soviet Union and China backed North Korea, with the later participation of millions of Chinese troops. After an ebb and flow that saw both sides almost pushed to the brink of extinction, and massive losses among Korean civilians in both the north and the south, the war eventually reached a stalemate. The 1953 armistice, never signed by South Korea, split the peninsula along the demilitarized zone near the original demarcation line. No peace treaty was ever signed, resulting in the two countries remaining technically at war. Over 1.2 million people died during the Korean War.

In 1960, a student uprising (the "April 19 Revolution") led to the resignation of the autocratic President Syngman Rhee. A period of political instability followed, broken by General Park Chung-hee's May 16 coup against the weak and ineffectual government the next year. Park took over as president until his assassination in 1979, overseeing rapid export-led economic growth as well as implementing political repression. Park was heavily criticised as a ruthless military dictator, who in 1972 extended his rule by creating a new constitution, which gave the president sweeping (almost dictatorial) powers and permitted him to run for an unlimited number of six-year terms. However, the Korean economy developed significantly during Park's tenure and the government developed the nationwide expressway system, the Seoul subway system, and laid the foundation for economic development during his 17-year tenure.

The years after Park's assassination were marked again by political turmoil, as the previously suppressed opposition leaders all campaigned to run for president in the sudden political void. In 1979 there came the Coup d'état of December Twelfth led by General Chun Doo-hwan. Following the Coup d'état, Chun Doo-hwan planned to rise to power through several measures. On May 17, Chun Doo-hwan forced the Cabinet to expand martial law to the whole nation, which had previously not applied to the island of Jejudo. The expanded martial law closed universities, banned political activities and further curtailed the press. Chun's assumption of the presidency in the events of May 17, triggered nationwide protests demanding democracy, in particular in the city of Gwangju, to which Chun sent special forces to violently suppress the Gwangju Democratization Movement.

Chun subsequently created the National Defense Emergency Policy Committee and took the presidency according to his political plan. Chun and his government held South Korea under a despotic rule until 1987, when a Seoul National University student, Park Jong-chul, was tortured to death. On , the Catholic Priests Association for Justice revealed the incident, igniting the June Democracy Movement around the country. Eventually, Chun's party, the Democratic Justice Party, and its leader, Roh Tae-woo announced the 6.29 Declaration, which included the direct election of the president. Roh went on to win the election by a narrow margin against the two main opposition leaders, Kim Dae-Jung and Kim Young-Sam. Seoul hosted the Olympic Games in 1988, widely regarded as successful and a significant boost for South Korea's global image and economy.

South Korea was formally invited to become a member of the United Nations in 1991. The transition of Korean from autocracy to modern democracy was marked in 1997 by the election of Kim Dae-jung, who was sworn in as the eighth president of South Korea, on February 25, 1998. His election was significant given that he had in earlier years been a political prisoner sentenced to death (later commuted to exile). He won against the backdrop of the 1997 Asian Financial Crisis, where he took IMF advice to restructure the economy and the nation soon recovered its economic growth, albeit at a slower pace.

In June 2000, as part of president Kim Dae-jung's "Sunshine Policy" of engagement, a North–South summit took place in Pyongyang, the capital of North Korea. Later that year, Kim received the Nobel Peace Prize "for his work for democracy and human rights in South Korea and in East Asia in general, and for peace and reconciliation with North Korea in particular". However, because of discontent among the population for fruitless approaches to the North under the previous administrations and, amid North Korean provocations, a conservative government was elected in 2007 led by President Lee Myung-bak, former mayor of Seoul. Meanwhile, South Korea and Japan jointly co-hosted the 2002 FIFA World Cup. However, South Korean and Japanese relations later soured because of conflicting claims of sovereignty over the Liancourt Rocks.

In 2010, there was an escalation in attacks by North Korea. In March 2010 the South Korean warship ROKS Cheonan was sunk with the loss of 46 South Korean sailors, allegedly by a North Korean submarine. In November 2010 Yeonpyeong island was attacked by a significant North Korean artillery barrage, with 4 people losing their lives. The lack of a strong response to these attacks from both South Korea and the international community (the official UN report declined to explicitly name North Korea as the perpetrator for the Cheonan sinking) caused significant anger with the South Korean public. South Korea saw another milestone in 2012 with the first ever female president Park Geun-hye elected and assuming office. Daughter of another former president, Park Chung-hee, she carried on a conservative brand of politics. President Park Geun-hye's administration was formally accused of corruption, bribery, and influence-peddling for the involvement of close friend Choi Soon-sil in state affairs. There followed a series of massive public demonstrations from November 2016 and she was removed from office. After the fallout of President Park's impeachment and dismissal, new elections were held and Moon Jae-in of the Democratic party won the presidency, assuming office on the 10th May 2017. His tenure so far has seen an improving political relationship with North Korea, some increasing divergence in the military alliance with the United States, and the successful hosting of the Winter Olympics in Pyeongchang.

South Korea occupies the southern portion of the Korean Peninsula, which extends some from the Asian mainland. This mountainous peninsula is flanked by the Yellow Sea to the west, and the Sea of Japan to the east. Its southern tip lies on the Korea Strait and the East China Sea.

The country, including all its islands, lies between latitudes 33° and 39°N, and longitudes 124° and 130°E. Its total area is .

South Korea can be divided into four general regions: an eastern region of high mountain ranges and narrow coastal plains; a western region of broad coastal plains, river basins, and rolling hills; a southwestern region of mountains and valleys; and a southeastern region dominated by the broad basin of the Nakdong River.

South Korea's terrain is mostly mountainous, most of which is not arable. Lowlands, located primarily in the west and southeast, make up only 30% of the total land area.

About three thousand islands, mostly small and uninhabited, lie off the western and southern coasts of South Korea. Jeju-do is about off the southern coast of South Korea. It is the country's largest island, with an area of . Jeju is also the site of South Korea's highest point: Hallasan, an extinct volcano, reaches above sea level. The easternmost islands of South Korea include Ulleungdo and Liancourt Rocks (Dokdo/Takeshima), while Marado and Socotra Rock are the southernmost islands of South Korea.

South Korea has 20 national parks and popular nature places like the Boseong Tea Fields, Suncheon Bay Ecological Park, and the first national park of Jirisan.

South Korea tends to have a humid continental climate and a humid subtropical climate, and is affected by the East Asian monsoon, with precipitation heavier in summer during a short rainy season called "jangma" (), which begins end of June through the end of July. Winters can be extremely cold with the minimum temperature dropping below in the inland region of the country: in Seoul, the average January temperature range is , and the average August temperature range is . Winter temperatures are higher along the southern coast and considerably lower in the mountainous interior. Summer can be uncomfortably hot and humid, with temperatures exceeding in most parts of the country. South Korea has four distinct seasons; spring, summer, autumn and winter. Spring usually lasts from late March to early May, summer from mid-May to early September, autumn from mid-September to early November, and winter from mid-November to mid-March.

Rainfall is concentrated in the summer months of June through September. The southern coast is subject to late summer typhoons that bring strong winds, heavy rains and sometime floods. The average annual precipitation varies from in Seoul to in Busan.

During the first 20 years of South Korea's growth surge, little effort was made to preserve the environment. Unchecked industrialization and urban development have resulted in deforestation and the ongoing destruction of wetlands such as the Songdo Tidal Flat. However, there have been recent efforts to balance these problems, including a government run five-year green growth project that aims to boost energy efficiency and green technology.

The green-based economic strategy is a comprehensive overhaul of South Korea's economy, utilizing nearly two percent of the national GDP. The greening initiative includes such efforts as a nationwide bike network, solar and wind energy, lowering oil dependent vehicles, backing daylight savings and extensive usage of environmentally friendly technologies such as LEDs in electronics and lighting. The country – already the world's most wired – plans to build a nationwide next-generation network that will be 10 times faster than broadband facilities, in order to reduce energy usage.

The renewable portfolio standard program with renewable energy certificates runs from 2012 to 2022.
Quota systems favor large, vertically integrated generators and multinational electric utilities, if only because certificates are generally denominated in units of one megawatt-hour. They are also more difficult to design and implement than a Feed-in tariff. Around 350 residential micro combined heat and power units were installed in 2012.

Seoul's tap water recently became safe to drink, with city officials branding it "Arisu" in a bid to convince the public. Efforts have also been made with afforestation projects. Another multibillion-dollar project was the restoration of Cheonggyecheon, a stream running through downtown Seoul that had earlier been paved over by a motorway. One major challenge is air quality, with acid rain, sulfur oxides, and annual yellow dust storms being particular problems. It is acknowledged that many of these difficulties are a result of South Korea's proximity to China, which is a major air polluter.

South Korea is a member of the Antarctic-Environmental Protocol, Antarctic Treaty, Biodiversity Treaty, Kyoto Protocol (forming the Environmental Integrity Group (EIG), regarding UNFCCC, with Mexico and Switzerland), Desertification, Endangered Species, Environmental Modification, Hazardous Wastes, Law of the Sea, Marine Dumping, Comprehensive Nuclear-Test-Ban Treaty (not into force), Ozone Layer Protection, Ship Pollution, Tropical Timber 83, Tropical Timber 94, Wetlands, and Whaling.

Under its current constitution the state is sometimes referred to as the Sixth Republic of South Korea. Like many democratic states, South Korea has a government divided into three branches: executive, judicial, and legislative. The executive and legislative branches operate primarily at the national level, although various ministries in the executive branch also carry out local functions. Local governments are semi-autonomous, and contain executive and legislative bodies of their own. The judicial branch operates at both the national and local levels. South Korea is a constitutional democracy.
The South Korean government's structure is determined by the Constitution of the Republic of Korea. This document has been revised several times since its first promulgation in 1948 at independence. However, it has retained many broad characteristics and with the exception of the short-lived Second Republic of South Korea, the country has always had a presidential system with an independent chief executive. The first direct election was also held in 1948. Although South Korea experienced a series of military dictatorships from the 1960s up until the 1980s, it has since developed into a successful liberal democracy. Today, the CIA World Factbook describes South Korea's democracy as a "fully functioning modern democracy". South Korea is ranked 37th on the Corruption Perceptions Index, with moderate control on corruption.

The major administrative divisions in South Korea are eight provinces, one special self-governing province, six metropolitan cities (self-governing cities that are not part of any province), one special city and one metropolitan autonomous city.

In April 2016, South Korea's population was estimated to be around 50.8 million by National Statistical Office, with continuing decline of working age population and total fertility rate. The country is noted for its population density, which was an estimated 505 per square kilometer in 2015, more than 10 times the global average. Most South Koreans live in urban areas, because of rapid migration from the countryside during the country's quick economic expansion in the 1970s, 1980s and 1990s. The capital city of Seoul is also the country's largest city and chief industrial center. According to the 2005 census, Seoul had a population of inhabitants. The Seoul National Capital Area has inhabitants (about half of South Korea's entire population) making it the world's second largest metropolitan area. Other major cities include Busan (), Incheon (), Daegu (), Daejeon (), Gwangju () and Ulsan ().
The population has also been shaped by international migration. After World War II and the division of the Korean Peninsula, about four million people from North Korea crossed the border to South Korea. This trend of net entry reversed over the next 40 years because of emigration, especially to the United States and Canada. South Korea's total population in 1955 was , and has more than doubled, to 50 million, by 2010.

South Korea is considered one of the most ethnically homogeneous societies in the world with ethnic Koreans representing approximately 96% of total population. Precise numbers are difficult since statistics do not record ethnicity and given many immigrants are ethnically Korean themselves, whilst some Korean citizens are not ethnically Korean. South Korea is nevertheless becoming a more multi-ethnic society over time due to immigration.

The percentage of foreign nationals has been growing rapidly. , South Korea had 1,413,758 foreign residents, 2.75% of the population; however, many of them are ethnic Koreans with a foreign citizenship. For example, migrants from China (PRC) make up 56.5% of foreign nationals, but approximately 70% of the Chinese citizens in Korea are (), PRC citizens of Korean ethnicity. Regardless of the ethnicity, there are 28,500 US military personnel serving in South Korea, most serving a one-year unaccompanied tour (though approximately 10% serve longer tours accompanied by family), according to the Korea National Statistical Office. In addition, about 43,000 English teachers from English-speaking countries reside temporarily in Korea. Currently, South Korea has one of the highest rates of growth of foreign born population, with about 30,000 foreign born residents obtaining South Korean citizenship every year since 2010.

South Korea's birthrate was the world's lowest in 2009. If this continues, its population is expected to decrease by 13% to in 2050. South Korea's annual birthrate is approximately 9 births per 1000 people. However the birthrate has increased by 5.7% since 2010 and Korea no longer has the world's lowest birthrate. According to a 2011 report from "The Chosun Ilbo", South Korea's total fertility rate (1.23 children born per woman) is higher than those of Taiwan (1.15) and Japan (1.21). The average life expectancy in 2008 was 79.10 years, (which was 34th in the world) but by 2015 it had increased to around 81. South Korea has the steepest decline in working age population of the OECD nations. In 2015, National Statistical Office estimated that the population of the country will have reached its peak by 2035.

A centralized administration in South Korea oversees the process for the education of children from kindergarten to the third and final year of high school. The school year is divided into two semesters, the first of which begins at the beginning of March and ends in mid-July, the second of which begins in late August and ends in mid-February. The schedules are not uniformly standardized and vary from school to school. Most South Korean middle schools and high schools have school uniforms, modeled on western-style uniforms. Boys' uniforms usually consist of trousers and white shirts, and girls wear skirts and white shirts (this only applies in middle schools and high schools). The country adopted a new educational program to increase the number of their foreign students through 2010. According to the Ministry of Education, Science and Technology, the number of scholarships for foreign students in South Korea would have (under the program) doubled by that time, and the number of foreign students would have reached 100,000.

South Korea is one of the top-performing OECD countries in reading literacy, maths and sciences with the average student scoring 542 and has one of the worlds most highly educated labour forces among OECD countries. The country is well known for its highly feverish outlook on education, where its national obsession with education has been called "education fever". This obsession with education has catapulted the resource poor nation consistently atop the global education rankings where in 2014 national rankings of students’ math and science scores by the Organization for Economic and Cooperation and Development (OECD), South Korea ranked second place worldwide, after Singapore.

Higher education is a serious issue in South Korea society, where it is viewed as one of the fundamental cornerstones of South Korean life. Education is regarded with a high priority for South Korean families as success in education holds a cultural status as well as a necessity to improve one's socioeconomic position in South Korean society. Academic success is often a source of pride for families and within South Korean society at large. South Koreans view education as the main propeller of social mobility for themselves and their family as a gateway to the South Korean middle class. Graduating from a top university is the ultimate marker of prestige, high socioeconomic status, promising marriage prospects, and a respectable career path. An average South Korean child's life revolves around education as pressure to succeed academically is deeply ingrained in South Korean children from an early age. Not having a university degree carries a major cultural stigma as those who lack a formal university education face social prejudice and are often looked down upon by others.

In 2015, the country spent 4.7% of its GDP on all levels of education – roughly equal to the Organization for Economic Cooperation and Development (OECD) average of 4.7% also. A strong investment in education, a militant drive for success as well as the passion for excellence has helped the resource poor country rapidly grow its economy over the past 60 years from a war torn wasteland. South Korea’s zeal for education and its students’ desires to get into a prestigious university is one of the highest in the world, as the entrance into a top tier higher educational institution leads to a prestigious, secure and well-paid white collar job with the government, banks, a major South Korean conglomerate such as Samsung, Hyundai or LG Electronics. With incredible pressure on high school students to secure places at the nation’s best universities, its institutional reputation and alumni networks are strong predictors of future career prospects. The top three universities in South Korea, often referred to as "SKY", are Seoul National University, Korea University and Yonsei University. Intense competition for top grades and academic pressure to be the top student is deeply ingrained in the psyche of South Korean students at a young age. Yet with only so many places at universities and even fewer places at top-tier companies, many young people remain disappointed and are often unwilling to lower their sights with the result of many feeling as underachievers. There is a major cultural taboo in South Korean society attached to those who have not achieved formal university education where those who don't hold university degrees face social prejudice and are often looked down by others as second-class citizens resulting in fewer opportunities for employment, improvement of one's socioeconomic position and prospects for marriage.

International opinion regarding the South Korean education system has been divided. It has been praised for various reasons, including its comparatively high test results and its major role in ushering South Korea's economic development creating one of the world's most educated workforces.
South Korea's highly enviable academic performance has persuaded British education ministers to actively remodel their own curriculums and exams to try to emulate Korea's militant drive and passion for excellence and high educational achievement. Former U.S. President Barack Obama has also praised the country's rigorous school system, where over 80 percent of South Korean high school graduates go on to university. The nation's high university entrance rate has created a highly skilled workforce making South Korea among the most highly educated countries in the world with one of the highest percentages of its citizens holding a tertiary education degree. Bachelor's degrees are held by 68 percent of South Koreans aged 25–34, the most in the OECD.

The system's rigid and hierarchical structure has been criticized for stifling creativity and innovation; described as intensely and "brutally" competitive, the system is often blamed for the high suicide rate in the country, particularly the growing rates among those aged 10–19. Various media outlets attribute the country's high suicide rate to the nationwide anxiety around the country's college entrance exams, which determine the trajectory of students' entire lives and careers. Former South Korean "hagwon" teacher Se-Woong Koo wrote that the South Korean education system amounts to child abuse and that it should be "reformed and restructured without delay". The system has also been criticized for producing an excess supply of university graduates creating an overeducated and underemployed labor force; in the first quarter of 2013 alone, nearly 3.3 million South Korean university graduates were jobless, leaving many graduates overqualified for jobs requiring less education. Further criticism has been stemmed for causing labor shortages in various skilled blue collar labor and vocational occupations, where many go unfilled as the negative social stigma associated with vocational careers and not having a university degree continues to remain deep-rooted in South Korean society.

Korean is the official language of South Korea, and is classified by most linguists as a language isolate. Korean is not related to any Chinese languages, although it incorporates a number of words that are Chinese in origin. Additionally, Korean spoken in South Korea uses a significant number of loan words from English and other European languages. Korean uses an indigenous writing system called Hangul, created in 1446 by King Sejong to provide a convenient alternative to the Classical Chinese Hanja characters that were difficult to learn and did not fit the Korean language well. South Korea still uses some Chinese Hanja characters in limited areas, such as print media and legal documentation.

The Korean language in South Korea has a standard dialect known as Seoul (after the capital city), with an additional 4 Korean language dialect groups in use around the country.

Almost all South Korean students today learn English throughout their education, with some optionally choosing Japanese or Mandarin as well.

According to the results of the census of 2015 more than half of the South Korean population (56.9%) declared themselves not affiliated with any religious organizations. Korean shamanism (also known as Sindo or Muism) is the native religion of the Koreans, and it may represent a large part of the unaffiliated. Indeed, according to a 2012 survey, only 15% of the population declared themselves not religious in the sense of "atheism". Of the people who are affiliated with a religious organization, most are Christians and Buddhists. According to the 2015 census, 27.6% of the population were Christians (19.7% identified themselves as Protestants, 7.9% as Roman Catholics), and 15.5% were Buddhists. Other religions include Islam (130,000 Muslims, mostly migrant workers from Pakistan and Bangladesh but including some 35,000 Korean Muslims,) the homegrown sect of Wonbuddhism, and a variety of indigenous religions, including Cheondoism (a Confucianizing religion), Jeungsanism, Daejongism, Daesun Jinrihoe and others. Freedom of religion is guaranteed by the constitution, and there is no state religion. Overall, between the 2005 and 2015 censuses there has been a slight decline of Christianity (down from 29% to 27.6%), a sharp decline of Buddhism (down from 22.8% to 15.5%), and a rise of the unaffiliated population (from 47.2% to 56.9%).

Christianity is South Korea's largest organized religion, accounting for more than half of all South Korean adherents of religious organizations. There are approximately 13.5 million Christians in South Korea today; about two thirds of them belonging to Protestant churches, and the rest to the Roman Catholic Church. The number of Protestants has been stagnant throughout the 1990s and the 2000s, but increased to a peak level throughout the 2010s. Roman Catholics increased significantly between the 1980s and the 2000s, but declined throughout the 2010s. Christianity, unlike in other East Asian countries, found fertile ground in Korea in the 18th century, and by the end of the 18th century it persuaded a large part of the population as the declining monarchy supported it and opened the country to widespread proselytism as part of a project of Westernization. The weakness of Korean Sindo, which, unlike Japanese Shinto and China's religious system, never developed into a national religion of high status, combined with the impoverished state of Korean Buddhism (after 500 years of suppression at the hands of the Joseon state, by the 20th century it was virtually extinct) left a free hand to Christian churches. Christianity's similarity to native religious narratives has been studied as another factor that contributed to its success in the peninsula. The Japanese colonization of the first half of the 20th century further strengthened the identification of Christianity with Korean nationalism, as the Japanese coopted native Korean Sindo into the Nipponic Imperial Shinto that they tried to establish in the peninsula. Widespread Christianization of the Koreans took place during State Shinto, after its abolition, and then in the independent South Korea as the newly established military government supported Christianity and tried to utterly oust native Sindo.
Among Christian denominations, Presbyterianism is the largest. About nine million people belong to one of the hundred different Presbyterian churches; the biggest ones are the HapDong Presbyterian Church, TongHap Presbyterian Church, the Koshin Presbyterian Church. South Korea is also the second-largest missionary-sending nation, after the United States.

Buddhism was introduced to Korea in the 4th century. It became soon a dominant religion in the southeastern kingdom of Silla, the region that hitherto hosts the strongest concentration of Buddhists in South Korea. In the other states of the Three Kingdoms Period, Goguryeo and Baekje, it was made the state religion respectively in 372 and 528. It remained the state religion in Later Silla (North South States Period) and Goryeo. It was later suppressed throughout much of the subsequent history under the unified kingdom of Joseon (1392–1897), which officially adopted a strict Korean Confucianism. Today, South Korea has about 7 million Buddhists, most of them affiliated to the Jogye Order. Most of the National Treasures of South Korea are Buddhist artifacts.

South Korea has a universal healthcare system.

Suicide in South Korea is a serious and widespread problem and the country ranks poorly on world happiness reports for a high-income state. The suicide rate was the highest in the G20 in 2015 (24.1 deaths per 100,000 persons).

South Korean hospitals have advanced medical equipment and facilities readily available, ranking 4th for MRI units per capita and 6th for CT scanners per capita in the OECD. It also had the OECD's second largest number of hospital beds per 1000 people at 9.56 beds.

Life expectancy has been rising rapidly and South Korea ranked 11th in the world for life expectancy at 82.3 years by the WHO in 2015.

South Korea maintains diplomatic relations with more than 188 countries. The country has also been a member of the United Nations since 1991, when it became a member state at the same time as North Korea. On January 1, 2007, Former South Korean Foreign Minister Ban Ki-moon served as UN Secretary-General from 2007 to 2016. It has also developed links with the Association of Southeast Asian Nations as both a member of "ASEAN Plus three," a body of observers, and the East Asia Summit (EAS).

In November 2009 South Korea joined the OECD Development Assistance Committee, marking the first time a former aid recipient country joined the group as a donor member.

South Korea hosted the G-20 Summit in Seoul in November 2010, a year that saw South Korea and the European Union conclude a free trade agreement (FTA) to reduce trade barriers. South Korea went on to sign a Free Trade Agreements with Canada and Australia in 2014, and another with New Zealand in 2015.

Both North and South Korea claim complete sovereignty over the entire peninsula and outlying islands. Despite mutual animosity, reconciliation efforts have continued since the initial separation between North and South Korea. Political figures such as Kim Koo worked to reconcile the two governments even after the Korean War. With longstanding animosity following the Korean War from 1950 to 1953, North Korea and South Korea signed an agreement to pursue peace. On October 4, 2007, Roh Moo-Hyun and North Korean leader Kim Jong-il signed an eight-point agreement on issues of permanent peace, high-level talks, economic cooperation, renewal of train services, highway and air travel, and a joint Olympic cheering squad.
Despite the Sunshine Policy and efforts at reconciliation, the progress was complicated by North Korean missile tests in 1993, 1998, 2006, 2009, and 2013. , relationships between North and South Korea were very tense; North Korea had been reported to have deployed missiles, ended its former agreements with South Korea, and threatened South Korea and the United States not to interfere with a satellite launch it had planned. North and South Korea are still technically at war (having never signed a peace treaty after the Korean War) and share the world's most heavily fortified border. On May 27, 2009, North Korean media declared that the Armistice is no longer valid because of the South Korean government's pledge to "definitely join" the Proliferation Security Initiative. To further complicate and intensify strains between the two nations, the sinking of the South Korean warship Cheonan in March 2010, is affirmed by the South Korean government to have been caused by a North Korean torpedo, which the North denies. President Lee Myung-bak declared in May 2010 that Seoul would cut all trade with North Korea as part of measures primarily aimed at striking back at North Korea diplomatically and financially, except for the joint Kaesong Industrial Project, and humanitarian aid. North Korea initially threatened to sever all ties, to completely abrogate the previous pact of non-aggression, and to expel all South Koreans from a joint industrial zone in Kaesong, but backtracked on its threats and decided to continue its ties with South Korea. Despite the continuing ties, Kaesong industrial zone has seen a large decrease in investment and manpower as a result of this military conflict. In February 2016, the Kaesong complex was closed by Seoul in reaction to North Korea's launch of a rocket earlier in the month unanimously condemned by the United Nations security council.
The 2017 election of President Moon Jae-in has seen a change in approach towards the North, and both sides used the South Korean held 2018 Winter Olympics as an opportunity for engagement, with a very senior North Korean political delegation sent to the games, along with a reciprocal visit by senior South Korean cabinet members to the North soon afterwards.

Historically, Korea had close relations with the dynasties in China, and some Korean kingdoms were members of the Imperial Chinese tributary system.The Korean kingdoms also ruled over some Chinese kingdoms including the Kitan people and the Manchurians before the Qing dynasty and received tributes from them.In modern times, before the formation of South Korea, Korean independence fighters worked with Chinese soldiers during the Japanese occupation. However, after World War II, the People's Republic of China embraced Maoism while South Korea sought close relations with the United States. The PRC assisted North Korea with manpower and supplies during the Korean War, and in its aftermath the diplomatic relationship between South Korea and the PRC almost completely ceased. Relations thawed gradually and South Korea and the PRC re-established formal diplomatic relations on August 24, 1992. The two countries sought to improve bilateral relations and lifted the forty-year-old trade embargo, and South Korean–Chinese relations have improved steadily since 1992. The Republic of Korea broke off official relations with the Republic of China (Taiwan) upon gaining official relations with the People's Republic of China, which doesn't recognise Taiwan's sovereignty.

China has become South Korea's largest trading partner by far, sending 26% of South Korean exports in 2016 worth $124 billion, as well as an additional $32 billion worth of exports to Hong Kong. South Korea is also China's 4th largest trading partner, with $93 billion of Chinese imports in 2016.

The 2017 deployment of THAAD defence missiles by the United States military in South Korea in response to North Korean missile tests has been protested strongly by the Chinese government, concerned that the technologically advanced missile defence could be used more broadly against China. Relations between the governments have cooled in response, with South Korean commercial and cultural interests in China having been targeted, and Chinese tourism to South Korea having been curtailed. The situation was largely resolved by South Korea making significant military concessions to China in exchange for THAAD, including not deploying any more anti-ballistic missile systems in South Korea and not participating in an alliance between the United States and Japan.

Korea and Japan have had difficult relations since ancient times, but also significant cultural exchange, with Korea acting as the gateway between Asia and Japan. Contemporary perceptions of Japan are still largely defined by Japan's 35 year colonization of Korea in the 20th century, which is generally regarded in South Korea as having been very negative. Japan is today South Korea's third largest trading partner, with 12% ($46 billion) of exports in 2016.

There were no formal diplomatic ties between South Korea and Japan directly after independence the end of World War II in 1945. South Korea and Japan eventually signed the Treaty on Basic Relations between Japan and the Republic of Korea in 1965 to establish diplomatic ties. There is heavy anti-Japanese sentiment in South Korea because of a number of unsettled Japanese-Korean disputes, many of which stem from the period of Japanese occupation after the Japanese annexation of Korea. During World War II, more than 100,000 Koreans served in the Imperial Japanese Army. Korean women were coerced and forced to serve the Imperial Japanese Army as sexual slaves, called comfort women, in both Korea and throughout the Japanese war fronts.

Longstanding issues such as Japanese war crimes against Korean civilians, the negationist re-writing of Japanese textbooks relating Japanese atrocities during World War II, the territorial disputes over the Liancourt Rocks, known in South Korea as "Dokdo" and in Japan as "Takeshima", and visits by Japanese politicians to the Yasukuni Shrine, honoring Japanese people (civilians and military) killed during the war continue to trouble Korean-Japanese relations. The Liancourt Rocks were the first Korean territories to be forcibly colonized by Japan in 1905. Though it was again returned to Korea along with the rest of its territory in 1951 with the signing of the Treaty of San Francisco, Japan does not recant on its claims that the Liancourt Rocks are Japanese territory.
In response to then-Prime Minister Junichiro Koizumi's visits to the Yasukuni Shrine, former President Roh Moo-hyun suspended all summit talks between South Korea and Japan in 2009.
A summit between the nations' leaders was eventually held on February 9th, 2018 during the Korean held Winter Olympics.

The European Union (EU) and South Korea are important trading partners, having negotiated a free trade agreement for many years since South Korea was designated as a priority FTA partner in 2006. The free trade agreement was approved in September 2010, and took effect on July 1, 2011. South Korea is the EU's tenth largest trade partner, and the EU has become South Korea's fourth largest export destination. EU trade with South Korea exceeded €90 billion in 2015 and has enjoyed an annual average growth rate of 9.8% between 2003 and 2013.

The EU has been the single largest foreign investor in South Korea since 1962, and accounted for almost 45% of all FDI inflows into Korea in 2006. Nevertheless, EU companies have significant problems accessing and operating in the South Korean market because of stringent standards and testing requirements for products and services often creating barriers to trade. Both in its regular bilateral contacts with South Korea and through its FTA with Korea, the EU is seeking to improve this situation.

The close relationship began directly after World War II, when the United States temporarily administrated Korea for three years (mainly in the South, with the Soviet Union engaged in North Korea) after Japan. Upon the onset of the Korean War in 1950, U.S. forces were sent to defend against an invasion from North Korea of the South, and subsequently fought as the as the largest contributor of UN troops. The United States participation was critical for preventing the near defeat of the Republic of Korea by northern forces, as well as fighting back for the territory gains that define the South Korean nation today.

Following the Armistice, South Korea and the U.S. agreed to a "Mutual Defense Treaty", under which an attack on either party in the Pacific area would summon a response from both. In 1967, South Korea obliged the mutual defense treaty, by sending a large combat troop contingent to support the United States in the Vietnam War. The US has over 23,000 troops stationed in South Korea, including the U.S. Eighth Army, Seventh Air Force, and U.S. Naval Forces Korea. The two nations have strong economic, diplomatic, and military ties, although they have at times disagreed with regard to policies towards North Korea, and with regard to some of South Korea's industrial activities that involve usage of rocket or nuclear technology. There had also been strong anti-American sentiment during certain periods, which has largely moderated in the modern day.

The two nations also share a close economic relationship, with the U.S being South Korea's second largest trading partner, receiving $66 billion in exports in 2016. In 2007, a free trade agreement known as the Republic of Korea-United States Free Trade Agreement (KORUS FTA) was signed between South Korea and the United States, but its formal implementation was repeatedly delayed, pending approval by the legislative bodies of the two countries. On October 12, 2011, the U.S. Congress passed the long-stalled trade agreement with South Korea. It went into effect on March 15, 2012.

The unresolved tension with North Korea have prompted South Korea to allocate 2.6% of its GDP and 15% of all government spending to its military (Government share of GDP: 14.967%), while maintaining compulsory conscription for men. Consequently, South Korea has the world's seventh largest number of active troops (630,000 in 2017), the world's highest number of reserve troops (7,500,000 in 2017) and the tenth largest defense budget.

The South Korean military consists of the Army (ROKA), the Navy (ROKN), the Air Force (ROKAF), and the Marine Corps (ROKMC), and reserve forces. Many of these forces are concentrated near the Korean Demilitarized Zone. All South Korean males are constitutionally required to serve in the military, typically 21 months. Previous exceptions for South Korean citizens of mixed race no longer apply since 2011.

In addition to male conscription in South Korea's sovereign military, 1,800 Korean males are selected every year to serve 21 months in the KATUSA Program to further augment the United States Forces Korea (USFK). In 2010, South Korea was spending ₩1.68 trillion in a cost-sharing agreement with the US to provide budgetary support to the US forces in Korea, on top of the ₩29.6 trillion budget for its own military.

The South Korean army has 2,500 tanks in operation, including the K1A1 and K2 Black Panther, which form the backbone of the South Korean army's mechanized armor and infantry forces. A sizable arsenal of many artillery systems, including 1,700 self-propelled K55 and K9 Thunder howitzers and 680 helicopters and UAVs of numerous types, are assembled to provide additional fire, reconnaissance, and logistics support. South Korea's smaller but more advanced artillery force and wide range of airborne reconnaissance platforms are pivotal in the counter-battery suppression of North Korea's large artillery force, which operates more than 13,000 artillery systems deployed in various state of fortification and mobility.

The South Korean navy has made its first major transformation into a blue-water navy through the formation of the Strategic Mobile Fleet, which includes a battle group of Chungmugong Yi Sun-sin class destroyers, Dokdo class amphibious assault ship, AIP-driven Type 214 submarines, and King Sejong the Great class destroyers, which is equipped with the latest baseline of Aegis fleet-defense system that allows the ships to track and destroy multiple cruise missiles and ballistic missiles simultaneously, forming an integral part of South Korea's indigenous missile defense umbrella against the North Korean military's missile threat.

The South Korean air force operates 840 aircraft, making it world's ninth largest air force, including several types of advanced fighters like F-15K, heavily modified KF-16C/D, and the indigenous F/A-50, supported by well-maintained fleets of older fighters such as F-4E and KF-5E/F that still effectively serve the air force alongside the more modern aircraft. In an attempt to gain strength in terms of not just numbers but also modernity, the commissioning of four Boeing 737 AEW&C aircraft, under Project Peace Eye for centralized intelligence gathering and analysis on a modern battlefield, will enhance the fighters' and other support aircraft's ability to perform their missions with awareness and precision.

In May 2011, Korea Aerospace Industries Ltd., South Korea's largest plane maker, signed a $400 million deal to sell 16 T-50 Golden Eagle trainer jets to Indonesia, making South Korea the first country in Asia to export supersonic jets.

From time to time, South Korea has sent its troops overseas to assist American forces. It has participated in most major conflicts that the United States has been involved in the past 50 years. South Korea dispatched 325,517 troops to fight alongside American, Australian, Filipino, New Zealand and South Vietnamese soldiers in the Vietnam War, with a peak strength of 50,000. In 2004, South Korea sent 3,300 troops of the Zaytun Division to help re-building in northern Iraq, and was the third largest contributor in the coalition forces after only the US and Britain. Beginning in 2001, South Korea had so far deployed 24,000 troops in the Middle East region to support the War on Terrorism. A further 1,800 were deployed since 2007 to reinforce UN peacekeeping forces in Lebanon.

The United States has stationed a substantial contingent of troops to defend South Korea. There are approximately 28,500 U.S. Military personnel stationed in Korea, most of them serving one year unaccompanied tours. The American troops, which are primarily ground and air units, are assigned to USFK and mainly assigned to the Eighth United States Army of the US Army and Seventh Air Force of the US Air Force. They are stationed in installations at Osan, Kunsan, Yongsan, Dongducheon, Sungbuk, Camp Humphreys, and Daegu, as well as at Camp Bonifas in the DMZ Joint Security Area. <br>
A fully functioning UN Command is at the top of the chain of command of all forces in South Korea, including the US forces and the entire South Korean military – if a sudden escalation of war between North and South Korea were to occur the United States would assume control of the South Korean armed forces in all military and paramilitary moves. There has been long term agreement between the United States and South Korea that South Korea should eventually assume the lead for its own defense. This transition to a South Korean command has been slow and often postponed, although it is currently scheduled to occur in the early 2020s.

Male citizens who refuse or reject to undertake military services because of conscientious objection are typically imprisoned, with over 600 individuals usually imprisoned at any given time; more than the rest of the world put together. The vast majority of these are young men from the Jehovah's Witnesses Christian denomination.
See Conscription in South Korea.

South Korea's mixed economy ranks 11th nominal and 13th purchasing power parity GDP in the world, identifying it as one of the G-20 major economies. It is a developed country with a high-income economy and is the most industrialized member country of the OECD. South Korean brands such as LG Electronics and Samsung are internationally famous and garnered South Korea's reputation for its quality electronics and other manufactured goods.

Its massive investment in education has taken the country from mass illiteracy to a major international technological powerhouse. The country's national economy benefits from a highly skilled workforce and is among the most educated countries in the world with one of the highest percentages of its citizens holding a tertiary education degree. South Korea's economy was one of the world's fastest-growing from the early 1960s to the late 1990s, and was still one of the fastest-growing developed countries in the 2000s, along with Hong Kong, Singapore and Taiwan, the other three Asian Tigers. South Koreans refer to this growth as the Miracle on the Han River. The South Korean economy is heavily dependent on international trade, and in 2014, South Korea was the fifth-largest exporter and seventh-largest importer in the world.

Despite the South Korean economy's high growth potential and apparent structural stability, the country suffers damage to its credit rating in the stock market because of the belligerence of North Korea in times of deep military crises, which has an adverse effect on South Korean financial markets. The International Monetary Fund compliments the resilience of the South Korean economy against various economic crises, citing low state debt and high fiscal reserves that can quickly be mobilized to address financial emergencies. Although it was severely harmed by the Asian economic crisis of the late 1990s, the South Korean economy managed a rapid recovery and subsequently tripled its GDP.

Furthermore, South Korea was one of the few developed countries that were able to avoid a recession during the global financial crisis. Its economic growth rate reached 6.2 percent in 2010 (the fastest growth for eight years after significant growth by 7.2 percent in 2002), a sharp recovery from economic growth rates of 2.3% in 2008 and 0.2% in 2009, when the global financial crisis hit. The unemployment rate in South Korea also remained low in 2009, at 3.6%.

South Korea became a member of the Organization for Economic Co-operation and Development (OECD) in 1996.

The following list includes the largest South Korean companies by revenue in 2017 who are all listed as part of the Fortune Global 500:

South Korea has a technologically advanced transport network consisting of high-speed railways, highways, bus routes, ferry services, and air routes that crisscross the country. Korea Expressway Corporation operates the toll highways and service amenities en route.

Korail provides frequent train services to all major South Korean cities. Two rail lines, Gyeongui and Donghae Bukbu Line, to North Korea are now being reconnected. The Korean high-speed rail system, KTX, provides high-speed service along Gyeongbu and Honam Line. Major cities including Seoul, Busan, Incheon, Daegu, Daejeon and Gwangju have urban rapid transit systems. Express bus terminals are available in most cities.

South Korea's main gateway and largest airport is Incheon International Airport, serving passengers in 2016. Other international airports include Gimpo, Busan and Jeju. There are also a large number of airports that were built as part of the infrastructure boom but are barely used. There are a large number of heliports.

The national carrier, Korean Air served over 26,800,000 passengers, including almost 19,000,000 international passengers in 2016. A second carrier, Asiana Airlines also serves domestic and international traffic. Combined, South Korean airlines serve 297 international routes. Smaller airlines, such as Jeju Air, provide domestic service with lower fares.

South Korea is the world's fifth-largest nuclear power producer and the second-largest in Asia . Nuclear power in South Korea supplies 45% of electricity production, and research is very active with investigation into a variety of advanced reactors, including a small modular reactor, a liquid-metal fast/transmutation reactor and a high-temperature hydrogen generation design. Fuel production and waste handling technologies have also been developed locally. It is also a member of the ITER project.

South Korea is an emerging exporter of nuclear reactors, having concluded agreements with the UAE to build and maintain four advanced nuclear reactors, with Jordan for a research nuclear reactor, and with Argentina for construction and repair of heavy-water nuclear reactors. , South Korea and Turkey are in negotiations regarding construction of two nuclear reactors. South Korea is also preparing to bid on construction of a light-water nuclear reactor for Argentina.

South Korea is not allowed to enrich uranium or develop traditional uranium enrichment technology on its own, because of US political pressure, unlike most major nuclear powers such as Japan, Germany, and France, competitors of South Korea in the international nuclear market. This impediment to South Korea's indigenous nuclear industrial undertaking has sparked occasional diplomatic rows between the two allies. While South Korea is successful in exporting its electricity-generating nuclear technology and nuclear reactors, it cannot capitalize on the market for nuclear enrichment facilities and refineries, preventing it from further expanding its export niche. South Korea has sought unique technologies such as pyroprocessing to circumvent these obstacles and seek a more advantageous competition. The US has recently been wary of South Korea's burgeoning nuclear program, which South Korea insists will be for civilian use only.

South Korea is the third highest ranked Asian country in the World Economic Forum's Network Readiness Index (NRI) after Singapore and Hong Kong respectively – an indicator for determining the development level of a country's information and communication technologies. South Korea ranked number 10 overall in the 2014 NRI ranking, up from 11 in 2013.

In 2016, 17 million foreign tourists visited South Korea With rising tourist prospects, especially from foreign countries outside of Asia, the South Korean government has set a target of attracting 20 million foreign tourists a year by 2017.

South Korean tourism is driven by many factors, including the popularity of South Korean pop music and television dramas, known as Korean Wave (Hallyu), throughout East Asia, traditional culture, cuisine and natural environment. The Hyundai Research Institute reported that the Korean Wave has a direct impact in encouraging direct foreign investment back into the country through demand for products, and the tourism industry. Among Asian countries, China was the most receptive, investing 1.4 billion in South Korea, with much of the investment within its service sector, a sevenfold increase from 2001. According to an analysis by economist Han Sang-Wan, a 1 percent increase in the exports of Korean cultural content pushes consumer goods exports up 0.083 percent while a 1 percent increase in Korean pop content exports to a country produces a 0.019 percent bump in tourism.

The South Korean pension system was created to provide benefits to persons reaching old age, families and persons stricken with death of their primary breadwinner, and for the purposes of stabilizing its nations welfare state. South Korea's pensions system structure is primarily based on taxation and is income-related. In 2007 there was a total of 18,367,000 insured individuals with only around 511,000 persons excluded from mandatory contribution. The current pension system is divided into four categories distributing benefits to participants through national, military personnel, governmental, and private school teacher pension schemes. The national pension scheme is the primary welfare system providing allowances to the majority of persons. Eligibility for the national pension scheme is not dependent on income but on age and residence, where those between the ages of 18 to 59 are covered. Any one who is under the age of 18 are dependents of someone who is covered or under a special exclusion where they are allowed to alternate provisions. The national pension scheme is divided into four categories of insured persons – the workplace-based insured, the individually insured, the voluntarily insured, and the voluntarily and continuously insured.

Employees between the ages of 18 to 59 are covered under the workplace-based pension scheme and contribute 4.5% of their gross monthly earnings. The national pension covers employees who work in firms that employ five or more employees, fishermen, farmers, and the self-employed in both rural and urban areas. Employers are also covered under the workplace-based pension scheme and help cover their employees obligated 9% contribution by providing the remaining 4.5%. Anyone who is not employed, of the age of 60 or above, and excluded by article 6 of the National Pension Act but of the ages between 18 and 59, is covered under the individually insured pension scheme. Persons covered by the individually insured pension scheme are in charge of paying the entire 9% contribution themselves. Voluntarily insured persons are not subjected to mandatory coverage but can choose to be. This category comprises retirees who voluntarily choose to have additional benefits, individuals under the age of 27 without income, and individuals whose spouses are covered under a public welfare system, whether military, governmental, or private school teacher pensions. Like the Individually insured persons, they too are in charge of covering the full amount of the contribution. Voluntarily and continuously insured persons consists of individuals 60 years of age who want to fulfill the minimum insured period of 20 years to qualify for old age pension benefits. Excluding the workplace-based insured persons, all the other insured persons personally cover their own 9% contribution.

South Korea's old-age pension scheme covers individuals age 60 or older for the rest of their life as long as they have satisfied the minimum of 20 years of national pension coverage before hand. Individuals with a minimum of 10 years covered under the national pension scheme and who are 60 years of age are able to be covered by under a 'reduced old-age pension' scheme. There also is an 'active old-age pension' scheme that covers individuals age 60 to 65 engaged in activities yielding earned income. Individuals age of 55 and younger than 60 who are not engaged in activities yielding earned income are eligible to be covered under the 'early old-age pension' scheme. Around 60% of all Korean elders, age 65 and over are entitled to a 5% benefit of their past average income at an average of 90,000 Korean Won (KRW). Basic old-age pension schemes covered individuals 65 years of age who earned below an amount set by presidential order. In 2010, that ceiling was 700,00 KRW for a single individual and 1,120,000 for a couple, equivalent to around $600.00 and $960.00.

Scientific and technological development in the South Korea at first did not occur largely because of more pressing matters such as the division of Korea and the Korean War that occurred right after its independence. It wasn't until the 1960s under the dictatorship of Park Chung-hee where South Korea's economy rapidly grew from industrialisation and the Chaebol corporations such as Samsung and LG. Ever since the industrialization of South Korea's economy, South Korea has placed its focus on technology-based corporations, which has been supported by infrastructure developments by the government. South Korean corporations Samsung and LG were ranked first and third largest mobile phone companies in the world in the first quarter of 2012, respectively. An estimated 90% of South Koreans own a mobile phone. Aside from placing/receiving calls and text messaging, mobile phones in the country are widely used for watching Digital Multimedia Broadcasting (DMB) or viewing websites. Over one million DMB phones have been sold and the three major wireless communications providers SK Telecom, KT, and LG U+ provide coverage in all major cities and other areas. South Korea has the fastest Internet download speeds in the world, with an average download speed of 25.3 Mbit/s.

South Korea leads the OECD in graduates in science and engineering. The country ranks first among the most innovative countries in the Bloomberg Innovation Index. Additionally, South Korea today is known as a Launchpad of a mature mobile market, where developers can reap benefits of a market where very few technology constraints exist. There is a growing trend of inventions of new types of media or apps, utilizing the 4G and 5G internet infrastructure in South Korea. South Korea has today the infrastructures to meet a density of population and culture that has the capability to create strong local particularity.

Following cyberattacks in the first half of 2013, whereby government, news-media, television station, and bank websites were compromised, the national government committed to the training of 5,000 new cybersecurity experts by 2017. The South Korean government blamed North Korea for these attacks, as well as incidents that occurred in 2009, 2011 and 2012, but Pyongyang denies the accusations.

In late September 2013, a computer-security competition jointly sponsored by the defense ministry and the National Intelligence Service was announced. The winners were announced on September 29, 2013 and shared a total prize pool of 80 million won (US$74,000).

South Korea has sent up 10 satellites from 1992, all using foreign rockets and overseas launch pads, notably Arirang-1 in 1999, and Arirang-2 in 2006 as part of its space partnership with Russia. Arirang-1 was lost in space in 2008, after nine years in service.

In April 2008, Yi So-yeon became the first Korean to fly in space, aboard the Russian Soyuz TMA-12.

In June 2009, the first spaceport of South Korea, Naro Space Center, was completed at Goheung, Jeollanam-do. The launch of Naro-1 in August 2009 resulted in a failure. The second attempt in June 2010 was also unsuccessful. However, the third launch of the Naro 1 in January 2013 was successful. The government plans to develop Naro-2 by the year 2018.

South Korea's efforts to build an indigenous space launch vehicle is marred because of persistent political pressure of the United States, who had for many decades hindered South Korea's indigenous rocket and missile development programs in fear of their possible connection to clandestine military ballistic missile programs, which Korea many times insisted did not violate the research and development guidelines stipulated by US-Korea agreements on restriction of South Korean rocket technology research and development. South Korea has sought the assistance of foreign countries such as Russia through MTCR commitments to supplement its restricted domestic rocket technology. The two failed KSLV-I launch vehicles were based on the Universal Rocket Module, the first stage of the Russian Angara rocket, combined with a solid-fueled second stage built by South Korea.

Robotics has been included in the list of main national R&D projects in Korea since 2003. In 2009, the government announced plans to build robot-themed parks in Incheon and Masan with a mix of public and private funding.

In 2005, Korea Advanced Institute of Science and Technology (KAIST) developed the world's second walking humanoid robot, HUBO. A team in the Korea Institute of Industrial Technology developed the first Korean android, EveR-1 in May 2006.
EveR-1 has been succeeded by more complex models with improved movement and vision.

Plans of creating English-teaching robot assistants to compensate for the shortage of teachers were announced in February 2010, with the robots being deployed to most preschools and kindergartens by 2013. Robotics are also incorporated in the entertainment sector as well; the "Korean Robot Game Festival" has been held every year since 2004 to promote science and robot technology.

Since the 1980s, the Korean government has invested in the development of a domestic biotechnology industry, and the sector is projected to grow to by 2010. The medical sector accounts for a large part of the production, including production of hepatitis vaccines and antibiotics.

Recently, research and development in genetics and cloning has received increasing attention, with the first successful cloning of a dog, Snuppy (in 2005), and the cloning of two females of an endangered species of wolves by the Seoul National University in 2007.

The rapid growth of the industry has resulted in significant voids in regulation of ethics, as was highlighted by the scientific misconduct case involving Hwang Woo-Suk.

South Korea shares its traditional culture with North Korea, but the two Koreas have developed distinct contemporary forms of culture since the peninsula was divided in 1945. Historically, while the culture of Korea has been heavily influenced by that of neighboring China, it has nevertheless managed to develop a unique cultural identity that is distinct from its larger neighbor. Its rich and vibrant culture left 19 UNESCO Intangible Cultural Heritages of Humanity, the third largest in the world, along with 12 World Heritage Sites. The South Korean Ministry of Culture, Sports and Tourism actively encourages the traditional arts, as well as modern forms, through funding and education programs.

The industrialization and urbanization of South Korea have brought many changes to the way modern Koreans live. Changing economics and lifestyles have led to a concentration of population in major cities, especially the capital Seoul, with multi-generational households separating into nuclear family living arrangements. A 2014 Euromonitor study found that South Koreans drink the most alcohol on a weekly basis compared to the rest of the world. South Koreans drink 13.7 shots of liquor per week on average and, of the 44 other countries analyzed, Russia, the Philippines, and Thailand follow.

Korean art has been highly influenced by Buddhism and Confucianism, which can be seen in the many traditional paintings, sculptures, ceramics and the performing arts. Korean pottery and porcelain, such as Joseon's "baekja" and buncheong, and Goryeo's celadon are well known throughout the world. The Korean tea ceremony, pansori, talchum and buchaechum are also notable Korean performing arts.

Post-war modern Korean art started to flourish in the 1960s and 1970s, when South Korean artists took interest in geometrical shapes and intangible subjects. Establishing a harmony between man and nature was also a favorite of this time. Because of social instability, social issues appeared as main subjects in the 1980s. Art was influenced by various international events and exhibits in Korea, and with it brought more diversity. The Olympic Sculpture Garden in 1988, the transposition of the 1993 edition of the Whitney Biennial to Seoul, the creation of the Gwangju Biennale and the Korean Pavilion at the Venice Biennale in 1995 were notable events.

Because of South Korea's tumultuous history, construction and destruction has been repeated endlessly, resulting in an interesting melange of architectural styles and designs.

Korean traditional architecture is characterized by its harmony with nature. Ancient architects adopted the bracket system characterized by thatched roofs and heated floors called "ondol". People of the upper classes built bigger houses with elegantly curved tiled roofs with lifting eaves. Traditional architecture can be seen in the palaces and temples, preserved old houses called "hanok", and special sites like Hahoe Folk Village, Yangdong Village of Gyeongju and Korean Folk Village. Traditional architecture may also be seen at the nine UNESCO World Heritage Sites in South Korea.

Western architecture was first introduced to Korea at the end of the 19th century. Churches, offices for foreign legislation, schools and university buildings were built in new styles. With the annexation of Korea by Japan in 1910 the colonial regime intervened in Korea's architectural heritage, and Japanese-style modern architecture was imposed. The anti-Japanese sentiment, and the Korean War, led to the destruction of most buildings constructed during that time.

Korean architecture entered a new phase of development during the post-Korean War reconstruction, incorporating modern architectural trends and styles. Stimulated by the economic growth in the 1970s and 1980s, active redevelopment saw new horizons in architectural design. In the aftermath of the 1988 Seoul Olympics, South Korea has witnessed a wide variation of styles in its architectural landscape due, in large part, to the opening up of the market to foreign architects. Contemporary architectural efforts have been constantly trying to balance the traditional philosophy of "harmony with nature" and the fast-paced urbanization that the country has been going through in recent years.

Korean cuisine, "hanguk yori" (한국요리; 韓國料理), or "hansik" (한식; 韓食), has evolved through centuries of social and political change. Ingredients and dishes vary by province. There are many significant regional dishes that have proliferated in different variations across the country in the present day. The Korean royal court cuisine once brought all of the unique regional specialties together for the royal family. Meals consumed both by the royal family and ordinary Korean citizens have been regulated by a unique culture of etiquette.

Korean cuisine is largely based on rice, noodles, tofu, vegetables, fish and meats. Traditional Korean meals are noted for the number of side dishes, "banchan" (반찬), which accompany steam-cooked short-grain rice. Every meal is accompanied by numerous banchan. Kimchi (김치), a fermented, usually spicy vegetable dish is commonly served at every meal and is one of the best known Korean dishes. Korean cuisine usually involves heavy seasoning with sesame oil, "doenjang" (된장), a type of fermented soybean paste, soy sauce, salt, garlic, ginger, and "gochujang" (고추장), a hot pepper paste. Other well-known dishes are "Bulgogi" (불고기), grilled marinated beef, "Gimbap" (김밥), and "Tteokbokki" (떡볶이), a spicy snack consisting of rice cake seasoned with gochujang or a spicy chili paste.

Soups are also a common part of a Korean meal and are served as part of the main course rather than at the beginning or the end of the meal. Soups known as "guk" (국) are often made with meats, shellfish and vegetables. Similar to guk, "tang" (탕; 湯) has less water, and is more often served in restaurants. Another type is "jjigae" (찌개), a stew that is typically heavily seasoned with chili pepper and served boiling hot.

Popular Korean alcoholic beverages include Soju, Makgeolli and Bokbunja ju.

Korea is unique among Asian countries in its use of metal chopsticks. Metal chopsticks have been discovered in Goguryeo archaeological sites.

In addition to domestic consumption, South Korea has a thriving entertainment industry where various facets of South Korean entertainment including television dramas, films, and popular music has generated significant financial revenues for the nation's economy. The cultural phenomenon known as "Hallyu" or the "Korean Wave", has swept many countries across Asia making South Korea a major soft power as an exporter of popular culture and entertainment, rivaling Western nations such as the United States and the United Kingdom.

Until the 1990s, trot and traditional Korean folk based ballads dominated South Korean popular music. The emergence of the South Korean pop group Seo Taiji and Boys in 1992 marked a turning point for South Korean popular music, also known as K-pop, as the genre modernized itself from incorporating elements of popular musical genres from across the world such as Western popular music, experimental, jazz, gospel, Latin, classical, hip hop, rhythm and blues, electronic dance, reggae, country, folk, and rock on top of its uniquely traditional Korean music roots. Western-style pop, hip hop, rhythm and blues, rock, folk, electronic dance oriented acts have become dominant in the modern South Korean popular music scene, though trot is still enjoyed among older South Koreans. K-pop stars and groups are well known across Asia and have found international fame making millions of dollars in export revenue. Many K-pop acts have also been able secure a strong overseas following following using online social media platforms such as the video sharing website YouTube. South Korean singer PSY became an international sensation when his song "Gangnam Style" topped global music charts in 2012. 

Since the success of the film "Shiri" in 1999, the Korean film industry has begun to gain recognition internationally. Domestic film has a dominant share of the market, partly because of the existence of screen quotas requiring cinemas to show Korean films at least 73 days a year.

South Korean television shows have become popular outside of Korea. South Korean television dramas, known as K-dramas have begun to find fame internationally. Many dramas tend to have a romantic focus, such as "Princess Hours", "You're Beautiful", "Playful Kiss", "My Name is Kim Sam Soon", "Boys Over Flowers", "Winter Sonata", "Autumn in My Heart", "Full House", "City Hunter", "All About Eve", "Secret Garden", "I Can Hear Your Voice", "Master's Sun", "My Love from the Star", "Healer", "Descendants of the Sun" and "". Historical dramas have included "Faith", "Dae Jang Geum", "The Legend", "Dong Yi", "Moon Embracing the Sun", and "Sungkyunkwan Scandal".

There are many official public holidays in South Korea. Korean New Year's Day, or "Seollal", is celebrated on the first day of the Korean lunar calendar. Korean Independence Day falls on March 1, and commemorates the March 1 Movement of 1919. Memorial Day is celebrated on June 6, and its purpose is to honor the men and women who died in South Korea's independence movement. Constitution Day is on July 17, and it celebrates the promulgation of Constitution of the Republic of Korea. Liberation Day, on August 15, celebrates Korea's liberation from the Empire of Japan in 1945. Every 15th day of the 8th lunar month, Koreans celebrate the Midautumn Festival, in which Koreans visit their ancestral hometowns and eat a variety of traditional Korean foods. On October 1, Armed Forces day is celebrated, honoring the military forces of South Korea. October 3 is National Foundation Day. Hangul Day, on October 9 commemorates the invention of hangul, the native alphabet of the Korean language.

The martial art taekwondo originated in Korea. In the 1950s and 1960s, modern rules were standardized, with taekwondo becoming an official Olympic sport in 2000. Other Korean martial arts include taekkyeon, hapkido, Tang Soo Do, Kuk Sool Won, kumdo and subak.

Football and baseball have traditionally been regarded as the most popular sports in Korea. Recent polling indicates that a majority, 41% of South Korean sports fans continue to self-identify as football fans, with baseball ranked 2nd at 25% of respondents. However, the polling did not indicate the extent to which respondents follow both sports. The national football team became the first team in the Asian Football Confederation to reach the FIFA World Cup semi-finals in the 2002 FIFA World Cup, jointly hosted by South Korea and Japan. The Korea Republic national team (as it is known) has qualified for every World Cup since Mexico 1986, and has broken out of the group stage twice: first in 2002, and again in 2010, when it was defeated by eventual semi-finalist Uruguay in the Round of 16. At the 2012 Summer Olympics, South Korea won the Bronze Medal for football.

Baseball was first introduced to Korea in 1905 and has since become increasingly popular, with some sources claiming it has surpassed football as the most popular sport in the country. Recent years have been characterized by increasing attendance and ticket prices for professional baseball games. The Korea Professional Baseball league, a 10-team circuit, was established in 1982. The South Korea national team finished third in the 2006 World Baseball Classic and second in the 2009 tournament. The team's 2009 final game against Japan was widely watched in Korea, with a large screen at Gwanghwamun crossing in Seoul broadcasting the game live. In the 2008 Summer Olympics, South Korea won the gold medal in baseball. Also in 1982, at the Baseball Worldcup, Korea won the gold medal. At the 2010 Asian Games, the Korean National Baseball team won the gold medal. Several Korean players have gone on to play in Major League Baseball.

Basketball is a popular sport in the country as well. South Korea has traditionally had one of the top basketball teams in Asia and one of the continent's strongest basketball divisions. Seoul hosted the 1967 and 1995 Asian Basketball Championship. The Korea national basketball team has won a record number of 23 medals at the event to date.
South Korea hosted the Asian Games in 1986 (Seoul), 2002 (Busan) and 2014 (Incheon). It also hosted the Winter Universiade in 1997, the Asian Winter Games in 1999 and the Summer Universiade in 2003, 2015. In 1988, South Korea hosted the Summer Olympics in Seoul, coming fourth with 12 gold medals, 10 silver medals and 11 bronze medals. South Korea regularly performs well in archery, shooting, table tennis, badminton, short track speed skating, handball, hockey, freestyle wrestling, Greco-Roman wrestling, baseball, judo, taekwondo, speed skating, figure Skating, and weightlifting. The Seoul Olympic Museum is a museum in Seoul, South Korea, dedicated to the 1988 Summer Olympics. On July 6, 2011 Pyeongchang was chosen by the IOC to host the 2018 Winter Olympics.

South Korea has won more medals in the Winter Olympics than any other Asian country with a total of 45 medals (23 gold, 14 silver, and 8 bronze). At the 2010 Winter Olympics, South Korea ranked fifth in the overall medal rankings. South Korea is especially strong in short track speed skating. However, speed skating and figure skating are very popular, too, and ice hockey is an emerging sport with Anyang Halla winning their first ever Asia League Ice Hockey title in March 2010.

Seoul hosted a professional triathlon race, which is part of the International Triathlon Union (ITU) World Championship Series in May 2010. In 2011, the South Korean city of Daegu hosted the 2011 IAAF World Championships in Athletics.
In October 2010, South Korea hosted its first Formula One race at the Korea International Circuit in Yeongam, about south of Seoul. The Korean Grand Prix was held from 2010 to 2013, but was not placed on the 2014 F1 calendar.

Domestic horse racing events are also followed by South Koreans and Seoul Race Park in Gwacheon, Gyeonggi-do is located closest to Seoul out of the country's three tracks.

Competitive video gaming, also called eSports (sometimes written e-Sports), has become more popular South Korea in recent years, particularly among young people. The two most popular games are League of Legends and StarCraft. The gaming scene of South Korea is managed by the Korean e-Sports Association (KeSPA for short) and has become something of a career for many players. They can make a living out of their activity and top players can even make a significant amount of money with some high end Starcraft II players ending up making six figure salaries.

Korean e-Sports Association

Korea Professional Sports League

International Championship Host





</doc>
<doc id="27020" url="https://en.wikipedia.org/wiki?curid=27020" title="History of South Korea">
History of South Korea

The history of South Korea formally begins with its establishment on August 17, 1948, although Rhee Syngman had officially declared independence two days prior.

Korea was administratively partitioned in 1945, at the end of World War II. As Korea was under Japanese rule during World War II, Korea was officially a belligerent against the Allies. The unconditional surrender of Japan led to the division of Korea into two occupation zones (similar to the four zones in Germany), with the United States administering the southern half of the peninsula and the Soviet Union administering the area north of the 38th parallel. This division was meant to be temporary (as was in Germany) and was first intended to return a unified Korea back to its people after the United States, United Kingdom, Soviet Union, and Republic of China could arrange a single government for the peninsula.

The two parties were unable to agree on the implementation of Joint Trusteeship over Korea. This led in 1948 to the establishment of two separate governments – the Communist-aligned Democratic People's Republic of Korea (DPRK) and the West-aligned First Republic of Korea – each claiming to be the legitimate government of all of Korea. On June 25,1950 the Korean War broke out. After much destruction, the war ended on July 27,1953 with the 1948 status quo being restored, as neither the DPRK nor the First Republic had succeeded in conquering the other's portion of the divided Korea. The peninsula was divided by the Korean Demilitarized Zone and the two separate governments stabilized into the existing political entities of North and South Korea.

South Korea's subsequent history is marked by alternating periods of democratic and autocratic rule. Civilian governments are conventionally numbered from the First Republic of Rhee Syngman to the contemporary Sixth Republic. The First Republic, arguably democratic at its inception, became increasingly autocratic until its collapse in 1960. The Second Republic was strongly democratic, but was overthrown in less than a year and replaced by an autocratic military regime. The Third, Fourth, and Fifth Republics were nominally democratic, but are widely regarded as the continuation of military rule. With the Sixth Republic, the country has gradually stabilized into a liberal democracy.

Since its inception, South Korea has seen substantial development in education, economy, and culture. Since the 1960s, the country has developed from one of Asia's poorest to one of the world's wealthiest nations. Education, particularly at the tertiary level, has expanded dramatically. It is said to be one of the "Four Tigers" of rising Asian states along with Singapore, Taiwan and Hong Kong.

Emperor Hirohito announced the surrender of the Empire of Japan to the Allied Powers on 15 August 1945. General Order No. 1 for the surrender of Japan (prepared by the Joint Chiefs of Staff of U.S. military forces and approved on 17 August 1945) prescribed separate surrender procedures for Japanese forces in Korea north and south of the 38th parallel. After Japan's surrender to the Allies (formalised on 2 September 1945), division at the 38th parallel marked the beginning of Soviet and U.S. occupation the North and South, respectively. This division was meant to be temporary, to be replaced by a trusteeship of the United States, United Kingdom, Soviet Union, and Republic of China which would prepare for Korean independence. The trusteeship had been discussed at the Yalta Conference in February 1945. U.S. forces landed at Incheon on September 8, 1945 and established a military government shortly thereafter. Lt. General John R. Hodge, their commander, took charge of the government. Faced with mounting popular discontent, in October 1945 Hodge established the Korean Advisory Council. The Provisional Government of the Republic of Korea, which had operated from China, sent a delegation with three interpreters to Hodge, but he refused to meet with them. Likewise, Hodge refused to recognize the newly formed People's Republic of Korea and its People's Committees, and outlawed it on 12 December. A year later, an interim legislature and interim government were established, headed by Kim Kyu-shik and Rhee Syngman respectively. Political and economic chaos - arising from a variety of causes - plagued the country in this period. The after-effects of the Japanese exploitation remained in the South, as in the North. In addition, the U.S. military was largely unprepared for the challenge of administering the country, arriving with no knowledge of the language, culture or political situation. Thus many of their policies had unintended destabilizing effects. Waves of refugees from North Korea and returnees from abroad added to the turmoil.

In December 1945 a conference convened in Moscow to discuss the future of Korea.
A 5-year trusteeship was discussed, and a US-Soviet joint commission was established. The commission met intermittently in Seoul but deadlocked over the issue of establishing a national government. In September 1947, with no solution in sight, the United States submitted the Korean question to the UN General Assembly.

The resolution from the UN General Assembly called for a UN-supervised general election in Korea, but after the North rejected this proposition, a general election for a Constitutional Assembly took place in the South only, in May 1948. A constitution was adopted, setting forth a presidential form of government and specifying a four-year term for the presidency. According to the provisions of the Constitution, an indirect presidential election took place in July. Rhee Syngman, as head of the new assembly, assumed the presidency and proclaimed the Republic of Korea (South Korea) on August 15, 1948.

On August 15, 1948, the Republic of Korea was formally established, with Rhee Syngman as the first president. With the establishment of Rhee's government, de jure sovereignty also passed into the new government. On September 9, 1948, a communist regime, the Democratic People's Republic of Korea (North Korea), was proclaimed under Kim Il-sung. However, on December 12, 1948, by its resolution 195 in the Third General Assembly, the United Nations recognized the Republic of Korea as the sole legal government of Korea.

In 1946, the North implemented land reforms by confiscating private property, Japanese and pro-Japanese owned facilities and factories, and placed them under state ownership. Demand for land reform in the South grew strong, and it was eventually enacted in June 1949. Koreans with large landholdings were obliged to divest most of their land. Approximately 40 percent of total farm households became small landowners. However, because preemptive rights were given to people who had ties with landowners before liberation, many pro-Japanese groups obtained or retained properties.

The country now divided, the relationship between the two Koreas turned more antagonistic as time passed. The Soviet forces having withdrawn in 1948, North Korea pressured the South to expel the United States forces, but Rhee sought to align his government strongly with America, and against both North Korea and Japan. Although talks towards normalization of relations with Japan took place, they achieved little. Meanwhile, the government took in vast sums of American aid, in amounts sometimes near the total size of the national budget. The nationalist government also continued many of the practices of the U.S. military government. In 1948, the Rhee government repressed military uprisings in Jeju, Suncheon and Yeosu. During the rebellion and its suppression 14,000 to 60,000 people were killed in all fighting.

The main policy of the First Republic of South Korea was anti-communism and "unification by expanding northward". The South's military was neither sufficiently equipped nor prepared, but the Rhee administration was determined to reunify Korea by military force with aid from the United States. However, in the second parliamentary elections held on May 30, 1950, the majority of seats went to independents who did not endorse this position, confirming the lack of support and the fragile state of the nation.

When the communist army attacked from the North in June, retreating South Korean forces executed tens of thousands suspected communists or sympathisers, either in prison or a in a reeducation movement, in what is known as the Bodo League massacre.

On June 25, 1950, North Korean forces invaded South Korea. Led by the U.S., a 16-member coalition undertook the first collective action under the United Nations Command (UNC) in defense of South Korea. Oscillating battle lines inflicted a high number of civilian casualties and wrought immense destruction. With the People's Republic of China's entry on behalf of North Korea in late 1950, the fighting came to a stalemate close to the original line of demarcation. Armistice negotiations, initiated in July 1951, finally concluded on July 27, 1953 at Panmunjeom, now in the Demilitarized Zone (DMZ). Following the armistice, the South Korean government returned to Seoul on the symbolic date of August 15, 1953.

After the armistice, South Korea experienced political turmoil under years of autocratic leadership of Rhee Syngman, which was ended by student revolt in 1960. Throughout his rule, Rhee sought to take additional steps to cement his control of government. These began in 1952, when the government was still based in Busan due to the ongoing war. In May of that year, Rhee pushed through constitutional amendments which made the presidency a directly-elected position. To do this, he declared martial law, arrested opposing members of parliament, demonstrators, and anti-government groups. Rhee was subsequently elected by a wide margin.

Rhee regained control of parliament in the 1954 elections, and thereupon pushed through an amendment to exempt himself from the eight-year term limit, and was once again re-elected in 1956. Soon after, Rhee's administration arrested members of the opposing party and executed the leader after accusing him of being a North Korean spy.

The administration became increasingly repressive while dominating the political arena, and in 1958, it sought to amend the National Security Law to tighten government control over all levels of administration, including the local units. These measures caused much outrage among the people, but despite public outcry, Rhee's administration rigged the March 15, 1960 presidential elections and won by a landslide.

On that election day, protests by students and citizens against the irregularities of the election burst out in the city of Masan. Initially these protests were quelled with force by local police, but when the body of a student was found floating in the harbor of Masan, the whole country was enraged and protests spread nationwide. On April 19, students from various universities and schools rallied and marched in protest in the Seoul streets, in what would be called the April Revolution. The government declared martial law, called in the army, and suppressed the crowds with open fire. Subsequent protests throughout the country shook the government, and after an escalated protest with university professors taking to the streets on April 25, Rhee submitted his official resignation on April 26 and fled into exile.

After the student revolution, power was briefly held by an interim administration under the foreign minister Heo Jeong. A new parliamentary election was held on July 29, 1960. The Democratic Party, which had been in the opposition during the First Republic, easily gained power and the Second Republic was established. The revised constitution dictated the Second Republic to take the form of a parliamentary cabinet system where the President took only a nominal role. This was the first and the only instance South Korea turned to a parliamentary cabinet system instead of a presidential system. The assembly elected Yun Bo-seon as President and Chang Myon as the prime minister and head of government in August, 1960.

The Second Republic saw the proliferation of political activity which had been repressed under the Rhee regime. Much of this activity was from leftist and student groups, which had been instrumental in the overthrow of the First Republic. Union membership and activity grew rapidly during the later months of 1960, including the Teachers' Union, Journalists' Union, and the Federation of Korean Trade Union. Around 2,000 demonstrations were held during the eight months of the Second Republic.

Under pressure from the left, the Chang government carried out a series of purges of military and police officials who had been involved in anti-democratic activities or corruption. A Special Law to this effect was passed on October 31, 1960. 40,000 people were placed under investigation; of these, more than 2,200 government officials and 4,000 police officers were purged. In addition, the government considered reducing the size of the army by 100,000, although this plan was shelved.

In economic terms as well, the government was faced with mounting instability. The government formulated a five-year economic development plan, although it was unable to act on it prior to being overthrown. The Second Republic saw the "hwan" lose half of its value against the dollar between fall 1960 and spring 1961.

Although the government had been established with support of the people, it had failed to implement effective reforms which brought about endless social unrest, political turmoil and ultimately, the 16 May coup d'état.

The May 16 coup, led by Major General Park Chung-hee on May 16, 1961, put an effective end to the Second Republic. Park was one of a group of military leaders who had been pushing for the de-politicization of the military. Dissatisfied with the cleanup measures undertaken by the Second Republic and convinced that the current disoriented state would collapse into communism, they chose to take matters into their own hands.

The National Assembly was dissolved and military officers replaced the civilian officials. In May 1961, the junta declared "Pledges of the Revolution": anticommunism was to be the nation's main policy; friendly relations would be strengthened with allies of the free world, notably the United States; all corruption and government misdeed would be disposed and "fresh and clean morality" would be introduced; the reconstruction of a self-reliant economy would be priority; the nation's ability would be nurtured to fight against communism and achieve reunification; and that government would be returned to a democratic civilian government within two years.

As a means to check the opposition, the military authority created the Korean Central Intelligence Agency (KCIA) in June 1961, with Kim Jong-pil, a relative of Park, as its first director. In December 1962, a referendum was held on returning to a presidential system of rule, which was allegedly passed with a 78% majority. Park and the other military leaders pledged not to run for office in the next elections. However, Park became presidential candidate of the new Democratic Republican Party (DRP), which consisted of mainly KCIA officials, ran for president and won the election of 1963 by a narrow margin.

Park's administration started the Third Republic by announcing the Five Year Economic development Plan, an export-oriented industrialization policy. Top priority was placed on the growth of a self-reliant economy and modernization; "Development First, Unification Later" became the slogan of the times and the economy grew rapidly with vast improvement in industrial structure, especially in the basic and heavy chemical industries. Capital was needed for such development, so the Park regime used the influx of foreign aid from Japan and the United States to provide loans to export businesses, with preferential treatment in obtaining low-interest bank loans and tax benefits. Cooperating with the government, these businesses would later become the "chaebol".

Relations with Japan were normalized by the Korea-Japan treaty ratified in June 1965. This treaty brought Japanese funds in the form of loans and compensation for the damages suffered during the colonial era without an official apology from the Japanese government, sparking much protest across the nation.

The government also kept close ties with the United States, and continued to receive large amounts of aid. A status of forces agreement was concluded in 1966, clarifying the legal situation of the US forces stationed there. Soon thereafter, Korea joined the Vietnam War, eventually sending a total of 300,000 soldiers from 1964 to 1973 to fight alongside US troops and South Vietnamese Armed Forces.

Economic and technological growth during this period improved the standard for living, which expanded opportunities for education. Workers with higher education were absorbed by the rapidly growing industrial and commercial sectors, and urban population surged. Construction of the Gyeongbu Expressway was completed and linked Seoul to the nation's southeastern region and the port cities of Incheon and Busan. Despite the immense economic growth, however, the standard of living for city laborers and farmers was still low. Laborers were working with low wages to increase the price competitiveness for the export-oriented economy plan, and farmers were in near poverty as the government controlled prices. As the rural economy steadily lost ground and caused dissent among the farmers, however, the government decided to implement measures to increase farm productivity and income by instituting the Saemauel Movement ("New Village Movement") in 1971. The movement's goal was to improve the quality of rural life, modernize both rural and urban societies and narrow the income gap between them.

Park ran again in the election of 1967, taking 51.4% of the vote. At the time the presidency was constitutionally limited to two terms, but a constitutional amendment was forced through the National Assembly in 1969 to allow him to seek a third term. Major protests and demonstrations against the constitutional amendment broke out, with large support gaining for the opposition leader Kim Dae-jung, but Park was again re-elected in the 1971 presidential election.

Parliamentary elections followed shortly after the presidential election where the opposition party garnered most of the seats, giving them the power to pass constitutional amendments. Park, feeling threatened, declared a state of national emergency on December 6, 1971. In the midst of this domestic insecurity, the Nixon Doctrine had eased tensions among the world superpowers on the international scene, which caused a dilemma for Park, who had justified his regime based on the state policy of anti-communism. In a sudden gesture, the government proclaimed a joint communiqué for reunification with North Korea on July 4, 1972, and held Red Cross talks in Seoul and Pyongyang. However, there was no change in government policy regarding reunification, and on October 17, 1972, Park declared martial law, dissolving the National Assembly and suspending the constitution.

The Fourth Republic began with the adoption of the Yushin Constitution on November 21, 1972. This new constitution gave Park effective control over the parliament and the possibility of permanent presidency. The president would be elected through indirect election by an elected body, and the term of presidency was extended to six years with no restrictions on reappointment. The legislature and judiciary were controlled by the government, and educational guidelines were under direct surveillance as well. Textbooks supporting the ideology of the military government were authorized by the government, diminishing the responsibilities of the Ministry of Education.

Despite social and political unrest, the economy continued to flourish under the authoritarian rule with the export-based industrialization policy. The first two five-year economic development plans were successful, and the 3rd and 4th five-year plans focused on expanding the heavy and chemical industries, raising the capability for steel production and oil refining. However, large conglomerate "chaebols" continuously received preferential treatment and came to dominate the domestic market. As most of the development had come from foreign capital, most of the profit went back to repaying the loans and interest.

Students and activists for democracy continued their demonstrations and protests for the abolition of the Yushin system and in the face of continuing popular unrest, Park's administration promulgated emergency decrees in 1974 and 1975, which led to the jailing of hundreds of dissidents. The protests grew larger and stronger, with politicians, intellectuals, religious leaders, laborers and farmers all joining in the movement for democracy. In 1978, Park was elected to another term by indirect election, which was met with more demonstrations and protests. The government retaliated by removing the opposition leader Kim Young-sam from the assembly and suppressing the activists with violent means. In 1979, mass anti-government demonstrations occurred nationwide, in the midst of this political turmoil, Park Chung-hee was assassinated by the director of the KCIA, Kim Jae-gyu, thus bringing the 18-year rule of military regime to an end.

After the assassination of Park Chung-hee, prime minister Choi Kyu-hah took the president's role only to be usurped 6 days later by Major General Chun Doo-hwan's 1979 Coup d'état of December Twelfth. In May of the following year, a vocal civil society composed primarily of university students and labour unions led strong protests against authoritarian rule all over the country. Chun Doo-hwan declared martial law on May 17, 1980, and protests escalated. Political opponents Kim Dae-jung and Kim Jong-pil were arrested, and Kim Young-sam was confined to house arrest.

On May 18, 1980, a confrontation broke out in the city of Gwangju between protesting students of Chonnam National University and the armed forces dispatched by the Martial Law Command. The incident turned into a citywide protest that lasted nine days until May 27 and resulted in the Gwangju massacre. Immediate estimates of the civilian death toll ranged from a few dozen to 2000, with a later full investigation by the civilian government finding nearly 200 deaths and 850 injured. In June 1980, Chun ordered the National Assembly to be dissolved. He subsequently created the National Defense Emergency Policy Committee, and installed himself as a member. On 17 July, he resigned his position of KCIA Director, and then held only the position of committee member. In September 1980, President Choi Kyu-ha was forced to resign from president to give way to the new military leader, Chun.

In September of that year, Chun was elected president by indirect election and inaugurated in March of the following year, officially starting the 5th Republic. A new Constitution was established with notable changes; maintaining the presidential system but limiting it to a single 7-year term, strengthening the authority of the National Assembly, and conferring the responsibilities of appointing judiciary to the Chief Justice of the Supreme Court. However, the system of indirect election of the president stayed and many military persons were appointed to highly ranked government positions, keeping the remnants of the Yushin era.

The government promised a new era of economic growth and democratic justice. Tight monetary laws and low interest rates contributed to price stability and helped the economy boom with notable growth in the electronics, semi-conductor, and automobile industries. The country opened up to foreign investments and GDP rose as Korean exports increased. This rapid economic growth, however, widened the gap between the rich and the poor, the urban and rural regions, and also exacerbated inter-regional conflicts. These dissensions, added to the hard-line measures taken against opposition to the government, fed intense rural and student movements, which had continued since the beginning of the republic.

In foreign policy, ties with Japan were strengthened by state visits by Chun to Japan and Japanese Prime Minister Nakasone Yasuhiro to Korea. U.S. President Ronald Reagan also paid a visit, and relations with the Soviet Union and China improved. The relationship with North Korea was strained when in 1983 a terrorist bomb attack in Burma killed 17 high-ranking officials attending memorial ceremonies and North Korea was alleged to be behind the attacks. However, in 1980 North Korea had submitted a "one nation, two system" reunification proposal which was met with a suggestion from the South to meet and prepare a unification constitution and government through a referendum. The humanitarian issue of reuniting separated families was dealt with first, and in September 1985, families from both sides of the border made cross visits to Seoul and Pyongyang in an historic event.The government made many efforts for cultural development: the National Museum of Korea, Seoul Arts Center, and National Museum of Contemporary Art were all constructed during this time. The 1986 Asian Games were held successfully, and the bid for the 1988 Summer Olympics in Seoul was successful as well.

Despite economic growth and success in diplomatic relations, the government that gained power by coup d'etat was essentially a military regime and the public's support and trust in it was low when the promises for democratic reform never materialized. In the 1985 National Assembly elections, opposition parties won more votes than the government party, clearly indicating that the public wanted a change. Many started to sympathize with the protesting students. The Gwangju Massacre was never forgotten and in January 1987, when a protesting Seoul National University student died under police interrogation, public fury was immense. In April 1987, President Chun made a declaration that measures would be taken to protect the current constitution, instead of reforming it to allow for the direct election of the president. This announcement consolidated and strengthened the opposition; in June 1987, more than a million students and citizens participated in the nationwide anti-government protests of the June Democracy Movement.

On June 29, 1987, the government's presidential nominee Roh Tae-woo gave in to the demands and announced the June 29 Declaration, which called for the holding of direct presidential elections and restoration of civil rights. In October 1987 a revised Constitution was approved by a national referendum and direct elections for a new president were held in December, bringing the 5th Republic to a close.

The Sixth Republic was established in 1987 and remains the current republic of South Korea.

Roh Tae-woo became president for the 13th presidential term in the first direct presidential election in 16 years. Although Roh was from a military background and one of the leaders of Chun's coup d'état, the inability of the opposition leaders Kim Dae Jung and Kim Young Sam to agree on a unified candidacy, led to his being elected. The first female presidential candidate, Hong Sook-ja, even withdrew from the race in order to back Kim Young Sam against Roh.Roh was officially inaugurated in February 1988. The government set out to eliminate past vestiges of authoritarian rule, by revising laws and decrees to fit democratic provisions. Freedom of the press was expanded, university autonomy recognised, and restrictions on overseas travels were lifted. However, the growth of the economy had slowed down compared to the 1980s, with strong labor unions and higher wages reducing the competitiveness of Korean products on the international market, resulting in stagnant exports, while commodity prices kept on rising.

Shortly after Roh's inauguration, the Seoul Olympics took place, raising South Korea's international recognition and also greatly influencing foreign policy. Roh's government announced the official unification plan, "Nordpolitik", and established diplomatic ties with the Soviet Union, China, and countries in East Europe.

A historic event was held in 1990 when North Korea accepted the proposal for exchange between the two Koreas, resulting in high-level talks, and cultural and sports exchanges. In 1991, a joint communiqué on denuclearization was agreed upon, and the two Koreas simultaneously became members of the UN.

Kim Young-sam was elected president in the 1992 elections after Roh's tenure. He was the country's first civilian president in 30 years and promised to build a "New Korea". The government set out to correct the mistakes of the previous administrations. Local government elections were held in 1995, and parliamentary elections in 1996. In a response to popular demand, former presidents Chun and Roh were both indicted on charges linked to bribery, illegal funds, and in the case of Chun, responsibility for the incident in Gwangju. They were tried and sentenced to prison in December, 1996.

Relations with the North improved and a summit meeting was planned, but postponed indefinitely with the death of Kim Il-sung. Tensions varied between the two Koreas thereafter, with cycles of small military skirmishes and apologies. The government also carried out substantial financial and economical reforms, joining the OECD in 1996, but encountered difficulties with political and financial scandals which involves his son. The country also faced a variety of catastrophes which claimed many lives: a train collision and a ship sinking in 1993, and the Seongsu Bridge and Sampoong Department Store collapses in 1994 and 1995. These incidents were a blow to the civilian government.

In 1997, the nation suffered a severe financial crisis, and the government approached the International Monetary Fund for relief funds. This was the limit to what the nation could bear and led to the opposition leader Kim Dae-jung winning the presidency in the same year. This is the first time an opposition candidate won the presidency.

In February 1998, Kim Dae-jung was officially inaugurated. South Korea had maintained its commitment to democratize its political processes and this was the first transfer of the government between parties by peaceful means. Kim's government faced the daunting task of overcoming the economic crisis, but with the joint efforts of the government's aggressive pursuit of foreign investment, cooperation from the industrial sector, and the citizen's gold-collecting campaign, the country was able to come out of the crisis in a relatively short period of time.

Industrial reconstruction of the big conglomerate "chaebols" was pursued, a national pension system was established in 1998, educational reforms were carried out, government support for the IT field was increased, and notable cultural properties were registered as UNESCO Cultural Heritage sites. The 2002 FIFA World Cup, co-hosted with Japan, was a major cultural event where millions of supporters gathered to cheer in public places.

In diplomacy, Kim Dae-jung pursued the "Sunshine Policy", a series of efforts to reconcile with North Korea. This culminated in reunions of the separated families of the Korean War and a summit talk with North Korean leader Kim Jong-il. For these efforts, Kim Dae-jung was awarded the Nobel Peace Prize in 2000. However, between a lack of peaceful cooperation from North Korea and the terrorist attacks on the United States on September 11, 2001, changing the view of the U.S. on North Korea, the efficacy of the Sunshine Policy was brought into question. With added allegations of corruption, support waned in the later years of the administration.

Roh Moo-hyun was elected to the presidency in December 2002 by direct election. His victory came with much support from the younger generation and civic groups who had hopes of a participatory democracy, and Roh's administration consequently launched with the motto of "participation government". Unlike the previous governments, the administration decided to take a long-term view and execute market-based reforms at a gradual pace. This approach did not please the public, however, and by the end of 2003, approval ratings were falling.

The Roh administration succeeded in overcoming regionalism in South Korean politics, diluting the collusive ties between politics and business, empowering the civil society, settling the Korea-United States FTA issue, continuing summit talks with North Korea, and launching the high-speed train system, KTX. But despite a boom in the stock market, youth unemployment rates were high, real estate prices skyrocketed and the economy lagged.

In March 2004, the National Assembly voted to impeach Roh on charges of breach of election laws and corruption. This motion rallied his supporters and affected the outcome of the parliamentary election held in April, with the ruling party becoming the majority. Roh was reinstated in May by the Constitutional Court, who had overturned the verdict. However, the ruling party then lost its majority in by-elections in 2005, as discontinued reform plans, continual labor unrest, Roh's personal feuds with the media, and diplomatic friction with the United States and Japan caused criticism of the government's competence on political and socioeconomic issues and on foreign affairs.

In April 2009, Roh Moo-hyun and his family members were investigated for bribery and corruption; Roh denied the charges. On 23 May 2009, Roh committed suicide by jumping into a ravine.

Roh's successor, Lee Myung-bak, was inaugurated in February 2008. Stating "creative pragmatism" as a guiding principle, Lee's administration set out to revitalize the flagging economy, re-energize diplomatic ties, stabilize social welfare, and meet the challenges of globalization. In April 2008, the ruling party secured a majority in the National Assembly elections. Also that month, summit talks with the United States addressed the Korea-US Free Trade Agreement and helped ease tensions between the two countries caused by the previous administrations. Lee agreed to lift the ban on US beef imports, which caused massive protests and demonstrations in the months that followed, as paranoia of potential mad cow disease gripped the country.

Many issues plagued the government in the beginning of the administration: controversies regarding the appointment of high-ranking government officials, rampant political conflicts, accusations of oppression of media and strained diplomatic relationships with North Korea and Japan. The economy was affected by the global recession as the worst economic crisis since 1997 hit the country. The Lee administration tackled these issues by actively issuing statements, reshuffling the cabinet, and implementing administrative and industrial reforms.

After regulatory and economic reforms, the economy bounced back, with the country's economy marking growth and apparently recovering from the global recession. The administration also pursued improved diplomatic relations by holding summit talks with the United States, China and Japan, and participating in the ASEAN-ROK Commemorative Summit to strengthen ties with other Asian countries. The 2010 G20 summit was held in Seoul, where issues regarding the global economic crisis were discussed.

Park Geun-hye was inaugurated in February 2013. She was the eleventh President of South Korea and is the eldest child of South Korea's third President, Park Chung-hee. She was the first woman to be elected South Korean president, but didn't successfully serve the 18th presidential term because a scandal led to her impeachment in December 2016. She was the first woman to be elected as a head of state in the modern history of Northeast Asia. However, after a corruption scandal involving Choi Soon-sil quickly blew up after reports from multiple news organizations (the most notable of which was JTBC) in 2016, nationwide protests ensued on a weekly basis, with participant count hitting a maximum of over 2.3 million (as reported by the protesters). These protests turned out to be the biggest series of mass protests in Korean history. The protests continued even after the Congress voted on Park's impeachment. Prime Minister Hwang Kyo-ahn acted as President of South Korea pending completion of investigations into the actions of Park Geun-hye, and in the absence of any intervening election. The impeachment was upheld by the Constitutional Court on 10 March 2017, ending Park's presidency and forcing her out of office.

Moon Jae-in is the current president of South Korea. He was inaugurated on May 10, 2017.





</doc>
<doc id="27021" url="https://en.wikipedia.org/wiki?curid=27021" title="Geography of South Korea">
Geography of South Korea

South Korea is located in East Asia, on the southern half of the Korean Peninsula jutting out from the far east of the Asian landmass. The only country with a land border to South Korea is North Korea, lying to the north with of border running along the Korean Demilitarized Zone. South Korea is mostly surrounded by water and has of coastline along three seas; to the west is the Yellow Sea (known in Korea as West Sea), to the south is the East China Sea, and to the east is the Sea of Japan (known in Korea as East Sea). Geographically, South Korea's land mass is approximately . of South Korea are occupied by water. The approximate coordinates are 37° North, 127° 30 East. Notable islands include Jeju Island (Jejudo), Ulleung Island (Ulleungdo)

The Korean Peninsula extends southward from the northeast part of the Asian continental landmass. The Japanese islands of Honshū and Kyūshū are located some 200 km (124 mi) to the southeast across the Korea Strait; the Shandong Peninsula of China lies 190 kilometers to the west. The west coast of the peninsula is bordered by the Korea Bay to the north and the Yellow Sea and Korea Strait to the south; the east coast is bordered by the Sea of Japan. The 8,640-kilometer coastline is highly indented. Some 3,579 islands lie adjacent to the peninsula. Most of them are found along the south and west coasts.

The line between the two Korean states was the thirty-eighth parallel of latitude. After the Korean War, the Korean Demilitarized Zone (DMZ) formed the boundary between the two. The DMZ is a heavily guarded, 4,000-meter-wide strip of land that runs along the Demarcation line established by the Korean Armistice Agreement, from the east to the west coasts for a distance of 241 kilometers (238 kilometers of that line from the land boundary with North Korea).

The total land area of the peninsula, including the islands, is 223,170 square kilometers. Some 44.8 percent (100 210 square kilometers) of this total, excluding the area within the DMZ, constitutes the territory of the Republic of Korea. The combined territories of North Korea and South Korea are about the same size as the U.S. state of Minnesota. South Korea alone is about the size of Portugal or Hungary.

The largest island, Jeju-do, lies off the southwest corner of the peninsula and has a land area of 1,825 square kilometers. Other important islands include Ulleung and Liancourt Rocks in the Sea of Japan and Ganghwa Island at the mouth of the Han River. Although the eastern coastline of South Korea is generally unindented, the southern and western coasts are jagged and irregular. The difference is caused by the fact that the eastern coast is gradually rising, while the southern and western coasts are subsiding.

Early European visitors to Korea remarked that the land resembled "a sea in a heavy gale" because of the large number of successive mountain ranges that crisscross the peninsula. The highest mountains are in North Korea. The highest mountain peak in South Korea is Hallasan (1,950 m), which is the cone of a volcanic formation constituting Jeju Island. There are three major mountain ranges within South Korea: the Taebaek Mountains, and Sobaek ranges, and the Jiri Massif.

Unlike Japan or the northern provinces of China, the Korean Peninsula is geologically stable. There are no active volcanoes (aside from Baekdu Mountain on the border between North Korea and China, most recently active in 1903), and there have been no strong earthquakes. Historical records, however, describe volcanic activity on Mount Halla during the Goryeo Dynasty (918–1392).

South Korea has no extensive plains; its lowlands are the product of mountain erosion. Approximately 30 percent of the area of South Korea consists of lowlands, with the rest consisting of uplands and mountains. The great majority of the lowland area lies along the coasts, particularly the west coast, and along the major rivers. The most important lowlands are the Han River plain around Seoul, the Pyeongtaek coastal plain southwest of Seoul, the Geum River basin, the Nakdong River basin, and the Yeongsan River and the Honam plains in the southwest. A narrow littoral plain extends along the east coast.

The Nakdong is South Korea's longest river (521 kilometers). The Han River, which flows through Seoul, is 514 kilometers long, and the Geum River is 401 kilometers long. Other major rivers include the Imjin, which flows through both North Korea and South Korea and forms an estuary with the Han River; the Bukhan, a tributary of the Han that also flows out of North Korea; and the Somjin. The major rivers flow north to south or east to west and empty into the Yellow Sea or the Korea Strait. They tend to be broad and shallow and to have wide seasonal variations in water flow.

In the early part of 20th century and especially the period during and after World War II and the Korean War, much of the existing Korean forests were cut down, which led to problems with flooding and soil erosion. Combination of reforestation efforts (e.g. Arbor day was celebrated as a national holiday starting in 1949) and policies designed to reduce use of firewood as a source of energy (e.g. restriction of inflow of firewood into Seoul and other major cities starting in 1958) helped to spark a recovery in the 1950s. Comprehensive reforestation programs starting in the 1970s and continuing into the late 1990s aided in an acceleration of forest volume increase, and the forest cover reached a peak of 65% of national land area in 1980 as opposed to a low of 35% in 1955.

News that North Korea was constructing a huge multipurpose dam at the base of Geumgangsan (1,638 m) north of the DMZ caused considerable consternation in South Korea during the mid-1980s. South Korean authorities feared that once completed, a sudden release of the dam's waters into the Pukhan River during north-south hostilities could flood Seoul and paralyze the capital region. During 1987 the Geumgangsan Dam was a major issue that Seoul sought to raise in talks with Pyongyang. Though Seoul completed a "Peace Dam" on the Pukhan River to counteract the potential threat of Pyongyang's dam project before the 1988 Olympics, the North Korean project apparently still was in its initial stages of construction in 1990.

Maritime claims:
"territorial sea:"
"contiguous zone:"

"exclusive economic zone:"

"continental shelf:"
not specified

Elevation extremes:
"lowest point:"
Sea level 0 m
"highest point:"
Hallasan 1,950 m

Part of the East Asian Monsoon region, South Korea has a temperate climate with four distinct seasons. The movement of air masses from the Asian continent exerts greater influence on South Korea's weather than does air movement from the Pacific Ocean. Winters are usually long, cold, and dry, whereas summers are short, hot, and humid. Spring and autumn are pleasant but short in duration. Seoul's mean temperature in January is ; in July the mean temperature is about . Because of its southern and seagirt location, Jeju Island has warmer and milder weather than other parts of South Korea. Mean temperatures on Jeju range from in January to in July.

The country generally has sufficient rainfall to sustain its agriculture. Rarely does less than of rain fall in any given year; for the most part, rainfall is over . Amounts of precipitation, however, can vary from year to year. Serious droughts occur about once every eight years, especially in the rice-producing southwestern part of the country. About two-thirds of the annual precipitation occurs between June and September.

South Korea is less vulnerable to typhoons than Japan, Taiwan, the east coast of China, or the Philippines. From one to three typhoons can be expected per year. Typhoons usually pass over South Korea in late summer, especially in August, and bring torrential rains. Flooding occasionally causes considerable damage, as do landslides, given the country's generally mountainous terrain.

In September 1984, record floods caused the deaths of 190 people and left 200,000 homeless. This disaster prompted the North Korean government to make an unprecedented offer of humanitarian aid in the form of rice, medicine, clothes, and building materials. South Korea accepted these items and distributed them to flood victims.

Graphically the seasons can be represented this way:

Natural resources:
South Korea produces coal, tungsten, graphite, molybdenum, lead, and has potential for hydropower.

Land use:
"arable land:"
15.3%
"permanent crops:"
2.2%
"permanent pasture:"
0.6%
"forest:"
63.9%
"other:"
18.0% (2011)

Irrigated land:
8,804 km² (2003)

Total renewable water resources:
69.7 km

Freshwater withdrawal (domestic/industrial/agricultural)
<br>"total:"
25.47 km/yr (26%/12%/62%)
<br>"per capita:"
548.7 m/yr (2003)

Natural hazards:
There are occasional typhoons that bring high winds and floods. There is also low-level seismic activity, which is common in the southwest.

Volcanism:
Hallasan (elev. 1,950 m) is considered historically active although it has not erupted in many centuries. Earthquake activity is minimal, however, since 2016 there have been two earthquakes over 5.4 magnitude.

Environment - current issues:
Habitat loss and degradation, especially of wetlands, through coastal reclamation (e.g. Saemangeum, Shiwa, Song Do, Namyang Bay, Asan Bay, in the south-west, Gwangyang Bay and the Nakdong Estuary) have caused huge declines in fisheries and of biodiversity. Most riverine wetland in Korea is now threatened by the proposed Grand Korean Waterway project. There are also some problems air pollution in large cities; as well as water pollution from the discharge of sewage and industrial effluents. Drift netting is another issue.

Environment - international agreements:
"party to:"
Antarctic-Environmental Protocol, Antarctic-Marine Living Resources, Antarctic Treaty, Biodiversity, Climate Change, Climate Change-Kyoto Protocol, Desertification, Endangered Species, Environmental Modification, Hazardous Wastes, Law of the Sea, Marine Dumping, Ozone Layer Protection, Ship Pollution (MARPOL 73/78), Tropical Timber 83, Tropical Timber 94, Wetlands, Whaling
"signed, but not ratified:" none of the selected agreements



</doc>
<doc id="27022" url="https://en.wikipedia.org/wiki?curid=27022" title="Demographics of South Korea">
Demographics of South Korea

This article is about the demographic features of the population of South Korea, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.

In June 2012, South Korea's population reached 50 million. Since the 2000s, South Korea has been struggling with a low birthrate, leading some researchers to suggest that if current population trends hold, the country's population will shrink to approximately 38 million population towards the end of the 21st century The demographics of South Korea include the population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population. In 2016, South Korea's population was 51.25 million people.

In South Korea, a variety of different Asian people had migrated to the Korean Peninsula in past centuries, however few have remain permanent. South Korea and North Korea are among the world's most ethnically homogenous nations. Both North Korea and South Korea equate nationality or citizenship with membership in a single, homogenous ethnic group and politicized notion of "race."

The common language and especially race are viewed as important elements by South Koreans in terms of identity, more than citizenship.

Population of South Korea by age and sex (demographic pyramid)

According to Worldometers' South Korea Population Forecast statistics, South Korea is suppose to have a .36% yearly change increase by 2020, a .28% yearly change increase by 2025, a .18% yearly change increase by 52,701,817, and a .04% yearly change increase by 2035. According to those same statistics, the years from 2040 to 2050 are suppose to have an steady decline of yearly change percentages.

The population of South Korea showed robust growth since the republic's establishment in 1948, and then dramatically slowed down with the effects of its economic growth. In the first official census, taken in 1949, the total population of South Korea was calculated at 20,188,641 people. The 1985 census total was 40,466,577. Population growth was slow, averaging about 1.1% annually during the period from 1949 to 1955, when the population registered at 21.5 million. Growth accelerated between 1955 and 1966 to 29.2 million or an annual average of 2.8%, but declined significantly during the period 1966 to 1985 to an annual average of 1.7%. Thereafter, the annual average growth rate was estimated to be less than 1%, similar to the low growth rates of most industrialized countries and to the target figure set by the Ministry of Health and Social Affairs for the 1990s. As of January 1, 1989, the population of South Korea was estimated to be approximately 42.2 million.

The proportion of the total population under fifteen years of age has risen and fallen with the growth rate. In 1955 approximately 41.2% of the population was under fifteen years of age, a percentage that rose to 43.5% in 1966 before falling to 38.3% in 1975, 34.2% in 1980, and 29.9% in 1985. In the past, the large proportion of children relative to the total population put great strains on the country's economy, particularly because substantial resources were invested in education facilities. With the slowdown in the population growth rate and a rise in the median age (from 18.7 years to 21.8 years between 1960 and 1980), the age structure of the population has begun to resemble the columnar pattern typical of developed countries, rather than the pyramidal pattern found in most parts of the Third World.

The decline in the population growth rate and in the proportion of people under fifteen years of age after 1966 reflected the success of official and unofficial birth control programs. The government of President Syngman Rhee (1948–60) was conservative in such matters. Although Christian churches initiated a family planning campaign in 1957, it was not until 1962 that the government of Park Chung Hee, alarmed at the way in which the rapidly increasing population was undermining economic growth, began a nationwide family planning program. Other factors that contributed to a slowdown in population growth included urbanization, later marriage ages for both men and women, higher education levels, a greater number of women in the labor force, and better health standards.

Public and private agencies involved in family planning included the Ministry of Health and Social Affairs, the Ministry of Home Affairs, the Planned Parenthood Federation of Korea, and the Korea Institute of Family Planning. In the late 1980s, their activities included distribution of free birth control devices and information, classes for women on family planning methods, and the granting of special subsidies and privileges (such as low-interest housing loans) to parents who agreed to undergo sterilization. There were 502,000 South Koreans sterilized in 1984, as compared with 426,000 in the previous year.

The 1973 Maternal and Child Health Law legalized abortion. In 1983 the government began suspending medical insurance benefits for maternal care for pregnant women with three or more children. It also denied tax deductions for education expenses to parents with two or more children.

As in China, cultural attitudes posed problems for family planning programs. A strong preference for sons—who in Korea's traditional Confucian value system are expected to care for their parents in old age and carry on the family name—means that parents with only daughters usually continued to have children until a son is born. The government encouraged married couples to have only one child. This has been a prominent theme in public service advertising, which stresses "have a single child and raise it well."

Total fertility rates (the average number of births a woman will have during her lifetime) fell from 6.1 births per female in 1960 to 4.2 in 1970, 2.8 in 1980, and 2.4 in 1984. The number of live births, recorded as 711,810 in 1978, grew to a high of 917,860 in 1982. This development stirred apprehensions among family planning experts of a new "baby boom." By 1986, however, the number of live births had declined to 806,041.

Decline in population growth continued, and between 2005 and 2010 total fertility rate for South Korean women was 1.21, one of the world's lowest according to the United Nations. Fertility rate well below the replacement level of 2.1 births per female has triggered a national alarm, with dire predictions of an aging society unable to grow or support its elderly. Recent Korean governments have prioritized the issue on its agenda, promising to enact social reforms that will encourage women to have children.

The country's population increased to 46 million by the end of the twentieth century, with growth rates ranging between 0.9% and 1.2%. The population is expected to stabilize (that is, cease to grow) in the year 2023 at around 52.6 million people. In the words of Asiaweek magazine, the "stabilized tally will approximate the number of Filipinos in 1983, but squeezed into less than a third of their [the Philippines'] space."

South Korea is one of the world's most densely populated countries, with an estimated 425 people per square kilometer in 1989—over sixteen times the average population density of the United States in the late 1980s. By comparison, China had an estimated 114 people, the Federal Republic of Germany (West Germany) 246 people, and Japan 323 people per square kilometer in the late 1980s. Because about 70% of South Korea's land area is mountainous and the population is concentrated in the lowland areas, actual population densities were in general greater than the average. As early as 1975, it was estimated that the density of South Korea's thirty-five cities, each of which had a population of 50,000 or more inhabitants, was 3,700 people per square kilometer. Because of continued migration to urban areas, the figure was doubtless higher in the late 1980s.

In 1988 Seoul had a population density of 17,030 people per square kilometer as compared with 13,816 people per square kilometer in 1980. The second largest city, Busan, had a density of 8,504 people per square kilometer in 1988 as compared with 7,272 people in 1980. Kyonggi Province, which surrounds the capital and contains Inch'on, the country's fourth largest city, was the most densely populated province; Kangwon Province in the northeast was the least densely populated province.

According to the government's Economic Planning Board, the population density will be 530 people per square kilometer by 2023, the year the population is expected to stabilize.

Rural areas in South Korea consist of agglomerated villages in river valleys and range from a few houses to several hundred. These villages are located in the south that are backed by hills and give strong protection from winter winds.

Since 1960, the pace of urbanization in South Korea has hit a considerable decline in population of rural areas and the traditional rural lifestyle has been slowly fading away.

South Korea faces the problem of a rapidly aging population. In fact, the speed of aging in Korea is unprecedented in human history, 18 years to double aging population from 7 – 14% (least number of years), overtaking even Japan. Statistics support this observation, the percentage of elderly aged 65 and above, has sharply risen from 3.3% in 1955 to 10.7% in 2009. The shape of its population has changed from a pyramid in the 1990s, with more young people and fewer old people, to a diamond shape in 2010, with less young people and a large proportion of middle-age individuals.

There are several implications and issues associated with an aging population. A rapidly aging population is likely to have several negative implications on the labour force. In particular, experts predict that this might lead to a shrinking of the labour force. As an increasing proportion of people enter their 50s and 60s, they either choose to retire or are forced to retire by their companies. As such, there would be a decrease in the percentage of economically active people in the population. Also, with rapid aging, it is highly likely that there would be an imbalance in the young-old percentage of the workforce. This might lead to a lack of vibrancy and innovation in the labour force, since it is helmed mainly by the middle-age workers. Data shows that while there are fewer young people in society, the percentage of economically active population, made up of people ages 15 – 64, has gone up by 20% from 55.5% to 72.5%. This shows that the labour force is indeed largely made up of middle-aged workers.

A possible consequence might be that South Korea would be a less attractive candidate for investment. Investors might decide to relocate to countries like Vietnam and China, where there is an abundance of cheaper, younger labour. If employers were to choose to maintain operations in South Korea, there is a possibility that they might incur higher costs in retraining or upgrading the skills of this group of middle-age workers. On top of that, higher healthcare costs might also be incurred and the government would need to set aside more money to maintain a good healthcare system to cater to the elderly.

Due to the very low birth rate, South Korea is predicted to enter a Russian Cross pattern once the large generation born in the 1960s starts to die off, with potentially decades of population decline.

Like other newly industrializing economies, South Korea experienced rapid growth of urban areas caused by the migration of large numbers of people from the countryside. In the eighteenth and nineteenth centuries, Seoul, by far the largest urban settlement, had a population of about 190,000 people. There was a striking contrast with Japan, where Edo (Tokyo) had as many as 1 million inhabitants and the urban population comprised as much as 10% to 15% of the total during the Tokugawa Period (1600–1868). During the closing years of the Choson Dynasty and the first years of Japanese colonial rule, the urban population of Korea was no more than 3% of the total. After 1930, when the Japanese began industrial development on the Korean Peninsula, particularly in the northern provinces adjacent to Manchuria, the urban portion of the population began to grow, reaching 11.6% for all of Korea in 1940.

Between 1945 and 1985, the urban population of South Korea grew from 14.5% to 65.4% of the total population. In 1988 the Economic Planning Board estimated that the urban portion of the population will reach 78.3% by the end of the twentieth century. Most of this urban increase was attributable to migration rather than to natural growth of the urban population. Urban birth rates have generally been lower than the national average. The extent of urbanization in South Korea, however, is not fully revealed in these statistics. Urban population was defined in the national census as being restricted to those municipalities with 50,000 or more inhabitants. Although many settlements with fewer than 50,000 inhabitants were satellite towns of Seoul or other large cities or mining communities in northeastern Kangwon Province, which would be considered urban in terms of the living conditions and occupations of the inhabitants, they still were officially classified as rural.

The dislocation caused by the Korean War accounted for the rapid increase in urban population during the early 1950s. Hundreds of thousands of refugees, many of them from North Korea, streamed into the cities. During the post-Korean War period, rural people left their ancestral villages in search of greater economic and educational opportunities in the cities. By the late 1960s, migration had become a serious problem, not only because cities were terribly overcrowded, but also because the rural areas were losing the most youthful and productive members of their labor force.

In 1970, the Park Chung Hee government launched the Saemaul Undong (New Community Movement) as a rural reconstruction and self-help movement to improve economic conditions in the villages, close the wide gap in income between rural and urban areas, and stem urban migration—as well as to build a political base. Despite a huge amount of government sponsored publicity, especially during the Park era, it was not clear by the late 1980s that the Saemaul undong had achieved its objectives. By that time many, if not most, farming and fishing villages consisted of older persons; relatively few able-bodied men and women remained to work in the fields or to fish. This trend was apparent in government statistics for the 1986–87 period: the proportion of people fifty years old or older living in farming communities grew from 28.7% in 1986 to 30.6% in 1987, while the number of people in their twenties living in farming communities declined from 11.3% to 10.8%. The nationwide percentages for people fifty years old or older and in their twenties were, in 1986, 14.9% and 20.2%, respectively (see Agriculture, ch. 3).

In 1985 the largest cities were Seoul (9,645,932 inhabitants), Busan (3,516,807), Daegu (2,030,672), Incheon (1,387,491), Gwangju (906,129), and Daejeon (866,695). According to government statistics, the population of Seoul, one of the world's largest cities, surpassed 10 million people in late 1988. Seoul's average annual population growth rate during the late 1980s was more than 3%. Two-thirds of this growth was attributable to migration rather than to natural increase. Surveys revealed that "new employment or seeking a new job," "job transfer," and "business" were major reasons given by new immigrants for coming to the capital. Other factors cited by immigrants included "education" and "a more convenient area to live."

To alleviate overcrowding in Seoul's downtown area, the city government drew up a master plan in the mid-1980s that envisioned the development of four "core zones" by 2000: the original downtown area, Yongdongpo-Yeouido, Yongdong, and Jamsil. Satellite towns also would be established or expanded. In the late 1980s, statistics revealed that the daytime or commuter population of downtown Seoul was as much as six times the officially registered population. If the master plan is successful, many commuters will travel to work in a core area nearer their homes, and the downtown area's daytime population will decrease. Many government ministries have been moved out of Seoul, and the army, navy, and air force headquarters have been relocated to Daejeon.

In 1985 the population of Seoul constituted 23.8% of the national total. Provincial cities, however, experienced equal and, in many cases, greater expansion than the capital. Growth was particularly spectacular in the southeastern coastal region, which encompasses the port cities of Busan, Masan, Yosu, Chinhae, Ulsan, and Pohang. Census figures show that Ulsan's population increased eighteenfold, growing from 30,000 to 551,300 inhabitants between 1960 and 1985. With the exception of Yosu, all of these cities are in South Kyongsang Province, a region that has been an especially favored recipient of government development projects. By comparison, the population of Kwangju, capital of South Cholla Province, increased less than threefold between 1960 and 1985, growing from 315,000 to 906,129 inhabitants.

Rapid urban growth has brought familiar problems to developed and developing countries alike. The construction of large numbers of high-rise apartment complexes in Seoul and other large cities alleviated housing shortages to some extent. But it also imposed hardship on the tens of thousands of people who were obliged to relocate from their old neighborhoods because they could not afford the rents in the new buildings. In the late 1980s, squatter areas consisting of one-story shacks still existed in some parts of Seoul. Housing for all but the wealthiest was generally cramped. The concentration of factories in urban areas, the rapid growth of motorized traffic, and the widespread use of coal for heating during the severe winter months caused dangerous levels of air and water pollution, issues that still persist today even after years of environmentally friendly policies.

Like other newly industrializing economies, South Korea experienced rapid growth of urban areas caused by the migration of large numbers of people from the countryside. In 2016, 82.59 percent of South Korea's total population lived in urban areas and cities.

Source:
The total fertility rate is the number of children born per woman. It is based on fairly good data for the entire period. Sources: Our World In Data and Gapminder Foundation.

Source:


South Korea is a relatively homogeneous society with an absolute majority of the population of Korean ethnicity who account for approximately 96% of the total population of the Korean Republic). However, with its emergence as an economic powerhouse, opportunities for foreign immigrants increased and in 2007 the number of foreign citizens resident in South Korea passed the million mark for the first time in history, and the number reached 2 million in 2016. 1,016,000 of them came from China, with more than half of them being ethnic Koreans of Chinese citizenship. The next largest group was from Vietnam with 149,000 residents. The third largest group was from the United States with 117,000 residents, excluding the American troops stationed in the country. Thailand, Philippines, Uzbekistan and other countries followed.

When the People's Republic of China and South Korea reformed relationship was settled in, several Chinese migrants had emerged in South Korea in 1992. In the early 1900s, a trade agreement allowed merchants from China conduct business trades in South Korea.

South Korea's immigration rules are especially strict for non-Asians. The South Koreans place various regulations on individuals applying for citizenship via marriage with one required to pass a Korean language proficiency test and have an annual income more than 14 million wons. Due to this fact, most North Americans come to the country either as tourists or professionals.

The relationship between Vietnamese and South Koreans goes back to the year 1200 when Ly Duong had left to Goreyeo in Korea after a succession of power dispute. Nowadays, Vietnamese migrants that go to South Korea are introduced to local husbands via marriage agencies.

The relationship between Filipinos and South Koreans can be traced back to the 1950s during the Korean war. Over 7,500 Filipino soldiers fought on the United Nations' side to assist South Korea's conflict with North Korea. During 2007, there was estimated to be around 70,000 Filipino immigrants in South Korea. The mass rural urban migration led to a shortage of young women in those areas. This led many Filipino brides find their way to South Korea and migrate over there.

Below are the foreigner groups in South Korea that number more than 5,000.

The Korean language is the native language spoken by the vast majority of the population. English is widely taught in both public and private schools as a foreign language. However, general fluency in English in the country is relatively low compared to other industrialized developed countries. There is a Chinese minority who speak Mandarin and Cantonese. Some elderly may still speak Japanese, which was official during the Japanese rule in Korea (1905–1945).

In different areas of South Korea, different dialects are spoken. For example, the Gyeongsang dialect spoken around Busan and Daegu to the south sounds quite rough and aggressive compared to standard Korean.

Koreans have historically, lived under the religious influences of shamanism, Buddhism, Daoism, or Confucianism.

Korea is a country where the world's most major religions, Christianity, Buddhism, Confucianism and Islam, peacefully coexist. According to 2005 statistics, 53% of Korean population has a religion and 2008 statistics show that over 510 religious organizations were in the South Korea population.


The following demographic statistics are from the CIA World Factbook, unless otherwise indicated.




Large-scale emigration from Korea began around 1904 and continued until the end of World War II. During the Korea under Japanese rule period, many Koreans emigrated to Manchuria (present-day China's northeastern provinces of Liaoning, Jilin, and Heilongjiang), other parts of China, the Soviet Union, Hawaii, and the contiguous United States.

Most emigrated for economic reasons; employment opportunities were scarce, and many Korean farmers lost their land after the Japanese introduced a system of land registration and private land tenure, imposed higher land taxes, and promoted the growth of an absentee landlord class charging exorbitant rents. Koreans from the northern provinces of Korea went mainly to Manchuria, China, and Siberia. Many people from the southern provinces went to Japan. Koreans were conscripted into Japanese labor battalions or the Japanese army, especially during World War II. In the 1940–44 period, nearly 2 million Koreans lived in Japan, 1.4 million in Manchuria, 600,000 in Siberia, and 130,000 in China. An estimated 40,000 Koreans were scattered among other countries. At the end of World War II, approximately 2 million Koreans were repatriated from Japan and Manchuria.

More than 4 million ethnic Koreans lived outside the peninsula during the early 1980s. The largest group, about 1.7 million people, lived in China, the descendants of the Korean farmers who had left the country during the Japanese occupation. Most had assumed Chinese citizenship. The Soviet Union had about 430,000 ethnic Koreans.

By contrast, many of Japan's approximately 700,000 Koreans had below-average standards of living. This situation occurred partly because of discrimination by the Japanese majority and partly because a large number of resident Koreans, loyal to the North Korean regime of Kim Il Sung, preferred to remain separate from and hostile to the Japanese mainstream. The pro–North Korea Chongryon (General Association of Korean Residents in Japan) initially was more successful than the pro–South Korea Mindan (Association for Korean Residents in Japan) in attracting adherents among residents in Japan. Since diplomatic relations were established between Seoul and Tokyo in 1965, however, the South Korean government has taken an active role in promoting the interests of their residents in Japan in negotiations with the Japanese government. It also has provided subsidies to Korean schools in Japan and other community activities.

By the end of 1988 there were over two million South Koreans residing overseas. North America was the destination of over 1.2 million. South Koreans also were residents of Australia (100,000), Central and South America (45,000), the Middle East (12,000), Western Europe (40,000), New Zealand (30,000), other Asian countries (27,000), and Africa (25,000). A limited number of South Korean government-sponsored migrants settled in Chile, Argentina, and other Latin American countries.

Because of South Korea's rapid economic expansion, an increasing number of its citizens reside abroad on a temporary basis as business executives, technical personnel, foreign students, and construction workers. A large number of formerly expatriate South Koreans have returned to South Korea primarily because of the country's much improved economic conditions and the difficulties they experienced in adjusting to living abroad.





</doc>
<doc id="27023" url="https://en.wikipedia.org/wiki?curid=27023" title="Politics of South Korea">
Politics of South Korea

The politics of the Republic of Korea takes place in the framework of a presidential representative democratic republic, whereby the President is the head of state, and of a multi-party system. Executive power is exercised by the government. Legislative power is vested in both the government and the National Assembly. The Judiciary is independent of the executive and the legislature and comprises a Supreme Court, appellate courts and a Constitutional Court. Since 1948, the constitution has undergone five major revisions, each signifying a new republic. The current Sixth Republic began with the last major constitutional revision in 1987.

The Economist Intelligence Unit has rated South Korea as the 20th most democratic country in 2017, the highest ranked Asian country and above Belgium, France or the United States.

The head of state is the president, who is elected by direct popular vote for a single five-year term. The president is Commander-in-Chief of the armed force of South Korea and enjoys considerable executive powers.

The president appoints the prime minister with approval of the National Assembly, as well as appointing and presiding over the State Council of chief ministers as the head of government. On 12 March 2004, the executive power of then president Roh Moo-hyun was suspended when the Assembly voted to impeach him and Prime Minister Goh Kun became an Acting President. On 14 May 2004, the Constitutional Court overturned the impeachment decision made by the Assembly and Roh was reinstated.

On 10 March 2017, Park Geun-hye became the first president to be removed by the Constitutional Court after impeachment by the National Assembly. Prime Minister Hwang Kyo-ahn temporarily served as an acting president between the suspension of Park since 9 December 2016 until the next presidential election, which was held in May 2017. On 9 May 2017, Moon Jae-in became the 19th president of South Korea, replacing acting president Hwang Kyo-ahn.

The National Assembly (, , "gukhoe") has 300 members, elected for a four-year term, 253 members in single-seat constituencies and 47 members by proportional representation. The ruling Democratic Party of Korea is the largest party in the Assembly.

The South Korean judiciary is independent of the other two branches. The highest judiciary body is the Supreme Court, whose justices are appointed by the president with the consent of the National Assembly. In addition, the Constitutional Court oversees questions of constitutionality. South Korea has not accepted compulsory ICJ jurisdiction.

South Korea elects on national level a head of state – the president – and a legislature. The president is elected for a five-year term by the people. The National Assembly ("Gukhoe") has 300 members, elected for a four-year term, 246 members in single-seat constituencies and 54 members by proportional representation.

The main political parties in South Korea are the liberal Democratic Party of Korea (lit. "Together Democratic Party", DPK), the conservative Liberty Korea Party (LKP), the centrist People's Party (PP), and the left-wing Justice Party (JP), etc.

The liberal camp and the conservative camp are the dominant forces of South Korean politics at present.

South Korea's political history has always been prone to splits from and merges with other parties. One reason is that there is greater empathise around the 'politics of the person' and rather than party, therefore party loyalty is not strong when disagreements occur. The graph below illustrates the extent of the political volatility within the last 10 years alone. These splits were intensified after the 2016 South Korean political scandal.


One Special City ("Teukbyeolsi", Capital City), six Metropolitan Cities ("Gwangyeoksi," singular and plural), nine Provinces ("Do," singular and plural) and one Special Autonomous City (Sejong City).


AfDB, APEC, AsDB, BIS, CP, EBRD, ESCAP, FAO, G-77, IAEA, IBRD, ICAO, ICCt, ICC, ICRM, IDA, IEA (observer), IFAD, IFC, IFRCS, IHO, ILO, IMF, IMO, Inmarsat, Intelsat, Interpol, IOC, IOM, ISO, ITU, ITUC, MINURSO, NAM (guest), NSG, OAS (observer), OECD, OPCW, OSCE (partner), UN, UNCTAD, UNESCO, UNIDO, UNMOGIP, UNOMIG, UNU, UPU, WCO, WHO, WIPO, WMO, WToO, WTrO, Zangger Committee



</doc>
<doc id="27024" url="https://en.wikipedia.org/wiki?curid=27024" title="Economy of South Korea">
Economy of South Korea

The economy of South Korea is the 4th largest in Asia and the 11th largest in the world. It is a mixed economy dominated by family-owned conglomerates called chaebols, however, the dominance of chaebol is unlikely and at risk to support the transformation of Korean economy for the future generations. South Korea is famous for its spectacular rise from one of the poorest countries in the world to a developed, high-income country in just one generation. This economic miracle, commonly known as the Miracle on the Han River, brought South Korea to the ranks of elite countries in the OECD and the G-20. South Korea still remains one of the fastest growing developed countries in the world following the Great Recession. It is included in the group of Next Eleven countries that will dominate the global economy in the middle of the 21st century.

By creating favorable policy directive for economic development as preceded by Japanese economic recovery as the logistic supplying bastion for American troops in the Korean peninsula during and after the Korean War, South Korea's rigorous education system and the establishment of a highly motivated and educated populace is largely responsible for spurring the country's high technology boom and rapid economic development. Having almost no natural resources and always suffering from human overpopulation in its small territory, which deterred continued population growth and the formation of a large internal consumer market, South Korea adapted an export-oriented economic strategy to fuel its economy, and in 2014, South Korea was the seventh largest exporter and seventh largest importer in the world. Bank of Korea and Korea Development Institute periodically release major economic indicators and economic trends of the economy of South Korea.

In the 1997 Asian financial crisis, the South Korean economy suffered a liquidity crisis and relied on the bailout by the IMF that restructured and modernized the South Korean economy with successive DJnomics policy by President Kim Dae Jung, including the resultant of the national development of the ICT industry. Historically, subsidies were used as means of speeding up adoption of new technology in Korea and has ultimately helped the adoption and development of faster mobile standards for the economy of South Korea. The growth of ICT industry has been far concentrated on the hardware sector, which focuses on expanding wired and wireless telecommunication network penetration rather than the software sector, which creates innovative applications and value-added services. The economy of South Korea is the global leader of consumer electronics, mobile broadband and smartphones. South Korea's LCD TV global market share also jumped to 37 percent in 2009, from 27 percent at the end of 2007, and it will soon replace Japan as the world’s number-one LCD TV supplier. The economy of South Korea ranks No.1 in the world in ICT Development Index 2015 and 2015 Bloomberg Innovation Index.

Despite the South Korean economy's high growth potential and apparent structural stability, South Korea suffers perpetual damage to its credit rating in the stock market due to the belligerence of North Korea in times of deep military crises, which has an adverse effect on the financial markets of the South Korean economy. However, renowned financial organizations, such as the International Monetary Fund, also compliment the resilience of the South Korean economy against various economic crises, citing low state debt, and high fiscal reserves that can quickly be mobilized to address any expected financial emergencies. Other financial organizations like the World Bank describe Korea as one of the fastest-growing major economies of the next generation along with BRIC and Indonesia. South Korea was one of the few developed countries that was able to avoid a recession during the global financial crisis, and its economic growth rate reached 6.2% in 2010, a sharp recovery from economic growth rates of 2.3% in 2008 and 0.2% in 2009 when the global financial crisis hit. The South Korean economy again recovered with the record-surplus of US$70.7 billion mark of the current account in the end of 2013, up 47 percent growth from 2012, amid uncertainties of the global economic turmoil, with major economic output being the technology products exports.

South Korea was a historical recipient of official development assistance (ODA) from OECD. Throughout the 1980s until the mid-1990s, South Korea's economic prosperity as measured in GDP by PPP per capita was still only a fraction of industrialized nations. In 1980, the South Korean GDP per capita was $2,300, about one-third of nearby developed Asian economies such as Singapore, Hong Kong, and Japan. Since then, South Korea has advanced into a developed economy to eventually attain a GDP per capita of $30,000 in 2010, almost thirteen times the figure thirty years ago. The whole country's GDP increased from $88 billion to $1,460 billion in the same time frame. In 2009, South Korea officially became the first major recipient of ODA to have ascended to the status of a major donor of ODA. Between 2008 and 2009, South Korea donated economic aid of $1.7 billion to countries other than North Korea. South Korea's separate annual economic aid to North Korea has historically been more than twice its ODA.

On June 23, 2012, South Korea was landmarked to become the 7th member of the 20-50 club (with the population surpassing 50 million and maintaining per capita income of US$20,000), chronologically, after Japan, United States of America, France, Italy, Germany and United Kingdom. A free trade agreement between the United States of America and the Republic of Korea was concluded on April 1, 2007. The European Union–South Korea Free Trade Agreement was signed on 15 October 2009. The South Korean economy is heavily dependent on the energy imports and the related refinery technologies in association with the Ministry of Knowledge Economy and in accordance with the South Korea-Australia Free Trade Agreement. The Canada–South Korea Free Trade Agreement was concluded in 2014. China–South Korea Free Trade Agreement went official on November 10, 2014. Also home to Lotte World, the largest indoor amusement park in the world, the rise of South Korea's large export-oriented music industry has generated large amounts of tourism, guided by the Ministry of Culture, Sports and Tourism.

Following the Korean War, South Korea remained one of the poorest countries in the world for over a decade. In 1960 its gross domestic product per capita was $79, lower than that of some sub-Saharan countries. The growth of the industrial sector was the principal stimulus to economic development. In 1986, manufacturing industries accounted for approximately 30 percent of the gross domestic product (GDP) and 25 percent of the work force. Benefiting from strong domestic encouragement and foreign aid, Seoul's industrialists introduced modern technologies into outmoded or newly built facilities at a rapid pace, increased the production of commodities—especially those for sale in foreign markets—and plowed the proceeds back into further industrial expansion. As a result, industry altered the country's landscape, drawing millions of laborers to urban manufacturing centers.

A downturn in the South Korean economy in 1989 spurred by a sharp decrease in exports and foreign orders caused deep concern in the industrial sector. Ministry of Trade and Industry analysts stated that poor export performance resulted from structural problems embedded in the nation's economy, including an overly strong won, increased wages and high labor costs, frequent strikes, and high interest rates. The result was an increase in inventories and severe cutbacks in production at a number of electronics, automobile, and textile manufacturers, as well as at the smaller firms that supplied the parts. Factory automation systems were introduced to reduce dependence on labor, to boost productivity with a much smaller work force, and to improve competitiveness. It was estimated that over two-thirds of South Korea's manufacturers spent over half of the funds available for facility investments on automation.

South Korea's real gross domestic product expanded by an average of more than 8 percent per year, from US$2.7 billion in 1962 to US$230 billion in 1989, breaking the trillion dollar mark in 2006. Nominal GDP per capita grew from $103.88 in 1962 to $5,438.24 in 1989, reaching the $20,000 milestone in 2006. The manufacturing sector grew from 14.3 percent of the GNP in 1962 to 30.3 percent in 1987. Commodity trade volume rose from US$480 million in 1962 to a projected US$127.9 billion in 1990. The ratio of domestic savings to GNP grew from 3.3 percent in 1962 to 35.8 percent in 1989. In 1965 South Korea's rate of growth first exceeded North Korea's rate of growth in most industrial areas, though South Korea's per capita GNP was still lower.

The most significant factor in rapid industrialization was the adoption of an outward-looking strategy in the early 1960s. This strategy was particularly well-suited to that time because of South Korea's poor natural resource endowment, low savings rate, and tiny domestic market. The strategy promoted economic growth through labor-intensive manufactured exports, in which South Korea could develop a competitive advantage. Government initiatives played an important role in this process. Through the model of export-led industrialization, the South Korean government incentivized corporations to develop new technology and upgrade productive efficiency in order to compete in the highly-competitive, global market. By adhering to state regulations and demands, firms were awarded subsidization and investment support to rapidly develop their export markets in the fast-paced, evolving international arena. In addition, the inflow of foreign capital was greatly encouraged to supplement the shortage of domestic savings. These efforts enabled South Korea to achieve rapid growth in exports and subsequent increases in income.

By emphasizing the industrial sector, Seoul's export-oriented development strategy left the rural sector relatively underdeveloped. Except for mining, most industries were located in the urban areas of the northwest and southeast. Heavy industries generally were located in the south of the country. Factories in Seoul contributed over 25 percent of all manufacturing value-added in 1978; taken together with factories in surrounding Gyeonggi Province, factories in the Seoul area produced 46 percent of all manufacturing that year. Factories in Seoul and Gyeonggi Province employed 48 percent of the nation's 2.1 million factory workers. Increasing income disparity between the industrial and agricultural sectors became a serious problem by the 1970s and remained a problem, despite government efforts to raise farm income and improve rural living standards.

In the early 1980s, in order to control inflation, a conservative monetary policy and tight fiscal measures were adopted. Growth of the money supply was reduced from the 30 percent level of the 1970s to 15 percent. Seoul even froze its budget for a short while. Government intervention in the economy was greatly reduced and policies on imports and foreign investment were liberalized to promote competition. To reduce the imbalance between rural and urban sectors, Seoul expanded investments in public projects, such as roads and communications facilities, while further promoting farm mechanization.

The measures implemented early in the decade, coupled with significant improvements in the world economy, helped the South Korean economy regain its lost momentum in the late 1980s. South Korea achieved an average of 9.2 percent real growth between 1982 and 1987 and 12.5 percent between 1986 and 1988. The double-digit inflation of the 1970s was brought under control. Wholesale price inflation averaged 2.1 percent per year from 1980 through 1988; consumer prices increased by an average of 4.7 percent annually. Seoul achieved its first significant surplus in its balance of payments in 1986 and recorded a US$7.7 billion and a US$11.4 billion surplus in 1987 and 1988 respectively. This development permitted South Korea to begin reducing its level of foreign debt. The trade surplus for 1989, however, was only US$4.6 billion, and a small negative balance was projected for 1990.

For the first half of the 1990s, the South Korean economy continued a stable and strong growth in both private consumption and GDP. Things changed quickly in 1997 with the Asian Financial crisis. After several other Asian currencies were attacked by speculators, the Korean won started to heavily depreciate in October 1997. The problem was exacerbated by the problem of non-performing loans at many of Korea's merchant banks. By December 1997, the IMF had approved a USD $21 billion loan, that would be part of a USD $58.4 billion bailout plan. By January 1998, the government had shut down a third of Korea's merchant banks. Throughout 1998, Korea's economy would continue to shrink quarterly at an average rate of -6.65%. Korean chaebol Daewoo became a casualty of the crisis as it was dismantled by the government in 1999 due to debt problems. American company General Motors managed to purchase the motors division. Indian conglomerate Tata Group, purchased the trucks and heavy vehicles division of Daewoo.

Actions by the South Korean government and debt swaps by international lenders contained the country's financial problems. Much of South Korea's recovery from the Asian Financial Crisis can be attributed to labor adjustments (i.e. a dynamic and productive labor market with flexible wage rates) and alternative funding sources. By the first quarter of 1999, GDP growth had risen to 5.4%, and strong growth thereafter combined with deflationary pressure on the currency led to a yearly growth of 10.5%. In December 1999, president Kim Dae-jung declared the currency crisis over.

Korea's economy moved away from the centrally planned, government-directed investment model toward a more market-oriented one. These economic reforms, pushed by President Kim Dae-jung, helped Korea maintain one of Asia's few expanding economies, with growth rates of 10.8% in 1999 and 9.2% in 2000. Growth fell back to 3.3% in 2001 because of the slowing global economy, falling exports, and the perception that much-needed corporate and financial reforms have stalled.

After the bounce back from the crisis of the late nineties, the economy continued strong growth in 2000 with a GDP growth of 9.08%. However, the South Korean economy was affected by the September 11 Attacks. The slowing global economy, falling exports, and the perception that corporate and financial reforms had stalled caused growth to fall back to 3.8% in 2001 Thanks to industrialization GDP per hour worked (labor output) more than tripled from US$2.80 in 1963 to US$10.00 in 1989. More recently the economy stabilized and maintain a growth rate between 4-5% from 2003 onwards.

Led by industry and construction, growth in 2002 was 5.8%, despite anemic global growth. The restructuring of Korean conglomerates ("chaebols"), bank privatization, and the creation of a more liberalized economy—with a mechanism for bankrupt firms to exit the market—remain Korea's most important unfinished reform tasks. Growth slowed again in 2003, but production expanded 5% in 2006, due to popular demand for key export products such as HDTVs and mobile phones.

Like most industrialized economies, Korea suffered significant setbacks during the late-2000s recession that began in 2007. Growth fell by 3.4% in the fourth quarter of 2008 from the previous quarter, the first negative quarterly growth in 10 years, with year on year quarterly growth continuing to be negative into 2009. Most sectors of the economy reported declines, with manufacturing dropping 25.6% as of January 2009, and consumer goods sales dropping 3.1%. Exports in autos and semiconductors, two critical pillars of the economy, shrank 55.9% and 46.9% respectively, while exports overall fell by a record 33.8% in January, and 18.3% in February 2009 year on year. As in the 1997 crisis, Korea's currency also experienced massive fluctuations, declining by 34% against the dollar. Annual growth in the economy slowed to 2.3% in 2008, and was expected to drop to as low as -4.5% by Goldman Sachs, but South Korea was able to limit the downturn to a near standstill at 0.2% in 2009.

Despite the global financial crisis, the South Korean economy, helped by timely stimulus measures and strong domestic consumption of products that compensated for a drop in exports, was able to avoid a recession unlike most industrialized economies, posting positive economic growth for two consecutive years of the crisis. In 2010, South Korea made a strong economic rebound with a growth rate of 6.1%, signaling a return of the economy to pre-crisis levels. South Korea's export has recorded $424 billion in the first eleven months of the year 2010, already higher than its export in the whole year of 2008. The South Korean economy of the 21st century, as a Next Eleven economy, is expected to grow from 3.9% to 4.2% annually between 2011 and 2030, similar to growth rates of developing countries such as Brazil or Russia.

The South Korean government signed the Korea-Australia Free Trade Agreement (KAFTA) on December 5, 2013, with the Australian government seeking to benefit its numerous industries—including automotive, services, and resources and energy—and position itself alongside competitors, such as the US and ASEAN. South Korea is Australia’s third largest export market and fourth largest trading partner with a 2012 trade value of A$32 billion. The agreement contains an Investor State Dispute Settlement (ISDS) clause that permits legal action from South Korean corporations against the Australian government if their trade rights are infringed upon.

The government cut the work week from six days to five in phases, from 2004 to 2011, depending on the size of the firm. The number of public holidays was expanded to 16 by 2013.

In 1990, South Korean manufacturers planned a significant shift in future production plans toward high-technology industries. In June 1989, panels of government officials, scholars, and business leaders held planning sessions on the production of such goods as new materials, mechatronics—including industrial robotics—bioengineering, microelectronics, fine chemistry, and aerospace. This shift in emphasis, however, did not mean an immediate decline in heavy industries such as automobile and ship production, which had dominated the economy in the 1980s.

South Korea relies largely upon exports to fuel the growth of its economy, with finished products such as electronics, textiles, ships, automobiles, and steel being some of its most important exports. Although the import market has liberalized in recent years, the agricultural market has remained largely protectionist due to serious disparities in the price of domestic agricultural products such as rice with the international market. As of 2005, the price of rice in South Korea is about four times that of the average price of rice on the international market, and it was generally feared that opening the agricultural market would have disastrous effects upon the South Korean agricultural sector. In late 2004, however, an agreement was reached with the WTO in which South Korean rice imports will gradually increase from 4% to 8% of consumption by 2014. In addition, up to 30% of imported rice will be made available directly to consumers by 2010, where previously imported rice was only used for processed foods. Following 2014, the South Korean rice market will be fully opened.

Additionally, South Korea today is known as a Launchpad of a mature mobile market, where developers can reap benefits of a market where very few technology constraints exist. There is a growing trend of inventions of new types of media or apps, utilizing the 4G and 5G internet infrastructure in South Korea. South Korea has today the infrastructures to meet a density of population and culture that has the capability to create strong local particularity.

During the 1970s and 1980s, South Korea became a leading producer of ships, including oil supertankers, and oil-drilling platforms. The country's major shipbuilder was Hyundai, which built a 1-million-ton capacity drydock at Ulsan in the mid-1970s. Daewoo joined the shipbuilding industry in 1980 and finished a 1.2-million-ton facility at Okpo on Geoje Island, south of Busan, in mid-1981. The industry declined in the mid-1980s because of the oil glut and because of a worldwide recession. There was a sharp decrease in new orders in the late 1980s; new orders for 1988 totaled 3 million gross tons valued at US$1.9 billion, decreases from the previous year of 17.8 percent and 4.4 percent, respectively. These declines were caused by labor unrest, Seoul's unwillingness to provide financial assistance, and Tokyo's new low-interest export financing in support of Japanese shipbuilders. However, the South Korean shipping industry was expected to expand in the early 1990s because older ships in world fleets needed replacing. South Korea eventually became the world's dominant shipbuilder with a 50.6% share of the global shipbuilding market as of 2008. Notable Korean shipbuilders are Hyundai Heavy Industries, Samsung Heavy Industries, Daewoo Shipbuilding & Marine Engineering, and the now bankrupt STX Offshore & Shipbuilding.

The automobile industry was one of South Korea's major growth and export industries in the 1980s. By the late 1980s, the capacity of the South Korean motor industry had increased more than fivefold since 1984; it exceeded 1 million units in 1988. Total investment in car and car-component manufacturing was over US$3 billion in 1989. Total production (including buses and trucks) for 1988 totaled 1.1 million units, a 10.6 percent increase over 1987, and grew to an estimated 1.3 million vehicles (predominantly passenger cars) in 1989. Almost 263,000 passenger cars were produced in 1985—a figure that grew to approximately 846,000 units in 1989. In 1988 automobile exports totaled 576,134 units, of which 480,119 units (83.3 percent) were sent to the United States. Throughout most of the late 1980s, much of the growth of South Korea's automobile industry was the result of a surge in exports; 1989 exports, however, declined 28.5 percent from 1988. This decline reflected sluggish car sales to the United States, especially at the less expensive end of the market, and labor strife at home. South Korea today has developed into one of the world's largest automobile producers. The Hyundai Kia Automotive Group is South Korea's largest automaker in terms of revenue, production units and worldwide presence.

Most of the mineral deposits in the Korean Peninsula are located in North Korea, with the South only possessing an abundance of tungsten and graphite. Coal, iron ore, and molybdenum are found in South Korea, but not in large quantities and mining operations are on a small scale. Much of South Korea's minerals and ore are imported from other countries. Most South Korean coal is low-grade anthracite that is only used for heating homes and boilers.

Construction has been an important South Korean export industry since the early 1960s and remains a critical source of foreign currency and "invisible" export earnings. By 1981 overseas construction projects, most of them in the Middle East, accounted for 60 percent of the work undertaken by South Korean construction companies. Contracts that year were valued at US$13.7 billion. In 1988, however, overseas construction contracts totaled only US$2.6 billion (orders from the Middle East were US$1.2 billion), a 1 percent increase over the previous year, while new orders for domestic construction projects totaled US$13.8 billion, an 8.8 percent increase over 1987. South Korean construction companies therefore concentrated on the rapidly growing domestic market in the late 1980s. By 1989 there were signs of a revival of the overseas construction market: the Dong Ah Construction Company signed a US$5.3 billion contract with Libya to build the second phase (and other subsequent phases) of Libya's Great Man-Made River Project, with a projected cost of US$27 billion when all 5 phases were completed. South Korean construction companies signed over US$7 billion of overseas contracts in 1989. Korea's largest construction companies include Samsung C&T Corporation, which built some of the highest building's and most noteworthy skyscrapers such as three consecutively world's tallest buildings: Petronas Towers, Taipei 101, and Burj Khalifa.

During the 1960s, South Korea was largely dependent on the United States to supply its armed forces, but after the elaboration of President Richard M. Nixon's policy of Vietnamization in the early 1970s, South Korea began to manufacture many of its own weapons.

Since the 1980s, South Korea, now in possession of more modern military technology than in previous generations, has actively begun shifting its defense industry's areas of interest more from its previously homeland defense-oriented militarization efforts, to the promotion of military equipment and technology as mainstream products of exportation to boost its international trade. Some of its key military export projects include the T-155 Firtina self-propelled artillery for Turkey; the K11 air-burst rifle for United Arab Emirates; the Bangabandhu class guided-missile frigate for Bangladesh; fleet tankers such as Sirius class for the navies of Australia, New Zealand, and Venezuela; Makassar class amphibious assault ships for Indonesia; and the KT-1 trainer aircraft for Turkey, Indonesia and Peru.

South Korea has also outsourced its defense industry to produce various core components of other countries' advanced military hardware. Those hardware include modern aircraft such as F-15K fighters and AH-64 attack helicopters which will be used by Singapore, whose airframes will be built by Korea Aerospace Industries in a joint-production deal with Boeing. In other major outsourcing and joint-production deals, South Korea has jointly produced the S-300 air defense system of Russia via Samsung Group, and will facilitate the sales of Mistral class amphibious assault ships to Russia that will be produced by STX Corporation. South Korea's defense exports were $1.03 billion in 2008 and $1.17 billion in 2009.

In 2012, 11.1 million foreign tourists visited South Korea, making it the 20th most visited country in the world, up from 8.5 million in 2010. Recently, the number of tourists, especially from China, Taiwan, Hong Kong, and Southeast Asia, has grown dramatically due to the increased popularity of the Korean Wave ("Hallyu").

Seoul is the principal tourist destination for visitors; popular tourist destinations outside of Seoul include Seorak-san national park, the historic city of Gyeongju and semi-tropical Jeju Island.
In 2014 South Korea hosted the League of Legends season 4 championship.

Since 1991 there has been a steady upwards trend in South Korean M&A until 2018 with only a short break around 2004. Since 1991 around 18,300 deals in, into or out of South Korea have been announced, which sum up to a total value of over 941. bil. USD. The year 2016 has been the year with the largest deal value (1,818 in bil. USD) and the most number of deals (82,3).

Target industries are distributed very evenly with no industry taking a larger share than 10%. The top three target industries are Electronics (9.7%), Semiconductors (9.1%) and Metals and Mining (7.7%). However, over 51% of the acquiring companies originate from the financial and brokerage sector.

Here is a list of the top 10 deals:




</doc>
<doc id="27025" url="https://en.wikipedia.org/wiki?curid=27025" title="Telecommunications in South Korea">
Telecommunications in South Korea

Communications services improved dramatically in the 1980s with the assistance of foreign partners and as a result of the development of the electronics industry. The number of telephones in use in 1987 reached 9.2 million, a considerable increase from 1980, when there were 2.8 million subscribers (which, in turn, was four times the number of subscribers in 1972).

Radio, and in more recent years television, reached virtually every resident. By 1945 there were about 60,000 radio sets in the country. By 1987 there were approximately 42 million radio receivers in use, and more than 100 radio stations were broadcasting. Transistor radios and television sets have made their way to the most remote rural areas. Television sets, now mass-produced in South Korea, became far less expensive; most city people and a significant number of rural families owned or had access to a television. Ownership of television sets grew from 25,000 sets when broadcasting was initiated in 1961 to an estimated 8.6 million sets in 1987, and more than 250 television stations were broadcasting.


There are three mobile phone service providers: SK Telecom, KT and LG Uplus.



South Korea has six national terrestrial television networks from four broadcaster; KBS 1TV, KBS 2TV, MBC TV, SBS TV, EBS 1TV, and EBS 2TV. All terrestrial channels are digital (ATSC) since January 2013. 

From November 2011, four generalist channel are available on cable television; JTBC, Channel A, TV Chosun, and Maeil Broadcasting Network.

(Total population: 50 million (July 2012 est.)


Today, South Korea has the highest number of broadband users. The rapid growth of the Korean broadband market was the result of a combination of government pushes and market factors. The government was active in promoting privatization and deregulation in general, and the information technology (IT) sector was no exception.

The government implemented structural reforms in July 1990. Since the mid-1990s, the Ministry of Information and Communications (MIC) has pursued a policy of high-speed telecommunication infrastructure as a foundation to build a “knowledge-based society.” In the telecommunications sector, competition was allowed on an incremental basis and, in the market for value added services, full competition was allowed. In March 1995, Korea Information Infrastructure (KII) was established. KII’s goal was to advance the nation’s IT infrastructure. In August 1995, the Framework Act on Information Promotion was enacted.

The country then experienced economic crisis in 1997 with the rest of the region. During the economic reforms being implemented after the financial crisis, the information technology (IT) sector was one of several that was targeted and considered to be an important factor in the recovery of the nation’s economy. In 1999, the government implemented the program known as Cyber Korea 21, which was intended to accelerate IT development.

In 1999, the government provided US$77 million in loans with preferential rates to facilities service providers (FSP). In 2000, another US$77 million was provided in loans for suburban areas, small cities and towns, and regional industrial areas. Another US$926 million was provided until 2005 in order to supply the rural areas with broadband.

Commensurate with its investment funding, the government implemented various policies designed to increase internet use among the general population. The government provided “internet literacy” lessons to homemakers, the elderly, military personnel, and farmers. In June 2000, the government implemented what was known as the “Ten Million People Internet Education” project, the purpose of which was to provide internet education to ten million people.

The number of broadband subscribers in Korea reached 10 million in October 2002, with about 70% out of 14.3 million homes connected at the speed of over 2 Mbit/s.

In 2002, there were six operators providing broadband services in Korea. The market share leader was Korea Telecom (KT), with approximately 45.8% market share (4.5 million subscribers), followed by Hanaro Telecom with approximately 28.6% of the market and Thrunet with approximately 13.1%. of the market. In terms of technology, KT primarily uses Digital Subscriber Line (DSL). Hanaro uses a mix of cable and DSL. Thrunet service is mainly provided through cable modem.

At end of June 2011, subscribers of Voice over Internet Protocol (VoIP) service achieve 10.1 million or around 20 percent of South Korea's population.

This article relied on information from:

Yun, Kyounglim, Heejin Lee and So-Hye Lim, The Growth of Broadband Internet Connections in South Korea: Contributing Factors, Asia/Pacific Research Center, Stanford University (September 2002).

Choudrie, Jyoti and Heejin Lee, Broadband Development in South Korea: Institutional and Cultural Factors, European Journal of Information Systems v. 13, pp. 103–14 (2004).




</doc>
<doc id="27026" url="https://en.wikipedia.org/wiki?curid=27026" title="Transport in South Korea">
Transport in South Korea

Transportation in South Korea is provided by extensive networks of railways, highways, bus routes, ferry services and air routes that criss-cross the country. South Korea is the third country in the world to operate a commercial maglev train.

Development of modern infrastructure began with the first Five-Year Development Plan (1962–66), which included the construction of 275 kilometers of railways and several small highway projects. Construction of the Gyeongbu Expressway, which connects the two major cities of Seoul and Busan, was completed on 7 July 1970.

The 1970s saw increased commitment to infrastructure investments. The third Five-Year Development Plan (1972–76) added the development of airports, seaports. The Subway system was built in Seoul, the highway network was expanded by 487 km and major port projects were started in Pohang, Ulsan, Masan, Incheon and Busan.

The railroad network experienced improvements in the 1980s with electrification and additional track projects. Operation speed was also increased on the main lines. Though the railroad was still more useful for transportation of freight, passenger traffic was also growing. There was 51,000 kilometers of roadways by 1988. Expressway network was expanded to connect more major cities and reached a combined length of 1,539 kilometers before the end of the decade.

The largest railway operator is Korail. Railway network is managed by Korea Rail Network Authority.

Korea Train Express began service in April 2004, as Korea's first high-speed service. Intercity services are provided by ITX-Saemaeul and Mugunghwa-ho. ITX-Saemaeul generally stops less than Mugunghwa-ho. They stop in all stations and seat reservation is not available. On routes where KTX operates, air travel significantly declined with less passengers choosing to fly and airlines offering fewer flights.

Nuriro Train service runs between Seoul-Sinchang route and other lines. Nuriro Train serves commuters around Seoul Metropolitan Area, providing shorter travel time than Seoul Subway. The rapid trains have same cost and seat reservation as Mugunghwa-ho. Korail plans to expand the service area. 

South Korea's six largest cities — Seoul, Busan, Daegu, Gwangju, Daejeon and Incheon — all have subway systems.

Seoul's subway system is the oldest system in the country, with the Seoul Station – Cheongnyangni section of Line 1 opening in 1974.

The first tram line in Seoul started operation between Seodaemun and Cheongnyangni in December 1898. The network was expanded to cover the whole downtown area (Jung-gu and Jongno-gu districts) as well as surrounding neighbourhoods, including Cheongnyangni in the east, Mapo-gu in the west, and Noryangjin across the Han River to the south.

The networks reached its peak in 1941, but was abandoned in favor of cars and the development of a subway system in 1968. Seoul Subway Line 1 and Line 2 follow the old streetcar routes along Jongno and Euljiro, respectively.

Virtually all towns in South Korea of all sizes are served by regional bus service. Regional routes are classified as "gosok bus" (고속버스, "high speed" express bus) or "sioe bus" (시외버스, "suburban" intercity bus) with gosok buses operating over the longer distances and making the fewest (if any) stops en route. Shioe buses typically operate over shorter distances, are somewhat slower, and make more stops.

Within cities and towns, two types of city bus operate in general: "jwaseok" (좌석, "coach") and "dosihyeong" (도시형, "city type") or "ipseok" (입석, "standing"). Both types of bus often serve the same routes, make the same (or fewer) stops and operate on similar frequencies, but jwaseok buses are more expensive and offer comfortable seating, while doshihyeong buses are cheaper and have fewer and less comfortable seats. Many small cities and towns do not have jwaseok buses and their buses are officially called "nongeochon" (농어촌, "rural area" bus).

Some cities have their own bus classifying systems.

Incheon International Airport is served by an extensive network of high-speed buses from all parts of the country.

Beginning in the late 1990s, many department stores operated their own small networks of free buses for shoppers, but government regulation, confirmed by a court decision on June 28, 2001, have banned department stores from operating buses. However, most churches, daycare centres and private schools send buses around to pick up their congregants, patients or pupils.

Highways in South Korea are classified as freeways (expressways/motorways), national roads and various classifications below the national level. Almost all freeways are toll highways and most of the expressways are built, maintained and operated by Korea Expressway Corporation (KEC).

The freeway network serves most parts of South Korea. Tolls are collected using an electronic toll collection system. KEC also operates service amenities (dining and service facilities) en route.

There are also several privately financed toll roads. Nonsan-Cheonan Expressway, Daegu-Busan Expressway, Incheon International Airport Expressway, Seoul-Chuncheon Expressway and parts of the Seoul Ring Expressway are wholly privately funded and operated BOT concessions. Donghae Expressway was built in cooperation between KEC and the National Pension Service.

Total length of the South Korean road network was 86,989 km in 1998. Of this, 1,996 km was expressways and 12,447 km national roads. By 2009, combined length of the expressways had reached approximately 3,000 km, it mostly equal to the whole area of South Korea

Virtually cut off from the Asian mainland, South Korea is a seafaring nation, with one of the world's largest shipbuilding industries and an extensive system of ferry services. South Korea operates one of the largest merchant fleets serving China, Japan and the Middle East. Most fleet operators are large conglomerates, while most ferry operators are small, private operators.

There are 1,609 km of navigable waterways in South Korea, though use is restricted to small craft.

The southern and westerns coasts of the country are dotted with small islands which are served by ferries. In addition, the larger offshore Jeju and Ulleung Islands are also served by ferry. Major centres for ferry service include Incheon, Mokpo, Pohang and Busan, as well as China and Japan.

The cities have major ports Jinhae, Incheon, Gunsan, Masan, Mokpo, Pohang, Busan, Donghae, Ulsan, Yeosu, Jeju.

In 1999, there was a total of 461 merchant ships (1,000 GRT or over) totalling 5,093,620 GRT/. These are divisible by type as follows:

Korean Air was founded by the government in 1962 to replace Korean National Airlines and has been privately owned since 1969. It was South Korea's sole airline until 1988. In 2008, Korean Air served 2,164 million passengers, including 1,249 million international passengers.

A second carrier, Asiana Airlines, was established in 1988 and originally served Seoul, Jeju and Busan domestically and Bangkok, Singapore, Japan and Los Angeles internationally. By 2006, Asiana served 12 domestic cities, 66 cities in 20 foreign countries for commercial traffic and 24 cities in 17 countries for cargo traffic.

Combined, South Korean airlines currently serve 297 international routes. Smaller airliners, such as Air Busan, Jin Air, Eastar Jet and Jeju Air, provide domestic service and Japan/Southeast Asian route with lower fares.

South Korea contains the busiest passenger air corridor as measured by passengers per year. Over ten million people traveled between Seoul Gimpo Airport and Jeju in 2015 alone. As competition is fierce and prices affordable, the trend has been increasingly towards more air travel on this route. Similarly, air travel is also growing between Jeju and other mainland airports.

Along other routes, air travel competes with the KTX high speed rail service and has declined in the 2000s and 2010s

Construction of South Korea's largest airport, Incheon International Airport, was completed in 2001, in time for the 2002 FIFA World Cup. By 2007, the airport was serving 30 million passengers a year. The airport has been selected as the "Best Airport Worldwide" for four consecutive years since 2005 by Airports Council International.

Seoul is also served by Gimpo International Airport (formerly Kimpo International Airport). International routes mainly serve Incheon, while domestic services mainly use Gimpo. Other major airports are in Busan and Jeju.

There are 103 airports in South Korea (1999 est.) and these may be classified as follows.

Airports with paved runways:
"total:"
67
"over 3,047 m:"
1
"2,438 to 3,047 m:"
18
"1,524 to 2,437 m:"
15
"914 to 1,523 m:"
13
"under 914 m:"
20 (1999 est.)

Airports with unpaved runways:
"total:"
36
"over 3,047 m:"
1
"914 to 1,523 m:"
3
"under 914 m:"
32 (1999 est.)

Heliports:
203 (1999 est.)

These pipelines are for petroleum products.
Additionally, there is a parallel petroleum, oils and lubricants (POL) pipeline being completed




</doc>
<doc id="27027" url="https://en.wikipedia.org/wiki?curid=27027" title="Republic of Korea Armed Forces">
Republic of Korea Armed Forces

The Republic of Korea Armed Forces (), also known as the ROK Armed Forces, are the armed forces of South Korea. Created in 1948, following the division of Korea, the Republic of Korea Armed Forces is one of the largest standing armed forces in the world with a reported personnel strength of 3,725,000 in 2016 (625,000 active and 3,100,000 reserve). South Korea's military forces are responsible for maintaining the sovereignty and territorial integrity of the state, but often engage in humanitarian and disaster-relief efforts nationwide. More recently the South Korean military began increasing its participation in international affairs, acknowledging its role and responsibility as the eleventh economic power in the world in terms of GDP. The ROK military has participated in various peacekeeping operations, and counter-terrorism operations. It is recognized as one of the world's most professional militaries.

The South Korean armed forces were largely constabulary forces until the outbreak of the Korean War. It was heavily damaged by North Korean and Chinese attacks and in the beginning relied almost entirely on U.S. support for weapons, ammunition and technology. After the Korean War South Korea maintained a large military ground force, which in 1967 had about 585,000 personnel, much larger than the northern forces of about 345,000. During South Korea's period of rapid growth in the 1980s, the military modernised, benefiting from several government-sponsored technology transfer projects and indigenous defense capability initiatives. The GlobalSecurity.org website states that "in 1990 South Korean industries provided about 70 percent of the weapons, ammunition, communications and other types of equipment, vehicles, clothing, and other supplies needed by the military."

Today, the South Korean armed forces enjoy a good mix of avant-garde as well as older conventional weapons. South Korea has one of the highest defense budgets in the world, ranking 10th globally in 2016, with a budget of more than $36 billion U.S. dollars. Its capabilities include many sophisticated American and European weapon systems, complemented by a growing and increasingly more advanced indigenous defense manufacturing sector. For example, by taking advantage of the strong local shipbuilding industry, the ROK Navy has embarked on a rigorous modernization plan with ambitions to become a blue-water navy by 2020. South Korea has a joint military partnership with the United States, termed the ROK-U.S. Alliance, as outlined by the Mutual Defense Treaty signed after the Korean War. During the outbreak of the Vietnam War, ROK Army and the ROK Marines were among those fighting alongside South Vietnam and the United States. More recently, South Korea also takes part in regional as well as pan-Pacific national military wargames and exercises such as RIMPAC and RSOI. Among other components of the armed forces is the Defence Security Command, originally the Army Counter-Intelligence Corps, which had a major role in monitoring the military's loyalty during the period of military rule in South Korea.

Until January 2011, "mixed-race" men were prohibited from being conscripted into the South Korean military.

Homosexual relations constitute a criminal offense in the military code, giving rise to a maximum sentence of two years' imprisonment, take place or not in the units of the army.

The ROK Armed Forces consists of the:

In addition, reserve elements consist of the:

The President is the Commander-in-Chief Forces ex officio. The military authority runs from the President to the Minister of National Defense, who is commonly (but not legally bound to be) a retired 4-star General (equivalent to a British Army/Commonwealth full General or a Royal Navy/Commonwealth Admiral).

The Chairman of the Joint Chiefs of Staff, a 4-star General or Admiral, is the Senior Officer of the Armed Forces and has the Operational Authority over the Armed Forces, with directions from the President through the Minister of Defense. Traditionally (with one exception), the position is filled by an officer of the Army. The chain of Operational Authority runs straight from the Chairman of the Joint Chiefs of Staff to the Commandants of the several Operational Commands. Currently there are five Operational Commands in the Army, two in the Navy (including the Marine Corps) and one in the Air Force.

The respective Chiefs of Staff of each Service Branch (Army, Navy, Air Force) has administrative control over his or her own service. Each Chief of Staff is also a standing member of the Joint Chiefs of Staff.

The Republic of Korea Joint Chiefs of Staff headquarters (Hangul: 대한민국 합동참모본부, Hanja: 大韓民國 合同參謀本部; Daehanminguk Hapttongchammobonbu) is a group of Chiefs from each major branch of the armed services in the Republic of Korea Armed Forces. Unlike the United States, the Chairman of the Joint Chiefs of Staff has operational control over all military personnel of the armed forces.

All regular members are 4-star generals or admirals, although the deputy chairman sometimes has only 3 stars.

The ROK Army (ROKA) is by far the largest of the military branches, with 495,000 personnel as of 2014. This comes as a response to both the mountainous terrain native to the Korean Peninsula (70% mountainous) as well as the heavy North Korean presence, with its 1-million-strong army, two-thirds of which is permanently garrisoned in the frontline near the DMZ. The current administration has initiated a program of self-defense, whereby South Korea would be able to fully counter the North Korean threat with purely domestic means within the next two decades.

The ROK Army was formerly organized into three armies: the First Army (FROKA), Third Army (TROKA) and Second Operational Command each with its own headquarters, corps (not Second Operational Command), and divisions. The Third Army was responsible for the defense of the capital as well as the western section of the DMZ. The First Army was responsible for the defense of the eastern section of the DMZ whereas the Second Operational Command formed the rearguard.

Under a restructuring plan aimed at reducing redundancy, the First and Third Armies will be incorporated into the newly formed First Operations Command, whereas the Second ROK Army has been converted into the Second Operational Command. The army consists of the Army Headquarters, the Aviation Command, and the Special Warfare Command, with 7 corps, 39 divisions, some 520,000 troops and estimated as many as 5,850 tanks and armored vehicles, 11,337 artillery systems, 7,032 missile defense systems and 13,000 infantry support systems.

The army will take the brunt of the personnel reduction part of the Defense Reform 307. Associated with this personnel reduction would be a significant reduction in the ROK Army force structure, in particular decreasing the current force of 47 divisions (active duty and reserve) down to a force of about 28 divisions.

The ROK Navy (ROKN) is the armed forces branch responsible for conducting naval operations and amphibious landing operations. As a part of its mission, the ROK Navy has engaged in several peacekeeping operations since the turn of the century. The ROK Navy includes the Republic of Korea Navy Headquarters, Republic of Korea Fleet, Naval Logistics Command, Naval Education and Training Command, Naval Academy, and Republic of Korea Marine Corps, which is a quasi-autonomous organization. The Chief of Naval Operations (CNO) is the highest-ranking officer (four-star admiral) of the ROK Navy.

In 1995, Admiral An Pyongtae, the 20th Chief of Naval Operations, presented the vision of building a "blue ocean navy" for the future of the ROK Navy in his inaugural address. In 2001, then President Kim Dae-jung announced a plan for building up a Strategic Mobile Fleet. As a part of "Defense Reform 2020," which was proposed by the Roh Moo-hyun Administration, the ROK Navy is required to reform the organizations under Commander-in-Chief Republic of Korea Fleet (CINCROKFLT) by upgrading a submarine operations command (to fleet submarine force), a naval aviation operations command (to fleet air arm), and by establishing some Mobile Flotillas. The ROK Navy aims to become a blue-water navy by 2020.

In the first decade of the 21st century, the ROK Navy launched the lead ships of newly developed types: in 2002, ROKS "Chungmugong Yi Sunshin" (DDH 975), a 4,500-ton destroyer, was launched; in 2005, the 14,000-ton amphibious landing ship, ROKS "Dokdo" (LPH 6111) was launched; in 2006, the ROK Navy launched the "Sohn Won-yil" (SS 072), an 1,800-ton Type 214 submarine with Air-Independent propulsion (AIP) system. In 2007, the ROK Navy launched the lead ship (DDG 991) of the "King Sejong the Great" class destroyer, built around the Aegis combat system and the SPY-1D multi-function phased array radar. The ROK Navy is undertaking several shipbuilding projects: Korean Destroyer Experimental (KDX) program, Frigate Experimental (FFX), Landing Platform Experimental (LPX), Patrol Killer Experimental (PKX), and Korean Submarine (KSS) program.

The ROK Navy hosted its second international fleet review off the coast of Busan in October 2008.

Although the National Armed Forces Organisation Act stipulates that the ROK Navy includes the Republic of Korea Marine Corps, the ROKMC is a semi-autonomous organization that carries out much of its functions independently. During the Korean War, the ROKMC earned their nickname as "귀신잡는 해병대" ().

The motto of the ROK Marine Corps is "한번 해병은 영원한 해병" ().

The ROK Air Force (ROKAF) maintains a modern air force in order to defend itself from various modes of threats, including the North Korean Army. The ROK Air Force fields some 450 combat aircraft of American design. In contrast, the North Korean Army has roughly 650 combat aircraft, but mostly obsolete types of Soviet and Chinese origin.

Korea began a program for the development of indigenous jet trainers beginning in 1997. This project eventually culminated in the KAI T-50, dubbed the "Golden Eagle" which is used as a trainer for jet pilots, now being exported to Indonesia. A multirole all-weather version of the T-50 is the modified FA-50, which can be externally fitted with Rafael's Sky Shield or LIG Nex1's ALQ-200K ECM pods, Sniper or LITENING targeting pods, and Condor 2 reconnaissance pods to further improve the fighter's electronic warfare, reconnaissance, and targeting capabilities. Other improved weapon systems over FA-50 include SPICE multifunctional guidance kits, Textron CBU-97/105 Sensor Fuzed Weapon with WCMD tail kits, JDAM, and JDAM-ER for more comprehensive air-to-ground operations, and AIM-120 missiles for BVR air-to-air operations. FA-50 has provisions for, but does not yet integrate, Python and Derby missiles, also produced by Rafael, and other anti-ship missiles, stand-off weapons, and sensors to be domestically developed by Korea.

The Republic of Korea Air Force also expressed interests in acquiring the RQ-4 Global Hawk and Joint Direct Attack Munition kits to further improve their intelligence and offensive capabilities.

The replacement programs for the F-4D/E and F-5A/B/E/F are the KTX-2 and F-X, respectively. The latter has been fulfilled by the Boeing F-15K.

The South Korean government also announced its plan to develop indigenous helicopter manufacturing capacities to replace the aging UH-1 helicopters, many of which had seen service during the Vietnam War. The program originally included plans for the development of both a civilian and a military helicopter. This was later revised and gave priority to the utility helicopter program. Based on the success and experience of the civilian KMH (Korean Multi-purpose Helicopter) the attack helicopter, which would share a common configuration, will be developed.

Military service is mentioned as one of the Four Constitutional Duties (along with taxes, education, and labor) for all citizens. The current effective Conscription Law, however, applies only to males although women can volunteer as officers or non-commissioned officers. Military service varies according to branch: 21 months for the Army and Marine Corps, 23 months for the Navy, 24 months for the Air Force and civil service. The other professional civil service is from 26 months to 36 months. Korea had a bonus point system () which gives a person who completed military service bonus points when applying for a job, but it was found unconstitutional by South Korean constitutional court in 1999.

Recently, however, there has been significant pressure from the public demanding either a shortening of the term or a switch to voluntary military service.

In the Republic of Korea Armed Forces, ranks fall into one of four categories: commissioned officer, warrant officer, non-commissioned officer, and enlisted, in decreasing order of authority. Commissioned officer ranks are further subdivided into ""Janggwan"" or general officers, ""Yeonggwan"" or field grade officers, and ""Wigwan"" or company officers. The ""Wonsu"" is appointed from the ""Daejang"" who has distinguished achievements. However, there has been no one holding the rank of "Wonsu" in the history of the ROK Armed Forces. All branches share a common rank-system, with different colors used to denote the different branches (Army: Green & Black, Navy: White & Black, Marine Corps: Red & Yellow, Air Force: Green & Blue).

Note: The English titles are given as comparative examples with the US Army ranks.
Until April 2011, South Korean soldiers swore allegiance to the "Korean race" in their oaths of enlistment. Likewise, until 2007, the South Korean civilian pledge of allegiance was also to the "Korean race".

In 2008, officers and soldiers of Unit Dongmyeong, stationed in Lebanon with the UNIFIL, received honorary medals from the United Nations.

The ROK military forces are undergoing rapid modernization in preparation for assuming wartime operational control of the ROK's defenses by December 2015. Several cutting-edge military systems are currently being inducted. At the same time, the ROK Armed Forces will see a reduction in active duty personnel from 640,000 to 517,000 by the end of this decade.




</doc>
<doc id="27028" url="https://en.wikipedia.org/wiki?curid=27028" title="Foreign relations of South Korea">
Foreign relations of South Korea

The foreign relations of South Korea (officially the Republic of Korea) are South Korean relations with other governments.

The Republic of Korea maintains diplomatic relations with 190 countries. The country has also been a member of the United Nations since 1991, when it became a member state at the same time as North Korea. South Korea has also hosted major international events such as the 1988 Summer Olympics and 2002 World Cup Soccer Tournament (2002 FIFA World Cup co-hosted with Japan) and the 2011 IAAF World Championships Daegu South Korea Furthermore, South Korea will also be hosting the 2018 Winter Olympics which will take place in Pyeongchang South Korea.

South Korea is a member of the United Nations, WTO, OECD/DAC, ASEAN Plus Three, East Asia Summit (EAS), and G-20. It is also a founding member of Asia-Pacific Economic Cooperation (APEC) and the East Asia Summit.

On January 1, 2007, South Korean Foreign Minister Ban Ki-moon assumed the post of UN Secretary-General.

Inter-Korean relations may be divided into five periods. The first stage was between 1972 and 1973; the second stage was Pyongyang North Korea's delivery of relief goods to South Korea after a typhoon caused devastating floods in 1984 and the third stage was the exchange of home visits and performing artists in 1985. The fourth stage, activated by Nordpolitik under Roh, was represented by expanding public and private contacts between the two Koreas. The fifth stage was improved following the 1997 election of Kim Dae-jung. His "Sunshine Policy" of engagement with North Korea set the stage for the historic June 2000 Inter-Korean summit.

The possibility of Korean reunification has remained a prominent topic. However, no peace treaty has yet been signed with the North. In June 2000, a historic first North Korea-South Korea summit took place, part of the South Korea's continuing Sunshine Policy of engagement. Since then, regular contacts have led to a cautious thaw. President Kim was awarded the Nobel Peace Prize in 2000 for the policy.

With that policy, continued by the following administration of president Roh Moo-hyun, economic ties between the two countries have increased, humanitarian aid has been sent to North Korea and some divided families have been briefly reunited. Military ties remain fraught with tension, however, and in 2002 a brief naval skirmish left four South Korean sailors dead, leaving the future of the Sunshine policy uncertain. The North Korea cut off talks but the South remained committed to the policy of reconciliation and relations began to thaw again. The resurgence of the nuclear issue two years later would again cast relations in doubt, but South Korea has sought to play the role of intermediary rather than antagonist, and economic ties at the time seemed to be growing again.

Despite the Sunshine Policy and efforts at reconciliation, the progress was complicated by North Korean missile tests in 1993, 1998, 2006 and 2009. , relationships between North Korea and South Korea were very tense; North Korea had been reported to have deployed missiles, Ended its former agreements with South Korea and threatened South Korea and the United States not to interfere with a satellite launch it had planned.
As of 2009 North Korea and South Korea are still opposed and share a heavily fortified border.

On May 27, 2009 North Korea media declared that the armistice is no longer valid due to the South Korean government's pledge to "definitely join" the Proliferation Security Initiative. To further complicate and intensify strains between the two nations, the sinking of the South Korean warship Cheonan in March 2010, killing 46 seamen, is as of May 20, 2010 claimed by a team of researchers around the world to have been caused by a North Korean torpedo, which the North denies. South Korea agreed with the findings from the research group and president Lee Myung-bak declared in May 2010 that Seoul would cut all trade with North Korea as part of measures primarily aimed at striking back at North Korea diplomatically and financially. As a result of this, North Korea severed all ties and completely abrogated the previous pact of non aggression.

In November 2010, Unification Ministry officially declared the Sunshine Policy a failure, thus bringing the policy to an end. On November 23, 2010, North Korean artillery shelled Yeonpyeong with dozens of rounds at Yeonpyeong-ri and the surrounding area.

South Korea has the following trade agreements:

As of late 2016 states of Central America (Costa Rica, El Salvador, Guatemala, Honduras, Nicaragua, Panama, Paraguay), GCC (Gulf Cooperation Council—Bahrain, Kuwait, Oman, Qatar, Saudi Arabia, United Arab Emirates), Indonesia, Israel, Japan, Malaysia, MERCOSUR (Southern Common Market—Mercado comun del sur), Mexico, Mongolia, RCEP (Asian 10 Countries, Korea, China, Japan, Australia, New Zealand, India), Russia (BEPA), SACU (South Asia Cooperation Union) and Korea-China-Japan are in negotiations about the FTA with the Republic of Korea.

Active South Korean-Chinese people-to-people contacts have been encouraged. Academics, journalists and particularly families divided between the Republic of Korea and the People's Republic of China were able to exchange visits freely in the late 1980s. Nearly 2 million ethnic Koreans especially in the Yanbian Korean Autonomous Prefecture in China's Jilin Province have interacted with South Koreans.

Trade between the two countries continued to increase nonetheless, Furthermore, China has attempted to mediate between North Korea and the United States and between North Korea and the State of Japan also initiated and promoted tripartite talks—between Pyongyang Seoul and Washington United States of America.

The Republic of Korea had long been an ally of the Republic of China (Taiwan). Diplomatic ties between Seoul and Taipei Republic of China were nevertheless severed in 1992. Formal diplomatic relations were established between Seoul and Beijing People's Republic of China on August 24, 1992.

In 2004 the PRC government began the Northeast Project This sparked a massive uproar in South Korea when the project was widely publicized.

After the KORUS FTA (United States-South Korea Free Trade Agreement) was finalized on June 30, 2007 the Chinese government has immediately begun seeking an FTA agreement with South Korea. The FTA between Korea and China are under discussion South Korea has been running a trade surplus with China which hit a record US$32.5 billion in 2009.

On 23 August 1992, the Republic of China government by then only in control of the island of Taiwan and a few major outlying areas severed diplomatic relations with South Korea in advance of its announcement of formal recognition of the People's Republic of China based in Beijing China The Yonhap News said in 2002 that since then relations between the two governments have been "in a rut".

The relation between South Korea and Japan has both political conflicts and economic intimacies. Examples of conflicts include the East sea naming dispute, visits by successive Japanese Prime Ministers to the Yasukuni Shrine and the disputed ownership of Dokdo of the island Korea.

On January 18, 1952 The first president of South Korea Syngman Rhee declared that the vicinity of Dokdo was a territory of South Korea (Syngman Rhee line). Subsequently, some 3,000 Japanese fishermen who conducted fishery operations in this vicinity were captured. This incident, called the Dai Ichi Daihoumaru Ship case strained relations between South Korea and Japan.

June 22, 1965, The president in South Korea Park Chung-hee concluded the Treaty on Basic Relations between Japan and the Republic of Korea As a result, Japan considered South Korea to be the legitimate successor of the Korean peninsula.

The Republic of Korea's trade with the State of Japan was US$892.1 million in 2008, with a surplus of nearly US$327.1 million on the Japanese side. Japanese and South Koreans firms often had interdependent relations, which gave Japan advantages in South Korea's growing market.

In 1996 FIFA announced that the South Korea-Japan would jointly host the 2002 FIFA World Cup. The next few years would see leaders of both countries meet to warm relations in preparations for the games. The year 2005 was designated as the "Japan-South Korea Friendship Year".

However, the Liancourt Rocks controversy erupted again when Japan's Shimane prefecture declared "Takeshima Day", inciting mass demonstrations in South Korea.

Both countries established diplomatic relations on March 26, 1990. The Republic of Korea has an embassy in Ulaanbaatar Mongolia. Mongolia has an embassy in Seoul.

According to a 2013 BBC World Service Poll, 3% of South Koreans view the Democratic People's Republic of Korea's influence positively, with 91% expressing a negative view. A 2015 government-sponsored poll revealed that 41% of South Koreans consider North Korea to be an enemy, with negative views being more prevalent among younger respondents. Still, in a 2017 poll, 58% of South Koreans said they don't expect another war to break out with North Korea.

Since the establishment of diplomatic ties on 3 March 1949, the relationship between the Philippines and South Korea has flourished. The Philippines was one of the first countries that extended diplomatic recognition to South Korea. This was cemented with the Philippine government’s deployment of the Philippine Expeditionary Force to Korea (PEFTOK) to help South Korea against the invasion of the communist North during the Korean War in the 1950s. After the war, the Philippines provided development assistance to South Korea and helped the country rebuild itself.

Since then, the Philippines’s relations with South Korea have evolved with South Korea becoming one of the Philippines’s most important bilateral partners aside from the United States, China and Japan. The Philippine government seeks to cultivate strategic ties with South Korea given its increasing presence in the country. In the coming years, the Philippines anticipates to benefit from exploring unprecedented opportunities from South Korea that shall contribute significantly to the country’s trade and economy, defense and security, and society and culture.

In the 1980s South Korean president Roh Tae Woo's Nordpolitik and Mikhail Gorbachev's "New Thinking" were both attempts to reverse their nations' recent histories. Gorbachev had signaled Soviet interest in improving relations with all countries in the Asia-Pacific region including South Korea as explained in his July 1986 Vladivostok and August 1988 Krasnoyarsk speeches.

In initiating Nordpolitik Roh's confidential foreign policy adviser was rumored to have visited Moscow Russia to consult with Soviet policymakers. Kim Young Sam visited Moscow Russian Federation from June 2 to June 10, 1989 as the Kremlin announced that it would allow some 300,000 Soviet-South Koreans who had been on the Soviet island of Sahkalin since the end of World War II to return permanently to South Korea. Moscow even arranged Kim's meeting with the North Korean ambassador to the Soviet Union In June 1990, Roh held his first summit with president Gorbachev in San Francisco, United States.

The Republic of Korea and the USSR established diplomatic relations on September 30, 1990. This relation continued by the Russian Federation on December 27, 1991. Russian president Vladimir Putin visited Seoul in February 2001 while South Korean president Roh Moo-hyun visited Moscow Russia in September 2004.

Russian Federal Space Agency and the Korean Astronaut Program cooperated together to send South Korea's first astronaut into space. Yi So-Yeon became the first South Korean national as well as the third woman to be the first national in space on 8 April 2008 when Soyuz TMA-12 departed from Baikonur Cosmodrome.

Since the 1990s there has been greater trade and cooperation between the Russian Federation and South Korea. The total trade volume between the Republic of Korea and the Russian Federation in 2003 was 4.2 billion US dollars.

The establishment of diplomatic relations between the United Kingdom of Great Britain and Northern Ireland and the Republic of Korea began on 18 January 1949.

From the Republic of Korea to the United Kingdom of Great Britain and Northern Ireland:

From the United Kingdom of Great Britain and Northern Ireland to the Republic of Korea:

The United States engaged in the decolonization of Korea (mainly in the South, with the Soviet Union engaged in North Korea) from Japan after World War II. After three years of military administration by the United States, the South Korean government was established. Upon the onset of the Korean War, U.S. forces were sent to defend South Korea against invasion by North Korea and later China. Following the Armistice, South Korea and the U.S. agreed to a "Mutual Defense Treaty", under which an attack on either party in the Pacific area would summon a response from both.

In 1968, South Korea obliged the mutual defense treaty, by sending a large combat troop contingent to support the United States in the Vietnam War. The U.S. Eighth Army, Seventh Air Force, and U.S. Naval Forces Korea are stationed in South Korea. The two nations have strong economic, diplomatic, and military ties, although they have at times disagreed with regard to policies towards North Korea, and with regard to some of South Korea's industrial activities that involve usage of rocket or nuclear technology. There had also been strong anti-American sentiment during certain periods, which has largely moderated in the modern day.

Since the late 1980s, the country has instead sought to establish an American partnership, which has made the Seoul–Washington relationship subject to severe strains. Trade had become a serious source of friction between the two countries. In 1989, the United States was South Korea's largest and most important trading partner and South Korea was the seventh-largest market for United States goods and the second largest market for its agricultural products.

From Roh Tae-woo's administration to Roh Moo Hyun's administration, South Korea sought to establish an American partnership, which has made the Seoul–Washington relationship subject to some strains. In 2007, a free trade agreement known as the Republic of Korea-United States Free Trade Agreement (KORUS FTA) was reportedly signed between South Korea and the United States, but its formal implementation has been repeatedly delayed, pending further approval by the legislative bodies of the two countries.

The relations between the United States and South Korea have greatly strengthened under the Lee Myung-bak administration. At the 2009 G-20 London summit, U.S. President Barack Obama called South Korea "one of America's closest allies and greatest friends."
However, Anti-American sentiment in Korea still exists; The United States' alleged role in the May 1980 Gwangju uprising was the single most pressing South Korean political issue of the 1980s. Even after a decade, Gwangju citizens and other Koreans still blamed the United States for its perceived involvement in the bloody uprising. In 2008, the protests against U.S. beef has been a center of a major discussion in this year.

Letter from President of the Republic of Korea Lee Myung-bak About 37,000 Americans lost their lives. They fought for the freedom of Koreans they did not even know, and thanks to their sacrifices, the peace and democracy of the republic were protected On this significant occasion, all Koreans pay tribute to the heroes fallen in defense of freedom and democracy. I firmly believe that future generations in both countries will further advance the strong the Republic of Korea-the United States of America alliance into one befitting the spirit of the new age. - Los Angeles Times, June 25, 2010 -

The Alliance is adapting to changes in the 21st Century security environment. We will maintain a robust defense posture, backed by allied capabilities which support both nations' security interests We will continue to deepen our strong bilateral economic, trade and investment relations In the Asia-Pacific region we will work jointly with regional institutions and partners to foster prosperity, keep the peace, and improve the daily lives of the people of the region The United States of America and the Republic of Korea will work to achieve our common Alliance goals through strategic cooperation at every level. - The U.S. Government (June 16, 2009) -

The European Union (EU) and the Republic of Korea are important trading partners, having negotiated a free trade agreement for many years since South Korea was designated as a priority FTA partner in 2006. The free trade agreement has been approved in September 2010, following Italy's conditional withdrawal of its veto of the free trade agreement. The compromise made by Italy was that free trade agreement would take provisional effect on July 1, 2011. South Korea is the EU's eighth largest trade partner and the EU has become South Korea's second largest export destination. EU trade with South Korea exceeded €65 billion in 2008 and has enjoyed an annual average growth rate of 7.5% between 2004 and 2008.

The EU has been the single largest foreign investor in South Korea since 1962 and accounted for almost 45% of all FDI inflows into South Korea in 2006. Nevertheless, EU companies have significant problems accessing and operating in the Republic of Korea market due to stringent standards and testing requirements for products and services often creating barriers to trade. Both in its regular bilateral contacts with South Korea and through its FTA with South Korea The EU is seeking to improve this situation.

South Korea does not currently have any diplomatic relations with the following nations.

There are also no diplomatic relations with several unrecognized territories:




</doc>
<doc id="27029" url="https://en.wikipedia.org/wiki?curid=27029" title="List of cities in South Korea">
List of cities in South Korea

The largest cities of South Korea have an autonomous status equivalent to that of provinces. Seoul, the largest city and capital, is classified as a teukbyeolsi (Special City), while the next 6 largest cities (see the list below) are classified as gwangyeoksi (Metropolitan Cities; see Special cities of South Korea). Smaller cities are classified as si ("cities") and are under provincial jurisdiction, at the same level as counties (see Administrative divisions of South Korea).






</doc>
<doc id="27031" url="https://en.wikipedia.org/wiki?curid=27031" title="Schoolly D">
Schoolly D

Jesse Bonds Weaver, Jr. (born June 22, 1962), better known by the stage name Schoolly D (sometimes spelled Schooly D), is an American rapper from Philadelphia, Pennsylvania.

Schoolly D teamed up with DJ Code Money in the mid-1980s. His lyrics reflected urban realism, violence, and sexual bravado, making Schoolly D the first gangsta rapper. He is also interviewed in the 1986 cult documentary "Big Fun In The Big Town". He later embraced an Afrocentric style, bringing Afrocentric culture to hip hop along with KRS-One.

Schoolly D contributed songs and music to many Abel Ferrara films, including "P.S.K." and "Saturday Night" (from "Saturday Night! – The Album") as well as "King of New York" to Ferrara's film of the same name and the title track from "Am I Black Enough For You?" that was played during the climactic shoot-out in that film, the title track from "How a Black Man Feels", and "Signifying Rapper" (from "Smoke Some Kill"), which was used in Ferrara's film "Bad Lieutenant". Because Led Zeppelin successfully sued due to an uncleared interpolation of its song "Kashmir" in "Signifying Rapper", the song was omitted from the soundtrack of the film and from subsequent releases of the film.

Composer Joe Delia tapped Schoolly to co-write and record "The Player" for Ferrara's film "The Blackout", which Delia scored. Schoolly also wrote the score to Ferrara's "'R Xmas". In 2006, Schoolly D co-wrote the indie film soundtrack of the historical science fiction thriller "Order of the Quest" with Chuck Treece. The project series is produced by Benjamin Barnett, and Jay D Clark of Media Bureau. His last album, "Funk 'N Pussy", on Jeff "Met" Thies' Chord Recordings features guest appearances by Public Enemy's Chuck D, Chuck Chillout, Lady B and a drum and bass remix of the classic Schoolly D track "Mr. Big Dick" (remixed by UK trip hop crew The Sneaker Pimps).

Schoolly also performed the music and occasional narration for the cult animated series "Aqua Teen Hunger Force" on the Cartoon Network's Adult Swim programming block. He was a guest on an episode of "Space Ghost Coast to Coast". He also created the song "Sharkian Nights" for the "12 oz. Mouse". The character Jesse B. Weaver from "The Rudy and Gogo World Famous Cartoon Show" was also named after him.

In November 2006 Schoolly D and Cartoon Network were sued over the "Aqua Teen Hunger Force" theme music. A drummer by the name of Terence Yerves claimed he had also written the theme music alongside Schoolly D in 1999 while working at the Meat Locker Studio. Yerves was aware the song would be used for a television series but did not approve of it being used for "Aqua Teen Hunger Force", however, did not file the copyright to the Library of Congress until May 2006, after the series' fourth season had already started airing. In the lawsuit Yerves demanded he receive $150,000 for every time the series was aired after the lawsuit was filed, he also demanded that all existing copies of the series' DVDs be impounded and for "Aqua Teen Hunger Force" to cease broadcast.

Rapper Ice-T, who is often given credit for the creation of gangsta rap, says that Schoolly D was the first gangsta rapper.

In the DVD extra on the "King of New York", Schoolly D claims to have independently invented the sport of snowboarding by sledding down Philadelphia hills on pieces of linoleum. (Snowboarding has roots in snurfing, which was invented in 1965.)

Funk metal band Primus mentions Schoolly D in their song "Harold of the Rocks" on the album "Frizzle Fry".

The Beastie Boys sampled Schoolly D's song "Gucci Time" on their 1986 hit "Time to Get Ill."





</doc>
<doc id="27032" url="https://en.wikipedia.org/wiki?curid=27032" title="Rock–paper–scissors">
Rock–paper–scissors

Rock-paper-scissors (also known as scissors-paper-rock or other variants) is a hand game usually played between two people, in which each player simultaneously forms one of three shapes with an outstretched hand. These shapes are "rock" (a closed fist), "paper" (a flat hand), and "scissors" (a fist with the index finger and middle finger extended, forming a V). "Scissors" is identical to the two-fingered V sign (aka "victory" or "peace sign") except that it is pointed horizontally instead of being held upright in the air. 
A simultaneous, zero-sum game, it has only two possible outcomes: a draw, or a win for one player and a loss for the other.

A player who decides to play rock will beat another player who has chosen scissors ("rock crushes scissors" or sometimes "blunts scissors"), but will lose to one who has played paper ("paper covers rock"); a play of paper will lose to a play of scissors ("scissors cuts paper"). If both players choose the same shape, the game is tied and is usually immediately replayed to break the tie. The type of game originated in China and spread with increased contact with East Asia, while developing different variants in signs over time. Other names for the game in the English-speaking world include roshambo and other orderings of the three items, with "rock" sometimes being called "stone".

Rock–paper–scissors is often used as a fair choosing method between two people, similar to coin flipping, drawing straws, or throwing dice in order to settle a dispute or make an unbiased group decision. Unlike truly random selection methods, however, rock–paper–scissors can be played with a degree of skill by recognizing and exploiting non-random behavior in opponents.

The players usually count aloud to three, or speak the name of the game (e.g. "Rock! Paper! Scissors!" or "Ro Sham Bo!"), each time either raising one hand in a fist and swinging it down on the count or holding it behind. They then "throw" by extending it towards their opponent. Variations include a version where players use only three counts before throwing their gesture (thus throwing on the count of "Scissors!" or "Bo!"), or a version where they shake their hands three times before "throwing".

The first known mention of the game was in the book "" by the Chinese Ming-dynasty writer ( 1600), who wrote that the game dated back to the time of the Chinese Han dynasty (206 BC – 220 AD). In the book, the game was called "shoushiling". Li Rihua's book "Note of Liuyanzhai" also mentions this game, calling it "shoushiling" (t. 手勢令; s. 手势令), "huozhitou" (t. 豁指頭; s. 豁指头), or "huoquan" (豁拳).
Throughout Japanese history there are frequent references to "sansukumi-ken", meaning "ken" (fist) games where "the three who are afraid of one another" (i.e. A beats B, B beats C, and C beats A). This type of game originated in China before being imported to Japan and subsequently also becoming popular among the Japanese.

The earliest Japanese "sansukumi-ken" game was known as "mushi-ken" (虫拳), which was imported directly from China. In "mushi-ken" the "frog" (represented by the thumb) is superseded by the "slug" (represented by the little finger), which, in turn is superseded by the "snake" (represented by the index finger), which is superseded by the "frog". Although this game was imported from China the Japanese version differs in the animals represented. In adopting the game, the original Chinese characters for the poisonous centipede (蜈蜙) were apparently confused with the characters for the slug (蛞蝓). The most popular "sansukumi-ken" game in Japan was "kitsune-ken" (狐拳). In the game, a supernatural fox called a kitsune (狐) defeats the village head, the village head (庄屋) defeats the hunter, and the hunter (猟師) defeats the fox. "Kitsune-ken", unlike "mushi-ken" or rock–paper–scissors, is played by making gestures with both hands.

Today, the best-known "sansukumi-ken" is called , which is a variation of the Chinese games introduced in the 17th century. "Jan-ken" uses the rock, paper, and scissors signs and is the game that the modern version of rock–paper–scissors derives from directly. Hand-games using gestures to represent the three conflicting elements of rock, paper, and scissors have been most common since the modern version of the game was created in the late 19th century, between the Edo and Meiji periods.

By the early 20th century, rock–paper–scissors had spread beyond Asia, especially through increased Japanese contact with the west. Its English-language name is therefore taken from a translation of the names of the three Japanese hand-gestures for rock, paper and scissors: elsewhere in Asia the open-palm gesture represents "cloth" rather than "paper". The shape of the scissors is also adopted from the Japanese style.

In Britain in 1924 it was described in a letter to "The Times" as a hand game, possibly of Mediterranean origin, called "zhot".
A reader then wrote in to say that the game "zhot" referred to was evidently Jan-ken-pon, which she had often seen played throughout Japan. Although at this date the game appears to have been new enough to British readers to need explaining, the appearance by 1927 of a popular thriller with the title "Scissors Cut Paper", followed by "Stone Blunts Scissors" (1929), suggests it quickly became popular.

In 1927 "La Vie au patronage", a children's magazine in France, described it in detail, referring to it as a "jeu japonais" ("Japanese game"). Its French name, "Chi-fou-mi", is based on the Old Japanese words for "one, two, three" ("hi, fu, mi").

A "New York Times" article of 1932 on the Tokyo rush hour describes the rules of the game for the benefit of American readers, suggesting it was not at that time widely known in the U.S. The 1933 edition of the "Compton's Pictured Encyclopedia" described it as a common method of settling disputes between children in its article on Japan; the name was given as "John Kem Po" and the article pointedly asserted, "This is such a good way of deciding an argument that American boys and girls might like to practice it too."

It is impossible to gain an advantage over a truly random opponent. However, by exploiting the weaknesses of non-random opponents, it is possible to gain a significant advantage. Indeed, human players tend to be non-random. As a result, there have been programming competitions for algorithms that play rock–paper–scissors.

In tournament play, some players employ tactics to confuse or trick the other player into making an illegal move, resulting in a loss. One such tactic is to shout the name of one move before throwing another, in order to misdirect and confuse their opponent. During tournaments, players often prepare their sequence of three gestures prior to the tournament's commencement.

The "rock" move, in particular, is notable in that it is typically represented by a closed fist—often identical to the fist made by players during the initial countdown. If a player is attempting to beat their opponent based on quickly reading their hand gesture as the players are making their moves, it is possible to determine if the opponent is about to throw "rock" based on their lack of hand movement, as both "scissors" and "paper" require the player to reposition their hand. This can likewise be used to deceive an anticipating opponent by keeping one's fist closed until the last possible second, leading them to believe that you are about to throw "rock".

As a consequence of rock–paper–scissors programming contests, many strong algorithms have emerged. For example, Iocaine Powder, which won the First International RoShamBo Programming Competition in 1999, uses a heuristically designed compilation of strategies. For each strategy it employs, it also has six metastrategies which defeat second-guessing, triple-guessing, as well as second-guessing the opponent, and so on. The optimal strategy or metastrategy is chosen based on past performance. The main strategies it employs are history matching, frequency analysis, and random guessing. Its strongest strategy, history matching, searches for a sequence in the past that matches the last few moves in order to predict the next move of the algorithm. In frequency analysis, the program simply identifies the most frequently played move. The random guess is a fallback method that is used to prevent a devastating loss in the event that the other strategies fail. More than ten years later, the top performing strategies on an ongoing rock–paper–scissors programming competition similarly use metastrategies. However, there have been some innovations, such as using multiple history matching schemes that each match a different aspect of the history – for example, the opponent's moves, the program's own moves, or a combination of both. There have also been other algorithms based on Markov chains.

In 2012, researchers from the Ishikawa Watanabe Laboratory at the University of Tokyo created a robot hand that can play rock–paper–scissors with a 100% win rate. Using a high-speed camera the robot recognizes within one millisecond which shape the human hand is making, then produces the corresponding winning shape.

In 2006, American federal judge Gregory Presnell from the Middle District of Florida ordered opposing sides in a lengthy court case to settle a trivial (but lengthily debated) point over the appropriate place for a deposition using the game of rock–paper–scissors. The ruling in "Avista Management v. Wausau Underwriters" stated:

The public release of this judicial order, widely circulated among area lawyers, was seemingly intended to shame the respective law firms regarding their litigation conduct by settling the dispute in a farcical manner.

In 2005, when Takashi Hashiyama, CEO of Japanese television equipment manufacturer Maspro Denkoh, decided to auction off the collection of Impressionist paintings owned by his corporation, including works by Paul Cézanne, Pablo Picasso, and Vincent van Gogh, he contacted two leading auction houses, Christie's International and Sotheby's Holdings, seeking their proposals on how they would bring the collection to the market as well as how they would maximize the profits from the sale. Both firms made elaborate proposals, but neither was persuasive enough to get Hashiyama's business. Unwilling to split up the collection into separate auctions, Hashiyama asked the firms to decide between themselves who would hold the auction, which included Cézanne's "Large Trees Under the Jas de Bouffan", worth $12–16 million.

The houses were unable to reach a decision. Hashiyama told the two firms to play rock–paper–scissors to decide who would get the rights to the auction, explaining that "it probably looks strange to others, but I believe this is the best way to decide between two things which are equally good".

The auction houses had a weekend to come up with a choice of move. Christie's went to the 11-year-old twin daughters of the international director of Christie's Impressionist and Modern Art Department Nicholas Maclean, who suggested "scissors" because "Everybody expects you to choose 'rock'." Sotheby's said that they treated it as a game of chance and had no particular strategy for the game, but went with "paper".

Christie's won the match and sold the $20 million collection, with millions of dollars of commission for the auction house.

In Japan, researchers have taught chimpanzees to play rock–paper–scissors.

In many games, it is common for a group of possible choices to interact in a rock–paper–scissors style, where each selection is strong against a particular choice, but weak against another. Such mechanics can make a game somewhat self-balancing, and prevent gameplay from being overwhelmed by a single dominant strategy. 

Many card-based video games in Japan use the rock–paper–scissors system as their core fighting system, with the winner of each round being able to carry out their designated attack. Sega Master System's Alex Kidd in Miracle World has a level where the player has to win a rock-paper-scissors game to go ahead. Others use simple variants of rock–paper–scissors as subgames like "Mario Party Advance" and "".

In Pokémon, there is a rock–paper–scissors element in the type effectiveness system. For example, a Grass-type Pokémon is weak to Fire, Fire is weak to Water, and Water is weak to Grass.

The common side-blotched lizard ("Uta stansburiana") exhibits a rock–paper–scissors pattern in its mating strategies. Of its three color types of males, "orange beats blue, blue beats yellow, and yellow beats orange" in competition for females, which is similar to the rules of rock-paper-scissors.

Some bacteria also exhibit a rock-paper-scissors dynamic when they engage in antibiotic production. The theory for this finding was demonstrated by computer simulation and in the laboratory by Benjamin Kerr, working at Stanford University with Brendan Bohannan. Additional "in vitro" results demonstrate rock-paper-scissors dynamics in additional species of bacteria. Biologist Benjamin C. Kirkup, Jr. demonstrated that these antibiotics, bacterioicins, were active as "Escherichia coli" compete with each other in the intestines of mice, and that the rock-paper-scissors dynamics allowed for the continued competition among strains: antibiotic-producers defeat antibiotic-sensitives; antibiotic-resisters multiply and withstand and out-compete the antibiotic-producers, letting antibiotic-sensitives multiply and out-compete others; until antibiotic-producers multiply again.

Rock–paper–scissors is the subject of continued research in bacterial ecology and evolution. It is considered one of the basic applications of game theory and non-linear dynamics to bacteriology. Models of evolution demonstrate how intragenomic competition can lead to rock-paper-scissors dynamics from a relatively general evolutionary model. The general nature of this basic non-transitive model is widely applied in theoretical biology to explore bacterial ecology and evolution.

Various competitive rock–paper–scissors tournaments have been organised by different groups.
Starting in 2002, the World Rock Paper Scissors Society standardized a set of rules for international play and has overseen annual International World Championships. These open, competitive championships have been widely attended by players from around the world and have attracted widespread international media attention. WRPS events are noted for their large cash prizes, elaborate staging, and colorful competitors.
In 2004, the championships were broadcast on the U.S. television network Fox Sports Net, with the winner being Lee Rammage, who went on to compete in at least one subsequent championship. The 2007 tournament was won by Andrea Farina. The last tournament hosted by the World Rock Paper Scissors Society was in Toronto, Canada, on November 14, 2009.

Several RPS events have been organised in the United Kingdom by Wacky Nation. 
The 1st UK Championship took place on 13 July 2007, and then again on 14 July 2008, in Rhayader, Powys.

The 3rd UK Championships took place on 9 June 2009, in Exeter, Devon. Nick Hemley, from Woking, Surrey, won the contest just beating Chris Grimwood.

The 4th UK Championships took place on 13 November 2010, at the Durell Arms in West London. Paul Lewis from Woking beat Ed Blake in the final and collected the £100 first prize and UK title. Richard Daynes Appreciation Society won the team event. 80 competitors took part in the main contest and 10 entries in the team contest.

The 5th UK Rock Paper Scissors Championships took place in London on Saturday 22 October 2011. The event was open to 128 individual competitors. There was also a team contest for 16 teams. The 2011 singles tournament was won by Max Deeley and the team contest won by The Big Faces (Andrew Bladon, Jamie Burland, Tom Wilkinson and Captain Joe Kenny).

The 6th UK Rock Paper Scissors Championships took place at Crosse Keys Pub, London on Saturday 13 October 2012 with over 200 competitors.

The 8th UK Rock Paper Scissors Championships took place at the Green Man Pub in London on Saturday 4 October 2014, and was won by Dan Tinkler of Leicester.

The 9th UK Rock Paper Scissors Championships took place at the Green Man Pub in London on Saturday 4 November 2015, and was won by Loic Zimou of London.

The 10th UK Rock Paper Scissors Championships took place at the Green Man Pub in London on Saturday 19 November 2016, and was won by Ronak Kansagra of Ealing.

The 11th UK Rock Paper Scissors Championships took place at the Crutched Friar pub in London on Saturday 18 November 2017.

USA Rock Paper Scissors League is sponsored by Bud Light. Leo Bryan Pacis was the first commissioner of the USARPS. Cody Louis Brown was elected as the second commissioner of the USARPS in 2014.

In April 2006, the inaugural USARPS Championship was held in Las Vegas. Following months of regional qualifying tournaments held across the US, 257 players were flown to Las Vegas for a single-elimination tournament at the House of Blues where the winner received $50,000. The tournament was shown on the A&E Network on 12 June 2006.

The $50,000 2007 USARPS Tournament took place at the Las Vegas Mandalay Bay in May 2007.

In 2008, Sean "Wicked Fingers" Sears beat 300 other contestants and walked out of the Mandalay Bay Hotel & Casino with $50,000 after defeating Julie "Bulldog" Crossley in the finals.

The inaugural Budweiser International Rock, Paper, Scissors Federation Championship was held in Beijing, China after the close of the 2008 Summer Olympic Games at Club Bud. A Belfast man won the competition.

The international tournament was held in London 2012. UK Champions Team GB (Andrew Bladon, Jamie Burland, Tom Wilkinson and Stephen Preston) went in as overwhelming favorites, but after a "domestic incident" team captain and UK Team Champion Joe Kenny was forced to pull out, allowing Stephen Preston to take his place. Great Britain came a respectable third to achieve the Bronze Medal, while the crowd favorite Vatican City got the Silver and Lapland A took the prestigious Gold Medal. British team captain Tom Wilkinson commented "after a 4-0 whitewash of hot favorites Vatican City we thought we had it. A simple lapse of concentration lost it for us, but we are happy with our bronze medal. We'll come back from this and look to take the title back again next year. The support was immense, and we are thankful of everyone who came out to support us".

The XtremeRPS National Competition is a US nationwide RPS competition with Preliminary Qualifying contests that started in January 2007 and ended in May 2008, followed by regional finals in June and July 2008. The national finals were to be held in Des Moines, Iowa in August 2008, with a chance to win up to $5,000.

The largest Rock-Paper-Scissors tournament is 2,950 and was achieved by Oomba, Inc. (USA) at Gen Con 2014 in Indianapolis, Indiana, USA, on 17 August 2014.

Former Celebrity Poker Showdown host and USARPS Head Referee Phil Gordon has hosted an annual $500 World Series of Rock, Paper, Scissors event in conjunction with the World Series of Poker since 2005. The winner of the WSORPS receives an entry into the WSOP Main Event. The event is an annual fundraiser for the "Cancer Research and Prevention Foundation" via Gordon's charity "Bad Beat on Cancer". Poker player Annie Duke won the Second Annual World Series of Rock, Paper, Scissors. The tournament is taped by ESPN and highlights are covered during "The Nuts" section of ESPN's annual WSOP broadcast. 2009 was the fifth year of the tournament.

Jackpot En Poy is a game segment of the Philippines' longest running noontime show, Eat Bulaga!. The game is based on the classic children's game rock–paper–scissors where four players are paired to compete in the three-round segment. In the first round, the first pair plays against each other until one player wins three times. The next pair then plays against each other in the second round. The winners from the first two rounds then compete against each other to finally determine the ultimate winner. The winner of the game then moves on to the final round. In the final round, the player is presented with several Dabarkads, each holding different amounts of cash prize. The player will then pick three Dabarkads who he or she will play rock–paper–scissors against. The player plays against them one at a time. If the player wins against any of the Eat Bulaga! host, he or she will win the cash prize.

Players have developed numerous cultural and personal variations on the game, from simply playing the same game with different objects, to expanding into more weapons and rules.

In Korea, a two-player upgraded version exists by the name muk-jji-ppa.

In Japan, a "strip-poker" variant of rock-paper-scissors is known as 野球拳 (Yakyuken). The loser of each round removes an article of clothing. The game is a minor part of porn culture in Japan and other Asian countries after the influence of TV variety shows and Soft On Demand.

In the Philippines, the game is called "jak-en-poy", from one of the Japanese names of the game, transliterated as "jan-ken-pon". In a longer version of the game, a four-line song is sung, with hand gestures displayed at the end of each (or the final) line: "Jack-en-poy! / Hali-hali-hoy! / Sino'ng matalo, / siya'ng unggoy!" ("Jack-en-poy! / Hali-hali-hoy! / Whoever loses is the monkey!") In the former case, the person with the most wins at the end of the song, wins the game. A shorter version of the game uses the chant "Bato-bato-pick" ("Rock-rock-pick [i.e. choose]") instead.

A multiple player variation can be played: Players stand in a circle and all throw at once. If rock, paper, and scissors are all thrown, it is a stalemate, and they rethrow. If only two throws are present, all players with the losing throw are eliminated. Play continues until only the winner remains.

In the Malaysian version of the game, "scissors" is replaced by "bird," represented with the finger tips of five fingers brought together to form a beak. The open palm represents water. Bird beats water (by drinking it); stone beats bird (by hitting it); and stone loses to water (because it sinks in it).

Singapore also has a related hand-game called "ji gu pa," where "ji" refers to the bird gesture, "gu" refers to the stone gesture, and "pa" refers to the water gesture. The game is played by two players using both hands. At the same time, they both say, ji gu pa!" At "pa!" they both show two open-palmed hands. One player then changes his hand gestures while calling his new combination out (e.g., "pa gu!"). At the same time, the other player changes his hand gestures as well. If one of his hand gestures is the same as the other one, that hand is "out" and he puts it behind his back; he is no longer able to play that hand for the rest of the round. The players take turns in this fashion, until one player loses by having both hands sent "out." "Ji gu pa" is most likely a transcription of the Japanese names for the different hand gestures in the original jan-ken game, "choki" (scissors), "guu" (rock) and "paa" (paper).

Using the same tripartite division, there is a full-body variation in lieu of the hand signs called "Bear, Hunter, Ninja". In this iteration the participants stand back-to-back and at the count of three (or ro-sham-bo as is traditional) turn around facing each other using their arms evoking one of the totems. The players' choices break down as: Hunter shoots bear; Bear eats ninja; Ninja kills hunter. The game was popularized with a FedEx commercial where warehouse employees had too much free time on their hands.

As long as the number of moves is an odd number and each move defeats exactly half of the other moves while being defeated by the other half, any combination of moves will function as a game. For example, 5-, 7-, 9-, 11-, 15-, 25-, and 101-weapon versions exist. Adding new gestures has the effect of reducing the odds of a tie, while increasing the complexity of the game. The probability of a tie in an odd-number-of-weapons game can be calculated based on the number of weapons n as 1/n, so the probability of a tie is 1/3 in standard rock-paper-scissors, but 1/5 in a version that offered five moves instead of three.

Similarly, the French game "pierre, papier, ciseaux, puits" (stone, paper, scissors, well) is unbalanced; both the stone and scissors fall in the well and lose to it, while paper covers both stone and well. This means two "weapons", well and paper, can defeat two moves, while the other two weapons each defeat only one of the other three choices. The rock has no advantage to well, so optimal strategy is to play each of the other objects (paper, scissors and well) one third of the time. 
One popular five-weapon expansion is "", invented by Sam Kass and Karen Bryla, which adds "Spock" and "lizard" to the standard three choices. "Spock" is signified with the "Star Trek" Vulcan salute, while "lizard" is shown by forming the hand into a sock-puppet-like mouth. Spock smashes scissors and vaporizes rock; he is poisoned by lizard and disproven by paper. Lizard poisons Spock and eats paper; it is crushed by rock and decapitated by scissors. This variant was mentioned in a 2005 article in "The Times" of London and was later the subject of an episode of the American sitcom "The Big Bang Theory" in 2008 (as rock-paper-scissors-lizard-Spock).

The majority of such proposed generalizations are isomorphic to a simple game of modular arithmetic, where half the differences are wins for player one. For instance, rock-paper-scissors-Spock-lizard (note the different order of the last two moves) may be modeled as a game in which each player picks a number from one to five. Subtract the number chosen by player two from the number chosen by player one, and then take the remainder modulo 5 of the result. Player one is the victor if the difference is one or three, and player two is the victor if the difference is two or four. If the difference is zero, the game is a tie.

Alternatively, the rankings in rock-paper-scissors-Spock-lizard may be modeled by a comparison of the parity of the two choices. If it is the same (two odd-numbered moves or two even-numbered ones) then the lower number wins, while if they are different (one odd and one even) the higher wins. Using this algorithm, additional moves can easily be added two at a time while keeping the game balanced:




</doc>
<doc id="27033" url="https://en.wikipedia.org/wiki?curid=27033" title="Logudorese dialect">
Logudorese dialect

Logudorese Sardinian (, ) is a standardised variety of Sardinian, often considered the most conservative of all Romance languages. Its ISO 639-3 code is "src". Italian-speakers do not understand Sardinian, which is a separate language.

Latin G and K before , are not palatalized in Logudorese, in stark contrast with all other Romance languages. Compare Logudorese ' with Italian ' , Spanish ' and French ' . Like the other varieties of Sardinian, most subdailects of Logudorese also underwent Lenition in the intervocalic plosives of --, --, and --/ (e.g. Lat. "focum" > "fogu" "fire", "ripa" > "riba" "shore, bank", "rota" > "roda" "wheel"). Logudorese also turns medial and into and and , respectively (e.g. Lat. "Sardinia" > "Sardigna" and "folium" > "fogia" "leaf"). Finally, Logudorese shifts the Latin labiovelars and into medially and word-initially (Lat. "lingua" > "limba" "tongue", "qualem" > "cale" "what")

Logudorese is intelligible to those from the southern part of Sardinia, where Campidanese Sardinian is spoken, but it is not to those from the extreme north of the island, where Corsican–Sardinian dialects are spoken.

The area of Logudoro (term originated as a blend of the kingdom's name of Logu de Torres) in which it is spoken, a northern subregion of the island of Sardinia with close ties to Ozieri ("Othieri") and Nuoro ("Nùgoro") for culture and language, as well as history, with important particularities in the western area, where the most important town is Ittiri. It is an area of roughly 150 × 100 km with some 500,000–700,000 inhabitants.

The language's origins have been investigated by Eduardo Blasco Ferrer and others. The language derives from Latin and a pre-Latin, Paleo-Sardinian (Nuragic) substratum, but has been influenced by Catalan and Spanish due to the dominion of the Crown of Aragon and later the Spanish Empire over the island. Logudorese is the northern macro-dialect of the Sardinian language, the southern macro-dialect being Campidanese, spoken in the southern half of the island. The two variants share a clear common origin and history, but have experienced somewhat different developments.

Though the language is typically Romance, some words in it are not of Latin origin, and are of uncertain etymology. One such is "nura", found in "nuraghe", the main form of pre-Roman building, hence the term for the pre-Roman era as the Nuragic Period. Various place names similarly have roots that defy analysis.

Logudorese changed only very slowly from Vulgar Latin in comparison to other Romance lects, with Linguist Mario Pei reporting a 8% degree of separation from Latin in the Nuorese subdialect, the most conservative compared to other Romance Languages. Because of this reason, as well as the preservation of many works of traditional literature from the 15th century onwards, Logudorese is often considered to be the most prestigious variety of Sardinian.

Logudorese has multiple dialects, some confined to individual villages or valleys. Though such differences can be noticeable, the dialects are mutually intelligible, and share some mutual intelligibility with the neighbouring Campidanese dialects.

Northern Logudorese

Spoken in the north of Sardinia, this subdialect contains the following features:

, , changes to , , (Lat. "plovere" > "piòere" "rain", "florem" > "fiore" "flower", "clavem" > "ciae" "key")

Central (Common) Logudorese

Spoken in Central Sardinia, this subdialect contains the following features:

, , changes to , , (Lat. "plovere" > "pròere" "rain", "florem" > "frore" "flower", "clavem" > "crae" "key")

Nuorese

The Nuorese subdialects (spoken in Nuoro and Baronia) have some distinctive features not found anywhere else in Sardinia, many features demonstrating the conservative nature of the dialect:

No lenition of intervocalic plosives (e.g. Lat. "focum" > "focu" "fire", "ripa" > "ripa" "shore, bank", "rota" > "rota" "wheel")

No palatal realisation of and , instead turning into and and , respectively (e.g. Lat. "Sardinia" > "Sardinna" and "folium" > "folla" "leaf").

Preservation of intervocalic , , and (Lat. "augustus" "August" > Log. "austu" but Nuo. "agustu", Lat. "credere" "to believe" > Log. "creere" but Nuo. "credere", Lat. "novem" "nine" > Log. "noe" vs Nuo. "nobe" < "nove")

Betacism of in Nuoro but not in Baronia.

Latin before yod to in Nuorese ("plateam" "street, courtyard" > "pratha"), albeit the sound is in the process of becoming ("pratza").

"In nomine Domini amen. Ego iudice Mariano de Lacon fazo ista carta ad onore de omnes homines de Pisas pro xu toloneu ci mi pecterunt: e ego donolislu pro ca lis so ego amicu caru e itsos a mimi; ci nullu imperatore ci lu aet potestare istu locu de non (n)apat comiatu de leuarelis toloneu in placitu: de non occidere pisanu ingratis: e ccausa ipsoro ci lis aem leuare ingratis, de facerlis iustitia inperatore ci nce aet exere intu locu [...]"

"Tando su rey barbaru su cane renegadu / de custa resposta multu restayt iradu / & issu martiriu fetit apparigiare / itu su quale fesit fortemente ligare / sos sanctos martires cum bonas catenas / qui li segaant sos ossos cum sas veinas / & totu sas carnes cum petenes de linu."

"A sos tempos de sa pitzinnìa, in bidda, totus chistionaiamus in limba sarda. In domos nostras no si faeddaiat atera limba. E deo, in sa limba nadìa, cominzei a connoscher totu sas cosas de su mundu. A sos ses annos, intrei in prima elementare e su mastru de iscola proibeit, a mie e a sos fedales mios, de faeddare in s'unica limba chi connoschiamus: depiamus chistionare in limba italiana, «la lingua della Patria», nos nareit, seriu seriu, su mastru de iscola. Gai, totus sos pitzinnos de 'idda, intraian in iscola abbistos e allirgos e nde bessian tontos e cari-tristos."

A large body of Sardinian poetry, songs and literature is composed in Logudorese.




</doc>
<doc id="27034" url="https://en.wikipedia.org/wiki?curid=27034" title="Sardinian language">
Sardinian language

Sardinian or Sard ("sardu" , "limba sarda" or "língua sarda" ) is the primary indigenous Romance language spoken on most of the island of Sardinia (Italy). Among the Romance languages, it is considered one of the closest genealogical descendants, if not the closest, to Latin by the Romance linguists. However, it also incorporates a Pre-Latin (mostly Paleo-Sardinian, also known as Nuragic, and, to a much lesser degree, Punic) substratum, and a Byzantine Greek, Catalan, Spanish and Italian superstratum due to the political membership of the island from the late Middle Ages onwards, first falling into the Iberian sphere of influence and in the 18th century towards the Italian one.

The Sardinian language is traditionally thought to consist of two mutually intelligible varieties, Campidanese and Logudorese, spoken respectively in the Southern half and in the North-Central part of Sardinia; the view of there being a definite dialectal boundary has been subjected to more recent research, that shows on the contrary a continuum from the Northern to the Southern ends of the island. Such perception of the Sardinian dialects, rather than pointing to an actual isogloss, is in fact the result of a psychological adherence to the way Sardinia was administratively subvidided into a "Caput Logudori" ("Cabu de Susu") and a "Caput Calaris" ("Cabu de Jossu") by the Spanish. Some attempts have been recently made to introduce a standardized writing system for administrative purposes, like the LSU ("Limba Sarda Unificada", "Unified Sardinian Language") and then the LSC ("Limba Sarda Comuna", "Common Sardinian Language"), but they have not been generally acknowledged by native speakers.

In 1997, Sardinian was recognized by a regional law, along with other languages spoken on the island; since 1999, Sardinian is also one of the twelve "historical language minorities" of Italy, being granted recognition by the national Law no. 482/1999. However, the vitality of the Sardinian-speaking community is threatened and UNESCO classifies the language as "definitely endangered", although an estimated 68.4 percent of the islanders report to have a good oral command of Sardinian. While the level of language competence is in fact relatively high among the older generation beyond retirement age, it has been estimated to have dropped to around 13 percent among children, with Sardinian being kept as a heritage language.

Sardinian is considered the most conservative Romance language, and its substratum (Paleo-Sardinian or Nuragic) has also been researched. A 1949 study by Italian-American linguist Mario Pei, analyzing the degree of difference from a language's parent (Latin, in the case of Romance languages) by comparing phonology, inflection, syntax, vocabulary, and intonation, indicated the following percentages (the higher the percentage, the greater the distance from Latin): Sardinian 8%, Italian 12%, Spanish 20%, Romanian 23.5%, Occitan 25%, Portuguese 31%, and French 44%. For example, Latin ""Pone mihi tres panes in bertula"" (put three loaves of bread [from home] in the bag for me) would be the very similar ""Ponemi tres panes in bertula"" in Sardinian.

Compared to the mainland Italian dialects, Sardinian is virtually incomprehensible for Italians, being actually an autonomous linguistic group.

Sardinia's relative isolation from mainland Europe encouraged the development of a Romance language preserving traces of its indigenous, pre-Roman language(s). The language is posited to have substratal influences from Paleo-Sardinian language, which some scholars have linked to Basque and Etruscan. Adstratal influences include Catalan, Spanish, and Italian. The situation of Sardinian language with regard to the politically dominant ones did not change until the 1950s.

The origins of the Paleo-Sardinian language are currently not known. Research has attempted to discover obscure, indigenous, pre-Romance roots; the root "s(a)rd", present in many place names and denoting the island's people, is reportedly from the Sherden (one of the Sea Peoples), although this assertion is hotly debated.

In 1984, Massimo Pittau said he found in the Etruscan language the etymology of many Latin words after comparing it with the Nuragic language(s). Etruscan elements, formerly thought to have originated in Latin, would indicate a connection between the ancient Sardinian culture and the Etruscans. According to Pittau, the Etruscan and Nuragic language(s) are descended from Lydian (and therefore Indo-European) as a consequence of contact with Etruscans and other Tyrrhenians from Sardis as described by Herodotus. Although Pittau suggests that the Tirrenii landed in Sardinia and the Etruscans landed in modern Tuscany, his views are not shared by most Etruscologists.

According to Alberto Areddu the Sherden were of Illyrian origin, on the basis of some lexical elements, unanimously acknowledged as belonging to the indigenous Substrate. Areddu asserts that in ancient Sardinia, especially in the most interior area (Barbagia and Ogliastra), the locals supposedly spoke a particular branch of Indo-European. There are in fact some correspondences, both formal and semantic, with the few testimonies of Illyrian (or Thracian) languages, and above all with their claimed linguistical continuer, Albanian. He finds such correlations: Sard. "eni, enis, eniu" 'yew' = Alb. "enjë" 'yew'; Sard. "urtzula" 'clematis' = Alb. "urth" 'ivy'; Sard. "rethi" 'tendril' = Alb. "rrypthi" 'tendril'. Recently he also discovered important correlations with the Balkan bird world.

According to Bertoldi and Terracini, Paleo-Sardinian has similarities with the Iberic languages and Siculian; for example, the suffix "-ara" in proparoxytones indicated the plural. Terracini proposed the same for suffixes in -, -/ànna/, -/énna/, -/ònna/ + + a paragogic vowel (such as the toponym "Bunnànnaru"). Rohlfs, Butler and Craddock add the suffix - (such as the toponym "Barùmini") as a unique element of Paleo-Sardinian. Suffixes in /a, e, o, u/ + -rr- found a correspondence in north Africa (Terracini), in Iberia (Blasco Ferrer) and in southern Italy and Gascony (Rohlfs), with a closer relationship to Basque (Wagner and Hubschmid). However, these early links to a Basque precursor have been questioned by some Basque linguists. According to Terracini, suffixes in -, -, -, and - are common to Paleo-Sardinian and northern African languages. Pittau emphasized that this concerns terms originally ending in an accented vowel, with an attached paragogic vowel; the suffix resisted Latinization in some place names, which show a Latin body and a Nuragic suffix. According to Bertoldi, some toponyms ending in - and -/asài/ indicated an Anatolic influence. The suffix -/aiko/, widely used in Iberia and possibly of Celtic origin, and the ethnic suffix in -/itanos/ and -/etanos/ (for example, the Sardinian "Sulcitanos") have also been noted as Paleo-Sardinian elements (Terracini, Ribezzo, Wagner, Hubschmid and Faust).

Linguists Blasco Ferrer (2009, 2010), Morvan (2009) and Arregi (2017) have attempted to revive a theoretical connection with Basque by linking words such as Sardinian "ospile" "fresh grazing for cattle" and Basque "ozpil"; Sardinian "arrotzeri" "vagabond" and Basque "arrotz" "stranger"; Sardinian "galostiu" and Basque "gorostoi"; Gallurese (Corso-Sardinian) "zerru" "pig" and Basque "zerri". Genetic data on the distribution of HLA antigens have suggested a common origin for the Basques and Sardinians.

Since the Neolithic period, some degree of variance across the island's regions is also attested. The Arzachena culture, for instance, suggests a link between the northernmost Sardinian region (Gallura) and southern Corsica that finds further confirmation in the Naturalis Historia by Pliny the Elder. There are also some stylistic differences across Northern and Southern Nuragic Sardinia, which may indicate the existence of two other tribal groups (Balares and Ilienses) mentioned by the same Roman author. According to the archeologist Giovanni Ugas, these tribes may have in fact played a role in shaping the current regional linguistic differences of the island.

Although Roman domination, which began in 238 , brought Latin to Sardinia, it was unable to completely supplant the pre-Latin Sardinian languages, including Punic, which continued to be spoken in the 4th century as attested by votive inscriptions. Some obscure Nuragic roots remained unchanged, and in many cases the Latin accepted local roots (like "nur", which makes its appearance in nuraghe, "Nurra", "Nurri" and many other toponyms). Barbagia, the mountainous central region of the island, derives its name from the Latin "Barbaria" (a term meaning "Land of the Barbarians", similar in origin to the "Barbary" word), because its people refused cultural and linguistic assimilation for a long time: 50% of toponyms of central Sardinia, particularly in the territory of Olzai, are actually not related to any known language. Besides the place names, on the island there are still a few names of plants, animals and geological formations directly traceable to the ancient Nuragic era. Cicero called the Sardinian rebels "latrones mastrucati" ("thieves with rough wool cloaks") to emphasize Roman superiority.

During the long Roman domination Latin gradually become however the speech of the majority of the island's inhabitants. As a result of this process of romanization the Sardinian language is today classified as Romance or neo-Latin, with some phonetic features resembling Old Latin. Some linguists assert that modern Sardinian, being part of the Island Romance group, was the first language to split off from Latin, all others evolving from Latin as Continental Romance.

At that time, the only literature being produced in Sardinia was mostly in Latin: the native (Paleo-Sardinian) and non-native (Punic) pre-Roman languages were then already extinct (the last Punic inscription in Bithia, southern Sardinia, is from the second or third century A.D.). Some engraved poems in ancient Greek and Latin (the two most prestigious languages in the Roman Empire) are to be seen in Viper Cave, Cagliari, (" Grutta 'e sa Pibera " in Sardinian, "Grotta della Vipera" in Italian, "Cripta Serpentum" in Latin), a burial monument built by Lucius Cassius Philippus (a Roman who had been exiled to Sardinia) in remembrance of his dead spouse Atilia Pomptilla. We also have some religious works by Saint Lucifer and Eusebius, both from Caralis (Cagliari).

Although Sardinia was culturally influenced and politically ruled by the Byzantine Empire for almost five centuries, Greek did not enter the language except for some ritual or formal expressions in Sardinian using Greek structure and, sometimes, the Greek alphabet. Evidence for this is found in the "condaghes", the first written documents in Sardinian. From the long Byzantine era there are only a few entries but they already provide a glimpse of the sociolinguistical situation on the island in which, in addition to the community's everyday Neo-Latin language, Greek was also spoken by the ruling classes. Some toponyms, such as Jerzu (thought to derive from the Greek "khérsos", "untilled"), together with the personal names Mikhaleis, Konstantine and Basilis, demonstrate Greek influence.

As the Muslims conquered southern Italy and Sicily, communications broke down between Constantinople and Sardinia, whose districts became progressively more autonomous from the Byzantine oecumene (Greek: οἰκουμένη). Sardinia was then brought back into the Latin cultural sphere.

Sardinian was the first Romance language of all to gain official status, being used by the four Giudicati ("Judgedoms" or "Judicatures"), former quarrelling Byzantine districts that became independent political entities after the Arab expansion in the Mediterranean cut off any ties left between the island and Byzantium. One of the oldest documents left in Sardinian (the so-called "Carta Volgare") comes from the Judgedom of Cagliari and was issued by Torchitorio I de Lacon-Gunale in around 1070, employing the Greek alphabet. Old Sardinian had a greater number of archaisms and Latinisms than the present language does. While the earlier documents show the existence of an early Sardinian Koine, the language used by the various Judgedoms already displayed a certain range of dialectal variation. A special position was occupied by the Judgedom of Arborea, the last Sardinian kingdom to fall to foreign powers, in which a transitional dialect was spoken, that of Middle Sardinian. The Carta de Logu of the Kingdom of Arborea, one of the first constitutions in history drawn up in 1355–1376 by Marianus IV and the Queen, the "Lady Judge" ("judikessa" in Sardinian, "jutgessa" in Catalan, "giudicessa" in Italian) Eleanor, was written in this transitional variety of Sardinian, and remained in force until 1827. It is presumed the Arborean judges attempted to unify the Sardinian dialects in order to be legitimate rulers of the entire island under a single state ("republica sardisca" "Sardinian Republic").
Dante Alighieri wrote in his 1302–05 essay "De vulgari eloquentia" that Sardinians, not being Italians ("Latii") and having no "lingua vulgaris" of their own, resorted to aping Latin instead. Dante's view has been dismissed, because Sardinian had evolved enough to be unintelligible to non-islanders. A popular 12th-century verse from the poem "Domna, tant vos ai preiada" quotes the provençal troubadour Raimbaut de Vaqueiras, saying "No t'entend plui d'un Todesco / Sardesco o Barbarì" ("I don't understand you any more than I understand a German / or a Sardinian or a Berber"); the Tuscan poet Fazio degli Uberti refers to the Sardinians in his poem "Dittamondo" as "una gente che niuno non la intende / né essi sanno quel ch'altri pispiglia" ("a people that no one is able to understand / nor do they come to a knowledge of what other peoples say"). The Muslim geographer Muhammad al-Idrisi, who lived in Palermo, Sicily at the court of King Roger II, wrote in his work " Kitab Nuzhat al-mushtāq fi'khtirāq al-āfāq " ("The book of pleasant journeys into faraway lands" or, simply, "The book of Roger") that "Sardinia is large, mountainous, poorly provided with water, two hundred and eighty miles long and one hundred and eighty long from west to east. [...] Sardinians are ethnically "Rūm Afāriqah" (Latins of Africa), like the Berbers; they shun contacts with all the other "Rūm" nations and are people of purpose and valiant that never leave the arms" ("Wa ahl Ğazīrat Sardāniya fī aṣl Rūm Afāriqa mutabarbirūn mutawaḥḥišūn min ağnās ar-Rūm wa hum ahl nağida wa hazm lā yufariqūn as-silāḥ").
The literature of this period primarily consists of legal documents, besides the aforementioned Carta de Logu. The first document containing Sardinian elements is a 1063 donation to the abbey of Montecassino signed by Barisone I of Torres. Other documents are the "Carta Volgare" (1070–1080) in Campidanese, the 1080 Logudorese Privilege, the 1089 Donation of Torchitorio (in the Marseille archives), the 1190–1206 Marsellaise Chart (in Campidanese) and an 1173 communication between the Bishop Bernardo of Civita and Benedetto, who oversaw the Opera del Duomo in Pisa. The Statutes of Sassari (1316) and Castelgenovese (c. 1334) are written in Logudorese.

The 1297 feoffment of Sardinia by Pope Boniface VIII led to the creation of the Aragonese Kingdom of Sardinia and a long period of war between the Aragonese and Sardinians, ending with a Aragonese victory at Sanluri in 1409 and the renunciation of any succession right signed by William III of Narbonne in 1420. During this period the clergy adopted Catalan as their primary language, relegating Sardinian to a secondary status. According to attorney Sigismondo Arquer (Cagliari, 1530 – Toledo, 4 giugno 1571), author of "Sardiniae brevis historia et descriptio" in Sebastian Münster's Cosmographia Universalis, Sardinian prevailed in rural areas and Catalan was spoken in the cities, where the ruling class eventually became bilingual in both languages; Alghero is still a Catalan-speaking enclave on Sardinia to this day.

The long-lasting war and the so-called Black Death had a devastating effect on the island, depopulating large parts of it. People from the neighbouring island of Corsica began to settle in the northern Sardinian coast, leading to the birth of the Tuscan-sounding Sassarese and Gallurese.

Despite Catalan being widely spoken and written on the island at this time (leaving a lasting influence in Sardinian), there are some written records of Sardinian. One is the 15th-century "Sa Vitta et sa Morte, et Passione de sanctu Gavinu, Brothu et Ianuariu", written by Antòni Canu (1400–1476) and published in 1557: "Tando su rey Barbaru, su cane renegadu / de custa resposta multu restayt iradu / et issu martiriu fetit apparigiare / Itu su quale fetit fortemente ligare / sos sanctos martires cum bonas catenas / qui li segaant sos ossos cum sas veinas / et totu sas carnes cum petenes de linu... ".

The 16th century is instead marked by a new Sardinian literary revival: "Rimas Spirituales", by Hieronimu Araolla, was aimed at "glorifying and enriching Sardinian, our language" ("magnificare et arrichire sa limba nostra sarda") as Spanish, French and Italian poets had already done for their languages ("la Deffense et illustration de la langue françoyse" and "il Dialogo delle lingue"). Antonio Lo Frasso, a poet born in Alghero (a city he remembered fondly) who spent his life in Barcelona, wrote lyric poetry in Sardinian:
 ... "Non podende sufrire su tormentu / de su fogu ardente innamorosu. / Videndemi foras de sentimentu / et sensa una hora de riposu, / pensende istare liberu e contentu / m'agato pius aflitu e congoixosu, / in essermi de te senora apartadu, / mudende ateru quelu, ateru istadu ...".

Through the marriage of Isabella I of Castile and Ferdinand II of Aragon in 1469 and, later in 1624, the reorganization of the monarchy led by the Count-Duke of Olivares, Sardinia would progressively join a broad Spanish cultural sphere and leave the exclusive Aragonese one. Spanish was perceived as an elitist language, gaining solid ground among the ruling Sardinian class; Spanish had thus a profound influence on Sardinian, especially in those words, styles and cultural models owing to the prestigious international role of the Habsburg Monarchy as well as the Court. Most Sardinian authors would write in both Spanish and Sardinian until the 19th century and were well-versed in the former, like Vicente Bacallar y Sanna that was one of the founders of the Real Academia Española. A notable exception was Pedro Delitala (1550–1590), who decided to write in Italian instead. Nonetheless, the Sardinian language retained much of its importance, being the only one the people from rural areas kept speaking. In ""Legendariu de Santas Virgines, et Martires de Iesu Christu"", the Orgolese priest Ioan Matheu Garipa called Sardinian the closest living relative of classical Latin: "Las apo voltadas in sardu menjus qui non in atera limba pro amore de su vulgu [...] qui non tenjan bisonju de interprete pro bi-las decrarare, et tambene pro esser sa limba sarda tantu bona, quanta participat de sa latina, qui nexuna de quantas limbas si plàtican est tantu parente assa latina formale quantu sa sarda."

A 1620 proclamation is in the Bosa archives.

The War of the Spanish Succession gave Sardinia to Austria, whose sovereignty was confirmed by the 1713–14 treaties of Utrecht and Rastatt. In 1717 a Spanish fleet reoccupied Cagliari, and the following year Sardinia was ceded to Victor Amadeus II of Savoy in exchange for Sicily. This transfer would not initially entail any social nor linguistic changes, though: Sardinia would still retain for a long time its Hispanic character, so much so that only in 1767 were the Aragonese and Spanish dynastic symbols replaced by the Savoyard cross.

During the Savoyard period, a number of essays written by philologist Matteo Madau and professor (and senator) Giovanni Spano attempted to establish a unified orthography based on Logudorese, just like Florentine would become the basis for Italian. In 1811, Vincenzo Raimondo Porru published the first essay on the Southern Sardinian grammar and in 1832 the first Sardinian-Italian dictionary as well.

However, the Savoyard government imposed Italian on Sardinia in July 1760, for reasons related more to the Savoyard need of drawing the island away from the Spanish influence than for Italian nationalism, which would be later pursued by the King Charles Albert. Spanish was thus replaced as the official language and Sardinian faced again marginalization. The relationship between the newly imposed tongue and the native one had been perceived from the start by the locals, educated and uneducated alike, as a relationship (albeit unequal in terms of political power and prestige) between two very different languages, and not between a language and one of its dialects like in other regions; the plurisecular Iberian period contributed in making the Sardinians feel more detached from the Italian language and its cultural sphere, and the Spanish themselves, comprising both the Aragonese and Castilian ruling class, long considered already Sardinian to be a distinct language with respect to their own tongues and Italian as well. At the time, Italian was a foreign language to the Sardinians.

Carlo Baudi di Vesme (Cuneo, 1809 – Turin, 1877) claimed that the suppression of Sardinian and the imposition of Italian was desirable in order to make the islanders "civilized Italians". a basic primary education was thus offered exclusively through Italian, importing solely Italian-speaking teachers from the mainland, and Piedmontese cartographers replaced many Sardinian place names with Italian ones. Eventually, Sardinian came to be perceived as "sa limba de su famine" / "sa lingua de su famini", literally translating into English as "the language of hunger" (i.e. the language of the poor), and Sardinian parents strongly supported the teaching of the new tongue to their children, since they saw it as the portal to escaping from a poverty-stricken, rural, isolated and underprivileged life.

Despite the assimilation policy the anthem of the Savoyard Kingdom of Sardinia was "S'hymnu sardu nationale" ("the Sardinian National Anthem"), also known as "Cunservet Deus su Re" ("God save the King"), with Sardinian lyrics first in Campidanese and then Logudorese.

During the mobilization for World War I, the Italian Army compelled all Sardinians to enlist as Italian subjects and established the Sassari Infantry Brigade on 1 March 1915 at Tempio Pausania and Sinnai. Unlike the other infantry brigades of Italy, Sassari's conscripts were only Sardinians (including many officers). It is currently the only unit in Italy with an anthem in a language other than Italian: "Dimonios" ("Devils"), written in 1994 by Luciano Sechi. Its title derives from "Rote Teufel" (German for "red devils"). However, compulsory military service played a role in language shift.

Under Fascism, all languages other than Italian were banned, including Sardinia's improvised poetry competitions in Sardinian, and a large number of Sardinian surnames were changed to sound more Italian (e.g. "Lussu" becoming "Lusso", "Pilu" changing to "Pilo" and so on). This period saw the most aggressive cultural assimilation effort by the central government, which led to an even further sociolinguistic degradation of Sardinian. However, the Sardinian Hymn of the Piedmontese Kingdom was a chance to use a regional language without penalty; as a royal tradition, it could not be forbidden.

After World War II, awareness around the Sardinian language and the danger of its slipping away did not seem to concern the Sardinian elites and entered the political spaces much later than in other European peripheries marked by the long-standing presence of ethno-linguistic minorities; on the contrary, the language was rejected by the already Italianized middle class. At the time of drafting of the statute in 1948, the legislator eventually decided to specify the "Sardinian specialty" as a single criterion for political autonomy just on the grounds of a couple of socio-economic issues devoid of considerations of a distinct cultural, historical and geographical identity, that were on the contrary looked down upon as a potential prelude to more autonomist or separatist claims. Eventually, the special statute of 1948 did not recognize any special geographical conditions about the region nor made any mention of a distinct cultural and linguistic element, preferring instead to concentrate on state-funded plans (baptised with the Italian name of "piani di rinascita") for the heavy industrial development of the island. In the meantime, the emphasis on Italian-only assimilation policies continued, with historical sites and ordinary objects renamed in Italian. The Ministry of Public Education reportedly requested that the Sardinian teachers be put under surveillance. The rejection of the indigenous language, along with a rigid model of Italian-language education, corporal punishment and shaming, led to poor schooling for Sardinians. As of 2015, Sardinia had the highest rate of school and university drop-out in Italy.

There have been many campaigns, often expressed in the form of political demands from the late '60s onwards, to give Sardinian equal status with Italian as a means to promote cultural identity. One of the first demands was formulated in a resolution adopted by the University of Cagliari in 1971, calling upon the national and regional authorities to recognize the Sardinians as an ethno-linguistic minority and Sardinian as the island's co-official language. Critical acclaim in Sardinian cultural circles followed the patriotic poem "No sias isciau" ("Don't be a slave") by Raimondo ("Remundu") Piras some months before his death in 1977, urging bilingual education to reverse the trend of deSardization. Following tensions and claims of the Sardinian nationalist movement for concrete cultural and political autonomy, including the recognition of the Sardinians as an ethnic and linguistic minority, three separate bills were presented to the Regional Council in the '80s. A survey conducted in 1984 (cited in Pinna Catte's work, 1992) showed that many Sardinians had a relatively positive attitude towards bilingual education (22% wanted Sardinian to be compulsory in Sardinian schools, while 54.7% would prefer to see teaching in Sardinian as optional). Such consensus remains relatively stable to this day; a further survey in 2008 reported that more than half of the interviewees, 57.3%, are in favour of the introduction of Sardinian into schools alongside Italian.

In the 1990s, there has been a resurgence of Sardinian-language music, ranging from the more traditional genres ("cantu a tenore", "cantu a chiterra", "gosos" etc.) to rock ("Kenze Neke", "Askra", "Tzoku", "Tazenda" etc.) and even hip hop and rap ("Dr. Drer e CRC Posse", "Quilo", "Sa Razza", "Malam", "Menhir", "Stranos Elementos", "Malos Cantores", "Randagiu Sardu", "Futta" etc.), and with artists who used the language as a means to promote the island and address its long-standing issues and the new challenges. A few films (like "Su Re", "Bellas Mariposas", "Treulababbu", "Sonetaula" etc.) have also been dubbed in Sardinian, and some others (like Metropolis) were provided with subtitles in the language.

Eventually, sustained activism made possible the formal recognition of twelve minority languages (Sardinian, Albanian, Catalan, German, Greek, Slovenian, Croatian, French, Franco-Provençal, Friulian, Ladin and Occitan) in the late 1990s by the framework law no. 482/1999, following Art. 6 of the Italian Constitution. While the first section of said law states that Italian is the official language of the Republic, a number of provisions are included in order to normalize the use of such languages and let them become part of the national fabric. Nevertheless, many people in the country continue to regard Sardinian as an "Italian dialect", likewise many school and university books in Italy did not stop to group the language under "Linguistica italiana" (Italian linguistics), "Dialetti italiani" (Italian dialects) or "Dialettologia italiana" (Italian dialectology). As of 2018, Sardinian is not taught at school, apart from a few experimental cases; furthermore, its use has not ceased to be disincentivized as antiquated or even indicative of a lack of education, leading many locals to associate it with negative feelings of shame, backwardness, and provincialism.
Besides, a number of other factors like a considerable immigration flow from mainland Italy, the interior rural exodus to urban areas, where Sardinian is spoken by a much lower percentage of the population, and the use of Italian as a prerequisite for jobs and social advancement actually hinder any policy set up to promote the language. Therefore, UNESCO classifies Sardinian as "definitely endangered", since "children no longer learn the language as mother tongue in the home".

As of 2010, language use was far from stable: reports showed that, while an estimated 68 percent of the islanders had a good oral command of Sardinian, language ability among the children dropped to around 13 percent, if not even less; some linguists, like Mauro Maxia, cite the low number of Sardinian-speaking children as indicative of language decline, calling Sardinia "a case of linguistic suicide". According to the data published by ISTAT in 2006, 52.5% of the population in Sardinia speaks just Italian in the family environment, while 29.3% alternates Italian and Sardinian and only 16.6% uses Sardinian or other non-Italian languages; outside the social circle of family and friends, the numbers define Italian as the prevalent language (77,1%), while the usage of Sardinian and other languages drops to 5,2%. Today, most people who use Sardinian as part of day-to-day life reside mainly in the sparsely populated areas in the countryside, like the mountainous region of Barbagia.

A bill proposed by former prime minister Mario Monti's cabinet would have lowered Sardinian's protection level, distinguishing between languages protected by international agreements (German, Slovenian, French and Ladin) and indigenous languages. This bill, which was not implemented (Italy, along with France and Malta, has signed but not ratified the European Charter for Regional or Minority Languages), triggered a reaction on the island. Students have expressed an interest in taking all (or part) of their exit examinations in Sardinian.
In response to a 2013 Italian initiative to remove bilingual signs on the island, a group of Sardinians began a virtual campaign on Google Maps to replace Italian place names with the original Sardinian names. After about one month, Google changed the place names back to Italian. After a signature campaign, it has been made possible to change the language setting on Facebook from any language to Sardinian. It is also possible to switch to Sardinian even in Telegram and a couple of other apps, like Vivaldi, F-Droid, Diaspora, OsmAnd, Notepad++, Swiftkey, Stellarium, Skype, VLC media player for Android, Linux Mint Debina Edition 2 "Betsy", etc. In 2016, the first automatic translation software from Italian to Sardinian was developed.
In 2015, all the political parties in the Sardinian regional council have reached an agreement involving a series of amendments to the old 1997 law in order to introduce the optional teaching of the language in Sardinia's schools. The Unified Text on the Discipline of the Regional linguistic policy has been eventually approved in June 27, 2018, with the aim of setting in motion a path towards bilingual administration, contributions to bilingual mass media, publishing, IT schools and websites; it also allows for the foundation of a Sardinian board ("Consulta de su Sardu") with thirty experts that will propose a linguistic standard based on the main historical macrovarieties, and shall also have advisory duties towards the Regional body. Although there is still not an option to teach Sardinian on the island itself, let alone in Italy, some language courses are instead sometimes available in Germany (Universities of Stuttgart, Munich, Tübingen, Mannheim etc.), Spain (University of Girona), Iceland and Czech Republic (Brno university). Shigeaki Sugeta also taught Sardinian to his students of Romance languages at the Waseda University in Tokyo, Japan.

At present, the Sardinian-speaking community is the least protected one in Italy, despite being the largest minority language group officially recognized by the state. In fact the language, which is receding in all domains of use, is still not given access to any field of public life, such as education (Italian–Sardinian bilingualism is still frowned upon, while the local universities do not play pretty much any role whatsoever in supporting the language), politics (with the exception of some nationalist groups), justice, administrative authorities and public services, media, and cultural, ecclesiastical, economic and social activities, as well as facilities. According to a 2017 report on the digital language diversity in Europe, Sardinian appears to be particularly vital on social media as part of many people's everyday life for private use, but such vitality does not still translate into a strong and wide availability of Internet media for the language. In 2017, a 60-hour Sardinian language course has been introduced for the first time in Sardinia and Italy at the University of Cagliari, although such a course was already available in other universities abroad.

In 2015, the Council of Europe commented on the status of national minorities in Italy, noting the "à la carte" approach of the Italian state towards them with the exception of the German, French and Slovenian languages, where Italy has applied full bilingualism due to international agreements. Despite the formal recognition from the Italian state, Italy does not in fact collect any information on the ethnic and linguistic composition of the population, apart from South Tyrol. There is also virtually no print and broadcasting media exposure in politically or numerically weaker minorites like Sardinian. Moreover, the resources allocated to cultural projects like bilingual education, which lacks a consistent approach, are largely insufficient to meet "even the most basic expectations".

With cultural assimilation having already occurred, Sardinian is considered by many people on the island, both natives and from the Mainland, a low means of communication relegated to little more than highly localised levels of interaction. With a solution to the Sardinian question being unlikely to be found anytime soon, the language has become highly endangered: the late recognition as a minority language, as well as the gradual Italianization promoted by the education system, the administration system and the media, followed by the intergenerational language replacement, made it so that Sardinian's vitality has been heavily compromised. Most of the younger generation, although they do understand some Sardinian, is now in fact Italian monolingual and monocultural, speaking a Sardinian-influenced dialect of Italian that is often nicknamed "italiànu porcheddìnu" ("pig Italian", meaning more or less "broken Italian") by many native Sardinian speakers.

Whatever the fate of the declining Sardinian language might be, it shall form the substratum of the one prevailing now, Italian, in a number of linguistic components specific to the island.

All dialects of Sardinian have phonetic features that are relatively archaic compared to other Romance languages. The degree of archaism varies, with the dialect spoken in the Province of Nuoro being considered the most conservative. Medieval evidence indicates that the language spoken on Sardinia and Corsica at the time was similar to modern Nuorese Sardinian. The other dialects are thought to have evolved through Catalan, Spanish and later Italian influences.

The examples listed below are from the Logudorese dialect:

Sardinian contains the following phonetic innovations:

Although the latter two features were acquired during Spanish rule, the others indicate a deeper relationship between ancient Sardinia and the Iberian world; the retroflex "d", "l" and "r" are found in southern Italy, Tuscany and Asturias, and were probably involved in the palatalization process of the Latin clusters -"ll"-, "pl"-, "cl"- (-"ll"- > Spanish and Catalan -"ll"- , Gascon -"th" ; "cl"- > Galician-Portuguese "ch"- , Ital. "chi"- ).

According to Eduardo Blasco Ferrer, Sardinian has the following phonemes:

The five vowels , without length differentiation. There are also nasal vowels [ã], [ẽ], [ĩ], [õ], [ũ] in some varieties, and even nasal diphthongs when an intervocalic -n- is deleted like in "beni" [bẽj̃~bẽĩ].

There are three series of plosives or corresponding approximants:
In Cagliari and neighboring dialects, the soft has become (rhotacism): "digitus" > "didu" = "diru" (finger).

The double-voiced retroflex stop (written "dd") derives from the former retroflex lateral approximant .




Some permutations of "l" and "r" are seen; in most dialects, a preconsonant "l" (for example, "lt" or "lc") becomes "r": Latin "altum" > "artu", "marralzu" = "marrarzu", "rock".

In palatal context, Latin "l" changed into , , or , rather than the of Italian: "achizare" (Italian "accigliare"), "*volia" > "bòlla" = "bòlza" = "bòza", "wish" (Italian "vòglia"), "folia" > "fogia" = "folla" = "foza", "leaf" (Italian "foglia"), "filia" > "filla" = "fidza" = "fiza", "daughter" (Italian "figlia").

Some distinctive features typical of Sardinian are:

Historically, the Sardinian population has always been quite small and scattered across isolated cantons, sharing similar demographic patterns with the Corsican one. Starting from Francesco Cetti's description in the 18th century, the Sardinian language has been traditionally subdivided into two macro-varieties, each spoken by roughly half of the entire community: the North-Central or Logudorese dialects ("su sardu logudoresu"), and the South-Central or Campidanese dialects ("su sardu campidanesu"). All the Sardinian dialects differ primarily in phonetics, which does not hamper intelligibility. The Logudorese dialects are generally considered more conservative, with the Nuorese subdialect ("su sardu nugoresu") being the most conservative of all. They have all retained the classical Latin pronunciation of the stop velars ("kena" versus "cena", "supper"), the front middle vowels (compare Campidanese iotacism, probably from Byzantine Greek) and assimilation of close-mid vowels ("cane" versus "cani", "dog" and "gattos" versus "gattus", "cats"). Labio-velars become plain labials ("limba" versus "lingua", "language" and "abba" versus "acua", "water"). "I" is prosthesized before consonant clusters beginning in "s" ("iscala" versus Campidanese "scala", "stairway" and "iscola" versus "scola", "school"). An east-west strip of villages in central Sardinia speaks a transitional group of dialects ("su sardu de mesania"). Examples include "is limbas" (the languages) and "is abbas" (the waters). The Campidanese dialects are spoken in the southern half of Sardinia (including Cagliari, once the metropolis of the Roman province), and was more influenced by Carthage, Rome, Constantinople and Late Latin. Examples include "is fruminis" (the rivers) and "is domus" (the houses).

Sardinian is the indigenous and historical language of most Sardinian communities. However, Sardinian is not spoken as the native and primary language in a significant number of other ones, amounting to 20% of the Sardinian population. Two Sardinian–Corsican transitional languages (Gallurese and Sassarese) are spoken in the northernmost part of Sardinia, although some Sardinian is also understood by the majority of people living there (73,6% in Gallura and 67,8% in the Sassarese-speaking subregion). Sassari, the second-largest city on Sardinia and the main center of the northern half of the island ("cabu de susu" in Sardinian, "capo di sopra" in Italian), is located there. There are also two language islands, the Catalan Algherese-speaking community from the inner city of Alghero (northwest Sardinia) and the Ligurian-speaking towns of Carloforte, in San Pietro Island, and Calasetta in Sant'Antioco island (south-west Sardinia).




</doc>
<doc id="27035" url="https://en.wikipedia.org/wiki?curid=27035" title="Shot reverse shot">
Shot reverse shot

Shot reverse shot (or shot/countershot) is a film technique where one character is shown looking at another character (often off-screen), and then the other character is shown looking back at the first character. Since the characters are shown facing in opposite directions, the viewer assumes that they are looking at each other.

Shot reverse shot is a feature of the "classical" Hollywood style of continuity editing, which deemphasizes transitions between shots such that the spectator perceives one continuous action that develops linearly, chronologically, and logically. It is an example of an eyeline match.


</doc>
<doc id="27036" url="https://en.wikipedia.org/wiki?curid=27036" title="Stop motion">
Stop motion

Stop motion is an animated-film making technique in which objects are physically manipulated in small increments between individually photographed frames so that they appear to exhibit independent motion when the series of frames is played back as a fast sequence. Dolls with movable joints or clay figures are often used in stop motion for their ease of repositioning. Stop-motion animation using plasticine figures is called clay animation or "clay-mation". Not all stop motion, however, requires figures or models: stop-motion films can also be made using humans, household appliances, and other objects, usually for comedic effect. Stop motion can also use sequential drawing in a similar manner to traditional animation, such as a flip book. Stop motion using humans is sometimes referred to as pixilation or pixilate animation.

The term "stop motion", relating to the animation technique, is often spelled with a hyphen, "stop-motion". Both orthographical variants, with and without the hyphen, are correct, but the hyphenated one has, in addition, a second meaning, not related to animation or cinema: "a device for automatically stopping a machine or engine when something has gone wrong" ("The New Shorter Oxford English Dictionary", 1993 edition).

Stop motion should not be confused with the time-lapse technique in which still photographs of a live scene are taken at regular intervals and then combined to make a continuous film. Time lapse is a technique whereby the frequency at which film frames are captured is much lower than that used to view the sequence. When played at normal speed, time appears to be moving faster.

Stop-motion animation has a long history in film. It was often used to show objects moving as if by magic, but really by animation. The first instance of the stop-motion technique can be credited to Albert E. Smith and J. Stuart Blackton for Vitagraph's "The Humpty Dumpty Circus" (1897), in which a toy circus of acrobats and animals comes to life. In 1902, the film "Fun in a Bakery Shop" used the stop trick technique in the "lightning sculpting" sequence. French trick film maestro Georges Méliès used stop-motion animation once to produce moving title-card letters in one of his short films, and a number of his special effects are based on stop-motion photography. In 1907, "The Haunted Hotel" is a new stop-motion film by J. Stuart Blackton, and was a resounding success when released. Segundo de Chomón (1871–1929), from Spain, released "El Hotel Eléctrico" later that same year, and used similar techniques as the Blackton film. In 1908, "A Sculptor's Welsh Rarebit Nightmare" was released, as was "The Sculptor's Nightmare", a film by Billy Bitzer. Italian animator Roméo Bossetti impressed audiences with his object animation tour-de-force, "The Automatic Moving Company" in 1912. The great European pioneer of stop motion was Wladyslaw Starewicz (1892–1965), who animated "The Beautiful Lukanida" (1910), "The Battle of the Stag Beetles" (1910), and "The Ant and the Grasshopper" (1911).

One of the earliest clay animation films was "Modelling Extraordinary", which impressed audiences in 1912. December 1916 brought the first of Willie Hopkins' 54 episodes of "Miracles in Mud" to the big screen. Also in December 1916, the first woman animator, Helena Smith Dayton, began experimenting with clay stop motion. She would release her first film in 1917, an adaptation of William Shakespeare's "Romeo and Juliet".

In the turn of the century, there was another well known animator known as Willis O' Brien (known by others as O'bie). His work on "The Lost World" (1925) is well known, but he is most admired for his work on "King Kong" (1933), a milestone of his films made possible by stop-motion animation.

O'Brien's protege and eventual successor in Hollywood was Ray Harryhausen. After learning under O'Brien on the film "Mighty Joe Young" (1949), Harryhausen would go on to create the effects for a string of successful and memorable films over the next three decades. These included "The Beast from 20,000 Fathoms" (1953), "It Came from Beneath the Sea" (1955), "Jason and the Argonauts" (1963), "The Golden Voyage of Sinbad" (1973) and "Clash of the Titans" (1981).

In a 1940 promotional film, Autolite, an automotive parts supplier, featured stop-motion animation of its products marching past Autolite factories to the tune of Franz Schubert's "Military March". An abbreviated version of this sequence was later used in television ads for Autolite, especially those on the 1950s CBS program "Suspense", which Autolite sponsored.

In the 1960s and 1970s, independent clay animator Eliot Noyes Jr. refined the technique of "free-form" clay animation with his Oscar-nominated 1965 film "Clay (or the Origin of Species)". Noyes also used stop motion to animate sand lying on glass for his musical animated film "Sandman" (1975).

Stop motion was used by Rankin/Bass Productions on some of their television programs and feature films including "The New Adventures of Pinocchio" (1960–1961), "Willy McBean and his Magic Machine" (1963, 1965) and most notably seasonal/holiday favorites like "Rudolph the Red-Nosed Reindeer" (1964), "Mad Monster Party?" (1966, 1967), "The Little Drummer Boy" (1968), "Santa Claus is Comin' to Town" (1970) and "Here Comes Peter Cottontail" (1971). Under the name of "Animagic", the stop-motion works of Rankin/Bass were supervised by Tadahito Mochinaga at his MOM Production in Tokyo, Japan.

In 1975, filmmaker and clay animation experimenter Will Vinton joined with sculptor Bob Gardiner to create an experimental film called "Closed Mondays" which became the world's first stop-motion film to win an Oscar. Will Vinton followed with several other successful short film experiments including "The Great Cognito", "Creation", and "Rip Van Winkle" which were each nominated for Academy Awards. In 1977, Vinton made a documentary about this process and his style of animation which he dubbed "claymation"; he titled the documentary "Claymation". Soon after this documentary, the term was trademarked by Vinton to differentiate his team's work from others who had been, or were beginning to do, "clay animation". While the word has stuck and is often used to describe clay animation and stop motion, it remains a trademark owned currently by Laika Entertainment, Inc. Twenty clay-animation episodes featuring the clown Mr. Bill were a feature of "Saturday Night Live", starting from a first appearance in February 1976.

At very much the same time in the UK, Peter Lord and David Sproxton formed Aardman Animations. In 1976 they created the character Morph who appeared as an animated side-kick to the TV presenter Tony Hart on his BBC TV programme Take Hart. The five-inch-high presenter was made from a traditional British modelling clay called Plasticine. In 1977 they started on a series of animated films, again using modelling clay, but this time made for a more adult audience. The soundtrack for Down and Out was recorded in a Salvation Army Hostel and Plasticine puppets were animated to dramatise the dialogue. A second film, also for the BBC followed in 1978. A TV series The Amazing Adventures of Morph was aired in 1980.

Sand-coated puppet animation was used in the Oscar-winning 1977 film "The Sand Castle", produced by Dutch-Canadian animator Co Hoedeman. Hoedeman was one of dozens of animators sheltered by the National Film Board of Canada, a Canadian government film arts agency that had supported animators for decades. A pioneer of refined multiple stop-motion films under the NFB banner was Norman McLaren, who brought in many other animators to create their own creatively controlled films. Notable among these are the pinscreen animation films of Jacques Drouin, made with the original pinscreen donated by Alexandre Alexeieff and Claire Parker.

Italian stop-motion films include "Quaq Quao" (1978), by Francesco Misseri, which was stop motion with origami, "The Red and the Blue" and the clay animation kittens "Mio and Mao". Other European productions included a stop-motion-animated series of Tove Jansson's "The Moomins" (from 1979, often referred to as "The Fuzzy Felt Moomins"), produced by Film Polski and Jupiter Films.

One of the main British Animation teams, John Hardwick and Bob Bura, were the main animators in many early British TV shows, and are famous for their work on the "Trumptonshire" trilogy.

Disney experimented with several stop-motion techniques by hiring independent animator-director Mike Jittlov to make the first stop-motion animation of Mickey Mouse toys ever produced, in a short sequence called "Mouse Mania", part of a TV special, "Mickey's 50", which commemorated Mickey's 50th anniversary in 1978. Jittlov again produced some impressive multi-technique stop-motion animation a year later for a 1979 Disney special promoting their release of the feature film "The Black Hole". Titled "Major Effects", Jittlov's work stood out as the best part of the special. Jittlov released his footage the following year to 16mm film collectors as a short film titled "The Wizard of Speed and Time", along with four of his other short multi-technique animated films, most of which eventually evolved into his own feature-length film of the same title. Effectively demonstrating almost all animation techniques, as well as how he produced them, the film was released to theaters in 1987 and to video in 1989.

In the 1970s and 1980s, Industrial Light & Magic often used stop-motion model animation in such films as the original "Star Wars" trilogy: the chess sequence in "Star Wars", the Tauntauns and AT-AT walkers in "The Empire Strikes Back", and the AT-ST walkers in "Return of the Jedi" were all filmed using stop-motion animation, some of it using the Go films. The many shots including the ghosts in "Raiders of the Lost Ark" and the first two feature films in the "RoboCop" series use Phil Tippett's go motion version of stop motion.

In the UK, Aardman Animations continued to grow. Channel 4 funded a new series of clay animated films, "Conversation Pieces", using recorded soundtracks of real people talking. A further series in 1986, called "Lip Sync", premiered the work of Richard Goleszowski ("Ident"), Barry Purves ("Next"), and Nick Park ("Creature Comforts"), as well as further films by Sproxton and Lord. "Creature Comforts" won the Oscar for Best Animated Short in 1990.

In 1980, Marc Paul Chinoy directed the 1st feature-length clay animated film, based on the famous "Pogo" comic strip. Titled "I go Pogo". It was aired a few times on American cable channels but has yet to be commercially released. Primarily clay, some characters required armatures, and walk cycles used pre-sculpted hard bases legs. 

Stop motion was also used for some shots of the final sequence of "Terminator" movie, also for the scenes of the small alien ships in Spielberg's "Batteries Not Included" in 1987, animated by David W. Allen. Allen's stop-motion work can also be seen in such feature films as "The Crater Lake Monster" (1977), "Q - The Winged Serpent" (1982), "The Gate" (1986) and "Freaked" (1993). Allen's King Kong Volkswagen commercial from the 1970s is now legendary among model animation enthusiasts.

In 1985, Will Vinton and his team released an ambitious feature film in stop motion called "The Adventures Of Mark Twain" based on the life and works of the famous American author. While the film may have been a little sophisticated for young audiences at the time, it got rave reviews from critics and adults in general. Vinton's team also created the Nomes and the Nome King for Disney's "Return to Oz" feature, for which they received an Academy Award Nomination for Special Visual Effects. In the 80's and early 90's, Will Vinton became very well known for his commercial work as well with stop-motion campaigns including The California Raisins.

Of note are the films of Czech filmmaker Jan Švankmajer, which mix stop motion and live actors. These include "Alice", an adaptation of Lewis Carroll's "Alice's Adventures in Wonderland", and "Faust", a rendition of the legend of the German scholar. The Czech school is also illustrated by the series "Pat & Mat" (1979–present). Created by Lubomír Beneš and Vladimír Jiránek, and it was wildly popular in a number of countries.

Since the general animation renaissance headlined by the likes of "Who Framed Roger Rabbit" and "The Little Mermaid" at the end of the 1980s and the beginning of the 1990s, there have been an increasing number of traditional stop-motion feature films, despite advancements with computer animation. "The Nightmare Before Christmas", directed by Henry Selick and produced by Tim Burton, was one of the more widely released stop-motion features and become the highest grossing stop-motion animated movie of its time, grossing over $50 million domestic. Henry Selick also went on to direct "James and the Giant Peach" and "Coraline", and Tim Burton went on to direct "Corpse Bride" and "Frankenweenie".

In 1999, Will Vinton launched the first prime-time stop-motion television series called "The PJs", co-created by actor-comedian Eddie Murphy. The Emmy-winning sitcom aired on Fox for two seasons, then moved to the WB for an additional season. Vinton launched another series, "Gary & Mike", for UPN in 2001.

Another individual who found fame in clay animation is Nick Park, who created the characters Wallace and Gromit. In addition to a series of award-winning shorts and featurettes, he won the Academy Award for Best Animated Feature for the feature-length outing "". "Chicken Run", to date, is the highest grossing stop motion animated movie ever grossing nearly $225 million worldwide.

The BBC commissioned thirteen episodes of stop frame animated "Summerton Mill" in 2004 as inserts into their flagship pre-school program, "Tikkabilla". Created and produced by Pete Bryden and Ed Cookson, the series was then given its own slot on BBC1 and BBC2 and has been broadcast extensively around the world.

Other notable stop-motion feature films released since 1990 include "The Secret Adventures of Tom Thumb" (1993), "Fantastic Mr. Fox" and "$9.99", both released in 2009, and "Anomalisa" (2015).

Cutout animation is a variant of stop-motion animation that utilises flat materials such as paper, fabrics and photographs in its production, producing a 2D animation as a result. Prominent examples of cutout animation include the early episodes of "South Park", and the "Charley Says" series of British public information films.

Stop motion has very rarely been shot in stereoscopic 3D throughout film history. The first 3D stop-motion short was "In Tune With Tomorrow" (also known as "Motor Rhythm"), made in 1939 by John Norling. The second stereoscopic stop-motion release was "The Adventures of Sam Space" in 1955 by Paul Sprunck. The third and latest stop motion short in stereo 3D was "The Incredible Invasion of the 20,000 Giant Robots from Outer Space" in 2000 by Elmer Kaan and Alexander Lentjes. This is also the first ever 3D stereoscopic stop motion and CGI short in the history of film. The first all stop-motion 3D feature is "Coraline" (2009), based on Neil Gaiman's best-selling novel and directed by Henry Selick.
Another recent example is the Nintendo 3DS video software which comes with the option for Stop Motion videos. This has been released December 8, 2011 as a 3DS system update. Also, the movie ParaNorman is in 3D stop motion.

Another more complicated variation on stop motion is go motion, co-developed by Phil Tippett and first used on the films "The Empire Strikes Back" (1980), "Dragonslayer" (1981), and the "RoboCop" films. Go motion involved programming a computer to move parts of a model slightly during each exposure of each frame of film, combined with traditional hand manipulation of the model in between frames, to produce a more realistic motion blurring effect. Tippett also used the process extensively in his 1984 short film "Prehistoric Beast", a 10 minutes long sequence depicting a herbivorous dinosaur ("Monoclonius"), being chased by a carnivorous one ("Tyrannosaurus"). With new footage "Prehistoric Beast" became "Dinosaur!" in 1985, a full-length dinosaurs documentary hosted by Christopher Reeve. Those Phil Tippett's go motion tests acted as motion models for his first photo-realistic use of computers to depict dinosaurs in "Jurassic Park" in 1993. A low-tech, manual version of this blurring technique was originally pioneered by Wladyslaw Starewicz in the silent era, and was used in his feature film "The Tale of the Fox" (1931).

Reasons for using stop motion instead of the more advanced computer-generated imagery (CGI) include the low entry price and the appeal of its distinct look. It is now mostly used in children's programming, in commercials and some comic shows such as "Robot Chicken". Another merit of stop motion is that it legitimately displays actual real-life textures, as CGI texturing is more artificial, therefore not quite as close to realism. This is appreciated by a number of animation directors, such as Tim Burton, Henry Selick, Wes Anderson, and Travis Knight.

Dominating children's TV stop-motion programming for three decades in America was Art Clokey's "Gumby" series (1955–1989) and its feature film, "" (1992, 1995), using both freeform and character clay animation. Clokey started his adventures in clay with a 1953 freeform clay short film called "Gumbasia" (1953) which shortly thereafter propelled him into his more structured "Gumby" TV series. In partnership with the United Lutheran Church in America, he also produces "Davey and Goliath" (1960–2004).

In November 1959, the first episode of "Sandmännchen" was shown on East German television, a children's show that had Cold War propaganda as its primary function. New episodes, minus any propaganda, are still being produced in the now-reunified Germany, making it one of the longest running animated series in the world.

In the 1960s, the French animator Serge Danot created the well-known "The Magic Roundabout" (1965) which played for many years on the BBC. Another French/Polish stop-motion animated series was "Colargol" ("Barnaby the Bear" in the UK, "Jeremy" in Canada), by Olga Pouchine and Tadeusz Wilkosz.

A British TV series, "Clangers" (1969), became popular on television. The British artists Brian Cosgrove and Mark Hall (Cosgrove Hall Films) produced the two stop-motion animated adaptions of Enid Blyton's "Noddy" book series including the original series of the same name (1975–1982) and "Noddy's Toyland Adventures" (1992–2001), a full-length film "The Wind in the Willows" (1983) and later a multi-season TV series, both based on Kenneth Grahame's classic children's book of the same title. They also produced a documentary of their production techniques, "Making Frog and Toad". Since the 1970s and continuing into the 21st century, Aardman Animations, a British studio, has produced short films, television series, commercials and feature films, starring plasticine characters such as Wallace and Gromit; they also produced a notable music video for "Sledgehammer", a song by Peter Gabriel.

During 1986 to 1991, Churchill Films produced "The Mouse and the Motorcycle", "Runaway Ralph", and "Ralph S. Mouse" for ABC television. The shows featured stop-motion characters combined with live action, based on the books of Beverly Cleary. John Clark Matthews was animation director, with Justin Kohn, Joel Fletcher, and Gail Van Der Merwe providing character animation.

From 1986 to 2000, over 150 five-minute episodes of "Pingu", a Swiss children's comedy were produced by Trickfilmstudio. In the 1990s Trey Parker and Matt Stone made two shorts and the pilot of "South Park" almost entirely out of construction paper.

In 1999, Tsuneo Gōda directed an official 30-second sketches of the character Domo. With the shorts animated by stop-motion studio dwarf is still currently produced in Japan and has then received universal critical acclaim from fans and critics. Gōda also directed the stop-motion movie series "Komaneko" in 2004.

In 2003, the pilot film for the series "Curucuru and Friends", produced by Korean studio Ffango Entertoyment is greenlighted into a children's animated series in 2004 after an approval with the Gyeonggi Digital Contents Agency. It was aired in KBS1 on November 24, 2006 and won the 13th Korean Animation Awards in 2007 for Best Animation. Ffango Entertoyment also worked with Frontier Works in Japan to produce the 2010 film remake of "Cheburashka".

Since 2005, "Robot Chicken" has mostly utilized stop-motion animation, using custom made action figures and other toys as principal characters.

Since 2009 Laika, the stop-motion successor to Will Vinton Studios, has released four feature films, which have collectively grossed over $400 million.

Many young people begin their experiments in movie making with stop motion, thanks to the ease of modern stop-motion software and online video publishing. Many new stop-motion shorts use clay animation into a new form.

Singer-songwriter Oren Lavie's music video for the song Her Morning Elegance was posted on YouTube on January 19, 2009. The video, directed by Lavie and Yuval and Merav Nathan, uses stop motion and has achieved great success with over 25.4 million views, also earning a 2010 Grammy Award nomination for "Best Short Form Music Video".

Stop motion has occasionally been used to create the characters for computer games, as an alternative to CGI. The Virgin Interactive Entertainment Mythos game Magic and Mayhem (1998) featured creatures built by stop-motion specialist Alan Friswell, who made the miniature figures from modelling clay and latex rubber, over armatures of wire and ball-and-socket joints. The models were then animated one frame at a time, and incorporated into the CGI elements of the game through digital photography. "ClayFighter" for the Super NES and The Neverhood for the PC are other examples.

Scientists at IBM used a scanning tunneling microscope to single out and move individual atoms which were used to make characters in "A Boy and His Atom". This was the tiniest scale stop-motion video made at that time.





</doc>
<doc id="27037" url="https://en.wikipedia.org/wiki?curid=27037" title="Screwball comedy film">
Screwball comedy film

Screwball comedy is a genre of comedy film that became popular during the Great Depression, originating in the early 1930s and thriving until the early 1940s. Many secondary characteristics of this genre are similar to film noir, but it distinguishes itself for being characterized by a female that dominates the relationship with the male central character, whose masculinity is challenged. The two engage in a humorous battle of the sexes, which was a new theme for Hollywood and audiences at the time. Other elements are fast-paced repartee, farcical situations, escapist themes, and plot lines involving courtship and marriage. Screwball comedies often depict social classes in conflict, as in "It Happened One Night" (1934) and "My Man Godfrey" (1936). Some comic plays are also described as screwball comedies.

Screwball comedy has proven to be one of the most popular and enduring film genres. "It Happened One Night" (1934), is often cited as being the first true screwball, though "Bombshell" starring Jean Harlow preceded it by a year. Although many film scholars agree that its classic period had effectively ended by 1942, elements of the genre have persisted or have been paid homage to in contemporary film.

During the Great Depression, there was a general demand for films with a strong social class critique and hopeful, escapist-oriented themes. The screwball format arose largely as a result of the major film studios' desire to avoid censorship by the increasingly enforced Hays Code. In order to incorporate prohibited risqué elements into their plots, filmmakers resorted to handling these elements covertly. Verbal sparring between the sexes served as a stand-in for physical, sexual tension.

The screwball comedy has close links with the theatrical genre of farce, and some comic plays are also described as screwball comedies. Many elements of the screwball genre can be traced back to such stage plays as "Lysistrata" by Aristophanes, William Shakespeare's "Much Ado About Nothing", "As You Like It" and "A Midsummer Night's Dream" and Oscar Wilde's "The Importance of Being Earnest". Other genres with which screwball comedy is associated include slapstick, situation comedy, romantic comedy and bedroom farce.

Films definitive of the genre usually feature farcical situations, a combination of slapstick with fast-paced repartee and show the struggle between economic classes. They also generally feature a self-confident and often stubborn central female protagonist and a plot involving courtship and marriage or remarriage. These traits can be seen in both "It Happened One Night" and "My Man Godfrey" (1936). The film critic Andrew Sarris has defined the screwball comedy as "a sex comedy without the sex."

Like farce, screwball comedies often involve mistaken identities or other circumstances in which a character or characters try to keep some important fact a secret. Sometimes screwball comedies feature male characters cross-dressing, further contributing to the misunderstandings ("Bringing Up Baby" (1938) "I Was a Male War Bride" (1949), and "Some Like It Hot" (1959)). They also involve a central romantic story, usually in which the couple seem mismatched and even hostile to each other at first, but eventually overcome their differences in an amusing or entertaining way that leads to romance. Often this mismatch comes about because the man is much further down the economic scale than the woman ("Bringing Up Baby", "Holiday", both 1938). The final romantic union is often planned by the woman from the outset, while the man does not know about her intention at all. "Bringing Up Baby" contains a rare statement on that, when the woman says to a third party: "He's the man I'm going to marry. He doesn't know it, but I am."
These pictures also offered a kind of cultural escape valve: a safe battleground on which to explore serious issues such as class under a comedic (and non-threatening) framework. Class issues are a strong component of screwball comedies: the upper class tend to be shown as idle and pampered and having difficulty coping with the real world. The most famous example is "It Happened One Night"; some critics believe that this portrayal of the upper class was brought about by the Great Depression, and the financially struggling moviegoing public's desire to see the rich upper class taught a lesson in humanity. (See also "My Man Godfrey", 1936.) By contrast, when lower-class people attempt to pass themselves off as upper-class, they are able to do so with relative ease ("The Lady Eve", 1941).

Another common element is fast-talking, witty repartee ("You Can't Take It With You" (1937) and "His Girl Friday" (1940)). This stylistic device did not originate in the genre (although it may be argued to have reached its zenith there): it can also be found in many of the old Hollywood cycles, including gangster films, and romantic comedies.

Screwball comedies also tend to contain ridiculous, farcical situations, such as in "Bringing Up Baby", in which a couple must take care of a pet leopard during much of the film. Slapstick elements are also frequently present, such as the numerous pratfalls Henry Fonda takes in "The Lady Eve" (1941).

One subgenre of screwball is known as the comedy of remarriage, in which characters divorce and then remarry one another ("The Awful Truth" (1937), "The Philadelphia Story" (1940)). Some scholars point to this frequent device as evidence of the shift in the American moral code, as it showed freer attitudes toward divorce (though the divorce always turns out to have been a mistake).

Another subgenre of screwball comedy has the woman chasing a man who is oblivious to or disinterested in her. Examples include Barbara Stanwyck chasing Henry Fonda ("The Lady Eve" (1941), Marion Davies chasing Antonio Moreno ("The Cardboard Lover" (1928), Marion Davies chasing Bing Crosby ("Going Hollywood" (1933), and Carole Lombard chasing William Powell ("My Man Godfrey" (1936).

The philosopher Stanley Cavell has noted that many classic screwball comedies turn on an interlude in the state of Connecticut ("Bringing Up Baby", "The Lady Eve", "The Awful Truth"). In "Christmas in Connecticut" (1945), the action moves to Connecticut and remains there for the duration of the film.


Other films from this period in other genres incorporate elements of the screwball comedy. For example, Alfred Hitchcock's thriller "The 39 Steps" (1935) features the gimmick of a young couple who find themselves handcuffed together and who eventually, almost in spite of themselves, fall in love with one another, and Woody Van Dyke's detective comedy "The Thin Man" (1934), which portrays a witty, urbane couple who trade barbs as they solve mysteries together. Many of the Fred Astaire and Ginger Rogers musicals of the 1930s also feature screwball comedy plots, notably "The Gay Divorcee" (1934) and "Top Hat" (1935). The Eddie Cantor musicals "Whoopee!" (1930) and "Roman Scandals" (1933), and slapstick road movies such as "Six of a Kind" (1934) include screwball elements. Some of the Joe E Brown comedies also fall into this category, particularly "Broadminded" (1931) and "Earthworm Tractors" (1936).

Actors and actresses frequently featured in or associated with screwball comedy include:

Some notable directors of screwball comedies include:

Various later films are considered by some critics to have revived elements of the classic era screwball comedies, including:

Elements of classic screwball comedy often found in more recent films which might otherwise simply be classified as romantic comedies include the "battle of the sexes" ("Down with Love", "How to Lose a Guy in 10 Days"), witty repartee ("Down with Love"), and the contrast between the wealthy and the middle class ("You've Got Mail", "Two Weeks Notice"). Many of Elvis Presley's films from the 1960s had drawn, consciously or unconsciously, the many characteristics of the screwball comedy genre. Some examples are "Double Trouble", "Tickle Me", "Girl Happy" and "Live A Little, Love A Little". Modern updates on screwball comedy also sometimes are categorized as black comedy ("Intolerable Cruelty", which also features a twist on the classic screwball element of divorce and remarriage). The Coen Brothers often include screwball elements in a film which may not otherwise be considered screwball or even a comedy.

The Golmaal movies, a series of Hindi-language Indian films, have been described as a screwball comedy franchise.

In his 2008 production of the classic Beaumarchais comedy "The Marriage of Figaro", author William James Royce trimmed the five-act play down to three acts and labeled it a "classic screwball comedy." The playwright made Suzanne the central character, endowing her with all the feisty comedic strengths of her classic film counterparts. In his adaptation, entitled "One Mad Day!" (a play on Beaumarchais' original French title) Royce underscored all of the elements of the classic screwball comedy, suggesting that Beaumarchais may have had a hand in the origins of the genre.

The television series "Moonlighting" (1985–1989), "Married... with Children" (1987–1997), "NewsRadio" (1995–1999), "Gilmore Girls" (2000–2007), "The O.C." (2003-2007), "Standoff" (2006–2007), and "Gossip Girl" (2007-2012) have also adapted elements of the screwball comedy genre for the small screen.

The second part of the 1978 film "Superman", set in fictional Metropolis, takes on a screwball tone after the seriousness of the original story.

"The Adventures of Tintin" comic "The Castafiore Emerald" contains settings, plots, comic devices, and character types that share many similarities to screwball comedies.

The plot of "Corrupting Dr. Nice", a science fiction novel by John Kessel involving time travel, is modeled on films such as "The Lady Eve" and "Bringing Up Baby".





</doc>
<doc id="27038" url="https://en.wikipedia.org/wiki?curid=27038" title="Lists of science fiction films">
Lists of science fiction films

This is a list of science fiction films organized chronologically. These films have been released to a cinema audience by the commercial film industry and are widely distributed with reviews by reputable
critics. (The exception are the films on the made-for-TV list, which are normally not released to a cinema audience.) This includes silent film–era releases, serial films, and feature-length films. All of the films include core elements of science fiction, but can cross into other genres such as drama, mystery, action, horror, fantasy, and comedy.

Among the listed movies are films that have won motion-picture and science fiction awards as well as films that have been listed among the worst movies ever made, or have won one or more Golden Raspberry Awards. Critically distinguished films are indicated by footnotes in the listings.

Subgenre lists

Related films

Related lists

Film ratings



</doc>
<doc id="27040" url="https://en.wikipedia.org/wiki?curid=27040" title="Schutzstaffel">
Schutzstaffel

The Schutzstaffel (SS; also stylized as with Armanen runes; ; literally "Protection Squadron") was a major paramilitary organization under Adolf Hitler and the Nazi Party (NSDAP) in Nazi Germany, and later throughout German-occupied Europe during World War II. It began with a small guard unit known as the "Saal-Schutz" ("Hall Security") made up of NSDAP volunteers to provide security for party meetings in Munich. In 1925 Heinrich Himmler joined the unit, which had by then been reformed and given its final name. Under his direction (1929–45) it grew from a small paramilitary formation to one of the most powerful organizations in Nazi Germany. From 1929 until the regime's collapse in 1945, the SS was the foremost agency of security, surveillance, and terror within Germany and German-occupied Europe.

The two main constituent groups were the "Allgemeine SS" (General SS) and Waffen-SS (Armed SS). The "Allgemeine SS" was responsible for enforcing the racial policy of Nazi Germany and general policing, whereas the Waffen-SS consisted of combat units within Nazi Germany's military. A third component of the SS, the "SS-Totenkopfverbände" (SS-TV), ran the concentration camps and extermination camps. Additional subdivisions of the SS included the Gestapo and the "Sicherheitsdienst" (SD) organizations. They were tasked with the detection of actual or potential enemies of the Nazi state, the neutralization of any opposition, policing the German people for their commitment to Nazi ideology, and providing domestic and foreign intelligence.

The SS was the organization most responsible for the genocidal killing of an estimated 5.5 to 6 million Jews and millions of other victims in the Holocaust. Members of all of its branches committed war crimes and crimes against humanity during World War II (1939–45). The SS was also involved in commercial enterprises and exploited concentration camp inmates as slave labor. After Nazi Germany's defeat, the SS and the NSDAP were judged by the International Military Tribunal at Nuremberg to be criminal organizations. Ernst Kaltenbrunner, the highest-ranking surviving SS main department chief, was found guilty of crimes against humanity at the Nuremberg trials and hanged in 1946.

By 1923, the Nazi Party (NSDAP) led by Adolf Hitler had created a small volunteer guard unit known as the "Saal-Schutz" (Hall Security) to provide security at their meetings in Munich. The same year, Hitler ordered the formation of a small bodyguard unit dedicated to his personal service. He wished it to be separate from the "suspect mass" of the party, including the paramilitary "Sturmabteilung" ("Storm Battalion"; SA), which he did not trust. The new formation was designated the "Stabswache" (Staff Guard). Originally the unit was composed of eight men, commanded by Julius Schreck and Joseph Berchtold, and was modeled after the Erhardt Naval Brigade, a "Freikorps" of the time. The unit was renamed "Stoßtrupp" (Shock Troops) in May 1923.

The "Stoßtrupp" was abolished after the failed 1923 Beer Hall Putsch, an attempt by the NSDAP to seize power in Munich. In 1925, Hitler ordered Schreck to organize a new bodyguard unit, the "Schutzkommando" (Protection Command). It was tasked with providing personal protection for Hitler at NSDAP functions and events. That same year, the "Schutzkommando" was expanded to a national organization and renamed successively the "Sturmstaffel" (Storm Squadron), and finally the "Schutzstaffel" (Protection Squad; SS). Officially, the SS marked its foundation on 9 November 1925 (the second anniversary of the Beer Hall Putsch). The new SS was to provide protection for NSDAP leaders throughout Germany. Hitler's personal SS protection unit was later enlarged to include combat units.

Schreck, a founding member of the SA and a close confidant of Hitler, became the first SS chief in March 1925. On 15 April 1926, Joseph Berchtold succeeded him as chief of the SS. Berchtold changed the title of the office to "Reichsführer-SS" (Reich Leader-SS). Berchtold was considered more dynamic than his predecessor, but became increasingly frustrated by the authority the SA had over the SS. This led to him transferring leadership of the SS to his deputy, Erhard Heiden, on 1 March 1927. Under Heiden's leadership, a stricter code of discipline was enforced than would have been tolerated in the SA.

Between 1925 and 1929, the SS was considered to be a small "Gruppe" (battalion) of the SA. Except in the Munich area, the SS was unable to maintain any momentum in its membership numbers, which declined from 1,000 to 280 as the SA continued its rapid growth. As Heiden attempted to keep the SS from dissolving, Heinrich Himmler became his deputy in September 1927. Himmler displayed good organizational abilities compared to Heiden. The SS established a number of "Gau"s (regions or provinces). The SS-Gaus consisted of "SS-Gau Berlin", "SS-Gau Berlin Brandenburg", "SS-Gau Franken", "SS-Gau Niederbayern", "SS-Gau Rheinland-Süd", and "SS-Gau Sachsen".

With Hitler's approval, Himmler assumed the position of "Reichsführer-SS" in January 1929. There are differing accounts of the reason for Heiden's dismissal from his position as head of the SS. The party announced that it was for "family reasons." Under Himmler, the SS expanded and gained a larger foothold. He considered the SS an elite, ideologically driven National Socialist organization, a "conflation of Teutonic knights, the Jesuits, and Japanese Samurai". His ultimate aim was to turn the SS into the most powerful organization in Germany and most influential branch of the party. He expanded the SS to 3,000 members in his first year as its leader.

In 1929, the "SS-Hauptamt" (main SS office) was expanded and reorganized into five main offices dealing with general administration, personnel, finance, security, and race matters. At the same time, the SS-Gaus were expanded into three "SS-Oberführerbereiche" areas, namely the "SS-Oberführerbereich Ost", "SS-Oberführerbereich West", and "SS-Oberführerbereich Süd". The lower levels of the SS remained largely unchanged. Although officially still considered a sub-organization of the SA and answerable to the "Stabschef" (SA Chief of Staff), it was also during this time that Himmler began to establish the independence of the SS from the SA. The SS grew in size and power due to its exclusive loyalty to Hitler, as opposed to the SA, which was seen as semi-independent and a threat to Hitler's hegemony over the party, mainly because they demanded a "second revolution" beyond the one that brought the NSDAP to power. By the end of 1933, the membership of the SS reached 209,000. Under Himmler's leadership the SS continued to gather greater power as more and more state and party functions were assigned to its jurisdiction. Over time the SS became answerable only to Hitler, a development typical of the organizational structure of the entire Nazi regime, where legal norms were replaced by actions undertaken under the "Führerprinzip" (leader principle), where Hitler's will was considered to be above the law.

In the latter half of 1934, Himmler oversaw the creation of "SS-Junkerschule" (Junker schools), institutions where SS officer candidates received leadership training, political and ideological indoctrination, and military instruction. The training stressed ruthlessness and toughness as part of the SS value system, which helped foster a sense of superiority among the men and taught them self-confidence. The first schools were established at Bad Tölz and Braunschweig, with additional schools opening at Klagenfurt and Prague during the war.

The SS was regarded as the NSDAP's elite unit. In keeping with the racial policy of Nazi Germany, in the early days all SS officer candidates had to provide proof of Aryan ancestry back to 1750 and for other ranks to 1800. Once the war started and it became more difficult to confirm ancestry, the regulation was amended to just proving the candidate's grandparents were Aryan, as spelled out in the Nuremberg Laws. Other requirements were complete obedience to the Führer and a commitment to the German people and nation. Himmler also tried to institute physical criteria based on appearance and height, but these requirements were only loosely enforced, and over half the SS men did not meet the criteria. Inducements such as higher salaries and larger homes were provided to members of the SS, since they were expected to produce more children than the average German family as part of their commitment to NSDAP doctrine.

Commitment to SS ideology was emphasized throughout the recruitment, membership process, and training. Members of the SS were indoctrinated in the racial policy of Nazi Germany, and were taught that it was necessary to remove from Germany people deemed by that policy as inferior. Esoteric rituals and the awarding of regalia and insignia for milestones in the SS man's career suffused SS members even further with Nazi ideology. Members were expected to renounce their Christian faith, and Christmas was replaced with a solstice celebration. Church weddings were replaced with SS "Ehewein", a pagan ceremony invented by Himmler. These pseudo-religious rites and ceremonies often took place near SS-dedicated monuments or in special SS-designated places. In 1933, Himmler bought Wewelsburg, a castle in Westphalia. He initially intended it to be used as an SS training centre, but its role came to include hosting SS dinners and neo-pagan rituals.

The SS ideology included the application of brutality and terror as a solution to military and political problems. The SS stressed total loyalty and obedience to orders unto death. Hitler used this as a powerful tool to further his aims and those of the NSDAP. The SS was entrusted with the commission of atrocities, illegal activities, and war crimes. Himmler once wrote that an SS man "hesitates not for a single instant, but executes unquestioningly ..." any "Führer-Befehl" (Führer order). Their official motto was ""Meine Ehre heißt Treue"" (My Honour is Loyalty).

As part of its race-centric functions during World War II, the SS oversaw the isolation and displacement of Jews from the populations of the conquered territories, seizing their assets and deporting them to concentration camps and ghettos, where they were used as slave labor or immediately killed. Chosen to implement the Final Solution ordered by Hitler, the SS were the main group responsible for the institutional killing and democide of more than 20 million people during the Holocaust, including approximately 5.2 million Jews and 10.5 million Slavs. A significant number of victims were members of other racial or ethnic groups such as the 258,000 Romani. The SS was involved in killing people viewed as threats to race hygiene or NSDAP ideology, including the mentally or physically handicapped, homosexuals, and political dissidents. Members of trade unions and those perceived to be affiliated with groups that opposed the regime (religious, political, social, and otherwise), or those whose views were contradictory to the goals of the NSDAP government, were rounded up in large numbers; these included clergy of all faiths, Jehovah's Witnesses, Freemasons, Communists, and Rotary Club members. According to the judgments rendered at the Nuremberg trials as well as many war crimes investigations and trials conducted since then, the SS was responsible for the majority of Nazi war crimes. In particular, it was the primary organization which carried out the Holocaust.

After Hitler and the NSDAP came to power on 30 January 1933, the SS were considered a state organization and a branch of the government. Law enforcement gradually became the purview of the SS, and many SS organizations became de facto government agencies.

The SS established a police state within Nazi Germany, using the secret state police and security forces under Himmler's control to suppress resistance to Hitler. In his role as Minister President of Prussia, Hermann Göring had in 1933 created a Prussian secret police force, the "Geheime Staatspolizei" or Gestapo, and appointed Rudolf Diels as its head. Concerned that Diels was not ruthless enough to use the Gestapo effectively to counteract the power of the SA, Göring handed over its control to Himmler on 20 April 1934. Also on that date, in a departure from long-standing German practice that law enforcement was a state and local matter, Hitler appointed Himmler chief of all German police outside Prussia. Himmler named his deputy and protégé Reinhard Heydrich chief of the Gestapo on 22 April 1934. Heydrich also continued as head of the "Sicherheitsdienst" (SD; security service).

The Gestapo's transfer to Himmler was a prelude to the Night of the Long Knives, in which most of the SA leadership were arrested and subsequently executed. The SS and Gestapo carried out most of the killings. On 20 July 1934, Hitler detached the SS from the SA, which was no longer an influential force after the purge. The SS became an independent elite corps of the NSDAP, answerable only to Hitler. Himmler's title of "Reichsführer-SS" now became his actual rank, equivalent to the rank of field marshal in the army (his previous rank was "Obergruppenführer"). As Himmler's position and authority grew, so did his de facto rank.

On 17 June 1936, all police forces throughout Germany were united under the purview of Himmler and the SS. Himmler and Heydrich thus became two of the most powerful men in the country's administration. Police and intelligence forces brought under their administrative control included the SD, Gestapo, "Kriminalpolizei" (Kripo; criminal investigative police), and "Ordnungspolizei" (Orpo; regular uniformed police). In September 1939, the security and police agencies, including the "Sicherheitspolizei" (SiPo; security police) and SD (but not the Orpo), were consolidated into the Reich Main Security Office (RSHA), headed by Heydrich. This further increased the collective authority of the SS.

In September 1939, the authority of the SS expanded further when the senior SS officer in each military district also became its chief of police. Most of these SS and police leaders held the rank of SS-"Gruppenführer" or above, and answered directly to Himmler in all SS matters within their district. Their role was to police the population and oversee the activities of the SS men within their district. By declaring an emergency, they could bypass the district administrative offices for the SS, SD, SiPo, "SS-Totenkopfverbände" (SS-TV; concentration camp guards), and Orpo, thereby gaining direct operational control of these groups.

During "Kristallnacht" (9–10 November 1938), SS security services clandestinely coordinated violence against Jews as the SS, Gestapo, SD, Kripo, SiPo and regular police did what they could to ensure that while Jewish synagogues and community centers were destroyed, Jewish-owned businesses and housing remained intact so that they could later be seized. In the end, thousands of Jewish businesses, homes, and graveyards were vandalized and looted, particularly by members of the SA. Some 500 to 1,000 synagogues were destroyed, mostly by arson. On 11 November, Heydrich reported a death toll of 36 people, but later assessments put the number of deaths at up to two thousand. On Hitler's orders, around 30,000 Jewish men were arrested and sent to concentration camps by 16 November. It is likely that as many as 2,500 of these people died in the following months. It was at this point that the SS state began in earnest its campaign of terror against political and religious opponents, who they imprisoned without trial or judicial oversight for the sake of "security, re-education, or prevention".

As the SS grew in size and importance, so too did Hitler's personal protection units. Three main SS groups were assigned to protect Hitler. In 1933, his larger personal bodyguard unit (previously the 1st SS-Standarte) was called to Berlin to replace the Army Chancellery Guard, assigned to protect the Chancellor of Germany. Sepp Dietrich commanded the new unit, previously known as SS-Stabswache Berlin; the name was changed to "SS-Sonderkommando Berlin". In November 1933, the name was changed to "Leibstandarte Adolf Hitler". In April 1934, Himmler modified the name to "Leibstandarte SS Adolf Hitler" (LSSAH). The LSSAH guarded Hitler's private residences and offices, providing an outer ring of protection for the Führer and his visitors. LSSAH men manned sentry posts at the entrances to the old Reich Chancellery and the new Reich Chancellery. The number of LSSAH guards was increased during special events. At the Berghof, Hitler's residence in the Obersalzberg, a large contingent of the LSSAH patrolled an extensive cordoned security zone.

From 1941, forward, the "Leibstandarte" became four distinct entities, the Waffen-SS division (unconnected to Hitler's personal protection but a formation of the Waffen-SS), the Berlin Chancellory Guard, the SS security regiment assigned to the Obersalzberg, and a Munich-based bodyguard unit which protected Hitler when he visited his personal apartment and the Brown House NSDAP headquarters in Munich. Although the unit was nominally under Himmler, Dietrich was the real commander and handled day-to-day administration.

Two other SS units composed the inner ring of Hitler's personal protection. The "SS-Begleitkommando des Führers" (Escort Command of the Führer), formed in February 1932, served as Hitler's protection escort while he was travelling. This unit consisted of eight men who served around the clock protecting Hitler in shifts. Later the "SS-Begleitkommando" was expanded and became known as the "Führerbegleitkommando" (Führer Escort Command; FBK). It continued under separate command and remained responsible for Hitler's personal protection. The "Führer Schutzkommando" (Führer Protection Command; FSK) was a protection unit founded by Himmler in March 1933. Originally it was charged with protecting Hitler only while he was inside the borders of Bavaria. In early 1934, they replaced the "SS-Begleitkommando" for Hitler's protection throughout Germany. The FSK was renamed the "Reichssicherheitsdienst" (Reich Security Service; RSD) in August 1935. Johann Rattenhuber, chief of the RSD, for the most part took his orders directly from Hitler. The current FBK chief acted as his deputy. Wherever Hitler was in residence, members of the RSD and FBK would be present. RSD men patrolled the grounds and FBK men provided close security protection inside. The RSD and FBK worked together for security and personal protection during Hitler's trips and public events, but they operated as two groups and used separate vehicles. By March 1938, both units wore the standard field grey uniform of the SS. The RSD uniform had the SD diamond on the lower left sleeve.

The SS was closely associated with Nazi Germany's concentration camp system. On 26 June 1933, Himmler appointed SS-"Oberführer" Theodor Eicke as commandant of Dachau concentration camp, one of the first Nazi concentration camps. It was created to consolidate the many small camps that had been set up by various police agencies and the NSDAP to house political prisoners. The organizational structure Eicke instituted at Dachau stood as the model for all later concentration camps. After 1934, Eicke was named commander of the "SS-Totenkopfverbände" (SS-TV), the SS formation responsible for running the concentration camps under the authority of the SS and Himmler. Known as the "Death's Head Units", the SS-TV was first organized as several battalions, each based at one of Germany's major concentration camps. Leadership at the camps was divided into five departments: commander and adjutant, political affairs division, protective custody, administration, and medical personnel. By 1935, Himmler secured Hitler's approval and the finances necessary to establish and operate additional camps. Six concentration camps housing 21,400 inmates (mostly political prisoners) existed at the start of the war in September 1939. By the end of the war, hundreds of camps of varying size and function had been created, holding nearly 715,000 people, most of whom were targeted by the regime because of their race. The concentration camp population rose in tandem with the defeats suffered by the Nazi regime; the worse the catastrophe seemed, the greater the fear of subversion, prompting the SS to intensify their repression and terror.

By the outbreak of World War II, the SS had consolidated into its final form, which comprised three main organizations: the "Allgemeine SS", "SS-Totenkopfverbände", and the Waffen-SS, which was founded in 1934 as the "SS-Verfügungstruppe" (SS-VT) and renamed in 1940. The Waffen-SS evolved into a second German army alongside the Wehrmacht and operated in tandem with them, especially with the "Heer" (German Army). Although SS ranks generally had equivalents in the other services, the SS rank system did not copy the terms and ranks used by the Wehrmacht's branches. Instead it used the ranks established by the post-World War I "Freikorps" and the SA. This was primarily done to emphasize the SS as being independent from the Wehrmacht.

In the September 1939 invasion of Poland, the LSSAH and SS-VT fought as separate mobile infantry regiments. The LSSAH became notorious for torching villages without military justification. Members of the LSSAH committed atrocities in numerous towns, including the murder of 50 Polish Jews in Błonie and the massacre of 200 civilians, including children, who were machine gunned in Złoczew. Shootings also took place in Bolesławiec, Torzeniec, Goworowo, Mława, and Włocławek. Some senior members of the Wehrmacht were not convinced the units were fully prepared for combat. Its units took unnecessary risks and had a higher casualty rate than the army. "Generaloberst" Fedor von Bock was quite critical; following an April 1940 visit of the "SS-Totenkopf" division, he found their battle training was "insufficient". Hitler thought the criticism was typical of the army's "outmoded conception of chivalry." In its defence, the SS insisted that its armed formations had been hampered by having to fight piecemeal and were improperly equipped by the army. 

After the invasion, Hitler entrusted the SS with extermination actions codenamed Operation Tannenberg and AB-Aktion to remove potential leaders who could form a resistance to German occupation. The killings were committed by "Einsatzgruppen" (task forces; deployment groups), assisted by local paramilitary groups. Men for the "Einsatzgruppen" units were drawn from the SS, the SD, and the police. Some 65,000 Polish civilians, including activists, intelligentsia, scholars, teachers, actors, former officers, and others, were killed by the end of 1939. When the army leadership registered complaints about the brutality being meted out by the "Einsatzgruppen", Heydrich informed them that he was acting "in accordance with the special order of the Führer." The first systematic mass shooting of Jews by the "Einsatzgruppen" took place on 6 September 1939 during the attack on Kraków.
Satisfied with their performance in Poland, Hitler allowed further expansion of the armed SS formations, but insisted new units remain under the operational control of the army. While the "SS-Leibstandarte" remained an independent regiment functioning as Hitler's personal bodyguards, the other regiments—"SS-Deutschland", "SS-Germania," and "SS-Der Führer"—were combined to form the "SS-Verfügungs-Division". A second SS division, the "SS-Totenkopf", was formed from SS-TV concentration camp guards, and a third, the "SS-Polizei", was created from police volunteers. The SS gained control over its own recruitment, logistics, and supply systems for its armed formations at this time. The SS, Gestapo, and SD were in charge of the provisional military administration in Poland until the appointment of Hans Frank as Governor-General on 26 October 1939.

On 10 May 1940, Hitler launched the Battle of France, a major offensive against France and the Low Countries. The SS supplied two of the 89 divisions employed. The LSSAH and elements of the SS-VT participated in the ground invasion of the Battle of the Netherlands. Simultaneously, airborne troops were dropped to capture key Dutch airfields, bridges, and railways. In the five-day campaign, the LSSAH linked up with army units and airborne troops after a number of clashes with Dutch defenders.

SS troops did not take part in the thrust through the Ardennes and the river Meuse. Instead, the "SS-Totenkopf" was summoned from the army reserve to fight in support of "Generalmajor" Erwin Rommel's 7th Panzer Division as they advanced toward the English Channel. On 21 May, the British launched an armored counterattack against the flanks of 7th Panzer Division and "SS-Totenkopf". The Germans then trapped the British and French troops in a huge pocket at Dunkirk. On 27 May, 4 Company, "SS-Totenkopf" perpetrated the Le Paradis massacre, where 97 men of the 2nd Battalion, Royal Norfolk Regiment were machine gunned after surrendering, with survivors finished off with bayonets. Two men survived. By 28 May the "SS-Leibstandarte" had taken Wormhout, from Dunkirk. There, soldiers of the 2nd Battalion were responsible for the Wormhoudt massacre, where 80 British and French soldiers were murdered after they surrendered. According to historian Charles Sydnor, the "fanatical recklessness in the assault, suicidal defense against enemy attacks, and savage atrocities committed in the face of frustrated objectives" exhibited by the "SS-Totenkopf" division during the invasion were typical of the SS troops as a whole.

At the close of the campaign, Hitler expressed his pleasure with the performance of the "SS-Leibstandarte", telling them: "Henceforth it will be an honour for you, who bear my name, to lead every German attack." The SS-VT was renamed the Waffen-SS in a speech made by Hitler in July 1940. Hitler then authorized the enlistment of "people perceived to be of related stock", as Himmler put it, to expand the ranks. A number of Danes, Dutch, Norwegians, Swedes, and Finns volunteered to fight in the Waffen-SS under the command of German officers. They were brought together to form the new division "SS-Wiking". In January 1941, the "SS-Verfügungs" Division was renamed "SS-Reich" Division (Motorized), and was renamed as the "2nd SS Panzer Division Das Reich" when it was reorganized as a "Panzergrenadier" division in 1942.

In April 1941, the German Army invaded Yugoslavia and Greece. The LSSAH and "Das Reich" were attached to separate army Panzer corps. Fritz Klingenberg, a company commander in the "Das Reich", led his men across Yugoslavia to the capital, Belgrade, where a small group in the vanguard accepted the surrender of the city on 13 April. A few days later Yugoslavia surrendered. SS police units immediately began taking hostages and carrying out reprisals, a practice that became common. In some cases, they were joined by the Wehrmacht. Similar to Poland, the war policies of the Nazis in the Balkans resulted in brutal occupation and racist mass murder. Serbia became the second country (after Estonia) declared "Judenfrei" (free of Jews).

In Greece, the Wehrmacht and Waffen-SS encountered resistance from the British Expeditionary Force (BEF) and Greek Army. The fighting was intensified by the mountainous terrain, with its heavily defended narrow passes. The LSSAH was at the forefront of the German push. The BEF evacuated by sea to Crete, but had to flee again in late May when the Germans arrived. Like Yugoslavia, the conquest of Greece brought its Jews into danger, as the Nazis immediately took a variety of measures against them. Initially confined in ghettos, most were transported to Auschwitz concentration camp in March 1943, where they were killed in the gas chambers on arrival. Of Greece's 80,000 Jews, only 20 percent survived the war.

On 22 June 1941, Hitler launched Operation Barbarossa, the invasion of the Soviet Union. The expanding war and the need to control occupied territories provided the conditions for Himmler to further consolidate the police and military organs of the SS. Rapid acquisition of vast territories in the East placed considerable strain on the SS police organizations as they struggled to adjust to the changing security challenges.

The 1st and 2nd SS Infantry Brigades, which had been formed from surplus concentration camp guards of the SS-TV, and the SS Cavalry Brigade moved into the Soviet Union behind the advancing armies. At first they fought Soviet partisans, but by the autumn of 1941, they left the anti-partisan role to other units and actively took part in the Holocaust. While assisting the "Einsatzgruppen", they formed firing parties that participated in the liquidation of the Jewish population of the Soviet Union.

On 31 July 1941, Göring gave Heydrich written authorization to ensure the cooperation of administrative leaders of various government departments to undertake genocide of the Jews in territories under German control. Heydrich was instrumental in carrying out these exterminations, as the Gestapo was ready to organize deportations in the West and his "Einsatzgruppen" were already conducting extensive killing operations in the East. On 20 January 1942, Heydrich chaired a meeting, called the Wannsee Conference, to discuss the implementation of the plan.

During battles in Soviet Union in 1941 and 1942, the Waffen-SS suffered enormous casualties. The LSSAH and "Das Reich" lost over half their troops to illness and combat casualties. In need of recruits, Himmler began to accept soldiers that did not fit the original SS racial profile. In early 1942, "SS-Leibstandarte", "SS-Totenkopf", and "SS-Das Reich" were withdrawn to the West to refit and were converted to "Panzergrenadier" divisions. The SS-Panzer Corps returned to the Soviet Union in 1943 and participated in the Third Battle of Kharkov in February and March.

The SS was built on a culture of violence, which was exhibited in its most extreme form by the mass murder of civilians and prisoners of war on the Eastern Front. Augmented by personnel from the Kripo, Orpo (Order Police), and Waffen-SS, the "Einsatzgruppen" reached a total strength of 3,000 men. "Einsatzgruppen" A, B, and C were attached to Army Groups North, Centre, and South; "Einsatzgruppe" D was assigned to the 11th Army. The "Einsatzgruppe" for Special Purposes operated in eastern Poland starting in July 1941. The historian Richard Rhodes describes them as being "outside the bounds of morality"; they were "judge, jury and executioner all in one", with the authority to kill anyone at their discretion. Following Operation Barbarossa, these "Einsatzgruppen" units, together with the Waffen-SS and Order Police, engaged in the mass killing of the Jewish population in occupied eastern Poland and the Soviet Union. The greatest extent of "Einsatzgruppen" action occurred in 1941 and 1942 in Ukraine and Russia. Before the invasion there were five million registered Jews throughout the Soviet Union, with three million of those residing in the territories occupied by the Germans; by the time the war ended, over two million of these had been murdered.

The extermination activities of the "Einsatzgruppen" generally followed a standard procedure, with the "Einsatzgruppen" chief contacting the nearest Wehrmacht unit commander to inform him of the impending action; this was done so they could coordinate and control access to the execution grounds. Initially the victims were shot, but this method proved impracticable for an operation of this scale. Also, after Himmler observed the shooting of 100 Jews at Minsk in August 1941, he grew concerned about the impact such actions were having on the mental health of his SS men. He decided that alternate methods of killing should be found, which led to introduction of gas vans. However, these were not popular with the men, because removing the dead bodies from the van and burying them was a horrible ordeal. Prisoners or auxiliaries were often assigned to do this task so as to spare the SS men the trauma.

In response to the army's difficulties in dealing with Soviet partisans, Hitler decided in July 1942 to transfer anti-partisan operations to the police. This placed the matter under Himmler's purview. As Hitler had ordered on 8 July 1941 that all Jews were to be regarded as partisans, the term "anti-partisan operations" was used as a euphemism for the murder of Jews as well as actual combat against resistance elements. In July 1942 Himmler ordered that the term "partisan" should no longer be used; instead resisters to Nazi rule would be described as "bandits".

Himmler set the SS and SD to work on developing additional anti-partisan tactics and launched a propaganda campaign. Sometime in June 1943, Himmler issued the "Bandenbekämpfung" (bandit fighting) order, simultaneously announcing the existence of the "Bandenkampfverbände" (bandit fighting formations), with SS-"Obergruppenführer" Erich von dem Bach-Zelewski as its chief. Employing troops primarily from the SS police and Waffen-SS, the "Bandenkampfverbände" had four principal operational components: propaganda, centralized control and coordination of security operations, training of troops, and battle operations. Once the Wehrmacht had secured territorial objectives, the "Bandenkampfverbände" first secured communications facilities, roads, railways, and waterways. Thereafter, they secured rural communities and economic installations such as factories and administrative buildings. An additional priority was securing agricultural and forestry resources. The SS oversaw the collection of the harvest, which was deemed critical to strategic operations. Any Jews in the area were rounded up and killed. Communists and people of Asiatic descent were killed presumptively under the assumption that they were Soviet agents.

After the start of the war, Himmler intensified the activity of the SS within Germany and in Nazi occupied Europe. An increasing numbers of Jews and German citizens deemed politically suspect or social outsiders were arrested. As the Nazi regime became more oppressive, the concentration camp system grew in size and lethal operation, and grew in scope as the economic ambitions of the SS intensified.

Intensification of the killing operations took place in late 1941 when the SS began construction of stationary gassing facilities to replace the use of "Einsatzgruppen" for mass killings. Victims at these new extermination camps were killed with the use of carbon monoxide gas from automobile engines. During Operation Reinhard, run by officers from the "Totenkopfverbände", who were sworn to secrecy, three death camps were built in occupied Poland: Bełżec (operational by March 1942), Sobibór (operational by May 1942), and Treblinka (operational by July 1942), with squads of Trawniki men (Eastern European collaborators) overseeing hundreds of "Sonderkommando" prisoners, who were forced to work in the gas chambers and crematoria before being murdered themselves. On Himmler's orders, by early 1942 the concentration camp at Auschwitz was greatly expanded to include the addition of gas chambers, where victims were killed using the pesticide Zyklon B.

For administrative reasons, all concentration camp guards and administrative staff became full members of the Waffen-SS in 1942. The concentration camps were placed under the command of the "SS-Wirtschafts-Verwaltungshauptamt" (SS Main Economic and Administrative Office; WVHA) under Oswald Pohl. Richard Glücks served as the Inspector of Concentration Camps, which in 1942 became office "D" under the WVHA. Exploitation and extermination became a balancing act as the military situation deteriorated. The labor needs of the war economy, especially for skilled workers, meant that some Jews escaped the genocide. On 30 October 1942, due to severe labor shortages, Himmler ordered that large numbers of able-bodied people in the Soviet occupied territories should be taken prisoner and sent to Germany as forced labor.

By 1944, the SS-TV had been organized into three divisions: staff of the concentration camps in Germany and Austria, in the occupied territories, and of the extermination camps in Poland. By 1944, it became standard practice to rotate SS members in and out of the camps, partly based on manpower needs, but also to provide easier assignments to wounded Waffen-SS members. This rotation of personnel meant that nearly the entire SS knew what was going on inside the concentration camps, making the entire organization liable for war crimes and crimes against humanity.

In 1934, Himmler founded the first SS business venture, Nordland-Verlag, a publishing house that released propaganda material and SS training manuals. Thereafter, he purchased Allach Porcelain, which then began to produce SS memorabilia. Because of the labor shortage and a desire for financial gain, the SS started exploiting concentration camp inmates as slave labor. Most of the SS businesses lost money until Himmler placed them under the administration of Pohl's "Verwaltung und Wirtschaftshauptamt Hauptamt" (Administration and Business office; VuWHA) in 1939. Even then, most of the enterprises were poorly run and did not fare well, as SS men were not selected for their business experience, and the workers were starving. In July 1940 Pohl established the "Deutsche Wirtschaftsbetriebe GmbH" (German Businesses Ltd; DWB), an umbrella corporation under which he took over administration of all SS business concerns. Eventually the SS founded nearly 200 holding companies for their businesses.

In May 1941 the VuWHA founded the "Deutsche Ausrüstungswerke" GmbH (German Equipment Works; DAW), which was created to integrate the SS business enterprises with the burgeoning concentration camp system. Himmler subsequently established four major new concentration camps in 1941: Auschwitz, Gross-Rosen, Natzweiler-Struthof, and Neuengamme. Each had at least one factory or quarry nearby where the inmates were forced to work. Himmler took a particular interest in providing laborers for IG Farben, which was constructing a synthetic rubber factory at Auschwitz III–Monowitz. The plant was almost ready to commence production when it was overrun by Soviet troops in 1945. Life expectancy of inmates at Monowitz averaged about three months. This was typical of the camps, as inmates were underfed and lived under disastrously bad living conditions. Their workload was intentionally made impossibly high, under the policy of extermination through labor.

In 1942, Himmler consolidated all of the offices for which Pohl was responsible into one, creating the SS Main Economic and Administrative Office ("Wirtschafts- und Verwaltungshauptamt"; WVHA). The entire concentration camp system was placed under the authority of the WVHA. The SS owned Sudetenquell GbmH, a mineral water producer in Sudetenland. By 1944, the SS had purchased 75 percent of the mineral water producers in Germany and were intending to acquire a monopoly. Several concentration camps produced building materials such as stone, bricks, and cement for the SS-owned "Deutsche Erd- und Steinwerke" (German Earth And Stone Works; DEST). In the occupied Eastern territories, the SS acquired a monopoly in brick production by seizing all 300 extant brickworks. The DWB also founded the "Ost-Deutsche Baustoffwerke" (East German Building Supply Works; GmbH or ODBS) and "Deutsche Edelmöbel" GmbH (German Noble Furniture). These operated in factories the SS had confiscated from Jews and Poles.

The SS owned experimental farms, bakeries, meat packing plants, leather works, clothing and uniform factories, and small arms factories. Under the direction of the WVHA, the SS sold camp labor to various factories at a rate of three to six "Reichsmarks" per prisoner per day. The SS confiscated and sold the property of concentration camp inmates, confiscated their investment portfolios and their cash, and profited from their dead bodies by selling their hair to make felt and melting down their dental work to obtain gold from the fillings. The total value of assets looted from the victims of Operation Reinhard alone (not including Auschwitz) was listed by Odilo Globocnik as 178,745,960.59 Reichsmarks. Items seized included 2,909.68 kilograms of gold worth 843,802.75 RM, as well as 18,733.69 kg of silver, 1,514 kg of platinum, 249,771.50 American dollars, 130 diamond solitaires, 2,511.87 carats of brilliants, 13,458.62 carats of diamonds, and 114 kg of pearls. According to Nazi legislation, Jewish property belonged to the state, but many SS camp commandants and guards stole items such as diamonds or currency for personal gain, or took seized foodstuffs and liquor to sell on the black market.

On 5 July 1943, the Germans launched the Battle of Kursk, an offensive designed to eliminate the Kursk salient. The Waffen-SS by this time had been expanded to 12 divisions, and most took part in the battle. Due to stiff Soviet resistance, Hitler halted the attack by the evening of 12 July. On 17 July he called off the operation and ordered a withdrawal. Thereafter, the Germans were forced onto the defensive as the Red Army began the liberation of Western Russia. The losses incurred by the Waffen-SS and the Wehrmacht during the Battle of Kursk occurred nearly simultaneously with the Allied assault into Italy, opening a two-front war for Germany.

Alarmed by the raids on St Nazaire and Dieppe in 1942, Hitler had ordered the construction of fortifications he called the Atlantic Wall all along the Atlantic coast, from Spain to Norway, to protect against an expected Allied invasion. Concrete gun emplacements were constructed at strategic points along the coast, and wooden stakes, metal tripods, mines, and large anti-tank obstacles were placed on the beaches to delay the approach of landing craft and impede the movement of tanks. In addition to several static infantry divisions, eleven panzer and "Panzergrenadier" divisions were deployed nearby. Four of these formations were Waffen-SS divisions. In addition, the "SS-Das Reich" was located in Southern France, the LSSAH was in Belgium refitting after fighting in the Soviet Union, and the newly formed panzer division "SS-Hitlerjugend", consisting of 17- and 18-year-old Hitler Youth members supported by combat veterans and experienced NCOs, was stationed west of Paris. The creation of the "SS-Hitlerjugend" was a sign of Hitler's desperation for more troops, especially ones with unquestioning obedience.

The Normandy landings took place beginning 6 June 1944. 21st Panzer Division under "Generalmajor" Edgar Feuchtinger, positioned south of Caen, was the only panzer division close to the beaches. The division included 146 tanks and 50 assault guns, plus supporting infantry and artillery. At 02:00, "Generalleutnant" Wilhelm Richter, commander of the 716th Static Infantry Division, ordered 21st Panzer Division into position to counter-attack. However, as the division was part of the armoured reserve, Feuchtinger was obliged to seek clearance from OKW before he could commit his formation. Feuchtinger did not receive orders until nearly 09:00, but in the meantime on his own initiative he put together a battle group (including tanks) to fight the British forces east of the Orne. "SS-Hitlerjugend" began to deploy in the afternoon of 6 June, with its units undertaking defensive actions the following day. They also took part in the Battle for Caen (June–August 1944). On 7–8 and 17 June, members of the "SS-Hitlerjugend" shot and killed twenty Canadian prisoners of war in the Ardenne Abbey massacre.

The Allies continued to make progress in the liberation of France, and on 4 August Hitler ordered a counter-offensive (Operation Lüttich) from Vire towards Avranches. The operation included LSSAH, "Das Reich", 2nd, and 116th Panzer Divisions, with support from infantry and elements of the 17th SS Panzergrenadier Division "Götz von Berlichingen" under SS-"Oberstgruppenführer" Paul Hausser. These forces were to mount an offensive near Mortain and drive west through Avranches to the coast. The Allied forces were prepared for this offensive, and an air assault on the combined German units proved devastating. On 21 August, 50,000 German troops, including most of the LSSAH, were encircled by the Allies in the Falaise Pocket. Remnants of the LSSAH which escaped were withdrawn to Germany for refitting. Paris was liberated on 25 August, and the last of the German forces withdrew over the Seine by the end of August, ending the Normandy campaign.

Waffen-SS units which had survived the summer campaigns were withdrawn from the front line to refit. Two of them, the 9th SS and 10th SS Panzer Divisions, did so in the Arnhem region of Holland in early September 1944. Coincidentally, on 17 September, the Allies launched in the same area Operation Market Garden, a combined airborne and land operation designed to seize control of the lower Rhine. The 9th and 10th Panzers were among the units that repulsed the attack.
In December 1944, Hitler launched the Ardennes Offensive, also known as the Battle of the Bulge, a significant counterattack against the western Allies through the Ardennes with the aim of reaching Antwerp while encircling the Allied armies in the area. The offensive began with an artillery barrage shortly before dawn on 16 December. Spearheading the attack were two panzer armies composed largely of Waffen-SS divisions. The battle groups found advancing through the forests and wooded hills of the Ardennes difficult in the winter weather, but they initially made good progress in the northern sector. They soon encountered strong resistance from the US 2nd and 99th Infantry Divisions. By 23 December, the weather improved enough that the Allied air forces could attack the German forces and their supply columns, causing fuel shortages. In increasingly difficult conditions, the German advance slowed and was stopped. Hitler's failed offensive cost 700 tanks and most of their remaining mobile forces in the west, as well as most of their irreplaceable reserves of manpower and materiel.

During the battle, SS-"Obersturmbannführer" Joachim Peiper left a path of destruction, which included Waffen-SS soldiers under his command murdering American POWs and unarmed Belgian civilians in the Malmedy massacre. Captured SS soldiers who were part of "Kampfgruppe Peiper" were tried during the Malmedy massacre trial following the war for this massacre and several others in the area. Many of the perpetrators were sentenced to hang, but the sentences were commuted. Peiper was imprisoned for eleven years for his role in the killings.

In the east, the Red Army resumed their offensive on 12 January 1945. German forces were outnumbered twenty to one in aircraft, eleven to one in infantry, and seven to one in tanks on the Eastern Front. By the end of the month, the Red Army had made bridgeheads across the Oder, the last geographic obstacle before Berlin. The western Allies continued to advance as well, but not as rapidly as the Red Army. The Panzer Corps conducted a successful defensive operation on 17–24 February at the Hron River, stalling the Allied advance towards Vienna. The 1st and 2nd SS Panzer Corps made their way towards Austria, but were slowed by damaged railways.

Budapest fell on 13 February. Hitler ordered Dietrich's 6th SS Panzer Army to move into Hungary to protect the Nagykanizsa oilfields and refineries, which he deemed the most strategically valuable fuel reserves on the Eastern Front. "Frühlingserwachsen" (Operation Spring Awakening), the final German offensive in the east, took place in early March. German forces attacked near Lake Balaton, with 6th SS Panzer Army advancing north towards Budapest and 2nd Panzer Army moving east and south. Dietrich's forces at first made good progress, but as they drew near the Danube, the combination of muddy terrain and strong Soviet resistance brought them to a halt. By 16 March the battle was lost. Enraged by the defeat, Hitler ordered the Waffen-SS units involved to remove their cuff titles as a mark of disgrace. Dietrich refused to carry out the order.

By this time, on both the Eastern and Western Front, the activities of the SS were becoming clear to the Allies, as the concentration and extermination camps were being overrun. Allied troops were filled with disbelief and repugnance at the evidence of Nazi brutality in the camps.

On 9 April 1945 Königsberg fell to the Red Army, and on 13 April Dietrich's SS unit was forced out of Vienna. The Battle of Berlin began at 03:30 on 16 April with a massive artillery barrage. Within the week, fighting was taking place inside the city. Among the many elements defending Berlin were French, Latvian, and Scandinavian Waffen-SS troops. Hitler, now living in the "Führerbunker" under the Reich Chancellery, still hoped that his remaining SS soldiers could rescue the capital. In spite of the hopelessness of the situation, members of the SS patrolling the city continued to shoot or hang soldiers and civilians for what they considered to be acts of cowardice or defeatism. The Berlin garrison surrendered on 2 May, two days after Hitler committed suicide. As members of SS expected little mercy from the Red Army, they attempted to move westward to surrender to the western Allies instead.

Heydrich held the title of "Chef des Sicherheitspolizei und SD" (Chief of the Security Police and SD) until 27 September 1939, when he became chief of the newly established Reich Main Security Office (RSHA). From that point forward, the RSHA was in charge of SS security services. It had under its command the SD, Kripo, and Gestapo, as well as several offices to handle finance, administration, and supply. Heinrich Müller, who had been chief of operations for the Gestapo, was appointed Gestapo chief at this time. Arthur Nebe was chief of the Kripo, and the two branches of SD were commanded by a series of SS officers, including Otto Ohlendorf and Walter Schellenberg. The SD was considered an elite branch of the SS, and its members were better educated and typically more ambitious than those within the ranks of the "Allgemeine" SS. Members of the SD were specially trained in criminology, intelligence, and counter-intelligence. They also gained a reputation for ruthlessness and unwavering commitment to Nazi ideology.

Heydrich was attacked in Prague on 27 May 1942 by a British-trained team of Czech and Slovak soldiers who had been sent by the Czechoslovak government-in-exile to kill him in Operation Anthropoid. He died from his injuries a week later. Himmler ran the RSHA personally until 30 January 1943, when Heydrich's positions were taken over by Ernst Kaltenbrunner.

Beginning in 1938 and throughout World War II, the SS enacted a procedure where offices and units of the SS could form smaller sub-units, known as "SS-Sonderkommandos", to carry out special tasks, including large-scale murder operations. The use of "SS-Sonderkommandos" was widespread. According to former "SS Sturmbannführer" Wilhelm Höttl, not even the SS leadership knew how many "SS-Sonderkommandos" were constantly being formed, disbanded, and reformed for various tasks, especially on the Eastern Front.

A "SS-Sonderkommando" unit led by "SS-Sturmbannführer" Herbert Lange murdered 1,201 psychiatric patients at the Tiegenhof psychiatric hospital in the Free City of Danzig, 1,100 patients in Owińska, 2,750 patients at Kościan, and 1,558 patients at Działdowo, as well as hundreds of Poles at Fort VII, where the mobile gas van and gassing bunker were developed. In 1941–42, "SS-Sonderkommando Lange" set up and managed the first extermination camp, at Chełmno, where 152,000 Jews were killed using gas vans.

After the battle of Stalingrad in February 1943, Himmler realised that Germany would likely lose the war, and ordered the formation of "Sonderkommando" 1005, a special task force under SS-"Standartenführer" Paul Blobel. The unit's assignment was to visit mass graves on the Eastern Front to exhume bodies and burn them in an attempt to cover up the genocide. The task remained unfinished at the end of the war, and many mass graves remain unmarked and unexcavated.

The "Eichmann Sonderkommando" was a task force headed by Adolf Eichmann that arrived in Budapest on 19 March 1944, the same day that Axis forces invaded Hungary. Their task was to take a direct role in the deportation of Hungarian Jews to Auschwitz. The "SS-Sonderkommandos" enlisted the aide of antisemitic elements from the Hungarian gendarmerie and pro-German administrators from within the Hungarian Interior Ministry. Round-ups began on 16 April, and from 14 May, four trains of 3,000 Jews per day left Hungary and travelled to the camp at Auschwitz II-Birkenau, arriving along a newly built spur line that terminated a few hundred metres from the gas chambers. Between 10 and 25 percent of the people on each train were chosen as forced laborers; the rest were killed within hours of arrival. Under international pressure, the Hungarian government halted deportations on 6 July 1944, by which time over 437,000 of Hungary's 725,000 Jews had died.

The "Einsatzgruppen" had its origins in the ad hoc "Einsatzkommando" formed by Heydrich following the "Anschluss" in Austria in March 1938. Two units of "Einsatzgruppen" were stationed in the Sudetenland in October 1938. When military action turned out not to be necessary because of the Munich Agreement, the "Einsatzgruppen" were assigned to confiscate government papers and police documents. They secured government buildings, questioned senior civil servants, and arrested as many as 10,000 Czech communists and German citizens. The "Einsatzgruppen" also followed Wehrmacht troops and killed potential partisans. Similar groups were used in 1939 for the occupation of Czechoslovakia.

Hitler felt that the planned extermination of the Jews was too difficult and important to be entrusted to the military. In 1941 the "Einsatzgruppen" were sent into the Soviet Union to begin large-scale genocide of Jews, Romani people, and communists. Historian Raul Hilberg estimates that between 1941 and 1945 the "Einsatzgruppen" and related agencies killed more than two million people, including 1.3 million Jews. The largest mass shooting perpetrated by the "Einsatzgruppen" was at Babi Yar outside Kiev, where 33,771 Jews were killed in a single operation on 29–30 September 1941. In the Rumbula massacre (November–December 1941), 25,000 victims from the Riga ghetto were killed. Another mass shooting early in 1942 claimed the lives of over 10,000 Jews in Kharkov.

The last "Einsatzgruppen" were disbanded in mid-1944 (although some continued to exist on paper until 1945) due to the German retreat on both fronts and the consequent inability to continue extermination activities. Former "Einsatzgruppen" members were either assigned duties in the Waffen-SS or concentration camps. Twenty-four "Einsatzgruppen" commanders were tried for war crimes following the war.

The SS Court Main Office ("Hauptamt SS-Gericht") was an internal legal system for conducting investigations, trials, and punishment of the SS and police. It had more than 600 lawyers on staff in the main offices in Berlin and Munich. Proceedings were conducted at 38 regional SS courts throughout Germany. It was the only authority authorized to try SS personnel, except for SS members who were on active duty in the Wehrmacht (in such cases, the SS member in question was tried by a standard military tribunal). Its creation placed the SS beyond the reach of civilian legal authority. Himmler personally intervened as he saw fit regarding convictions and punishment. The historian Karl Dietrich Bracher describes this court system as one factor in the creation of the Nazi totalitarian police state, as it removed objective legal procedures, rendering citizens defenseless against the "summary justice of the SS terror."

Shortly after Hitler seized power in 1933, most horse riding associations were taken over by the SA and SS. Members received combat training to serve in the "Reiter-SS" (SS Cavalry Corps). The first SS cavalry regiment, designated "SS-Totenkopf Reitstandarte 1", was formed in September 1939. Commanded by then SS-"Standartenführer" Hermann Fegelein, the unit was assigned to Poland, where they took part in the extermination of Polish intelligentsia. Additional squadrons were added in May 1940, for a total of fourteen.

The unit was split into two regiments in December 1939, with Fegelein in charge of both. By March 1941 their strength was 3,500 men. In July 1941, they were assigned to the Pripyat swamps punitive operation, tasked with rounding up and exterminating Jews and partisans. The two regiments were amalgamated into the SS Cavalry Brigade on 31 July, twelve days after the operation started. Fegelein's final report, dated 18 September 1941, states that they killed 14,178 Jews, 1,001 partisans, and 699 Red Army soldiers, with 830 prisoners taken. The historian Henning Pieper estimates the actual number of Jews killed was closer to 23,700. The SS Cavalry Brigade took serious losses in November 1941 in the Battle of Moscow, with casualties of up to 60 per cent in some squadrons. Fegelein was appointed as commander of the 8th SS Cavalry Division "Florian Geyer" on 20 April 1943. This unit saw service in the Soviet Union in attacks on partisans and civilians. In addition, SS Cavalry regiments served in Croatia and Hungary.

The SS Medical Corps were initially known as the "Sanitätsstaffel" (sanitary units). After 1931, the SS formed the headquarters office "Amt" V as the central office for SS medical units. An SS medical academy was established in Berlin in 1938 to train Waffen-SS physicians. SS medical personnel did not often provide actual medical care; their primary responsibility was medicalized genocide. At Auschwitz, about three-quarters of new arrivals, including almost all children, women with small children, all the elderly, and all those who appeared on brief and superficial inspection by an SS doctor not to be completely fit were killed within hours of arrival. In their role as "Desinfektoren" (disinfectors), SS doctors also made selections among existing prisoners as to their fitness to work, and supervised the killing of those deemed unfit. Inmates in deteriorating health were examined by SS doctors, who decided whether or not they would be able to recover in less than two weeks. Those too ill or injured to recover in that time frame were killed.

At Auschwitz, the actual delivery of gas to the victims was always handled by the SS, on the order of the supervising SS doctor. Many of the SS doctors also conducted inhumane medical experiments on camp prisoners. The most infamous SS doctor, Josef Mengele, served as a medical officer at Auschwitz under the command of Eduard Wirths of the camp's medical corps. Mengele undertook selections even when he was not assigned to do so in the hope of finding subjects for his experiments. He was particularly interested in locating sets of twins. In contrast to most of the doctors, who viewed undertaking selections as one of their most stressful and horrible duties, Mengele undertook the task with a flamboyant air, often smiling or whistling a tune. After the war, many SS doctors were charged with war crimes for their inhumane medical experiments and for their role in gas chamber selections.

The "Ahnenerbe" (Ancestral Heritage Organization) was founded in 1935 by Himmler, and became part of the SS in 1939. It was an umbrella agency for more than fifty organizations tasked with studying the German racial identity and ancient Germanic traditions and language. The agency sponsored archaeological expeditions in Germany, Scandinavia, the Middle East, Tibet, and elsewhere to search for evidence of Aryan roots, influence, and superiority. Further planned expeditions were postponed indefinitely at the start of the war.

The "SS-Frauenkops" was an auxiliary reporting and clerical unit, which included the "SS-Helferinnenkorps" (Women Helper Corps), made up of female volunteers. Members were assigned as administrative staff and supply personnel, and served in command positions and as guards at women's concentration camps. Like their male equivalents in the SS, females participated in atrocities against Jews, Poles, and others.

In 1942, Himmler set up the "Reichsschule für SS Helferinnen" (Reich school for SS helpers) in Oberehnheim to train women in communications so that they could free up men for combat roles. Himmler also intended to replace all female civilian employees in his service with "SS-Helferinnen" members, as they were selected and trained according to NSDAP ideology. The school was closed on 22 November 1944 due to the Allied advance.

The "SS-Mannschaften" (Auxiliary-SS) were not considered regular SS members, but were conscripted from other branches of the German military, the NSDAP, SA, and the "Volkssturm" for service in concentration camps and extermination camps.

Beginning in 1940, Himmler opened up Waffen-SS recruiting to ethnic Germans that were not German citizens. In March 1941, the SS Main Office established the "Germanische Leitstelle" (Germanic Guidance Office) to establish Waffen-SS recruiting offices in Nazi-occupied Europe. The majority of the resulting foreign Waffen-SS units wore a distinctive national collar patch and preceded their SS rank titles with the prefix "Waffen" instead of SS. Volunteers from Scandinavian countries filled the ranks of two divisions, the "SS-Wiking" and "SS-Nordland". Belgian Flemings joined Dutchmen to form the "SS-Nederland" legion, and their Walloon compatriots joined the "SS-Wallonien". By the end of 1943 about a quarter of the SS were ethnic Germans from across Europe, and by June 1944, half the Waffen-SS were foreign nationals.
Additional Waffen-SS units were added from the Ukrainians, Albanians from Kosovo, Serbians, Croatians, Turkic, Caucasians, Cossack, and Tatars. The Ukrainians and Tatars, who had suffered persecution under Stalin, were likely motivated primarily by opposition to the Soviet government rather than ideological agreement with the SS. The exiled Grand Mufti of Jerusalem Amin al-Husseini was made an SS-"Gruppenführer" by Himmler in May 1943. He subsequently used antisemitism and anti-Serb racism to recruit a Waffen-SS division of Bosnian Muslims, the "SS-Handschar". The year-long Soviet occupation of the Baltic states at the beginning of World War II resulted in volunteers for Latvian and Estonian Waffen-SS units. The Estonian Legion had 1,280 volunteers under training by the end of 1942. Approximately 25,000 men served in the Estonian SS division, with thousands more conscripted into Police Front battalions and border guard units. Most of the Estonians were fighting primarily to regain their independence and as many as 15,000 of them died fighting alongside the Germans. In early 1944, Himmler even contacted Pohl to suggest releasing Muslim prisoners from concentration camps to supplement his SS troops.

The Indian Legion was a Wehrmacht unit formed in August 1942 chiefly from disaffected Indian soldiers of the British Indian Army captured in the North African Campaign. In August 1944 it was transferred to the auspices of the Waffen-SS as the "Indische Freiwilligen-Legion der Waffen-SS". There was also a French volunteer division, "SS-Charlemagne", which was formed in 1944 mainly from the remnants of the Legion of French Volunteers Against Bolshevism and French "Sturmbrigade".

The SS established its own symbolism, rituals, customs, ranks and uniforms to set itself apart from other organizations. Before 1929, the SS wore the same brown uniform as the SA, with the addition of a black tie and a black cap with a "Totenkopf" (death's head) skull and bones symbol, moving to an all-black uniform in 1932. In 1935, the SS combat formations adopted a service uniform in field grey for everyday wear. The SS also developed its own field uniforms, which included reversible smocks and helmet covers printed with camouflage patterns. Uniforms were manufactured in hundreds of licensed factories, with some workers being prisoners of war performing forced labor. Many were produced in concentration camps.

Hitler and the NSDAP understood the power of emblems and insignia to influence public opinion. The stylized lightning bolt logo of the SS was chosen in 1932. The logo is a pair of runes from a set of 18 Armanen runes created by Guido von List in 1906. It is similar to the ancient Sowilō rune, which symbolizes the sun, but was renamed as "Sig" (victory) in List's iconography. The "Totenkopf" symbolized the wearer's willingness to fight unto the death, and also served to frighten the enemy.

After 1933 a career in the SS became increasingly attractive to Germany's social elite, who began joining the movement in great numbers, usually motivated by political opportunism. By 1938 about one-third of the SS leadership were members of the upper middle class. The trend reversed after the first Soviet counter-offensive of 1942.

By 1942 all activities of the SS were managed through twelve main offices.

The term "Austrian SS" is often used to describe that portion of the SS membership from Austria, but it was never a recognized branch of the SS. In contrast to SS members from other countries, who were grouped into either the Germanic-SS or the Foreign Legions of the Waffen-SS, Austrian SS members were regular SS personnel. It was technically under the command of the SS in Germany, but often acted independently concerning Austrian affairs. The Austrian SS was founded in 1930 and by 1934 was acting as a covert force to bring about the "Anschluss" with Germany, which occurred in March 1938. Early Austrian SS leaders were Kaltenbrunner and Arthur Seyss-Inquart. Austrian SS members served in every branch of the SS. Political scientist David Art of Tufts University notes that Austrians constituted 8 percent of the Third Reich's population and 13 percent of the SS; he states that 40 percent of the staff and 75 percent of commanders at death camps were Austrian.

After the "Anschluss", the Austrian SS was folded into "SS-Oberabschnitt Donau". The third regiment of the "SS-Verfügungstruppe" ("Der Führer") and the fourth "Totenkopf" regiment ("Ostmark") were recruited in Austria shortly thereafter. On Heydrich's orders, mass arrests of potential enemies of the Reich began immediately after the "Anschluss". Mauthausen was the first concentration camp opened in Austria following the "Anschluss". Before the invasion of the Soviet Union, Mauthausen was the harshest of the camps in the Greater German Reich.

The Hotel Metropole was transformed into Gestapo headquarters in Vienna in April 1938. With a staff of 900 (80 percent of whom were recruited from the Austrian police), it was the largest Gestapo office outside Berlin. An estimated 50,000 people were interrogated or tortured there. The Gestapo in Vienna was headed by Franz Josef Huber, who also served as chief of the Central Agency for Jewish Emigration in Vienna. Although its de facto leaders were Adolf Eichmann and later Alois Brunner, Huber was nevertheless responsible for the mass deportation of Austrian Jews.

Following Nazi Germany's collapse, the SS ceased to exist. Numerous members of the SS, many of them still committed Nazis, remained at large in Germany and across Europe. On 21 May 1945, the British captured Himmler, who was in disguise and using a false passport. At an internment camp near Lüneburg, he committed suicide by biting down on a cyanide capsule. Several other leading members of the SS fled, but some were quickly captured. Kaltenbrunner, chief of the RSHA and the highest-ranking surviving SS main department chief upon Himmler's suicide, was captured and arrested in the Bavarian Alps. He was among the 24 defendants put on trial at the International Military Tribunal in 1945–46.

Some SS members were subject to summary execution, torture, and beatings at the hands of freed prisoners, displaced persons, or Allied soldiers. American soldiers of the 157th Regiment, who entered the concentration camp at Dachau in April 1945 and saw the human deprivation and cruelty committed by the SS, shot some of the remaining SS camp guards. On 15 April 1945, British troops entered Bergen-Belsen. They placed the SS guards on starvation rations, made them work without breaks, forced them to deal with the remaining corpses, and stabbed them with bayonets or struck them with their rifle butts if they slowed their pace. Some members of the US Army Counter Intelligence Corps delivered captured SS camp guards to displaced persons camps, where they knew they would be subject to summary execution.

The Allies commenced legal proceedings against captured Nazis, establishing the International Military Tribunal at Nuremberg in 1945. The first war crimes trial of 24 prominent figures such as Hermann Göring, Albert Speer, Joachim von Ribbentrop, Alfred Rosenberg, Hans Frank, and Kaltenbrunner took place beginning in November 1945. They were accused of four counts: conspiracy, waging a war of aggression, war crimes, and crimes against humanity in violation of international law. Twelve received the death penalty, including Kaltenbrunner, who was convicted of crimes against humanity and executed on 16 October 1946. The former commandant at Auschwitz, Rudolf Höss, who testified on behalf of Kaltenbrunner and others, was tried and executed in 1947.

Additional SS trials and convictions followed. Many defendants attempted to exculpate themselves using the excuse that they were merely following superior orders, which they had to obey unconditionally as part of their sworn oath and duty. The courts did not find this to be a legitimate defense. A trial of 40 SS officers and guards from Auschwitz took place in Kraków in November 1947. Most were found guilty, and 23 received the death penalty. In addition to those tried by the Western allies, an estimated 37,000 members of the SS were tried and convicted in Soviet courts. Sentences included hangings and long terms of hard labor. Piotr Cywiński, the director of the Auschwitz-Birkenau Museum, estimates that of the 70,000 members of the SS involved in crimes in concentration camps, only about 1,650 to 1,700 were tried after the war. The International Military Tribunal declared the SS a criminal organization in 1946.

After the war, many former Nazis fled to South America, especially to Argentina, where they were welcomed by Juan Perón's regime. In the 1950s, former Dachau inmate Lothar Hermann discovered that Buenos Aires resident Ricardo Klement was in fact Adolf Eichmann, who had in 1948 obtained false identification and a landing permit for Argentina through an organization directed by Bishop Alois Hudal, an Austrian cleric with Nazi sympathies then residing in Italy. Eichmann was captured in Buenos Aires on 11 May 1960 by Mossad, the Israeli intelligence agency. At his trial in Jerusalem in 1961, he was found guilty and sentenced to death by hanging. Eichmann was quoted as having stated, "I will jump into my grave laughing, because the fact that I have the death of five million Jews [or Reich enemies, as he later claimed to have said] on my conscience gives me extraordinary satisfaction." Franz Stangl, the commandant of Treblinka, also escaped to South America with the assistance of Hudal's network. He was deported to Germany in 1967 and was sentenced to life in prison in 1970. He died in 1971.

Mengele, worried that his capture would mean a death sentence, fled Germany on 17 April 1949. Assisted by a network of former SS members, he traveled to Genoa, where he obtained a passport under the alias "Helmut Gregor" from the International Committee of the Red Cross. He sailed to Argentina in July. Aware that he was still a wanted man, he moved to Paraguay in 1958 and Brazil in 1960. In both instances he was assisted by former Luftwaffe pilot Hans-Ulrich Rudel. Mengele suffered a stroke while swimming and drowned in 1979.

Thousands of Nazis, including former SS members such as Trawniki guard Jakob Reimer and Circassian collaborator Tscherim Soobzokov, fled to the United States under the guise of refugees, sometimes using forged documents. Other SS men, such as Soobzokov, SD officer Wilhelm Höttl, Eichmann aide Otto von Bolschwing, and accused war criminal Theodor Saevecke, were employed by American intelligence agencies against the Soviets. As CIA officer Harry Rositzke noted, "It was a visceral business of using any bastard so long as he was anti-Communist ... The eagerness or desire to enlist collaborators means that sure, you didn't look at their credentials too closely." Similarly, the Soviets used SS personnel after the war; Operation Theo, for instance, disseminated "subversive rumours" in Allied-occupied Germany.

Simon Wiesenthal and others have speculated about the existence of a Nazi fugitive network code-named ODESSA (an acronym for "Organisation der ehemaligen SS-Angehörigen", Organization of former SS members) that allegedly helped war criminals find refuge in Latin America. British writer Gitta Sereny, who conducted interviews with SS men, considers the story untrue and attributes the escapes to postwar chaos and Hudal's Vatican-based network. While the existence of ODESSA remains unproven, Sereny notes that "there certainly were various kinds of Nazi aid organizations after the war — it would have been astonishing if there hadn't been."


Informational notes
Citations
Bibliography

Further reading



</doc>
<doc id="27045" url="https://en.wikipedia.org/wiki?curid=27045" title="New Wave science fiction">
New Wave science fiction

The New Wave is a movement in science fiction produced in the 1960s and 1970s and characterized by a high degree of experimentation, both in form and in content, a "literary" or artistic sensibility, and a focus on "soft" as opposed to hard science. New Wave writers often saw themselves as part of the modernist tradition and sometimes mocked the traditions of pulp science fiction, which some of them regarded as stodgy, adolescent and poorly written.

The New Wave science fiction of the 1960s emphasized stylistic experimentation and literary merit over scientific accuracy or prediction. It was conceived as a deliberate break from the traditions of pulp SF, which many of the writers involved considered irrelevant and unambitious. It was, according to academic Brian McHale, the edge of science fiction which ambitioned it to reach literary status, making it a case, among all of the arts, which were to constitute the emergence of postmodernism.

The most prominent source of New Wave science fiction was the magazine "New Worlds" under the editorship of Michael Moorcock, who assumed the position in 1964. Moorcock sought to use the magazine to "define a new avant-garde role" for science fiction by the use of "new literary techniques and modes of expression." It was also a period marked by the emergence of a greater variety of voices in science fiction, most notably the rise in the number of female writers, including Joanna Russ, Ursula K. Le Guin and James Tiptree, Jr..

The term "New Wave" is borrowed from the French film movement the "nouvelle vague".

Gary K. Wolfe, professor of humanities and English at Roosevelt University, identifies the introduction of the term New Wave to science fiction as occurring in 1966 in an essay for "The Magazine of Fantasy & Science Fiction" written by Judith Merril, who was indirectly yet it seems unambiguously referring to that term in order to comment on the experimental fiction that had begun to appear in the English magazine "New Worlds", after Michael Moorcock assumed editorship in 1964. However, Judith Merril denies she ever used that term.

Merril later popularized this fiction in the United States through her edited anthology "England Swings SF: Stories of Speculative Fiction" (Doubleday 1968), although an earlier anthology (Harlan Ellison's "Dangerous Visions" [Doubleday 1967]) is a key harbinger of New Wave science fiction in the US.

Though the New Wave began in the 1960s, some of its tenets can be found in H. L. Gold's editorship of "Galaxy", a science fiction magazine which began publication in 1950. James Gunn described Gold's focus as being "not on the adventurer, the inventor, the engineer, or the scientist, but on the average citizen," and according to SF historian David Kyle, Gold's work would lead to the New Wave.

Algis Budrys in 1965 wrote of the "recurrent strain in 'Golden Age' science fiction of the 1940's—the implication that sheer technological accomplishment would solve all the problems, hooray, and that all the problems were what they seemed to be on the surface". The New Wave did not define itself as a development from the science fiction which came before it, but initially reacted against it. New Wave writers did not operate as an organized group, but some of them felt the tropes of the pulp and Golden Age periods had become worn out, and should be abandoned: J. G. Ballard stated in 1962 that "science fiction should turn its back on space, on interstellar travel, extra-terrestrial life forms, (and) galactic wars", and Brian Aldiss said in "Trillion Year Spree: The History of Science Fiction" that "the props of SF are few: rocket ships, telepathy, robots, time travel...like coins, they become debased by over-circulation." Harry Harrison summarised the period by saying "old barriers were coming down, pulp taboos were being forgotten, new themes and new manners of writing were being explored".

New Wave writers began to look outside the traditional scope of science fiction for influence; some looked to the example of beat writer William S. Burroughs – New Wave authors Philip José Farmer and Barrington J. Bayley wrote pastiches of his work ("The Jungle Rot Kid on the Nod" and "The Four Colour Problem", respectively), while J. G. Ballard published an admiring essay in an issue of "New Worlds". Burroughs' use of experimentation such as the cut-up technique and his appropriation of science fiction tropes in radical ways proved the extent to which prose fiction could prove revolutionary, and some New Wave writers sought to emulate this style.

Ursula K. Le Guin, one of the writers to emerge in the 1960s, describes the transition to the New Wave era thus:

Critic Rob Latham identifies three trends that linked the advent of the New Wave in the 1960s to the emergence of cyberpunk in the 1980s. He said that changes in technology as well as an economic recession constricted the market for science fiction, generating a "widespread" malaise among fans, while established writers were forced to reduce their output (or, like Isaac Asimov, shifted their emphasis to other subjects); finally, editors encouraged fresh approaches that earlier ones discouraged.

There is no consensus on a precise starting point of the New Wave – Adam Roberts refers to Alfred Bester as having singlehandedly invented the genre, and in the introduction to a collection of Leigh Brackett's short fiction, Michael Moorcock referred to her as one of the genre's "true godmothers". Budrys said that in New Wave writers "there are echoes ... of Philip K. Dick, Walter Miller, Jr. and, by all odds, Fritz Leiber". However, it is widely accepted among critics that the New Wave began in England with the magazine "New Worlds" and Michael Moorcock. who was appointed editor in 1964 (first issue number 142, May and June)

While the American magazines "Amazing Stories", with Cele Goldsmith as editor, and "Magazine of Fantasy & Science Fiction" had from the start printed unusually literary stories, Moorcock turned that into a concerted policy. No other science fiction magazine sought as consistently to distance itself from traditional science fiction as much as "New Worlds". By the time it ceased regular publication it had backed away from the science fiction genre itself, styling itself as an experimental literary journal.

Under Moorcock's editorship "galactic wars went out; drugs came in; there were fewer encounters with aliens, more in the bedroom. Experimentation in prose styles became one of the orders of the day, and the baleful influence of William Burroughs often threatened to gain the upper hand." Judith Merril observed:

In 1963 Moorcock wrote:

In 1962 Ballard wrote:

Moorcock, Ballard, and others engendered much animosity from the established SF community. When reviewing "", Lester del Rey described it as "the first of the New Wave-Thing movies, with the usual empty symbolism". Budrys in "Galaxy", when reviewing a collection of recent stories from the magazine, said in 1965 that "There is this sense in this book ... that modern science fiction reflects a dissatisfaction with things as they are, sometimes to the verge of indignation, but also retains optimism about the eventual outcome". When reviewing "" he mocked Ellison's "'Repent, Harlequin!' Said the Ticktockman" and two other stories as "rudimentary social consciousness ... deep stuff" and insufficient for "an outstanding science-fiction story". Hartwell noted Budrys's "ringing scorn and righteous indignation" that year in "one of the classic diatribes against Ballard and the new mode of SF then emergent": 
Despite his criticism of Ballard and Aldiss ("the least talented" of the four), Budrys called them, Roger Zelazny, and Samuel R. Delany "an earthshaking new kind" of writers. Asimov said in 1967 of the New Wave, "I want science fiction. I think science fiction isn't really science fiction if it lacks science. And I think the better and truer the science, the better and truer the science fiction", but Budrys that year warned that the four would soon leave those "still reading everything from the viewpoint of the 1944 "Astounding" ... nothing but a complete collection of yellowed, crumble-edged bewilderment". While acknowledging the New Wave's "energy, high talent and dedication", and stating that it "may in fact be the shape of tomorrow's science fiction generally — hell, it may be the shape of today's science fiction", as examples of the movement Budrys much preferred Zelazny's "This Immortal" to Thomas Dischs "The Genocide". Predicting that Zelazny's career would be more important and lasting than Disch's, he described the latter's book as "unflaggingly derivative of" the New Wave and filled with "dumb, resigned victims" who "run, hide, slither, grope and die", like Ballard's "The Drowned World" but unlike "The Moon is a Harsh Mistress" ("about people who do something about their troubles"). Writing in "The Dreams Our Stuff Is Made Of", Disch observed that:

Roger Luckhurst pointed out that Ballard's essay "Which Way to Inner Space?" "showed the influence of media theorist Marshall McLuhan and the 'anti-psychiatry' of R. D. Laing." Luckhurst traces the influence of both these thinkers in Ballard's fiction, in particular "The Atrocity Exhibition" (1970) 

Another central concern of the New Wave was a fascination with entropy – that the world (and the universe) must tend to disorder, to eventually run down to 'heat death'. Ballard provided

Like other writers for "New Worlds" Zoline uses "science-fictional and scientific language and imagery to describe perfectly 'ordinary' scenes of life", and by doing so produces "altered perceptions of reality in the reader."

Judith Merril, "whose annual anthologies were the first heralds of the coming of the [New Wave] cult," writing in 1967 in "The Magazine of Fantasy and Science Fiction" contrasts the SF New Wave (which she here terms 'The New Thing') in England and the United States:

Judith Merril's annual anthologies (1957–1968), Damon Knight's "Orbit" series, and Harlan Ellison's "Dangerous Visions" featured American writers inspired by British writers (although some of the writers anthologized were British). Brooks Landon, professor of English at the University of Iowa, says of "Dangerous Visions" that it

The New Wave also had a political subtext:

Eric S. Raymond, looking at the New Wave with an even narrower political focus, observed:

For example, Judith Merril, "one of the most visible -- and voluble -- apostles of the New Wave in 1960s sf" remembers her return from England to the United States:

Merril said later "At the end of the Convention week, the taste of America was sour in all our mouths," and "by the end of the Sixties, Merril was a political refugee living in Canada."

Roger Luckhurst disagreed with those critics (he gives the example of Thomas Clareson) who perceived the New Wave in terms of rupture, suggesting that such a model

Caution is needed when assessing any literary movement. Science fiction writer Bruce Sterling, reacting to his association with another SF movement in the 1980s, remarked:

Similarly Rob Latham observed:

Bearing this proviso in mind it is still possible to sum up the New Wave in terms of rupture as is done for example by Darren Harris-Fain of Shawnee State University:

In the opening paragraph of an essay on the New Wave Rob Latham relates that

Latham remarks that this analysis by Harlan Ellison "obscures Ellison's own prominent role – and that of other professional authors and editors such as Judith Merril, Michael Moorcock, Lester Del Rey, Frederik Pohl, and Donald A. Wollheim – in fomenting the conflict, …"

In the early 1970s a number of writers and readers pointed out that

The closing of New Worlds magazine in 1970 "marked the containment of New Wave experiment with the rest of the counter-culture. The various limping manifestations of New World across the 1970s … demonstrated the posthumous nature of its avant-gardism.

In an essay "The Alien Encounter" Professor Patrick Parrinder stated that "any meaningful act of defamiliarization can only be relative, since it is not possible for man to imagine what is utterly alien to him; the utterly alien would also be meaningless." He continues later:

Veteran science fiction writer Jack Williamson (1908–2006) when asked in 1991: "Did the [New] Wave's emphasis on experimentalizm and its conscious efforts to make SF more 'literary' have any kind of permanent effects on the field?" replied:

It has been observed that

Hartwell maintained that after the New Wave, science fiction had still managed to retain this "marginality and tenuous self-identity":

Scientific accuracy was more important than literary style to Campbell, and top "Astounding" contributors Asimov, Heinlein, and L. Sprague de Camp were trained scientists and engineers. Asimov said in 1967 "I hope that when the New Wave has deposited its froth and receded, the vast and solid shore of "science" fiction will appear once more". Asimov himself was to illustrate just how that "SF shore" did indeed re-emerged, vast, solid—but changed. A biographer noted that during the 1960s

Darren Harris-Fain observed on this return to writing SF by Asimov that

Other themes dealt with in the novel are concerns for the environment and "human stupidity and the delusional belief in human superiority", both frequent topics in New Wave SF.

Commenting in 2002 on the publication of the 35th Anniversary edition of the "Dangerous Visions" anthology edited by Harlan Ellison, the critic Greg L. Johnson remarked that

Asimov agreed that "on the whole, the New Wave was a good thing". He described several "interesting side effects" of the New Wave. Non-American SF became more prominent and the genre became international phenomenon. Other changes noted were that

The noted academic writer on science fiction Edward James described the New Wave and its impact as follows:

John Brunner is a primary exponent of dystopian New Wave science fiction. Critic John Clute wrote of M. John Harrison's early writing that it "... reveals its New-Wave provenance in narrative discontinuities and subheads after the fashion of J. G. Ballard". Brian Aldiss, Harlan Ellison, Robert Silverberg, Norman Spinrad, Roger Zelazny are writers whose work, though not considered New Wave at the time of publication, later became to be associated with the label. Of later authors, the work of Joanna Russ is considered by scholar Peter Nicholls to bear stylistic resemblance to New Wave. Kaoru Kurimoto is also considered to be among the New Wave canon. Thomas M. Disch repudiated the "new wave" label: "If you mean to ask--do I feel solidarity with all writers who have ever been lumped together under that heading--certainly I do not."




</doc>
<doc id="27052" url="https://en.wikipedia.org/wiki?curid=27052" title="Administrative division">
Administrative division

An administrative division, unit, entity, area or region, also referred to as a subnational entity, statoid, constituent unit, or country subdivision, is a portion of a country or other region delineated for the purpose of administration. Administrative divisions are granted a certain degree of autonomy and are usually required to manage themselves through their own local governments. Countries are divided up into these smaller units to make managing their land and the affairs of their people easier. A country may be divided into provinces, which, in turn, are divided into counties, which, in turn, may be divided in whole or in part into municipalities.

Administrative divisions are conceptually separate from dependent territories, with the former being an integral part of the state and the other being only under some lesser form of control. However, the term "administrative division" can include dependent territories as well as accepted administrative divisions (for example, in geographical databases).

For clarity and convenience the standard neutral reference for the largest administrative subdivision of a country is called the "first-level administrative division" or "first administrative level". Next smaller is called "second-level administrative division" or "second administrative level".

In many of the following terms originating from British cultural influence, areas of relatively low mean population density might bear a title of an entity one would expect to be either larger or smaller. There is no fixed rule, for "all politics is local" as is perhaps well demonstrated by their relative lack of systemic order. In the realm of self-government, any of these can and does occur along a stretch of road—which for the most part is passing through rural unsettled countryside. Since the terms are administrative political subdivisions of the local regional government their exact relationship and definitions are subject to home rule considerations, tradition, as well as state statute law and local governmental (administrative) definition and control. In British cultural legacy, most territorial entities begin with fairly expansive counties which encompass an appreciably large area, but which may evolve over time into a number of smaller entities.

Within those entities are the large and small cities or towns, which may or may not be the county seat. Some of the world's larger cities culturally, if not officially, span several counties and those crossing state or provincial boundaries culturally are quite common as well, but are rarely incorporated within the same municipal government. Many sister cities share a water boundary which quite often serves as a border of both cities and counties. For example, Cambridge and Boston, Massachusetts appear to the casual traveler as one large city, while locally they each are quite culturally different and occupy different counties.




Due to variations in their use worldwide, consistency in the translation of terms from non-English to English is sometimes difficult to maintain.





</doc>
<doc id="27054" url="https://en.wikipedia.org/wiki?curid=27054" title="Service mark">
Service mark

A service mark or servicemark is a trademark used in the United States and several other countries to identify a service rather than a product.

When a service mark is federally registered, the standard registration symbol ® or "Reg U.S. Pat & TM Off" may be used (the same symbol is used to mark registered trademarks). Before it is registered, it is common practice (with some legal standing) to use the service mark symbol ℠ (a superscript SM).

The service mark symbol is mapped in Unicode as . A Unicode-capable browser is needed to display this character properly, which appears similar to "". The HTML entity is codice_1.

A service mark differs from a trademark in that the mark is used on the advertising of the service rather than on the packaging or delivery of the service, since there is generally no "package" to place the mark on, which is the practice for trademarks. For example, a private carrier can paint its service mark on its vehicles, such as on planes or buses. Personal service providers can place their service marks on their delivery vehicles, such as on the trucks of plumbers or on moving vans. However, if the service deals with communications, it is possible to use a service mark consisting of a sound (a sound trademark) in the process of delivering the service. This has been done in the case of AT&T, which uses a tone sound followed by a woman speaking the company's name to identify its long distance service; MGM, which uses the sound of a lion's roar; and RKO Pictures, which used a Morse code signal for their motion pictures.

Under United States law, service marks have a different standard of use in order to count as a use in commerce, which is necessary to complete registration and to stop infringement by competitors. A trademark normally needs to be used on or directly in association with the sale of goods, such as on a store display. As services are not defined by a concrete product, use of a service mark on the uniforms or vehicles of service providers or in advertisements is instead accepted as a use in commerce. However, like trademarks, service marks must pass a test of distinctiveness for it to be qualified as a service mark. For example, Thrifty, Inc. attempted to submit a service mark application that described aspects of their business (uniforms, buildings, certain vehicles) as "being blue." The application was rejected for not being specific enough, and the rejection was upheld on appeal.



</doc>
<doc id="27057" url="https://en.wikipedia.org/wiki?curid=27057" title="Scott Adams">
Scott Adams

Scott Adams (born June 8, 1957) is the creator of the "Dilbert" comic strip and the author of several nonfiction works of satire, commentary, and business.

His "Dilbert" series came to national prominence through the downsizing period in 1990s America and was then distributed worldwide. Adams worked in various roles at big businesses before he became a full-time cartoonist in 1995. He writes in a satirical, often sarcastic, way about the social and psychological landscape of white-collar workers in modern business corporations.

Scott Raymond Adams was born in 1957 in Windham, New York, the son of Paul and Virginia (née Vining) Adams. He is of half-German descent, and he also has English, Irish, Welsh, Scottish, Dutch, and "a small amount" of American Indian ancestry.

He was a fan of the "Peanuts" comics while growing up, and he started drawing his own comics at age six. He won a drawing competition at age 11. 

Adams graduated valedictorian at Windham-Ashland-Jewett Central School in 1975 in a class of 39. He remained in the area and received a BA in economics from Hartwick College in 1979. He moved to California a few months after his graduation.

Adams worked closely with telecommunications engineers at Crocker National Bank in San Francisco between 1979 and 1986. Upon joining the organization, he entered a management training program after being held at gunpoint twice in four months as a teller. Over the years, his positions included management trainee, computer programmer, budget analyst, commercial lender, product manager, and supervisor. He earned an MBA in economics and management from the University of California, Berkeley in 1986.

Adams created "Dilbert" during this period; the name came from ex-boss Mike Goodwin. Dogbert, originally named Dildog, was loosely based on his family's deceased pet beagle Lucy. Submissions to various publications of both "Dilbert" and non-"Dilbert" comic panels failed to win publication. These included "The New Yorker" and "Playboy". However, an inspirational letter from a fan persuaded Adams to keep trying.

He worked at Pacific Bell between 1986 and June 1995; the personalities he encountered there became the inspiration for many of his "Dilbert" characters. Adams first published "Dilbert" with United Media in 1989, while still employed at Pacific Bell. He had to draw his cartoons at 4 a.m. in order to work a full day at the company. His first paycheck for "Dilbert" was a monthly royalty check of $368.62. Gradually, "Dilbert" became more popular, and was published by 100 newspapers in 1991 and 400 by 1994. Adams attributes his success to his idea of including his e-mail address in the panels, thus facilitating feedback from readers.

Adams's success grew, and he became a full-time cartoonist with "Dilbert" in 800 newspapers. In 1996, "The Dilbert Principle" was released, his first business book.

Logitech CEO Pierluigi Zappacosta invited Adams to impersonate a management consultant, which he did wearing a wig and false mustache. He tricked Logitech managers into adopting a mission statement that Adams described as "so impossibly complicated that it has no real content whatsoever". That year, he won the National Cartoonists Society's Reuben Award for Outstanding Cartoonist of the Year and Best Newspaper Comic Strip of 1997, the most prestigious awards in the field.

In 1998, "Dilbert" began as a TV series, but was canceled in 2000. By 2000, the comic was in 2,000 newspapers in 57 countries and 19 languages.

Adams was a fan of the science fiction TV series "Babylon 5", and he appeared in the season 4 episode "Moments of Transition" as a character named "Mr. Adams" who hires former head of security Michael Garibaldi to locate his megalomaniacal dog and cat. He also had a cameo in "Review", a third-season episode of the TV series "NewsRadio", in which Matthew Brock (played by Andy Dick) becomes an obsessed "Dilbert" fan. Adams is credited as "Guy in line behind Dave and Joe in first scene".

Adams is the CEO of Scott Adams Foods, Inc., makers of the Dilberito and Protein Chef, and a co-owner of Stacey's Café in Pleasanton, California.

Adams is a member of the International Academy of Digital Arts and Sciences and a former member of Mensa.

In recent years, Adams has had two notable health problems. Since late 2004, he has suffered from a reemergence of focal dystonia, which has affected his ability to draw for lengthy periods on paper, though it causes no real problem now that he draws the comic on a graphics tablet. He also suffered from spasmodic dysphonia, a condition that causes the vocal cords to behave in an abnormal manner. He recovered from this condition temporarily, but in July 2008 underwent surgery to reroute the nerve connections to his vocal cords. The operation was successful, and Adams' voice is now completely functional.

Adams is a vegetarian and trained as a hypnotist. He credits his own success to affirmations, including "Dilbert"s success and achieving a ninety-fourth percentile on a difficult qualification exam for business school, among other unlikely events. He states that the affirmations give him focus. He has described a method which he has used that he says gave him success. He pictured in his mind what he wanted, and wrote it down 15 times a day on a piece of paper. 

In addition to his cartoon work, he has written two books on religion, "God's Debris" (2001), and "The Religion War" (2004). "God's Debris" lays out a theory of Pandeism, in which God blows itself up to see what will happen, which becomes the cause of our universe. 

Adams married Shelly Miles in 2006. She has two children named Savannah and Justin Miles. In a February 2014 blog posting he revealed that he is no longer married. Kristina Basham, a model and baker, is Adams' girlfriend with whom he lives. She has two daughters, and is vice president of WhenHub.

Adams has often commented on political matters. Despite this, in 2016 he wrote on his blog "I don't vote and I am not a member of a political party." In 2007, he suggested that Michael Bloomberg would make a good presidential candidate.

Before the 2008 presidential election he said, "On social issues, I lean Libertarian, minus the crazy stuff", but said in December 2011 that, if he were president, he would do whatever Bill Clinton advised him to do because that "would lead to policies that are a sensible middle ground". In a blog post from September 2017, Adams considers himself to be "left of Bernie [Sanders], but with a preference for plans that can work."

On October 17, 2012, he wrote "while I don't agree with Romney's positions on most topics, I'm endorsing him for president".

In 2015, although Adams stated that he would not endorse a candidate for the 2016 elections, he repeatedly praised Donald Trump's persuasion skills, especially on his blog, extensively detailing what he called Trump's "talent stack". 

Adams correctly predicted Trump would win the Republican nomination and the general election; in the 2016 election campaign's final weeks, except for a temporary reversal in early October, Adams repeatedly said Trump would win. 

Adams has shared on his blog and elsewhere that men may feel emasculated by the nomination of a female candidate for president. Of the 2016 Democratic National Convention, he said the following: "If you're an undecided voter, and male, you're seeing something different. You're seeing a celebration that your role in society is permanently diminished. And it's happening in an impressive venue that was, in all likelihood, designed and built mostly by men." 

Adams said that he temporarily endorsed Hillary Clinton purely out of fear for his own life, stating he had received direct and indirect death threats. In late September, however, Adams officially switched his endorsement from Clinton to Trump. Among his primary reasons for the switch were his respect for Trump's persuasion skills over Clinton's, Clinton's proposal to raise the inheritance tax to 65%, and his concerns over Clinton's health. Adams states that writing about Donald Trump ended his speaking career and reduced his income by about 40%.






Adams has received recognition for his work, including the National Cartoonist Society Reuben Award and Newspaper Comic Strip Award for 1997 for his work on "Dilbert". He had also been climbing the European Foundation for Management Development (EFMD) rankings of the 50 most influential management thinkers placing 31st in 2001, 27th in 2003, and 12th in 2005, but fell to 21st in 2007. He did not place in 2009.

He received the NCTE George Orwell Award for Distinguished Contribution to Honesty and Clarity in Public Language for his participation in "Mission Impertinent" ("San Jose Mercury News West Magazine", November 16, 1997).

Adams has coined or popularized several words and phrases over the years, such as:




</doc>
<doc id="27058" url="https://en.wikipedia.org/wiki?curid=27058" title="Steel">
Steel

Steel is an alloy of iron and carbon and other elements. Because of its high tensile strength and low cost, it is a major component used in buildings, infrastructure, tools, ships, automobiles, machines, appliances, and weapons.

Iron is the base metal of steel. Iron is able to take on two crystalline forms (allotropic forms), body centered cubic (BCC) and face centered cubic (FCC), depending on its temperature. In the body-centred cubic arrangement, there is an iron atom in the centre and eight atoms at the vertices of each cube; in the face-centred cubic, there is one atom at the center of each of the six faces of the cube and eight atoms at the vertices. It is the interaction of the allotropes of iron with the alloying elements, primarily carbon, that gives steel and cast iron their range of unique properties.

In pure iron, the crystal structure has relatively little resistance to the iron atoms slipping past one another, and so pure iron is quite ductile, or soft and easily formed. In steel, small amounts of carbon, other elements, and inclusions within the iron act as hardening agents that prevent the movement of dislocations that are common in the crystal lattices of iron atoms.

The carbon in typical steel alloys may contribute up to 2.14% of its weight. Varying the amount of carbon and many other alloying elements, as well as controlling their chemical and physical makeup in the final steel (either as solute elements, or as precipitated phases), slows the movement of those dislocations that make pure iron ductile, and thus controls and enhances its qualities. These qualities include such things as the hardness, quenching behavior, need for annealing, tempering behavior, yield strength, and tensile strength of the resulting steel. The increase in steel's strength compared to pure iron is possible only by reducing iron's ductility.

Steel was produced in bloomery furnaces for thousands of years, but its large-scale, industrial use began only after more efficient production methods were devised in the 17th century, with the production of blister steel and then crucible steel. With the invention of the Bessemer process in the mid-19th century, a new era of mass-produced steel began. This was followed by the Siemens-Martin process and then the Gilchrist-Thomas process that refined the quality of steel. With their introductions, mild steel replaced wrought iron.

Further refinements in the process, such as basic oxygen steelmaking (BOS), largely replaced earlier methods by further lowering the cost of production and increasing the quality of the final product. Today, steel is one of the most common man-made materials in the world, with more than 1.6 billion tons produced annually. Modern steel is generally identified by various grades defined by assorted standards organizations.

The noun "steel" originates from the Proto-Germanic adjective "stahliją" or "stakhlijan" ("made of steel"), which is related to "stahlaz" or "stahliją" ("standing firm").

The carbon content of steel is between 0.002% and 2.14% by weight for plain iron–carbon alloys. These values vary depending on alloying elements such as manganese, chromium, nickel, tungsten, and so on. Basically, steel is an iron-carbon alloy that does not undergo eutectic reaction. In contrast, cast iron does undergo eutectic reaction. Too little carbon content leaves (pure) iron quite soft, ductile, and weak. Carbon contents higher than those of steel make a brittle alloy commonly called pig iron. While iron alloyed with carbon is called carbon steel, alloy steel is steel to which other alloying elements have been intentionally added to modify the characteristics of steel. Common alloying elements include: manganese, nickel, chromium, molybdenum, boron, titanium, vanadium, tungsten, cobalt, and niobium. Additional elements are also important in steel: phosphorus, sulfur, silicon, and traces of oxygen, nitrogen, and copper, that are most frequently considered undesirable.

Plain carbon-iron alloys with a higher than 2.1% carbon content are known as cast iron. With modern steelmaking techniques such as powder metal forming, it is possible to make very high-carbon (and other alloy material) steels, but such are not common. Cast iron is not malleable even when hot, but it can be formed by casting as it has a lower melting point than steel and good castability properties. Certain compositions of cast iron, while retaining the economies of melting and casting, can be heat treated after casting to make malleable iron or ductile iron objects. Steel is distinguishable from wrought iron (now largely obsolete), which may contain a small amount of carbon but large amounts of slag.

Iron is commonly found in the Earth's crust in the form of an ore, usually an iron oxide, such as magnetite or hematite. Iron is extracted from iron ore by removing the oxygen through its combination with a preferred chemical partner such as carbon which is then lost to the atmosphere as carbon dioxide. This process, known as smelting, was first applied to metals with lower melting points, such as tin, which melts at about , and copper, which melts at about , and the combination, bronze, which has a melting point lower than . In comparison, cast iron melts at about . Small quantities of iron were smelted in ancient times, in the solid state, by heating the ore in a charcoal fire and then welding the clumps together with a hammer and in the process squeezing out the impurities. With care, the carbon content could be controlled by moving it around in the fire. Unlike copper and tin, liquid or solid iron dissolves carbon quite readily.

All of these temperatures could be reached with ancient methods used since the Bronze Age. Since the oxidation rate of iron increases rapidly beyond , it is important that smelting take place in a low-oxygen environment. Smelting, using carbon to reduce iron oxides, results in an alloy (pig iron) that retains too much carbon to be called steel. The excess carbon and other impurities are removed in a subsequent step.

Other materials are often added to the iron/carbon mixture to produce steel with desired properties. Nickel and manganese in steel add to its tensile strength and make the austenite form of the iron-carbon solution more stable, chromium increases hardness and melting temperature, and vanadium also increases hardness while making it less prone to metal fatigue.

To inhibit corrosion, at least 11% chromium is added to steel so that a hard oxide forms on the metal surface; this is known as stainless steel. Tungsten slows the formation of cementite, keeping carbon in the iron matrix and allowing martensite to preferentially form at slower quench rates, resulting in high speed steel. On the other hand, sulfur, nitrogen, and phosphorus are considered contaminants that make steel more brittle and are removed from the steel melt during processing.

The density of steel varies based on the alloying constituents but usually ranges between , or .

Even in a narrow range of concentrations of mixtures of carbon and iron that make a steel, a number of different metallurgical structures, with very different properties can form. Understanding such properties is essential to making quality steel. At room temperature, the most stable form of pure iron is the body-centered cubic (BCC) structure called alpha iron or α-iron. It is a fairly soft metal that can dissolve only a small concentration of carbon, no more than 0.005% at and 0.021 wt% at . The inclusion of carbon in alpha iron is called ferrite. At 910 °C pure iron transforms into a face-centered cubic (FCC) structure, called gamma iron or γ-iron. The inclusion of carbon in gamma iron is called austenite. The more open FCC structure of austenite can dissolve considerably more carbon, as much as 2.1% (38 times that of ferrite) carbon at , which reflects the upper carbon content of steel, beyond which is cast iron. When carbon moves out of solution with iron it forms a very hard, but brittle material called cementite (FeC).

When steels with exactly 0.8% carbon (known as a eutectoid steel), are cooled, the austenitic phase (FCC) of the mixture attempts to revert to the ferrite phase (BCC). The carbon no longer fits within the FCC austenite structure, resulting in an excess of carbon. One way for carbon to leave the austenite is for it to precipitate out of solution as cementite, leaving behind a surrounding phase of BCC iron called ferrite with a small percentage of carbon in solution. The two, ferrite and cementite, precipitate simultaneously producing a layered structure called pearlite, named for its resemblance to mother of pearl. In a hypereutectoid composition (greater than 0.8% carbon), the carbon will first precipitate out as large inclusions of cementite at the austenite grain boundaries until the percenage of carbon in the grains has decreased to the eutectoid composition (0.8% carbon), at which point the pearlite structure forms. For steels that have less than 0.8% carbon (hypoeutectoid), ferrite will first form within the grains until the remaining composition rises to 0.8% of carbon, at which point the pearlite structure will form. No large inclusions of cementite will form at the boundaries in hypoeuctoid steel. The above assumes that the cooling process is very slow, allowing enough time for the carbon to migrate.

As the rate of cooling is increased the carbon will have less time to migrate to form carbide at the grain boundaries but will have increasingly large amounts of pearlite of a finer and finer structure within the grains; hence the carbide is more widely dispersed and acts to prevent slip of defects within those grains, resulting in hardening of the steel. At the very high cooling rates produced by quenching, the carbon has no time to migrate but is locked within the face-centered austenite and forms martensite. Martensite is a highly strained and stressed, supersaturated form of carbon and iron and is exceedingly hard but brittle. Depending on the carbon content, the martensitic phase takes different forms. Below 0.2% carbon, it takes on a ferrite BCC crystal form, but at higher carbon content it takes a body-centered tetragonal (BCT) structure. There is no thermal activation energy for the transformation from austenite to martensite. Moreover, there is no compositional change so the atoms generally retain their same neighbors.

Martensite has a lower density (it expands during the cooling) than does austenite, so that the transformation between them results in a change of volume. In this case, expansion occurs. Internal stresses from this expansion generally take the form of compression on the crystals of martensite and tension on the remaining ferrite, with a fair amount of shear on both constituents. If quenching is done improperly, the internal stresses can cause a part to shatter as it cools. At the very least, they cause internal work hardening and other microscopic imperfections. It is common for quench cracks to form when steel is water quenched, although they may not always be visible.

There are many types of heat treating processes available to steel. The most common are annealing, quenching, and tempering. Heat treatment is effective on compositions above the eutectoid composition (hypereutectoid) of 0.8% carbon. Hypoeutectoid steel does not benefit from heat treatment.

Annealing is the process of heating the steel to a sufficiently high temperature to relieve local internal stresses. It does not create a general softening of the product but only locally relieves strains and stresses locked up within the material. Annealing goes through three phases: recovery, recrystallization, and grain growth. The temperature required to anneal a particular steel depends on the type of annealing to be achieved and the alloying constituents.

Quenching involves heating the steel to create the austenite phase then quenching it in water or oil. This rapid cooling results in a hard but brittle martensitic structure. The steel is then tempered, which is just a specialized type of annealing, to reduce brittleness. In this application the annealing (tempering) process transforms some of the martensite into cementite, or spheroidite and hence it reduces the internal stresses and defects. The result is a more ductile and fracture-resistant steel.

When iron is smelted from its ore, it contains more carbon than is desirable. To become steel, it must be reprocessed to reduce the carbon to the correct amount, at which point other elements can be added. In the past, steel facilities would cast the raw steel product into ingots which would be stored until use in further refinement processes that resulted in the finished product. In modern facilities, the initial product is close to the final composition and is continuously cast into long slabs, cut and shaped into bars and extrusions and heat treated to produce a final product. Today only a small fraction is cast into ingots. Approximately 96% of steel is continuously cast, while only 4% is produced as ingots.

The ingots are then heated in a soaking pit and hot rolled into slabs, billets, or blooms. Slabs are hot or cold rolled into sheet metal or plates. Billets are hot or cold rolled into bars, rods, and wire. Blooms are hot or cold rolled into structural steel, such as I-beams and rails. In modern steel mills these processes often occur in one assembly line, with ore coming in and finished steel products coming out. Sometimes after a steel's final rolling it is heat treated for strength, however this is relatively rare.

Steel was known in antiquity and was produced in bloomeries and crucibles.

The earliest known production of steel is seen in pieces of ironware excavated from an archaeological site in Anatolia (Kaman-Kalehöyük) and are nearly 4,000 years old, dating from 1800 BC. Horace identifies steel weapons such as the "falcata" in the Iberian Peninsula, while Noric steel was used by the Roman military.

The reputation of "Seric iron" of South India (wootz steel) grew considerably in the rest of the world. Metal production sites in Sri Lanka employed wind furnaces driven by the monsoon winds, capable of producing high-carbon steel. Large-scale Wootz steel production in Tamilakam using crucibles and carbon sources such as the plant Avāram occurred by the sixth century BC, the pioneering precursor to modern steel production and metallurgy.

The Chinese of the Warring States period (403–221 BC) had quench-hardened steel, while Chinese of the Han dynasty (202 BC – 220 AD) created steel by melting together wrought iron with cast iron, gaining an ultimate product of a carbon-intermediate steel by the 1st century AD.

Evidence of the earliest production of high carbon steel in the Indian Subcontinent are found in Kodumanal in Tamil Nadu area, Golconda in Andhra Pradesh area and Karnataka, and in Samanalawewa areas of Sri Lanka. This came to be known as Wootz steel, produced in South India by about sixth century BC and exported globally. The steel technology existed prior to 326 BC in the region as they are mentioned in literature of Sangam Tamil, Arabic and Latin as the finest steel in the world exported to the Romans, Egyptian, Chinese and Arab worlds at that time – what they called "Seric Iron". A 200 BC Tamil trade guild in Tissamaharama, in the South East of Sri Lanka, brought with them some of the oldest iron and steel artifacts and production processes to the island from the classical period. The Chinese and locals in Anuradhapura, Sri Lanka had also adopted the production methods of creating Wootz steel from the Chera Dynasty Tamils of South India by the 5th century AD. In Sri Lanka, this early steel-making method employed a unique wind furnace, driven by the monsoon winds, capable of producing high-carbon steel. Since the technology was acquired from the Tamilians from South India, the origin of steel technology in India can be conservatively estimated at 400–500 BC.

The manufacture of what came to be called Wootz, or Damascus steel, famous for its durability and ability to hold an edge, may have been taken by the Arabs from Persia, who took it from India. It was originally created from a number of different materials including various trace elements, apparently ultimately from the writings of Zosimos of Panopolis. In 327 BC, Alexander the Great was rewarded by the defeated King Porus, not with gold or silver but with 30 pounds of steel. Recent studies have suggested that carbon nanotubes were included in its structure, which might explain some of its legendary qualities, though given the technology of that time, such qualities were produced by chance rather than by design. Natural wind was used where the soil containing iron was heated by the use of wood. The ancient Sinhalese managed to extract a ton of steel for every 2 tons of soil, a remarkable feat at the time. One such furnace was found in Samanalawewa and archaeologists were able to produce steel as the ancients did.

Crucible steel, formed by slowly heating and cooling pure iron and carbon (typically in the form of charcoal) in a crucible, was produced in Merv by the 9th to 10th century AD. In the 11th century, there is evidence of the production of steel in Song China using two techniques: a "berganesque" method that produced inferior, inhomogeneous, steel, and a precursor to the modern Bessemer process that used partial decarbonization via repeated forging under a cold blast.

Since the 17th century, the first step in European steel production has been the smelting of iron ore into pig iron in a blast furnace. Originally employing charcoal, modern methods use coke, which has proven more economical.

In these processes pig iron was refined (fined) in a finery forge to produce bar iron, which was then used in steel-making.

The production of steel by the cementation process was described in a treatise published in Prague in 1574 and was in use in Nuremberg from 1601. A similar process for case hardening armour and files was described in a book published in Naples in 1589. The process was introduced to England in about 1614 and used to produce such steel by Sir Basil Brooke at Coalbrookdale during the 1610s.

The raw material for this process were bars of iron. During the 17th century it was realized that the best steel came from oregrounds iron of a region north of Stockholm, Sweden. This was still the usual raw material source in the 19th century, almost as long as the process was used.

Crucible steel is steel that has been melted in a crucible rather than having been forged, with the result that it is more homogeneous. Most previous furnaces could not reach high enough temperatures to melt the steel. The early modern crucible steel industry resulted from the invention of Benjamin Huntsman in the 1740s. Blister steel (made as above) was melted in a crucible or in a furnace, and cast (usually) into ingots.

The modern era in steelmaking began with the introduction of Henry Bessemer's Bessemer process in 1855, the raw material for which was pig iron. His method let him produce steel in large quantities cheaply, thus mild steel came to be used for most purposes for which wrought iron was formerly used. The Gilchrist-Thomas process (or "basic Bessemer process") was an improvement to the Bessemer process, made by lining the converter with a basic material to remove phosphorus.

Another 19th-century steelmaking process was the Siemens-Martin process, which complemented the Bessemer process. It consisted of co-melting bar iron (or steel scrap) with pig iron.

These methods of steel production were rendered obsolete by the Linz-Donawitz process of basic oxygen steelmaking (BOS), developed in the 1950s, and other oxygen steel making methods. Basic oxygen steelmaking is superior to previous steelmaking methods because the oxygen pumped into the furnace limited impurities, primarily nitrogen, that previously had entered from the air used. Today, electric arc furnaces (EAF) are a common method of reprocessing scrap metal to create new steel. They can also be used for converting pig iron to steel, but they use a lot of electrical energy (about 440 kWh per metric ton), and are thus generally only economical when there is a plentiful supply of cheap electricity.

The steel industry is often considered an indicator of economic progress, because of the critical role played by steel in infrastructural and overall economic development. In 1980, there were more than 500,000 U.S. steelworkers. By 2000, the number of steelworkers fell to 224,000.

The economic boom in China and India caused a massive increase in the demand for steel. Between 2000 and 2005, world steel demand increased by 6%. Since 2000, several Indian and Chinese steel firms have risen to prominence, such as Tata Steel (which bought Corus Group in 2007), Baosteel Group and Shagang Group. ArcelorMittal is however the world's largest steel producer. In 2005, the British Geological Survey stated China was the top steel producer with about one-third of the world share; Japan, Russia, and the US followed respectively.

In 2008, steel began trading as a commodity on the London Metal Exchange. At the end of 2008, the steel industry faced a sharp downturn that led to many cut-backs.

Steel is one of the world's most-recycled materials, with a recycling rate of over 60% globally; in the United States alone, over were recycled in the year 2008, for an overall recycling rate of 83%.

Modern steels are made with varying combinations of alloy metals to fulfill many purposes. Carbon steel, composed simply of iron and carbon, accounts for 90% of steel production. Low alloy steel is alloyed with other elements, usually molybdenum, manganese, chromium, or nickel, in amounts of up to 10% by weight to improve the hardenability of thick sections. High strength low alloy steel has small additions (usually < 2% by weight) of other elements, typically 1.5% manganese, to provide additional strength for a modest price increase.

Recent Corporate Average Fuel Economy (CAFE) regulations have given rise to a new variety of steel known as Advanced High Strength Steel (AHSS). This material is both strong and ductile so that vehicle structures can maintain their current safety levels while using less material. There are several commercially available grades of AHSS, such as dual-phase steel, which is heat treated to contain both a ferritic and martensitic microstructure to produce a formable, high strength steel. Transformation Induced Plasticity (TRIP) steel involves special alloying and heat treatments to stabilize amounts of austenite at room temperature in normally austenite-free low-alloy ferritic steels. By applying strain, the austenite undergoes a phase transition to martensite without the addition of heat. Twinning Induced Plasticity (TWIP) steel uses a specific type of strain to increase the effectiveness of work hardening on the alloy.

Carbon Steels are often galvanized, through hot-dip or electroplating in zinc for protection against rust.

Stainless steels contain a minimum of 11% chromium, often combined with nickel, to resist corrosion. Some stainless steels, such as the ferritic stainless steels are magnetic, while others, such as the austenitic, are nonmagnetic. Corrosion-resistant steels are abbreviated as CRES.

Some more modern steels include tool steels, which are alloyed with large amounts of tungsten and cobalt or other elements to maximize solution hardening. This also allows the use of precipitation hardening and improves the alloy's temperature resistance. Tool steel is generally used in axes, drills, and other devices that need a sharp, long-lasting cutting edge. Other special-purpose alloys include weathering steels such as Cor-ten, which weather by acquiring a stable, rusted surface, and so can be used un-painted. Maraging steel is alloyed with nickel and other elements, but unlike most steel contains little carbon (0.01%). This creates a very strong but still malleable steel.

Eglin steel uses a combination of over a dozen different elements in varying amounts to create a relatively low-cost steel for use in bunker buster weapons. Hadfield steel (after Sir Robert Hadfield) or manganese steel contains 12–14% manganese which when abraded strain-hardens to form an incredibly hard skin which resists wearing. Examples include tank tracks, bulldozer blade edges and cutting blades on the jaws of life.

In 2016, a breakthrough in creating a strong light aluminium steel alloy which might be suitable in applications such as aircraft was announced by researchers at Pohang University of Science and Technology. Adding small amounts of nickel was found to result in precipitation as nano particles of brittle B2 intermetallic compounds which had previously resulted in weakness. The result was a cheap strong light steel alloy—nearly as strong as titanium at 10% of the cost—which is slated for trial production at industrial scale by POSCO, a Korean steelmaker.

Most of the more commonly used steel alloys are categorized into various grades by standards organizations. For example, the Society of Automotive Engineers has a series of grades defining many types of steel. The American Society for Testing and Materials has a separate set of standards, which define alloys such as A36 steel, the most commonly used structural steel in the United States. The JIS also define series of steel grades that are being used extensively in Japan as well as in third world countries.

Iron and steel are used widely in the construction of roads, railways, other infrastructure, appliances, and buildings. Most large modern structures, such as stadiums and skyscrapers, bridges, and airports, are supported by a steel skeleton. Even those with a concrete structure employ steel for reinforcing. In addition, it sees widespread use in major appliances and cars. Despite growth in usage of aluminium, it is still the main material for car bodies. Steel is used in a variety of other construction materials, such as bolts, nails, and screws and other household products and cooking utensils.

Other common applications include shipbuilding, pipelines, mining, offshore construction, aerospace, white goods (e.g. washing machines), heavy equipment such as bulldozers, office furniture, steel wool, tools, and armour in the form of personal vests or vehicle armour (better known as rolled homogeneous armour in this role).

Before the introduction of the Bessemer process and other modern production techniques, steel was expensive and was only used where no cheaper alternative existed, particularly for the cutting edge of knives, razors, swords, and other items where a hard, sharp edge was needed. It was also used for springs, including those used in clocks and watches.

With the advent of speedier and thriftier production methods, steel has become easier to obtain and much cheaper. It has replaced wrought iron for a multitude of purposes. However, the availability of plastics in the latter part of the 20th century allowed these materials to replace steel in some applications due to their lower fabrication cost and weight. Carbon fiber is replacing steel in some cost insensitive applications such as aircraft, sports equipment and high end automobiles.





Steel manufactured after World War II became contaminated with radionuclides by nuclear weapons testing. Low-background steel, steel manufactured prior to 1945, is used for certain radiation-sensitive applications such as Geiger counters and radiation shielding.





</doc>
<doc id="27059" url="https://en.wikipedia.org/wiki?curid=27059" title="Stainless steel">
Stainless steel

In metallurgy, stainless steel, also known as inox steel or inox from French "inoxydable" (inoxidizable), is a steel alloy with a minimum of 10.5% chromium content by mass.

Stainless steels are notable for their corrosion resistance, which increases with increasing chromium content. Additions of molybdenum increase corrosion resistance in reducing acids and against pitting attack in chloride solutions. Thus, there are numerous grades of stainless steel with varying chromium and molybdenum contents to suit the environment the alloy must endure. Stainless steel’s resistance to corrosion and staining, low maintenance, and familiar lustre make it an ideal material for many applications where both the strength of steel and corrosion resistance are required.

Stainless steels are rolled into sheets, plates, bars, wire, and tubing to be used in cookware, cutlery, surgical instruments, major appliances and as construction material in large buildings, such as the Chrysler Building. As well as, industrial equipment (for example, in paper mills, chemical plants, water treatment), and storage tanks and tankers for chemicals and food products (for example, chemical tankers and road tankers). Stainless steel's corrosion resistance, the ease with which it can be steam cleaned and sterilized and no need for other surface coatings has also influenced its use in commercial kitchens and food processing plants.

Stainless steels do not suffer uniform corrosion, like carbon steel, when exposed to wet environments. Unprotected carbon steel rusts readily when exposed to the combination of air and moisture. The resulting iron oxide surface layer (the rust) is porous and fragile. Since iron oxide occupies a larger volume than the original steel this layer expands and tends to flake and fall away exposing the underlying steel to further attack. In comparison, stainless steels contain sufficient chromium to undergo passivation, spontaneously forming a microscopically thin inert surface film of chromium oxide by reaction with the oxygen in air and even the small amount of dissolved oxygen in water. This passive film prevents further corrosion by blocking oxygen diffusion to the steel surface and thus prevents corrosion from spreading into the bulk of the metal. This film is self-repairing if it is scratched or temporarily disturbed by an upset condition in the environment that exceeds the inherent corrosion resistance of that grade.

However, stainless steels may suffer uniform corrosion when exposed to acidic or basic solutions. Whether a stainless steel corrodes depends on the kind and concentration of acid or base, and the solution temperature. Uniform corrosion is typically easy to avoid because of extensive published corrosion data or easy to perform laboratory corrosion testing.

Unfortunately, stainless steels are susceptible to localized corrosion under certain conditions, which need to be recognized and avoided. Such localized corrosion is problematic for stainless steels because it is unexpected and difficult to predict.

Acidic solutions can be categorized into two general categories, reducing acids such as hydrochloric acid and dilute sulfuric acid, and oxidizing acids such as nitric acid and concentrated sulfuric acid. Increasing chromium and molybdenum contents provide increasing resistance to reducing acids, while increasing chromium and silicon contents provide increasing resistance to oxidizing acids.

Sulfuric acid is one of the largest tonnage industrial chemical manufactured. At room temperature Type 304 is only resistant to 3% acid at room temperature while Type 316 is resistant to 3% acid up to 50C and 20% acid at room temperature. Thus Type 304 is rarely used in contact with sulfuric acid. Type 904L and Alloy 20 are resistant to sulfuric acid at even higher concentrations above room temperature.

Concentrated sulfuric acid possesses oxidizing characteristics like nitric acid and thus silicon bearing stainless steels also find application.

Hydrochloric acid will damage any kind of stainless steel, and should be avoided.

All types of stainless steel resist attack from phosphoric acid and nitric acid at room temperature. At high concentration and elevated temperature attack will occur and higher alloy stainless steels are required.

In general, organic acids are less corrosive than mineral acids such as hydrochloric and sulfuric acid. As the molecular weight of organic acids increase their corrosivity decreases. Formic acid has the lowest molecular weight and is a strong acid. Type 304 can be used with formic acid though it will tend to discolor the solution. Acetic acid is probably the most commercially important of the organic acids and Type 316 is commonly used for storing and handling acetic acid.

Stainless steels Type 304 and 316 are unaffected by any of the weak bases such as ammonium hydroxide, even in high concentrations and at high temperatures. The same grades of stainless exposed to stronger bases such as sodium hydroxide at high concentrations and high temperatures will likely experience some etching and cracking.

Increasing chromium and nickel contents provide increasing resistance.

All grades resist damage from aldehydes and amines, though in the latter case Type 316 is preferable to 304; cellulose acetate will damage 304 unless the temperature is kept low. Fats and fatty acids only affect Type 304 at temperatures above , and Type 316 above , while Type 317 is unaffected at all temperatures. Type 316L is required for processing of urea.

Localized corrosion can occur in a number of ways, e.g. pitting corrosion, crevice corrosion and stress corrosion cracking. Such localized attack is most common in the presence of chloride ions. Increasing chromium, molybdenum and nitrogen contents provide increasing resistance to localized corrosion and thus increasing chloride levels require more highly alloyed stainless steels. Design and good fabrication techniques combined with correct alloy selection can prevent such corrosion.

Localized corrosion can be difficult to predict because it is dependent on many factors including:

Galvanic corrosion (also called ' dissimilar metal corrosion') refers to corrosion damage induced when two dissimilar materials are coupled in a corrosive electrolyte. The most common electrolyte is water, ranging from fresh water to seawater. When a galvanic couple forms, one of the metals in the couple becomes the anode and corrodes faster than it would all by itself, while the other becomes the cathode and corrodes slower than it would alone. Stainless steel, due to its superior corrosion resistance relative to most other metals, including steel and aluminum, becomes the cathode accelerating the corrosion of the anodic metal. An example, is the corrosion of aluminum rivets fastening stainless steel sheets in contact with water.

At elevated temperatures all metals react with hot gases. The most common high temperature gaseous mixture is air, and oxygen is the most reactive component of air. Carbon steel is limited to ~ in air. Chromium in stainless steel reacts with oxygen to form a chromium oxide scale which reduces oxygen diffusion into the material. The minimum 10.5% chromium in stainless steels provides resistance to ~, while 26% chromium provides resistance up to ~. Type 304, the most common grade of stainless steel with 18% chromium is resistant to ~. Other gases such as sulfur dioxide, hydrogen sulfide, carbon monoxide, chlorine, etc. also attack stainless steel. Resistance to other gases is dependent on the type of gas, the temperature and the alloying content of the stainless steel.

Like steel, stainless steels are relatively poor conductors of electricity, with significantly lower electrical conductivity than copper.

Ferritic and martensitic stainless steels are magnetic. Annealed austenitic stainless steels are non-magnetic. Work hardening can make austenitic stainless steels slightly magnetic.

Galling, sometimes called cold welding, is a form of severe adhesive wear which can occur when two metal surfaces are in relative motion to each other and under heavy pressure. Austenitic stainless steel fasteners are particularly susceptible to thread galling, although it also occurs in other alloys that self-generate a protective oxide surface film, such as aluminum and titanium. Under high contact-force sliding this oxide can be deformed, broken and removed from parts of the component, exposing bare reactive metal. When the two surfaces are the same material, these exposed surfaces can easily fuse together. Separation of the two surfaces can result in surface tearing and even complete seizure of metal components or fasteners.

Galling can be mitigated by the use of dissimilar materials (bronze against stainless steel), or using different stainless steels (martensitic against austenitic). Additionally, threaded joints may be lubricated to provide a film between the two parts and prevent galling. Also, Nitronic 60, made by selective alloying with manganese, silicon and nitrogen, has demonstrated a reduced tendency to gall.

The corrosion resistance of iron-chromium alloys was first recognized in 1821 by French metallurgist Pierre Berthier, who noted their resistance against attack by some acids and suggested their use in cutlery. Metallurgists of the 19th century were unable to produce the combination of low carbon and high chromium found in most modern stainless steels, and the high-chromium alloys they could produce were too brittle to be practical.

In 1872, the Englishmen Clark and Woods patented an alloy that would today be considered a stainless steel.

In the late 1890s Hans Goldschmidt of Germany developed an aluminothermic (thermite) process for producing carbon-free chromium. Between 1904 and 1911 several researchers, particularly Leon Guillet of France, prepared alloys that would today be considered stainless steel.

Friedrich Krupp Germaniawerft built the 366-ton sailing yacht "Germania" featuring a chrome-nickel steel hull in Germany in 1908. In 1911, Philip Monnartz reported on the relationship between chromium content and corrosion resistance. On 17 October 1912, Krupp engineers Benno Strauss and Eduard Maurer patented austenitic stainless steel as Nirosta.

Similar developments were taking place contemporaneously in the United States, where Christian Dantsizen and Frederick Becket were industrializing ferritic stainless steel. In 1912, Elwood Haynes applied for a US patent on a martensitic stainless steel alloy, which was not granted until 1919.
In 1912, Harry Brearley of the Brown-Firth research laboratory in Sheffield, England, while seeking a corrosion-resistant alloy for gun barrels, discovered and subsequently industrialized a martensitic stainless steel alloy. The discovery was announced two years later in a January 1915 newspaper article in "The New York Times". The metal was later marketed under the "Staybrite" brand by Firth Vickers in England and was used for the new entrance canopy for the Savoy Hotel in London in 1929. Brearley applied for a US patent during 1915 only to find that Haynes had already registered a patent. Brearley and Haynes pooled their funding and with a group of investors formed the American Stainless Steel Corporation, with headquarters in Pittsburgh, Pennsylvania.

In the beginning stainless steel was sold in the US under different brand names like "Allegheny metal" and "Nirosta steel". Even within the metallurgy industry the eventual name remained unsettled; in 1921 one trade journal was calling it "unstainable steel". In 1929, before the Great Depression hit, over 25,000 tons of stainless steel were manufactured and sold in the US.

There are five main families, which are primarily classified by their crystalline structure:

Austenitic stainless steel is the largest family of stainless steels, making up about two-thirds of all stainless steel production. They possess an austenitic microstructure, which is a face-centered cubic crystal structure. This microstructure is achieved by alloying with sufficient nickel and/or manganese and nitrogen to maintain an austenitic microstructure at all temperatures from the cryogenic region to the melting point. Thus austenitic stainless steels are not hardenable by heat treatment since they possess the same microstructure at all temperatures. Though they can be strengthened by cold working, but this is limited to thin sheet and small diameter bar. Their austenitic microstructure gives them excellent formability and weldability and they are essentially non-magnetic and maintain their ductility at cryogenic temperatures.

They can be further subdivided into two sub-groups, 200 series and 300 series:

Low-carbon versions, for example 316L or 304L, are used to avoid corrosion problems caused by welding. The "L" means that the carbon content of the alloy is below 0.03%, which prevents sensitization (precipitation of chromium carbides at grain boundaries) caused by the high temperatures involved in welding.
Superaustenitic stainless steels, such as Allegheny Technologies' alloy AL-6XN and Outokumpu’s alloy 254 SMO, possess even greater resistance to chloride pitting and crevice corrosion because of their high molybdenum content (>6%) and nitrogen additions. They possess useful service to seawater applications.

Ferritic stainless steels possess a ferrite microstructure like carbon steel, which is a body-centered cubic crystal structure and contain between 10.5% and 27% chromium with very little or no nickel. This microstructure is present at all temperatures, due to the chromium addition, and like austenitic stainless steels are not hardenable by heat treatment. They cannot be strengthened by cold work to the same degree as austenitic stainless steels. They are magnetic like carbon steel. They are problematic to weld due to grain growth in the heat affected zone which reduces ductility and may result in cracks. Increasing chromium and molybdenum contents increase corrosion resistance as it does for austenitic stainless steels, however, this high alloying results in the precipitation of embrittling intermetallic phase upon welding. These microstructural problems due to welding restrict the use of ferritic stainless steels to very thin thicknesses and thus are not used in the construction of large heavy walled vessels and tanks, and structures like austenitic stainless steels.

Common ferritic grades are:
Type 430 with 17% chromium, which is used in washing machine drums, dishwasher interiors and refrigerator exteriors.
Type 409 with 11% chromium is used extensively in the manufacture of automotive exhausts.

Martensitic stainless steels offer a wide range of properties and are used as steinless engineering steels, stainless tool steels, creep resisting steels.

They fall into 4 categories (with some overlap)

i) Fe - Cr - C grades: They were the first grades used and they are still widely used in engineering and wear-resistant applications

ii) Fe-Cr-Ni-C grades: In these grades, some of the Carbon is replaced by Nickel. They offer a higher toughness and a higher corrosion resistance.

iii) Precipitation Hardening grades: Grade EN 1.4542 (a.k.a 17/4PH), the best known grade, combines martensitic hardening and precipitation hardening. It achieves high strength and good toughness and is used in aerospace among other applications.

iv) Creep-resisiting grades: small additions of Nb, V, B, Co increase the strength and creep resistance up to about 650 °C

Chemical composition of major martensitic and precipitation hardening stainlesss steels, from EN 10088-1:2005 standard
In addition, there are a number of proprietary gades, most of them being close to standardized ones

Martensitic stainless steels form a family of stainless steels that can be heat treated to provide the adequate level of mechanical properties.

The heat treatment typically involves three steps >

- Austenitizing, in which the steel is heated to a temperature in the range 980 - 1050 °C -depending on the grades. The austenite is a face centered cublc phase

- Quenching (a rapid cooling in air, oil or water). The austenite is transformed into martenisite, a hard a body-centered tetragonal crystal structure. The as-quenched martensite is very hard and too brittle for most applications. Some residual austenite may remain.

- Tempering, i.e. heating around 500 °C, holding at temperature , then air cooling. Increasing the tempering temperature decreases the Yield and Ultimate tensile strength but increases the elongation and the impact resistance.

"Note 1: Stress-relieving, i.e. heat treatment around 200 °C is carried out to keep the highest hardness with gaining some ductility. it is used for cutting blades and other tool applications"

"Note 2: When the lowest strength level is required (usually for processing), tempering is sometimes called annealing. The temperature is the highest possible without forming austenite again. For some grades, a double tempering is necessary."

"Note 3: some grades exhibit an increase in strength, called secondary hardening, upon tempering at around 500 °C"

Mechanical properties of heat-treated martensitic stainless steels form EN 10088-3:2014 Standard

In the mechanical properties of martensitic stainless steels below, QT stands for Quenched & Tempered

The annealed condition, allow easy machining and forming, whereas the QT condition is required for strength (shafts, valve stems, mechanical components...)

The above standard provides also guidelines for the heat treatment required to obtain the right mechanical properties
It should be pointed out that there are a number of proprietary grades that are not listed here.

Martensitic stainless steels, especially those with a High Carbon ( i.e. above about 0.4%) are mostly used for cutting tools: Cutlery, Razor blades, Blender blades, etc..

The graph below represents the position of a number of common martensitic stainless steels grades used for cutting tools.

For cutting ability, a high hardness is required.

The higher the PREN the higher the corrosion resistance.

Replacing some of the Carbon in martensitic stainless steels by Nitrogen is a fairly recent development. The limited solubility of Nitrogen has been increased by the PESR process (Pressure Electroslag Refining) in which melting is carried out under a high nitrogen pressure. Up to 0.4% N contents have been achieved leading to higher hardness/strength and higher corrosion resistance. As the PESR is expensive, lower but significant N contents have been achived using the standard AOD process.

They are magnetic. They are not as corrosion resistant as the common ferritic and austenitic stainless steels due to their low chromium content. . 

Duplex stainless steels have a mixed microstructure of austenite and ferrite, the aim usually being to produce a 50/50 mix, although in commercial alloys the ratio may be 40/60. They are characterized by high chromium (19–32%) and molybdenum (up to 5%) and lower nickel contents than austenitic stainless steels. Duplex stainless steels have roughly twice the strength compared to austenitic stainless steels. Their mixed microstructure provides improved resistance to chloride stress corrosion cracking in comparison to austenitic stainless steels Types 304 and 316.

The properties of duplex stainless steels are achieved with an overall lower alloy content than similar-performing super-austenitic grades, making their use cost-effective for many applications. Duplex grades are characterized into groups based on their alloy content and corrosion resistance.

Mechanical properties from European Standard EN 10088-3 (2014) (for product thickness below 160mm):

Precipitation hardening stainless steels have corrosion resistance comparable to austenitic varieties, but can be precipitation hardened to even higher strengths than the other martensitic grades. The most common, 17-4PH, uses about 17% chromium and 4% nickel.

The designation "CRES" is used in various industries to refer to corrosion-resistant steel. Most mentions of CRES refer to stainless steel, although the correspondence is not absolute, because there are other materials that are corrosion-resistant but not stainless steel.

There are over 150 grades of stainless steel, of which 15 are most commonly used. There are a number of systems for grading stainless and other steels, including US SAE steel grades.

Standard mill finishes can be applied to flat rolled stainless steel directly by the rollers and by mechanical abrasives. Steel is first rolled to size and thickness and then annealed to change the properties of the final material. Any oxidation that forms on the surface (mill scale) is removed by pickling, and a passivation layer is created on the surface. A final finish can then be applied to achieve the desired aesthetic appearance.

Stainless steel is used for buildings for both practical and aesthetic reasons. Stainless steel was in vogue during the art deco period. The most famous example of this is the upper portion of the Chrysler Building (pictured). Some diners and fast-food restaurants use large ornamental panels and stainless fixtures and furniture. Because of the durability of the material, many of these buildings still retain their original appearance. Stainless steel is used today in building construction because of its durability and because it is a weldable building metal that can be made into aesthetically pleasing shapes. An example of a building in which these properties are exploited is the Art Gallery of Alberta in Edmonton, which is wrapped in stainless steel.

Type 316 stainless is used on the exterior of both the Petronas Twin Towers and the Jin Mao Building, two of the world's tallest skyscrapers.

The Parliament House of Australia in Canberra has a stainless steel flagpole weighing over .

The aeration building in the Edmonton Composting Facility, the size of 14 hockey rinks, is the largest stainless steel building in North America.

The Helix Bridge is a pedestrian bridge linking Marina Centre with Marina South in the Marina Bay area in Singapore.


Stainless steel is a modern trend for roofing material for airports due to its low glare reflectance to keep pilots from being blinded, also for its properties that allow thermal reflectance in order to keep the surface of the roof close to ambient temperature. The Hamad International Airport in Qatar was built with all stainless steel roofing for these reasons, as well as the Sacramento International Airport in California.

Stainless steels have a long history of application in contact with water due to their excellent corrosion resistance. Applications include a range of conditions from plumbing, potable and waste water treatment to desalination. Types 304 and 316 stainless steels are standard materials of construction in contact with water. However, with increasing chloride contents higher alloyed stainless steels such as Type 2205 and super austenitic and super duplex stainless steels are utilized.

Important considerations to achieve optimum corrosion performance are:

Stainless steels are used extensively in the Pulp and Paper industry for two primary reasons, to avoid iron contamination of the product and their corrosion resistance to the various chemicals used in the paper making process.

A wide range stainless steels are used throughout the paper making process. For example, duplex stainless steels are being used in digesters to convert wood chips into wood pulp. 6% Mo superaustenitics are used in the bleach plant and Type 316 is used extensively in the paper machine.

Stainless steels are used extensively in these industries for their corrosion resistance to both aqueous, gaseous and high temperature environments, their mechanical properties at all temperatures from cryogenic to the very high, and occasionally for other special physical properties.

Austenitic (300 series) stainless steel, in particular Type 304 and 316, is the material of choice for the Food & Beverage industry. Stainless steels do not affect the taste of the product, they are easily cleaned and sterilized to prevent bacterial contamination of the food, and they are durable.

Stainless steels are used extensively in:
Acidic foods with high salt additions, such as tomato sauce, and highly salted condiments, such as soya sauce may require higher alloyed stainless steels such as 6% Mo superaustenitics to prevent pitting corrosion by chloride.

The Allegheny Ludlum Corporation worked with Ford on various concept cars with stainless steel bodies from the 1930s through the 1970s to demonstrate the material's potential. The 1957 and 1958 Cadillac Eldorado Brougham had a stainless steel roof. In 1981 and 1982, the DeLorean DMC-12 production automobile used Type-304 stainless steel body panels over a glass-reinforced plastic monocoque. Intercity buses made by Motor Coach Industries are partially made of stainless steel. The aft body panel of the Porsche Cayman model (2-door coupe hatchback) is made of stainless steel. It was discovered during early body prototyping that conventional steel could not be formed without cracking (due to the many curves and angles in that automobile). Thus, Porsche was forced to use stainless steel on the Cayman.

Some automotive manufacturers use stainless steel as decorative highlights in their vehicles.

Rail cars have commonly been manufactured using corrugated stainless steel panels (for additional structural strength). This was particularly popular during the 1960s and 1970s, but has since declined. One notable example was the early Pioneer Zephyr. Notable former manufacturers of stainless steel rolling stock included the Budd Company (USA), which has been licensed to Japan's Tokyu Car Corporation, and the Portuguese company Sorefame. Many railcars in the United States are still manufactured with stainless steel, unlike other countries who have shifted away.

Budd also built two airplanes, the Budd BB-1 Pioneer and the Budd RB-1 Conestoga, of stainless steel tube and sheet. The first, which had fabric wing coverings, is on display at the Franklin Institute, being the longest continuous display of an aircraft ever, since 1934. The RB-2 Was almost all stainless steel, save for the control surfaces. One survives at the Pima Air & Space Museum, adjacent to Davis–Monthan Air Force Base.

The American Fleetwings Sea Bird amphibious aircraft of 1936 was also built using a spot-welded stainless steel hull.

Due to its thermal stability, the Bristol Aeroplane Company built the all-stainless steel Bristol 188 high-speed research aircraft, which first flew in 1963. However, the practical problems encountered meant that Concorde employed aluminium alloys.

The use of stainless steel in mainstream aircraft is hindered by its excessive weight compared to other materials, such as aluminium.

Surgical tools and medical equipment are usually made of stainless steel, because of its durability and ability to be sterilized in an autoclave. In addition, surgical implants such as bone reinforcements and replacements (e.g. hip sockets and cranial plates) are made with special alloys formulated to resist corrosion, mechanical wear, and biological reactions "in vivo".

Stainless steel is used in a variety of applications in dentistry. It is common to use stainless steel in many instruments that need to be sterilized, such as needles, endodontic files in root canal therapy, metal posts in root canal–treated teeth, temporary crowns and crowns for deciduous teeth, and arch wires and brackets in orthodontics. The surgical stainless steel alloys (e.g., 316 low-carbon steel) have also been used in some of the early dental implants.

Stainless steels are extensively used in all manner of power stations, from nuclear to solar. Furthermore, stainless steels are ideally suited as mechanical supports for power generation units when the permeation of gases or liquids are required, such as filters in cooling water or hot gas clean up or as structural supports in electrolytic power generation.

Stainless steel is often preferred for kitchen sinks because of its ruggedness, durability, heat resistance, and ease of cleaning. In better models, acoustic noise is controlled by applying resilient undercoating to dampen vibrations. The material is also used for cladding of surfaces such as appliances and backsplashes.

Cookware and bakeware may be clad in stainless steels, to enhance their cleanability and durability, and to permit their use in induction cooking (this requires a magnetic grade of stainless steel, such as 432). Because stainless steel is a poor conductor of heat, it is often used as a thin surface cladding over a core of copper or aluminium, which conduct heat more readily.

Cutlery is normally stainless steel, for low corrosion, ease of cleaning, negligible toxicity, as well as not flavoring the food by electrolytic activity.

Stainless steel is used for jewelry and watches, with 316L being the type commonly used for such applications. Oxidizing stainless steel briefly gives it radiant colors that can also be used for coloration effects.
Valadium, a stainless steel and 12% nickel alloy is used to make class and military rings. Valadium is usually silver-toned, but can be electro-plated to give it a gold tone. The gold tone variety is known as Sun-lite Valadium. Other "Valadium" types of alloy are trade-named differently, with such names as "Siladium" and "White Lazon".

Some firearms incorporate stainless steel components as an alternative to blued or parkerized steel. Some handgun models, such as the Smith & Wesson Model 60 and the Colt M1911 pistol, can be made entirely from stainless steel. This gives a high-luster finish similar in appearance to nickel plating. Unlike plating, the finish is not subject to flaking, peeling, wear-off from rubbing (as when repeatedly removed from a holster), or rust when scratched.

Some 3D printing providers have developed proprietary stainless steel sintering blends for use in rapid prototyping. One of the more popular stainless steel grades used in 3D printing is 316L stainless steel. Due to the high temperature gradient and fast rate of solidification, stainless steel products manufactured via 3D printing tend to have a more refined microstructure; this in turn results in better mechanical properties. However, stainless steel is not used as much as materials like Ti6Al4V in the 3D printing industry; this is because manufacturing stainless steel products via traditional methods is currently much more economically competitive.

Stainless steel is 100% recyclable. An average stainless steel object is composed of about 60% recycled material of which approximately 40% originates from end-of-life products and about 60% comes from manufacturing processes. According to the International Resource Panel's Metal Stocks in Society report, the per capita stock of stainless steel in use in society is 80–180 kg in more developed countries and 15 kg in less-developed countries.

There is a secondary market that recycles usable scrap for many stainless steel markets. The product is mostly coil, sheet, and blanks. This material is purchased at a less-than-prime price and sold to commercial quality stampers and sheet metal houses. The material may have scratches, pits, and dents but is made to the current specifications.

Stainless steel nanoparticles have been produced in the laboratory. This synthesis uses oxidative Kirkendall diffusion to build a thin protective barrier which prevent further oxidation. These may have applications as additives for high performance applications. For examples, sulfurization, phosphorization and nitridation treatments to produce nanoscale stainless steel based catalysts could enhance the electrocatalytic performance of stainless steel for water splitting.

Stainless steel is generally considered to be biologically inert, but some sensitive individuals develop a skin irritation due to a nickel allergy caused by certain alloys.

Stainless steel leaches small amounts of nickel and chromium during cooking.



</doc>
<doc id="27060" url="https://en.wikipedia.org/wiki?curid=27060" title="Stig Anderson">
Stig Anderson

Stig Erik Leopold Anderson (25 January 1931 – 12 September 1997) better known as Stikkan Anderson, was a Swedish entrepreneur, music manager, music executive, lyricist, music publisher and actor who was the co-founder of Polar Music, and best known for managing the Swedish pop band ABBA.

Stig Anderson was born on 25 January 1931 in Hova, Sweden, the son of a single mother named Ester. Anderson began his career as a chemistry and mathematics teacher at a primary school, by taking night classes after leaving school at the age of 15. Having written his first song at age 16, Anderson soon entered the Swedish popular music scene, becoming a music producer, manager and occasional performer. 
Anderson had begun writing songs as early as 1951, and in 1959 he got his breakthrough with the song ""Är du kär i mig ännu, Klas-Göran?"" ("Are You Still in Love With Me, Klas-Göran?"), written for Swedish singer Lill-Babs. During the 1960s he was one of Sweden's most prolific songwriters, producing more than 3,000 published titles. Anderson also founded Sweden Music in 1960, as well as several other companies. Three years later, Anderson co-founded Polar Music with Bengt Bernhag. 

By the late 1960s, he was the manager and producer of Björn Ulvaeus and Benny Andersson, who would become the two main songwriters of ABBA. He joined the careers of these two, after previously managing the Hootenanny Singers, of which Ulvaeus was a member. Later, in 1972, he began managing Anni-Frid Lyngstad, and finally in 1976 Agnetha Fältskog (until December 1975, Fältskog was still bound to Cupol/CBS Records under a contract). Prior to ABBA's formation, Björn and Benny were living in a Volkswagen bus and had decided to leave music and gain regular jobs; however, Anderson persuaded them to start a new band, investing considerable money in the group.

Before his time with ABBA, Anderson managed some of the biggest Swedish artists at that time, and had a huge number of hits on the Swedish charts. His success earned him the nickname "The Business" since he often had several artists in the Top 10 at any time with whom he had written, published, and recorded the songs. When requiring fresh ideas, Anderson would travel to New York City, buy songs that had been American hits, and then translate or transcribe the lyrics on the return journey ready for a recording session shortly after and then have the record on the shelves within a few days. Some songs were sent to IFPI/ASCAP for copyright infringement.

In the early stages of ABBA, Anderson co-wrote many of the songs' lyrics, among them some of the band's biggest hits, such as "Ring Ring" (1973), "Waterloo" (1974), "Honey, Honey" (1974), "I Do, I Do, I Do, I Do, I Do" (1975), "Mamma Mia" (1975), "S.O.S" (1975), "Fernando" (1976), "Dancing Queen" (1976), "Knowing Me, Knowing You" (1977), and "The Name of the Game" (1977).

Frequently known as the fifth member of ABBA, besides contributing to the lyrics, he also owned the record label and the publishing company. Anderson gave shares in the company as gifts to Benny Andersson, Björn Ulvaeus and Michael B. Tretow out of friendship as the group became highly successful. Tretow worked on the mixing board and was the sound engineer for the group. According to PolarmusicPrize.org, Anderson's most treasured award was "Billboard" Magazine's "Trendsetter Award". Such a distinction had only been presented to one European before Anderson, and that was Brian Epstein, manager of The Beatles.

Anderson was one of the dominant figures behind ABBA, representing their commercial interests and global success through successful record deals. At the same time, he also managed the investment of funds and the enormous financial incomes of Polar Music, holding the majority of stocks. This was an agreement dating back to 1974, and a great deal of the money came from individual record deals he struck for the group, including a ground-breaking agreement for record sales in the Soviet Union in which ABBA recordings were released in exchange for barrels of oil. In the mid-1980s, a considerable part of ABBA's fortune was lost by mismanagement, bad investments, high demands for tax, and the rise of credit rates.

The contract with the performers, as well as the international distribution, ran from a standard publishing and recording deal involving identical contracts, rather than from one written specifically with the performers of the band. This led to problems later, when three of the four ABBA members terminated their relationship with Anderson when it was revealed that Anderson had used this contract to take a percentage of profits at a value of 4.5 million euros over the course of many years. A complaint against Anderson was submitted to the Stockholm District Court in June 1990 by Agnetha Fältskog's company "Agnetha Fältskog Produktion AB", Benny Andersson's company "Mono Music AB", as well as a Dutch company holding Björn Ulvaeus's rights. The dispute was eventually settled out of court in July 1991; the terms of the settlement remain undisclosed.

In 1982, Anni-Frid Lyngstad had sold all the shares in the Polar Music company given to her by Anderson, as she moved abroad. She remains the only member never to seek legal recourse for past royalty fees and was not involved in any way with the legal proceedings against Anderson.

In 1989, Anderson made a substantial financial endowment to found the Polar Music Prize from money he made when he sold the multimillion-dollar record company Polar Records. In this deal nearly all utilisation and license rights, and the registered ABBA trademark, were sold for an unknown sum of money to PolyGram shortly before the ABBA members took him to court over royalty back payments. Previously, Anderson had licensed ABBA and the members' solo releases to various labels worldwide as a way to earn more royalties. In 1998, PolyGram was in turn sold to Seagram and merged into what is now one of the Big Three record labels, the Universal Music Group, the company that today holds the rights to the entire ABBA back catalogue.

Stig Anderson was married to Gudrun Anderson (née Rystedt, 1931-2010), and had two sons, Anders and Lasse, and a daughter Marie. His daughter, Marie Ledin (born in 1957, wife of Swedish star and ABBA concert backing vocalist Tomas Ledin) was also involved in the music industry. In the mid-1980s she started a new, highly successful record label called "Record Station" (sold to German BMG in the early 1990s), followed by "Anderson Records". "Anderson Records" released Anni-Frid Lyngstad's Swedish comeback album "Djupa andetag" in 1996, as well as Michael B. Tretow's "Greatest Hits" in 1999.

On 12 September 1997, at the age of 66, Stig Anderson died of a heart attack. He also had severe alcohol problems in the later years of his life. His funeral was broadcast live by Sveriges Television, an honour generally only reserved for distinguished statesmen or royalty.



</doc>
<doc id="27061" url="https://en.wikipedia.org/wiki?curid=27061" title="Soft drink">
Soft drink

A soft drink (see terminology for other names) is a drink that typically contains carbonated water (although some lemonades are not carbonated), a sweetener, and a natural or artificial flavoring. The sweetener may be sugar, high-fructose corn syrup, fruit juice, sugar substitutes (in the case of diet drinks), or some combination of these. Soft drinks may also contain caffeine, colorings, preservatives, and other ingredients.

Soft drinks are called "soft" in contrast with "hard" alcoholic drinks. Small amounts of alcohol may be present in a soft drink, but the alcohol content must be less than 0.5% of the total volume if the drink is to be considered non-alcoholic. Fruit punch, tea, and other such non-alcoholic drinks are technically soft drinks by this definition but are not generally referred to as such.

Soft drinks may be served chilled, over ice cubes or at room temperature. They are available in many formats, including cans, glass bottles, and plastic bottles (the latter in a variety of sizes ranging from small bottles to large 2-liter containers). Soft drinks are also widely available at fast food restaurants, movie theaters, convenience stores, casual dining restaurants, dedicated soda stores, and bars from soda fountain machines. Soda fountain drinks are typically served in paper or plastic disposable cups in the first three venues. In casual dining restaurants and bars, soft drinks are often served in glasses. Soft drinks may be drunk with straws or sipped directly from the cups.

Soft drinks are mixed with other ingredients in several contexts. In Western countries, in bars and other places where alcohol is served (e.g., airplanes, restaurants and nightclubs) many mixed drinks are made by blending a soft drink with hard liquor and serving the drink over ice. One well-known example is the rum and coke, which may also contain lime juice. Some homemade fruit punch recipes, which may or may not contain alcohol, contain a mixture of various fruit juices and soda pop (e.g., ginger ale). At ice cream parlours and 1950s-themed diners, ice cream floats are often sold.

While the term "soft drink" is commonly used in product labeling and on restaurant menus, in many countries these drinks are more commonly referred to by regional names, including carbonated drink, cool drink, cold drink, fizzy drink, fizzy juice, lolly water, pop, seltzer, soda, coke, soda pop, tonic, and mineral. Due to the high sugar content in typical soft drinks, they may also be called sugary drinks.

In the United States, the 2003 Harvard Dialect Survey tracked the usage of the nine most common names. Over half of the survey respondents preferred the term "soda", which was dominant in the Northeastern United States, California, and the areas surrounding Milwaukee and St. Louis. The term "pop", which was preferred by 25% of the respondents, was most popular in the Midwest and Pacific Northwest, while the genericized trademark "coke", used by 12% of the respondents, was most popular in the Southern United States. The term "tonic" is hyperlocal to eastern Massachusetts, although usage is declining.

In the English-speaking parts of Canada, the term "pop" is prevalent, but "soft drink" is the most common English term used in Montreal.

In the United Kingdom and Ireland, the terms "fizzy drink" and the genericized trademark "coke" are common. "Pop" and "fizzy pop" are used in northern England, while "mineral" or "lemonade" (as a general term) are used in Ireland. In Scotland, "fizzy juice" or even simply "juice" is colloquially encountered. In Australia and New Zealand, "fizzy drink" or "soft drink" is typically used. In South African English, "cool drink" and "cold drink" are used, but in South African Indian English, "cool drink" is most prevalent. Older people often use the term 'mineral'.

In Spanish, speakers often use the word "refresco", meaning 'refreshment'. Given its carbonated content, it is also commonly called "gaseosa", from "agua gaseosa".

The origins of soft drinks lie in the development of fruit-flavored drinks. In the medieval Middle East, a variety of fruit-flavoured soft drinks were widely drunk, such as sharbat, and were often sweetened with ingredients such as sugar, syrup and honey. Other common ingredients included lemon, apple, pomegranate, tamarind, jujube, sumac, musk, mint and ice. Middle-Eastern drinks later became popular in medieval Europe, where the word "syrup" was derived from Arabic. In Tudor England, 'water imperial' was widely drunk; it was a sweetened drink with lemon flavor and containing cream of tartar. 'Manays Cryste' was a
sweetened cordial flavored with rosewater, violets or cinnamon.

Another early type of soft drink was lemonade, made of water and lemon juice sweetened with honey, but without carbonated water. The "Compagnie des Limonadiers" of Paris was granted a monopoly for the sale of lemonade soft drinks in 1676. Vendors carried tanks of lemonade on their backs and dispensed cups of the soft drink to Parisians.

In the late 18th century, scientists made important progress in replicating naturally carbonated mineral waters. In 1767, Englishman Joseph Priestley first discovered a method of infusing water with carbon dioxide to make carbonated water when he suspended a bowl of distilled water above a beer vat at a local brewery in Leeds, England. His invention of carbonated water (also known as soda water) is the major and defining component of most soft drinks.

Priestley found that water treated in this manner had a pleasant taste, and he offered it to his friends as a refreshing drink. In 1772, Priestley published a paper entitled "Impregnating Water with Fixed Air" in which he describes dripping "oil of vitriol" (or sulfuric acid as it is now called) onto chalk to produce carbon dioxide gas, and encouraging the gas to dissolve into an agitated bowl of water.

Another Englishman, John Mervin Nooth, improved Priestley's design and sold his apparatus for commercial use in pharmacies. Swedish chemist Torbern Bergman invented a generating apparatus that made carbonated water from chalk by the use of sulfuric acid. Bergman's apparatus allowed imitation mineral water to be produced in large amounts. Swedish chemist Jöns Jacob Berzelius started to add flavors (spices, juices, and wine) to carbonated water in the late eighteenth century.

Thomas Henry, an apothecary from Manchester, was the first to sell artificial mineral water to the general public for medicinal purposes, beginning in the 1770s. His recipe for 'Bewley's Mephitic Julep' consisted of 3 drachms of fossil alkali to a quart of water, and the manufacture had to 'throw in streams of fixed air until all the alkaline taste is destroyed'.

Johann Jacob Schweppe developed a similar process to manufacture carbonated mineral water at the same time. He founded the Schweppes Company in Geneva in 1783 to sell carbonated water, and relocated his business to London in 1792. His drink soon gained in popularity; among his new found patrons was Erasmus Darwin. In 1843, Schweppes commercialised Malvern Water at the Holywell Spring in the Malvern Hills, and was appointed the official supplier to the Royal Family.

It was not long before flavoring was combined with carbonated water. The earliest reference to carbonated ginger beer is in a "Practical Treatise on Brewing". published in 1809. The drinking of either natural or artificial mineral water was considered at the time to be a healthy practice, and was promoted by advocates of temperance. Pharmacists selling mineral waters began to add herbs and chemicals to unflavored mineral water. They used birch bark (see birch beer), dandelion, sarsaparilla, fruit extracts, and other substances. Flavorings were also added to improve the taste.

Soft drinks soon outgrew their origins in the medical world and became a widely consumed product, available cheaply for the masses. By the 1840s there were more than fifty soft drink manufacturers – an increase from just ten in the previous decade. Carbonated lemonade was widely available in British refreshment stalls in 1833, and in 1845 R. White's Lemonade went on sale in the UK. For the Great Exhibition of 1851 in London, Schweppes was designated the official drink supplier and sold over a million bottles of lemonade, ginger beer, Seltzer water and soda-water. There was a Schweppes soda water fountain, situated directly at the entrance to the exhibition.

Mixer drinks became popular in the second half of the century. Tonic water was originally quinine added to water as a prophylactic against malaria and was consumed by British officials stationed in the tropical areas of South Asia and Africa. As the quinine powder was so bitter people began mixing the powder with soda and sugar, and a basic tonic water was created. The first commercial tonic water was produced in 1858. The mixed drink gin and tonic also originated in British colonial India, when the British population would mix their medicinal quinine tonic with gin.
A persistent problem in the soft drinks industry was the lack of an effective sealing of the bottles. Carbonated drink bottles are under great pressure from the gas, so inventors tried to find the best way to prevent the carbon dioxide or bubbles from escaping. The bottles could also explode if the pressure was too great. Hiram Codd devised a patented bottling machine while working at a small mineral water works in the Caledonian Road, Islington, in London in 1870. His Codd-neck bottle was designed to enclose a marble and a rubber washer in the neck. The bottles were filled upside down, and pressure of the gas in the bottle forced the marble against the washer, sealing in the carbonation. The bottle was pinched into a special shape to provide a chamber into which the marble was pushed to open the bottle. This prevented the marble from blocking the neck as the drink was poured.

By mid-1873 he had granted 20 licences and received a further 50 applications. This was boosted further by a Trade Show held in London in the same year. By 1874 the licence was free to bottle manufacturers as long as they purchased the marbles, sealing rings and used his groove tool, and the mineral water firms they traded with had already bought a licence to use his bottle.

In 1892, the "Crown Cork Bottle Seal" was patented by William Painter, a Baltimore, Maryland machine shop operator. It was the first bottle top to successfully keep the bubbles in the bottle. In 1899, the first patent was issued for a glass-blowing machine for the automatic production of glass bottles. Earlier glass bottles had all been hand-blown. Four years later, the new bottle-blowing machine was in operation. It was first operated by the inventor, Michael Owens, an employee of Libby Glass Company. Within a few years, glass bottle production increased from 1,400 bottles a day to about 58,000 bottles a day.

In America, soda fountains were initially more popular, and many Americans would frequent the soda fountain daily. Beginning in 1806, Yale University chemistry professor Benjamin Silliman sold soda waters in New Haven, Connecticut. He used a Nooth apparatus to produce his waters. Businessmen in Philadelphia and New York City also began selling soda water in the early 19th century. In the 1830s, John Matthews of New York City and John Lippincott of Philadelphia began manufacturing soda fountains. Both men were successful and built large factories for fabricating fountains. Due to problems in the U.S. glass industry, bottled drinks remained a small portion of the market throughout much of the 19th century. (However, they were known in England. In "The Tenant of Wildfell Hall", published in 1848, the caddish Huntingdon, recovering from months of debauchery, wakes at noon and gulps a bottle of soda-water.)

In the early 20th century, sales of bottled soda increased exponentially, and in the second half of the 20th century, canned soft drinks became an important share of the market.

During the 1920s, "Home-Paks" were invented. "Home-Paks" are the familiar six-pack cartons made from cardboard. Vending machines also began to appear in the 1920s. Since then, soft drink vending machines have become increasingly popular. Both hot and cold drinks are sold in these self-service machines throughout the world.

Soft drinks are made by mixing dry or fresh ingredients with water. Production of soft drinks can be done at factories or at home. Soft drinks can be made at home by mixing a syrup or dry ingredients with carbonated water, or by lacto-fermentation. Syrups are commercially sold by companies such as Soda-Club; dry ingredients are often sold in pouches, in a style of the popular U.S. drink mix Kool-Aid. Carbonated water is made using a soda siphon or a home carbonation system or by dropping dry ice into water. Food-grade carbon dioxide, used for carbonating drinks, often comes from ammonia plants.

Drinks like ginger ale and root beer are often brewed using yeast to cause carbonation.

Of most importance is that the ingredient meets the agreed specification on all major parameters. This is not only the functional parameter (in other words, the level of the major constituent), but the level of impurities, the microbiological status, and physical parameters such as color, particle size, etc.

Some soft drinks contain measurable amounts of alcohol. In some older preparations, this resulted from natural fermentation used to build the carbonation. In the United States, soft drinks (as well as other products such as non-alcoholic beer) are allowed by law to contain up to 0.5% alcohol by volume. Modern drinks introduce carbon dioxide for carbonation, but there is some speculation that alcohol might result from fermentation of sugars in a non-sterile environment. A small amount of alcohol is introduced in some soft drinks where alcohol is used in the preparation of the flavoring extracts such as vanilla extract.

In every area of the world there are major soft drink producers. However, a few major North American companies are present in most of the countries of the world, such as Pepsi and Coca Cola. Major North American producers other than the two previously-named companies include Cott, Dr Pepper Snapple Group, and Jones Soda.

The over-consumption of sugar-sweetened soft drinks is associated with obesity, hypertension, type 2 diabetes, dental caries, and low nutrient levels. Experimental studies tend to support a causal role for sugar-sweetened soft drinks in these ailments, though this is challenged by other researchers. "Sugar-sweetened" includes drinks that use high-fructose corn syrup, as well as those using sucrose.

Many soft drinks contain ingredients that are themselves sources of concern: caffeine is linked to anxiety and sleep disruption when consumed in excess, and some critics question the health effects of added sugars and artificial sweeteners. Sodium benzoate has been investigated by researchers at University of Sheffield as a possible cause of DNA damage and hyperactivity. Other substances have negative health effects, but are present in such small quantities that they are unlikely to pose any substantial health risk provided that the drinks are consumed only in moderation.

In 1998, the Center for Science in the Public Interest published a report titled "Liquid Candy: How Soft Drinks are Harming Americans' Health". The report examined statistics relating to the increase in soft drink consumption and claimed that consumption is "likely contributing to health problems". It also criticized marketing efforts by soft drink companies. In 2005, the CSPI called for warning labels on soft drinks, similar to those on cigarettes and alcohol.

From 1977 to 2002, Americans doubled their consumption of sweetened beverages—a trend that was paralleled by doubling the prevalence of obesity. The consumption of sugar-sweetened beverages is associated with weight and obesity, and changes in consumption can help predict changes in weight.

It remains possible that the correlation is due to a third factor: people who lead unhealthy lifestyles might consume more soft drinks. If so, then the association between soft drink consumption and weight gain could reflect the consequences of an unhealthy lifestyle rather than the consequences of consuming soft drinks. Experimental evidence is needed to definitively establish the causal role of soft drink consumption. Reviews of the experimental evidence suggest that soft drink consumption does cause weight gain, but the effect is often small except for overweight individuals.

Many of these experiments examined the influence of sugar-sweetened soft drinks on weight gain in children and adolescents. In one experiment, adolescents replaced sugar-sweetened soft drinks in their diet with artificially sweetened soft drinks that were sent to their homes over 25 weeks. Compared with children in a control group, children who received the artificially sweetened drinks saw a smaller increase in their BMI (by −0.14 kg/m), but this effect was only statistically significant among the heaviest children (who saw a benefit of −0.75 kg/m). In another study, an educational program encouraged schoolchildren to consume fewer soft drinks. During the school year, the prevalence of obesity decreased among children in the program by 0.2%, compared to a 7.5% increase among children in the control group. Another study, published in Pediatrics in 2013, concluded that for children from the age of 2 to 5, their risk of obesity increased by 43% if they were regular soft drink consumers as opposed to those who rarely or never consumed them.

Sugar-sweetened drinks have also been speculated to cause weight gain in adults. In one study, overweight individuals consumed a daily supplement of sucrose-sweetened or artificially sweetened drinks or foods for a 10-week period. Most of the supplement was in the form of soft drinks. Individuals in the sucrose group gained 1.6 kg, and individuals in the artificial-sweetener group lost 1.0 kg. A two-week study had participants supplement their diet with sugar-sweetened soft drinks, artificially sweetened soft drinks, or neither. Although the participants gained the most weight when consuming the sugar-sweetened drinks, some of the differences were unreliable: the differences between men who consumed sugar-sweetened drinks or no drinks was not statistically significant.

Other research suggests that soft drinks could play a special role in weight gain. One four-week experiment compared a 450 calorie/day supplement of sugar-sweetened soft drinks to a 450 calorie/day supplement of jelly beans. The jelly bean supplement did not lead to weight gain, but the soft drink supplement did. The likely reason for the difference in weight gain is that people who consumed the jelly beans lowered their caloric intake at subsequent meals, while people who consumed soft drinks did not. Thus, the low levels of satiety provided by sugar-sweetened soft drinks may explain their association with obesity. That is, people who consume calories in sugar-sweetened drinks may fail to adequately reduce their intake of calories from other sources. Indeed, people consume more total calories in meals and on days when they are given sugar-sweetened drinkss than when they are given artificially sweetened drinks or water. However, these results are contradicted by a study by Adam Drewnowski published in 2004, in which "32 subjects consumed a 300-calorie snack of fat-free raspberry cookies or regular cola on two occasions each – either two hours ("early") or 20 minutes ("late") before lunch." It found that "...the calories eaten at lunch were not affected by whether the snack was cookies or cola."

The consumption of sugar-sweetened soft drinks can also be associated with many weight-related diseases, including diabetes, metabolic syndrome and cardiovascular risk factors, and elevated blood pressure.

According to research presented at the American Heart Association's Epidemiology and Prevention/Nutrition, Physical Activity and Metabolism 2013 Scientific Sessions by researchers at the Harvard School of Public Health, sugar-sweetened beverages may be responsible for 180,000 deaths every year worldwide.

Most soft drinks contain high concentrations of simple carbohydrates: glucose, fructose, sucrose and other simple sugars. If oral bacteria ferment carbohydrates and produce acids that may dissolve tooth enamel and induce dental decay, then sweetened drinks may increase the risk of dental caries. The risk would be greater if the frequency of consumption is high.

A large number of soda pops are acidic as are many fruits, sauces and other foods. Drinking acidic drinks over a long period and continuous sipping may erode the tooth enamel. A 2007 study determined that some flavored sparkling waters are as erosive or more so than orange juice.

Using a drinking straw is often advised by dentists as the drink does not come into as much contact with the teeth. It has also been suggested that brushing teeth right after drinking soft drinks should be avoided as this can result in additional erosion to the teeth due to the presence of acid.

There have been a handful of published reports describing individuals with severe hypokalemia (low potassium levels) related to extreme consumption of colas.

In a meta-analysis of 88 studies, drinking soda correlates with a decrease in milk consumption along with the vitamin D, vitamin B6, vitamin B12, calcium, protein and other micronutrients. Phosphorus, a micronutrient, can be found in cola-type drinks, but there may be a risk in consuming too much. Phosphorus and calcium are used in the body to create calcium-phosphate, which is the main component of bone. However, the combination of too much phosphorus with too little calcium in the body can lead to a degeneration of bone mass.
Research suggests a statistically significant inverse relationship between consumption of carbonated drinks and bone mineral density in young girls, which places them at increased risk of fractures.

One hypothesis to explain this relationship is that the phosphoric acid contained in some soft drinks (colas) displaces calcium from the bones, lowering bone density of the skeleton and leading to weakened bones, or osteoporosis. However, 2001 calcium metabolism studies by Dr. Robert Heaney suggested that the net effect of carbonated soft drinks, (including colas, which use phosphoric acid as the acidulant) on calcium excretion in urine was negligible. Heaney concluded that carbonated soft drinks, which do not contain the nutrients needed for bone health, may displace other foods which do, and that the real issue is that people who drink a lot of soft drinks also tend to have an overall diet that is low in calcium.

A 2006 study of several thousand men and women, found that women who regularly drank cola-based sodas (three or more a day) had significantly lower bone mineral density (BMD) of ~4 % in the hip than those who didn't, even though researchers controlled for important factors like calcium and vitamin D intake. The study also found that women who drank non-cola soft drinks didn't appear to have lower BMD and that BMD of women drinking decaffeineted cola wasn't as low as women drinking caffeinated cola sodas. The study found that the effect of regular consumption of cola sodas was not significant on men's BMD.

In the 1950s and 1960s there were attempts in France and Japan to ban the sale of Coca-Cola as dangerous since phosphates can block calcium absorption. However, these were unsuccessful as the amounts of phosphate were shown to be too small to have a significant effect.

The USDA's recommended daily intake (RDI) of added sugars is less than 10 teaspoons per day for a 2,000-calorie diet. High caloric intake contributes to obesity if not balanced with exercise, with a large amount of exercise being required to offset even small but calorie-rich food and drinks.

Until 1985, most of the calories in soft drinks came from sugar or corn syrup. As of 2010, in the United States high-fructose corn syrup (HFCS) is used nearly exclusively as a sweetener because of its lower cost, while in Europe, sucrose dominates, because EU agricultural policies favor production of sugar beets in Europe proper and sugarcane in the former colonies over the production of corn. HFCS has been criticized as having a number of detrimental effects on human health, such as promoting diabetes, hyperactivity, hypertension, and a host of other problems. Although anecdotal evidence has been presented to support such claims, it is well known that the human body breaks sucrose down into glucose and fructose before it is absorbed by the intestines. Simple sugars such as fructose are converted into the same intermediates as in glucose metabolism. However, metabolism of fructose is extremely rapid and is initiated by fructokinase. Fructokinase activity is not regulated by metabolism or hormones and proceeds rapidly after intake of fructose. While the intermediates of fructose metabolism are similar to those of glucose, the rates of formation are excessive. This fact promotes fatty acid and triglyceride synthesis in the liver, leading to accumulation of fat throughout the body and possibly non-alcoholic fatty liver disease. Increased blood lipid levels also seem to follow fructose ingestion over time. A sugar drink or high-sugar drink may refer to any drink consisting primarily of water and sugar (often cane sugar or high-fructose corn syrup), including some soft drinks, some fruit juices, and energy drinks.

In 2006, the United Kingdom Food Standards Agency published the results of its survey of benzene levels in soft drinks, which tested 150 products and found that four contained benzene levels above the World Health Organization (WHO) guidelines for drinking water.

The United States Food and Drug Administration released its own test results of several soft drinks containing benzoates and ascorbic or erythorbic acid. Five tested drinks contained benzene levels above the Environmental Protection Agency's recommended standard of 5 ppb. The Environmental Working Group has uncovered additional FDA test results that showed the following results: Of 24 samples of diet soda tested between 1995 and 2001 for the presence of benzene, 19 (79%) had amounts of benzene in excess of the federal tap water standard of 5 ppb. Average benzene levels were 19 ppb, about four times tap water standard. One sample contained 55 ppb of benzene, 11 fold tap water standards. Despite these findings, as of 2006, the FDA stated its belief that "the levels of benzene found in soft drinks and other beverages to date do not pose a safety concern for consumers".

In 2003, the Delhi non-profit Centre for Science and Environment published a disputed report finding pesticide levels in Coke and Pepsi soft drinks sold in India at levels 30 times that considered safe by the European Economic Community. This was found in primarily 12 cold drink brands sold in and around New Delhi. The Indian Health Minister said the CSE tests were inaccurate, and said that the government's tests found pesticide levels within India's standards but above EU standards.

A similar CSE report in August 2006 prompted many state governments to have issued a ban of the sale of soft drinks in schools. Kerala issued a complete ban on the sale or manufacture of soft drinks altogether. (These were later struck down in court.) In return, the soft drink companies like Coca-Cola and Pepsi have issued ads in the media regarding the safety of consumption of the drinks.

The UK-based Central Science Laboratory, commissioned by Coke, found its products met EU standards in 2006. Coke and the University of Michigan commissioned an independent study of its bottling plants by The Energy and Resources Institute (TERI), which reported in 2008 no unsafe chemicals in the water supply used.

A study published in the "Clinical Journal of the American Society of Nephrology" in 2013 concluded that consumption of soft drinks was associated with a 23% higher risk of developing kidney stones.

Since at least 2006, debate on whether high-calorie soft drink vending machines should be allowed in schools has been on the rise. Opponents of the (soft drink) vending machines believe that soft drinks are a significant contributor to childhood obesity and tooth decay, and that allowing soft drink sales in schools encourages children to believe they are safe to consume in moderate to large quantities. Opponents argue that schools have a responsibility to look after the health of the children in their care, and that allowing children easy access to soft drinks violates that responsibility. Vending machine proponents believe that obesity is a complex issue and soft drinks are not the only cause. A 2011 bill to tax soft drinks in California failed, with some opposing lawmakers arguing that parents—not the government—should be responsible for children's drink choices.

On May 3, 2006, the Alliance for a Healthier Generation, Cadbury Schweppes, The Coca-Cola Company, PepsiCo, and the American Beverage Association announced new guidelines that will voluntarily remove high-calorie soft drinks from all U.S. schools.

On May 19, 2006, the British education secretary, Alan Johnson, announced new minimum nutrition standards for school food. Among a wide range of measures, from September 2006, school lunches will be free from carbonated drinks. Schools will also end the sale of junk food (including carbonated drinks) in vending machines and tuck shops.

In 2008, Samantha K Graff published an article in the "Annals of the American Academy of Political and Social Science" regarding the "First Amendment Implications of Restricting Food and Beverages Marketing in Schools". The article examines a school district's policy regarding limiting the sale and marketing of soda in public schools, and how certain policies can invoke a violation of the First Amendment. Due to district budget cuts and loss in state funding, many school districts allow commercial businesses to market and advertise their product (including junk food and soda) to public school students for additional revenue. Junk food and soda companies have acquired exclusive rights to vending machines throughout many public school campuses. Opponents of corporate marketing and advertising on school grounds urge school officials to restrict or limit a corporation's power to promote, market, and sell their product to school students. In the 1970s, the Supreme Court ruled that advertising was not a form of free expression, but a form of business practices which should be regulated by the government. In the 1976 case of "Virginia State Board of Pharmacy v. Virginia Citizens Consumer Council", the Supreme Court ruled that advertising, or "commercial speech", to some degree is protected under the First Amendment. To avoid a First Amendment challenge by corporations, public schools could create contracts that restrict the sale of certain product and advertising. Public schools can also ban the selling of all food and drink products on campus, while not infringing on a corporation's right to free speech.

On December 13, 2010, President Obama signed the Healthy Hunger Free Kids Act of 2010 (effective in 2014) that mandates schools that receive federal funding must offer healthy snacks and drinks to students. The act bans the selling of soft drinks to students and requires schools to provide healthier options such as water, unflavored low-fat milk, 100% fruit and vegetable drinks or sugar-free carbonated drinks. The portion sizes available to students will be based on age: eight ounces for elementary schools, twelve ounces for middle and high schools. Proponents of the act predict the new mandate it will make it easier for students to make healthy drink choices while at school.

In 2015, Terry-McElarth and colleagues published a study in the "American Journal of Preventative Medicine" on regular soda policies and their effect on school drink availability and student consumption. The purpose of the study was to determine the effectiveness of a program beginning in the 2014–2015 school year that requires schools participating in federally reimbursable meal programs to remove all competitive venues (a la carte cafeteria sales, vending machines, and stores/snack bars/carts), on the availability of unhealthy drinks at schools and student consumption. The study analyzed state- and school district-level policies mandating soda bans and found that state bans were associated with significantly lower school soda availability but district bans showed no significant associations. In addition, no significant correlation was observed between state policies and student consumption. Among student populations, state policy was directly associated with significantly lower school soda availability and indirectly associated with lower student consumption. The same was not observed for other student populations.

In the United States, legislators, health experts and consumer advocates are considering levying higher taxes on the sale of soft drinks and other sweetened products to help curb the epidemic of obesity among Americans, and its harmful impact on overall health. Some speculate that higher taxes could help reduce soda consumption. Others say that taxes should help fund education to increase consumer awareness of the unhealthy effects of excessive soft drink consumption, and also help cover costs of caring for conditions resulting from overconsumption. The food and drink industry holds considerable clout in Washington, DC, as it has contributed more than $50 million to legislators since 2000.

In January 2013, a British lobby group called for the price of sugary fizzy drinks to be increased, with the money raised (an estimated £1 billion at 20p per litre) to be put towards a "Children's Future Fund", overseen by an independent body, which would encourage children to eat healthily in school.

In 2017, the United Arab Emirates imposed a 50% tax on soft drinks and energy drinks to curb excess consumption of the commodity and for additional revenue.

In March 2013, New York City's mayor Michael Bloomberg proposed to ban the sale of non-diet soft drinks larger than 16 ounces, except in convenience stores and supermarkets. A lawsuit against the ban was upheld by a state judge, who voiced concerns that the ban was "fraught with arbitrary and capricious consequences". Bloomberg announced that he would be appealing the verdict.




</doc>
<doc id="27062" url="https://en.wikipedia.org/wiki?curid=27062" title="Sockerdricka">
Sockerdricka

Sockerdricka (Swedish for: "sugar drink") is a soft drink dating from the 19th century. Originally it was brewed and also contained ginger, but nowadays it consists of carbonated water, sugar, citric acid, and flavorants. It is quite similar to Fruktsoda. It is still sold, despite strong competition from more modern inventions such as 7 Up and Sprite. Sockerdricka is a popular base for many cocktails.


</doc>
<doc id="27064" url="https://en.wikipedia.org/wiki?curid=27064" title="Steve Crocker">
Steve Crocker

Stephen D. Crocker (born October 15, 1944, in Pasadena, California) is the inventor of the Request for Comments series, authoring the very first RFC and many more. He received his bachelor's degree (1968) and PhD (1977) from the University of California, Los Angeles. Crocker is chair of the board of the Internet Corporation for Assigned Names and Numbers, ICANN.

Steve Crocker has worked in the Internet community since its inception. As a UCLA graduate student in the 1960s, he was part of the team that developed the protocols for the ARPANET which were the foundation for today's Internet. For this work, Crocker was awarded the 2002 IEEE Internet Award.

While at UCLA Crocker taught an extension course on computer programming (for the IBM 7094 mainframe computer). The class was intended to teach digital processing and assembly language programming to high school teachers, so that they could offer such courses in their high schools. A number of high school students were also admitted to the course, to ensure that they would be able to understand this new discipline. Crocker was also active in the newly formed UCLA Computer Club.

Crocker has been a program manager at Defense Advanced Research Projects Agency (DARPA), a senior researcher at USC's Information Sciences Institute, founder and director of the Computer Science Laboratory at The Aerospace Corporation and a vice president at Trusted Information Systems. In 1994, Crocker was one of the founders and chief technology officer of CyberCash, Inc. In 1998, he founded and ran Executive DSL, a DSL-based ISP. In 1999 he cofounded and was CEO of Longitude Systems. He is currently CEO of Shinkuro, a research and development company.

Steve Crocker was instrumental in creating the ARPA "Network Working Group", which later was the context in which the IETF was created.

He has also been an IETF security area director, a member of the Internet Architecture Board, chair of the ICANN Security and Stability Advisory Committee, a board member of the Internet Society and numerous other Internet-related volunteer positions.

In 2012, Crocker was inducted into the Internet Hall of Fame by the Internet Society.



</doc>
<doc id="27065" url="https://en.wikipedia.org/wiki?curid=27065" title="Standardization">
Standardization

Standardization or standardisation is the process of implementing and developing technical standards based on the consensus of different parties that include firms, users, interest groups, standards organizations and governments Standardization can help to maximize compatibility, interoperability, safety, repeatability, or quality. It can also facilitate commoditization of formerly custom processes. In social sciences, including economics, the idea of "standardization" is close to the solution for a coordination problem, a situation in which all parties can realize mutual gains, but only by making mutually consistent decisions. This view includes the case of "spontaneous standardization processes", to produce de facto standards.

Standard weights and measures were developed by the Indus Valley Civilisation. The centralised weight and measure system served the commercial interest of Indus merchants as smaller weight measures were used to measure luxury goods while larger weights were employed for buying bulkier items, such as food grains etc. Weights existed in multiples of a standard weight and in categories. Technical standardisation enabled gauging devices to be effectively used in angular measurement and measurement for construction. Uniform units of length were used in the planning of towns such as Lothal, Surkotada, Kalibangan, Dolavira, Harappa, and Mohenjo-daro. The weights and measures of the Indus civilisation also reached Persia and Central Asia, where they were further modified. Shigeo Iwata describes the excavated weights unearthed from the Indus civilisation:

The implementation of standards in industry and commerce became highly important with the onset of the Industrial Revolution and the need for high-precision machine tools and interchangeable parts.

Henry Maudslay developed the first industrially practical screw-cutting lathe in 1800. This allowed for the standardisation of screw thread sizes for the first time and paved the way for the practical application of interchangeability (an idea that was already taking hold) to nuts and bolts.

Before this, screw threads were usually made by chipping and filing (that is, with skilled freehand use of chisels and files). Nuts were rare; metal screws, when made at all, were usually for use in wood. Metal bolts passing through wood framing to a metal fastening on the other side were usually fastened in non-threaded ways (such as clinching or upsetting against a washer). Maudslay standardized the screw threads used in his workshop and produced sets of taps and dies that would make nuts and bolts consistently to those standards, so that any bolt of the appropriate size would fit any nut of the same size. This was a major advance in workshop technology.

Maudslay's work, as well as the contributions of other engineers, accomplished a modest amount of industry standardization; some companies' in-house standards spread a bit within their industries.
Joseph Whitworth's screw thread measurements were adopted as the first (unofficial) national standard by companies around the country in 1841. It came to be known as the British Standard Whitworth, and was widely adopted in other countries.

This new standard specified a 55° thread angle and a thread depth of 0.640327"p" and a radius of 0.137329"p", where "p" is the pitch. The thread pitch increased with diameter in steps specified on a chart. An example of the use of the Whitworth thread is the Royal Navy's Crimean War gunboats. These were the first instance of "mass-production" techniques being applied to marine engineering.

With the adoption of BSW by British railway lines, many of which had previously used their own standard both for threads and for bolt head and nut profiles, and improving manufacturing techniques, it came to dominate British manufacturing.

American Unified Coarse was originally based on almost the same imperial fractions. The Unified thread angle is 60° and has flattened crests (Whitworth crests are rounded). Thread pitch is the same in both systems except that the thread pitch for the  in bolt is 12 threads per inch (tpi) in BSW versus 13 tpi in the UNC.

By the end of the 19th century, differences in standards between companies, was making trade increasingly difficult and strained. For instance, an iron and steel dealer recorded his displeasure in "The Times": "Architects and engineers generally specify such unnecessarily diverse types of sectional material or given work that anything like economical and continuous manufacture becomes impossible. In this country no two professional men are agreed upon the size and weight of a girder to employ for given work."

The Engineering Standards Committee was established in London in 1901 as the world's first national standards body. It subsequently extended its standardization work and became the British Engineering Standards Association in 1918, adopting the name British Standards Institution in 1931 after receiving its Royal Charter in 1929. The national standards were adopted universally throughout the country, and enabled the markets to act more rationally and efficiently, with an increased level of cooperation.

After the First World War, similar national bodies were established in other countries. The Deutsches Institut für Normung was set up in Germany in 1917, followed by its counterparts, the American National Standard Institute and the French Commission Permanente de Standardisation, both in 1918.

By the mid to late 19th century, efforts were being made to standardize electrical measurement. Lord Kelvin was an important figure in this process, introducing accurate methods and apparatus for measuring electricity. In 1857, he introduced a series of effective instruments, including the quadrant electrometer, which cover the entire field of electrostatic measurement. He invented the current balance, also known as the "Kelvin balance" or "Ampere balance" ("SiC"), for the precise specification of the ampere, the standard unit of electric current.
Another important figure was R. E. B. Crompton, who became concerned by the large range of different standards and systems used by electrical engineering companies and scientists in the early 20th century. Many companies had entered the market in the 1890s and all chose their own settings for voltage, frequency, current and even the symbols used on circuit diagrams. Adjacent buildings would have totally incompatible electrical systems simply because they had been fitted out by different companies. Crompton could see the lack of efficiency in this system and began to consider proposals for an international standard for electric engineering.

In 1904, Crompton represented Britain at the International Electrical Congress, held in connection with Louisiana Purchase Exposition in Saint Louis as part of a delegation by the Institute of Electrical Engineers. He presented a paper on standardisation, which was so well received that he was asked to look into the formation of a commission to oversee the process. By 1906 his work was complete and he drew up a permanent constitution for the first international standards organization, the International Electrotechnical Commission. The body held its first meeting that year in London, with representatives from 14 countries. In honour of his contribution to electrical standardisation, Lord Kelvin was elected as the body's first President.
The International Federation of the National Standardizing Associations (ISA) was founded in 1926 with a broader remit to enhance international cooperation for all technical standards and specifications. The body was suspended in 1942 during World War II.

After the war, ISA was approached by the recently formed United Nations Standards Coordinating Committee (UNSCC) with a proposal to form a new global standards body. In October 1946, ISA and UNSCC delegates from 25 countries met in London and agreed to join forces to create the new International Organization for Standardization (ISO); the new organization officially began operations in February 1947.

In general, each country or economy has a single recognized National Standards Body (NSB). Examples include ABNT, AENOR, AFNOR, ANSI, BSI, DGN, DIN, IRAM, JISC, KATS, SABS, SAC, SCC, SIS. An NSB is likely the sole member from that economy in ISO.

NSBs may be either public or private sector organizations, or combinations of the two. For example, the three NSBs of Canada, Mexico and the United States are respectively the Standards Council of Canada (SCC), the General Bureau of Standards (, DGN), and the American National Standards Institute (ANSI). SCC is a Canadian Crown Corporation, DGN is a governmental agency within the Mexican Ministry of Economy, and ANSI and AENOR are a 501(c)(3) non-profit organization with members from both the private and public sectors. The determinants of whether an NSB for a particular economy is a public or private sector body may include the historical and traditional roles that the private sector fills in public affairs in that economy or the development stage of that economy.

Standards can be:

The existence of a published standard does not necessarily imply that it is useful or correct. Just because an item is stamped with a standard number does not, by itself, indicate that the item is fit for any particular use. The people who use the item or service (engineers, trade unions, etc.) or specify it (building codes, government, industry, etc.) have the responsibility to consider the available standards, specify the correct one, enforce compliance, and use the item correctly: validation and verification.

Standardization is implemented greatly when companies release new products to market. Compatibility is important for products to be successful; this allows consumers to use their new items along with what they already own.

In the context of social criticism and social science, standardization often means the process of establishing standards of various kinds and improving efficiency to handle people, their interactions, cases, and so forth. Examples include formalization of judicial procedure in court, and establishing uniform criteria for diagnosing mental disease. Standardization in this sense is often discussed along with (or synonymously to) such large-scale social changes as modernization, bureaucratization, homogenization, and centralization of society.

In the context of information exchange, standardization refers to the process of developing standards for specific business processes using specific formal languages. These standards are usually developed in voluntary consensus standards bodies such as the United Nations Center for Trade Facilitation and Electronic Business (UN/CEFACT), the World Wide Web Consortium W3C, the Telecommunications Industry Association (TIA), and the Organization for the Advancement of Structured Information Standards (OASIS).

There are many specifications that govern the operation and interaction of devices and software on the Internet, but they are rarely referred to as standards, so as to preserve that word as the domain of relatively disinterested bodies such as ISO. The W3C, for example, publishes "Recommendations", and the IETF publishes "Requests for Comments" (RFCs). However, these publications are sometimes referred to as standards.

In the context of customer service, standardization refers to the process of developing an international standard that enables organizations to focus on customer service, while at the same time providing recognition of success through a third party organization, such as the British Standards Institution. An international standard has been developed by The International Customer Service Institute.

In the context of supply chain management and materials management, standardization covers the process of specification and use of any item the company must buy in or make, allowable substitutions, and build or buy decisions.

In the context of defense, standardization has been defined by NATO as "The development and implementation of concepts, doctrines, procedures and designs to achieve and maintain the required levels of compatibility, interchangeability or commonality in the operational, procedural, material, technical and administrative fields to attain interoperability."

The process of standardization can itself be standardized. There are at least four levels of standardization: compatibility, interchangeability, commonality and reference. These standardization processes create compatibility, similarity, measurement and symbol standards.

There are typically four different techniques for standardization

Types of standardization process:

Standardization/ Standardisation has a variety of benefits and drawbacks for firms and consumers participating in the market, and on technology and innovation.

The primary effect of standardization on firms is that the basis of competition is shifted from integrated systems to individual components within the system. Prior to standardization a company's product must span the entire system because individual components from different competitors are incompatible, but after standardization each company can focus on providing an individual component of the system. When the shift toward competition based on individual components takes place, firms selling tightly integrated systems must quickly shift to a modular approach, supplying other companies with subsystems or components.

Standardization has a variety of benefits for consumers, but one of the greatest benefits is enhanced network effects. Standards increase compatibility and interoperability between products, allowing information to be shared within a larger network and attracting more consumers to use the new technology, further enhancing network effects. Other benefits of standardization to consumers are reduced uncertainty, because consumers can be more certain that they are not choosing the wrong product, and reduced lock-in, because the standard makes it more likely that there will be competing products in the space. Consumers may also get the benefit of being able to mix and match components of a system to align with their specific preferences. Once these initial benefits of standardization are realized, further benefits that accrue to consumers as a result of using the standard are driven mostly by the quality of the technologies underlying that standard.

Probably the greatest downside of standardization for consumers is lack of variety. There is no guarantee that the chosen standard will meet all consumers' needs or even that the standard is the best available option. Another downside is that if a standard is agreed upon before products are available in the market, then consumers are deprived of the penetration pricing that often results when rivals are competing to rapidly increase market share in an attempt to increase the likelihood that their product will become the standard. It is also possible that a consumer will choose a product based upon a standard that fails to become dominant. In this case, the consumer will have spent resources on a product that is ultimately less useful to him or her as the result of the standardization process.

Much like the effect on consumers, the effect of standardization on technology and innovation is mixed. Meanwhile, the various links between research and standardization have been identified, also as a platform of knowledge transfer and translated into policy measures (e.g. WIPANO).

Increased adoption of a new technology as a result of standardization is important because rival and incompatible approaches competing in the marketplace can slow or even kill the growth of the technology (a state known as market fragmentation). The shift to a modularized architecture as a result of standardization brings increased flexibility, rapid introduction of new products, and the ability to more closely meet individual customer's needs.

The negative effects of standardization on technology have to do with its tendency to restrict new technology and innovation. Standards shift competition from features to price because the features are defined by the standard. The degree to which this is true depends on the specificity of the standard. Standardization in an area also rules out alternative technologies as options while encouraging others.





</doc>
<doc id="27067" url="https://en.wikipedia.org/wiki?curid=27067" title="Sub-Saharan Africa">
Sub-Saharan Africa

Sub-Saharan Africa is, geographically, the area of the continent of Africa that lies south of the Sahara. According to the United Nations, it consists of all African countries that are fully or partially located south of the Sahara. It contrasts with North Africa, whose territories are part of the League of Arab states within the Arab world. Somalia, Djibouti, Comoros and Mauritania are geographically in Sub-Saharan Africa, but are likewise Arab-speaking states and part of the Arab world.

The Sahel is the transitional zone in between the Sahara and the tropical savanna of the Sudan region and farther south the forest-savanna mosaic of tropical Africa.

Since probably 3500 BCE, the Saharan and Sub-Saharan regions of Africa have been separated by the extremely harsh climate of the sparsely populated Sahara, forming an effective barrier interrupted by only the Nile in Sudan, though the Nile was blocked by the river's cataracts. The Sahara pump theory explains how flora and fauna (including "Homo sapiens") left Africa to penetrate the Middle East and beyond. African pluvial periods are associated with a Wet Sahara phase, during which larger lakes and more rivers existed.

The use of the term has been criticized because it refers to the South only by cartography conventions and projects a connotation of inferiority; a vestige of colonialism, which some say, divided Africa into European terms of homogeneity.

Geographers historically divided the region into several distinct ethnographic sections based on each area's respective inhabitants.

Commentators in Arabic in the medieval period used the general term "bilâd as-sûdân" ("Land of the Blacks") for the vast Sudan region (an expression denoting West and Central Africa), or sometimes extending from the coast of West Africa to Western Sudan. Its equivalent in Southeast Africa was "Zanj" ("Country of the Blacks"), which was situated in the vicinity of the Great Lakes region.

The geographers drew an explicit ethnographic distinction between the Sudan region and its analogue Zanj, from the area to their extreme east on the Red Sea coast in the Horn of Africa. In modern-day Ethiopia and Eritrea was "Al-Habash" or Abyssinia, which was inhabited by the "Habash" or Abyssinians, who were the forebears of the Habesha. In northern Somalia was "Barbara" or the "Bilad al-Barbar" ("Land of the Berbers"), which was inhabited by the Eastern "Baribah" or "Barbaroi", as the ancestors of the Somalis were referred to by medieval Arab and ancient Greek geographers, respectively.

In the 19th and 20th centuries, the populations south of the Sahara were divided into three broad ancestral groups: Hamites and Semites in the Horn of Africa and Sahel related to those in North Africa, who spoke languages belonging to the Afroasiatic family; Negroes in most of the rest of the subcontinent (hence, the former toponym "Black Africa" for Tropical Africa), who spoke languages belonging to the Niger-Congo and Nilo-Saharan families; and Khoisan in Southern Africa, who spoke languages belonging to the Khoisan family.

Sub-Saharan Africa has a wide variety of climate zones or biomes. South Africa and the Democratic Republic of the Congo in particular are considered Megadiverse countries. It has a dry winter season and a wet summer season.


According to paleontology, early hominid skull anatomy was similar to that of their close cousins, the great African forest apes, gorilla and chimpanzee. However, they had adopted a bipedal locomotion and freed hands, giving them a crucial advantage enabling them to live in both forested areas and on the open savanna at a time when Africa was drying up, with savanna encroaching on forested areas. This occurred 10 million to 5 million years ago.

By 3 million years ago several australopithecine hominid species had developed throughout southern, eastern and central Africa. They were tool users rather than tool manufacturers. The next major evolutionary step occurred around 2.3 million BCE, when primitive stone tools were used to scavenge the carcasses of animals killed by other predators, both for their meat and their marrow. In hunting, "H. habilis" was most likely not capable of competing with large predators and was more prey than hunter, although "H. habilis" probably did steal eggs from nests and may have been able to catch small game and weakened larger prey such as cubs and older animals. The tools were classed as Oldowan.

Roughly 1.8 million years ago, "Homo ergaster" first appeared in the fossil record in Africa. From "Homo ergaster", "Homo erectus" (upright man) evolved 1.5 million years ago. Some of the earlier representatives of this species were small-brained and used primitive stone tools, much like "H. habilis". The brain later grew in size, and "H. erectus" eventually developed a more complex stone tool technology called the Acheulean. Potentially the first hominid to engage in hunting, "H. erectus" mastered the art of making fire. They were the first hominids to leave Africa, going on to colonize the entire Old World, and perhaps later on giving rise to "Homo floresiensis". Although some recent writers suggest that "H. georgicus", a "H. habilis" descendant, was the first and most primitive hominid to ever live outside Africa, many scientists consider "H. georgicus" to be an early and primitive member of the "H. erectus" species.

The fossil record shows "Homo sapiens" lived in southern and eastern Africa anywhere from 100,000 to 150,000 years ago. Between 50,000 and 60,000 years ago, their expansion out of Africa launched the colonization of the planet by modern humans. By 10,000 BCE, "Homo sapiens" had spread to all corners of the world. This dispersal of the human species is suggested by linguistic, cultural and genetic evidence.

After the Sahara became a desert, it did not present a totally impenetrable barrier for travelers between north and south because of the application of animal husbandry towards carrying water, food, and supplies across the desert. Prior to the introduction of the camel, the use of oxen, mule, and horses for desert crossing was common, and trade routes followed chains of oases that were strung across the desert. The trans-saharan trade was in full motion by 500 BCE with Carthage being a major economic force for its establishment. It is thought that the camel was first brought to Egypt after the Persian Empire conquered Egypt in 525 BCE, although large herds did not become common enough in North Africa for camels to be the pack animal of choice for the trans-saharan trade.

Archaeological finds in Central Africa provide evidence of human settlement that may date back over 10 000 years. According to Zangato and Holl, there is evidence of iron-smelting in the Central African Republic and Cameroon that may date back to 3000 to 2500 BCE. Extensive walled sites and settlements have recently been found in Zilum, Chad. The area is located approximately southwest of Lake Chad, and has been radiocarbon dated to the first millennium BCE.

Trade and improved agricultural techniques supported more sophisticated societies, leading to the early civilizations of Sao, Kanem, Bornu, Shilluk, Baguirmi, and Wadai.

Following the Bantu Migration into Central Africa, during the 14th century, the Luba Kingdom in southeast Congo came about under a king whose political authority derived from religious, spiritual legitimacy. The kingdom controlled agriculture and regional trade of salt and iron from the north and copper from the Zambian/Congo copper belt.

Rival kingship factions which split from the Luba Kingdom later moved among the Lunda people, marrying into its elite and laying the foundation of the Lunda Empire in the 16th century. The ruling dynasty centralised authority among the Lunda under the Mwata Yamyo or Mwaant Yaav. The Mwata Yamyo's legitimacy, like that of the Luba king, came from being viewed as a spiritual religious guardian. This imperial cult or system of divine kings was spread to most of central Africa by rivals in kingship migrating and forming new states. Many new states received legitimacy by claiming descent from the Lunda dynasties.

The Kingdom of Kongo existed from the Atlantic west to the Kwango river to the east. During the 15th century, the Bakongo farming community was united with its capital at M'banza-Kongo, under the king title, Manikongo. Other significant states and peoples included the Kuba Kingdom, producers of the famous raffia cloth, the Eastern Lunda, Bemba, Burundi, Rwanda, and the Kingdom of Ndongo.

The Axumite Empire spanned the southern Sahara, south Arabia and the Sahel along the western shore of the Red Sea. Located in northern Ethiopia and Eritrea, Aksum was deeply involved in the trade network between India and the Mediterranean. Growing from the proto-Aksumite Iron Age period circa the 4th century BCE, it rose to prominence by the 1st century CE. The Aksumites constructed monolithic stelae to cover the graves of their kings, such as King Ezana's Stele. The later Zagwe dynasty, established in the 12th century, built churches out of solid rock. These rock-hewn structures include the Church of St. George at Lalibela.

In ancient Somalia, city-states flourished such as Opone, Mosyllon and Malao that competed with the Sabaeans, Parthians and Axumites for the wealthy Indo–Greco–Roman trade.

In the Middle Ages, several powerful Somali empires dominated the regional trade including the Ajuran Sultanate, which excelled in hydraulic engineering and fortress building, the Sultanate of Adal, whose General Ahmed Gurey was the first African commander in history to use cannon warfare on the continent during Adal's conquest of the Ethiopian Empire, and the Geledi Sultanate, whose military dominance forced governors of the Omani empire north of the city of Lamu to pay tribute to the Somali Sultan Ahmed Yusuf. In the late 19th century after the Berlin conference had ended, European empires sailed with their armies to the Horn of Africa. The imperial armies in Somalia alarmed the Dervish leader Mohammed Abdullah Hassan, who gathered Somali soldiers from across the Horn of Africa and began one of the longest anti-colonial wars known as the Somaliland Campaign.

Settlements of Bantu-speaking peoples, who were iron-using agriculturists and herdsmen, were already present south of the Limpopo River by the 4th or 5th century displacing and absorbing the original Khoisan speakers. They slowly moved south, and the earliest ironworks in modern-day KwaZulu-Natal Province are believed to date from around 1050. The southernmost group was the Xhosa people, whose language incorporates certain linguistic traits from the earlier Khoisan inhabitants. They reached the Fish River in today's Eastern Cape Province.

Monomotapa was a medieval kingdom (c. 1250–1629), which existed between the Zambezi and Limpopo rivers of Southern Africa in the territory of modern-day Zimbabwe and Mozambique. Its old capital was located at Great Zimbabwe.

In 1487, Bartolomeu Dias became the first European to reach the southernmost tip of Africa. In 1652, a victualling station was established at the Cape of Good Hope by Jan van Riebeeck on behalf of the Dutch East India Company. For most of the 17th and 18th centuries, the slowly expanding settlement was a Dutch possession.

Great Britain seized the Cape of Good Hope area in 1795, ostensibly to prevent it from falling into the hands of the French but also to use Cape Town in particular as a stop on the route to Australia and India. It was later returned to the Dutch in 1803, but soon afterwards the Dutch East India Company declared bankruptcy, and the British annexed the Cape Colony in 1806.

The Zulu Kingdom was a Southern African tribal state in what is now KwaZulu-Natal in southeastern South Africa. The small kingdom gained world fame during and after the Anglo-Zulu War.

During the 1950s and early 1960s, most Sub-Saharan African nations achieved independence from colonial rule.

According to the theory of recent African origin of modern humans, the mainstream position held within the scientific community, all humans originate from either Southeast Africa or the Horn of Africa. During the first millennium CE, Nilotic and Bantu-speaking peoples moved into the region, and the latter now account for three-quarters of Kenya's population.

On the coastal section of Southeast Africa, a mixed Bantu community developed through contact with Muslim Arab and Persian traders, leading to the development of the mixed Arab, Persian and African Swahili City States. The Swahili culture that emerged from these exchanges evinces many Arab and Islamic influences not seen in traditional Bantu culture, as do the many Afro-Arab members of the Bantu Swahili people. With its original speech community centered on the coastal parts of Tanzania (particularly Zanzibar) and Kenya a seaboard referred to as the Swahili Coast the Bantu Swahili language contains many Arabic loan-words as a consequence of these interactions.

The earliest Bantu inhabitants of the Southeast coast of Kenya and Tanzania encountered by these later Arab and Persian settlers have been variously identified with the trading settlements of Rhapta, Azania and Menouthias referenced in early Greek and Chinese writings from 50 CE to 500 CE, ultimately giving rise to the name for Tanzania. These early writings perhaps document the first wave of Bantu settlers to reach Southeast Africa during their migration.

Between the 14th and 15th centuries, large medieval Southeast African kingdoms and states emerged, such as the Buganda and Karagwe kingdoms of Uganda and Tanzania.

During the early 1960s, the Southeast African nations achieved independence from colonial rule.

Nubia in present-day Northern Sudan and southernmost of Egypt, was referred to as "Aethiopia" ("land of the burnt face") by the Greeks.

Nubia at her greatest phase is considered Sub-Saharan Africa's oldest urban civilisation. Nubia was a major source of gold for the ancient world. Nubians built famous structures and numerous pyramids. Sudan, the site of ancient Nubia, has more pyramids than anywhere in the world.

The Bantu expansion is a major migration movement originating in West Africa around 2500 BCE, reaching East and Central Africa by 1000 BCE and Southern Africa by the early centuries CE.

The Djenné-Djenno city-state flourished from 250 BCE to 900 CE and was influential to the development of the Ghana Empire.

The Nok culture is known from a type of terracotta figure found in Nigeria, dating to between 500 BCE and 200 CE.

There were a number of medieval empires of the southern Sahara and the Sahel, based on trans-Saharan trade, including the Ghana Empire and the Mali Empire, Songhai Empire, the Kanem Empire and the subsequent Bornu Empire. They built stone structures like in Tichit, but mainly constructed in adobe. The Great Mosque of Djenne is most reflective of Sahelian architecture and is the largest adobe building in the world.

In the forest zone, several states and empires emerged. The Ashanti Empire arose in the 16th century in modern-day Ghana and Ivory Coast. The Kingdom of Nri, was established by the Igbo in the 11th century. Nri was famous for having a priest-king who wielded no military power. Nri was a rare African state which was a haven for freed slaves and outcasts who sought refuge in their territory. Other major states included the kingdoms of Ifẹ and Oyo in the western block of Nigeria which became prominent about 700–900 and 1400 respectively, and center of Yoruba culture. The Yoruba's built massive mud walls around their cities, the most famous being Sungbo's Eredo. Another prominent kingdom in southwestern Nigeria was the Kingdom of Benin 9th–11th century whose power lasted between the 15th and 19th century and was one of the greatest Empires of African history documented all over the world. Their dominance reached as far as the well-known city of Eko which was named Lagos by the Portuguese traders and other early European settlers. The Edo speaking people of Benin are known for their famous bronze casting and rich coral, wealth, ancient science and technology and the Walls of Benin, which is the largest man-made structure in the world.

In the 18th century, the Oyo and the Aro confederacy were responsible for most of the slaves exported from Nigeria, with Great Britain, France and Portugal shipping the majority of the slaves. Following the Napoleonic Wars, the British expanded trade with the Nigerian interior. In 1885, British claims to a West African sphere of influence received international recognition, and in the following year the Royal Niger Company was chartered under the leadership of Sir George Taubman Goldie. In 1900, the company's territory came under the control of the British Government, which moved to consolidate its hold over the area of modern Nigeria. On 1 January 1901, Nigeria became a British protectorate, part of the British Empire, the foremost world power at the time.

By 1960, most of the region achieved independence from colonial rule.

According to , the population of sub-Saharan Africa was in . The current growth rate is 2.3%. The UN predicts for the region a population between 1.5 and 2 billion by 2050 with a population density of 80 per km compared to 170 for Western Europe, 140 for Asia and 30 for the Americas.

Sub-Saharan African countries top the list of countries and territories by fertility rate with 40 of the highest 50, all with TFR greater than 4 in 2008. All are above the world average except South Africa and Seychelles. More than 40% of the population in sub-Saharan countries is younger than 15 years old, as well as in Sudan, with the exception of South Africa.

GDP per Capita "(2006 in dollars (US$))", Life (Exp.) "(Life Expectancy 2006)", Literacy (Male/Female 2006), Trans "(Transparency 2009)", HDI "(Human Development Index)", EODBR "(Ease of Doing Business Rank June 2008 through May 2009)", SAB ("Starting a Business June 2008 through May 2009)", PFI "(Press Freedom Index 2009)"

Sub-Saharan Africa contains over 1,000 languages, which is around 1/6 of the world's total.

With the exception of the extinct Sumerian (a language isolate) of Mesopotamia, Afro-Asiatic has the oldest documented history of any language family in the world. Egyptian was recorded as early as 3200 BCE. The Semitic branch was recorded as early as 2900 BCE in the form of the Akkadian language of Mesopotamia (Assyria and Babylonia) and circa 2500 BCE in the form of the Eblaite language of north eastern Syria.

The distribution of the Afroasiatic languages within Africa is principally concentrated in North Africa and the Horn of Africa. Languages belonging to the family's Berber branch are mainly spoken in the north, with its speech area extending into the Sahel (northern Mauritania, northern Mali, northern Niger). The Cushitic branch of Afroasiatic is centered in the Horn, and is also spoken in the Nile Valley and parts of the African Great Lakes region. Additionally, the Semitic branch of the family, in the form of Arabic, is widely spoken in the parts of Africa that are within the Arab world. South Semitic languages are also spoken in parts of the Horn of Africa (Ethiopia, Eritrea). The Chadic branch is distributed in Central and West Africa. Hausa, its most widely spoken language, serves as a lingua franca in West Africa (Niger, Ghana, Togo, Benin, Cameroon, and Chad).

The several families lumped under the term Khoi-San include languages indigenous to Southern Africa and Tanzania, though some, such as the Khoi languages, appear to have moved to their current locations not long before the Bantu expansion. In Southern Africa, their speakers are the Khoikhoi and San (Bushmen), in Southeast Africa, the Sandawe and Hadza.

The Niger–Congo family is the largest in the world in terms of the number of languages (1,436) it contains. The vast majority of languages of this family are tonal such as Yoruba, and Igbo, However, others such as Fulani and Wolof are not. A major branch of the Niger–Congo languages is Bantu, which covers a greater geographic area than the rest of the family. Bantu speakers represent the majority of inhabitants in southern, central and southeastern Africa, though San, Pygmy, and Nilotic groups, respectively, can also be found in those regions. Bantu-speakers can also be found in parts of Central Africa such as the Gabon, Equatorial Guinea and southern Cameroon. Swahili, a Bantu language with many Arabic, Persian and other Middle Eastern and South Asian loan words, developed as a "lingua franca" for trade between the different peoples in southeastern Africa. In the Kalahari Desert of Southern Africa, the distinct people known as Bushmen (also "San", closely related to, but distinct from "Hottentots") have long been present. The San evince unique physical traits, and are the indigenous people of southern Africa. Pygmies are the pre-Bantu indigenous peoples of Central Africa.

The Nilo-Saharan languages are concentrated in the upper parts of the Chari and Nile rivers of Central Africa and Southeast Africa. They are principally spoken by Nilotic peoples and are also spoken in Sudan among the Fur, Masalit, Nubian and Zaghawa peoples and in West and Central Africa among the Songhai, Zarma and Kanuri. The Old Nubian language is also a member of this family.

Major languages of Africa by region, family and number of primary language speakers in millions:

A 2017 archaeogenetic study of prehistoric fossils in Sub-Saharan Africa observed a wide-ranging early presence of Khoisan populations in the region. Khoisan-related ancestry was inferred to have contributed to two thirds of the ancestry of hunter-gatherer populations inhabiting Malawi between 8,100 and 2,500 years ago and to one third of the ancestry of hunter gatherers inhabiting Tanzania as late as 1,400 years ago. Also in Tanzania, a pastoralist individual was found to carry ancestry related to the pre-pottery Levant. These diverse early ancestries are believed to have been largely replaced after the Bantu expansion into central, eastern and southern Africa.

A 2009 genetic clustering study, which genotyped 1327 polymorphic markers in various African populations, identified six ancestral clusters through Bayesian analysis and fourteen ancestral clusters through STRUCTURE analysis within the continent. The clustering corresponded closely with ethnicity, culture and language.

In addition, whole genome sequencing analysis of modern populations inhabiting Sub-Saharan Africa has observed several primary inferred ancestry components: a Pygmy-related component carried by the Mbuti and Biaka Pygmies in Central Africa, a Khoisan-related component carried by Khoisan-speaking populations in Southern Africa, a Niger-Congo-related component carried by Niger-Congo-speaking populations throughout Sub-Saharan Africa, a Nilo-Saharan-related component carried by Nilo-Saharan-speaking populations in the Nile Valley and African Great Lakes, and a West Eurasian-related component carried by Afroasiatic-speaking populations in the Horn of Africa and Nile Valley.

Sub-Saharan Africa has several large cities. Lagos is a city in the Nigerian state of Lagos. The city, with its adjoining conurbation, is the most populous in Nigeria, and the second most populous on the African continent after Cairo, Egypt. It is one of the fastest growing cities in the world, and also one of the most populous urban agglomerations. Lagos is a major financial centre in Africa; the megacity has the highest GDP, and also houses one of the largest and busiest ports on the continent.

Dar es Salaam is the former capital as well as the most populous city in Tanzania and a regionally important economic centre. It is located on the Swahili coast.

Johannesburg is the largest city in South Africa. It is the provincial capital and largest city in Gauteng, which is the wealthiest province in South Africa. While Johannesburg is not one of South Africa's three capital cities, it is the seat of the Constitutional Court. The city is located in the mineral-rich Witwatersrand range of hills and is the centre of large-scale gold and diamond trade

Nairobi is the capital and the largest city of Kenya. The name comes from the Maasai phrase "Enkare Nyrobi", which translates to "cool water", a reference to the Nairobi River which flows through the city. The city is popularly referred to as the Green City in the Sun.

Other major cities in Sub-Saharan Africa include Abidjan, Cape Town, Kinshasa, Luanda, Mogadishu, Addis Ababa.

In the mid-2010s, private capital flows to Sub-Saharan Africa primarily from the BRICs, private-sector investment portfolios, and remittances began to exceed official development assistance.

As of 2011, Africa is one of the fastest developing regions in the world. Six of the world's ten fastest-growing economies over the previous decade were situated below the Sahara, with the remaining four in East and Central Asia. Between 2011 and 2015, the economic growth rate of the average nation in Africa is expected to surpass that of the average nation in Asia. Sub-Saharan Africa is by then projected to contribute seven out of the ten fastest growing economies in the world. According to the World Bank, the economic growth rate in the region had risen to 4.7% in 2013, with a rate of 5.2% forecasted for 2014. This continued rise was attributed to increasing investment in infrastructure and resources as well as steady expenditure per household.

, fifty percent of Africa is rural with no access to electricity. Africa generates 47 GW of electricity, less than 0.6% of the global market share. Many countries are affected by power shortages.

Because of rising prices in commodities such as coal and oil, thermal sources of energy are proving to be too expensive for power generation. Sub-Saharan Africa is expected to build additional hydropower generation capacity of at least 20,165 MW by 2014. The region has the potential to generate 1,750 TWh of energy, of which only 7% has been explored. The failure to exploit its full energy potential is largely due to significant underinvestment, as at least four times as much (approximately $23 billion a year) and what is currently spent is invested in operating high cost power systems and not on expanding the infrastructure.

African governments are taking advantage of the readily available water resources to broaden their energy mix. Hydro Turbine Markets in Sub-Saharan Africa generated revenues of $120.0 million in 2007 and is estimated to reach $425.0 million. Asian countries, notably China, India, and Japan, are playing an active role in power projects across the African continent. The majority of these power projects are hydro-based because of China's vast experience in the construction of hydro-power projects and part of the Energy & Power Growth Partnership Services programme.

With electrification numbers, Sub-Saharan Africa with access to the Sahara and being in the tropical zones has massive potential for solar photovoltaic electrical potential. Six hundred million people could be served with electricity based on its photovoltaic potential. China is promising to train 10,000 technicians from Africa and other developing countries in the use of solar energy technologies over the next five years. Training African technicians to use solar power is part of the China-Africa science and technology cooperation agreement signed by Chinese science minister Xu Guanhua and African counterparts during premier Wen Jiabao's visit to Ethiopia in December 2003.

The New Partnership for Africa's Development (NEPAD) is developing an integrated, continent-wide energy strategy. This has been funded by, amongst others, the African Development Bank (AfDB) and the EU-Africa Infrastructure Trust Fund. These projects must be sustainable, involve a cross-border dimension and/or have a regional impact, involve public and private capital, contribute to poverty alleviation and economic development, involve at least one country in Sub-Saharan Africa.

Radio is the major source of information in Sub-Saharan Africa. Average coverage stands at more than a third of the population. Countries such as Gabon, Seychelles, and South Africa boast almost 100% penetration. Only five countries Burundi, Djibouti, Eritrea, Ethiopia, and Somalia still have a penetration of less than 10%. Broadband penetration outside of South Africa has been limited where it is exorbitantly expensive. Access to the internet via cell phones is on the rise.

Television is the second major source of information. Because of power shortages, the spread of television viewing has been limited. Eight percent have television, a total of 62 million. But those in the television industry view the region as an untapped green market. Digital television and pay for service are on the rise.

According to researchers at the Overseas Development Institute, the lack of infrastructure in many developing countries represents one of the most significant limitations to economic growth and achievement of the Millennium Development Goals (MDGs). Less than 40% of rural Africans live within two kilometers of an all-season road, the lowest level of rural accessibility in the developing world. Spending on roads averages just below 2% of GDP with varying degree among countries. This compares with 1% of GDP that is typical in industrialised countries, and 2–3% of GDP found in fast-growing emerging economies. Although the level of effort is high relative to the size of Africa's economies, it remains little in absolute terms, with low-income countries spending an average of about US$7 per capita per year. Infrastructure investments and maintenance can be very expensive, especially in such as areas as landlocked, rural and sparsely populated countries in Africa.

Infrastructure investments contributed to Africa's growth, and increased investment is necessary to maintain growth and tackle poverty. The returns to investment in infrastructure are very significant, with on average 30–40% returns for telecommunications (ICT) investments, over 40% for electricity generation and 80% for roads.

In Africa, it is argued that in order to meet the MDGs by 2015 infrastructure investments would need to reach about 15% of GDP (around $93 billion a year). Currently, the source of financing varies significantly across sectors. Some sectors are dominated by state spending, others by overseas development aid (ODA) and yet others by private investors. In Sub-Saharan Africa, the state spends around $9.4 billion out of a total of $24.9 billion. In irrigation, SSA states represent almost all spending; in transport and energy a majority of investment is state spending; in ICT and water supply and sanitation, the private sector represents the majority of capital expenditure. Overall, aid, the private sector and non-OECD financiers between them exceed state spending. The private sector spending alone equals state capital expenditure, though the majority is focused on ICT infrastructure investments. External financing increased from $7 billion (2002) to $27 billion (2009). China, in particular, has emerged as an important investor.

The region is a major exporter to the world of gold, uranium, chromium, vanadium, antimony, coltan, bauxite, iron ore, copper and manganese. South Africa is a major exporter of manganese as well as chromium. A 2001 estimate is that 42% of the world's reserves of chromium may be found in South Africa. South Africa is the largest producer of platinum, with 80% of the total world's annual mine production and 88% of the world's platinum reserve. Sub-Saharan Africa produces 33% of the world's bauxite, with Guinea as the major supplier. Zambia is a major producer of copper. Democratic Republic of Congo is a major source of coltan. Production from Congo is very small but has 80% of proven reserves. Sub-saharan Africa is a major producer of gold, producing up to 30% of global production. Major suppliers are South Africa, Ghana, Zimbabwe, Tanzania, Guinea, and Mali. South Africa had been first in the world in terms of gold production since 1905, but in 2007 it moved to second place, according to GFMS, the precious metals consultancy. Uranium is major commodity from the region. Significant suppliers are Niger, Namibia, and South Africa. Namibia was the number one supplier from Sub-Saharan Africa in 2008. The region produces 49% of the world's diamonds.

By 2015, it is estimated that 25% of North American oil will be from Sub-Saharan Africa, ahead of the Middle East.
Sub-Saharan Africa has been the focus of an intense race for oil by the West, China, India, and other emerging economies, even though it holds only 10% of proven oil reserves, less than the Middle East. This race has been referred to as the second Scramble for Africa. All reasons for this global scramble come from the reserves' economic benefits. Transportation cost is low and no pipelines have to be laid as in Central Asia. Almost all reserves are offshore, so political turmoil within the host country will not directly interfere with operations. Sub-Saharan oil is viscous, with a very low sulfur content. This quickens the refining process and effectively reduces costs. New sources of oil are being located in Sub-Saharan Africa more frequently than anywhere else. Of all new sources of oil, ⅓ are in Sub-Saharan Africa.

Sub-Saharan Africa has more variety of grains than anywhere in the world. Between 13,000 and 11,000 BCE wild grains began to be collected as a source of food in the cataract region of the Nile, south of Egypt. The collecting of wild grains as source of food spread to Syria, parts of Turkey, and Iran by the eleventh millennium BCE. By the tenth and ninth millennia southwest Asians domesticated their wild grains, wheat, and barley after the notion of collecting wild grains spread from the Nile.

Numerous crops have been domesticated in the region and spread to other parts of the world. These crops included sorghum, castor beans, coffee, cotton okra, black-eyed peas, watermelon, gourd, and pearl millet. Other domesticated crops included teff, enset, African rice, yams, kola nuts, oil palm, and raffia palm.

Domesticated animals include the guinea fowl and the donkey.

Agriculture represents 20% to 30% of GDP and 50% of exports. In some cases, 60% to 90% of the labor force are employed in agriculture. Most agricultural activity is subsistence farming. This has made agricultural activity vulnerable to climate change and global warming. Biotechnology has been advocated to create high yield, pest and environmentally resistant crops in the hands of small farmers. The Bill and Melinda Gates foundation is a strong advocate and donor to this cause. Biotechnology and GM crops have met resistance both by natives and environmental groups.

Cash crops include cotton, coffee, tea, cocoa, sugar, and tobacco.

The OECD says Africa has the potential to become an agricultural superbloc if it can unlock the wealth of the savannahs by allowing farmers to use their land as collateral for credit. There is such international interest in Sub-Saharan agriculture, that the World Bank increased its financing of African agricultural programs to $1.3 billion in the 2011 fiscal year. Recently, there has been a trend to purchase large tracts of land in Sub-Sahara for agricultural use by developing countries.
Early in 2009, George Soros highlighted a new farmland buying frenzy caused by growing population, scarce water supplies and climate change. Chinese interests bought up large swathes of Senegal to supply it with sesame. Aggressive moves by China, South Korea and Gulf states to buy vast tracts of agricultural land in Sub-Saharan Africa could soon be limited by a new global international protocol.

Forty percent of African scientists live in OECD countries, predominantly in Europe, the United States and Canada. This has been described as an African brain drain. According to Naledi Pandor, the South African Minister of Science and Technology, even with the drain enrollments in Sub-Saharan African universities tripled between 1991 and 2005, expanding at an annual rate of 8.7%, which is one of the highest regional growth rates in the world. In the last 10 to 15 years interest in pursuing university level degrees abroad has increased. In some OECD countries, like the United States, Sub-Saharan Africans are the most educated immigrant group.

According to the CIA, low global literacy rates are concentrated in Sub-Saharan Africa, West Asia and South Asia. However, the literacy rates in Sub-Saharan Africa vary significantly between countries. The highest registered literacy rate in the region is in Zimbabwe (90.7%; 2003 est.), while the lowest literacy rate is in South Sudan (27%).

Sub-Saharan African countries spent an average of 0.3% of their GDP on science and technology on in 2007. This represents an increase from US$1.8 billion in 2002 to US$2.8 billion in 2007, a 50% increase in spending.

At the World Conference held in Jomtien, Thailand in 1990, delegates from 155 countries and representatives of some 150 organizations gathered with the goal to promote universal primary education and the radical reduction of illiteracy before the end of the decade. The World Education Forum, held ten years later in Dakar, Senegal, provided the opportunity to reiterate and reinforce these goals. This initiative contributed to having education made a priority of the Millennium Development Goals in 2000, with the aim of achieving universal schooling (MDG2) and eliminating gender disparities, especially in primary and secondary education (MDG3). Since the World Education Forum in Dakar, considerable efforts have been made to respond to these demographic challenges in terms of education. The amount of funds raised has been decisive. Between 1999 and 2010, public spending on education as a percentage of gross national product (GNP) increased by 5% per year in sub-Saharan Africa, with major variations between countries, with percentages varying from 1.8% in Cameroon to over 6% in Burundi. As of 2015, governments in sub-Saharan Africa spend on average 18% of their total budget on education, against 15% in the rest of the world.

In the years immediately after the Dakar Forum, the efforts made by African States towards achieving EFA produced multiple results in sub-Saharan Africa. The greatest advance was in access to primary education, which governments had made their absolute priority. The number of children in primary school in sub-Saharan Africa thus rose from 82 million in 1999 to 136.4 million in 2011. In Niger for example, the number of children entering school increased more than three and a half times between 1999 and 2011. In Ethiopia, over the same period, over 8.5 million more children were admitted to primary school. The net rate of first year access in sub-Saharan Africa has thus risen by 19 points in 12 years, from 58% in 1999 to 77% in 2011. Despite the considerable efforts, the latest available data from the UNESCO Institute for Statistics estimates that, for 2012, there were still 57.8 million children who were not in school. Of these, 29.6 million were in sub-Saharan Africa alone, a figure which has not changed for several years. Many sub-Saharan countries have notably included the first year of secondary school in basic education. In Rwanda, the first year of secondary school was attached to primary education in 2009, which significantly increased the number of pupils enrolled at this level of education. In 2012 the primary completion rate (PCR) – which measures the proportion of children reaching the final year of primary school – was 70%, meaning that more than three out of ten children entering primary school do not reach the final primary year.

In 1987, the Bamako Initiative conference organized by the World Health Organization was held in Bamako, the capital of Mali, and helped reshape the health policy of Sub-Saharan Africa. The new strategy dramatically increased accessibility through community-based healthcare reform, resulting in more efficient and equitable provision of services. A comprehensive approach strategy was extended to all areas of health care, with subsequent improvement in the health care indicators and improvement in health care efficiency and cost.

In 2011, Sub-Saharan Africa was home to 69% of all people living with HIV/AIDS worldwide. In response, a number of initiatives have been launched to educate the public on HIV/AIDS. Among these are combination prevention programmes, considered to be the most effective initiative, the abstinence, be faithful, use a condom campaign, and the Desmond Tutu HIV Foundation's outreach programs. According to a 2013 special report issued by the Joint United Nations Programme on HIV/AIDS (UNAIDS), the number of HIV positive people in Africa receiving anti-retroviral treatment in 2012 was over seven times the number receiving treatment in 2005, with an almost 1 million added in the last year alone. The number of AIDS-related deaths in Sub-Saharan Africa in 2011 was 33 percent less than the number in 2005. The number of new HIV infections in Sub-Saharan Africa in 2011 was 25 percent less than the number in 2001.
Malaria is an endemic illness in Sub-Saharan Africa, where the majority of malaria cases and deaths worldwide occur. Routine immunization has been introduced in order to prevent measles. Onchocerciasis ("river blindness"), a common cause of blindness, is also endemic to parts of the region. More than 99% of people affected by the illness worldwide live in 31 countries therein. In response, the African Programme for Onchocerciasis Control (APOC) was launched in 1995 with the aim of controlling the disease. Maternal mortality is another challenge, with more than half of maternal deaths in the world occurring in Sub-Saharan Africa. However, there has generally been progress here as well, as a number of countries in the region have halved their levels of maternal mortality since 1990. Additionally, the African Union in July 2003 ratified the Maputo Protocol, which pledges to prohibit female genital mutilation (FGM).

National health systems vary between countries. In Ghana, most health care is provided by the government and largely administered by the Ministry of Health and Ghana Health Services. The healthcare system has five levels of providers: health posts which are first level primary care for rural areas, health centers and clinics, district hospitals, regional hospitals and tertiary hospitals. These programs are funded by the government of Ghana, financial credits, Internally Generated Fund (IGF), and Donors-pooled Health Fund.

African countries below the Sahara are largely Christian, while those above the Sahara, in North Africa, are predominantly Islamic. There are also Muslim majorities in parts of the Horn of Africa (Djibouti and Somalia) and in the Sahel and Sudan regions (the Gambia, Sierra Leone, Guinea, Mali, Niger and Senegal), as well as significant Muslim communities in Ethiopia and Eritrea, and on the Swahili Coast (Tanzania and Kenya). Mauritius is the only country in Africa to have a Hindu majority.

Traditional African religions can be broken down into linguistic cultural groups, with common themes. Among Niger–Congo-speakers is a belief in a creator God; ancestor spirits; territorial spirits; evil caused by human ill will and neglecting ancestor spirits; priest of territorial spirits. New world religions such as Santería, Vodun, and Candomblé, would be derived from this world view. Among Nilo-Saharan speakers is the belief in Divinity; evil is caused by divine judgement and retribution; prophets as middlemen between Divinity and man. Among Afro-Asiatic-speakers is henotheism, the belief in one's own gods but accepting the existence of other gods; evil here is caused by malevolent spirits. The Semitic Abrahamic religion of Judaism is comparable to the latter world view. San religion is non-theistic but a belief in a Spirit or Power of existence which can be tapped in a trance-dance; trance-healers.

Traditional religions in Sub-Saharan Africa often display complex ontology, cosmology and metaphysics. Mythologies, for example, demonstrated the difficulty fathers of creation had in bringing about order from chaos. Order is what is right and natural and any deviation is chaos. Cosmology and ontology is also neither simple or linear. It defines duality, the material and immaterial, male and female, heaven and earth. Common principles of being and becoming are widespread: Among the Dogon, the principle of "Amma" (being) and "Nummo" (becoming), and among the Bambara, "Pemba" (being) and "Faro" (becoming).





Sub-Saharan traditional divination systems display great sophistication. For example, the bamana sand divination uses well established symbolic codes that can be reproduced using four bits or marks. A binary system of one or two marks are combined. Random outcomes are generated using a fractal recursive process. It is analogous to a digital circuit but can be reproduced on any surface with one or two marks. This system is widespread in Sub-Saharan Africa.

Sub-Saharan Africa is diverse, with many communities, villages and cities, each with their own beliefs and traditions. Traditional African Societies are communal, they believe that the needs of the many far out weigh an individual needs and achievements. Basically, an individual's keep must be shared with other extended family members. Extended families are made up of various individuals and families who have shared responsibilities within the community. This extended family is one of the core aspects of every African community. “An African will refer to an older person as auntie or uncle. Siblings of parents will be called father or mother rather than uncle and aunt. Cousins will be called brother or sister”. This system can be very difficult for outsiders to understand; however, it is no less important. “Also reflecting their communal ethic, Africans are reluctant to stand out in a crowd or to appear different from their neighbors or colleagues, a result of social pressure to avoid offense to group standards and traditions." Women also have a very important role in African culture because they take care of the house and children. Traditionally “men do the heavy work of clearing and plowing the land, women sow the seeds, tend the fields, harvest the crops, haul the water, and bear the major burden for growing the family’s food”. Despite their work in the fields women are expected to be subservient to men in some African cultures. “When young women migrate to cities, this imbalance between the sexes, as well as financial need, often causes young women of lower economic status, who lack education and job training, to have sexual relationships with older men who are established in their work or profession and can afford to support a girlfriend or two”.

Traditional Sub-Saharan African music is as diverse as the region's various populations. The common perception of Sub-Saharan African music is that it is rhythmic music centered around the drums. It is partially true. A large part of Sub-Saharan music, mainly among speakers of Niger–Congo and Nilo-Saharan languages, is rhythmic and centered around the drum. Sub-Saharan music is polyrhythmic, usually consisting of multiple rhythms in one composition. Dance involves moving multiple body parts. These aspect of Sub-Saharan music has been transferred to the new world by enslaved Sub-Saharan Africans and can be seen in its influence on music forms as Samba, Jazz, Rhythm and Blues, Rock & Roll, Salsa, Reggae and Rap music.

But Sub-Saharan music involves a lot of music with strings, horns, and very little poly-rhythms. Music from the eastern sahel and along the nile, among the Nilo-Saharan, made extensive use of strings and horns in ancient times. Among the Afro-Asiatics, we see extensive use of string instruments. Dancing involve swaying body movements and footwork. Among the San is extensive use of string instruments with emphasis on footwork.

Modern Sub-Saharan African music has been influence by music from the New World (Jazz, Salsa, Rhythm and Blues etc.) vice versa being influenced by enslaved Sub-Saharan Africans. Popular styles are Mbalax in Senegal and Gambia, Highlife in Ghana, Zoblazo in Ivory Coast, Makossa in Cameroon, Soukous in the Democratic Republic of Congo, Kizomba in Angola, and Mbaqanga in South Africa. New World styles like Salsa, R&B/Rap, Reggae, and Zouk also have widespread popularity.

The oldest abstract art in the world is a shell necklace, dated to 82,000 years in the Cave of Pigeons in Taforalt, eastern Morocco. The second oldest abstract form of art and the oldest rock art is found in the Blombos Cave at the Cape in South Africa, dated 77,000 years. Sub-Saharan Africa has some of the oldest and most varied style of rock art in the world.

Although Sub-Saharan African art is very diverse there are some common themes. One is the use of the human figure. Second, there is a preference for sculpture. Sub-Saharan African art is meant to be experienced in three dimensions, not two. A house is meant to be experienced from all angles. Third, art is meant to be performed. Sub-Saharan Africans have specific name for masks. The name incorporates the sculpture, the dance, and the spirit that incorporates the mask. The name denotes all three elements. Fourth, art that serves a practical function, utilitarian. The artist and craftsman are not separate. A sculpture shaped like a hand can be used as a stool. Fifth, the use of fractals or non-linear scaling. The shape of the whole is the shape of the parts at different scales. Before the discovery of fractal geometry], Leopold Sedar Senghor, Senegal's first president, referred to this as "dynamic symmetry." William Fagg, the British art historian, compared it to the logarithmic mapping of natural growth by biologist D’Arcy Thompson. Lastly, Sub-Saharan African art is visually abstract, instead of naturalistic. Sub-Saharan African art represents spiritual notions, social norms, ideas, values, etc. An artist might exaggerated the head of a sculpture in relations to the body not because he does not know anatomy but because he wants to illustrate that the head is the seat of knowledge and wisdom. The visual abstraction of African art was very influential in the works of modernist artist like Pablo Picasso, Henri Matisse, and Jacques Lipchitz.

Sub-Saharan African cuisine like everything about Africa is very diverse. A lot of regional overlapping occurs, but there are dominant elements region by region.

West African cuisine can be described as starchy, flavorfully spicey. Dishes include fufu, kenkey, couscous, garri, foutou, and banku. Ingredients are of native starchy tubers, yams, cocoyams, and cassava. Grains include millet, sorghum, and rice, usually in the sahel, are incorporated. Oils include palm oil and shea butter(sahel). One finds recipes that mixes fish and meat. Beverages are palm wine(sweet or sour) and millet beer. Roasting, baking, boiling, frying, mashing, and spicing are all cooking techniques.

Southeast African cuisine especially those of the Swahilis reflects its Islamic, geographical Indian Ocean cultural links. Dishes include ugali, sukuma wiki, and halva. Spices such as curry, saffron, cloves, cinnamon, pomegranate juice, cardamon, ghee, and sage are used, especially among Muslims. Meat includes cattle, sheep, and goats, but is rarely eaten since its viewed as currency and wealth.

In the Horn of Africa, pork and non-fish seafood is avoided by Christians and Muslims. Dairy products and all meats are avoided during lent by Ethiopians. Maize (corn) is a major staple. Cornmeal is used to make ugali, a popular dish with different names. Teff is used to make injera or canjeero (Somali) bread. Other important foods include enset, noog, lentils, rice, banana, leafy greens, chiles, peppers, coconut milk and tomatoes. Beverages are coffee (domesticated in Ethiopia), chai tea, fermented beer from banana or millet. Cooking techniques include roasting and marinating.
Central African cuisine connects with all major regions of Sub-Saharan Africa: Its cuisine reflects that. Ugali and fufu are eaten in the region. Central African cuisine is very starchy and spicy hot. Dominant crops include plantains, cassava, peanuts, chillis, and okra. Meats include beef, chicken, and sometimes exotic meats called bush meat (antelope, warthog, crocodile). Widespread spicy hot fish cuisine is one of the differentiating aspects. Mushroom is sometimes used as a meat substitute.

Traditional Southern African cuisine surrounds meat. Traditional society typically focused on raising, sheep, goats, and especially cattle. Dishes include braai (barbecue meat), sadza, bogobe, pap (fermented cornmeal), milk products (buttermilk, yoghurt). Crops utilised are sorghum, maize (corn), pumpkin beans, leafy greens, and cabbage. Beverages include ting (fermented sorghum or maize), milk, chibuku (milky beer). Influences from the Indian and Malay community can be seen its use of curries, sambals, pickled fish, fish stews, chutney, and samosa. European influences can be seen in cuisines like biltong (dried beef strips), potjies (stews of maize, onions, tomatoes), French wines, and crueler or koeksister (sugar syrup cookie).

Like most of the world, Sub-Saharan Africans have adopted Western-style clothing. In some country like Zambia, used Western clothing has flooded markets, causing great angst in the retail community. Sub-Saharan Africa boasts its own traditional clothing style. Cotton seems to be the dominant material.

In East Africa, one finds extensive use of cotton clothing. Shemma, shama, and kuta are types of Ethiopian clothing. Kanga are Swahili cloth that comes in rectangular shapes, made of pure cotton, and put together to make clothing. Kitenges are similar to kangas and kikoy, but are of a thicker cloth, and have an edging only on a long side. Kenya, Uganda, Tanzania, and South Sudan are some of the African countries where kitenge is worn. In Malawi, Namibia and Zambia, kitenge is known as Chitenge. One of the unique materials, which is not a fiber and is used to make clothing is barkcloth, an innovation of the Baganda people of Uganda. It came from the Mutuba tree (Ficus natalensis). On Madagascar a type of draped cloth called lamba is worn.
In West Africa, again cotton is the material of choice. In the Sahel and other parts of West Africa the boubou and kaftan style of clothing are featured. Kente cloth is created by the Akan people of Ghana and Ivory Coast, from silk of the various moth species in West Africa. Kente comes from the Ashanti twi word "kenten" which means basket. It is sometimes used to make dashiki and kufi. Adire is a type of Yoruba cloth that is starch resistant. Raffia cloth and barkcloth are also utilised in the region.

In Central Africa, the Kuba people developed raffia cloth from the raffia plant fibers. It was widely used in the region. Barkcloth was also extensively used.

In Southern Africa one finds numerous uses of animal hide and skins for clothing. The Ndau in central Mozambique and the Shona mix hide with barkcloth and cotton cloth. Cotton cloth is referred to as machira. Xhosa, Tswana, Sotho, and Swazi also made extensive use of hides. Hides come from cattle, sheep, goat, and elephant. Leopard skins were coveted and were a symbol of kingship in Zulu society. Skins were tanned to form leather, dyed, and embedded with beads.

Football (soccer) is the most popular sport in Sub-Saharan Africa. Sub-Saharan men are its main patrons. Major competitions include the African Champions League, a competition for the best clubs on the continent and the Confederation Cup, a competition primarily for the national cup winner of each African country. The Africa Cup of Nations is a competition of 16 national teams from various African countries held every two years. South Africa hosted the 2010 FIFA World Cup, a first for a Sub-Saharan country. In 2010, Cameroon played in the World Cup for the sixth time, which is the current record for a Sub-Saharan team. In 1996 Nigeria won the Olympic gold for football. In 2000 Cameroon maintained the continent's supremacy by winning the title too. Momentous achievements for Sub-Saharan African football. Famous Sub-Saharan football stars include Abedi Pele, Emmanuel Adebayor, George Weah, Michael Essien, Didier Drogba, Roger Milla, Nwankwo Kanu, Jay-Jay Okocha, Bruce Grobbelaar, Samuel Eto'o, Kolo Touré, Yaya Touré, Sadio Mané and Pierre-Emerick Aubameyang. The most talented Sub-Saharan African football players find themselves courted and sought after by European leagues. There are currently more than 1000 Africans playing for European clubs. Sub-Saharan Africans have found themselves the target of racism by European fans. FIFA has been trying hard to crack down on racist outburst during games.

Rugby is also popular in Sub-Saharan Africa. The Confederation of African Rugby governs rugby games in the region. South Africa is a major force in the game and won the Rugby World Cup in 1995 and in 2007. Africa is also allotted one guaranteed qualifying place in the Rugby World Cup.

Boxing is also a popular sport. Battling Siki the first world champion to come out of Sub-Saharan Africa. Countries such as Nigeria, Ghana and South Africa have produced numerous professional world champions such as Dick Tiger, Hogan Bassey, Gerrie Coetzee, Samuel Peter, Azumah Nelson and Jake Matlala.

Cricket has a following. The African Cricket Association is an international body which oversees cricket in African countries. South Africa and Zimbabwe have their own governing bodies. In 2003 the Cricket World Cup was held in South Africa, first time it was held in Sub-Saharan Africa.

Over the years, Ethiopia and Kenya have produced many notable long-distance athletes. Each country has federations that identify and cultivate top talent. Athletes from Ethiopia and Kenya hold, save for two exceptions, all the men's outdoor records for Olympic distance events from 800m to the marathon. Famous runners include Haile Gebrselassie, Kenenisa Bekele, Paul Tergat, and John Cheruiyot Korir.
The development of tourism in this region has been identified as having the ability to create jobs and improve the economy. South Africa, Namibia, Mauritius, Botswana, Ghana, Cape Verde, Tanzania, and Kenya have been identified as having well developed tourism industries. Cape Town and the surrounding area is very popular with tourists.

Only seven African countries are not geopolitically a part of Sub-Saharan Africa: Algeria, Egypt, Libya, Morocco, Tunisia, Western Sahara (claimed by Morocco) and Sudan; they form the UN subregion of Northern Africa, which also makes up the largest bloc of the Arab World. Nevertheless, some international organisations include Sudan as part of Sub-Saharan Africa. Although a long-standing member of the Arab League, Sudan has around 30% non-Arab populations in the west (Darfur, Masalit, Zaghawa), far north (Nubian) and south (Kordofan, Nuba). Mauritania and Niger only include a band of the Sahel along their southern borders. All other African countries have at least significant portions of their territory within Sub-Saharan Africa.







Depending on classification Sudan is often not considered part of Sub-Saharan Africa, as it is considered part of North Africa.








</doc>
<doc id="27068" url="https://en.wikipedia.org/wiki?curid=27068" title="Sahara Desert (ecoregion)">
Sahara Desert (ecoregion)

The Sahara Desert ecoregion, as defined by the World Wide Fund for Nature (WWF), includes the hyper-arid center of the Sahara, between 18° and 30° N. It is one of several desert and xeric shrubland ecoregions that cover the northern portion of the African continent.

The Sahara Desert is the world's largest hot desert, located in North Africa. It stretches from the Red Sea to the Atlantic Ocean. The vast desert encompasses several ecologically distinct regions. The Sahara Desert ecoregion covers an area of in the hot, hyper-arid center of the Sahara, surrounded on the north, south, east, and west by desert ecoregions with higher rainfall and more vegetation.

The North Saharan steppe and woodlands ecoregion lies to the north and west, bordering the Mediterranean climate regions of Africa's Mediterranean and North Atlantic coasts. The North Saharan steppe and woodlands receives more regular winter rainfall than the Sahara Desert ecoregion. The South Saharan steppe and woodlands ecoregion lies to the south, between the Sahara Desert ecoregion and the Sahel grasslands. The South Saharan steppe and woodlands receives most of its annual rainfall during the summer. The Red Sea coastal desert lies in the coastal strip between the Sahara Desert ecoregion and the Red Sea.

Some mountain ranges rise up from the desert and receive more rainfall and cooler temperatures. These Saharan mountains are home to two distinct ecoregions; the West Saharan montane xeric woodlands in the Ahaggar, Tassili n'Ajjer, Aïr, and other ranges in the western and central Sahara Desert, and the Tibesti-Jebel Uweinat montane xeric woodlands in the Tibesti and Jebel Uweinat of the eastern Sahara.

The surface of the desert ranges from large areas of sand dunes (erg), to stone plateaus (hamadas), gravel plains (reg), dry valleys (wadis), and salt flats. The only permanent river that crosses the ecoregion is the Nile River, which originates in central Africa and empties northwards into the Mediterranean Sea. Some areas encompass vast underground aquifers resulting in oases, while other regions severely lack water reserves.
The Sahara Desert features a hot desert climate (Köppen climate classification "BWh"). The Sahara Desert is one of the driest and hottest regions of the world, with a mean temperature sometimes over and the averages high temperatures in summer are over for months at a time, and can even soar to . In desert rocky mountains such as the Tibesti in Libya or the Hoggar in Algeria, averages highs in summer are slightly moderated by the high elevation and are between at elevation. Daily variations may also be extreme: a swing from has been observed. Typical temperature swings are between .

Precipitation in the Sahara Desert is scarce, as the whole desert generally receives less than of rain per year except on the northernmost and southernmost edge as well as in the highest desert mountains. More than half of the desert area is hyper-arid and virtually rainless, with an average annual precipitation below and many consecutive years may pass without any rainfall. The south of the Sahara Desert, along the boundary with the hot semi-arid climate ("BSh") of the Sahel, receives most of its annual rainfall during the highest-sun months (summer) when the Inter-Tropical Convergence Zone moves up from the south. Wind- and sandstorms occur in early spring. Local inhabitants protect themselves from the heat, the sunshine, the dry air, the high diurnal temperature ranges and the sometimes dusty or sandy winds by covering their heads, such as the cheche garment worn by Tuareg.

The Sahara was one of the first regions of Africa to be farmed. Some 5,000 years ago, the area was not so arid and the vegetation might have been closer to a savanna. Previous fauna may be recognised in stone carvings. However, desertification set in around 3000 BCE, and the area became much like it is today.

The Sahara is largely undisturbed. The most degradation is found in areas where there is water, such as aquifer oases or along the desert margins where some rain usually falls most years. In these areas, animals such as addaxes, scimitar-horned oryxes, and bustards are over-hunted for their meat. Only one area of conservation is recorded in the Sahara: the Zellaf Nature Reserve in Libya.


</doc>
