<doc id="27643" url="https://en.wikipedia.org/wiki?curid=27643" title="Shanghai">
Shanghai

Shanghai (; ) is one of the four direct-controlled municipalities of China and the second most populous city proper in the world, with a population of more than 24 million . It is a global financial centre and transport hub, with the world's busiest container port. Located in the Yangtze River Delta, it sits on the south edge of the estuary of the Yangtze in the middle portion of the East China coast. The municipality borders the provinces of Jiangsu and Zhejiang to the north, south and west, and is bounded to the east by the East China Sea.

As a major administrative, shipping and trading city, Shanghai grew in importance in the 19th century due to trade and recognition of its favourable port location and economic potential. The city was one of five treaty ports forced open to foreign trade following the British victory over China in the First Opium War. The subsequent 1842 Treaty of Nanking and 1844 Treaty of Whampoa allowed the establishment of the Shanghai International Settlement and the French Concession. The city then flourished as a centre of commerce between China and other parts of the world (predominantly the Occident), and became the primary financial hub of the Asia-Pacific region in the 1930s. However, with the Communist Party takeover of the mainland in 1949, trade was limited to other socialist countries, and the city's global influence declined. In the 1990s, the economic reforms introduced by Deng Xiaoping resulted in an intense re-development of the city, aiding the return of finance and foreign investment to the city. It has since re-emerged as a hub for international trade and finance; it is the home of the Shanghai Stock Exchange, one of the world's largest by market capitalization.

Shanghai has been described as the "showpiece" of the booming economy of mainland China;

The two Chinese characters in the city's name are ("shàng"/"zan", "upon") and ("hǎi"/"hae,""sea"), together meaning "Upon-the-Sea". The earliest occurrence of this name dates from the 11th-century Song Dynasty, at which time there was already a river confluence and a town with this name in the area. There are disputes as to exactly how the name should be understood, but Chinese historians have concluded that during the Tang dynasty Shanghai was literally on the sea.

Shanghai is officially abbreviated ("Hù/Wu") in Chinese, a contraction of ("Hù Dú/Vu Doh", "Harpoon Ditch"), a 4th- or Jin name for the mouth of Suzhou Creek when it was the main conduit into the ocean. This character appears on all motor vehicle license plates issued in the municipality today.

Another alternative name for Shanghai is "Shēn" () or "Shēnchéng" (, "Shen City"), from Lord Chunshen, a third-century BC nobleman and prime minister of the state of Chu, whose fief included modern Shanghai. Sports teams and newspapers in Shanghai often use Shen in their names, such as Shanghai Shenhua F.C. and "Shen Bao".

"Huating" () was another early name for Shanghai. In AD 751, during the mid-Tang dynasty, Huating County was established by the Governor of Wu Commandery Zhao Juzhen at modern-day Songjiang, the first county-level administration within modern-day Shanghai. Today, Huating appears as the name of a four-star hotel in the city.

The city also has various nicknames in English, including "Pearl of the Orient" and "Paris of the East".

During the Spring and Autumn period (approximately 771 to 476 BC), the Shanghai area belonged to the Kingdom of Wu, which was conquered by the Kingdom of Yue, which in turn was conquered by the Kingdom of Chu. During the Warring States period (475 BC), Shanghai was part of the fief of Lord Chunshen of Chu, one of the Four Lords of the Warring States. He ordered the excavation of the Huangpu River. Its former or poetic name, the Chunshen River, gave Shanghai its nickname of "Shen". Fishermen living in the Shanghai area then created a fishing tool called the "hu", which lent its name to the outlet of Suzhou Creek north of the Old City and became a common nickname and abbreviation for the city.

During the Tang and Song dynasties, Qinglong Town () in modern Qingpu District was a major trading port. Established in 746 (fifth year of the Tang Tianbao era), it developed into what contemporary sources called a "giant town of the Southeast", with thirteen temples and seven pagodas. The famous Song scholar and artist Mi Fu served as its mayor. The port had a thriving trade with provinces along the Yangtze River and the Chinese coast, as well as foreign countries such as Japan and Silla.

By the end of the Song dynasty, the center of trading had moved downstream of the Wusong River to Shanghai, which was upgraded in status from a village to a market town in 1074, and in 1172 a second sea wall was built to stabilize the ocean coastline, supplementing an earlier dike. From the Yuan dynasty in 1292 until Shanghai officially became a municipality in 1927, central Shanghai was administered as a county under Songjiang Prefecture, whose seat was at the present-day Songjiang District.

Two important events helped promote Shanghai's development in the Ming dynasty. A city wall was built for the first time in 1554 to protect the town from raids by Japanese pirates. It measured high and in circumference. During the Wanli reign (1573–1620), Shanghai received an important psychological boost from the erection of a City God Temple in 1602. This honour was usually reserved for prefectural capitals and not normally given to a mere county seat such as Shanghai. It probably reflected the town's economic importance, as opposed to its low political status.

During the Qing dynasty, Shanghai became one of the most important sea ports in the Yangtze Delta region as a result of two important central government policy changes: In 1684, the Kangxi Emperor reversed the Ming dynasty prohibition on oceangoing vessels – a ban that had been in force since 1525; and in 1732 the Yongzheng Emperor moved the customs office for Jiangsu province (; see Customs House, Shanghai) from the prefectural capital of Songjiang to Shanghai, and gave Shanghai exclusive control over customs collections for Jiangsu's foreign trade. As a result of these two critical decisions, by 1735 Shanghai had become the major trade port for all of the lower Yangtze region, despite still being at the lowest administrative level in the political hierarchy.

International attention to Shanghai grew in the 19th century due to European recognition of its economic and trade potential at the Yangtze. During the First Opium War (1839–1842), British forces occupied the city. The war ended with the 1842 Treaty of Nanking, which allowed the British to dictate opening the treaty ports, Shanghai included, for international trade. The Treaty of the Bogue signed in 1843, and the Sino-American Treaty of Wanghia signed in 1844 forced Chinese concession to European and American desires for visitation and trade on Chinese soil. Britain, France (under the 1844 Treaty of Whampoa), and the United States all carved out concessions outside the walled city of Shanghai, which was still ruled by the Chinese.

The Chinese-held old city of Shanghai fell to the rebels of the Small Swords Society in 1853 but was recovered by the Qing government in February 1855. In 1854, the Shanghai Municipal Council was created to manage the foreign settlements. Between 1860–1862, the Taiping rebels twice attacked Shanghai and destroyed the city's eastern and southern suburbs, but failed to take the city. In 1863, the British settlement to the south of Suzhou Creek (northern Huangpu District) and the American settlement to the north (southern Hongkou District) joined in order to form the Shanghai International Settlement. The French opted out of the Shanghai Municipal Council and maintained its own concession to the south and southwest.

Citizens of many countries and all continents came to Shanghai to live and work during the ensuing decades; those who stayed for long periods – some for generations – called themselves "Shanghailanders". In the 1920s and 1930s, almost 20,000 White Russians and Russian Jews fled the newly established Soviet Union and took up residence in Shanghai. These Shanghai Russians constituted the second-largest foreign community. By 1932, Shanghai had become the world's fifth largest city and home to 70,000 foreigners. In the 1930s, some 30,000 Jewish refugees from Europe arrived in the city.

The Sino-Japanese War concluded with the Treaty of Shimonoseki, which elevated Japan to become another foreign power in Shanghai. Japan built the first factories in Shanghai, which were soon copied by other foreign powers. Shanghai was then the most important financial center in the Far East. All this international activity gave Shanghai the nickname "the Great Athens of China".

Under the Republic of China, Shanghai's political status was raised to that of a municipality on 14 July 1927. Although the territory of the foreign concessions was excluded from their control, this new Chinese municipality still covered an area of , including the modern-day districts of Baoshan, Yangpu, Zhabei, Nanshi, and Pudong. Headed by a Chinese mayor and municipal council, the new city government's first task was to create a new city center in Jiangwan town of Yangpu district, outside the boundaries of the foreign concessions. The "Greater Shanghai Plan" included a public museum, library, sports stadium, and city hall, which were partially constructed when the plan was interrupted by the Japanese invasion.

On 28 January 1932, Japanese forces invaded Shanghai and the Chinese resisted, fighting to a standstill; a ceasefire was brokered in May. The Battle of Shanghai in 1937 resulted in the occupation of the Chinese administered parts of Shanghai outside of the International Settlement and the French Concession. The foreign concessions were ultimately occupied by the Japanese on 8 December 1941 and remained occupied until Japan's surrender in 1945, during which time many war crimes were committed.

On 27 May 1949, the People's Liberation Army took control of Shanghai. Under the new People's Republic of China (PRC), Shanghai was one of only three municipalities not merged into neighboring provinces over the next decade (the others being Beijing and Tianjin). Shanghai underwent a series of changes in the boundaries of its subdivisions over the next decade. After 1949, most foreign firms moved their offices from Shanghai to Hong Kong, as part of a foreign divestment due to the Communist victory.

During the 1950s and 1960s, Shanghai became the center for radical leftism since it was the industrial centre of China with most skilled industrial workers. The radical leftist Jiang Qing and her three allies, together the Gang of Four, were based in the city. Yet, even during the most tumultuous times of the Cultural Revolution, Shanghai was able to maintain high economic productivity and relative social stability. During most of the history of the PRC, Shanghai has been a comparatively heavy contributor of tax revenue to the central government, with Shanghai in 1983 contributing more in tax revenue to the central government than Shanghai had received in investment in the prior 33 years combined. This came at the cost of severely crippling welfare of Shanghainese people and Shanghai's infrastructural and capital development. Its importance to the fiscal well-being of the central government also denied it economic liberalizations begun in 1978. Shanghai was finally permitted to initiate economic reforms in 1991, starting the massive development still seen today and the birth of Lujiazui in Pudong.

Shanghai lies on China's east coast roughly equidistant from Beijing and Guangzhou. The Old City and modern downtown Shanghai are now located in the center of an expanding peninsula between the Yangtze River Delta to the north and Hangzhou Bay to the south, formed by the Yangtze's natural deposition and by modern land reclamation projects. The provincial-level Municipality of Shanghai administers both the eastern area of this peninsula and many of its surrounding islands. It is bordered on the north and west by Jiangsu, on the south by Zhejiang, and on the east by the East China Sea. Its northernmost point is on Chongming Island, now the second-largest island in mainland China after its expansion during the 20th century. The municipality does not, however, include an exclave of Jiangsu on northern Chongming or the two islands forming Shanghai's Yangshan Port, which are part of Zhejiang's Shengsi County. This deep-water port was made necessary by the increasing size of container ships but also the silting of the Yangtze, which narrows to less than as far out as from Hengsha.

Downtown Shanghai is bisected by the Huangpu River, a man-made tributary of the Yangtze that was created by order of Lord Chunshen during the Warring States period. The historic center of the city was located on the west bank of the Huangpu (Puxi), near the mouth of Suzhou Creek, connecting it with Lake Tai and the Grand Canal. The central financial district Lujiazui has grown up on the east bank of the Huangpu (Pudong). The destruction of local wetlands occasioned by the creation of Pudong International Airport along the peninsula's eastern shore has been somewhat offset by the protection and expansion of the nearby shoals of Jiuduansha as a nature preserve.

Shanghai's location on an alluvial plain means that the vast majority of its land area is flat, with an average elevation of . Its sandy soil has required its skyscrapers to be built with deep concrete piles to stop them from sinking into the soft ground of the central area. The few hills such as She Shan lie to the southwest and the highest point is the peak of Dajinshan Island in Hangzhou Bay (). The city has many rivers, canals, streams and lakes and is known for its rich water resources as part of the Lake Tai drainage area.

Shanghai has a humid subtropical climate (Köppen "Cfa") and experiences four distinct seasons. Winters are chilly and damp, with northwesterly winds from Siberia can cause nighttime temperatures to drop below freezing, although most years there are only one or two days of snowfall. Summers are hot and humid, with an average of 8.7 days exceeding annually; occasional downpours or freak thunderstorms can be expected. The city is also susceptible to typhoons in summer and the beginning of autumn, none of which in recent years has caused considerable damage. The most pleasant seasons are spring, although changeable and often rainy, and autumn, which is generally sunny and dry. The city averages in January and in July, for an annual mean of . With monthly percent possible sunshine ranging from 34% in March to 54% in August, the city receives 1,895 hours of bright sunshine annually. Extremes since 1951 have ranged from on 31 January 1977 (unofficial record of was set on 19 January 1893) to on 6 and 8 August 2013. A highest record of was registered in Xujiahui, a downtown station on 21 July 2017.

Like virtually all governing institutions in the mainland People's Republic of China, the politics of Shanghai is structured in a dual party-government system, in which the Party Committee Secretary, officially termed the Communist Party of China Shanghai Municipal Committee Secretary (currently Li Qiang), outranks the Mayor (currently Ying Yong).

Political power in Shanghai is widely seen as a stepping stone to higher positions in the national government. Since Jiang Zemin became the General Secretary of the Communist Party of China in June 1989, all former Shanghai party secretaries but one were elevated to the Politburo Standing Committee, the "de facto" highest decision-making body in China, including Jiang himself (Party General Secretary), Zhu Rongji (Premier), Wu Bangguo (Chairman of the National People's Congress), Huang Ju (Vice Premier), Xi Jinping (current General Secretary), and Yu Zhengsheng. Zeng Qinghong, a former deputy party secretray of Shanghai, also rose to the Politburo Standing Committee and became the Vice President and an influential power broker. The only exception is Chen Liangyu, who was fired in 2006 and later convicted of corruption. Officials with ties to the Shanghai administration form a powerful faction in the national government, the so-called Shanghai Clique, which was often thought to compete against the rival Youth League Faction over personnel appointments and policy decisions. Xi Jinping, successor to Hu Jintao as General Secretary and President, was a compromise candidate between the two groups with supporters in both camps.

Dong Yunhu was elected chairman of the 13th Shanghai municipal CPPCC in January 2018.

Shanghai is administratively equal to a province and is divided into 16 county-level districts. Even though every district has its own urban core, the real city center is between Bund to the east, Nanjing Rd to the north, Old City Temple and Huaihai Road to the south. Prominent central business areas include Lujiazui on the east bank of the Huangpu River, and The Bund and Hongqiao areas in the west bank of the Huangpu River. The city hall and major administration units are located in Huangpu District, which also serve as a commercial area, including the famous Nanjing Road. Other major commercial areas include Xintiandi and the classy Huaihai Road (previously "Avenue Joffre") in Huangpu District and Xujiahui (formerly Romanized as "Zikawei or Siccawei", reflecting the Shanghainese pronunciation) in Xuhui District. Many universities in Shanghai are located in residential areas of Yangpu District and Putuo District.

Seven of the districts govern Puxi ( "The West Bank"), the older part of urban Shanghai on the west bank of the Huangpu River. These seven districts are collectively referred to as Shanghai Proper () or the core city (), which comprise Huangpu, Xuhui, Changning, Jing'an, Putuo, Hongkou, and Yangpu.

Pudong ( "The East Bank"), the newer part of urban and suburban Shanghai on the east bank of the Huangpu River, is governed by Pudong New Area (Chuansha County until 1992, merged with Nanhui District in 2009 and with oversight of the Jiuduansha shoals).

Seven of the districts govern suburbs, satellite towns, and rural areas further away from the urban core: Baoshan (Baoshan County until 1988), Minhang (original Minhang District & Shanghai County until 1992), Jiading (Jiading County until 1992), Jinshan (Jinshan County until 1997), Songjiang (Songjiang County until 1998), Qingpu (Qingpu County until 1999), and Fengxian (Fengxian County until 2001).

The islands of Changxing and Hengsha and most (but not all) of Chongming Island form Chongming.

The former district of Nanhui was absorbed into Pudong District in 2009. In 2011 Luwan District merged with Huangpu District.

, these county-level divisions are further divided into the following 210 township-level divisions: 109 towns, 2 townships, 99 subdistricts. Those are in turn divided into the following village-level divisions: 3,661 neighborhood committees and 1,704 village committees.

Shanghai is the commercial and financial center of China, and ranks 13th in the 2017 edition of the Global Financial Centres Index (and fourth most competitive in Asia after Singapore, Hong Kong, and Tokyo) published by the Z/Yen Group and Qatar Financial Centre Authority. It also ranks the most expensive city to live in Mainland China, according to the study of Economist Intelligence Unit in 2017. It was the largest and most prosperous city in East Asia during the 1930s, and rapid re-development began in the 1990s. This is exemplified by the Pudong District, a former swampland reclaimed to serve as a pilot area for integrated economic reforms. By the end of 2009, there were 787 financial institutions, of which 170 were foreign-invested. In 2009, the Shanghai Stock Exchange ranked third among worldwide stock exchanges in terms of trading volume and sixth in terms of the total capitalization of listed companies, and the trading volume of six key commodities including rubber, copper and zinc on the Shanghai Futures Exchange all ranked first in the world. In September 2013, with the backing of Chinese Premier Li Keqiang the city launched the China (Shanghai) Pilot Free-Trade Zone-the first free-trade zone in mainland China. The Zone introduced a number of pilot reforms designed to create a preferential environment for foreign investment. In April 2014, The Banker reported that Shanghai "has attracted the highest volumes of financial sector foreign direct investment in the Asia-Pacific region in the 12 months to the end of January 2014". In August 2014, Shanghai was named FDi magazine's Chinese Province of the Future 2014/15 due to "particularly impressive performances in the Business Friendliness and Connectivity categories, as well as placing second in the Economic Potential and Human Capital and Lifestyle categories".

In the last two decades Shanghai has been one of the fastest developing cities in the world. Since 1992 Shanghai has recorded double-digit growth almost every year except during the global recession of 2008 and 2009. In 2011, Shanghai's total GDP grew to 1.92 trillion yuan (US$297 billion) with GDP per capita of 82,560 yuan (US $12,784). The three largest service industries are financial services, retail, and real estate. The manufacturing and agricultural sectors accounted for 39.9 percent and 0.7 percent of the total output respectively. Average annual disposable income of Shanghai residents, based on the first three quarters of 2009, was 21,871 RMB.

Located at the heart of the Yangtze River Delta, Shanghai has the world's busiest container port, which handled 29.05 million TEUs in 2010. Shanghai aims to be an international shipping center in the near future.

Shanghai is one of the main industrial centers of China, playing a key role in China's heavy industries. A large number of industrial zones, including Shanghai Hongqiao Economic and Technological Development Zone, Jinqiao Export Economic Processing Zone, Minhang Economic and Technological Development Zone, and Shanghai Caohejing High-Tech Development Zone, are backbones of Shanghai's secondary industry. Heavy industries accounted for 78% of the gross industrial output in 2009. China's largest steelmaker Baosteel Group, China's largest shipbuilding base – Hudong-Zhonghua Shipbuilding Group, and the Jiangnan Shipyard, one of China's oldest shipbuilders are all located in Shanghai. Auto manufacture is another important industry. The Shanghai-based SAIC Motor is one of the three largest automotive corporations in China, and has strategic partnerships with Volkswagen and General Motors.

The conference and meeting sector is also growing. In 2012, the city hosted 780 international gatherings, up from 754 in 2011. The high supply of hotel rooms has kept room rates lower than expected, with the average room rate for four- and five-star hotels in 2012 at just RMB950 (US$153).

As of September 2013, Shanghai is also home to the largest free-trade zone in mainland China, the China (Shanghai) Pilot Free-Trade Zone. The zone covers an area of 29 km and integrates four existing bonded zones — Waigaoqiao Free Trade Zone, Waigaoqiao Free Trade Logistics Park, Yangshan Free Trade Port Area and Pudong Airport Comprehensive Free Trade Zone. Several preferential policies have been implemented to attract foreign investment in various industries to the FTZ. Because the Zone is not technically considered PRC territory for tax purposes, commodities entering the zone are not subject to duty and customs clearance as would otherwise be the case.
The 2010 census put Shanghai's total population at 23,019,148, a growth of 37.53% from 16,737,734 in 2000. 20.6 million of the total population, or 89.3%, are urban, and 2.5 million (10.7％) are rural. Based on the population of its total administrative area, Shanghai is the second largest of the four direct-controlled municipalities of China, behind Chongqing, but is generally considered the largest Chinese city because Chongqing's urban population is much smaller.

Shanghai also has 150,000 officially registered foreigners, including 31,500 Japanese, 21,000 Americans and 20,700 Koreans, but the real number of foreign citizens in the city is probably much higher. Shanghai is also a domestic immigration city, which means a huge population of citizens come from other cities in China.

The encompassing metropolitan area was estimated by the OECD (Organisation for Economic Co-operation and Development) to have, , a population of 34 million.

In 2017 the Chinese Government implemented population controls for Shanghai. Latest statistics show that from this policy, Shanghai population declined by 10,000.

Due to its cosmopolitan history, Shanghai has a blend of religious heritage as shown by the religious buildings and institutions still scattered around the city. According to a 2012 survey only around 13% of the population of Shanghai belongs to organised religions, the largest groups being Buddhists with 10.4%, followed by Protestants with 1.9%, Catholics with 0.7% and other faiths with 0.1%. Around 87% of the population may be either irreligious or involved in worship of nature deities and ancestors, Confucian churches, Taoism and folk religious sects.

There are folk religious temples such as a Temple of the Chenghuangshen (City God), at the heart of the old city, and a temple dedicated to the Three Kingdoms general Guan Yu. The White Cloud Temple of Shanghai is an important Taoist centre in the city. The "Wenmiao" (Temple of the God of Culture) is dedicated to Confucius.

Buddhism, in its Chinese varieties, has had a presence in Shanghai since ancient times. The Longhua Temple, the largest temple in Shanghai, and the Jing'an Temple, were first founded in the Three Kingdoms period. Another important temple is the Jade Buddha Temple, which is named after a large statue of Buddha carved out of jade in the temple. In recent decades, dozens of modern temples have been built throughout the city.

Islam came into Shanghai 700 years ago and a mosque was built in 1295 in Songjiang. In 1843, a teachers' college was also set up. The Shanghai Muslim Association is located in the Xiaotaoyuan Mosque in Huangpu.

Shanghai has one of the largest proportions of Catholics in China (2003). Among Catholic churches, St Ignatius Cathedral in Xujiahui is one of the largest, while She Shan Basilica is an active pilgrimage site.

Other forms of Christianity in Shanghai include Eastern Orthodox minorities and, since 1996, registered Christian Protestant churches. During World War II thousands of Jews descended upon Shanghai in an effort to flee Hitler's regime. The Jews lived side-by-side in a designated area called Shanghai Ghetto and formed a vibrant community centered on the Ohel Moishe Synagogue, which is preserved remnant of this portion of Shanghai's complex religious past.

Shanghai ranked first in the 2009 and 2012 Program for International Student Assessment (PISA), a worldwide study of academic performance of 15-year-old students conducted by the OECD. Shanghai students, including migrant children, scored highest in every aspect (math, reading and science) in the world. The study concludes that public-funded schools in Shanghai have the highest educational quality in the world. Critics of PISA results counter that, in Shanghai and other Chinese cities, most children of migrant workers can only attend city schools up to the ninth grade, and must return to their parents' hometowns for high school due to hukou restrictions, thus skewing the composition of the city's high school students in favor of wealthier local families.

Shanghai is the first city in the country to implement 9-year mandatory education. The 2010 census shows that out of Shanghai's total population, 22.0% had a college education, double the level from 2000, while 21.0% had high school, 36.5% middle school, and 1.35% primary school education. 2.74% of residents age 15 and older were illiterate.

Shanghai has more than 930 kindergartens, 1,200 primary and 850 middle schools. Over 760,000 middle schools students and 871,000 primary school students are taught by 76,000 and 64,000 teaching staff respectively.

Shanghai is a major center of higher education in China with over 30 universities and colleges. A number of China's most prestigious universities are based in Shanghai, including Fudan University, Shanghai Jiao Tong University, Tongji University, East China Normal University (these universities are selected as "985 universities" by the Chinese Government in order to build world-class universities). In 2012 NYU Shanghai was established in Pudong by New York University in partnership with East China Normal University as the first Sino-US joint venture university. In 2013 the Shanghai Municipality and the Chinese Academy of Sciences founded the ShanghaiTech University in the Zhangjiang Hi-Tech Park in Pudong. This new research university is aiming to be a first-class institution on a national and international level. The cadre school China Executive Leadership Academy in Pudong is also located in Shanghai, as well as the China Europe International Business School.

Children with foreign passports are permitted to attend any public school in Shanghai. Prior to 2007 they were permitted to attend 150 select public schools. In 2006 about 2,000 non-Chinese nationals under 18 years of age attended Shanghai public schools. Students with Hanyu Shuiping Kaoshi (HSK) above 3 or 4 may attend public schools using Mandarin Chinese as the medium of instruction, while students below HSK 3–4 may attend international divisions of public schools or private international schools.

Shanghai has the largest number of international schools of any city in China. In November 2015 Christopher Cottrell of the "Global Times" wrote that Shanghai "prides itself on its international schools".

Shanghai has an extensive public transport system, largely based on metros, buses and taxis. Payment of all these public transportation tools can be made by using the Shanghai Public Transportation Card.

Shanghai's rapid transit system, the Shanghai Metro, incorporates both subway and light metro lines and extends to every core urban district as well as neighboring suburban districts. , there are 16 metro lines (excluding the Shanghai Maglev Train and Jinshan Railway), 395 stations and of lines in operation, making it the longest network in the world. On 31 December 2016, it set a record of daily ridership of 11.7 million. The fare depends on the length of travel distance starting from 3 RMB.

In 2010, Shanghai reintroduced trams, this time as a modern rubber tyred Translohr system, in Zhangjiang area of East Shanghai as Zhangjiang Tram. A separate conventional tram system is being constructed in Songjiang District. Additional tram lines are under study in Hongqiao Subdistrict and Jiading District.

Shanghai also has the world's most extensive network of urban bus routes, with nearly one thousand bus lines, operated by numerous transportation companies. The system includes the world's oldest continuously operating trolleybus system. Bus fare normally costs 2 RMB.
Taxis are plentiful in Shanghai. The base fare is currently ¥14(sedan)/¥16(MPV) (inclusive of a ¥1 fuel surcharge; ¥18 between 11:00 pm and 5:00 am) which covers the first . Additional km cost ¥2.4 each (¥3.2 between 11:00 pm and 5:00 am).

Shanghai is a major hub of China's expressway network. Many national expressways (prefixed with G) pass through or terminate in Shanghai, including G2 Beijing–Shanghai Expressway (overlapping G42 Shanghai–Chengdu), G15 Shenyang–Haikou, G40 Shanghai–Xi'an, G50 Shanghai–Chongqing, G60 Shanghai–Kunming (overlapping G92 Shanghai–Ningbo), and G1501 Shanghai Ring Expressway. In addition, there are also numerous municipal expressways prefixed with S (S1, S2, S20, etc.). Shanghai has one bridge-tunnel crossing spanning the mouth of the Yangtze to the north of the city.

In the city center, there are several elevated expressways to lessen traffic pressure on surface streets, but the growth of car use has made demand far outstrip capacity, with heavy congestion being commonplace. There are bicycle lanes separate from car traffic on many surface streets, but bicycles and motorcycles are banned from many main roads including the elevated expressways. Recently, cycling has seen a resurgence in popularity thanks to the emergence of a large number of dockless app based bikeshares such as Mobike, Bluegogo and Ofo.

Private car ownership in Shanghai has been rapidly increasing in recent years, but a new private car cannot be driven until the owner buys a license in the monthly private car license plate auction. Around 11,500 license plates are auctioned each month and the average price is about 84,000 RMB ($12,758). According to the municipal regulation in 2016, only those who are Shanghai registered residents or have paid social insurance or individual incomer tax for over 3 years in a row. The purpose of this policy is to limit the growth of automobile traffic and to alleviate congestion.

Shanghai has four major railway stations: Shanghai Railway Station, Shanghai South Railway Station, Shanghai West Railway Station, and Shanghai Hongqiao Railway Station. All are connected to the metro network and serve as hubs in the railway network of China. Two main railways terminate in Shanghai: Jinghu Railway from Beijing, and Huhang Railway from Hangzhou. Hongqiao Station also serves as the main Shanghai terminus of three high-speed rail lines: the Shanghai–Hangzhou High-Speed Railway, the Shanghai–Nanjing High-Speed Railway, and the Beijing–Shanghai High-Speed Railway.
Shanghai is one of the leading air transport gateways in Asia. The city has two commercial airports: Shanghai Pudong International Airport and Shanghai Hongqiao International Airport. Pudong Airport is the main international airport, while Hongqiao Airport mainly operates domestic flights with limited short-haul international flights. In 2010 the two airports served 71.7 million passengers (Pudong 40.4 million, Hongqiao 31.3 million), and handled 3.7 million tons of cargo (Pudong 3.22 million tons, Hongqiao 480 thousand tons).

Shanghai has a rich collection of buildings and structures of various architectural styles. The Bund, located by the bank of the Huangpu River, is home to a row of early 20th-century architecture, ranging in style from the neoclassical HSBC Building to the art deco Sassoon House. Many areas in the former foreign concessions are also well-preserved, the most notable being the French Concession.
Shanghai has one of the world's largest number of Art Deco buildings as a result of the construction boom during the 1920s and 1930s. One of the most famous architects working in Shanghai was László Hudec, a Hungarian-Slovak architect who lived in the city between 1918 and 1947. Some of his most notable Art Deco buildings include the Park Hotel and the Grand Theater. Other prominent architects who contributed to the Art Deco style are Parker & Palmer, who designed the Peace Hotel, Metropole Hotel, and the Broadway Mansions, and Austrian architect GH Gonda, who designed the Capital Theatre. The Bund's first revitalization started in 1986, with a new promenade by the Dutch Architect Paulus Snoeren, and was completed in the mid-1990s.
In recent years, a great deal of architecturally distinctive and even eccentric buildings have sprung up throughout Shanghai. Notable examples of contemporary architecture include the Shanghai Museum, Shanghai Grand Theatre in the People's Square precinct, and the Shanghai Oriental Art Center. Despite rampant redevelopment, the old city still retains some traditional architecture and designs, such as the Yuyuan Garden, an elaborate traditional garden in the Jiangnan style.

One uniquely Shanghainese cultural element is the "shikumen" (石库门) residences, which are two- or three-story townhouses, with the front yard protected by a high brick wall. Each residence is connected and arranged in straight alleys, known as a "longtang" (弄堂), pronounced "longdang" in Shanghainese. The entrance to each alley is usually surmounted by a stylistic stone arch. The whole resembles terrace houses or townhouses commonly seen in Anglo-American countries, but distinguished by the tall, heavy brick wall in front of each house. The name "shikumen" means "stone storage door", referring to the strong gateway to each house.

The shikumen is a cultural blend of elements found in Western architecture with traditional Lower Yangtze (Jiangnan) Chinese architecture and social behavior. All traditional Chinese dwellings had a courtyard, and the shikumen was no exception. Yet, to compromise with its urban nature, it was much smaller and provided an "interior haven" to the commotions in the streets, allowing for raindrops to fall and vegetation to grow freely within a residence. The courtyard also allowed sunlight and adequate ventilation into the rooms.

Less than Beijing, the city also has some examples of Soviet neoclassical architecture or Stalinist architecture. These buildings were mostly erected during the period from the founding of the People's Republic in 1949 until the Sino-Soviet Split in the late 1960s. During this decade, large numbers of Soviet experts, including architects, poured into China to aid the country in the construction of a communist state. Examples of Soviet neoclassical architecture in Shanghai include what is today the Shanghai Exhibition Centre.

The Pudong district of Shanghai is home to a number of skyscrapers, many of which rank among the tallest in the world. Among the most prominent examples are the Jin Mao Tower and the taller Shanghai World Financial Center, which at tall is the third tallest skyscraper in mainland China and ranks tenth in the world. The Shanghai Tower, completed in 2015, is the tallest building in China, as well as the second tallest in the world. With a height of , the building has 128 floors and a total floor area of above ground. The distinctive Oriental Pearl Tower, at , is located nearby, as is One Lujiazui, standing at .

The extensive public park system in Shanghai offers the citizens some reprieve from the urban jungle. By the year 2012, the city had 157 parks, with 138 of them free of charge. Some of the parks, aside from offering a green public space to locals, became popular tourist attractions due to their unique location, history or architecture. The former racetrack turned central park, People's Square park, located in the heart of downtown Shanghai, is especially well known for its proximity to other major landmarks in the city. Fuxing Park, located in the former French Concession of Shanghai, features formal French-style gardens and is surrounded by high end bars and cafes. Zhongshan Park in northwestern central Shanghai is famous for its monument of Chopin, the tallest statue dedicated to the composer in the world. Built in 1914 as Jessfield Park, it once contained the campus of St. John's University, Shanghai's first international college; today, it is known for its extensive rose and peony gardens, a large children's play area, and as the location of an important transfer station on the city's metro system. Shanghai Botanical Garden is located southwest of the city center and was established in 1978. One of the newest parks is in the Xujiahui area – Xujiahui Park, built in 1999 on the former grounds of the Great Chinese Rubber Works Factory and the EMI Recording Studio (now La Villa Rouge restaurant). The park has a man-made lake with a sky bridge running across the park, and offers a pleasant respite for Xujiahui shoppers. Other well-known Shanghai parks include: People's Square Park, Gongqing Forest Park, Fuxing Park, Zhongshan Park, Lu Xun Park, Century Park, and Jing'an Park.

The Shanghai Disney Resort Project was approved by the government on 4 November 2009, and opened in 2016. 
The $4.4 billion theme park and resort in Pudong features a castle that is the biggest among Disney's resorts.

Public awareness of the environment is growing, and the city is investing in a number of environmental protection projects. A 10-year, US$1 billion cleanup of Suzhou Creek, which runs through the city-center, was expected to be finished in 2008, and the government also provides incentives for transportation companies to invest in LPG buses and taxis. Additionally, the government has moved almost all the factories within the city center to either the outskirts or other provinces in the recent decades.

Air pollution in Shanghai is low compared to other Chinese cities, but still substantial by world standards. During the December 2013 Eastern China smog, air pollution rates reached between 23 and 31 times the international standard. On 6 December 2013, levels of PM particulate matter in Shanghai rose above 600 micrograms per cubic meter and in the surrounding area, above 700 micrograms per cubic metre. Levels of PM in Putuo District reached 726 micrograms per cubic meter. As a result, the Shanghai Municipal Education Commission received orders to suspend students' outdoor activities. Authorities pulled nearly one-third of government vehicles from the roads, while a mass of construction work was halted. Most of inbound flights were cancelled, and more than 50 flights were diverted at Pudong International Airport.

On 23 January 2014, Yang Xiong, the mayor of Shanghai municipality announced that three main measures would be taken to manage the air pollution in Shanghai, along with surrounding Anhui, Jiangsu and Zhejiang provinces. The measures involved delivery of the 2013 air cleaning program, linkage mechanism with the three surrounding provinces and improvement of the ability of early warning of emergency situation. On 12 February 2014, China's cabinet announced that a 10-billion-renminbi (US$1.7-billion) fund will be set up to help companies to meet new environmental standards.

Shanghai is sometimes considered a center of innovation and progress in China. It was in Shanghai, for example, that the first motor car was driven and (technically) the first train tracks and modern sewers were laid. It was also the intellectual battleground between socialist writers who concentrated on critical realism, which was pioneered by Lu Xun, Mao Dun, Nien Cheng and the famous French novel by André Malraux, "Man's Fate", and the more "bourgeois", more romantic and aesthetically inclined writers, such as Shi Zhecun, Shao Xunmei, Ye Lingfeng and Eileen Chang.

In the past years Shanghai has been widely recognized as a new influence and inspiration for cyberpunk culture. Futuristic buildings such as the Oriental Pearl Tower and the neon-illuminated Yan'an Elevated Road are a few examples that have helped to boost Shanghai's cyberpunk image.

The vernacular language spoken in the city is Shanghainese, a dialect of the Taihu Wu subgroup of the Wu Chinese family. This makes it a different language from the official language nationwide, which is Mandarin, itself completely mutually unintelligible with Wu Chinese. Most Shanghai residents are the descendants of immigrants from the two adjacent provinces of Jiangsu and Zhejiang who moved to Shanghai in the late 19th and early 20th centuries. The population of those regions speak different Wu Chinese dialects. From the 1990s, many migrants outside of the Wu-speaking region have come to Shanghai for work and education. They often cannot speak or learn the local language and therefore use Mandarin as a lingua franca.

Modern Shanghainese is based on different dialects of Taihu Wu: Suzhounese, Ningbonese and dialects of Shanghai's traditional areas (now lying within the Hongkou, Baoshan and Pudong districts). The prestige dialect of Wu Chinese is spoken within the city of Shanghai prior to its modern expansion. Known as "the local tongue" (), it is influenced to a lesser extent by the languages of other nearby regions from which large numbers of people have migrated to Shanghai since the 20th century, and includes a significant number of terms borrowed from European languages. The prevalence of Mandarin fluency is generally higher for those born after 1949 than those born before, while the prevalence of English fluency is higher for people who received their secondary and tertiary education before 1949 than those who did so after 1949 and before the 1990s. On the other hand, however, Shanghainese started to decline and fluency amongst young speakers weakened, as Mandarin and English are being favoured and taught over the native language. In recent years though, there have been movements within the city to protect and promote the local language from ever fading out.

Cultural curation in Shanghai has seen significant growth since 2013, with several new museums having been opened in the city. This is in part due to the city's most recently released city development plans, with aims in making the city "an excellent global city". As such, Shanghai has several museums of regional and national importance. The Shanghai Museum has one of the best collections of Chinese historical artifacts in the world, including a large collection of ancient Chinese bronzes. The China Art Museum, located in the former China Pavilion of Expo 2010, is the largest art museum in Asia. Power Station of Art is built in a converted power station, similar to London's Tate Modern. The Shanghai Natural History Museum and the Shanghai Science and Technology Museum are major natural history and science museums. In addition, there is a variety of smaller, specialist museums housed in important archaeological and historical sites such as the Songze Museum, the Museum of the First National Congress of the Chinese Communist Party, the site of the Provisional Government of the Republic of Korea, the former Ohel Moshe Synagogue (Shanghai Jewish Refugees Museum), and the General Post Office Building (Shanghai Postal Museum). The Rockbund Art Museum is also in Shanghai. There are also many art galleries, concentrated in the M50 Art District and Tianzifang. Shanghai is also home to one of China's largest aquariums, the Shanghai Ocean Aquarium. MoCA, Museum of Contemporary Art of Shanghai, is a private museum centrally located in People's Park on West Nanjing Road, and is committed to promote contemporary art and design.

Shanghai was the birthplace of Chinese cinema and theater. China's first short film, "The Difficult Couple" (1913), and the country's first fictional feature film, "An Orphan Rescues His Grandfather" (孤儿救祖记, "Gu'er Jiu Zuji", 1923) were both produced in Shanghai. These two films were very influential, and established Shanghai as the center of Chinese film-making. Shanghai's film industry went on to blossom during the early 1930s, generating great stars such as Hu Die, Ruan Lingyu, Zhou Xuan, Jin Yan, and Zhao Dan. Another film star, Jiang Qing, went on to become Madame Mao Zedong. The exile of Shanghainese filmmakers and actors as a result of the Second Sino-Japanese War and the Communist revolution contributed enormously to the development of the Hong Kong film industry. Many aspects of Shanghainese popular culture ("Shanghainese Pops") were transferred to Hong Kong by the numerous Shanghainese emigrants and refugees after the Communist Revolution. The movie "In the Mood for Love", which was directed by Wong Kar-wai (a native Shanghainese himself), depicts a slice of the displaced Shanghainese community in Hong Kong and the nostalgia for that era, featuring 1940s music by Zhou Xuan.

The "Shanghai School" was an important Chinese school of traditional arts during the Qing Dynasty and the 20th century. Under the masters from this school, traditional Chinese art developed into the modern style of "Chinese painting". The Shanghai School challenged and broke the elitist tradition of Chinese art, while also paying technical homage to the ancient masters and improving on existing traditional techniques. Members of this school were themselves educated "literati" who had come to question their very status and the purpose of art and had anticipated the impending modernization of Chinese society. In an era of rapid social change, works from the Shanghai School were widely innovative and diverse and often contained thoughtful yet subtle social commentary. The best known figures from this school include Qi Baishi, Ren Xiong, Ren Bonian, Zhao Zhiqian, Wu Changshuo, Sha Menghai, Pan Tianshou, Fu Baoshi, Xie Zhiliu, He Tianjian, and Wang Zhen. In literature, the term was used in the 1930s by some May Fourth Movement intellectualsnotably Zhou Zuoren and Shen Congwenas a derogatory label for the literature produced in Shanghai at the time. They argued that Shanghai School literature was merely commercial and therefore did not advance social progress. This became known as the "Jingpai" versus "Haipai" (Beijing v. Shanghai School) debate.

The "Songjiang School" (淞江派) was a small painting school during the Ming Dynasty. It is commonly considered as a further development of the Wu or Wumen School in the then-cultural center of the region, Suzhou. The Huating School (华亭派) was another important art school during the middle to late Ming Dynasty. Its main achievements were in traditional Chinese painting, calligraphy, and poetry. It was especially famous for its Renwen painting (人文画). Dong Qichang was one of the masters from this school.

Other Shanghainese cultural artifacts include the cheongsam (Shanghainese: "zansae"), a modernization of the traditional Manchurian qipao. This contrasts sharply with the traditional qipao, which was designed to conceal the figure and be worn regardless of age. The cheongsam went along well with the western overcoat and the scarf, and portrayed a unique East Asian modernity, epitomizing the Shanghainese population in general. As Western fashions changed, the basic cheongsam design changed, too, introducing high-neck sleeveless dresses, bell-like sleeves, and the black lace frothing at the hem of a ball gown. By the 1940s, cheongsams came in transparent black, beaded bodices, matching capes and even velvet. Later, checked fabrics became also quite common. The 1949 Communist Revolution ended the cheongsam and other fashions in Shanghai. However, the Shanghainese styles have seen a recent revival as stylish party dresses. The fashion industry has been rapidly revitalizing in the past decade. Like Shanghai's architecture, local fashion designers strive to create a fusion of western and traditional designs, often with innovative if controversial results.

In recent times Shanghai has established its own fashion week called Shanghai Fashion Week. It is held twice every year in October and April. The April session is a part of Shanghai International Fashion Culture Festival which usually lasts for a month, while Shanghai Fashion Week lasts for seven days, and the main venue is in Fuxing Park, Shanghai, while the opening and closing ceremony is in Shanghai Fashion Center. Supported by the People's Republic Ministry of Commerce, Shanghai Fashion Week is a major business and culture event of national significance hosted by the Shanghai Municipal Government. Shanghai Fashion Week is aiming to build up an international and professional platform, gathering all of the top design talents of Asia.
The event features international designers but the primary purpose is to showcase Chinese designers. The international presence has included many of the most promising young British fashion designers.

In regard to foreign publications in Shanghai, Hartmut Walravens of the IFLA Newspapers Section said that when the Japanese controlled Shanghai in the 1940s "it was very difficult to publish good papers – one either had to concentrate on emigration problems, or cooperate like the "Chronicle"".

Newspapers include:

Newspapers formerly published in Shanghai include:

Broadcasters:

Shanghai is home to several football teams, including two in the Chinese Super League – Shanghai Greenland Shenhua and Shanghai SIPG. Another professional team, Shanghai Shenxin, is currently in China League One. China's top tier basketball team, the Shanghai Sharks of the Chinese Basketball Association, developed Yao Ming before he entered the NBA. Shanghai also has an ice hockey team, China Dragon, and a baseball team, the Shanghai Golden Eagles, which plays in the China Baseball League.

Shanghai is the hometown of many outstanding and well-known Chinese professional athletes, such as Yao Ming, the 110-meter hurdler Liu Xiang, the table-tennis player Wang Liqin and the former world women's single champion and current Olympic silver medalist badminton player Wang Yihan.

Beginning in 2004, Shanghai started hosting the Chinese Grand Prix, one round of the Formula One World Championship. The race was staged at the Shanghai International Circuit. In 2010, Shanghai also became the host city of German Touring Car Masters (DTM), which raced in a street circuit in Pudong.

Shanghai also holds the Shanghai Masters tennis tournament which is part of ATP World Tour Masters 1000, and the BMW Masters and WGC-HSBC Champions golf tournaments.

The Shanghai Cricket Club is a cricket club based in Shanghai. The club dates back to 1858 when the first recorded cricket match was played between a team of British Naval officers and a Shanghai 11. Following a 45-year dormancy after the founding of the People's Republic of China in 1949, the club was re-established in 1994 by expatriates living in the city and has since grown to over 300 members. The Shanghai cricket team was a cricket team that played various international matches between 1866 and 1948. With cricket in the rest of China almost non-existent, for that period they were the de facto Chinese national side.

On 21 September 2017, Shanghai will be one of two cities to host an National Hockey League (NHL) ice hockey exhibition game that will feature the Los Angeles Kings vs. the Vancouver Canucks as an effort to garner fan interest in China before the start of the 2017–18 season.

The city is the home of the Shanghai Cooperation Organisation (SCO), a Eurasian political, economic, and security organisation.

<div class="noprint">
Shanghai is twinned with:





</doc>
<doc id="27644" url="https://en.wikipedia.org/wiki?curid=27644" title="Sinai Peninsula">
Sinai Peninsula

The Sinai Peninsula or simply Sinai (now usually ) is a peninsula in Egypt, and the only part of the country located in Asia. It is situated between the Mediterranean Sea to the north and the Red Sea to the south, and is a land bridge between Asia and Africa. Sinai has a land area of about and a population of approximately 1,400,000 people. Administratively, the Sinai Peninsula is divided into two governorates: the South Sinai Governorate and the North Sinai Governorate. Three other governorates span the Suez Canal, crossing into African Egypt: Suez Governorate on the southern end of the Suez Canal, Ismailia Governorate in the center, and Port Said Governorate in the north.

The Sinai Peninsula has been a part of Egypt from the First Dynasty of ancient Egypt ( BC). This comes in stark contrast to the region north of it, the Levant (present-day territories of Syria, Lebanon, Jordan, Israel and Palestine), which, due largely to its strategic geopolitical location and cultural convergences, has historically been the center of conflict between Egypt and various states of Mesopotamia and Asia Minor. In periods of foreign occupation, the Sinai was, like the rest of Egypt, also occupied and controlled by foreign empires, in more recent history the Ottoman Empire (1517–1867) and the United Kingdom (1882–1956). Israel invaded and occupied Sinai during the Suez Crisis (known in Egypt as the "Tripartite Aggression" due to the simultaneous coordinated attack by the UK, France and Israel) of 1956, and during the Six-Day War of 1967. On 6 October 1973, Egypt launched the Yom Kippur War to retake the peninsula, which was unsuccessful. In 1982, as a result of the Israel–Egypt Peace Treaty of 1979, Israel withdrew from all of the Sinai Peninsula except the contentious territory of Taba, which was returned after a ruling by a commission of arbitration in 1989.

Today, Sinai has become a tourist destination due to its natural setting, rich coral reefs, and biblical history. Mount Sinai is one of the most religiously significant places in the Abrahamic faiths.

The name "Sinai" (, ) may have been derived from the ancient moon-god Sin or from the Hebrew word "Seneh" ( "") The peninsula acquired the name due to the assumption that a mountain near Saint Catherine's Monastery is the Biblical Mount Sinai. However this assumption is contested.

Its modern Arabic name is ' (Egyptian Arabic '; ).
The modern Arabic is an adoption of the biblical name, the 19th-century Arabic designation of Sinai was "Jebel el-Tûr".
In addition to its formal name, Egyptians also refer to it as "" ( 'the land of turquoise'). The ancient Egyptians called it "Ta Mefkat", or 'land of turquoise'. 

In English, the name is now usually pronounced . The traditional pronunciation is or .

Sinai is triangular in shape, with northern shore lying on the southern Mediterranean Sea, and southwest and southeast shores on Gulf of Suez and Gulf of Aqaba of the Red Sea. It is linked to the African continent by the Isthmus of Suez, wide strip of land, containing the Suez Canal. The eastern isthmus, linking it to the Asian mainland, is around wide. The peninsula's eastern shore separates the Arabian plate from the African plate.

The southernmost tip is the Ras Muhammad National Park.

Most of the Sinai Peninsula is divided among the two governorates of Egypt: South Sinai (Ganub Sina) and North Sinai (Shamal Sina). Together, they comprise around 60,000 square kilometres (23,000 sq mi) and have a population (January 2013) of 597,000. Three more governates span the Suez Canal, crossing into African Egypt: Suez (el-Sewais) is on the southern end of the Suez Canal, Ismailia (el-Isma'ileyyah) in the centre, and Port Said in the north.

The largest city of Sinai is Arish, capital of the North Sinai, with around 160,000 residents. Other larger settlements include Sharm el-Sheikh and El-Tor, on the southern coast. Inland Sinai is arid (effectively a desert), mountainous and sparsely populated, the largest settlements being Saint Catherine and Nekhel.

Sinai is one of the coldest provinces in Egypt because of its high altitudes and mountainous topographies. Winter temperatures in some of Sinai's cities and towns reach .

Sinai was called "Mafkat" or 'country of turquoise' by the ancient Egyptians From the time of the First Dynasty or before, the Egyptians mined turquoise in Sinai at two locations, now called by their Egyptian Arabic names Wadi Magharah and Serabit El Khadim. The mines were worked intermittently and on a seasonal basis for thousands of years. Modern attempts to exploit the deposits have been unprofitable. These may be the first historically attested mines.

At the end of the time of Darius I, the Great (521–486 BCE) Sinai was part of the Persian province of Abar-Nahra, which means 'beyond the river [Euphrates]'.

Cambyses successfully managed the crossing of the hostile Sinai Desert, traditionally Egypt's first and strongest line of defence, and brought the Egyptians under Psamtik III, son and successor of Ahmose, to battle at Pelusium. The Egyptians lost and retired to Memphis; the city fell to the Persian control and the Pharaoh was carried off in captivity to Susa in mainland Persia. 

After the death of the last Nabatean king, Rabbel II Soter, in 106, the Roman emperor Trajan faced practically no resistance and conquered the kingdom on 22 March 106. With this conquest, the Roman Empire went on to control all shores of the Mediterranean Sea. The Sinai Peninsula became part of the Roman province of Arabia Petraea.

Saint Catherine's Monastery on the foot of Mount Sinai was constructed by order of the Emperor Justinian between 527 and 565. Most of the Sinai Peninsula became part of the province of Palaestina Salutaris in the 6th century.

During the Crusades it was under the control of Fatimid Caliphate. Later, Saladin abolished the Fatimid Caliphate in Egypt and took this region under his control too. It was the military route from Cairo to Damascus during the Crusades. And in order to secure this route, he built a citadel on the island of Pharaoh in Taba known by his name 'Saladin's citadel'.

The peninsula was governed as part of Egypt under the Mamluk Sultanate of Egypt from 1260 until 1517, when the Ottoman Sultan, Selim the Grim, defeated the Egyptians at the Battles of Marj Dabiq and al-Raydaniyya, and incorporated Egypt into the Ottoman Empire. From then until 1906, Sinai was administered by the Ottoman provincial government of the "Pashalik" of Egypt, even following the establishment of the Muhammad Ali Dynasty's rule over the rest of Egypt in 1805.

In 1906, the Ottoman Porte formally transferred administration of Sinai to the Egyptian government, which essentially meant that it fell under the control of the United Kingdom, who had occupied and largely controlled Egypt since 1882. The border imposed by the British runs in an almost straight line from Rafah on the Mediterranean shore to Taba on the Gulf of Aqaba. This line has served as the eastern border of Egypt ever since.

At the beginning of the 1948 Arab–Israeli War, Egyptian forces entered the former British Mandate of Palestine from Sinai to support Palestinian and other Arab forces against the newly declared State of Israel. For a period during the war, Israeli forces entered the north-eastern corner of Sinai. With the exception of Palestine's Gaza Strip, which came under the administration of the All-Palestine Government, the western frontier of the former Mandate of Palestine became the Egyptian–Israeli frontier under the 1949 Armistice Agreement. In 1958, the Gaza Strip came under direct Egyptian military administration, though it was governed separately from Sinai, and was never annexed by Egypt. The Egyptian government maintained that Egyptian administration would be terminated upon the end of the conflict with Israel.

In 1956, Egypt nationalised the Suez Canal, a waterway marking the boundary between Egyptian territory in Africa and the Sinai Peninsula. Thereafter, Israeli ships were prohibited from using the Canal, owing to the state of war between the two states. Egypt also prohibited ships from using Egyptian territorial waters on the eastern side of the peninsula to travel to and from Israel, effectively imposing a blockade on the Israeli port of Eilat. In October 1956, in what is known in Egypt as the Tripartite Aggression, Israeli forces, aided by Britain, and France (which sought to reverse the nationalization and regain control over the Suez Canal), invaded Sinai and occupied much of the peninsula within a few days. In March 1957, Israel withdrew its forces from Sinai, following strong pressure from the United States and the Soviet Union. Thereafter, the United Nations Emergency Force (UNEF) was stationed in Sinai to prevent any further conflict in the Sinai.

On16 May 1967, Egypt ordered the UNEF out of Sinai and reoccupied it militarily. Secretary-General U Thant eventually complied and ordered the withdrawal without Security Council authorisation. In the course of the Six-Day War that broke out shortly thereafter, Israel captured the entire Sinai Peninsula, and Gaza Strip from Egypt, the West Bank (including East Jerusalem) from Jordan (which Jordan had controlled since 1949), and the Golan Heights from Syria. The Suez Canal, the east bank of which was now occupied by Israel, was closed. Israel commenced efforts at large scale Israeli settlement in the Sinai Peninsula.

Following the Israeli conquest of Sinai, Egypt launched the War of Attrition (1967–70) aimed at forcing Israel to withdraw from the Sinai. The war saw protracted conflict in the Suez Canal Zone, ranging from limited to large scale combat. Israeli shelling of the cities of Port Said, Ismailia, and Suez on the west bank of the canal, led to high civilian casualties (including the virtual destruction of Suez), and contributed to the flight of 700,000 Egyptian internal refugees. Ultimately, the war concluded in 1970 with no change in the front line.

On 6 October 1973, Egypt commenced Operation Badr to retake the Sinai, while Syria launched a simultaneous operation to retake the Golan Heights, thereby beginning the Yom Kippur War (known in Egypt and much of Europe as the "October War"). Egyptian engineering forces built pontoon bridges to cross the Suez Canal, and stormed the Bar-Lev Line, Israel's defensive line along the Suez Canal's east bank. Though the Egyptians maintained control of most of the east bank of the Suez Canal, in the later stages of the war, the Israeli military crossed the southern section of the Suez Canal, cutting off the Egyptian 3rd Army, and occupied a section of the Suez Canal's west bank. The war ended following a mutually agreed-upon ceasefire. After the war, as part of the subsequent Sinai Disengagement Agreements, Israel withdrew from immediate proximity with the Suez Canal, with Egypt agreeing to permit passage of Israeli ships. The canal was reopened in 1975, with President Sadat leading the first convoy through the canal aboard an Egyptian destroyer.

In 1979, Egypt and Israel signed a peace treaty in which Israel agreed to withdraw from the entirety of the Sinai Peninsula. Israel subsequently withdrew in several stages, ending in 1982. The Israeli pull-out involved dismantling almost all Israeli settlements, including the settlement of Yamit in north-eastern Sinai. The exception was that the coastal city of Sharm el-Sheikh (which the Israelis had founded as Ofira during their occupation of the Sinai Peninsula) was not dismantled. The Treaty allows monitoring of Sinai by the Multinational Force and Observers, and limits the number of Egyptian military forces in the peninsula.

Since the early 2000s, Sinai has been the site of several terror attacks against tourists, the majority of whom are Egyptian. Investigations have shown that these were mainly motivated by a resentment of the poverty faced by many Bedouin in the area. Attacking the tourist industry was viewed as a method of damaging the industry so that the government would pay more attention to their situation. (See 2004 Sinai bombings, 2005 Sharm El Sheikh bombings and 2006 Dahab bombings). Since the 2011 Egyptian Revolution, unrest has become more prevalent in the area including the 2012 Egyptian-Israeli border attack in which 16 Egyptian soldiers were killed by militants. (See Sinai insurgency).

Also on the rise are kidnappings of refugees. According to Meron Estifanos, Eritrean refugees are often kidnapped by Bedouin in the northern Sinai, tortured, raped, and only released after receiving a large ransom.

Under President el-Sisi, Egypt has implemented a rigorous policy of controlling the border to the Gaza Strip, including the dismantling of tunnels between Gaza and Sinai.

The two governorates of North and South Sinai have a total population of 597,000 (January 2013). This figure rises to 1,400,000 by including Western Sinai, the parts of the Port Said, Ismailia and Suez Governorates lying east of the Suez Canal. Port Said alone has a population of roughly 500,000 people (January 2013). Portions of the populations of Ismailia and Suez live in west Sinai, while the rest live on the western side of the Suez Canal.

Population of Sinai has largely consisted of desert-dwelling Bedouins with their colourful traditional costumes and significant culture. Large numbers of Egyptians from the Nile Valley and Delta moved to the area to work in tourism, but development adversely affected the native Bedouin population. In order to help alleviate their problems, various NGOs began to operate in the region, including the Makhad Trust, a UK charity that assists the Bedouin in developing a sustainable income while protecting Sinai's natural environment, heritage and culture.

Since the Israeli–Egyptian peace treaty, Sinai's scenic spots (including coral reefs offshore) and religious structures have become important to the tourism industry. The most popular tourist destination in Sinai are Mount Sinai ("Jabal Musa") and St Catherine's Monastery, which is considered to be the oldest working Christian monastery in the world, and the beach resorts of Sharm el-Sheikh, Dahab, Nuweiba and Taba. Most tourists arrive at Sharm el-Sheikh International Airport, through Eilat, Israel and the Taba Border Crossing, by road from Cairo or by ferry from Aqaba in Jordan.







</doc>
<doc id="27646" url="https://en.wikipedia.org/wiki?curid=27646" title="Spy fiction">
Spy fiction

Spy fiction, a genre of literature involving espionage as an important context or plot device, emerged in the early twentieth century, inspired by rivalries and intrigues between the major powers, and the establishment of modern intelligence agencies. It was given new impetus by the development of fascism and communism in the lead-up to World War II, continued to develop during the Cold War, and received a fresh impetus from the emergence of rogue states, international criminal organizations, global terrorist networks, maritime piracy and technological sabotage and espionage as potent threats to Western societies.

As a genre, spy fiction is thematically related to the novel of adventure ("The Prisoner of Zenda", 1894, "The Scarlet Pimpernel", 1905), the thriller (such as the works of Edgar Wallace) and the politico–military thriller ("The Schirmer Inheritance", 1953, "The Quiet American", 1955).

Early examples of the espionage novel are "The Spy" (1821) and "The Bravo" (1831), by American novelist James Fenimore Cooper. "The Bravo" attacks European anti-republicanism, by depicting Venice as a city-state where a ruthless oligarchy wears the mask of the "serene republic".

In nineteenth-century France, the Dreyfus Affair (1894–99) contributed much to public interest in espionage. For some twelve years (ca. 1894–1906), the Affair, which involved elements of international espionage, treason, and anti-Semitism, dominated French politics. The details were reported by the world press: an Imperial German penetration agent betraying to Germany the secrets of the General Staff of the French Army; the French counter-intelligence riposte of sending a charwoman to rifle the trash in the German Embassy in Paris, were news that inspired successful spy fiction.

The major themes of spy in the lead-up to the First World War were the continuing rivalry between the European colonial powers for control of Asia, the growing threat of conflict in Europe, the domestic threat of revolutionaries and anarchists, and historical romance.

"Kim" (1901) by Rudyard Kipling concerns the Anglo–Russian Great Game of imperial and geopolitical rivalry and strategic warfare for supremacy in Central Asia, usually in Afghanistan. "The Secret Agent" (1907) by Joseph Conrad examines the psychology and ideology motivating the socially marginal men and women of a revolutionary cell determined to provoke revolution in Britain with a terrorist bombing of the Greenwich Observatory. Conrad's next novel, "Under Western Eyes" (1911), follows a reluctant spy sent by the Russian Empire to infiltrate a group of revolutionaries based in Geneva. G. K. Chesterton's "The Man Who Was Thursday" (1908) is a metaphysical thriller ostensibly based on the infiltration of an anarchist organisation by detectives; but the story is actually a vehicle for exploring society's power structures and the nature of suffering.
The fictional detective Sherlock Holmes, created by Arthur Conan Doyle, served as a spyhunter for Britain in the stories "The Adventure of the Second Stain" (1904), and "The Adventure of the Bruce-Partington Plans" (1912). In "His Last Bow" (1917), he served Crown and country as a double agent, transmitting false intelligence to Imperial Germany on the eve of the Great War.

"The Scarlet Pimpernel" (1905) by Baroness Orczy chronicled an English aristocrat's derring-do in rescuing French aristocrats from the Reign of Terror of the populist French Revolution (1789–99).

But the term "spy novel" was defined by "The Riddle of the Sands" (1903) by Irish author Robert Erskine Childers. It described amateur spies discovering a German plan to invade Britain. Its success created a market for the invasion literature subgenre, which was flooded by imitators. William Le Queux and E. Phillips Oppenheim became the most widely read and most successful British writers of spy fiction, especially of invasion literature. Their prosaic style and formulaic stories, produced voluminously from 1900 to 1914, proved of low literary merit.

During the War, the propagandist John Buchan became the pre-eminent British spy novelist. His well-written stories portray the Great War as a "clash of civilisations" between Western civilization and barbarism. His notable novels are "The Thirty-nine Steps" (1915), "Greenmantle" (1916) and sequels, all featuring the heroic Scotsman Richard Hannay. In France Gaston Leroux published the spy thriller "Rouletabille chez Krupp" (1917), in which a detective, Joseph Rouletabille, engages in espionage.

After the Russian Revolution (1917), the quality of spy fiction declined, perhaps because the Bolshevik enemy won the Russian Civil War (1917–23). Thus, the inter-war spy story usually concerns combating the Red Menace, which was perceived as another "clash of civilizations".

Spy fiction was dominated by British authors during this period, initially former intelligence officers and agents writing from inside the trade. Examples include "" (1928) by W. Somerset Maugham, which accurately portrays spying in the First World War, and "The Mystery of Tunnel 51" (1928) by Alexander Wilson whose novels convey an uncanny portrait of the first head of the Secret Intelligence Service, Mansfield Smith-Cumming, the original 'C'.

At a more popular level, Leslie Charteris' popular and long-running "Saint" series began, featuring Simon Templar, with "Meet the Tiger" (1928). "Water on the Brain" (1933) by former intelligence officer Compton Mackenzie was the first successful spy novel satire. Prolific author Dennis Wheatley also wrote his first spy novel, "The Eunuch of Stamboul" (1935) during this period.

The growing threat of fascism in Germany, Italy and Spain, and the imminence of war, attracted quality writers back to spy fiction.

British author Eric Ambler brought a new realism to spy fiction. "The Dark Frontier" (1936), "Epitaph for a Spy" (1938), "The Mask of Dimitrios" (US: "A Coffin for Dimitrios", 1939), and "Journey into Fear" (1940) feature amateurs entangled in espionage. The politics and ideology are secondary to the personal story that involved the hero or heroine. Ambler's Popular Front–period "œuvre" has a left-wing perspective about the personal consequences of "big picture" politics and ideology, which was notable, given spy fiction's usual right-wards tilt in defence of the Establishment attitudes underpinning empire and imperialism. Ambler's early novels "Uncommon Danger" (1937) and "Cause for Alarm" (1938), in which NKVD spies help the amateur protagonist survive, are especially remarkable among English-language spy fiction.

"Above Suspicion" (1939) by Helen MacInnes, about an anti-Nazi husband and wife spy team, features literate writing and fast-paced, intricate, and suspenseful stories occurring against contemporary historical backgrounds. MacInnes wrote many other spy novels in the course of a long career, including "Assignment in Brittany" (1942), "Decision at Delphi" (1961), and "Ride a Pale Horse" (1984).

Manning Coles published "Drink to Yesterday" (1940), a grim story occurring during the Great War, which introduces the hero Thomas Elphinstone Hambledon. However, later novels featuring Hambledon were lighter-toned, despite being set either in Nazi Germany or Britain during the Second World War (1939–45). After the War, the Hambledon adventures fell to formula, losing critical and popular interest.

The events leading up to the Second World War, and the War itself, continue to be fertile ground for authors of spy fiction. Notable examples include Ken Follett, "Eye of the Needle" (1978); Alan Furst, "Night Soldiers" (1988); and David Downing, the Station series, beginning with "Zoo Station" (2007).

The metamorphosis of the Second World War (1939–45) into the Soviet–American Cold War (1945–91) gave new impetus to spy novelists. "Atomsk" by Paul Linebarger, written in 1948 and published in 1949, appears to be the first espionage novel of the dawning conflict.

With "Secret Ministry" (1951), Desmond Cory introduced Johnny Fedora, the secret agent with a licence to kill, the government-sanctioned assassin. Ian Fleming, a former member of naval intelligence, followed swiftly with the glamorous James Bond, secret agent 007 of the British Secret Service, a mixture of counter-intelligence officer, assassin and playboy. Perhaps the most famous fictional spy, Bond was introduced in "Casino Royale" (1953). After Fleming's death the franchise continued under other British and American authors, including Kingsley Amis, Christopher Wood, John Gardner, Raymond Benson, Sebastian Faulks, Jeffery Deaver and William Boyd.

Despite the commercial success of Fleming's extravagant novels, John le Carré, himself a former spy, created anti-heroic protagonists who struggled with the ethical issues involved in espionage, and sometimes resorted to immoral tactics. Le Carré's middle-class George Smiley is a middle-aged spy burdened with an unfaithful, upper-class wife who publicly cuckolds him for sport.

Like Le Carré, former British Intelligence officer Graham Greene also examined the morality of espionage in left-leaning, anti-imperialist novels such as "The Heart of the Matter" (1948), set in Sierra Leone, the seriocomic "Our Man in Havana" (1959) occurring in the Cuba of dictator Fulgencio Batista before his deposition by Fidel Castro's popular Cuban Revolution (1953–59), and "The Human Factor" (1978) about British support for the apartheid National Party government of South Africa, against the Red Menace.

Other novelists followed a similar path. Len Deighton's anonymous spy, protagonist of "The IPCRESS File" (1962), "Horse Under Water" (1963), "Funeral in Berlin" (1964), and others, is a working-class man with a negative view of the Establishment.

Other notable examples of espionage fiction during this period were also built around recurring characters. These include James Mitchell's 'John Craig' series, written under his pseudonym 'James Munro', beginning with "The Man Who Sold Death" (1964); and Trevor Dudley-Smith's Quiller spy novel series written under the pseudonym 'Adam Hall', beginning with "The Berlin Memorandum" (US: "The Quiller Memorandum", 1965), a hybrid of glamour and dirt, Fleming and Le Carré; and William Garner's fantastic Michael Jagger in "Overkill" (1966), "The Deep, Deep Freeze" (1968), "The Us or Them War" (1969) and "A Big Enough Wreath" (1974).

Other important British writers who first became active in spy fiction during this period include Padraig Manning O'Brine, "Killers Must Eat" (1951); Michael Gilbert, "Be Shot for Sixpence" (1956); Alistair MacLean, "The Last Frontier" (1959); Brian Cleeve, "Assignment to Vengeance" (1961); Jack Higgins, "The Testament of Caspar Schulz" (1962); and Desmond Skirrow, "It Won't Get You Anywhere" (1966). Dennis Wheatley's 'Gregory Sallust' (1934-1968) and 'Roger Brook' (1947-1974) series were also largely written during this period.

During the war E. Howard Hunt wrote his first spy novel, "East of Farewell" (1943). In 1949 he joined the recently created CIA, and continued to write spy fiction for many years. Paul Linebarger, a China specialist for the CIA, published "Atomsk", the first novel of the Cold War, in 1949. In 1955, Edward S. Aarons began publishing the Sam Durell CIA "Assignment" series, which began with "Assignment to Disaster" (1955). Donald Hamilton published "Death of a Citizen" (1960) and "The Wrecking Crew" (1960), beginning the series featuring Matt Helm, a CIA assassin and counter-intelligence agent.

The Nick Carter-Killmaster series of spy novels, initiated by Michael Avallone and Valerie Moolman, but authored anonymously, ran to over 260 separate books between 1964 and the early 1990s and invariably pitted American, Soviet and Chinese spies against each other. With the proliferation of male protagonists in the spy fiction genre, writers and book packagers also started bringing out spy fiction with a female as the protagonist. One notable spy series is "The Baroness", featuring a sexy female superspy, with the novels being more action-oriented, in the mould of Nick Carter-Killmaster.

Other important American authors who became active in spy fiction during this period include Ross Thomas, "The Cold War Swap" (1966).

The June 1967 Six-Day War between Israel and its neighbours introduced new themes to espionage fiction - the conflict between Israel and the Palestinians, against the backdrop of continuing Cold War tensions, and the increasing use of terrorism as a political tool.

Notable recurring characters from this era include Adam Diment's Philip McAlpine is a long-haired, hashish-smoking fop in the novels "The Dolly Dolly Spy" (1967), "The Great Spy Race" (1968), "The Bang Bang Birds" (1968) and "Think, Inc." (1971); James Mitchell's 'David Callan' series, written in his own name, beginning with "Red File for Callan" (1969); William Garner's John Morpurgo in "Think Big, Think Dirty" (1983), "Rats' Alley" (1984), and "Zones of Silence" (1986); and Joseph Hone's 'Peter Marlow' series, beginning with "The Private Sector" (1971), set during Israel's Six-Day War (1967) against Egypt, Jordan and Syria. In all of these series the writing is literary and the tradecraft believable.

Noteworthy examples of the journalistic style and successful integration of fictional characters with historical events were the politico–military novels "The Day of the Jackal" (1971) by Frederick Forsyth and "Eye of the Needle" (1978) by Ken Follett. With the explosion of technology, Craig Thomas, launched the techno-thriller with "Firefox" (1977), describing the Anglo–American theft of a superior Soviet jet aeroplane.

Other important British writers who first became active in spy fiction during this period include Ian Mackintosh, "A Slaying in September" (1967); Kenneth Benton, "Twenty-fourth Level" (1969); Desmond Bagley, "Running Blind" (1970); Anthony Price, "The Labyrinth Makers" (1971); Gerald Seymour, "Harry's Game" (1975); Brian Freemantle, "Charlie M" (1977); Bryan Forbes, "Familiar Strangers" (1979); Reginald Hill, "The Spy's Wife" (1980); and Raymond Harold Sawkins, writing as Colin Forbes, "Double Jeopardy" (1982).

"The Scarlatti Inheritance" (1971) by Robert Ludlum is usually considered the first American modern (glamour and dirt) spy thriller weighing action and reflection. In the 1970s, former CIA man Charles McCarry began the Paul Christopher series with "The Miernik Dossier" (1973) and "The Tears of Autumn" (1978), which were well written, with believable tradecraft.

The first American techno-thriller was "The Hunt for Red October" (1984) by Tom Clancy. It introduced CIA deskman (analyst) Jack Ryan as a field agent; he reprised the role in the sequel "The Cardinal of the Kremlin" (1987).

Other important American authors who became active in spy fiction during this period include Robert Littell, "The Defection of A. J. Lewinter" (1973); James Grady, "Six Days of the Condor" (1974); William F. Buckley Jr., "Saving the Queen" (1976); Nelson DeMille, "The Talbot Odyssey" (1984); W. E. B. Griffin, the "Men at War" series (1984–); Stephen Coonts, "Flight of the Intruder" (1986); Canadian-American author David Morrell, "The League of Night and Fog" (1987); David Hagberg, "Without Honor" (1989); Noel Hynd, "False Flags" (1990); and Richard Ferguson, "Oiorpata" (1990).

French journalist Gérard de Villiers began to write his "SAS" series in 1965. The franchise now extends to 200 titles and 150 million books.

Julian Semyonov was an influential spy novelist, writing in the Eastern Bloc, whose range of novels and novel series featured a White Russian spy in the USSR; Max Otto von Stierlitz, a Soviet mole in the Nazi High Command, and Felix Dzerzhinsky, founder of the Cheka. In his novels, Semyonov covered much Soviet intelligence history, ranging from the Russian Civil War (1917–1923), through the Great Patriotic War (1941–45), to the Russo–American Cold War (1945–91).

Swedish author Jan Guillou also began to write his "Coq Rouge" series, featuring Swedish spy Carl Hamilton, during this period, beginning in 1986.

The end of the Cold War in 1991 mooted the USSR, Russia and other Iron Curtain countries as credible enemies of democracy, and the US Congress even considered disestablishing the CIA. Espionage novelists found themselves at a temporary loss for obvious nemeses. "The New York Times" ceased publishing a spy novel review column. Nevertheless, counting on the aficionado, publishers continued to issue spy novels by writers popular during the Cold War era, among them "Harlot's Ghost" (1991) by Norman Mailer.

In the US, the new novels "Moscow Club" (1991) by Joseph Finder, "Coyote Bird" (1993) by Jim DeFelice, "Masquerade" (1996) by Gayle Lynds, and "The Unlikely Spy" (1996) by Daniel Silva maintained the spy novel in the post–Cold War world. Other important American authors who first became active in spy fiction during this period include David Ignatius, "Agents of Innocence" (1997); David Baldacci, "Saving Faith" (1999); and Vince Flynn, with "Term Limits" (1999) and a series of novels featuring counter-terrorism expert Mitch Rapp.

In the UK, Robert Harris entered the spy genre with "Enigma" (1995). Other important British authors who became active during this period include Hugh Laurie, "The Gun Seller" (1996); Andy McNab, "Remote Control" (1998); Henry Porter, "Remembrance Day" (2000); and Charles Cumming, "A Spy By Nature" (2001).

The terrorist attacks against the US on 11 September 2001, and the subsequent War on Terror, reawakened interest in the peoples and politics of the world beyond its borders. Espionage genre elders such as John le Carré, Frederick Forsyth, Robert Littell, and Charles McCarry resumed work, and many new authors emerged.

Important British writers who wrote their first spy novels during this period include Stephen Leather, "Hard Landing" (2004); and William Boyd, "Restless" (2006).

New American writers include Brad Thor, "The Lions of Lucerne" (2002); Ted Bell, "Hawke" (2003); Alex Berenson, with John Wells appearing for the first time in "The Faithful Spy" (2006); Brett Battles, "The Cleaner" (2007); Ellis Goodman, "Bear Any Burden" (2008); Olen Steinhauer, "The Tourist" (2009); and Richard Ferguson, "Oiorpata" (2012). A number of other established writers began to write spy fiction for the first time, including Kyle Mills, "Fade" (2005) and James Patterson, "Private" (2010).

Swede Stieg Larsson, who died in 2004, was the world's second best-selling author for 2008 due to his "Millennium series", featuring Lisbeth Salander, published posthumously between 2005 and 2007. Other authors of note include Australian James Phelan, beginning with "Fox Hunt" (2010).

Recognising the importance of the thriller genre, including spy fiction, International Thriller Writers (ITW) was established in 2004, and held its first conference in 2006.

Many authors of spy fiction have themselves been intelligence officers working for British agencies such as MI5 or MI6, or American agencies such as the OSS or its successor, the CIA. 'Insider' spy fiction has a special claim to authenticity, and overlaps with biographical and other documentary accounts of secret service.

The first insider fiction emerged after World War 1 as the thinly disguised reminiscences of former British intelligence officers such as W. Somerset Maugham, Alexander Wilson, and Compton Mackenzie. The tradition continued during World War II with Helen MacInnes and Manning Coles.

Notable British examples from the Cold War period and beyond include Ian Fleming, John le Carré, Graham Greene, Brian Cleeve, Ian Mackintosh, Kenneth Benton, Bryan Forbes, Andy McNab and Chris Ryan. Notable American examples include Charles McCarry, William F. Buckley Jr., W. E. B. Griffin and David Hagberg.

Many post-attack period novels are written by insiders. At the CIA, the number of manuscripts submitted for pre-publication vetting doubled between 1998 and 2005. American examples include Barry Eisler, "A Clean Kill in Tokyo" (2002); Charles Gillen, "Saigon Station" (2003); R J Hillhouse, "Rift Zone" (2004); Gene Coyle, "The Dream Merchant of Lisbon" (2004) and "No Game For Amateurs" (2009); Thomas F. Murphy, "Edge of Allegiance" (2005); Mike Ramsdell, "A Train to Potevka" (2005); T. H. E. Hill, "" (2008); Duane Evans, "North from Calcutta" (2009); Jason Matthews, "Red Sparrow" (2013).; and T.L. Williams, "Zero Day: China's Cyber Wars" (2017).

British examples include "The Code Snatch" (2001) by Alan Stripp, formerly a cryptographer at Bletchley Park; "At Risk" (2004), "Secret Asset" (2006), "Illegal Action" (2007), and "Dead Line" (2008), by Dame Stella Rimington (Director General of MI5 from 1992 to 1996); and Matthew Dunn's "Spycatcher" (2011) and sequels.

Much spy fiction was adapted as spy films in the 1960s, ranging from the fantastical James Bond series to the realistic "The Spy Who Came in from the Cold" (1965), and the hybrid "The Quiller Memorandum" (1966). While Hamilton's Matt Helm novels were adult and well written, their cinematic interpretations were adolescent parody. This phenomenon spread widely in Europe in the 1960s and is known as the Eurospy genre.

English-language spy films of the 2000s include "The Bourne Identity" (2002), "" (1996); "Munich" (2005), "Syriana" (2005), and "The Constant Gardener" (2005).

Among the comedy films focusing on espionage are 1974's "S*P*Y*S" and 1985's "Spies Like Us".

In March 2015, filming of Howard Kaplan's best selling "The Damascus Cover" as "Damascus Cover" wrapped in Casablanca starring Jonathan Rhys Meyers, John Hurt, Jurgen Prochnow and Olivia Thirlby. It is set in Damascus and Jerusalem circa 1989 at the time of the Berlin Wall falling.

The American adaptation of "Casino Royale" (1954) featured Jimmy Bond in an episode of the "Climax!" anthology series. The narrative tone of television espionage ranged from the drama of "Danger Man" (1960–68) to the sardonicism of "The Man from U.N.C.L.E" (1964–68) and the flippancy of "I Spy" (1965–68) until the exaggeration, akin to that of William Le Queux and E. Phillips Oppenheim before the First World War (1914–18), degenerated to the parody of "Get Smart" (1965–70).

In 1973, Semyonov's novel "Seventeen Moments of Spring" (1968) was adapted to television as a twelve-part mini-series about the Soviet spy Maksim Isaev operating in wartime Nazi Germany as Max Otto von Stierlitz, charged with preventing a separate peace between Nazi Germany and America which would exclude the USSR. The programme "TASS Is Authorized to Declare..." also derives from his work.

However, the circle closed in the late 1970s when "The Sandbaggers" (1978–80) presented the grit and bureaucracy of espionage.

In the 1980s, US television featured the light espionage programmes "Airwolf" (1984–87) and "MacGyver" (1985–92), each rooted in the Cold War yet reflecting American citizens' distrust of their government, after the crimes of the Nixon Government (the internal, political espionage of the Watergate Scandal and the Vietnam War) were exposed. The spy heroes were independent of government; MacGyver, in later episodes and post-DXS employment, works for a non-profit, private think tank, and aviator Hawke and two friends work free-lance adventures. Although each series features an intelligence agency, the DXS in "MacGyver", and the FIRM, in "Airwolf", its agents could alternately serve as adversaries as well as allies for the heroes.

Television espionage programmes of the late 1990s to the early 2010s include "La Femme Nikita" (1997–2001), "Alias" (2001–2006), "24" (2001-2010, 2014), "Spooks" in the UK (release as "MI-5" in the USA and Canada) (2002-2011), CBBC's "The Secret Show" (2006-2011), NBC's "Chuck" (2007-2012), FX's "Archer" (2009–present), "Burn Notice", "Covert Affairs", "Homeland" and "The Americans".

In 2015, "Deutschland 83" is a German television series starring a 24-year-old native of East Germany who is sent to the West as an undercover spy for the HVA, the foreign intelligence agency of the Stasi.

In every medium, spy thrillers introduce children and adolescents to deception and espionage at earlier ages. The genre ranges from action adventure, such as Chris Ryan's "Alpha Force" series, through the historical espionage dramas of Y. S. Lee, to the girl orientation of Ally Carter's "Gallagher Girls" series, beginning with "I'd Tell You I Love You, But Then I'd Have to Kill You".

Leading examples include the "Agent Cody Banks" film, the Alex Rider adventure novels by Anthony Horowitz, and the CHERUB series, by Robert Muchamore. Ben Allsop, one of England's youngest novelists, also writes spy fiction. His titles include "Sharp" and "The Perfect Kill".

Spy-related films that are aimed towards younger audiences include movies such as the Spy Kids series of films and The Spy Next Door.

Other authors writing for adolescents include A. J. Butcher, Joe Craig, Charlie Higson, Andy McNab and Francine Pascal.

In contemporary digital video games, the player can be a vicarious spy, as in the "Metal Gear series", especially in the series' third installment, "Metal Gear Solid", unlike the games of the Third-Person Shooter genre, "Syphon Filter", and "Splinter Cell". The games feature complex stories and cinematic images. Games such as "No One Lives Forever" and the sequel "No One Lives Forever 2: A Spy in H.A.R.M.'s Way" humorously combine espionage and 1960s design. "Evil Genius (game)", contemporary to NOLF series, allows the player to be the villain and its strategy occurs real time.

The "Spyland" espionage theme park, in the Gran Scala pleasure dome, in Zaragoza province, Spain, opened in 2012.






</doc>
<doc id="27647" url="https://en.wikipedia.org/wiki?curid=27647" title="Star height problem">
Star height problem

The star height problem in formal language theory is the question whether all regular languages can be expressed using regular expressions of limited star height, i.e. with a limited nesting depth of Kleene stars. Specifically, is a nesting depth of one always sufficient? If not, is there an algorithm to determine how many are required? The problem was raised by .

The first question was answered in the negative when in 1963, Eggan gave examples of regular languages of star height "n" for every "n". Here, the star height "h"("L") of a regular language "L" is defined as the minimum star height among all regular expressions representing "L". The first few languages found by are described in the following, by means of giving a regular expression for each language:

The construction principle for these expressions is that expression formula_2 is obtained by concatenating two copies of formula_3, appropriately renaming the letters of the second copy using fresh alphabet symbols, concatenating the result with another fresh alphabet symbol, and then by surrounding the resulting expression with a Kleene star. The remaining, more difficult part, is to prove that for formula_3 there is no equivalent regular expression of star height less than "n"; a proof is given in .

However, Eggan's examples use a large alphabet, of size 2-1 for the language with star height "n". He thus asked whether we can also find examples over binary alphabets. This was proved to be true shortly afterwards by . 
Their examples can be described by an inductively defined family of regular expressions over the binary alphabet formula_5 as follows–cf. :

Again, a rigorous proof is needed for the fact that formula_3 does not admit an equivalent regular expression of lower star height. Proofs are given by and by .

In contrast, the second question turned out to be much more difficult, and the question became a famous open problem in formal language theory for over two decades . For years, there was only little progress. The pure-group languages were the first interesting family of regular languages for which the star height problem was proved to be decidable . But the general problem remained open for more than 25 years until it was settled by Hashiguchi, who in 1988 published an algorithm to determine the star height of any regular language. The algorithm wasn't at all practical, being of non-elementary complexity. To illustrate the immense resource consumptions of that algorithm, Lombardy and Sakarovitch (2002) give some actual numbers:

Notice that alone the number formula_8 has 10 billion zeros when written down in decimal notation, and is already "by far" larger than the number of atoms in the observable universe.

A much more efficient algorithm than Hashiguchi's procedure was devised by Kirsten in 2005. This algorithm runs, for a given nondeterministic finite automaton as input, within double-exponential space. Yet the resource requirements of this algorithm still greatly exceed the margins of what is considered practically feasible.

This algorithm has been optimized and generalized to trees by Colcombet and Löding in 2008, as part of the theory of regular cost functions.
It has been implemented in 2017 in the tool suite Stamina.




</doc>
<doc id="27648" url="https://en.wikipedia.org/wiki?curid=27648" title="William Crookes">
William Crookes

Sir William Crookes (; 17 June 1832 – 4 April 1919) was a British chemist and physicist who attended the Royal College of Chemistry in London, and worked on spectroscopy. He was a pioneer of vacuum tubes, inventing the Crookes tube which was made in 1875. In 1913, Crookes invented 100% ultraviolet blocking sunglass lens. Crookes was the inventor of the Crookes radiometer, which today is made and sold as a novelty item. Late in life, he became interested in spiritualism, and became the president of the Society for Psychical Research.

Crookes made a career of being a meteorologist and fierce lecturer for multiple studies and courses. Crookes worked in chemistry and physics. His experiments were notable for the originality of their design. He executed them skillfully. His interests, ranging over pure and applied science, economic and practical problems, and psychiatric research, made him a well-known personality. He received many public and academic honours. Crookes's life was one of unbroken scientific activity.

William Crookes (later Sir William Crookes) was born in London in 1832, the eldest of 16 siblings. His father, Joseph Crookes, was a wealthy tailor and real estate investor, of north-country origin, at that time living with his second wife, Mary Scott Lewis Rutherford Johnson.

At age 16, he entered the Royal College of Chemistry to study organic chemistry. There, he became an assistant to August Wilhelm von Hofmann, allowng him to attend the Royal Institution, and at a meeting Crookes met Michael Faraday who convinced him switch to optical physics.

From 1850 to 1854 he filled the position of assistant in the college, and soon embarked upon original work. It wasn't in organic chemistry which the focus of his teacher, August Wilhelm von Hofmann, might have been expected to lead him towards, but into new compounds of selenium. These were the subject of his first published papers, 1851. He worked with Manuel Johnson at the Radcliffe Observatory in Oxford in 1854, where he adapted the recent innovation of wax paper photography to machines built by Francis Ronalds to continuously record meteorological parameters. In 1855 he was appointed lecturer in chemistry at the Chester Diocesan Training College.

After his father's death he received a large inheritance and opened his own physics laboratory.

In 1856 he married Ellen, daughter of William Humphrey of Darlington. They had three sons and a daughter. Married and living in London, he was devoted mainly to independent work. In 1859, he founded the "Chemical News", a science magazine which he edited for many years and conducted on much less formal lines than was usual for the journals of scientific societies.

In 1861, Crookes discovered a previously unknown element with a bright green emission line in its spectrum and named the element thallium, from the Greek "thallos", a green shoot. Crookes wrote a standard treatise on "Select Methods in Chemical Analysis" in 1871. Crookes was effective in experimentation. The method of spectral analysis, introduced by Bunsen and Kirchhoff, was received by Crookes with great enthusiasm and to great effect. His first important discovery was that of the element thallium, announced in 1861, and made with the help of spectroscopy. By this work his reputation became firmly established, and he was elected a fellow of the "Royal Society" in 1863.

He developed the Crookes tubes, investigating cathode rays. He published numerous papers on spectroscopy and conducted research on a variety of minor subjects. In his investigations of the conduction of electricity in low pressure gases, he discovered that as the pressure was lowered, the negative electrode (cathode) appeared to emit rays (the so-called "cathode rays", now known to be a stream of free electrons, and used in cathode ray display devices). As these examples indicate, he was a pioneer in the construction and use of vacuum tubes for the study of physical phenomena. He was, as a consequence, one of the first scientists to investigate what is now called a plasma and identified it as the fourth state of matter in 1879. He also devised one of the first instruments for studying nuclear radioactivity, the spinthariscope.

Crookes investigated the properties of cathode rays, showing that they travel in straight lines, cause fluorescence when they fall on some substances, and that their impact can produce great heat. He believed that he had discovered a fourth state of matter, which he called "radiant matter", but his theoretical views on the nature of "radiant matter" were to be superseded. He believed the rays to consist of streams of particles of ordinary molecular magnitude. It remained for Sir J. J. Thomson to expound on the subatomic nature of cathode rays (consisting of streams of negative electrons). Nevertheless, Crookes's experimental work in this field was the foundation of discoveries which eventually changed the whole of chemistry and physics.

Crookes' attention had been attracted to the vacuum balance in the course of his research into thallium. He soon discovered the phenomenon which drives the movement in a Crookes radiometer, in which a set of vanes, each blackened on one side and polished on the other, rotate when exposed to radiant energy. Crookes did not, however, provide the true explanation of this apparent "attraction and repulsion resulting from radiation".

After 1880, he lived at 7 Kensington Park Gardens where all his later work was done, in his private laboratory.

Crookes identified the first known sample of helium, in 1895. Crookes was knighted in 1897.

In 1903, Crookes turned his attention to the newly discovered phenomenon of radioactivity, achieving the separation from uranium of its active transformation product, "uranium-X" (later established to be protactinium). Crookes observed the gradual decay of the separated transformation product, and the simultaneous reproduction of a fresh supply in the original uranium. At about the same time as this important discovery, he observed that when ""p-particles"", ejected from radio-active substances, impinge upon zinc sulfide, each impact is accompanied by a minute scintillation, an observation which forms the basis of one of the most useful methods in the technique of radioactivity.

In 1913, Crookes created an 100% ultraviolet and 90% infrared blocking lens made from glass containing cerium, but only lightly tinted. They were an unintended by-product of Crookes's research to find a lens glass formulation that would protect glass workers from cataracts. Crookes tested more than 300 formulations, each numbered and labelled. Crookes Glass 246 was the tint recommended for glassworkers. The best-known Crookes tints are "A" (withdrawn to its uranium), "A1", "B", and "B2", which absorb all ultraviolet below 350nm while darkening visual light. Crookes’ samples were made by Whitefriars, London, stained glass makers, and Chance Brothers, Birmingham.

Crookes became interested in spiritualism in the late 1860s. In this he was possibly influenced by the death of his younger brother Philip in 1867 at age 21 from yellow fever contracted while on an expedition to lay a telegraph cable from Cuba to Florida. In 1867, influenced by Cromwell Fleetwood Varley, Crookes attended a séance to try to get in touch with his brother.

Between 1871 and 1874, Crookes studied the mediums Kate Fox, Florence Cook, and Daniel Dunglas Home. After his investigation he believed that the mediums could produce genuine paranormal phenomena and communicate with spirits. Psychologists Leonard Zusne and Warren H. Jones have described Crookes as gullible as he endorsed fraudulent mediums as genuine.

The anthropologist Edward Clodd noted that Crookes had poor eyesight which may have explained his belief in spiritualist phenomena and quoted William Ramsay as saying Crookes is "so shortsighted that, despite his unquestioned honesty, he cannot be trusted in what he tells you he has seen." Biographer William Hodson Brock wrote that Crookes was "evidently short-sighted, but did not wear spectacles until the 1890s. Until then he may have used a monocle or pocket magnifying glass when necessary. What limitations this imposed upon his psychic investigations we can only imagine."

After studying the reports of Florence Cook, the science historian Sherrie Lynne Lyons wrote that the alleged spirit "Katie King" was Cook herself and at other times an accomplice. Regarding Crookes, Lyons wrote "Here was a man with a flawless scientific reputation, who discovered a new element, but could not detect a real live maiden who was masquerading as a ghost." Cook was repeatedly exposed as a fraudulent medium but she had been "trained in the arts of the séance" which managed to trick Crookes. Some researchers such as Trevor H. Hall suspected that Crookes had an affair with Cook.

In a series of experiments in London, England at the house of Crookes in February 1875, the medium Anna Eva Fay managed to fool Crookes into believing she had genuine psychic powers. Fay later confessed to her fraud and revealed the tricks she had used. Regarding Crookes and his experiments with mediums, the magician Harry Houdini suggested that Crookes had been deceived. The physicist Victor Stenger wrote that the experiments were poorly controlled and "his desire to believe blinded him to the chicanery of his psychic subjects."

In 1897, John Grier Hibben wrote that Crookes' idea of ether waves explaining telepathy was not a scientific hypothesis "he presents no facts to indicate its probability or to save it from being relegated to the sphere of bare conjecture."

In 1906, William Hope tricked Crookes with a fake spirit photograph of his wife. Oliver Lodge revealed there had been obvious signs of double exposure, the picture of Lady Crookes had been copied from a wedding anniversary photograph, however, Crookes was a convinced spiritualist and claimed it was genuine evidence for spirit photography.

The physiologist Gordon Stein suspected that Crookes was too ashamed to admit he had been duped by the medium Florence Cook or he conspired with her for sexual favors. He also suggested that Crookes had conspired with Anna Eva Fay. He noted that contrary to popular belief Home had been exposed as a fraud on several occasions. Stein concluded that all the feats of Home were conjuring tricks. In a review biographer William Brock wrote that Stein made his "case against Crookes and Home clearly and logically."

Crookes joined the Society for Psychical Research, becoming its president in the 1890s: he also joined the Theosophical Society and The Ghost Club, of which he was president from 1907 to 1912. In 1890 he was initiated into the Hermetic Order of the Golden Dawn.




</doc>
<doc id="27650" url="https://en.wikipedia.org/wiki?curid=27650" title="September 16">
September 16







</doc>
<doc id="27651" url="https://en.wikipedia.org/wiki?curid=27651" title="September 23">
September 23







</doc>
<doc id="27655" url="https://en.wikipedia.org/wiki?curid=27655" title="Sonny Bono">
Sonny Bono

Salvatore Phillip "Sonny" Bono (; February 16, 1935 – January 5, 1998) was an American musician, singer-songwriter, producer, actor, and politician who came to fame in partnership with his second wife Cher, as the popular singing duo Sonny & Cher. He was mayor of Palm Springs, California from 1988 to 1992, and the Republican congressman for California's 44th district from 1995 until his death in 1998.

The United States Copyright Term Extension Act of 1998, which extended the term of copyright by 20 years, was named in honor of Bono when it was passed by Congress nine months after his death. Mary Bono (Sonny's last wife) had been one of the original sponsors of the legislation, commonly known as the Sonny Bono Copyright Term Extension Act.

Bono was born in Detroit, to Santo Bono (born in Montelepre, Palermo, Italy) and Zena "Jean" Bono (née La Valle). His mother gave him the nickname "Sonny", which lasted his lifetime. Sonny was the youngest of three siblings; he had two older sisters, Fran and Betty. The family moved to Inglewood, California when he was seven. He attended Inglewood High School, but did not graduate, opting to drop out so he could begin to pursue a career in music. He worked at a variety of jobs while trying to break into the music business, including waiter, truck driver, construction laborer, and butcher's helper.

Bono began his music career as a songwriter at Specialty Records, where his song "Things You Do to Me" was recorded by Sam Cooke, and went on to work for record producer Phil Spector in the early 1960s as a promotion man, percussionist and "gofer". One of his earliest songwriting efforts, "Needles and Pins" was co-written with Jack Nitzsche, another member of Spector's production team. Later in the same decade, he achieved commercial success with his then-wife Cher in the singing duo Sonny and Cher. Bono wrote, arranged, and produced a number of hit records including the singles "I Got You Babe" and "The Beat Goes On", although Cher received more attention as a performer. He played a major part in Cher's solo recording career, writing and producing singles including "Bang Bang" and "You Better Sit Down Kids".

Bono co-wrote "She Said Yeah", covered by The Rolling Stones on their 1965 LP "December's Children". His lone hit single as a solo artist, "Laugh at Me," was released in 1965 and peaked at No. 10 on the "Billboard" Hot 100. In live concerts, Bono would introduce the song by saying "I'd like to sing a medley of my hit." His only other single as a solo artist, "The Revolution Kind," reached No. 70 on the "Billboard" Hot 100 later that year. His solo album, "Inner Views", was released in 1967.
Sonny continued to work with Cher through the early and mid-1970s, starring in a popular television variety show, "The Sonny and Cher Comedy Hour," which ran on CBS from 1971 to 1974. From 1976 to 1977, the duo, since divorced, returned to perform together on "The Sonny and Cher Show". Their last appearance together was on "Late Night with David Letterman" on November 13, 1987, on which they sang "I Got You Babe".

In 2011, Sonny Bono was inducted into the Michigan Rock and Roll Legends Hall of Fame.
Bono's acting career included bit parts as a guest performer in such television series as "Fantasy Island", "Charlie's Angels", "The Love Boat", and "CHiPs". In the 1975 TV movie "Murder on Flight 502", where he played Jack Marshall. He appeared in the 1980 miniseries "Top of the Hill". He played the role of mad bomber Joe Selucci in "" (1982) and appeared in the horror film "Troll" (1986). He also portrayed racist entrepreneur Franklin Von Tussle in the John Waters film "Hairspray" (1988). In "Men in Black" (1997), Bono is one of several oddball celebrities seen on a wall of video screens that monitor extraterrestrials living among us. He also appeared as the Mayor of Palm Springs (which he actually was at the time) in several episodes of "P.S. I Luv U" during the 1991–92 TV season, and on "" (in Season 1, Episode 9, which aired November 21, 1993), in which he played Mayor Frank Berkowitz. He also made a minor appearance as himself in the comedy film "First Kid" (1996).

Bono guest-starred as himself on "The Golden Girls" episode "Mrs. George Devereaux" (originally broadcast November 17, 1990), in which he vied with Lyle Waggoner for Dorothy's (Bea Arthur) affection in a dream sequence. In Blanche's (Rue McClanahan) dream, her husband is still alive, and Bono uses his power as Mayor of Palm Springs to have Waggoner falsely arrested so he can have Dorothy to himself.

Bono entered politics after experiencing great frustration with local government bureaucracy in trying to open a restaurant in Palm Springs, California. Bono placed a successful bid to become the new mayor of Palm Springs. He served four years, from 1988 to 1992. He was instrumental in spearheading the creation of the Palm Springs International Film Festival, which is held each year in Bono's memory.

Bono ran for the Republican nomination for United States Senate in 1992, but the nomination went to the more conservative Bruce Herschensohn, and the election to the Democrat Barbara Boxer. Bono and Herschensohn became close friends after the campaign. Bono was elected to the United States House of Representatives in 1994 to represent California's 44th congressional district. He was one of twelve co-sponsors of a House bill extending copyright. Although that bill was never voted on in the Senate, a similar Senate bill was passed after his death and named the Sonny Bono Copyright Term Extension Act in his memory. It is also known (derisively) as the Mickey Mouse Protection Act.

He championed the restoration of the Salton Sea, bringing the giant lake's plight to national attention. Speaker of the House Newt Gingrich made a public appearance and speech at the shore of the lake on Bono's behalf.

In their book "Tell Newt to Shut Up", David Maraniss and Michael Weisskopf credit Bono with being the first person to recognize Gingrich's public relations problems in 1995. Drawing on his long experience as a celebrity and entertainment producer, Bono (according to Maraniss and Weisskopf) recognized that Gingrich's status had changed from politician to celebrity and that Gingrich was not making allowances for that change:

Bono remains the only member of Congress to have scored a number-one pop single on the US "Billboard" Hot 100 chart.

Bono married his first wife, Donna Rankin, on November 3, 1954. Their daughter Christine ("Christy") was born on June 24, 1958. They divorced in 1962. In 1964, Bono married singer and actress Cher. They had one child, Chaz (born Chastity), born March 4, 1969. In 1975 the duo divorced. Bono then married Susie Coelho but divorced her in 1984. He wed Mary Whitaker in 1986 and they had two children, son Chesare Elan on April 25, 1988, and daughter Chianna Maria on February 2, 1991.

Bono was named a godparent of Anthony Kiedis, who would go on to become a musical artist with his band, Red Hot Chili Peppers. Sonny was a close friend of Anthony's father, Blackie Dammett, and would often take the boy on weekend trips.

Bono was a champion of the Salton Sea in southeastern California, where a park was named in his honor. The 2005 documentary film "Plagues & Pleasures on the Salton Sea" (narrated by John Waters) features Bono and documented the lives of the inhabitants of Bombay Beach, Niland and Salton City, as well as the ecological issues associated with the Sea.

In 1996, a Golden Palm Star on the Palm Springs, California, Walk of Stars was dedicated to him.

He became interested in Scientology and took Scientology courses partly because of the influence of Mimi Rogers, but stated that he was a Roman Catholic on all official documents, campaign materials and websites. His wife Mary also took Scientology courses. However, after his death, Mary Bono stated that "Sonny did try to break away [from the Church of Scientology] at one point, and they made it very difficult for him."

The Church of Scientology said there was no estrangement from Bono.

Bono died on January 5, 1998, of injuries sustained when he hit a tree while skiing on the Nevada side of Heavenly Ski Resort near South Lake Tahoe, California. His death came less than a week after Michael Kennedy, a son of Robert F. Kennedy, died in a similar skiing accident in Aspen, Colorado. After Bono's death, Mary Bono told an interviewer from "TV Guide" that Sonny had been addicted to prescription drugs, mainly Vicodin and Valium. Though Mary claimed that Sonny's drug use caused the accident, the autopsy performed by the Douglas County Coroner showed no indication of any substances or alcohol.

At Mary Bono's request, Cher gave a eulogy at Sonny's funeral. He was buried at Desert Memorial Park in Cathedral City, California. The epitaph on Bono's headstone reads: "AND THE BEAT GOES ON", written in uppercase.

Mary Bono was elected to fill the remainder of his congressional term. She was elected in her own right seven subsequent times before being defeated in the election of 2012.





</doc>
<doc id="27656" url="https://en.wikipedia.org/wiki?curid=27656" title="Single market">
Single market

A single market is a type of trade bloc in which most trade barriers have been removed (for goods) with some common policies on product regulation, and freedom of movement of the factors of production (capital and labour) and of enterprise and services. The goal is that the movement of capital, labour, goods, and services between the members is as easy as within them. The physical (borders), technical (standards) and fiscal (taxes) barriers among the member states are removed to the maximum extent possible. These barriers obstruct the freedom of movement of the four factors of production.

A common market is usually referred to as the first stage towards the creation of a single market. It usually is built upon a free trade area with no tariffs for goods and relatively free movement of capital and of services, but not so advanced in reduction of non-tariff trade barriers.

A unified market is the last stage and ultimate goal of a single market. It requires the total free movement of goods, services (including financial services), capital and people without regard to national boundaries.

A common market allows for the free movement of capital and services but large amounts of trade barriers remain. It eliminates all quotas and “tariffs” – duties on imported goods – from trade in goods within it. However “non-tariff barriers” remain such as differences between the Member States’ safety, packaging requirements and national administrative procedures. They prevent for example manufacturers from marketing the same goods in all member states. The objective of a common market is most often economic convergence and the creation of an integrated single market. It is sometimes considered as the first stage of a single market. The European Economic Community was the first example of a common market.

A single market (sometimes called 'internal market') allows for people, goods, services and capital to move around a union as freely as they do within a single country – instead of being obstructed by national borders and barriers as they were in the past. Citizens can study, live, shop, work and retire in any member state. Consumers enjoy a vast array of products from all member states and businesses have unrestricted access to more consumers. A single market is commonly described as "frontier-free". However, several barriers remain such as differences in national tax systems, differences in parts of the services sector and different requirements for e-commerce. In addition separate national markets still exist for financial services, energy and transport. Laws concerning the recognition of professional qualifications also may not be fully harmonized.
The Eurasian Economic Union, the Gulf Cooperation Council, CARICOM and the European Union are current examples of single markets, although the GCC's single market has been described as "malfunctioning" in 2014. The European Union is the only economic union whose objective is "completing the single market."

A completed, unified market usually refers to the complete removal of barriers and integration of the remaining national markets. Complete economic integration can be seen within many countries, whether in a single unitary state with a single set of economic rules, or among the members of a strong national federation. For example, the sovereign states of the United States do to some degree have different local economic regulations (e.g. licensing requirements for professionals, rules and pricing for utilities and insurance, consumer safety laws, environmental laws, minimum wage) and taxes, but are subordinate to the federal government on any matter of interstate commerce the national government chooses to assert itself. Movement of people and goods among the states is unrestricted and without tariffs.

A single market has many benefits: with full freedom of movement for all the factors of production between the member countries, the factors of production become more efficiently allocated, further increasing productivity.

For both business within the market and consumers, a single market is a competitive environment, making the existence of monopolies more difficult. This means that inefficient companies will suffer a loss of market share and may have to close down. However, efficient firms can benefit from economies of scale, increased competitiveness and lower costs, as well as expecting profitability to increase as a result. Consumers are benefited by the single market in the sense that the competitive environment brings them cheaper products, more efficient providers of products and also increased choice of products. What is more, businesses in competition will innovate to create new products; another benefit for consumers.

Transition to a single market can have a negative impact on some sectors of a national economy due to increased international competition. Enterprises that previously enjoyed national market protection and national subsidy (and could therefore continue in business despite falling short of international performance benchmarks) may struggle to survive against their more efficient peers, even for its traditional markets. Ultimately, if the enterprise fails to improve its organization and methods, it will fail. The consequence may be unemployment or migration.


Note: Every economic union and economic and monetary union includes a common market.


A unified market is the economic term for a single market where goods, services, capital and people can move freely without regard to national boundaries. These "four freedoms" are implemented by, among other things, removal of tariffs on the transfer of goods and services among the member nations, imposition of uniform product standards, revision of laws to permit "market-wide" financial services, and the restructuring of most government procurement practices, so as not to favour local businesses over other member states' businesses.



</doc>
<doc id="27658" url="https://en.wikipedia.org/wiki?curid=27658" title="Special administrative regions of China">
Special administrative regions of China

The special administrative regions (SAR) are one type of provincial-level administrative divisions of China directly under Central People's Government, which enjoys the highest degree of autonomy, and no or less interference by either Central Government or the Communist Party of China.

The legal basis for the establishment of SARs, unlike the administrative divisions of Mainland China, is provided for by Article 31, rather than Article 30, of the Constitution of the People's Republic of China of 1982. Article 31 reads: "The state may establish special administrative regions when necessary. The systems to be instituted in special administrative regions shall be prescribed by law enacted by the National People's Congress in the light of the specific conditions".

At present, there are two SARs established according to the Constitution, namely the Hong Kong SAR and the Macau SAR, former British and Portuguese dependencies respectively, transferred to China in 1997 and 1999 respectively pursuant to the Sino-British Joint Declaration of 1984 and the Sino-Portuguese Joint Declaration of 1987. Pursuant to their Joint Declarations, which are binding inter-state treaties registered with the United Nations, and their Basic laws, the Chinese SARs "shall enjoy a high degree of autonomy". There is additionally the Wolong Special Administrative Region in Sichuan province, which is however not established according to Article 31 of the Constitution. Generally, the two SARs are not considered to constitute a part of Mainland China, by both Chinese and SAR authorities.

The provision to establish special administrative regions appeared in the constitution in 1982, in anticipation of the talks with the United Kingdom over the question of the sovereignty over Hong Kong. It was envisioned as the model for the eventual reunification with Taiwan and other islands, where the Republic of China has resided since 1949. Special administrative regions should not be confused with special economic zones, which are areas in which special economic laws apply to promote trade and investments.

Under the One country, two systems principle, the two SARs continue to possess their own governments, multi-party legislatures, legal systems, police forces, monetary systems, separate customs territory, immigration policies, national sports teams, official languages, postal systems, academic and educational systems, and substantial competence in external relations that are different or independent from the People's Republic of China.

Special administrative regions should be distinguished from the constituent countries system in the United Kingdom or Kingdom of the Netherlands.

There are currently two special administrative regions established according to Article 31 of the Chinese Constitution. For the Wolong Special Administrative Region in Sichuan province, please see the section below.

The two special administrative regions of Hong Kong and Macau (created in 1997 and 1999 respectively) each have a codified constitution called Basic Law. The law provides the regions with a high degree of autonomy, a separate political system, and a capitalist economy under the principle of "one country, two systems" proposed by Deng Xiaoping.

Currently, the two SARs of Hong Kong and Macau are responsible for all affairs except those regarding diplomatic relations and national defense. Consequently, the National People's Congress authorizes the SAR to exercise a high degree of autonomy and enjoy executive, legislative and independent judicial power, and each with their own Courts of Final Appeal.

Special administrative regions are empowered to contract a wide range of agreements with other countries and territories such as mutual abolition of visa requirement, mutual legal aid, air services, extradition, handling of double taxation and others, with no Chinese Government involvement. However, in some diplomatic talks involving a SAR, the SAR concerned may choose to send officials to be part of the Chinese delegation. For example, when former Director of Health of Hong Kong Margaret Chan became the World Health Organization (WHO) Director-General, she served as a delegate from the People's Republic of China to the WHO.

In sporting events the SARs participate under the respective names of ""Hong Kong, China"" and ""Macau, China"", and compete as different entities as they had done since they were under foreign rules, but both SARs are usually allowed to omit the term "", China"" for informal use.

The Government of Hong Kong has established Hong Kong Economic and Trade Offices (HKETOs) in few countries as well as Greater China Region. HKETOs serve as a quasi-interests section in favor of Hong Kong. For regions with no HKETOs, Chinese diplomatic missions take charge of protecting Hong Kong-related interests.

Some countries which have a diplomatic relationship with the central Chinese government maintain Consulate-General offices in Hong Kong.

The People's Liberation Army is garrisoned in both SARs. PRC authorities have said the PLA will not be allowed to interfere with the local affairs of Hong Kong and Macau, and must abide by its laws. In 1988, scholar Chen Fang of the Academy of Military Science even tried to propose the "One military, two systems" concept to separate the defence function and public functions in the army. The PLA does not participate in the governance of the SAR but the SAR may request them for civil-military participation, in times of emergency such as natural disasters. Defence is the responsibility of the PRC government.

A 1996 draft PRC law banned People's Liberation Army–run businesses in HK, but loopholes allow them to operate while the profits are ploughed back into the military. There are many PLA-run corporations in Hong Kong. The PLA also have sizable land-holdings in Hong Kong worth billions of dollars.

Each of the SARs issues passports on its own to its permanent residents who are concurrently Chinese (PRC) citizens. PRC citizens must also satisfy one of the following conditions:


Apart from affording the holder consular protection by the Ministry of Foreign Affairs of the People's Republic of China, these passports also specify that the holder has right of abode in the issuing SAR.

The National People's Congress has also put each SAR in charge of administering the PRC's Nationality Law in its respective realms, namely naturalization, renunciation and restoration of PRC nationality and issuance of proof of nationality.

Due to their colonial past, many inhabitants of the SARs hold some form of non-Chinese nationality (e.g. British National (Overseas) status, British citizenship, British Overseas citizenship or Portuguese citizenship). However, SAR residents who are Chinese descent have always been considered as Chinese citizens by the PRC authorities, an exception to this case is Macau, wherein residents of Chinese descent may choose Chinese or Portuguese nationality. Special interpretation of the Nationality Law, while not recognizing dual nationality, has allowed Chinese citizens to keep their foreign "right of abode" and use travel documents issued by the foreign country. However, such travel documents cannot be used to travel to mainland China and persons concerned must use Home Return Permit. Therefore, master nationality rule applies so the holder may not enjoy consular protection while in mainland China. Chinese citizens who also have foreign citizenship may declare a change of nationality at the Immigration Department of the respective SARs, and upon approval, would no longer be considered Chinese citizens.

SAR permanent residents who are not Chinese citizens (including stateless persons) are not eligible for SAR passports. Persons who hold a non-Chinese citizenship must obtain passports from foreign diplomatic missions which represents their countries of citizenship. For those who are stateless, each SAR may issue its own form of certificates of identity, e.g. Document of Identity, in lieu of national passports to the persons concerned. Chinese citizens who are non-permanent residents of two SARs are also ineligible for SAR passports but may obtain CIs just like stateless persons.

The status of a special administrative region for Taiwan and other areas controlled by the Republic of China was first proposed in 1981. The 1981 proposal was put forth by Ye Jianying called "Ye's nine points" (葉九條). A series of different offers have since appeared. On 25 June 1983 Deng Xiaoping appeared at Seton Hall University in the US to propose "Deng's six points" (鄧六條), which called for a "Taiwan Special Administrative Region" (台灣特別行政區). It was envisioned that after Taiwan's unification with the PRC as an SAR, the PRC would become the sole representative of China. Under this proposal, Taiwan would be guaranteed its own military, its own administrative and legislative powers, an independent judiciary and the right of adjudication, although it would not be considered a separate government of China.

In 2005 the Anti-Secession Law of the PRC was enacted. It promises the lands currently ruled by the authorities of Taiwan a high degree of autonomy, among other things. The PRC can also employ non-peaceful means and other necessary measures to defend its claims to sovereignty over the ROC's territories in the event of an outright declaration of independence by Taiwan (ROC).

The Wolong Special Administrative Region () is located in the southwest of Wenchuan County, Ngawa Tibetan and Qiang Autonomous Prefecture of Sichuan. It was formerly known as Wolong Special Administrative Region of Wenchuan County, Sichuan Province and was founded in March 1983 with approval of the State Council. It was given its current name and placed under Sichuan provincial government with administrative supervision by the provincial department of forestry. Its area supersedes Sichuan Wolong National Nature Reserve and its administrative office is the same as the Administrative Bureau of the State Forestry Administration for the reserve. It currently has a population of 5343.

Despite its name, the Wolong Special Administrative Region is not an SAR as defined by Article 31 of the Constitution of the People's Republic of China; as a result, it has been proposed the Wenchuan Wolong Special Administrative Region of Sichuan Province change its name, with designations such as special area or township.

In the Republic of China (ROC) when it governed Mainland China, "special administrative regions" () were historically used to designate special areas, most of which were eventually converted into provinces. All were suspended or abolished after the end of the Chinese Civil War, with the establishment of the People's Republic of China (PRC) and the ROC government's retreat to Taiwan. The regions were:

Chahar was made a special administrative region in 1914 by the Republic of China, as a subdivision of the then Zhili Province, with 6 banners and 11 counties. In 1928 it became a province, with 5 of its counties partitioned to Suiyuan, and 10 counties were included from Hebei.


</doc>
<doc id="27660" url="https://en.wikipedia.org/wiki?curid=27660" title="Seattle Mariners">
Seattle Mariners

The Seattle Mariners are an American professional baseball team based in Seattle, Washington. The Mariners compete in Major League Baseball (MLB) as a member club of the American League (AL) West Division. The team joined the American League as an expansion team in 1977. Since July , the Mariners' home ballpark has been Safeco Field, located in the SoDo neighborhood of Seattle.

The "Mariners" name originates from the prominence of marine culture in the city of Seattle. They are nicknamed the M's, a title featured in their primary logo from 1987 to 1992. They adopted their current team colors – Navy blue, northwest green (teal), and silver – prior to the 1993 season, after having been royal blue and gold since the team's inception. Their mascot is the Mariner Moose.

The organization did not field a winning team until 1991, and any real success eluded them until when they won their first division championship and defeated the New York Yankees in the ALDS. The game-winning hit in Game 5, in which Edgar Martínez drove home Ken Griffey Jr. to win the game in the 11th inning, clinched a series win for the Mariners, served as a powerful impetus to preserve baseball in Seattle, and has since become an iconic moment in team history.

The Mariners won 116 games in , which set the American League record for most wins in a single season and tied the 1906 Chicago Cubs for the Major League record for most wins in a single season.

Through the 2017 season, the franchise has finished with a losing record in 28 of 41 seasons. The Mariners are one of seven Major League Baseball teams who have never won a World Series championship, and one of two (along with the Washington Nationals) never to have played in a World Series. With the National Football League's Buffalo Bills ending their 17-year playoff drought on December 31, 2017, the Mariners now hold the longest playoff drought in all of the four major North American professional sports, having not qualified for the playoffs since 2001.

The Mariners were created as a result of a lawsuit. In , in the aftermath of the Seattle Pilots' purchase and relocation to Milwaukee as the Milwaukee Brewers by Bud Selig, the city of Seattle, King County, and the state of Washington (represented by then-state attorney general Slade Gorton) sued the American League for breach of contract. Confident that Major League Baseball would return to Seattle within a few years, King County built the multi-purpose Kingdome, which would become home to the National Football League's expansion Seattle Seahawks in 1976. The name "Mariners" was chosen by club officials in August 1976 from over 600 names submitted by 15,000 entrants in a name-the-team contest.

The Mariners played their first game on April 6, , to a sold-out crowd of 57,762 at the Kingdome, losing 7–0 to the California Angels. The first home run in team history was hit on April 10, 1977, by designated hitter Juan Bernhardt.
That year, star pitcher Diego Seguí, in his last major league season, became the only player to play for both the Pilots and the Mariners. The Mariners finished with a 64–98 record, echoing the record the 1969 Pilots once held. In 1979, Seattle hosted the 50th Major League Baseball All-Star Game. After the 1981 season, the Mariners were sold to California businessman George Argyros, who in turn sold the team to Jeff Smulyan in 1989, and then to Nintendo of America in 1992.

During the 1992–93 offseason, the Mariners hired manager Lou Piniella, who had led the Cincinnati Reds to victory in the 1990 World Series. Mariner fans embraced Piniella, and he would helm the team from through , winning two American League Manager of the Year Awards along the way.

The Mariners club finished with a record of 116-46, leading all of Major League Baseball in winning percentage for the duration of the season and easily winning the American League West division championship. In doing so, the team broke the 1998 Yankees American League single-season record of 114 wins and matched the all-time MLB single-season record for wins set by the Chicago Cubs. At the end of the season, Ichiro Suzuki won the AL MVP, AL Rookie of the Year, and one of three outfield Gold Glove Awards, becoming the first player since the Boston Red Sox's Fred Lynn to win all three in the same season.

On October 22, the Mariners announced the hiring of Jack Zduriencik, formerly scouting director of the Milwaukee Brewers, as their general manager. Weeks later, on November 18, the team named Oakland Athletics bench coach Don Wakamatsu as its new field manager. Wakamatsu and Zduriencik hired an entirely new coaching staff for 2009, which included former World Series MVP John Wetteland as bullpen coach. The off-season also saw a litany of roster moves, headlined by a 12-player, 3-team trade that included sending All-Star closer J. J. Putz to the New York Mets and brought 5 players—including prospect Mike Carp and outfielder Endy Chávez from New York and outfielder Franklin Gutiérrez from the Cleveland Indians—to Seattle. Many of the moves, like the free agent signing of Mike Sweeney, were made in part with the hope of squelching the clubhouse infighting that plagued the Mariners in 2008. It also saw the return of Seattle favorite Griffey Jr. The 2009–10 offseason was highlighted by the trade for 2008 American League Cy Young Award winner Cliff Lee from the Philadelphia Phillies, the signing of third baseman Chone Figgins and the contract extension of star pitcher "King" Félix Hernández.

Griffey Jr. announced his retirement on June 2, 2010, after 22 MLB seasons.

The Mariners fired field manager Don Wakamatsu along with bench coach Ty Van Burkleo, pitching coach Rick Adair and performance coach Steve Hecht on August 9, 2010. Daren Brown, the manager of the AAA affiliate Tacoma Rainiers, took over as interim field manager. Roger Hansen, the former Minor League catching coordinator, was promoted to bench coach. Carl Willis, the former Minor League pitching coordinator, was promoted to pitching coach.

The Mariners hired former Cleveland Indians manager Eric Wedge as their new manager on October 19, 2010.

Dave Niehaus, the Mariners' play-by-play announcer since the team's inception, died of a heart attack on November 10, 2010, at the age of 75. In memory of Niehaus, Seattle rapper Macklemore wrote a tribute song called "My Oh My" in December 2010. He performed the song at the Mariners' Opening Day game on April 8, .

On April 21, 2012, Philip Humber of the Chicago White Sox threw the third perfect game in Chicago White Sox history against the Mariners at Safeco Field in Seattle. It was the 21st perfect game in MLB history. Mariners starting pitcher Kevin Millwood and five other pitchers combined to throw the tenth combined no-hitter in MLB history and the first in team history on June 8, 2012. The last combined one occurred in 2003, when six Houston Astros no-hit the New York Yankees in New York. The six pitchers used in a no-hitter is a major league record. Félix Hernández pitched the first perfect game in team history, shutting down the Tampa Bay Rays 1-0 at Safeco Field on August 15, 2012. It was the 23rd perfect game in Major League Baseball history. The Mariners became the first team in Major League Baseball to be involved in a perfect game two times in one season.

General Manager (GM) Jack Zduriencik was relieved of his position by the team on August 28, 2015. Jerry Dipoto, who formerly served as GM of the Los Angeles Angels of Anaheim, was hired as the new GM of the Mariners one month later. On October 9, 2015, manager Lloyd McClendon was fired, and the search for a new manager was begun. Scott Servais was named the new Mariners' manager on October 23, 2015.

Nintendo of America issued a press release on April 27, 2016, stating it would sell most shares it held of Seattle Mariners ownership to First Avenue Entertainment limited partnership. Nintendo retained a 10% ownership share of the team after the sale was completed in August 2016.

The Mariners donned their current uniforms in (with a slight change to the color arrangement made in 2015). White jerseys and pants are worn for most home games, while gray jerseys and pants are worn on the road. In 2011, the team brought back an alternate "Northwest Green" jersey that was previously part of the uniform set from 1994 to 1996 to be worn during Friday home games. A navy blue alternate jersey is worn for occasional road games; other variations of a navy jersey had been used as home alternates prior to the reintroduction of the Northwest Green jersey.

A navy blue cap that features a ball and compass "S" logo is paired with the home white, road gray, and navy blue jerseys. A variation of this cap with a Northwest Green brim is worn with the home alternate jersey.

In January 2015 the team announced a new alternate uniform to be worn for Sunday home games. This cream-colored "fauxback" uniform features the current logo and lettering style in a royal blue and gold color scheme, a throwback to the original team colors. Unlike the rest of the uniform set, the back of the jersey does not display the player name. The cap features the current cap logo in the throwback colors.

The Peoria Sports Complex in Peoria, Arizona, has been the Mariners' home spring training facility since 1994. The complex is shared with the San Diego Padres. On March 25, 2013, in a 16-0 victory over the Cincinnati Reds, the Mariners broke the team record for total home runs during a spring training season with 52.

"This is a partial list listing the past 20 completed regular seasons. For the full season records, see here."

Has been home to the Seattle Mariners since the first game vs. the San Diego Padres on July 15, 1999. There were 44,607 people in attendance that night.

Seattle Mariners former chairman and CEO John Ellis announced on June 14, 1997 the creation of a Mariners Hall of Fame. It is operated by the Seattle Mariners organization. It honors the players, staff and other individuals that greatly contributed to the history and success of the Mariners franchise. It is located at the Baseball Museum of the Pacific Northwest in Safeco Field.

The Mariners plan to retire uniform numbers only very selectively and subject to substantially higher expectations than those applied to the Mariners' Hall of Fame. To be eligible to have one's number retired, in addition to the criteria outlined for the Mariners' Hall of Fame, the former Mariners should have either:<br>      a) been elected to the National Baseball Hall of Fame and been in a Mariner's uniform for at least five years, or <br>      b) come close to such election and have spent substantially his entire career with the Mariners. <br>Eligibility shall not commence until after the former player has been voted on once for the National Baseball Hall of Fame, which for all practical purposes means six years after retirement.

Ken Griffey Jr.'s number 24 was retired at the beginning of the 2016 season, with the retirement ceremony taking place on August 6, 2016. Griffey had been elected to the Hall of Fame in January of that year.

Edgar Martínez's number 11 was retired during the 2017 season, with the retirement ceremony taking place on August 12, 2017. Martínez played his entire major-league career in Seattle and first appeared on the Hall of Fame ballot in . His best Hall of Fame voting count was in 2018, when he received 70.4% of the vote (75% is required for induction). Jersey number 11 was not issued to anyone else between Martínez's retirement as a player in 2004 until his return to the Mariners as hitting coach in 2015.

Currently, only one other player has definitively met the requirements to have his number retired: Randy Johnson, who played 10 seasons with the Mariners (1989–1998) and was elected to the Hall of Fame in 2015.

Despite not officially retiring number 19, the team has not reissued it since Jay Buhner left the team in 2001. 

Number 51, worn by Randy Johnson, was withheld from players from 1998 until 2001, when it was issued to Ichiro Suzuki upon his request after wearing it for his entire career in Japan. It was presumably taken out of circulation again, following Ichiro's 2012 trade to the Yankees coupled with Johnson's 2015 election into the Baseball Hall of Fame. The number was once again worn by Ichiro after he returned to the team in 2018. 

Number 14 (Lou Piniella) was not given to any uniformed personnel between Piniella's 2002 departure and 2015, but it was issued to third-base coach Manny Acta for the 2016 season.

Jackie Robinson's number 42 was retired throughout Major League Baseball on April 15, 1997.

Uniform number 00 is presumed off-limits, as it has been worn by the Mariner Moose since 1997 (outfielder Jeffrey Leonard was the last player to wear 00 for the M's, in 1990). From 1990 to 1996, the Moose wore the last 2 digits of the year of the current season.

As part of the seventh inning stretch, after the crowd is led in singing "Take Me Out To The Ball Game" or "God Bless America" the public address system begins playing the Kingsmen's version of "Louie Louie". This commemorates a 1985 prank attempt to make "Louie Louie" the state song of Washington.

As part of the mid-inning entertainment during games, three animated hydroplanes race on the Safeco Field Jumbotron screen. Each boat is "sponsored" by a local business. Fans are encouraged to cheer the boats on. The hydroplane races are inspired by actual hydroplane races held annually during Seafair, Seattle's primary summer event.

Similarly, in a separate event, a baseball is hidden underneath one of three animated Mariners baseball caps and the fans are encouraged to shout out which cap they think the ball is under at the end of the caps' "dance." Both events are strictly for fun and no prizes are awarded.

In 1994, the Mariners started a promotion called "Buhner Buzz Cut Night" Inspired by Jay Buhner's shaved head; any fan who was willing to have their head shaved before the game—or was already bald—would receive a free ticket to the game and a T-shirt with a slogan such as "Bald is Buhnerful" or "Take Me Out To The Bald Game". Hair 10 inches or longer was collected for charity. The promotion continued until Buhner's retirement in 2001, with a year's hiatus in 2000, and is still remembered by fans today.

Rally Fries are a baseball tradition started by Mariners broadcaster Mike Blowers in 2007. During a game against the Cincinnati Reds, a fan tried to catch a foul ball along the right-field line but in turn spilled his tray of french fries along the track. While chatting on the air and seeing the mishap, Blowers' partner, Dave Sims, suggested that he should send a new tray of fries to the fan. Blowers agreed, and sent his intern to deliver a plate of fries to the man.

At the Mariners' next game, fans made signs and boards asking Blowers for fries as well. Coincidentally, every time the fries were delivered, the Mariners seem to score or rally from a deficit, and thus the "Rally Fries" were created. This became so popular with the fans that signs were even seen when the Mariners were the visiting team, although on August 1, 2009, Blowers established that he only gives out fries at home games.

Generally, Blowers will select a person or a group of people that appeals to him, whether it is through fans wearing elaborate costumes or waving funny signs and boards. The fries are usually delivered from Ivar's, a Seattle-based seafood restaurant with a location at Safeco Field. The amount of fries given out varies with the size of the winning group of fans. The winners are generally selected around the 5th or 6th inning, although potential candidates are shown in almost every inning beforehand.

As the 2011 season progressed, the Mariners marketing staff came up with an idea to encourage the growing fanbase of star pitcher "King" Félix Hernández. Every Hernandez start at Safeco Field is now accompanied by the King's Court, a designated cheering section for fans to sing, chant, and cheer while donning yellow T-shirts and K cards that are supplied by the team.

The King's Court is both a personal rooting section for Hernandez and trend-setter for Safeco Field. The team encouraged fans to dress like Larry Bernandez, Hernandez's alter ego from a Mariners TV Commercial, or show up in wacky costumes, rewarding the best with a ceremonial turkey leg.

The Supreme Court is a special occurrence when the King's Court is extended to the entirety of Safeco Field. The first Supreme Court was Félix's first home game following his perfect game in 2012. Since then it has occurred each year at Félix's first home game of each season.

The ultimately disappointing 2017 season had a few bright spots, including the establishment of the Maple Grove, an offshoot of the King’s Court which honors James Paxton rather than Félix Hernández. On days where James Paxton starts, a group of fans hold up “Eh” Cards, a tip of the cap to Paxton’s home country of Canada. A potted maple tree is also present for the occasion. 
The Maple Grove differs from the King’s Court in that it was created by fans, while the Court was thought of by the Mariners marketing team. When asked, Paxton stated that fans creating the Maple Grove was really special to him and that he never imagined that something of the sort would ever be done for him.

The following elected members of the Baseball Hall of Fame spent part of their careers with the Mariners.

The Mariners' flagship radio station is KIRO-AM (710 ESPN Radio), which previously broadcast Mariners contests from 1985 to 2002. Former flagship stations include KOMO-AM (2003–2008), and KVI-AM 570 (1977–1984). Television rights are held by Root Sports Northwest. During the 2016 season, the Mariners averaged a 5.84 rating and 103,000 viewers on primetime TV broadcasts. In years past, Mariners games have also appeared in Seattle on over-the-air stations KING-TV, KIRO-TV, KTZZ-TV (now KZJO), and KSTW. Selected Mariners games are also available on Canadian television, due to an agreement between Root Sports Northwest and Rogers Sportsnet Pacific.

Since 2013, Rick Rizzs and Aaron Goldsmith have called games on the radio. The television broadcasts are anchored by play-by-play announcer Dave Sims and color commentator (and former Mariners player) Mike Blowers. Seattle radio personality Matt Pitman hosts the post-game show on the Mariners' radio network, along with clubhouse reporter Shannon Drayer. Spanish-language radio broadcast duties are handled by Alex Rivera on play-by-play and former second baseman Julio Cruz providing color commentary.

The Mariners' broadcast team for 2010 featured Dave Niehaus and Rizzs—back for their 32nd and 23rd seasons with the club, respectively—as well as Sims and Blowers. For the first three innings of each game, Niehaus worked the television broadcast with Blowers while Rizzs and Sims handled radio duties; after the third inning, Niehaus and Sims traded places. Niehaus, who had broadcast for the Mariners since their inaugural season of 1977, died on November 10, 2010. For the 2011 season, Dave Niehaus' duties in the broadcast booth were filled by a collection of former Mariners broadcasters such as Ron Fairly, Ken Levine, and Ken Wilson; and former Mariners' players such as Dave Valle, Dan Wilson, Jay Buhner, and Dave Henderson.

Tom Hutyler has been the Mariners' public address announcer since 1987, first at the Kingdome, and presently at Safeco Field. While KOMO 1000 AM was the Mariners' flagship radio station, Hutyler occasionally hosted the post-game radio show.




</doc>
<doc id="27661" url="https://en.wikipedia.org/wiki?curid=27661" title="Source code">
Source code

In computing, source code is any collection of code, possibly with comments, written using a "human-readable" programming language, usually as plain text. The source code of a program is specially designed to facilitate the work of computer programmers, who specify the actions to be performed by a computer mostly by writing source code. The source code is often transformed by an assembler or compiler into binary machine code understood by the computer. The machine code might then be stored for execution at a later time. Alternatively, source code may be interpreted and thus immediately executed.

Most application software is distributed in a form that includes only executable files. If the source code were included it would be useful to a user, programmer or a system administrator, any of whom might wish to study or modify the program.

The Linux Information Project defines source code as:
Source code (also referred to as source or code) is the version of software as it is originally written (i.e., typed into a computer) by a human in plain text (i.e., human readable alphanumeric characters).
The notion of source code may also be taken more broadly, to include machine code and notations in graphical languages, neither of which are textual in nature. An example from an article presented on the annual IEEE conference and on Source Code Analysis and Manipulation:

For the purpose of clarity "source code" is taken to mean any fully executable description of a software system. It is therefore so construed as to include machine code, very high level languages and executable graphical representations of systems.

Often there are several steps of program translation or minification between the original source code typed by a human and an executable program. While some, like the FSF, argue that an intermediate file "is not real source code and does not count as source code", others find it convenient to refer to each intermediate file as the source code for the next steps.

The earliest programs for stored-program computers were entered in binary through the front panel switches of the computer. This first-generation programming language had no distinction between source code and machine code.

When IBM first offered software to work with its machine, the source code was provided at no additional charge. At that time, the cost of developing and supporting software was included in the price of the hardware. For decades, IBM distributed source code with its software product licenses, after 1999.

Most early computer magazines published source code as type-in programs.

Occasionally the entire source code to a large program is published as a hardback book, such as "Computers and Typesetting", vol. B: "TeX, The Program" by Donald Knuth, "PGP Source Code and Internals" by Philip Zimmermann, "PC SpeedScript" by Randy Thompson, and "µC/OS, The Real-Time Kernel" by Jean Labrosse.

The source code which constitutes a program is usually held in one or more text files stored on a computer's hard disk; usually these files are carefully arranged into a directory tree, known as a source tree. Source code can also be stored in a database (as is common for stored procedures) or elsewhere.
The source code for a particular piece of software may be contained in a single file or many files. Though the practice is uncommon, a program's source code can be written in different programming languages. For example, a program written primarily in the C programming language, might have portions written in assembly language for optimization purposes. It is also possible for some components of a piece of software to be written and compiled separately, in an arbitrary programming language, and later integrated into the software using a technique called library linking. In some languages, such as Java, this can be done at run time (each class is compiled into a separate file that is linked by the interpreter at runtime).

Yet another method is to make the main program an interpreter for a programming language, either designed specifically for the application in question or general-purpose, and then write the bulk of the actual user functionality as macros or other forms of add-ins in this language, an approach taken for example by the GNU Emacs text editor.

The code base of a computer programming project is the larger collection of all the source code of all the computer programs which make up the project. It has become common practice to maintain code bases in version control systems. Moderately complex software customarily requires the compilation or assembly of several, sometimes dozens or maybe even hundreds, of different source code files. In these cases, instructions for compilations, such as a Makefile, are included with the source code. These describe the programming relationships among the source code files, and contain information about how they are to be compiled.

The revision control system is another tool frequently used by developers for source code maintenance.

Source code is primarily used as input to the process that produces an executable program (i.e., it is compiled or interpreted). It is also used as a method of communicating algorithms between people (e.g., code snippets in books).

Computer programmers often find it helpful to review existing source code to learn about programming techniques. The sharing of source code between developers is frequently cited as a contributing factor to the maturation of their programming skills. Some people consider source code an expressive artistic medium.

Porting software to other computer platforms is usually prohibitively difficult without source code. Without the source code for a particular piece of software, portability is generally computationally expensive. Possible porting options include binary translation and emulation of the original platform.

Decompilation of an executable program can be used to generate source code, either in assembly code or in a high-level language.

Programmers frequently adapt source code from one piece of software to use in other projects, a concept known as software reusability.

The situation varies worldwide, but in the United States before 1974, software and its source code was not copyrightable and therefore always public domain software.

In 1974, the US Commission on New Technological Uses of Copyrighted Works (CONTU) decided that "computer programs, to the extent that they embody an author's original creation, are proper subject matter of copyright".

In 1983 in the United States court case "Apple v. Franklin" it was ruled that the same applied to object code; and that the Copyright Act gave computer programs the copyright status of literary works.

In 1999, in the United States court case "Bernstein v. United States" it was further ruled that source code could be considered a constitutionally protected form of free speech. Proponents of free speech argued that because source code conveys information to programmers, is written in a language, and can be used to share humor and other artistic pursuits, it is a protected form of communication.

An author of a non-trivial work like software, has several exclusive rights, among them the copyright for the source code and object code. The author has the right and possibility to grant customers and users of his software some of his exclusive rights in form of software licensing. Software, and its accompanying source code, can be associated with several licensing paradigms; the most important distinction is open source vs proprietary software. This is done by including a copyright notice that declares licensing terms. If no notice is found, then the default of "All rights reserved" is implied.

Generally speaking, software is "open source" if the source code is free to use, distribute, modify and study, and "proprietary" if the source code is kept secret, or is privately owned and restricted. One of the first software licenses to be published and to explicitly grant these freedoms was the GNU General Public License in 1989; the BSD license is another early example from 1990.

For proprietary software, the provisions of the various copyright laws, trade secrecy and patents are used to keep the source code closed. Additionally, many pieces of retail software come with an end-user license agreement (EULA) which typically prohibits decompilation, reverse engineering, analysis, modification, or circumventing of copy protection. Types of source code protection—beyond traditional compilation to object code—include code encryption, code obfuscation or code morphing.

The way a program is written can have important consequences for its maintainers. Coding conventions, which stress readability and some language-specific conventions, are aimed at the maintenance of the software source code, which involves debugging and updating. Other priorities, such as the speed of the program's execution, or the ability to compile the program for multiple architectures, often make code readability a less important consideration, since code "quality" generally depends on its "purpose".





</doc>
<doc id="27667" url="https://en.wikipedia.org/wiki?curid=27667" title="Space">
Space

Space is the boundless three-dimensional extent in which objects and events have relative position and direction. Physical space is often conceived in three linear dimensions, although modern physicists usually consider it, with time, to be part of a boundless four-dimensional continuum known as spacetime. The concept of space is considered to be of fundamental importance to an understanding of the physical universe. However, disagreement continues between philosophers over whether it is itself an entity, a relationship between entities, or part of a conceptual framework.

Debates concerning the nature, essence and the mode of existence of space date back to antiquity; namely, to treatises like the "Timaeus" of Plato, or Socrates in his reflections on what the Greeks called "khôra" (i.e. "space"), or in the "Physics" of Aristotle (Book IV, Delta) in the definition of "topos" (i.e. place), or in the later "geometrical conception of place" as "space "qua" extension" in the "Discourse on Place" ("Qawl fi al-Makan") of the 11th-century Arab polymath Alhazen. Many of these classical philosophical questions were discussed in the Renaissance and then reformulated in the 17th century, particularly during the early development of classical mechanics. In Isaac Newton's view, space was absolute—in the sense that it existed permanently and independently of whether there was any matter in the space. Other natural philosophers, notably Gottfried Leibniz, thought instead that space was in fact a collection of relations between objects, given by their distance and direction from one another. In the 18th century, the philosopher and theologian George Berkeley attempted to refute the "visibility of spatial depth" in his "Essay Towards a New Theory of Vision". Later, the metaphysician Immanuel Kant said that the concepts of space and time are not empirical ones derived from experiences of the outside world—they are elements of an already given systematic framework that humans possess and use to structure all experiences. Kant referred to the experience of "space" in his "Critique of Pure Reason" as being a subjective "pure "a priori" form of intuition".

In the 19th and 20th centuries mathematicians began to examine geometries that are non-Euclidean, in which space is conceived as "curved", rather than "flat". According to Albert Einstein's theory of general relativity, space around gravitational fields deviates from Euclidean space. Experimental tests of general relativity have confirmed that non-Euclidean geometries provide a better model for the shape of space.

In the seventeenth century, the philosophy of space and time emerged as a central issue in epistemology and metaphysics. At its heart, Gottfried Leibniz, the German philosopher-mathematician, and Isaac Newton, the English physicist-mathematician, set out two opposing theories of what space is. Rather than being an entity that independently exists over and above other matter, Leibniz held that space is no more than the collection of spatial relations between objects in the world: "space is that which results from places taken together". Unoccupied regions are those that "could" have objects in them, and thus spatial relations with other places. For Leibniz, then, space was an idealised abstraction from the relations between individual entities or their possible locations and therefore could not be continuous but must be discrete.
Space could be thought of in a similar way to the relations between family members. Although people in the family are related to one another, the relations do not exist independently of the people.
Leibniz argued that space could not exist independently of objects in the world because that implies a difference between two universes exactly alike except for the location of the material world in each universe. But since there would be no observational way of telling these universes apart then, according to the identity of indiscernibles, there would be no real difference between them. According to the principle of sufficient reason, any theory of space that implied that there could be these two possible universes must therefore be wrong.
Newton took space to be more than relations between material objects and based his position on observation and experimentation. For a relationist there can be no real difference between inertial motion, in which the object travels with constant velocity, and non-inertial motion, in which the velocity changes with time, since all spatial measurements are relative to other objects and their motions. But Newton argued that since non-inertial motion generates forces, it must be absolute. He used the example of water in a spinning bucket to demonstrate his argument. Water in a bucket is hung from a rope and set to spin, starts with a flat surface. After a while, as the bucket continues to spin, the surface of the water becomes concave. If the bucket's spinning is stopped then the surface of the water remains concave as it continues to spin. The concave surface is therefore apparently not the result of relative motion between the bucket and the water. Instead, Newton argued, it must be a result of non-inertial motion relative to space itself. For several centuries the bucket argument was considered decisive in showing that space must exist independently of matter.

In the eighteenth century the German philosopher Immanuel Kant developed a theory of knowledge in which knowledge about space can be both "a priori" and "synthetic". According to Kant, knowledge about space is "synthetic", in that statements about space are not simply true by virtue of the meaning of the words in the statement. In his work, Kant rejected the view that space must be either a substance or relation. Instead he came to the conclusion that space and time are not discovered by humans to be objective features of the world, but imposed by us as part of a framework for organizing experience.

Euclid's "Elements" contained five postulates that form the basis for Euclidean geometry. One of these, the parallel postulate, has been the subject of debate among mathematicians for many centuries. It states that on any plane on which there is a straight line "L" and a point "P" not on "L", there is exactly one straight line "L" on the plane that passes through the point "P" and is parallel to the straight line "L". Until the 19th century, few doubted the truth of the postulate; instead debate centered over whether it was necessary as an axiom, or whether it was a theory that could be derived from the other axioms. Around 1830 though, the Hungarian János Bolyai and the Russian Nikolai Ivanovich Lobachevsky separately published treatises on a type of geometry that does not include the parallel postulate, called hyperbolic geometry. In this geometry, an infinite number of parallel lines pass through the point "P". Consequently, the sum of angles in a triangle is less than 180° and the ratio of a circle's circumference to its diameter is greater than pi. In the 1850s, Bernhard Riemann developed an equivalent theory of elliptical geometry, in which no parallel lines pass through "P". In this geometry, triangles have more than 180° and circles have a ratio of circumference-to-diameter that is less than pi.

Although there was a prevailing Kantian consensus at the time, once non-Euclidean geometries had been formalised, some began to wonder whether or not physical space is curved. Carl Friedrich Gauss, a German mathematician, was the first to consider an empirical investigation of the geometrical structure of space. He thought of making a test of the sum of the angles of an enormous stellar triangle, and there are reports that he actually carried out a test, on a small scale, by triangulating mountain tops in Germany.

Henri Poincaré, a French mathematician and physicist of the late 19th century, introduced an important insight in which he attempted to demonstrate the futility of any attempt to discover which geometry applies to space by experiment. He considered the predicament that would face scientists if they were confined to the surface of an imaginary large sphere with particular properties, known as a sphere-world. In this world, the temperature is taken to vary in such a way that all objects expand and contract in similar proportions in different places on the sphere. With a suitable falloff in temperature, if the scientists try to use measuring rods to determine the sum of the angles in a triangle, they can be deceived into thinking that they inhabit a plane, rather than a spherical surface. In fact, the scientists cannot in principle determine whether they inhabit a plane or sphere and, Poincaré argued, the same is true for the debate over whether real space is Euclidean or not. For him, which geometry was used to describe space was a matter of convention. Since Euclidean geometry is simpler than non-Euclidean geometry, he assumed the former would always be used to describe the 'true' geometry of the world.

In 1905, Albert Einstein published his special theory of relativity, which led to the concept that space and time can be viewed as a single construct known as "spacetime". In this theory, the speed of light in a vacuum is the same for all observers—which has the result that two events that appear simultaneous to one particular observer will not be simultaneous to another observer if the observers are moving with respect to one another. Moreover, an observer will measure a moving clock to tick more slowly than one that is stationary with respect to them; and objects are measured to be shortened in the direction that they are moving with respect to the observer.

Subsequently, Einstein worked on a general theory of relativity, which is a theory of how gravity interacts with spacetime. Instead of viewing gravity as a force field acting in spacetime, Einstein suggested that it modifies the geometric structure of spacetime itself. According to the general theory, time goes more slowly at places with lower gravitational potentials and rays of light bend in the presence of a gravitational field. Scientists have studied the behaviour of binary pulsars, confirming the predictions of Einstein's theories, and non-Euclidean geometry is usually used to describe spacetime.

In modern mathematics spaces are defined as sets with some added structure. They are frequently described as different types of manifolds, which are spaces that locally approximate to Euclidean space, and where the properties are defined largely on local connectedness of points that lie on the manifold. There are however, many diverse mathematical objects that are called spaces. For example, vector spaces such as function spaces may have infinite numbers of independent dimensions and a notion of distance very different from Euclidean space, and topological spaces replace the concept of distance with a more abstract idea of nearness.

Space is one of the few fundamental quantities in physics, meaning that it cannot be defined via other quantities because nothing more fundamental is known at the present. On the other hand, it can be related to other fundamental quantities. Thus, similar to other fundamental quantities (like time and mass), space can be explored via measurement and experiment.

Today, our three-dimensional space is viewed as embedded in a four-dimensional spacetime, called Minkowski space (see special relativity). The idea behind space-time is that time is hyperbolic-orthogonal to each of the three spatial dimensions.

Before Einstein's work on relativistic physics, time and space were viewed as independent dimensions. Einstein's discoveries showed that due to relativity of motion our space and time can be mathematically combined into one object–spacetime. It turns out that distances in space or in time separately are not invariant with respect to Lorentz coordinate transformations, but distances in Minkowski space-time along space-time intervals are—which justifies the name.

In addition, time and space dimensions should not be viewed as exactly equivalent in Minkowski space-time. One can freely move in space but not in time. Thus, time and space coordinates are treated differently both in special relativity (where time is sometimes considered an imaginary coordinate) and in general relativity (where different signs are assigned to time and space components of spacetime metric).

Furthermore, in Einstein's general theory of relativity, it is postulated that space-time is geometrically distorted- "curved" -near to gravitationally significant masses.

One consequence of this postulate, which follows from the equations of general relativity, is the prediction of moving ripples of space-time, called gravitational waves. While indirect evidence for these waves has been found (in the motions of the Hulse–Taylor binary system, for example) experiments attempting to directly measure these waves are ongoing at the LIGO and Virgo collaborations. LIGO scientists reported the first such direct observation of gravitational waves on 14 September 2015.

Relativity theory leads to the cosmological question of what shape the universe is, and where space came from. It appears that space was created in the Big Bang, 13.8 billion years ago and has been expanding ever since. The overall shape of space is not known, but space is known to be expanding very rapidly due to the cosmic inflation.

The measurement of "physical space" has long been important. Although earlier societies had developed measuring systems, the International System of Units, (SI), is now the most common system of units used in the measuring of space, and is almost universally used.

Currently, the standard space interval, called a standard meter or simply meter, is defined as the distance traveled by light in a vacuum during a time interval of exactly 1/299,792,458 of a second. This definition coupled with present definition of the second is based on the special theory of relativity in which the speed of light plays the role of a fundamental constant of nature.

Geography is the branch of science concerned with identifying and describing places on Earth, utilizing spatial awareness to try to understand why things exist in specific locations. Cartography is the mapping of spaces to allow better navigation, for visualization purposes and to act as a locational device. Geostatistics apply statistical concepts to collected spatial data of Earth to create an estimate for unobserved phenomena.

Geographical space is often considered as land, and can have a relation to ownership usage (in which space is seen as property or territory). While some cultures assert the rights of the individual in terms of ownership, other cultures will identify with a communal approach to land ownership, while still other cultures such as Australian Aboriginals, rather than asserting ownership rights to land, invert the relationship and consider that they are in fact owned by the land. Spatial planning is a method of regulating the use of space at land-level, with decisions made at regional, national and international levels. Space can also impact on human and cultural behavior, being an important factor in architecture, where it will impact on the design of buildings and structures, and on farming.

Ownership of space is not restricted to land. Ownership of airspace and of waters is decided internationally. Other forms of ownership have been recently asserted to other spaces—for example to the radio bands of the electromagnetic spectrum or to cyberspace.

Public space is a term used to define areas of land as collectively owned by the community, and managed in their name by delegated bodies; such spaces are open to all, while private property is the land culturally owned by an individual or company, for their own use and pleasure.

Abstract space is a term used in geography to refer to a hypothetical space characterized by complete homogeneity. When modeling activity or behavior, it is a conceptual tool used to limit extraneous variables such as terrain.

Psychologists first began to study the way space is perceived in the middle of the 19th century. Those now concerned with such studies regard it as a distinct branch of psychology. Psychologists analyzing the perception of space are concerned with how recognition of an object's physical appearance or its interactions are perceived, see, for example, visual space.

Other, more specialized topics studied include amodal perception and object permanence. The perception of surroundings is important due to its necessary relevance to survival, especially with regards to hunting and self preservation as well as simply one's idea of personal space.

Several space-related phobias have been identified, including agoraphobia (the fear of open spaces), astrophobia (the fear of celestial space) and claustrophobia (the fear of enclosed spaces).

The understanding of three-dimensional space in humans is thought to be learned during infancy using unconscious inference, and is closely related to hand-eye coordination. The visual ability to perceive the world in three dimensions is called depth perception.



</doc>
<doc id="27669" url="https://en.wikipedia.org/wiki?curid=27669" title="Spanish cuisine">
Spanish cuisine

Spanish cuisine is heavily influenced by regional cuisines and the particular historical processes that shaped culture and society in those territories. Geography and climate had great influence on cooking methods and available ingredients, and these particularities are still present in the gastronomy of the various regions that make up the country. Spanish cuisine derives from a complex history, where invasions of the country and conquests of new territories modified traditions and made new ingredients available.

Before the Roman Empire, Spain used to be divided in three territories for three different "clans": the Celts (north of Spain), the Iberians (center east), and the Tartessos (South). The Celts were a warrior based community, and lived in small fortified round houses, they fished and farmed. Even today we can see their influence as the north of Spain is renowned for their "mariscos" (sea food).The Iberians were mainly hunters and cattle keepers. The center of Spain is still considered to have great quality of meat. e.g. Cochinillo in Segovia (piglet) The Tartessos were goldsmiths, and did a lot of trading with Africa and Greece.

Authors such as Strabo, however, write about aboriginal people using nuts and acorns as staple food.

The Romans introduced the custom of collecting and eating mushrooms, which is still preserved in many parts of Spain, especially in the north. The Romans along with the Greeks introduced viticulture; it also appears that the extension of the vine along the Mediterranean seems to be due to the colonization of the Greeks. Together with the Greeks, the Phoenicians introduced the cultivation of olive oil. Spain is the largest producer of olive oil in the world.

The Visigoths introduced brewing. The change came in 711 AD, when Muslim troops composed of Arabs and Berbers crossed the Strait of Gibraltar, invading the Iberian Peninsula. The Muslim conquest brought new ingredients to Spanish cuisine from different parts of the world, such as Persia and India.

The cuisine of Al-Andalus included such ingredients as: rice, sorghum, sugar cane, spinach, eggplant, watermelon, lemon, peach, orange and almonds. However the Muslim religion does not allow alcoholic drinks such as wine, therefore many rulers of Al Ándalus used to uproot vineyards as a signal of piety.

The arrival of Europeans in America, in 1492, initiated the advent of new culinary elements, such as tomatoes, potatoes, corn, bell peppers, spicy peppers, paprika, vanilla and cocoa or chocolate. The latter caused a furor in the Spanish society in the sixteenth and seventeenth centuries; Spain was where it was first mixed with sugar to remove its natural bitterness. Other ingredients traveled to the Americas, such as rice, grapes, olives and many types of cereals.

Many traditional Spanish dishes such as tortilla de patata (an omelette made with potatoes), would not be possible without the discovery of America, and the potato. Gazpacho, salmorejo, and pan tumaca would not be possible without the tomato.

A continental-style breakfast ("desayuno") may be taken just after waking up, or before entering the workplace. Due to the large time span between breakfast and lunch, it is not uncommon to halt the working schedule to take a mid-morning snack.

Lunch ("el almuerzo" or simply "la comida", literally meaning "the food"), the large midday meal in Spain, contains several courses. It usually starts between 2:00 pm or 2:30 pm finishing around 3:00 pm to 3:30 pm, and is usually followed by Sobremesa, which refers to the tabletalk that Spanish people undertake. Menus are organized according to these courses and include five or six choices in each course. At home, Spanish meals wouldn't be too fancy, and would contain soup or a pasta dish, salad, a meat or a fish dish and a dessert such as fruit or cheese. Green salad with the meat or fish courses. In some regions of Spain, the word "almuerzo" refers to the mid-morning snack, instead of lunch.

"La cena", meaning both dinner or supper, is taken between 8:30pm and 10pm. It is lighter than lunch, consisting of one course and dessert. Due to the large time span between lunch and dinner, an afternoon snack, "la merienda", equivalent to afternoon tea, may take place at about 6pm.

Appetizers before lunch or dinner are common in the form of tapas (tiny rations).

In the last years, the Spanish government is starting to take action to shorten the lunch break, in order to end the working day earlier. Most businesses shut down for two or three hours for lunch, then resume the working day until dinner time in the evening.

The following is a list of traditional Spanish meals:

Andalusian cuisine is twofold: rural and coastal. Of all the Spanish regions, this region uses the most olive oil in its cuisine. The Andalusian dish that has achieved the most international fame is Gazpacho. It is a cold soup (or in an alternative view, a liquid salad) made with five vegetables, bread, vinegar, water, salt and olive oil. Other cold soups include: pulley, Zoque, salmorejo, etc.

Snacks made with olives are common. Meat dishes include: flamenquín, pringá, oxtail stew and Menudo Gitano (also called Andalusian tripe). The hot soups include cat soup (made with bread), dog stew (fish soup with orange juice) and Migas Canas. Fish dishes include: fried fish, cod pavías, and parpandúas. A culinary custom is the typical Andalusian breakfast, considered to be a traditional characteristic of laborers and today extending throughout Spain.

Cured meats include: Serrano Ham and Iberico Ham. Typical drinks in the area include: anise, wine (Malaga, Jerez, Pedro Ximénez, etc..) and sherry brandy.

The Aragonese cuisine has a rural and mountainous origin. The central part of Aragon, the flattest, is the richest in culinary specialties. Being a land of lambs raised on the slopes of the Pyrenees, one of its most famous dishes is roast lamb (asado de ternasco) (with garlic, salt and bacon fat), having the lamb to the shepherd, the heads of lamb and Highlanders asparagus (lamb tails). Pork dishes are also very popular, among them: Magras con tomate, roasted pork leg and Almojábanas de Cerdo. Among the recipes made with bread are: migas de Pastor, migas con chocolate, Regañaos (cakes with sardines or herring) and goguera. The most notable condiment is garlic-oil.

Legumes are very important and the most popular vegetables are borage and thistle. In terms of cured meats, ham from Teruel and Huesca are famous. Among the cheeses Tronchon is notable. Fruit-based cuisine includes the very popular Fruits of Aragon (Spanish: Frutas de Aragón) and Maraschino cherries.

Asturian cuisine has a long and rich history, deeply rooted in Celtic traditions of northern Europe. One of its most famous dishes is the Asturian bean stew, known as "fabada", which is the traditional stew of the region, made with white beans, sausages such as chorizo and morcilla and pork. Another well-known recipe is beans with clams. Asturian beans ("fabes") can also be cooked with hare, partridge, prawns or octopus. Also of note are Asturian stew (made with white beans, cabbage, potatoes and a variety of sausages and bacon) and vigil stew. Pork-based foods, for example chosco, Asturian-style tripe and bollos preñaos (chorizo-stuffed bread bums) are popular.

Common meat dishes include: carne gobernada (roasted veal meat), cachopo (a crunchy, crumb-coated veal steak stuffed with ham and cheese) and stew.

Fish and seafood play also an important role in Asturian cuisine. The Cantabrian Sea provides a rich variety of species, including tuna, hake and sardine. 

Asturian cheeses are very popular in the rest of Spain. Among them, the most representative is Cabrales cheese, a pungent, blue cheese developed in the regions near the Picos de Europa. Other popular cheese types are Gamonéu, Afuega'l pitu and ahumado de Pría. These can be enjoyed with the local cider, a low-alcohol drink made of Asturian apples, with a distinctive sourness.

Asturian cider, made of a special type of apple, is tradionally poured ("escanciada") from a certain height, usually over the head of the waiter/server: one hand holds the glass, slightly tilted, under the hip, while the other hand throws the cider from atop, the arm usually stretched upwards. When the cider falls into the glass from above, the drink "breaks", getting aerated and bubbly. It is consumed immediately after being served, in consecutive, tiny shots.

Notable desserts are frisuelos (similar to crêpes,usually filled with cream or apple jam), rice pudding (white rice cooked with milk, lemon zest and sugar), and carbayones (puff pastry cakes filled with almond mash and covered with sugar glaze).

The Balearic cuisine has purely Mediterranean characteristics due to its location. The islands have been conquered several times throughout their history by the French and the English, which has left some culinary influences. At present are well known: the sobrassada and arròs brut, mahón cheese, Gin de Menorca ("pellofa") and mayonnaise. Among the dishes are tumbet, frit mallorquí and roasted suckling pig.

Among the desserts we can find: ensaïmada, tambor d'ametlla, and suspiros de Manacor.

Balearic food is an example of the famous Mediterranean diet due to the importance of olive oil, legumes, unrefined cereals, fruits, vegetables and fish.

The cuisine of the Basque Country has a wide and varied range of ingredients and preparations. The culture of eating is very strong among the inhabitants of this region. Highlights include meat and fish dishes. Among fish, cod is produced in various preparations: bacalao al pil pil, cod Bilbao, etc.. Are also common anchovy, bream, bonito, etc.. Among the most famous dishes is the seafood changurro. Among the meats are: the beef steaks, pork loin with milk, fig leaf quail, marinated goose, etc.

The Canary Islands have a unique cuisine due to their geographical location in the Atlantic ocean. The Canary Islands were part of the trading routes to the American Continent, hence creating a melting pot of different culinary traditions. Fish (fresh or salted) and potatoes are among the most common staple foods in the islands. The consumption of cheese, fruits and pork meat also characterizes canarian cuisine. The closeness to Africa influences climate and creates a range of warm temperatures that in modern times have fostered the agriculture of tropical and semitropical crops: bananas, yams, mangoes, avocados and persimmons which are heavily used in canarian cuisine.

The aboriginal people Guanches based their diet on gofio (a type of flour made of different toasted grains), shellfish, and goat and pork products. Gofio is still consumed in the islands and has become part of the traditional cuisine of the islands.

A sauce called mojo (from Portuguese origins) is very common through the islands and has developed different varieties adapted to the main dish where it is being used. Fish dishes usually require a "green mojo" made from coriander or parsley, while roasted meats require a red variety made from chilli peppers that is commonly known as mojo picón.

Stew is a very common kind of fish stew, reminiscent of dishes in other culinary traditions. Some other classic dishes in the Canary Islands include papas arrugadas, almogrote, frangollo, rabbit in "salmorejo sauce" and stewed goat.

Some popular desserts are: truchas (pastries filled with sweet potato or pumpkin), roasted gofio (a gofio-based dough with nuts and honey), príncipe alberto (a mousse-like preparation with almonds, coffee, and chocolate) and quesillo (a variety of flan made with condensed milk)

Winery is common in the islands; however, only Malvasia wine from Lanzarote has gained international recognition.

A popular Cantabrian dish is "cocido montañés" (highlander stew), a rich stew made with beans, cabbage and pork. Seafood is widely used and bonito is present in the typical "sorropotún" or marmite. Recognized quality meats are Tudanca veal and game meat. Cantabrian pastries include "sobaos" and "quesadas pasiegas". Dairy products include Cantabrian cream cheese, smoked cheeses, "picón Bejes-Tresviso" and "quesucos de Liébana". "Orujo" is the Cantabrian pomace brandy. Cider ("sidra") and "chacoli" wine are increasing in popularity.

Cantabria has two wines labelled DOC: Costa de Cantabria and Liébana.

In this region, the culinary habits reflect the origin of foods eaten by shepherds and peasants. "Al-Manchara" means, in Arabic, "Dry Land" indicating the arid lands and the quality of its dishes. It is said that the best La Mancha cuisine cookbook is the novel "Don Quixote" by Miguel de Cervantes. Wheat and grains are dominant, used in bread, soups, gazpacho manchego, crumbs, porridge, etc.. One of the most abundant ingredients in Manchego cuisine is garlic, leading to dishes such as: ajo arriero, ajo puerco and garlic marinade.

Some traditional recipes are gazpacho manchego, pisto manchego and migas ruleras. Also popular is morteruelo, a kind of foie gras manchego. Manchego cheese is renowned.

Given the fact that its lands are dry, and thus unable to sustain large amounts of cattle living on grass, an abundance of small animals, such as rabbit, and especially birds (pheasant, quail, partridge, squab) can be found. This has led to game meat being incorporated into traditional dishes, such as Conejo al Ajillo (rabbit in garlic sauce), Perdiz Escabechada (marinated partridge) or Huevos de Codorniz (Quail's eggs).

In Castile and León characteristic dishes include morcilla, (a black pudding made with special spices), "judión de la granja", "sopa de ajo" (garlic soup), "Cochinillo asado" (roast piglet), "lechazo" (roast lamb), "botillo del Bierzo", "hornazo" from Salamanca, "Jamón de Guijuelo" (a cured ham from Guijuelo, Salamanca), "Salchichas de Zaratán" and other sausages, Serrada cheese, Burgos's soft cheese, and Ribera del Duero wines.

Major wines in Castilian-Leonese cuisine include the robust wine of Toro, reds from Ribera del Duero, whites from Rueda, and clarets from Cigales.

The cuisine of Catalonia is based in a rural culture; it is very extensive and a great culinary wealth. Notably, it was in Catalonia where the first cookbook was written in Spain. It has a triple cuisine: seafood, mountain and interior. Among the most popular dishes include: escudella and bread with tomato. Bean omelette, coca de recapte, samfaina, thyme soup and caragols a la llauna are famous dishes. Notable sauces are: romesco sauce, aioli, bouillabaisse of Catalan origin and picada.

Cured pork cuisine boasts botifarra (white and black) and the fuet of Vic. Among the fish dishes are: suquet de peix, cod stew and arròs negre. Among the vegetable dishes, the most famous are calçots and the escalivada (roasted vegetables). Among the desserts are: Catalan cream, carquinyolis, panellets, tortell, and neules.

La Rioja is recognized by the use of meats such as pork, and their cold cuts made after the traditional slaughter. The lamb is perhaps the second most popular meat product in this region (Sarmiento chops) and finally, veal is common in mountain areas. The most famous dish is Rioja style potatoes and Fritada. Lesser known are: Holy lunch and Ajo huevo (garlic eggs).

Another well-known dish is Rioja stew. Pimientos asados (roasted peppers) is a notable vegetable dish. Rioja wine has designated origin status.

La Rioja is famously known in Spain for its red wine, that is why most of these dishes are served with wine.

The cuisine of Extremadura is austere, with dishes prepared by shepherds. It is very similar to the cuisine of Castilla. Extremaduran cuisine is abundant in pork; it is said that the region is one of the best for breeding pigs in Spain, thanks to the acorns that grow in their fields: Iberian pig herds raised in the fields of Montánchez are characterized by dark skin and black, thin legs. This breed of pig is found exclusively in Spain and Portugal. Iberian pork sausages are common, such as pork stews (cocido extremeño).

Another meat dishes is lamb stew. It is also known that lizard is often cooked in Extremadura. Highlights include game meats such as wild boar, partridge, pheasant or venison. Famous cheeses are Torta de la Serena and Torta del Casar. Among the desserts are: Leche frita, perrunillas and fritters, as well as many sweets that have their origins in convents.

Galician cuisine is known in Spanish territory because of the emigration of its inhabitants. One of the most noted is Galician soup. Also notable is pork with turnip tops, a popular component of the Galician carnival meal laconadas. Another remarkable recipe is Caldo de castañas (a chestnut broth), which is commonly consumed during winter. Pork products are also popular.

The seafood dishes are very famous and rich in variety. Among these are: the Galician empanada, Galician octopus, scallops, crab and barnacles. Among the many dairy products is Queso de tetilla. Orujo is one of Galicia's alcoholic drinks. Sweets that are famous throughout the Iberian Peninsula are the Tarta de Santiago and Filloas (pancakes).

Cattle breeding is very common in Galicia, therefore, a lot of red meat is consumed; typically consumed with potatoes.

Madrid did not gain its own identity in the Court until 1561, when Philip II moved the capital to Madrid. Since then, due to immigration, many of Madrid's culinary dishes have been made from modifications to dishes from other Spanish regions. Madrid, due to the influx of visitors from the nineteenth century onwards, was one of the first cities to introduce the concept of the restaurant, hosting some of the earliest examples.

Notable dairy products are: rice pudding, meringue milk, cheese and curd. Some important fruits and vegetables are Aranjuez strawberries and melons. Madrid is rich in religious confectionery, with sweets such as chocolate con churros and buñuelos.
The nutritional value of the madrilian cuisine was discovered by the American epidemiologist Ancel Keys in the 1950, the Spanish cuisine being later often mentioned by epidemiologists as one of the best examples of the Mediterranean diet.

The cuisine of the region of Murcia has two sides with the influence of Manchego cuisine. The region of Murcia is famous for its varied fruit production. Among the most outstanding dishes are: Murcia tortilla, zarangollo, mojete, eggplants cream, pipirrana, etc.. A typical sauce of this area is the cabañil garlic, used to accompany meat dishes.

Among the culinary preparations are: the michirones (dried beans cooked with bay leaves, hot peppers and garlic). Among the cooked include: the olla gitana, cocido murciano con pelotas, mondongo, etc.. Among meat products Murcia find black pudding, which is flavored with oregano, and pastel murciano that is made with ground beef. Among the fish and seafood are: the golden salt, the Mar Menor prawns and octopus baked. Rices are common and among them are: the Caldero, the Arroz empedrado, rice with rabbit and snails, rice scribe, and the widower rice.

Among confectionery products are the exploradores and the pastel de Cierva. They are typical cakes in Murcia gastronomy and are found in almost every pastry shop in Murcia. They are both sweet and savoury at the same time.

The desserts are very abundant, among them are paparajotes Orchard, stuffed pastries and various other pastries. This region also has wine appellation of origin, as the wine from Jumilla, Bullas wine and wine Yecla.

The gastronomy of Navarra has many similarities with the Basque cuisine. Two of its flag dishes are: Tout Navarre Style and Ajoarriero, although we must not forget the lamb chilindrón or relleno. There are very curious recipes such as the Carlists eggs.

Salted products are common and, between them, include: chorizo de Pamplona, stuffing and sausage. The lamb and beef have, at present, designations of origin. Among the dairy products are: Roncal cheese, the curd or Idiazabal cheese. Among the most typical alcoholic drinks are: the claret and pacharán.

The cuisine of Valencia has two components: the rural (products of the field) and the other coastal, which is seafood. One popular Valencia creation is Paella, a rice dish cooked in a circular pan and topped with vegetables and meats (originally rabbit and chicken). Dishes such as Arroz con costra, Arròs negre, fideuá and throw rice, Arroz al horno, and rice with beans and turnips are also common in the city.

Coastal towns supply the region with fish, leading to popular dishes like "all i pebre" typical of the Albufera, or fish stew. Among the desserts are: coffee liqueur, chocolate Alicante, arnadí and horchata. Notably, during Christmas, nougat is made in Alicante and Jijona; also well-known are peladillas (almonds wrapped in a thick layer of caramel).

Other Spanish dishes:


Derivatives:



</doc>
<doc id="27670" url="https://en.wikipedia.org/wiki?curid=27670" title="Santiago de Compostela">
Santiago de Compostela

Santiago de Compostela is the capital of the autonomous community of Galicia, in northwestern Spain.

The city has its origin in the shrine of Saint James the Great, now the Cathedral of Santiago de Compostela, as the destination of the Way of St. James, a leading Catholic pilgrimage route since the 9th century. In 1985, the city's Old Town was designated a UNESCO World Heritage Site.

"Santiago" is the local Galician evolution of Vulgar Latin "Sanctus Iacobus" "Saint James". According to legend, "Compostela" derives from the Latin "Campus Stellae" (i.e., "field of the star"); it seems unlikely, however, that this phrase could have yielded the modern "Compostela" under normal evolution from Latin to Medieval Galician.

Other etymologies derive the name from Latin "compositum", local Vulgar Latin "Composita Tella", meaning "burial ground", or simply from Latin "compositella", meaning "the well-composed one". Other sites in Galicia share this toponym, akin to "Compostilla" in the province of León.

The cathedral borders the main plaza of the old and well-preserved city. According to medieval legend, the remains of the apostle James were brought to Galicia for burial; in 813, the light of a bright star guided a shepherd who was watching his flock at night to the burial site in Santiago de Compostela. This site was originally called Mount Libredon and its physical topography leads prevalent sea borne winds to clear the cloud deck immediately overhead. The shepherd quickly reported his discovery to the bishop of Iria, Bishop Teodomiro. The bishop declared that the remains were those of the apostle James and immediately notified King Alfonso II in Oviedo. To honour St. James, the cathedral was built on the spot where his remains were said to have been found. The legend, which included numerous miraculous events, enabled the Catholic faithful to bolster support for their stronghold in northern Spain during the Christian crusades against the Moors, but also led to the growth and development of the city.

Along the western side of the "Praza do Obradoiro" is the elegant 18th century Pazo de Raxoi, now the city hall. Across the square is the Pazo de Raxoi (Raxoi's Palace), the town hall, and on the right from the cathedral steps is the Hostal dos Reis Católicos, founded in 1492 by the Catholic Monarchs, Isabella of Castille and Ferdinand II of Aragon, as a pilgrims' hospice (now a parador). The Obradoiro façade of the cathedral, the best known, is depicted on the Spanish euro coins of 1 cent, 2 cents, and 5 cents (€0.01, €0.02, and €0.05).

Santiago is the site of the University of Santiago de Compostela, established in the early 16th century. The main campus can be seen best from an alcove in the large municipal park in the centre of the city.

Within the old town there are many narrow winding streets full of historic buildings. The new town all around it has less character though some of the older parts of the new town have some big flats in them.

Santiago de Compostela has a substantial nightlife. Both in the new town ("a zona nova" in Galician, "la zona nueva" in Spanish or "ensanche") and the old town ("a zona vella" in Galician or "la zona vieja" in Spanish, trade-branded as "zona monumental"), a mix of middle-aged residents and younger students maintain a lively presence until the early hours of the morning. Radiating from the centre of the city, the historic cathedral is surrounded by paved granite streets, tucked away in the old town, and separated from the newer part of the city by the largest of many parks throughout the city, "Parque da Alameda".

Santiago gives its name to one of the four military orders of Spain: Santiago, Calatrava, Alcántara and Montesa.

One of the most important economic centres in Galicia, Santiago is the seat for organisations like Association for Equal and Fair Trade Pangaea.

Under the Köppen climate classification, Santiago de Compostela has a temperate oceanic ("Cfb") climate, with mild to warm and somewhat dry summers and mild, wet winters. The prevailing winds from the Atlantic and the surrounding mountains combine to give Santiago some of Spain's highest rainfall: about annually. The climate is mild: frosts are common only in December, January and February, with an average of just 8 days per year, while snow is rare; temperatures over are exceptional.

The city is governed by a mayor-council form of government. Following the May 24, 2015 municipal elections the mayor of Santiago is Martiño Noriega Sánchez of Compostela Aberta (CA). No party has a majority in the city council ().

The population of the city in 2012 was 95,671 inhabitants, while the metropolitan area reaches 178,695.

In 2010 there were 4,111 foreigners living in the city, representing 4.3% of the total population. The main nationalities are Brazilians (11%), Portuguese (8%) and Colombians (7%).

By language, according to 2008 data, 21.17% of the population always speak in Galician, 15% always speak in Spanish, 31% mostly in Galician and the 32.17% mostly in Spanish. According to a Xunta de Galicia 2010 study the 38.5% of the city primary and secondary education students had Galician as their mother tongue.

The area of Santiago de Compostela was a Roman cemetery by the 4th century and was occupied by the Suebi in the early 5th century, when they settled in Galicia and Portugal during the initial collapse of the Roman Empire. The area was later attributed to the bishopric of Iria Flavia in the 6th century, in the partition usually known as Parochiale Suevorum, ordered by King Theodemar. In 585, the settlement was annexed along with the rest of Suebi Kingdom by Leovigild as the sixth province of the Visigothic Kingdom.

Possibly raided from 711 to 739 by the Arabs, the bishopric of Iria was incorporated into the Kingdom of Asturias c. 750. At some point between 818 and 842, during the reign of Alfonso II of Asturias, bishop Theodemar of Iria (d. 847) claimed to have found some remains which were attributed to Saint James the Greater. This discovery was accepted in part because the Leo III and Charlemagne—who had died in 814—had acknowledged Asturias as a kingdom and Alfonso II as king, and had also crafted close political and ecclesiastic ties. Around the place of the discovery a new settlement and centre of pilgrimage emerged, which was known to the author Usuard in 865 and which was called "Compostella" by the 10th century.

The cult of Saint James of Compostela was just one of many arising throughout northern Iberia during the 10th and 11th centuries, as rulers encouraged their own region-specific cults, such as Saint Eulalia in Oviedo and Saint Aemilian in Castile. After the centre of Asturian political power moved from Oviedo to León in 910, Compostela became more politically relevant, and several kings of Galicia and of León were acclaimed by the Galician noblemen and crowned and anointed by the local bishop at the cathedral, among them Ordoño IV in 958, Bermudo II in 982, and Alfonso VII in 1111, by which time Compostela had become capital of the Kingdom of Galicia. Later, 12th-century kings were also sepulchered in the cathedral, namely Fernando II and Alfonso IX, last of the Kings of León and Galicia before both kingdoms were united with the Kingdom of Castile.

During this same 10th century and in the first years of the 11th century Viking raiders tried to assault the town—Galicia is known in the Nordic sagas as "Jackobsland" or "Gallizaland"—and bishop Sisenand II, who was killed in battle against them in 968, ordered the construction of a walled fortress to protect the sacred place. In 997 Compostela was assaulted and partially destroyed by Ibn Abi Aamir (known as al-Mansur), Andalusian leader accompanied in his raid by Christian lords, who all received a share of the booty. However, the Andalusian commander showed no interest in the alleged relics of St James. In response to these challenges bishop Cresconio, in the mid-11th century, fortified the entire town, building walls and defensive towers.

According to some authors, by the middle years of the 11th century the site had already become a pan-European place of peregrination, while others maintain that the cult to Saint James was before 11-12th centuries an essentially Galician affair, supported by Asturian and Leonese kings to win over faltering Galician loyalties. Santiago would become in the course of the following century a main Catholic shrine second only to Rome and Jerusalem. In the 12th century, under the impulse of bishop Diego Gelmírez, Compostela became an archbishopric, attracting a large and multinational population. Under the rule of this prelate, the townspeople rebelled, headed by the local council, beginning a secular tradition of confrontation by the people of the city—who fought for self-government—against the local bishop, the secular and jurisdictional lord of the city and of its fief, the semi-independent "Terra de Santiago" ("land of Saint James"). The culminating moment in this confrontation was reached in the 14th century, when the new prelate, the Frenchman Bérenger de Landore, treacherously executed the counselors of the city in his castle of "A Rocha Forte" ("the strong rock, castle"), after inviting them for talks.

Santiago de Compostela was captured and sacked by the French during the Napoleonic Wars; as a result, the remains attributed to the apostle were lost for near a century, hidden inside a cist in the crypts of the cathedral of the city.

The excavations conducted in the cathedral during the 19th and 20th centuries uncovered a Roman "cella memoriae" or martyrium, around which grew a small cemetery in Roman and Suevi times which was later abandoned. This "martyrium", which proves the existence of an old Christian holy place, has been sometimes attributed to Priscillian, although without further proof.

Santiago's economy, although still heavily dependent on public administration (i.e. being the headquarters of the autonomous government of Galicia), cultural tourism, industry, and higher education through its university, is becoming increasingly diversified. New industries such as timber transformation (FINSA), the automotive industry (UROVESA), and telecommunications and electronics (Blusens and Televés) have been established. Banco Gallego, a banking institution owned by Novacaixagalicia, has its headquarters in downtown "rúa do Hórreo".

Tourism is very important thanks to the Way of St. James, particularly in Holy Compostelan Years (when 25 July falls on a Sunday). Following the Xunta's considerable investment and hugely successful advertising campaign for the Holy Year of 1993, the number of pilgrims completing the route has been steadily rising. More than 272,000 pilgrims made the trip during the course of the Holy Year of 2010. Following 2010, the next Holy Year will not be for another 11 years when St James feast day again falls on a Sunday. Outside of Holy Years, the city still receives a remarkable number of pilgrims. In 2013, 215,880 people completed the pilgrimage. In 2014, there were 237,983 persons. In 2015, there were 262,513 persons and in 2016, there were 277,854 persons.

Editorial Compostela owns daily newspaper El Correo Gallego, a local TV, and a radio station. Galician language online news portal Galicia Hoxe is also based in the city. Televisión de Galicia, the public broadcaster corporation of Galicia, has its headquarters in Santiago.

The legend that St James found his way to the Iberian Peninsula and had preached there is one of a number of early traditions concerning the missionary activities and final resting places of the apostles of Jesus. Although the 1884 Bull of Pope Leo XIII "Omnipotens Deus" accepted the authenticity of the relics at Compostela, the Vatican remains uncommitted as to whether the relics are those of Saint James the Greater, while continuing to promote the more general benefits of pilgrimage to the site. Pope Benedict XVI undertook a ceremonial pilgrimage to the site on his visit to Spain in 2010.

According to a tradition that can be traced back at least to the 12th century, when it was recorded in the "Codex Calixtinus", Saint James decided to return to the Holy Land after preaching in Galicia. There he was beheaded, but his disciples managed to get his body to Jaffa, where they found a marvelous stone ship which miraculously conducted them and the apostle's body to Iria Flavia, back in Galicia. There, the disciples asked the local pagan queen "Loba" ('She-wolf') for permission to bury the body; she, annoyed, decided to deceive them, sending them to pick a pair of oxen she allegedly had by the "Pico Sacro", a local sacred mountain where a dragon dwelt, hoping that the dragon would kill the Christians, but as soon as the beast attacked the disciples, at the sight of the cross, the dragon exploded. Then the disciples marched to collect the oxen, which were actually wild bulls which the queen used to punish her enemies; but again, at the sight of the Christian's cross, the bulls calmed down, and after being subjected to a yoke they carried the apostle's body to the place where now Compostela is. The legend was again referred with minor changes by the Czech traveller Jaroslav Lev of Rožmitál, in the 15th century.

The relics were said to have been later rediscovered in the 9th century by a hermit named Pelagius, who after observing strange lights in a local forest went for help after the local bishop, Theodemar of Iria, in the west of Galicia. The legend affirms that Theodemar was then guided to the spot by a star, drawing upon a familiar myth-element, hence "Compostela" was given an etymology as a corruption of Campus Stellae, "Field of Stars."

In the 15th century, the red banner which guided the Galician armies to battle, was still preserved in the Cathedral of Santiago de Compostela, in the centre Saint James riding a white horse and wearing a white cloak, sword in hand: The legend of the miraculous armed intervention of Saint James, disguised as a white knight to help the Christians when battling the Muslims, was a recurrent myth during the High Middle Ages.

The 1,000-year-old pilgrimage to the shrine of St. James in the Cathedral of Santiago de Compostela is known in English as the Way of St. James and in Spanish as the "Camino de Santiago". Over 100,000 pilgrims travel to the city each year from points all over Europe and other parts of the world. The pilgrimage has been the subject of many books, television programmes, and films, notably Brian Sewell's "The Naked Pilgrim" produced for the British television channel Channel 5 and the Martin Sheen/Emilio Estevez collaboration "The Way".

As the lowest-lying land on that stretch of coast, the city's site took on added significance. Legends supposed of Celtic origin made it the place where the souls of the dead gathered to follow the sun across the sea. Those unworthy of going to the Land of the Dead haunted Galicia as the "Santa Compaña" or "Estadea".

Santiago de Compostela is featured prominently in the 1988 historical fiction novel "Sharpe's Rifles", by Bernard Cornwell, which takes place during the French Invasion of Galicia, January 1809, during the Napoleonic Wars.

The music video for "Una Cerveza", by Ráfaga, is set in the historic part of Santiago de Compostela.

A pilgrimage to Santiago de Compostela provides the narrative framework of the Luis Buñuel film La Voie lactée (The Milky Way).

A mystic pilgrimage was portrayed in the autobiography and romance The Pilgrimage ("O Diário de um Mago") of brazilian writer Paulo Coelho, published in 1987.


Santiago de Compostela is served by Santiago de Compostela Airport and a rail service. The town is linked to the Spanish High Speed Railway Network. On 24 July 2013 there was a serious rail accident near the city in which 79 people died and at least 130 were injured when a train derailed on a bend as it approached Compostela station.



Santiago de Compostela is twinned with:




</doc>
<doc id="27672" url="https://en.wikipedia.org/wiki?curid=27672" title="Sailing">
Sailing

Sailing employs the wind—acting on sails, wingsails or kites—to propel a craft on the surface of the "water" (sailing ship, sailboat, windsurfer, or kitesurfer), on "ice" (iceboat) or on "land" (land yacht) over a chosen course, which is often part of a larger plan of navigation.

A course defined with respect to the true wind direction is called a point of sail.

Conventional sailing craft cannot derive power from sails on a point of sail that is too close into the wind. On a given point of sail, the sailor adjusts the alignment of each sail with respect to the apparent wind direction (as perceived on the craft) to mobilize the power of the wind. The forces transmitted via the sails are resisted by forces from the hull, keel, and rudder of a sailing craft, by forces from skate runners of an iceboat, or by forces from wheels of a land sailing craft to allow steering the course.

In the 21st century, most sailing represents a form of recreation or sport. Recreational sailing or yachting can be divided into racing and cruising. Cruising can include extended offshore and ocean-crossing trips, coastal sailing within sight of land, and daysailing.

Until the mid of the 19th century, sailing ships were the primary means for marine commerce, this period is known as Age of Sail.

Throughout history sailing has been instrumental in the development of civilization, affording humanity greater mobility than travel over land, whether for trade, transport or warfare, and the capacity for fishing. The earliest representation of a ship under sail appears on a painted disc found in Kuwait dating between 5000 and 5500 BCE. Polynesian oceanfarers traveled vast distances of open ocean in outrigger canoes using navigation methods such as stick charts. Advances in sailing technology from the Middle Ages onward enabled Arab, Chinese, Indian and European explorers to make longer voyages into regions with extreme weather and climatic conditions. There were improvements in sails, masts and rigging; improvements in marine navigation, including the cross tree and charts of both the sea and constellations, allowed more certainty in sea travel. From the 15th century onwards, European ships went further north, stayed longer on the Grand Banks and in the Gulf of St. Lawrence, and eventually began to explore the Pacific Northwest and the Western Arctic. Sailing has contributed to many great explorations in the world.

According to Jett, the Egyptians used a bipod mast to support a sail that allowed a reed craft to travel upriver with a following wind, as late as 3,500 BCE. Such sails evolved into the square-sail rig that persisted up to the 19th century. Such rigs generally could not sail much closer than 80° to the wind. Fore-and-aft rigs appear to have evolved in Southeast Asia—dates are uncertain—allowing for rigs that could sail as close as 60–75° off the wind.

The physics of sailing arises from a balance of forces between the wind powering the sailing craft as it passes over its sails and the resistance by the sailing craft against being blown off course, which is provided in the water by the keel, rudder, underwater foils and other elements of the underbody of a sailboat, on ice by the runners of an ice boat, or on land by the wheels of a sail-powered land vehicle.

Forces on sails depend on wind speed and direction and the speed and direction of the craft. The speed of the craft at a given point of sail contributes to the "apparent wind"—the wind speed and direction as measured on the moving craft. The apparent wind on the sail creates a total aerodynamic force, which may be resolved into drag—the force component in the direction of the apparent wind—and lift—the force component normal (90°) to the apparent wind. Depending on the alignment of the sail with the apparent wind ("angle of attack"), lift or drag may be the predominant propulsive component. Depending on the angle of attack of a set of sails with respect to the apparent wind, each sail is providing motive force to the sailing craft either from lift-dominant attached flow or drag-dominant separated flow. Additionally, sails may interact with one another to create forces that are different from the sum of the individual contributions each sail, when used alone.

The term "velocity" refers both to speed and direction. As applied to wind, "apparent wind velocity" (V) is the air velocity acting upon the leading edge of the most forward sail or as experienced by instrumentation or crew on a moving sailing craft. In nautical terminology, wind speeds are normally expressed in knots and wind angles in degrees. All sailing craft reach a constant "forward velocity" (V) for a given "true wind velocity" (V) and "point of sail". The craft's point of sail affects its velocity for a given true wind velocity. Conventional sailing craft cannot derive power from the wind in a "no-go" zone that is approximately 40° to 50° away from the true wind, depending on the craft. Likewise, the directly downwind speed of all conventional sailing craft is limited to the true wind speed. As a sailboat sails further from the wind, the apparent wind becomes smaller and the lateral component becomes less; boat speed is highest on the beam reach. In order to act like an airfoil, the sail on a sailboat is sheeted further out as the course is further off the wind. As an iceboat sails further from the wind, the apparent wind increases slightly and the boat speed is highest on the broad reach. In order to act like an airfoil, the sail on an iceboat is sheeted in for all three points of sail.

"Lift" on a sail, acting as an airfoil, occurs in a direction "perpendicular" to the incident airstream (the apparent wind velocity for the head sail) and is a result of pressure differences between the windward and leeward surfaces and depends on angle of attack, sail shape, air density, and speed of the apparent wind. The lift force results from the average pressure on the windward surface of the sail being higher than the average pressure on the leeward side. These pressure differences arise in conjunction with the curved air flow. As air follows a curved path along the windward side of a sail, there is a pressure gradient perpendicular to the flow direction with higher pressure on the outside of the curve and lower pressure on the inside. To generate lift, a sail must present an "angle of attack" between the chord line of the sail and the apparent wind velocity. Angle of attack is a function of both the craft's point of sail and how the sail is adjusted with respect to the apparent wind.

As the lift generated by a sail increases, so does lift-induced drag, which together with parasitic drag constitute total "drag", which acts in a direction "parallel" to the incident airstream. This occurs as the angle of attack increases with sail trim or change of course and causes the lift coefficient to increase up to the point of aerodynamic stall along with the lift-induced drag coefficient. At the onset of stall, lift is abruptly decreased, as is lift-induced drag. Sails with the apparent wind behind them (especially going downwind) operate in a stalled condition.

Lift and drag are components of the total aerodynamic force on sail, which are resisted by forces in the water (for a boat) or on the traveled surface (for an ice boat or land sailing craft). Sails act in two basic modes; under the "lift-predominant" mode, the sail behaves in a manner analogous to a "wing" with airflow attached to both surfaces; under the "drag-predominant" mode, the sail acts in a manner analogous to a "parachute" with airflow in detached flow, eddying around the sail.

Sails allow progress of a sailing craft to windward, thanks to their ability to generate lift (and the craft's ability to resist the lateral forces that result). Each sail configuration has a characteristic coefficient of lift and attendant coefficient of drag, which can be determined experimentally and calculated theoretically. Sailing craft orient their sails with a favorable angle of attack between the entry point of the sail and the apparent wind even as their course changes. The ability to generate lift is limited by sailing too close to the wind when no effective angle of attack is available to generate lift (causing luffing) and sailing sufficiently off the wind that the sail cannot be oriented at a favorable angle of attack to prevent the sail from stalling with flow separation.

When sailing craft are on a course where the angle between the sail and the apparent wind (the angle of attack) exceeds the point of maximum lift, separation of flow occurs. Drag increases and lift decreases with increasing angle of attack as the separation becomes progressively pronounced until the sail is perpendicular to the apparent wind, when lift becomes negligible and drag predominates. In addition to the sails used upwind, spinnakers provide area and curvature appropriate for sailing with separated flow on downwind points of sail, analogous to parachutes, which provide both lift and drag.


Wind speed increases with height above the surface; at the same time, wind speed may vary over short periods of time as gusts.

Wind shear affects sailing craft in motion by presenting a different wind speed and direction at different heights along the mast. Wind shear occurs because of friction above a water surface slowing the flow of air. The ratio of wind at the surface to wind at a height above the surface varies by a power law with an exponent of 0.11-0.13 over the ocean. This means that a 5-m/s (≈10-knot) wind at 3 m above the water would be approximately 6 m/s (≈12 knots) at 15 m above the water. In hurricane-force winds with 40-m/s (≈78 knots) at the surface the speed at 15 m would be 49 m/s (≈95 knots). This suggests that sails that reach higher above the surface can be subject to stronger wind forces that move the centre of effort on them higher above the surface and increase the heeling moment. Additionally, apparent wind direction moves aft with height above water, which may necessitate a corresponding twist in the shape of the sail to achieve attached flow with height.

Gusts may be predicted by the same value that serves as an exponent for wind shear, serving as a gust factor. So, one can expect gusts to be about 1.5 times stronger than the prevailing wind speed (a 10-knot wind might gust up to 15 knots). This, combined with changes in wind direction suggest the degree to which a sailing craft must adjust sail angle to wind gusts on a given course.

A sailing craft's ability to derive power from the wind depends on the point of sail it is on—the direction of travel under sail in relation to the true wind direction over the surface. The principal points of sail roughly correspond to 45° segments of a circle, starting with 0° directly into the wind. For many sailing craft 45° on either side of the wind is a "no-go" zone, where a sail is unable to mobilize power from the wind. Sailing on a course as close to the wind as possible—approximately 45°—is termed "close-hauled". At 90° off the wind, a craft is on a "beam reach". At 135° off the wind, a craft is on a "broad reach". At 180° off the wind (sailing in the same direction as the wind), a craft is "running downwind".

In points of sail that range from close-hauled to a broad reach, sails act substantially like a wing, with lift predominantly propelling the craft. In points of sail from a broad reach to down wind, sails act substantially like a parachute, with drag predominantly propelling the craft. For craft with little forward resistance ice boats and land yachts, this transition occurs further off the wind than for sailboats and sailing ships.

Wind direction for points of sail always refers to the "true wind"—the wind felt by a stationary observer. The "apparent wind"—the wind felt by an observer on a moving sailing craft—determines the motive power for sailing craft.

The waves give an indication of the "true wind" direction. The pennant (Canadian flag) gives an indication of "apparent wind" direction.

True wind velocity (V) combines with the sailing craft's velocity (V) to be the "apparent wind velocity" (V), the air velocity experienced by instrumentation or crew on a moving sailing craft. Apparent wind velocity provides the motive power for the sails on any given point of sail. It varies from being the true wind velocity of a stopped craft in irons in the no-go zone to being faster than the true wind speed as the sailing craft's velocity adds to the true windspeed on a reach, to diminishing towards zero, as a sailing craft sails dead downwind.

Sailing craft A is close-hauled. Sailing craft B is on a beam reach. Sailing craft C is on a broad reach.<br>Boat velocity (in black) generates an equal and opposite apparent wind component (not shown), which adds to the true wind to become apparent wind.

The speed of sailboats through the water is limited by the resistance that results from hull drag in the water. Ice boats typically have the least resistance to forward motion of any sailing craft. Consequently, a sailboat experiences a wider range of apparent wind angles than does an ice boat, whose speed is typically great enough to have the apparent wind coming from a few degrees to one side of its course, necessitating sailing with the sail sheeted in for most points of sail. On conventional sail boats, the sails are set to create lift for those points of sail where it's possible to align the leading edge of the sail with the apparent wind.

For a sailboat, point of sail affects lateral force significantly. The higher the boat points to the wind under sail, the stronger the lateral force, which requires resistance from a keel or other underwater foils, including daggerboard, centerboard, skeg and rudder. Lateral force also induces heeling in a sailboat, which requires resistance by weight of ballast from the crew or the boat itself and by the shape of the boat, especially with a catamaran. As the boat points off the wind, lateral force and the forces required to resist it become less important.
On ice boats, lateral forces are countered by the lateral resistance of the blades on ice and their distance apart, which generally prevents heeling.

Wind and currents are important factors to plan on for both offshore and inshore sailing. Predicting the availability, strength and direction of the wind is key to using its power along the desired course. Ocean currents, tides and river currents may deflect a sailing vessel from its desired course.

If the desired course is within the no-go zone, then the sailing craft must follow a zig-zag route into the wind to reach its waypoint or destination. Downwind, certain high-performance sailing craft can reach the destination more quickly by following a zig-zag route on a series of broad reaches.

Negotiating obstructions or a channel may also require a change direction of with respect to the wind, necessitating changing of tack with the wind on the opposite side of the craft, from before.

Changing tack is called "tacking" when the wind crosses over the bow of the craft as it turns and "jibing" (or "gybing") if the wind passes over the stern.

Winds and oceanic currents are both the result of the sun powering their respective fluid media. Wind powers the sailing craft and the ocean bears the craft on its course, as currents may alter the course of a sailing vessel on the ocean or a river.

A sailing craft can sail on a course anywhere outside of its no-go zone. If the next waypoint or destination is within the arc defined by the no-go zone from the craft's current position, then it must perform a series of tacking maneuvers to get there on a dog-legged route, called "beating to windward". The progress along that route is called the "course made good"; the speed between the starting and ending points of the route is called the "speed made good" and is calculated by the distance between the two points, divided by the travel time. The limiting line to the waypoint that allows the sailing vessel to leave it to leeward is called the "layline". Whereas some Bermuda-rigged sailing yachts can sail as close as 30° to the wind, most 20th-Century square riggers are limited to 60° off the wind. Fore-and-aft rigs are designed to operate with the wind on either side, whereas square rigs and kites are designed to have the wind come from one side of the sail only.

Because the lateral wind forces are highest on a sailing vessel, close-hauled and beating to windward, the resisting water forces around the vessel's keel, centerboard, rudder and other foils is also highest to mitigate leeway—the vessel sliding to leeward of its course. Ice boats and land yachts minimize lateral motion with sidewise resistance from their blades or wheels.


"Tacking" or "coming about" is a maneuver by which a sailing craft turns its bow into and through the wind (called the "eye of the wind") so that the apparent wind changes from one side to the other, allowing progress on the opposite tack. The type of sailing rig dictates the procedures and constraints on achieving a tacking maneuver. Fore-and-aft rigs allow their sails to hang limp as they tack; square rigs must present the full frontal area of the sail to the wind, when changing from side to side; and windsurfers have flexibly pivoting and fully rotating masts that get flipped from side to side.

A sailing craft can travel directly downwind only at a speed that is less than the wind speed. However, a variety of sailing craft can achieve a higher downwind speed made good by traveling on a series of broad reaches, punctuated by jibes in between. This is true of ice boats and sand yachts. On the water it was explored by sailing vessels, starting in 1975, and now extends to high-performance skiffs, catamarans and foiling sailboats.

Navigating a channel or a downwind course among obstructions may necessitate changes in direction that require a change of tack, accomplished with a jibe.

"Jibing" or "gybing" is a sailing maneuver by which a sailing craft turns its stern past the eye of the wind so that the apparent wind changes from one side to the other, allowing progress on the opposite tack. As with tacking, the type of sailing rig dictates the procedures and constraints for jibing. Fore-and-aft sails with booms, gaffs or sprits are unstable when they point into the eye of the wind and must be controlled to avoid a violent change to the other side; square rigs as they present the full area of the sail to the wind from the rear experience little change of operation from one tack to the other; and windsurfers again have flexibly pivoting and fully rotating masts that get flipped from side to side.

The most basic control of the sail consists of setting its angle relative to the wind. The control line that accomplishes this is called a "sheet." If the sheet is too loose the sail will flap in the wind, an occurrence that is called "luffing." Optimum sail angle can be approximated by pulling the sheet in just so far as to make the luffing stop, or by using tell-tails – small ribbons or yarn attached each side of the sail that both stream horizontally to indicate a properly trimmed sail. Finer controls adjust the overall shape of the sail.

Two or more sails are frequently combined to maximize the smooth flow of air. The sails are adjusted to create a smooth laminar flow over the sail surfaces. This is called the "slot effect". The combined sails fit into an imaginary aerofoil outline, so that the most forward sails are more in line with the wind, whereas the more aft sails are more in line with the course followed. The combined efficiency of this sail plan is greater than the sum of each sail used in isolation.

More detailed aspects include specific control of the sail's shape, e.g.:

An important safety aspect of sailing is to adjust the amount of sail to suit the wind conditions. As the wind speed increases the crew should progressively reduce the amount of sail. On a small boat with only jib and mainsail this is done by furling the jib and by partially lowering the mainsail, a process called 'reefing the main'.

Reefing means reducing the area of a sail without actually changing it for a smaller sail. Ideally, reefing does not only result in a reduced sail area but also in a lower centre of effort from the sails, reducing the heeling moment and keeping the boat more upright.

There are three common methods of reefing the mainsail:

Mainsail furling systems have become increasingly popular on cruising yachts, as they can be operated shorthanded and from the cockpit, in most cases. However, the sail can become jammed in the mast or boom slot if not operated correctly. Mainsail furling is almost never used while racing because it results in a less efficient sail profile. The classical slab-reefing method is the most widely used. Mainsail furling has an additional disadvantage in that its complicated gear may somewhat increase weight aloft. However, as the size of the boat increases, the benefits of mainsail roller furling increase dramatically.

An old saying goes, “Once you’ve realized it’s time to reef, it’s too late.” A similar one says, "The time to reef is when you first think about it."

Hull trim is the adjustment of a boat's loading so as to change its fore-and-aft attitude in the water. In small boats, it is done by positioning the crew. In larger boats, the weight of a person has less effect on the hull trim, but it can be adjusted by shifting gear, fuel, water, or supplies. Different hull trim efforts are required for different kinds of boats and different conditions. Here are just a few examples: In a lightweight racing dinghy like a Thistle, the hull should be kept level, on its designed water line for best performance in all conditions. In many small boats, weight too far aft can cause drag by submerging the transom, especially in light to moderate winds. Weight too far forward can cause the bow to dig into the waves. In heavy winds, a boat with its bow too low may capsize by pitching forward over its bow (pitch-pole) or dive under the waves (submarine). On a run in heavy winds, the forces on the sails tend to drive a boat's bow down, so the crew weight is moved far aft.

When a ship or boat leans over to one side, from the action of waves or from the centrifugal force of a turn or under wind pressure or from the amount of exposed topsides, it is said to 'heel'. A sailing boat that is over-canvassed and therefore heeling excessively, may sail less efficiently. This is caused by factors such as wind gusts, crew ability, the point of sail, or hull size & design.

When a vessel is subject to a heeling force (such as wind pressure), vessel buoyancy & beam of the hull will counteract the heeling force. A weighted keel provides additional means to right the boat. In some high-performance racing yachts, water ballast or the angle of a canting keel can be changed to provide additional righting force to counteract heeling. The crew may move their personal weight to the high (upwind) side of the boat, this is called "hiking", which also changes the centre of gravity & produces a righting lever to reduce the degree of heeling. Incidental benefits include faster vessel speed caused by more efficient action of the hull & sails. Other options to reduce heeling include reducing exposed sail area & efficiency of the sail setting & a variant of hiking called "trapezing". This can only be done if the vessel is designed for this, as in dinghy sailing. A sailor can (usually involuntarily) try turning upwind in gusts (it is known as "rounding up"). This can lead to difficulties in controlling the vessel if over-canvassed. Wind can be spilled from the sails by 'sheeting out', or loosening them. The number of sails, their size and shape can be altered. Raising the dinghy centreboard can reduce heeling by allowing more leeway.

The increasingly asymmetric underwater shape of the hull matching the increasing angle of heel may generate an increasing directional turning force into the wind. The sails' centre of effort will also increase this turning effect or force on the vessel's motion due to increasing lever effect with increased heeling which shows itself as increased human effort required to steer a straight course. Increased heeling reduces exposed sail area relative to the wind direction, so leading to an equilibrium state. As more heeling force causes more heel, weather helm may be experienced. This condition has a braking effect on the vessel but has the safety effect in that an excessively hard pressed boat will try and turn into the wind, therefore, reducing the forces on the sail. Small amounts (≤5 degrees) of weather helm are generally considered desirable because of the consequent aerofoil lift effect from the rudder. This aerofoil lift produces helpful motion to windward & the corollary of the reason why lee helm is dangerous. Lee helm, the opposite of weather helm, is generally considered to be dangerous because the vessel turns away from the wind when the helm is released, thus increasing forces on the sail at a time when the helmsperson is not in control.

Sailing boats with one hull are "monohulls", those with two are "catamarans", those with three are "trimarans". A boat is turned by a rudder, which itself is controlled by a tiller or a wheel, while at the same time adjusting the sheeting angle of the sails. Smaller sailing boats often have a stabilizing, raisable, underwater fin called a centreboard, daggerboard, or leeboard; larger sailing boats have a fixed (or sometimes canting) keel. As a general rule, the former are called dinghies, the latter keelboats. However, up until the adoption of the Racing Rules of Sailing, any vessel racing under sail was considered a yacht, be it a multi-masted ship-rigged vessel (such as a sailing frigate), a sailboard (more commonly referred to as a windsurfer) or remote-controlled boat, or anything in between. (See Dinghy sailing.)

Multihulls use flotation and/or weight positioned away from the centre line of the sailboat to counter the force of the wind. This is in contrast to heavy ballast that can account for up to 90% (in extreme cases like AC boats) of the weight of a monohull sailboat. In the case of a standard catamaran, there are two similarly-sized and -shaped slender hulls connected by beams, which are sometimes overlaid by a deck superstructure. Another catamaran variation is the proa. In the case of trimarans, which have an unballasted centre hull similar to a monohull, two smaller amas are situated parallel to the centre hull to resist the sideways force of the wind. The advantage of multihulled sailboats is that they do not suffer the performance penalty of having to carry heavy ballast, and their relatively lesser draft reduces the amount of drag, caused by friction and inertia when moving through the water.

One of the most common dinghy hulls in the world is the Laser hull. It was designed by Bruce Kirby in 1969 and unveiled at the New York boat show (1971). It was designed with speed and simplicity in mind. The Laser is 13 feet 10.5 inches long and a 12.5 foot water line and of sail.

A traditional modern yacht is technically called a "Bermuda sloop" (sometimes a "Bermudan sloop"). A sloop is any boat that has a single mast and usually a single headsail (generally a jib) in addition to the mainsail (Bermuda rig but c.f. Friendship sloop). A cutter (boat) also has a single mast, set further aft than a sloop and more than one headsail. Additionally, Bermuda sloops only have a single sail behind the mast. Other types of sloops are gaff-rigged sloops and lateen sloops. Gaff-rigged sloops have quadrilateral mainsails with a gaff (a small boom) at their upper edge (the "head" of the sail). Gaff-rigged vessels may also have another sail, called a topsail, above the gaff. Lateen sloops have triangular sails with the upper edge attached to a gaff, and the lower edge attached to the boom, and the boom and gaff are attached to each other via some type of hinge. It is also possible for a sloop to be square rigged (having large square sails like a Napoleonic Wars-era ship of the line). Note that a "sloop of war", in the naval sense, may well have more than one mast, and is not properly a sloop by the modern meaning.

If a boat has two masts, it may be a schooner, a ketch, or a yawl, if it is rigged fore-and-aft on all masts. A schooner may have any number of masts provided the second from the front is the tallest (called the "main mast"). In both a ketch and a yawl, the foremost mast is tallest, and thus the main mast, while the rear mast is shorter, and called the mizzen mast. The difference between a ketch and a yawl is that in a ketch, the mizzen mast is forward of the rudderpost (the axis of rotation for the rudder), while a yawl has its mizzen mast behind the rudderpost. In modern parlance, a brigantine is a vessel whose forward mast is rigged with square sails, while her after mast is rigged fore-and-aft. A brig is a vessel with two masts both rigged square.

As one gets into three or more masts the number of combinations rises and one gets barques, barquentines, and full-rigged ships.

A spinnaker is a large, full sail that is only used when sailing off wind either reaching or downwind, to catch the maximum amount of wind.

With modern technology, "wings", that is rigid sails, may be used in place of fabric sails. An example of this would be the International C-Class Catamaran Championship and the yacht USA 17 that won the 2010 America's Cup. Such rigid sails are typically made of thin plastic fabric held stretched over a frame. See also AC72 wing-sail catamarans which competed in the 2013 America's Cup.

Some non-traditional rigs capture energy from the wind in a different fashion and are capable of feats that traditional rigs are not, such as sailing directly into the wind. One such example is the wind turbine boat, also called the windmill boat, which uses a large windmill to extract energy from the wind, and a propeller to convert this energy to forward motion of the hull. A similar design, called the autogyro boat, uses a wind turbine without the propellor, and functions in a manner similar to a normal sail. A more recent (2010) development is a cart that uses wheels linked to a propeller to "sail" dead downwind at speeds exceeding wind speed.

Some sailing craft are propelled by kites, as with kitesurfing, which uses a tethered airfoil. Others use an airfoil on a pivoting spar, as with windsurfers. Both forms of sailing may use the airfoil in a way that provides an upward force, as well as a propulsive one, when the sailor controls the airfoil atop a planing board with a skeg.

Nautical terms for elements of a vessel: starboard (right-hand side), port or larboard (left-hand side), forward or fore (frontward), aft or abaft (rearward), bow (forward part of the hull), stern (aft part of the hull), beam (the widest part). Spars, supporting sails, include masts, booms, yards, gaffs and poles.

In most cases, "rope" is the term used only for raw material. Once a section of rope is designated for a particular purpose on a vessel, it generally is called a "line," as in "outhaul line" or "dock line". A very thick line is considered a "cable." Lines that are attached to sails to control their shapes are called "sheets", as in "mainsheet". If a rope is made of wire, it maintains its rope name as in 'wire rope' halyard.

Lines (generally steel cables) that support masts are stationary and are collectively known as a vessel's standing rigging, and individually as "shrouds" or "stays". The stay running forward from a mast to the bow is called the "forestay" or "headstay". Stays running aft are backstays or after stays.

Moveable lines that control sails or other equipment are known collectively as a vessel's running rigging. Lines that raise sails are called "halyards" while those that strike them are called "downhauls". Lines that adjust (trim) the sails are called "sheets". These are often referred to using the name of the sail they control (such as "main sheet", or "jib sheet"). Sail trim may also be controlled with smaller lines attached to the forward section of a boom such as a cunningham; a line used to hold the boom down is called a "vang", or a "kicker" in the United Kingdom. A "topping lift" is used to hold a boom up in the absence of sail tension. "Guys" are used to control the ends of other spars such as spinnaker poles.

Lines used to tie a boat up when alongside are called "docklines", "docking cables" or "mooring warps". In dinghies, the single line from the bow is referred to as the "painter". A "rode" is what attaches an anchored boat to its anchor. It may be made of chain, rope, or a combination of the two.

Some lines are referred to as ropes:

Walls are called "bulkheads" or "ceilings", while the surfaces referred to as ceilings on land are called "overheads" or "deckheads". Floors are called "soles" or "decks". The toilet is traditionally called the "head", the kitchen is the "galley". When lines are tied off, this may be referred to as "made fast" or "belayed." Sails in different sail plans have unchanging names, however. For the naming of sails, see sail-plan.

The tying and untying of knots and hitches, as well as the general handling of ropes and lines, are fundamental to the art of sailing. The RYA basic 'Start Yachting' syllabus lists the following knots and hitches:
It also lists securing a line around a cleat and the use of winches and jamming cleats.

The RYA Competent Crew syllabus adds the following to the list above, as well as knowledge of the correct use of each: 
In addition, it requires competent crewmembers to understand 'taking a turn' around a cleat and to be able to make cleated lines secure. Lines and halyards need to be coiled neatly for stowage and reuse. Dock lines need to be thrown and handled safely and correctly when coming alongside, up to a buoy, and when anchoring, as well as when casting off and getting under way.

Every vessel in coastal and offshore waters is subject to the International Regulations for Preventing Collisions at Sea (the COLREGS). On inland waterways and lakes other similar regulations, such as CEVNI in Europe, may apply. In some sailing events, such as the Olympic Games, which are held on closed courses where no other boating is allowed, specific racing rules such as the Racing Rules of Sailing (RRS) may apply. Often, in club racing, specific club racing rules, perhaps based on RRS, may be "superimposed" onto the more general regulations such as COLREGS or CEVNI.

In general, regardless of the activity, every sailor must
The stand-on vessel must hold a steady course and speed but be prepared to take late avoiding action to prevent an actual collision if the other vessel does not do so in time. The give-way vessel must take early, positive and obvious avoiding action, without crossing ahead of the other vessel. (Rules 16-17)


The COLREGS go on to describe the lights to be shown by vessels under way at night or in restricted visibility. Specifically, for sailing boats, red and green sidelights and a white sternlight are required, although for vessels under 7 metres (23.0 ft) in length, these may be substituted by a torch or white all-round lantern. (Rules 22 & 25)

Sailors are required to be aware not only of the requirements for their own boat, but of all the other lights, shapes and flags that may be shown by other vessels, such as those fishing, towing, dredging, diving etc., as well as sound signals that may be made in restricted visibility and at close quarters, so that they can make decisions under the COLREGS in good time, should the need arise. (Rules 32-37)

In addition to the COLREGS, CEVNI and/or any specific racing rules that apply to a sailing boat, there are also

Licensing regulations vary widely across the world. While boating on international waters does not require any license, a license may be required to operate a vessel on coastal waters or inland waters. Some jurisdictions require a license when a certain size is exceeded (e.g., a length of 20 meters), others only require licenses to pilot passenger ships, ferries or tugboats. For example, the European Union issues the International Certificate of Competence, which is required to operate pleasure craft in most inland waterways within the union. The United States, in contrast, has no licensing, but instead has voluntary certification organizations such as the American Sailing Association. These US certificates are often required to charter a boat, but are not required by any federal or state law.

Sailboat racing generally fits into one of two categories:

Sailing is a diverse sport with many pinnacles from the Olympic Games to many world championships titles to development based campaigns for the America's Cup to round the world races such as the Vendee Globe and Volvo Ocean Race.

Sailboat racing ranges from single person dinghy racing to large boats with 10 or more crew and from small boats costing a few thousand dollars to multimillion-dollar America's Cup campaigns. The costs of participating in the high-end large boat competitions make this type of sailing one of the most expensive sports in the world. However, there are inexpensive ways to get involved in sailboat racing, such as at community sailing clubs, classes offered by local recreation organizations and in some inexpensive dinghy and small catamaran classes. Under these conditions, sailboat racing can be comparable to or less expensive than sports such as golf and skiing. Sailboat racing is one of the few sports in which people of all ages and genders can regularly compete with and against each other.

The sport of Sailboat racing is governed by the World Sailing with most racing formats using the Racing Rules of Sailing.

Sailing regattas contain events which are defined by a combination of discipline, equipment, gender and sailor categories.

Common categories of equipment include the following dinghies, multihulls, keelboats sailing yacht windsurfers, kiteboarding and radio-controlled sailboats.

The following are the main disciplines:

The majority of sailing events are "open" events in which males and females compete together on equal terms either as individuals or part of a team. Sailing has had female only World Championships since the 1970s to encourage participation and now host more than 30 such World Championship titles each year. While many mixed gender crews have competed in open events compulsory mixed gender are now included as events in both Olympic (Nacra 17) and Paralympic (SKUD 18).

In addition, the following categories are sometimes applied to events:

Most sailboat and yacht racing is done in coastal or inland waters. However, in terms of endurance and risk to life, ocean races such as the Volvo Ocean Race, the solo VELUX 5 Oceans Race, and the non-stop solo Vendée Globe, rate as some of the most extreme and dangerous sporting events. Not only do participants compete for days with little rest, but an unexpected storm, a single equipment failure, or collision with an ice floe could result in the sailboat being disabled or sunk hundreds or thousands of miles from search and rescue.



Class racing can be further subdivided into measurement controlled and manufacturer controlled classes.

Manufacturer controlled classes strictly control the production and source of equipment. (e.g. 29er, Laser, Farr 40, RS Feva, Soling, etc.)

However, it is measurement controlled classes that offer the diversity in equipment. Some classes use measurement control to tightly control the boats as much as manufacturer class (e.g., 470, Contender, Star etc.)

At the other end of the extreme are the development classes which freely allow development within a defined framework. These are most commonly either formula based like the metre class or a box-rule that defines key criteria like maximum length, minimum weight, and maximum sail area. (e.g. Moth (dinghy), the A Class Catamaran, TP 52, and IMOCA 60.

Sailing for pleasure can involve short trips across a bay, day sailing, coastal cruising, and more extended offshore or 'blue-water' cruising. These trips can be singlehanded or the vessel may be crewed by families or groups of friends. Sailing vessels may proceed on their own, or be part of a flotilla with other like-minded voyagers. Sailing boats may be operated by their owners, who often also gain pleasure from maintaining and modifying their craft to suit their needs and taste, or may be rented for the specific trip or cruise. A professional skipper and even crew may be hired along with the boat in some cases. People take cruises in which they crew and 'learn the ropes' aboard craft such as tall ships, classic sailing vessels and restored working boats.

Cruising trips of several days or longer can involve a deep immersion in logistics, navigation, meteorology, local geography and history, fishing lore, sailing knowledge, general psychological coping, and serendipity. Once the boat is acquired it is not all that expensive an endeavor, often much less expensive than a normal vacation on land. It naturally develops self-reliance, responsibility, economy, and many other useful skills. Besides improving sailing skills, all the other normal needs of everyday living must also be addressed. There are work roles that can be done by everyone in the family to help contribute to an enjoyable outdoor adventure for all.

A style of casual coastal cruising called gunkholing is a popular summertime family recreational activity. It consists of taking a series of day sails to out of the way places and anchoring overnight while enjoying such activities as exploring isolated islands, swimming, fishing, etc. Many nearby local waters on rivers, bays, sounds, and coastlines can become great natural cruising grounds for this type of recreational sailing. Casual sailing trips with friends and family can become lifetime bonding experiences.

Long-distance voyaging, such as that across oceans and between far-flung ports, can be considered the near-absolute province of the cruising sailboat. Most modern yachts of 25–55 feet long, propelled solely by mechanical powerplants, cannot carry the fuel sufficient for a point-to-point voyage of even 250–500 miles without needing to resupply; but a well-prepared sail-powered yacht of similar length is theoretically capable of sailing anywhere its crew is willing to guide it. Even considering that the cost benefits are offset by a much-reduced cruising speed, many people traveling distances in small boats come to appreciate the more leisurely pace and increased time spent on the water.

Since the solo circumnavigation of Joshua Slocum in the 1890s, long-distance cruising under sail has inspired thousands of otherwise normal people to explore distant seas and horizons. The important voyages of Robin Lee Graham, Eric Hiscock, Don Street and others have shown that, while not strictly racing, ocean voyaging carries with it an inherent sense of competition, especially that between man and the elements.

Such a challenging enterprise requires keen knowledge of sailing in general as well as maintenance, navigation (especially celestial navigation), and often even international diplomacy (for which an entire set of protocols should be learned and practiced). But one of the great benefits of sailboat ownership is that one may at least imagine the type of adventure that the average affordable powerboat could never accomplish.




</doc>
<doc id="27675" url="https://en.wikipedia.org/wiki?curid=27675" title="Simple Mail Transfer Protocol">
Simple Mail Transfer Protocol

Simple Mail Transfer Protocol (SMTP) is an Internet standard for electronic mail (email) transmission. First defined by in 1982, it was last updated in 2008 with Extended SMTP additions by , which is the protocol in widespread use today. 

Although electronic mail servers and other mail transfer agents use SMTP to send and receive mail messages, user-level client mail applications typically use SMTP only for sending messages to a mail server for relaying. For retrieving messages, client applications usually use either IMAP or POP3.

SMTP communication between mail servers uses TCP port 25. Mail clients on the other hand, often submit the outgoing emails to a mail server on port 587. Despite being deprecated, mail providers sometimes still permit the use of nonstandard port 465 for this purpose. 

SMTP connections secured by TLS, known as SMTPS, can be made using STARTTLS. 

Although proprietary systems (such as Microsoft Exchange and IBM Notes) and webmail systems (such as Outlook.com, Gmail and Yahoo! Mail) use their own non-standard protocols to access mail box accounts on their own mail servers, all use SMTP when sending or receiving email from outside their own systems.

Various forms of one-to-one electronic messaging were used in the 1960s. People communicated with one another using systems developed for specific mainframe computers. As more computers were interconnected, especially in the US Government's ARPANET, standards were developed to allow users of different systems to email one another. SMTP grew out of these standards developed during the 1970s.

SMTP can trace its roots to two implementations described in 1971: the Mail Box Protocol, whose implementation has been disputed, but is discussed in and other RFCs, and the SNDMSG program, which, according to , Ray Tomlinson of BBN invented for TENEX computers to send mail messages across the ARPANET. Fewer than 50 hosts were connected to the ARPANET at this time.

Further implementations include FTP Mail and Mail Protocol, both from 1973. Development work continued throughout the 1970s, until the ARPANET transitioned into the modern Internet around 1980. Jon Postel then proposed a Mail Transfer Protocol in 1980 that began to remove the mail's reliance on FTP. SMTP was published as in November 1981, also by Postel.

The SMTP standard was developed around the same time as Usenet, a one-to-many communication network with some similarities.

SMTP became widely used in the early 1980s. At the time, it was a complement to Unix to Unix Copy Program (UUCP) mail, which was better suited for handling email transfers between machines that were intermittently connected. SMTP, on the other hand, works best when both the sending and receiving machines are connected to the network all the time. Both use a store and forward mechanism and are examples of push technology. Though Usenet's newsgroups are still propagated with UUCP between servers, UUCP as a mail transport has virtually disappeared along with the "bang paths" it used as message routing headers.

Sendmail, released with 4.1cBSD, right after , was one of the first mail transfer agents to implement SMTP. Over time, as BSD Unix became the most popular operating system on the Internet, sendmail became the most common MTA (mail transfer agent). Some other popular SMTP server programs include Postfix, qmail, Novell GroupWise, Exim, Novell NetMail, Microsoft Exchange Server and Oracle Communications Messaging Server.

Message submission () and SMTP-AUTH () were introduced in 1998 and 1999, both describing new trends in email delivery. Originally, SMTP servers were typically internal to an organization, receiving mail for the organization "from the outside", and relaying messages from the organization "to the outside". But as time went on, SMTP servers (mail transfer agents), in practice, were expanding their roles to become message submission agents for Mail user agents, some of which were now relaying mail "from the outside" of an organization. (e.g. a company executive wishes to send email while on a trip using the corporate SMTP server.) This issue, a consequence of the rapid expansion and popularity of the World Wide Web, meant that SMTP had to include specific rules and methods for relaying mail and authenticating users to prevent abuses such as relaying of unsolicited email (spam). Work on message submission () was originally started because popular mail servers would often rewrite mail in an attempt to fix problems in it, for example, adding a domain name to an unqualified address. This behavior is helpful when the message being fixed is an initial submission, but dangerous and harmful when the message originated elsewhere and is being relayed. Cleanly separating mail into submission and relay was seen as a way to permit and encourage rewriting submissions while prohibiting rewriting relay. As spam became more prevalent, it was also seen as a way to provide authorization for mail being sent out from an organization, as well as traceability. This separation of relay and submission quickly became a foundation for modern email security practices.

As this protocol started out purely ASCII text-based, it did not deal well with binary files, or characters in many non-English languages. Standards such as Multipurpose Internet Mail Extensions (MIME) were developed to encode binary files for transfer through SMTP. Mail transfer agents (MTAs) developed after Sendmail also tended to be implemented 8-bit-clean, so that the alternate "just send eight" strategy could be used to transmit arbitrary text data (in any 8-bit ASCII-like character encoding) via SMTP. Mojibake was still a problem due to differing character set mappings between vendors, although the email addresses themselves still allowed only ASCII. 8-bit-clean MTAs today tend to support the 8BITMIME extension, permitting binary files to be transmitted almost as easily as plain text. Recently the SMTPUTF8 extension was created to support UTF-8 text, allowing international content and addresses in non-Latin scripts like Cyrillic or Chinese.

Many people contributed to the core SMTP specifications, among them Jon Postel, Eric Allman, Dave Crocker, Ned Freed, Randall Gellens, John Klensin, and Keith Moore.

Email is submitted by a mail client (mail user agent, MUA) to a mail server (mail submission agent, MSA) using SMTP on TCP port 587. Most mailbox providers still allow submission on traditional port 25. The MSA delivers the mail to its mail transfer agent (mail transfer agent, MTA). Often, these two agents are instances of the same software launched with different options on the same machine. Local processing can be done either on a single machine, or split among multiple machines; mail agent processes on one machine can share files, but if processing is on multiple machines, they transfer messages between each other using SMTP, where each machine is configured to use the next machine as a smart host. Each process is an MTA (an SMTP server) in its own right.

The boundary MTA uses the Domain name system (DNS) to look up the mail exchanger record (MX record) for the recipient's domain (the part of the email address on the right of @). The MX record contains the name of the target host. Based on the target host and other factors, the MTA selects an exchange server: see the article MX record. The MTA connects to the exchange server as an SMTP client.

Message transfer can occur in a single connection between two MTAs, or in a series of hops through intermediary systems. A receiving SMTP server may be the ultimate destination, an intermediate "relay" (that is, it stores and forwards the message) or a "gateway" (that is, it may forward the message using some protocol other than SMTP). Each hop is a formal handoff of responsibility for the message, whereby the receiving server must either deliver the message or properly report the failure to do so.

Once the final hop accepts the incoming message, it hands it to a mail delivery agent (MDA) for local delivery. An MDA saves messages in the relevant mailbox format. As with sending, this reception can be done using one or multiple computers, but in the diagram above the MDA is depicted as one box near the mail exchanger box. An MDA may deliver messages directly to storage, or forward them over a network using SMTP or other protocol such as Local Mail Transfer Protocol (LMTP), a derivative of SMTP designed for this purpose.

Once delivered to the local mail server, the mail is stored for batch retrieval by authenticated mail clients (MUAs). Mail is retrieved by end-user applications, called email clients, using Internet Message Access Protocol (IMAP), a protocol that both facilitates access to mail and manages stored mail, or the Post Office Protocol (POP) which typically uses the traditional mbox mail file format or a proprietary system such as Microsoft Exchange/Outlook or Lotus Notes/Domino. Webmail clients may use either method, but the retrieval protocol is often not a formal standard.

SMTP defines message "transport", not the message "content". Thus, it defines the mail "envelope" and its parameters, such as the envelope sender, but not the header (except "trace information") nor the body of the message itself. STD 10 and define SMTP (the envelope), while STD 11 and define the message (header and body), formally referred to as the Internet Message Format.

SMTP is a connection-oriented, text-based protocol in which a mail sender communicates with a mail receiver by issuing command strings and supplying necessary data over a reliable ordered data stream channel, typically a Transmission Control Protocol (TCP) connection. An "SMTP session" consists of commands originated by an SMTP client (the initiating agent, sender, or transmitter) and corresponding responses from the SMTP server (the listening agent, or receiver) so that the session is opened, and session parameters are exchanged. A session may include zero or more SMTP transactions. An "SMTP transaction" consists of three command/reply sequences:


Besides the intermediate reply for DATA, each server's reply can be either positive (2xx reply codes) or negative. Negative replies can be permanent (5xx codes) or transient (4xx codes). A reject is a permanent failure and the client should send a bounce message to the server it received it from. A drop is a positive response followed by message discard rather than delivery.

The initiating host, the SMTP client, can be either an end-user's email client, functionally identified as a mail user agent (MUA), or a relay server's mail transfer agent (MTA), that is an SMTP server acting as an SMTP client, in the relevant session, in order to relay mail. Fully capable SMTP servers maintain queues of messages for retrying message transmissions that resulted in transient failures.

A MUA knows the "outgoing mail" SMTP server from its configuration. A relay server typically determines which server to connect to by looking up the MX (Mail eXchange) DNS resource record for each recipient's domain name. If no MX record is found, a conformant relaying server (not all are) instead looks up the A record. Relay servers can also be configured to use a smart host. A relay server initiates a TCP connection to the server on the "well-known port" for SMTP: port 25, or for connecting to an MSA, port 587. The main difference between an MTA and an MSA is that connecting to an MSA requires SMTP Authentication.

SMTP is a delivery protocol only. In normal use, mail is "pushed" to a destination mail server (or next-hop mail server) as it arrives. Mail is routed based on the destination server, not the individual user(s) to which it is addressed. Other protocols, such as the Post Office Protocol (POP) and the Internet Message Access Protocol (IMAP) are specifically designed for use by individual users retrieving messages and managing mail boxes. To permit an intermittently-connected mail server to "pull" messages from a remote server on demand, SMTP has a feature to initiate mail queue processing on a remote server (see Remote Message Queue Starting below). POP and IMAP are unsuitable protocols for relaying mail by intermittently-connected machines; they are designed to operate after final delivery, when information critical to the correct operation of mail relay (the "mail envelope") has been removed.

Remote Message Queue Starting is a feature of SMTP that permits a remote host to start processing of the mail queue on a server so it may receive messages destined to it by sending the TURN command. This feature however was deemed insecure and was extended in with the ETRN command which operates more securely using an authentication method based on Domain Name System information.

On-Demand Mail Relay (ODMR) is an SMTP extension standardized in that allows an intermittently-connected SMTP server to receive email queued for it when it is connected.

Users whose native script is not Latin based, or who use diacritic not in the ASCII character set have had difficulty with the Latin email address requirement. was created to solve that problem, providing internationalization features for SMTP, the SMTPUTF8 extension and support for multi-byte and non-ASCII characters in email addresses, such as those with diacritics and other language characters such as Greek and Chinese. 

Current support is limited, but there is strong interest in broad adoption of and the related RFCs in countries like China that have a large user base where Latin (ASCII) is a foreign script.

An email client needs to know the IP address of its initial SMTP server and this has to be given as part of its configuration (usually given as a DNS name). This server will deliver outgoing messages on behalf of the user.

Server administrators need to impose some control on which clients can use the server. This enables them to deal with abuse, for example spam. Two solutions have been in common use:

Under this system, an ISP's SMTP server will not allow access by users who are outside the ISP's network. More precisely, the server may only allow access to users with an IP address provided by the ISP, which is equivalent to requiring that they are connected to the Internet using that same ISP. A mobile user may often be on a network other than that of their normal ISP, and will then find that sending email fails because the configured SMTP server choice is no longer accessible.

This system has several variations. For example, an organisation's SMTP server may only provide service to users on the same network, enforcing this by firewalling to block access by users on the wider Internet. Or the server may perform range checks on the client's IP address. These methods were typically used by corporations and institutions such as universities which provided an SMTP server for outbound mail only for use internally within the organisation. However, most of these bodies now use client authentication methods, as described below.

Where a user is mobile, and may use different ISPs to connect to the internet, this kind of usage restriction is onerous, and altering the configured outbound email SMTP server address is impractical. It is highly desirable to be able to use email client configuration information that does not need to change.

Modern SMTP servers typically require authentication of clients by credentials before allowing access, rather than restricting access by location as described earlier. This more flexible system is friendly to mobile users and allows them to have a fixed choice of configured outbound SMTP server. SMTP Authentication, often abbreviated SMTP AUTH, is an extension of the SMTP in order to log in using an authentication mechanism.

A server that is accessible on the wider Internet and does not enforce these kinds of access restrictions is known as an open relay. This is now generally considered a bad practice worthy of blacklisting.

Communication between mail servers generally always uses the standard TCP port 25 designated for SMTP.

Mail "clients" however generally don't use this, instead using specific "submission" ports. Mail services generally accept email submission from clients on one of:

Port 2525 and others may be used by some individual providers, but have never been officially supported.

Most Internet service providers now block all outgoing port 25 traffic from their customers as an anti-spam measure.
For the same reason, businesses will typically configure their firewall to only allow outgoing port 25 traffic from their designated mail servers.

A typical example of sending a message via SMTP to two mailboxes ("alice" and "theboss") located in the same mail domain ("example.com" or "localhost.com") is reproduced in the following session exchange. (In this example, the conversation parts are prefixed with "S:" and "C:", for "server" and "client", respectively; these labels are not part of the exchange.)

After the message sender (SMTP client) establishes a reliable communications channel to the message receiver (SMTP server), the session is opened with a greeting by the server, usually containing its fully qualified domain name (FQDN), in this case "smtp.example.com". The client initiates its dialog by responding with a codice_1 command identifying itself in the command's parameter with its FQDN (or an address literal if none is available).

The client notifies the receiver of the originating email address of the message in a codice_2 command. This is also the return or bounce address in case the message cannot be delivered. In this example the email message is sent to two mailboxes on the same SMTP server: one for each recipient listed in the To and Cc header fields. The corresponding SMTP command is codice_3. Each successful reception and execution of a command is acknowledged by the server with a result code and response message (e.g., 250 Ok).

The transmission of the body of the mail message is initiated with a codice_4 command after which it is transmitted verbatim line by line and is terminated with an end-of-data sequence. This sequence consists of a new-line (<CR><LF>), a single full stop (period), followed by another new-line. Since a message body can contain a line with just a period as part of the text, the client sends "two" periods every time a line starts with a period; correspondingly, the server replaces every sequence of two periods at the beginning of a line with a single one. Such escaping method is called "dot-stuffing".

The server's positive reply to the end-of-data, as exemplified, implies that the server has taken the responsibility of delivering the message. A message can be doubled if there is a communication failure at this time, e.g. due to a power shortage: Until the sender has received that 250 reply, it must assume the message was not delivered. On the other hand, after the receiver has decided to accept the message, it must assume the message has been delivered to it. Thus, during this time span, both agents have active copies of the message that they will try to deliver. The probability that a communication failure occurs exactly at this step is directly proportional to the amount of filtering that the server performs on the message body, most often for anti-spam purposes. The limiting timeout is specified to be 10 minutes.

The codice_5 command ends the session. If the email has other recipients located elsewhere, the client would codice_5 and connect to an appropriate SMTP server for subsequent recipients after the current destination(s) had been queued. The information that the client sends in the codice_1 and codice_2 commands are added (not seen in example code) as additional header fields to the message by the receiving server. It adds a codice_9 and codice_10 header field, respectively.

Some clients are implemented to close the connection after the message is accepted (codice_11), so the last two lines may actually be omitted. This causes an error on the server when trying to send the codice_12 reply.

Clients learn what options a server supports, by using the codice_13 greeting, as exemplified below, instead of the original codice_1 (example above). Clients fall back to codice_1 only if the server does not support SMTP extensions.

Modern clients may use the ESMTP extension keyword codice_16 to query the server for the maximum message size that will be accepted. Older clients and servers may try to transfer excessively sized messages that will be rejected after consuming network resources, including connect time to network links that is paid by the minute.

Users can manually determine in advance the maximum size accepted by ESMTP servers. The client replaces the codice_1 command with the codice_13 command.
Thus "smtp2.example.com" declares that it will accept a fixed maximum message size no larger than 14,680,064 octets (8-bit bytes). Depending on the server's actual resource usage, it may be currently unable to accept a message this large.

In the simplest case, an ESMTP server will declare a maximum codice_16 immediately after receiving an codice_13. According to , however, the numeric parameter to the codice_16 extension in the codice_13 response is optional. Clients may instead, when issuing a codice_2 command, include a numeric estimate of the size of the message they are transferring, so that the server can refuse receipt of overly-large messages.

The original design of SMTP had no facility to authenticate senders, or check that servers were authorized to send on their behalf, with the result that email spoofing is possible, and commonly used in email spam and phishing.

Occasional proposals are made to modify SMTP extensively or replace it completely. One example of this is Internet Mail 2000, but neither it, nor any other has made much headway in the face of the network effect of the huge installed base of classic SMTP. Instead, mail servers now use a range of techniques, including DomainKeys, DomainKeys Identified Mail, Sender Policy Framework and DMARC, DNSBLs and greylisting to reject or quarantine suspicious emails.





</doc>
<doc id="27676" url="https://en.wikipedia.org/wiki?curid=27676" title="Shuttlecock">
Shuttlecock

A shuttlecock (also called a bird or birdie) is a high-drag projectile used in the sport of badminton. It has an open conical shape formed by feathers (or a synthetic alternative) embedded into a rounded cork (or rubber) base. The shuttlecock's shape makes it extremely aerodynamically stable. Regardless of initial orientation, it will turn to fly cork first, and remain in the cork-first orientation.

The name 'shuttlecock' originates in Victorian times, when badminton first became popular. It is frequently shortened to shuttle. The "shuttle" part of the name was probably derived from its back-and-forth motion during the game, resembling the shuttle of a loom, while the "cock" part of the name was probably derived from the resemblance of the feathers to those on a rooster.

A shuttlecock weighs around . It has 16 feathers with each feather in length. The diameter of the cork is and the diameter of the circle that the feathers make is around .

A shuttlecock is formed from 16 or so overlapping feathers, usually goose or duck, embedded into a rounded cork base. The cork is covered with thin leather. To ensure satisfactory flight properties, it is considered preferable to use feathers from right or left wings only in each shuttlecock, and not mix feathers from different wings, as the feathers from different wings are shaped differently. 

The feathers are brittle; shuttlecocks break easily and often need to be replaced several times during a game. For this reason, synthetic shuttlecocks have been developed that replace the feathers with a plastic skirt. Players often refer to synthetic shuttlecocks as "plastics" and feathered shuttlecocks as "feathers".

Feather shuttles need to be properly humidified for at least 4 hours prior to play in order to fly the correct distance at the proper speed and to last longer. Properly humidified feathers flex during play, enhancing the shuttle's speed change and durability. Dry feathers are brittle and break easily, causing the shuttle to wobble. Saturated feathers are 'mushy', making the feather cone narrow too much when strongly hit, which causes the shuttle to fly overly far and fast. Humidification boxes are often used, but a simple moist sponge inserted in the feather end of the closed shuttle tube will work nicely. Water should never touch the cork of the shuttle. Shuttles are tested prior to play to make sure they fly true and at the proper speed, and cover the proper distance. Different weights of shuttles are used to compensate for local atmospheric conditions. Both humidity and height above sea level affect shuttle flight. World Badminton Federation Rules say the shuttle should reach the far doubles service line plus or minus half the width of the tram. According to manufacturers (!) proper shuttle will generally travel from the back line of the court to just short of the long doubles service line on the opposite side of the net, with a full underhand hit from an average player.

The cost of good quality feathers is similar to that of good quality plastics, but plastics are far more durable, typically lasting many matches without any impairment to their flight. Shuttles are easily damaged and should be replaced every three or four games, and sooner if they are damaged and do not fly straight. This interferes with the game, as the impairment on the flight of the shuttle may misdirect the direction of the shuttlecock.

Most experienced and skillful players greatly prefer feathers, and serious tournaments or leagues are always played using feather shuttlecocks of the highest quality. Experienced players generally prefer the "feel" of feathered shuttlecocks and assert that they are able to control the flight of feathers better than that of plastics. In Asia, where feather shuttlecocks are more affordable than in Europe and North America, plastic shuttlecocks are hardly used at all.

The playing characteristics of plastics and feathers are substantially different. Plastics fly more slowly on initial impact, but slow down less towards the end of their flight. While feathers tend to drop straight down on a clear shot, plastics never quite return to a straight drop, falling more on a diagonal. Feather shuttles may come off the strings at speeds in excess of 320 km/h (200 mph) but slow down faster as they drop. For this reason, the feather shuttle makes the game seem faster, but also allows more time to play strokes. Because feather shuttles fly more quickly off the racquet face they also tend to cause less shoulder impact and injury. Shuttle game is a physically rigorous game needing to run, bend quickly and played indoor as either a singles or as doubles game.


5. <nowiki>http://www.thefreedictionary.com/shuttlecock</nowiki> - shuttlecock: badminton equipment consisting of a ball of cork or rubber with a crown of feathers.


</doc>
<doc id="27679" url="https://en.wikipedia.org/wiki?curid=27679" title="Soldering iron">
Soldering iron

A soldering iron is a hand tool used in soldering. It supplies heat to melt solder so that it can flow into the joint between two workpieces.

A soldering iron is composed of a heated metal tip and an insulated handle. Heating is often achieved electrically, by passing an electric current (supplied through an electrical cord or battery cables) through a resistive heating element. Cordless irons can be heated by combustion of gas stored in a small tank, often using a catalytic heater rather than a flame. Simple irons less commonly used today than in the past were simply a large copper bit on a handle, heated in a flame.

Soldering irons are most often used for installation, repairs, and limited production work in electronics assembly. High-volume production lines use other soldering methods. Large irons may be used for soldering joints in sheet metal objects. Less common uses include pyrography (burning designs into wood) and plastic welding.

For electrical and electronics work, a low-power iron, a power rating between 15 and 35 watts, is used. Higher ratings are available, but do not run at higher temperature; instead there is more heat available for making soldered connections to things with large thermal capacity, for example, a metal chassis. Some irons are temperature-controlled, running at a fixed temperature in the same way as a soldering station, with higher power available for joints with large heat capacity. Simple irons run at an uncontrolled temperature determined by thermal equilibrium; when heating something large their temperature drops a little, possibly too much to melt solder.

Small irons heated by a battery, or by combustion of a gas such as butane in a small self-contained tank, can be used when electricity is unavailable or cordless operation is required. The operating temperature of these irons is not regulated directly; gas irons may change power by adjusting gas flow. Gas-powered irons may have interchangeable tips including different size soldering tips, hot knife for cutting plastics, miniature blow-torch with a hot flame, and small hot air blower for such applications as shrinking heat shrink tubing.

Simple soldering irons reach a temperature determined by thermal equilibrium, dependent upon power input and cooling by the environment and the materials it comes into contact with. The iron temperature will drop when in contact with a large mass of metal such as a chassis; a small iron will lose too much temperature to solder a large connection. More advanced irons for use in electronics have a mechanism with a temperature sensor and method of temperature control to keep the tip temperature steady; more power is available if a connection is large. Temperature-controlled irons may be free-standing, or may comprise a head with heating element and tip, controlled by a base called a soldering station, with control circuitry and temperature adjustment and sometimes display.

A variety of means are used to control temperature. The simplest of these is a variable power control, much like a light dimmer, which changes the equilibrium temperature of the iron without automatically measuring or regulating the temperature. Another type of system uses a thermostat, often inside the iron's tip, which automatically switches power on and off to the element. A thermal sensor such as a thermocouple may be used in conjunction with circuitry to monitor the temperature of the tip and adjust power delivered to the heating element to maintain a desired temperature.

Another approach is to use magnetized soldering tips which lose their magnetic properties at a specific temperature, the Curie point. As long as the tip is magnetic, it closes a switch to supply power to the heating element. When it exceeds the design temperature it opens the contacts, cooling until the temperature drops enough to restore magnetisation. More complex Curie-point irons circulate a high-frequency AC current through the tip, using magnetic physics to direct heating only where the surface of the tip drops below the Curie point.

A soldering station, invariably temperature-controlled, consists of an electrical power supply, control circuitry with provision for user adjustment of temperature and display, and a soldering iron or soldering head with a tip temperature sensor. The station will normally have a stand for the hot iron when not in use, and a wet sponge for cleaning. It is most commonly used for soldering electronic components. Other functions may be combined; for example a rework station, mainly for surface-mount components may have a hot air gun, vacuum pickup tool, and a soldering head; a desoldering station will have a desoldering head with vacuum pump for desoldering through-hole components, and a soldering iron head.

For soldering and desoldering small surface-mount components with two terminals, such as some links, resistors, capacitors, and diodes, soldering tweezers can be used; they can be either free-standing or controlled from a soldering station. The tweezers have two heated tips mounted on arms whose separation can be manually varied by squeezing gently against spring force, like simple tweezers; the tips are applied to the two ends of the component. The main purpose of the soldering tweezers is to melt solder in the correct place; components are usually moved by simple tweezers or vacuum pickup.

A hot knife is a form of soldering iron equipped with a double-edged blade that is situated on a heating element. These tools can reach temperatures of up to 1,000 degrees Fahrenheit (538 degrees Celsius) allowing for cuts of fabric and foam materials without worry of fraying or beading. Hot knives can be utilized in automotive, marine, and carpeting applications, as well as other industrial and personal uses.

A soldering iron stand keeps the iron away from flammable materials, and often also comes with a cellulose sponge and flux pot for cleaning the tip. Some soldering irons for continuous and professional use come as part of a "soldering station," which allows the exact temperature of the tip to be adjusted, kept constant, and sometimes displayed.

Most soldering irons for electronics have interchangeable tips, also known as "bits", that vary in size and shape for different types of work. Pyramid tips with a triangular flat face and chisel tips with a wide flat face are useful for soldering sheet metal. Fine conical or tapered chisel tips are typically used for electronics work. Tips may be straight or have a bend. Concave or wicking tips with a chisel face with a concave well in the flat face to hold a small amount of solder are available. Tip selection depends upon the type of work and access to the joint; soldering of 0.5mm pitch surface-mount ICs, for example, is quite different from soldering a through-hole connection to a large area. A concave tip well is said to help prevent bridging of closely spaced leads; different shapes are recommended to correct bridging that has occurred. Due to patent restrictions not all manufacturers offer concave tips everywhere; in particular there are restrictions in the USA.

Older and very cheap irons typically use a bare copper tip, which is shaped with a file or sandpaper. This dissolves gradually into the solder, suffering pitting and erosion of the shape. Copper tips are sometimes filed when worn down. Iron-plated copper tips have become increasingly popular since the 1980s. Because iron is not readily dissolved by molten solder, the plated tip is more durable than a bare copper one, though it will eventually wear out and need replacing. This is especially important when working at the higher temperatures needed for modern lead-free solders. Solid iron and steel tips are seldom used because they store less heat, and rusting can break the heating element.

When the iron tip oxidises and burnt flux accumulates on it, solder no longer wets the tip, impeding heat transfer and making soldering difficult or impossible; tips must be periodically cleaned in use. Such problems happen with all kinds of solder, but are much more severe with the lead-free solders which have become widespread in electronics work, which require higher temperatures than solders containing lead. Exposed iron plating oxidises; if the tip is kept tinned with molten solder oxidation is inhibited. A clean unoxidised tip is tinned by applying a little solder and flux.

A wet small sponge, often supplied with soldering equipment, can be used to wipe the tip. For lead-free solder a slightly more aggressive cleaning, with brass shavings, can be used. Soldering flux will help to remove oxide; the more active the flux the better the cleaning, although acidic flux used on circuit boards that is not carefully cleaned off will cause corrosion. A tip which is cleaned but not retinned is susceptible to oxidation.

Soldering iron tips are made of a copper core plated with iron. The copper is used for heat transfer and the iron plating is used for durability. Copper is very easily corroded, eating away the tip, particularly in lead-free work; iron is not. Cleaning tips requires the removal of oxide without damaging the iron plating and exposing the copper to rapid corrosion. The use of solder already containing a small amount of copper can slow corrosion of copper tips.

In cases of severe oxidation not removable by gentler methods, abrasion with something hard enough to remove oxide but not so hard as to scratch the iron plating can be used. A brass wire scourer, brush, or wheel on a bench grinder, can be used with care. Sandpaper and other tools may be used but are likely to damage the plating.



</doc>
<doc id="27680" url="https://en.wikipedia.org/wiki?curid=27680" title="Supernova">
Supernova

A supernova ( plural: supernovae or supernovas, abbreviations: SN and SNe) is a transient astronomical event that occurs during the last stellar evolutionary stages of a star's life, either a massive star or a white dwarf, whose destruction is marked by one final, titanic explosion. This causes the sudden appearance of a "new" bright star, before slowly fading from sight over several weeks or months or years.

Supernovae are more energetic than novae. In Latin, "nova" means "new", referring astronomically to what appears to be a temporary new bright star. Adding the prefix "super-" distinguishes supernovae from ordinary novae, which are far less luminous. The word "supernova" was coined by Walter Baade and Fritz Zwicky in 1931.

Only three Milky Way naked-eye supernova events have been observed during the last thousand years, though many have been seen in other galaxies using telescopes. The most recent directly observed supernova in the Milky Way was Kepler's Supernova in 1604, but two more recent supernova remnants have also been found. Statistical observations of supernovae in other galaxies suggest they occur on average about three times every century in the Milky Way, and that any galactic supernova would almost certainly be observable with modern astronomical telescopes.

Supernovae may expel much, if not all, of the material away from a star at velocities up to or 10% of the speed of light. This drives an expanding and fast-moving shock wave into the surrounding interstellar medium, and in turn, sweeping up an expanding shell of gas and dust, which is observed as a supernova remnant. Supernovae create, fuse and eject the bulk of the chemical elements produced by nucleosynthesis. Supernovae play a significant role in enriching the interstellar medium with the heavier atomic mass chemical elements. Furthermore, the expanding shock waves from supernovae can trigger the formation of new stars. Supernova remnants are expected to accelerate a large fraction of galactic primary cosmic rays, but direct evidence for cosmic ray production was found only in a few of them so far. They are also potentially strong galactic sources of gravitational waves.

Theoretical studies indicate that most supernovae are triggered by one of two basic mechanisms: the sudden re-ignition of nuclear fusion in a degenerate star or the sudden gravitational collapse of a massive star's core. In the first instance, a degenerate white dwarf may accumulate sufficient material from a binary companion, either through accretion or via a merger, to raise its core temperature enough to trigger runaway nuclear fusion, completely disrupting the star. In the second case, the core of a massive star may undergo sudden gravitational collapse, releasing gravitational potential energy as a supernova. While some observed supernovae are more complex than these two simplified theories, the astrophysical collapse mechanics have been established and accepted by most astronomers for some time.

Due to the wide range of astrophysical consequences of these events, astronomers now deem supernova research, across the fields of stellar and galactic evolution, as an especially important area for investigation.

The earliest recorded supernova, SN 185, was viewed by Chinese astronomers in 185 AD. The brightest recorded supernova was SN 1006, which occurred in 1006 AD and was described in detail by Chinese and Islamic astronomers. The widely observed supernova SN 1054 produced the Crab Nebula. Supernovae SN 1572 and SN 1604, the latest to be observed with the naked eye in the Milky Way galaxy, had notable effects on the development of astronomy in Europe because they were used to argue against the Aristotelian idea that the universe beyond the Moon and planets was static and unchanging. Johannes Kepler began observing SN 1604 at its peak on October 17, 1604, and continued to make estimates of its brightness until it faded from naked eye view a year later. It was the second supernova to be observed in a generation (after SN 1572 seen by Tycho Brahe in Cassiopeia).

There is some evidence that the youngest galactic supernova, G1.9+0.3, occurred in the late 19th century, considerably more recently than Cassiopeia A from around 1680. Neither supernova was noted at the time. In the case of G1.9+0.3, high extinction along the plane of the galaxy could have dimmed the event sufficiently to go unnoticed. The situation for Cassiopeia A is less clear. Infrared light echos have been detected showing that it was a type IIb supernova and was not in a region of especially high extinction.

Before the development of the telescope, there have only been five supernovae seen in the last millennium. Compared to a star's entire history, the visual appearance of a galactic supernova is very brief, perhaps spanning several months, so that the chances of observing one is roughly once in a lifetime. Only a tiny fraction of the 100 billion stars in a typical galaxy have the capacity to become a supernova, restricted to either having large enough mass or under extraordinarily rare kinds of binary star in configurations containing white dwarf stars.

However, observation and discovery of extragalactic supernovae are now far more common; that started with SN 1885A in the Andromeda galaxy. Today, amateur and professional astronomers are finding several hundreds every year, some when near maximum brightness or others unrecognised on old astronomical photographs or plates. American astronomers Rudolph Minkowski and Fritz Zwicky developed the modern supernova classification scheme beginning in 1941. During the 1960s, astronomers found that the maximum intensities of supernovae could be used as standard candles, hence indicators of astronomical distances. Some of the most distant supernovae recently observed appeared dimmer than expected. This supports the view that the expansion of the universe is accelerating. Techniques were developed for reconstructing supernovae events that have no written records of being observed. The date of the Cassiopeia A supernova event was determined from light echoes off nebulae, while the age of supernova remnant RX J0852.0-4622 was estimated from temperature measurements and the gamma ray emissions from the radioactive decay of titanium-44.

The most luminous supernova ever recorded is ASASSN-15lh. It was first detected in June 2015 and peaked at , which is twice the bolometric luminosity of any other known supernova. However, the nature of this supernova continues to be debated and several alternative explanations have been suggested, e.g. tidal disruption of a star by a black hole.

Among the earliest detected since time of detonation, and for which the earliest spectra have been obtained (beginning at 6 hours after the actual explosion), is the Type II SN 2013fs (iPTF13dqy) which was recorded 3 hours after the supernova event on 6 October 2013 by the Intermediate Palomar Transient Factory (iPTF). The star is located in a spiral galaxy named NGC 7610, 160 million light years away in the constellation of Pegasus.

On 20 September 2016, amateur astronomer Victor Buso from Rosario, Argentina was testing out his new 16 inch telescope. When taking several twenty second exposures of galaxy NGC 613, Buso chanced upon a supernova that had just become visible on earth. After examining the images he contacted the Instituto de Astrofísica de La Plata. "It was the first time anyone had ever captured the initial moments of the “shock breakout” from an optical supernova, one not associated with a gamma-ray or X-ray burst." The odds of capturing such an event were put between one in ten million to one in a hundred million, according to astronomer Melina Bersten from the Instituto de Astrofísica.

The supernova Buso observed was a Type IIb made by a star twenty times the mass of the sun. Astronomer Alex Filippenko from the University of California remarked that professional astronomers had been searching for such an event for a long time. He stated: "Observations of stars in the first moments they begin exploding provide information that cannot be directly obtained in any other way."

Early work on what was originally believed to be simply a new category of novae was performed during the 1930s by two astronomers named Walter Baade and Fritz Zwicky at Mount Wilson Observatory. The name "super-novae" was first used during 1931 lectures held at Caltech by Baade and Zwicky, then used publicly in 1933 at a meeting of the American Physical Society. By 1938, the hyphen had been lost and the modern name was in use. Because supernovae are relatively rare events within a galaxy, occurring about three times a century in the Milky Way, obtaining a good sample of supernovae to study requires regular monitoring of many galaxies.

Supernovae in other galaxies cannot be predicted with any meaningful accuracy. Normally, when they are discovered, they are already in progress. Most scientific interest in supernovae—as standard candles for measuring distance, for example—require an observation of their peak luminosity. It is therefore important to discover them well before they reach their maximum. Amateur astronomers, who greatly outnumber professional astronomers, have played an important role in finding supernovae, typically by looking at some of the closer galaxies through an optical telescope and comparing them to earlier photographs.

Toward the end of the 20th century astronomers increasingly turned to computer-controlled telescopes and CCDs for hunting supernovae. While such systems are popular with amateurs, there are also professional installations such as the Katzman Automatic Imaging Telescope. Recently the Supernova Early Warning System (SNEWS) project has begun using a network of neutrino detectors to give early warning of a supernova in the Milky Way galaxy. Neutrinos are particles that are produced in great quantities by a supernova, and they are not significantly absorbed by the interstellar gas and dust of the galactic disk.

Supernova searches fall into two classes: those focused on relatively nearby events and those looking farther away. Because of the expansion of the universe, the distance to a remote object with a known emission spectrum can be estimated by measuring its Doppler shift (or redshift); on average, more-distant objects recede with greater velocity than those nearby, and so have a higher redshift. Thus the search is split between high redshift and low redshift, with the boundary falling around a redshift range of "z"=0.1–0.3—where "z" is a dimensionless measure of the spectrum's frequency shift.

High redshift searches for supernovae usually involve the observation of supernova light curves. These are useful for standard or calibrated candles to generate Hubble diagrams and make cosmological predictions. Supernova spectroscopy, used to study the physics and environments of supernovae, is more practical at low than at high redshift. Low redshift observations also anchor the low-distance end of the Hubble curve, which is a plot of distance versus redshift for visible galaxies. (See also Hubble's law).

Supernova discoveries are reported to the International Astronomical Union's Central Bureau for Astronomical Telegrams, which sends out a circular with the name it assigns to that supernova. The name is the marker "SN" followed by the year of discovery, suffixed with a one or two-letter designation. The first 26 supernovae of the year are designated with a capital letter from "A" to "Z". Afterward pairs of lower-case letters are used: "aa", "ab", and so on. Hence, for example, "SN 2003C" designates the third supernova reported in the year 2003. The last supernova of 2005 was SN 2005nc, indicating that it was the 367th supernova found in 2005. Since 2000, professional and amateur astronomers have been finding several hundreds of supernovae each year (572 in 2007, 261 in 2008, 390 in 2009; 231 in 2013).

Historical supernovae are known simply by the year they occurred: SN 185, SN 1006, SN 1054, SN 1572 (called "Tycho's Nova") and SN 1604 ("Kepler's Star"). Since 1885 the additional letter notation has been used, even if there was only one supernova discovered that year (e.g. SN 1885A, SN 1907A, etc.) — this last happened with SN 1947A. "SN", for SuperNova, is a standard prefix. Until 1987, two-letter designations were rarely needed; since 1988, however, they have been needed every year.

As part of the attempt to understand supernovae, astronomers have classified them according to their light curves and the absorption lines of different chemical elements that appear in their spectra. The first element for division is the presence or absence of a line caused by hydrogen. If a supernova's spectrum contains lines of hydrogen (known as the Balmer series in the visual portion of the spectrum) it is classified "Type II"; otherwise it is "Type I". In each of these two types there are subdivisions according to the presence of lines from other elements or the shape of the light curve (a graph of the supernova's apparent magnitude as a function of time).

Type I supernovae are subdivided on the basis of their spectra, with Type Ia showing a strong ionised silicon absorption line. Type I supernovae without this strong line are classified as Type Ib and Ic, with Type Ib showing strong neutral helium lines and Type Ic lacking them. The light curves are all similar, although Type Ia are generally brighter at peak luminosity, but the light curve is not important for classification of Type I supernovae.

A small number of Type Ia supernovae exhibit unusual features such as non-standard luminosity or broadened light curves, and these are typically classified by referring to the earliest example showing similar features. For example, the sub-luminous SN 2008ha is often referred to as SN 2002cx-like or class Ia-2002cx.

A small proportion of type Ic supernovae show highly broadened and blended emission lines which are taken to indicate very high expansion velocities for the ejecta. These have been classified as type Ic-BL or Ic-bl.

The supernovae of Type II can also be sub-divided based on their spectra. While most Type II supernovae show very broad emission lines which indicate expansion velocities of many thousands of kilometres per second, some, such as SN 2005gl, have relatively narrow features in their spectra. These are called Type IIn, where the 'n' stands for 'narrow'.

A few supernovae, such as SN 1987K and SN 1993J, appear to change types: they show lines of hydrogen at early times, but, over a period of weeks to months, become dominated by lines of helium. The term "Type IIb" is used to describe the combination of features normally associated with Types II and Ib.

Type II supernovae with normal spectra dominated by broad hydrogen lines that remain for the life of the decline are classified on the basis of their light curves. The most common type shows a distinctive "plateau" in the light curve shortly after peak brightness where the visual luminosity stays relatively constant for several months before the decline resumes. These are called Type II-P referring to the plateau. Less common are Type II-L supernovae that lack a distinct plateau. The "L" signifies "linear" although the light curve is not actually a straight line.

Supernovae that do not fit into the normal classifications are designated peculiar, or 'pec'.

Fritz Zwicky defined additional supernovae types, although based on a very few examples that did not cleanly fit the parameters for a Type I or Type II supernova. SN 1961i in NGC 4303 was the prototype and only member of the Type III supernova class, noted for its broad light curve maximum and broad hydrogen Balmer lines that were slow to develop in the spectrum. SN 1961f in NGC 3003 was the prototype and only member of the Type IV class, with a light curve similar to a Type II-P supernova, with hydrogen absorption lines but weak hydrogen emission lines. The Type V class was coined for SN 1961V in NGC 1058, an unusual faint supernova or supernova impostor with a slow rise to brightness, a maximum lasting many months, and an unusual emission spectrum. The similarity of SN 1961V to the Eta Carinae Great Outburst was noted. Supernovae in M101 (1909) and M83 (1923 and 1957) were also suggested as possible Type IV or Type V supernovae.

These types would now all be treated as peculiar Type II supernovae, of which many more examples have been discovered, although it is still debated whether SN 1961V was a true supernova following an LBV outburst or an impostor.

The type codes, described above given to supernovae, are "taxonomic" in nature: the type number describes the light observed from the supernova, not necessarily its cause. For example, Type Ia supernovae are produced by runaway fusion ignited on degenerate white dwarf progenitors while the spectrally similar Type Ib/c are produced from massive Wolf–Rayet progenitors by core collapse. The following summarizes what is currently believed to be the most plausible explanations for supernovae.

A white dwarf star may accumulate sufficient material from a stellar companion to raise its core temperature enough to ignite carbon fusion, at which point it undergoes runaway nuclear fusion, completely disrupting it. There are three avenues by which this detonation is theorized to happen: stable accretion of material from a companion, the collision of two white dwarfs, or accretion that causes ignition in a shell that then ignites. The dominant mechanism by which Type Ia supernovae are produced remains unclear. Despite this uncertainty in how Type Ia supernovae are produced, Type Ia supernovae have very uniform properties, and are useful standard candles over intergalactic distances. Some calibrations are required to compensate for the gradual change in properties or different frequencies of abnormal luminosity supernovae at high red shift, and for small variations in brightness identified by light curve shape or spectrum.

There are several means by which a supernova of this type can form, but they share a common underlying mechanism. If a carbon-oxygen white dwarf accreted enough matter to reach the Chandrasekhar limit of about 1.44 solar masses () (for a non-rotating star), it would no longer be able to support the bulk of its mass through electron degeneracy pressure and would begin to collapse. However, the current view is that this limit is not normally attained; increasing temperature and density inside the core ignite carbon fusion as the star approaches the limit (to within about 1%), before collapse is initiated.

Within a few seconds, a substantial fraction of the matter in the white dwarf undergoes nuclear fusion, releasing enough energy (1–) to unbind the star in a supernova. An outwardly expanding shock wave is generated, with matter reaching velocities on the order of 5,000–20,000 km/s, or roughly 3% of the speed of light. There is also a significant increase in luminosity, reaching an absolute magnitude of −19.3 (or 5 billion times brighter than the Sun), with little variation.

The model for the formation of this category of supernova is a closed binary star system. The larger of the two stars is the first to evolve off the main sequence, and it expands to form a red giant. The two stars now share a common envelope, causing their mutual orbit to shrink. The giant star then sheds most of its envelope, losing mass until it can no longer continue nuclear fusion. At this point it becomes a white dwarf star, composed primarily of carbon and oxygen. Eventually the secondary star also evolves off the main sequence to form a red giant. Matter from the giant is accreted by the white dwarf, causing the latter to increase in mass. Despite widespread acceptance of the basic model, the exact details of initiation and of the heavy elements produced in the catastrophic event are still unclear.

Type Ia supernovae follow a characteristic light curve—the graph of luminosity as a function of time—after the event. This luminosity is generated by the radioactive decay of nickel-56 through cobalt-56 to iron-56. The peak luminosity of the light curve is extremely consistent across normal Type Ia supernovae, having a maximum absolute magnitude of about −19.3. This allows them to be used as a secondary standard candle to measure the distance to their host galaxies.

Another model for the formation of Type Ia supernovae involves the merger of two white dwarf stars, with the combined mass momentarily exceeding the Chandrasekhar limit. There is much variation in this type of event, and in many cases there may be no supernova at all, but it is expected that they will have a broader and less luminous light curve than the more normal SN Type Ia.

Abnormally bright Type Ia supernovae are expected when the white dwarf already has a mass higher than the Chandrasekhar limit, possibly enhanced further by asymmetry, but the ejected material will have less than normal kinetic energy.

There is no formal sub-classification for the non-standard Type Ia supernovae. It has been proposed that a group of sub-luminous supernovae that occur when helium accretes onto a white dwarf should be classified as Type Iax. This type of supernova may not always completely destroy the white dwarf progenitor and could leave behind a zombie star.

One specific type of non-standard Type Ia supernova develops hydrogen, and other, emission lines and gives the appearance of mixture between a normal Type Ia and a Type IIn supernova. Examples are SN 2002ic and SN 2005gj. These supernova have been dubbed Type Ia/IIn, Type Ian, Type IIa and Type IIan.

Very massive stars can undergo core collapse when nuclear fusion becomes unable to sustain the core against its own gravity; passing this threshold is the cause of all types of supernova except Type Ia. The collapse may cause violent expulsion of the outer layers of the star resulting in a supernova, or the release of gravitational potential energy may be insufficient and the star may collapse into a black hole or neutron star with little radiated energy.

Core collapse can be caused by several different mechanisms: electron capture; exceeding the Chandrasekhar limit; pair-instability; or photodisintegration. When a massive star develops an iron core larger than the Chandrasekhar mass it will no longer be able to support itself by electron degeneracy pressure and will collapse further to a neutron star or black hole. Electron capture by magnesium in a degenerate O/Ne/Mg core causes gravitational collapse followed by explosive oxygen fusion, with very similar results. Electron-positron pair production in a large post-helium burning core removes thermodynamic support and causes initial collapse followed by runaway fusion, resulting in a pair-instability supernova. A sufficiently large and hot stellar core may generate gamma-rays energetic enough to initiate photodisintegration directly, which will cause a complete collapse of the core.

The table below lists the known reasons for core collapse in massive stars, the types of star that they occur in, their associated supernova type, and the remnant produced. The metallicity is the proportion of elements other than hydrogen or helium, as compared to the Sun. The initial mass is the mass of the star prior to the supernova event, given in multiples of the Sun's mass, although the mass at the time of the supernova may be much lower.

Type IIn supernovae are not listed in the table. They can potentially be produced by various types of core collapse in different progenitor stars, possibly even by Type Ia white dwarf ignitions, although it seems that most will be from iron core collapse in luminous supergiants or hypergiants (including LBVs). The narrow spectral lines for which they are named occur because the supernova is expanding into a small dense cloud of circumstellar material. It appears that a significant proportion of supposed Type IIn supernovae are actually supernova impostors, massive eruptions of LBV-like stars similar to the Great Eruption of Eta Carinae. In these events, material previously ejected from the star creates the narrow absorption lines and causes a shock wave through interaction with the newly ejected material.

When a stellar core is no longer supported against gravity, it collapses in on itself with velocities reaching 70,000 km/s (0.23"c"), resulting in a rapid increase in temperature and density. What follows next depends on the mass and structure of the collapsing core, with low mass degenerate cores forming neutron stars, higher mass degenerate cores mostly collapsing completely to black holes, and non-degenerate cores undergoing runaway fusion.

The initial collapse of degenerate cores is accelerated by beta decay, photodisintegration and electron capture, which causes a burst of electron neutrinos. As the density increases, neutrino emission is cut off as they become trapped in the core. The inner core eventually reaches typically 30 km diameter and a density comparable to that of an atomic nucleus, and neutron degeneracy pressure tries to halt the collapse. If the core mass is more than about then neutron degeneracy is insufficient to stop the collapse and a black hole forms directly with no supernova.

In lower mass cores the collapse is stopped and the newly formed neutron core has an initial temperature of about 100 billion kelvin, 6000 times the temperature of the sun's core. At this temperature, neutrino-antineutrino pairs of all flavors are efficiently formed by thermal emission. These thermal neutrinos are several times more abundant than the electron-capture neutrinos. About 10 joules, approximately 10% of the star's rest mass, is converted into a ten-second burst of neutrinos which is the main output of the event. The suddenly halted core collapse rebounds and produces a shock wave that stalls within milliseconds in the outer core as energy is lost through the dissociation of heavy elements. A process that is is necessary to allow the outer layers of the core to reabsorb around 10 joules (1 foe) from the neutrino pulse, producing the visible brightness, although there are also other theories on how to power the explosion.

Some material from the outer envelope falls back onto the neutron star, and for cores beyond about there is sufficient fallback to form a black hole. This fallback will reduce the kinetic energy created and the mass of expelled radioactive material, but in some situations it may also generate relativistic jets that result in a gamma-ray burst or an exceptionally luminous supernova.

Collapse of massive non-degenerate cores will ignite further fusion. When the core collapse is initiated by pair instability, oxygen fusion begins and the collapse may be halted. For core masses of , the collapse halts and the star remains intact, but core collapse will occur again when a larger core has formed. For cores of around , the fusion of oxygen and heavier elements is so energetic that the entire star is disrupted, causing a supernova. At the upper end of the mass range, the supernova is unusually luminous and extremely long-lived due to many solar masses of ejected Ni. For even larger core masses, the core temperature becomes high enough to allow photodisintegration and the core collapses completely into a black hole.

Stars with initial masses less than about eight times the sun never develop a core large enough to collapse and they eventually lose their atmospheres to become white dwarfs. Stars with at least (possibly as much as ) evolve in a complex fashion, progressively burning heavier elements at hotter temperatures in their cores. The star becomes layered like an onion, with the burning of more easily fused elements occurring in larger shells. Although popularly described as an onion with an iron core, the least massive supernova progenitors only have oxygen-neon(-magnesium) cores. These super AGB stars may form the majority of core collapse supernovae, although less luminous and so less commonly observed than those from more massive progenitors.

If core collapse occurs during a supergiant phase when the star still has a hydrogen envelope, the result is a Type II supernova. The rate of mass loss for luminous stars depends on the metallicity and luminosity. Extremely luminous stars at near solar metallicity will lose all their hydrogen before they reach core collapse and so will not form a Type II supernova. At low metallicity, all stars will reach core collapse with a hydrogen envelope but sufficiently massive stars collapse directly to a black hole without producing a visible supernova.

Stars with an initial mass up to about 90 times the sun, or a little less at high metallicity, are expected to result in a Type II-P supernova which is the most commonly observed type. At moderate to high metallicity, stars near the upper end of that mass range will have lost most of their hydrogen when core collapse occurs and the result will be a Type II-L supernova. At very low metallicity, stars of around will reach core collapse by pair instability while they still have a hydrogen atmosphere and an oxygen core and the result will be a supernova with Type II characteristics but a very large mass of ejected Ni and high luminosity.

These supernovae, like those of Type II, are massive stars that undergo core collapse. However the stars which become Types Ib and Ic supernovae have lost most of their outer (hydrogen) envelopes due to strong stellar winds or else from interaction with a companion. These stars are known as Wolf–Rayet stars, and they occur at moderate to high metallicity where continuum driven winds cause sufficiently high mass loss rates. Observations of Type Ib/c supernova do not match the observed or expected occurrence of Wolf–Rayet stars and alternate explanations for this type of core collapse supernova involve stars stripped of their hydrogen by binary interactions. Binary models provide a better match for the observed supernovae, with the proviso that no suitable binary helium stars have ever been observed. Since a supernova can occur whenever the mass of the star at the time of core collapse is low enough not to cause complete fallback to a black hole, any massive star may result in a supernova if it loses enough mass before core collapse occurs.

Type Ib supernovae are the more common and result from Wolf–Rayet stars of Type WC which still have helium in their atmospheres. For a narrow range of masses, stars evolve further before reaching core collapse to become WO stars with very little helium remaining and these are the progenitors of Type Ic supernovae.

A few percent of the Type Ic supernovae are associated with gamma-ray bursts (GRB), though it is also believed that any hydrogen-stripped Type Ib or Ic supernova could produce a GRB, depending on the circumstances of the geometry. The mechanism for producing this type of GRB is the jets produced by the magnetic field of the rapidly spinning magnetar formed at the collapsing core of the star. The jets would also transfer energy into the expanding outer shell, producing a super-luminous supernova.

Ultra-stripped supernovae occur when the exploding star has been stripped (almost) all the way to the metal core, via mass transfer in a close binary. As a result, very little material is ejected from the exploding star (c. ). In the most extreme cases, ultra-stripped supernovae can occur in naked metal cores, barely above the Chandrasekhar mass limit. SN 2005ek might be an observational example of an ultra-stripped supernova, giving rise to a relatively dim and fast decaying light curve. The nature of ultra-stripped supernovae can be both iron core-collapse and electron capture supernovae, depending on the mass of the collapsing core.

The core collapse of some massive stars may not result in a visible supernova. The main model for this is a sufficiently massive core that the kinetic energy is insufficient to reverse the infall of the outer layers onto a black hole. These events are difficult to detect, but large surveys have detected possible candidates. The red supergiant N6946-BH1 in NGC 6946 underwent a modest outburst in March 2009, before fading from view. Only a faint infrared source remains at the star's location.

A historic puzzle concerned the source of energy that can maintain the optical supernova glow for months. Although the energy that disrupts each type of supernovae is delivered promptly, the light curves are mostly dominated by subsequent radioactive heating of the rapidly expanding ejecta. Some have considered rotational energy from the central pulsar. The ejecta gases would dim quickly without some energy input to keep it hot. The intensely radioactive nature of the ejecta gases, which is now known to be correct for most supernovae, was first calculated on sound nucleosynthesis grounds in the late 1960s. It was not until SN 1987A that direct observation of gamma-ray lines unambiguously identified the major radioactive nuclei.

It is now known by direct observation that much of the light curve (the graph of luminosity as a function of time) after the occurrence of a Type II Supernova, such as SN 1987A, is explained by those predicted radioactive decays. Although the luminous emission consists of optical photons, it is the radioactive power absorbed by the ejected gases that keeps the remnant hot enough to radiate light. The radioactive decay of Ni through its daughters Co to Fe produces gamma-ray photons, primarily of 847keV and 1238keV, that are absorbed and dominate the heating and thus the luminosity of the ejecta at intermediate times (several weeks) to late times (several months). Energy for the peak of the light curve of SN1987A was provided by the decay of Ni to Co (half life 6 days) while energy for the later light curve in particular fit very closely with the 77.3 day half-life of Co decaying to Fe. Later measurements by space gamma-ray telescopes of the small fraction of the Co and Co gamma rays that escaped the SN 1987A remnant without absorption confirmed earlier predictions that those two radioactive nuclei were the power sources.

The visual light curves of the different supernova types all depend at late times on radioactive heating, but they vary in shape and amplitude because of the underlying mechanisms, the way that visible radiation is produced, the epoch of its observation, and the transparency of the ejected material. The light curves can be significantly different at other wavelengths. For example, at ultraviolet wavelengths there is an early extremely luminous peak lasting only a few hours corresponding to the breakout of the shock launched by the initial event, but that breakout is hardly detectable optically.

The light curves for Type Ia are mostly very uniform, with a consistent maximum absolute magnitude and a relatively steep decline in luminosity. Their optical energy output is driven by radioactive decay of ejected nickel-56 (half life 6 days), which then decays to radioactive cobalt-56 (half life 77 days). These radioisotopes excite the surrounding material to incandescence. Studies of cosmology today rely on Ni radioactivity providing the energy for the optical brightness of supernovae of Type Ia, which are the "standard candles" of cosmology but whose diagnostic 847keV and 1238keV gamma rays were first detected only in 2014. The initial phases of the light curve decline steeply as the effective size of the photosphere decreases and trapped electromagnetic radiation is depleted. The light curve continues to decline in the B band while it may show a small shoulder in the visual at about 40 days, but this is only a hint of a secondary maximum that occurs in the infra-red as certain ionised heavy elements recombine to produce infra-red radiation and the ejecta become transparent to it. The visual light curve continues to decline at a rate slightly greater than the decay rate of the radioactive cobalt (which has the longer half life and controls the later curve), because the ejected material becomes more diffuse and less able to convert the high energy radiation into visual radiation. After several months, the light curve changes its decline rate again as positron emission becomes dominant from the remaining cobalt-56, although this portion of the light curve has been little-studied.

Type Ib and Ic light curves are basically similar to Type Ia although with a lower average peak luminosity. The visual light output is again due to radioactive decay being converted into visual radiation, but there is a much lower mass of the created nickel-56. The peak luminosity varies considerably and there are even occasional Type Ib/c supernovae orders of magnitude more and less luminous than the norm. The most luminous Type Ic supernovae are referred to as hypernovae and tend to have broadened light curves in addition to the increased peak luminosity. The source of the extra energy is thought to be relativistic jets driven by the formation of a rotating black hole, which also produce gamma-ray bursts.

The light curves for Type II supernovae are characterised by a much slower decline than Type I, on the order of 0.05 magnitudes per day, excluding the plateau phase. The visual light output is dominated by kinetic energy rather than radioactive decay for several months, due primarily to the existence of hydrogen in the ejecta from the atmosphere of the supergiant progenitor star. In the initial destruction this hydrogen becomes heated and ionised. The majority of Type II supernovae show a prolonged plateau in their light curves as this hydrogen recombines, emitting visible light and becoming more transparent. This is then followed by a declining light curve driven by radioactive decay although slower than in Type I supernovae, due to the efficiency of conversion into light by all the hydrogen.

In Type II-L the plateau is absent because the progenitor had relatively little hydrogen left in its atmosphere, sufficient to appear in the spectrum but insufficient to produce a noticeable plateau in the light output. In Type IIb supernovae the hydrogen atmosphere of the progenitor is so depleted (thought to be due to tidal stripping by a companion star) that the light curve is closer to a Type I supernova and the hydrogen even disappears from the spectrum after several weeks.

Type IIn supernovae are characterised by additional narrow spectral lines produced in a dense shell of circumstellar material. Their light curves are generally very broad and extended, occasionally also extremely luminous and referred to as a superluminous supernova. These light curves are produced by the highly efficient conversion of kinetic energy of the ejecta into electromagnetic radiation by interaction with the dense shell of material. This only occurs when the material is sufficiently dense and compact, indicating that it has been produced by the progenitor star itself only shortly before the supernova occurs.

Large numbers of supernovae have been catalogued and classified to provide distance candles and test models. Average characteristics vary somewhat with distance and type of host galaxy, but can broadly be specified for each supernova type.

Notes:

A long-standing puzzle surrounding Type II supernovae is why the remaining compact object receives a large velocity away from the epicentre; pulsars, and thus neutron stars, are observed to have high velocities, and black holes presumably do as well, although they are far harder to observe in isolation. The initial impetus can be substantial, propelling an object of more than a solar mass at a velocity of 500 km/s or greater. This indicates an expansion asymmetry, but the mechanism by which momentum is transferred to the compact object a puzzle. Proposed explanations for this kick include convection in the collapsing star and jet production during neutron star formation.

One possible explanation for this asymmetry is a large-scale convection above the core. The convection can create variations in the local abundances of elements, resulting in uneven nuclear burning during the collapse, bounce and resulting expansion.

Another possible explanation is that accretion of gas onto the central neutron star can create a disk that drives highly directional jets, propelling matter at a high velocity out of the star, and driving transverse shocks that completely disrupt the star. These jets might play a crucial role in the resulting supernova. (A similar model is now favored for explaining long gamma-ray bursts.)

Initial asymmetries have also been confirmed in Type Ia supernovae through observation. This result may mean that the initial luminosity of this type of supernova depends on the viewing angle. However, the expansion becomes more symmetrical with the passage of time. Early asymmetries are detectable by measuring the polarization of the emitted light.

Although we are used to thinking of supernovae primarily as luminous visible events, the electromagnetic radiation they release is almost a minor side-effect. Particularly in the case of core collapse supernovae, the emitted electromagnetic radiation is a tiny fraction of the total energy released during the event.

There is a fundamental difference between the balance of energy production in the different types of supernova. In Type Ia white dwarf detonations, most of the energy is directed into heavy element synthesis and the kinetic energy of the ejecta. In core collapse supernovae, the vast majority of the energy is directed into neutrino emission, and while some of this apparently powers the observed destruction, 99%+ of the neutrinos escape the star in the first few minutes following the start of the collapse.

Type Ia supernovae derive their energy from a runaway nuclear fusion of a carbon-oxygen white dwarf. The details of the energetics are still not fully understood, but the end result is the ejection of the entire mass of the original star at high kinetic energy. Around half a solar mass of that mass is Ni generated from silicon burning. Ni is radioactive and decays into Co by beta plus decay (with a half life of six days) and gamma rays. Co itself decays by the beta plus (positron) path with a half life of 77 days into stable Fe. These two processes are responsible for the electromagnetic radiation from Type Ia supernovae. In combination with the changing transparency of the ejected material, they produce the rapidly declining light curve.

Core collapse supernovae are on average visually fainter than Type Ia supernovae, but the total energy released is far higher. In these type of supernovae, the gravitational potential energy is converted into kinetic energy that compresses and collapses the core, initially producing electron neutrinos from disintegrating nucleons, followed by all flavours of thermal neutrinos from the super-heated neutron star core. Around 1% of these neutrinos are thought to deposit sufficient energy into the outer layers of the star to drive the resulting catastrophe, but again the details cannot be reproduced exactly in current models. Kinetic energies and nickel yields are somewhat lower than Type Ia supernovae, hence the lower peak visual luminosity of Type II supernovae, but energy from the de-ionisation of the many solar masses of remaining hydrogen can contribute to a much slower decline in luminosity and produce the plateau phase seen in the majority of core collapse supernovae.

In some core collapse supernovae, fallback onto a black hole drives relativistic jets which may produce a brief energetic and directional burst of gamma rays and also transfers substantial further energy into the ejected material. This is one scenario for producing high luminosity supernovae and is thought to be the cause of Type Ic hypernovae and long duration gamma-ray bursts. If the relativistic jets are too brief and fail to penetrate the stellar envelope then a low luminosity gamma-ray burst may be produced and the supernova may be sub-luminous.

When a supernova occurs inside a small dense cloud of circumstellar material, it will produce a shock wave that can efficiently convert a high fraction of the kinetic energy into electromagnetic radiation. Even though the initial energy was entirely normal the resulting supernova will have high luminosity and extended duration since it does not rely on exponential radioactive decay. This type of event may cause Type IIn hypernovae.

Although pair-instability supernovae are core collapse supernovae with spectra and light curves similar to Type II-P, the nature after core collapse is more like that of a giant Type Ia with runaway fusion of carbon, oxygen, and silicon. The total energy released by the highest mass events is comparable to other core collapse supernovae but neutrino production is thought to be very low, hence the kinetic and electromagnetic energy released is very high. The cores of these stars are much larger than any white dwarf and the amount of radioactive nickel and other heavy elements ejected from their cores can be orders of magnitude higher, with consequently high visual luminosity.

The supernova classification type is closely tied to the type of star at the time of the collapse. The occurrence of each type of supernova depends dramatically on the metallicity, and hence the age of the host galaxy.

Type Ia supernovae are produced from white dwarf stars in binary systems and occur in all galaxy types. Core collapse supernovae are only found in galaxies undergoing current or very recent star formation, since they result from short-lived massive stars. They are most commonly found in Type Sc spirals, but also in the arms of other spiral galaxies and in irregular galaxies, especially starburst galaxies.

Type Ib/c and II-L, and possibly most Type IIn, supernovae are only thought to be produced from stars having near-solar metallicity levels that result in high mass loss from massive stars, hence they are less common in older, more-distant galaxies. The table shows the expected progenitor for the main types of core collapse supernova, and the approximate proportions that have been observed in the local neighbourhood.
There are a number of difficulties reconciling modelled and observed stellar evolution leading up to core collapse supernovae. Red supergiants are the expected progenitors for the vast majority of core collapse supernovae, and these have been observed but only at relatively low masses and luminosities, below about and respectively. Most progenitors of Type II supernovae are not detected and must be considerably fainter, and presumably less massive. It is now proposed that higher mass red supergiants do not explode as supernovae, but instead evolve back towards hotter temperatures. Several progenitors of Type IIb supernovae have been confirmed, and these were K and G supergiants, plus one A supergiant. Yellow hypergiants or LBVs are proposed progenitors for Type IIb supernovae, and almost all Type IIb supernovae near enough to observe have shown such progenitors.

Until just a few decades ago, hot supergiants were not considered likely to explode, but observations have shown otherwise. Blue supergiants form an unexpectedly high proportion of confirmed supernova progenitors, partly due to their high luminosity and easy detection, while not a single Wolf–Rayet progenitor has yet been clearly identified. Models have had difficulty showing how blue supergiants lose enough mass to reach supernova without progressing to a different evolutionary stage. One study has shown a possible route for low-luminosity post-red supergiant luminous blue variables to collapse, most likely as a Type IIn supernova. Several examples of hot luminous progenitors of Type IIn supernovae have been detected: SN 2005gy and SN 2010jl were both apparently massive luminous stars, but are very distant; and SN 2009ip had a highly luminous progenitor likely to have been an LBV, but is a peculiar supernova whose exact nature is disputed.

The progenitors of Type Ib/c supernovae are not observed at all, and constraints on their possible luminosity are often lower than those of known WC stars. WO stars are extremely rare and visually relatively faint, so it is difficult to say whether such progenitors are missing or just yet to be observed. Very luminous progenitors have not been securely identified, despite numerous supernovae being observed near enough that such progenitors would have been clearly imaged. Population modelling shows that the observed Type Ib/c supernovae could be reproduced by a mixture of single massive stars and stripped-envelope stars from interacting binary systems. The continued lack of unambiguous detection of progenitors for normal Type Ib and Ic supernovae may be due to most massive stars collapsing directly to a black hole without a supernova outburst. Most of these supernovae are then produced from lower-mass low-luminosity helium stars in binary systems. A small number would be from rapidly-rotating massive stars, likely corresponding to the highly-energetic Type Ic-BL events that are associated with long-duration gamma-ray bursts.

Supernovae are the major source of elements heavier than nitrogen. These elements are produced by nuclear fusion for nuclei up to S, by silicon photodisintegration rearrangement and quasiequilibrium (see Supernova nucleosynthesis) during silicon burning for nuclei between Ar and Ni, and by rapid captures of neutrons during the supernova's collapse for elements heavier than iron. Nucleosynthesis during silicon burning yields nuclei roughly 1000-100,000 times more abundant than the r-process isotopes heavier than iron. Supernovae are the most likely, although not undisputed, candidate sites for the r-process, which is the rapid capture of neutrons that occurs at high temperature and high density of neutrons. Those reactions produce highly unstable nuclei that are rich in neutrons and that rapidly beta decay into more stable forms. The r-process produces about half of all the heavier isotopes of the elements beyond iron, including plutonium and uranium. The only other major competing process for producing elements heavier than iron is the s-process in large, old, red-giant AGB stars, which produces these elements slowly over longer epochs and which cannot produce elements heavier than lead.

Remnants of many supernovae consist of a compact object and a rapidly expanding shock wave of material. This cloud of material sweeps up the surrounding interstellar medium during a free expansion phase, which can last for up to two centuries. The wave then gradually undergoes a period of adiabatic expansion, and will slowly cool and mix with the surrounding interstellar medium over a period of about 10,000 years.
The Big Bang produced hydrogen, helium, and traces of lithium, while all heavier elements are synthesized in stars and supernovae. Supernovae tend to enrich the surrounding interstellar medium with elements other than hydrogen and helium, which usually astronomers refer to as "metals".

These injected elements ultimately enrich the molecular clouds that are the sites of star formation. Thus, each stellar generation has a slightly different composition, going from an almost pure mixture of hydrogen and helium to a more metal-rich composition. Supernovae are the dominant mechanism for distributing these heavier elements, which are formed in a star during its period of nuclear fusion. The different abundances of elements in the material that forms a star have important influences on the star's life, and may decisively influence the possibility of having planets orbiting it.

The kinetic energy of an expanding supernova remnant can trigger star formation by compressing nearby, dense molecular clouds in space. The increase in turbulent pressure can also prevent star formation if the cloud is unable to lose the excess energy.

Evidence from daughter products of short-lived radioactive isotopes shows that a nearby supernova helped determine the composition of the Solar System 4.5 billion years ago, and may even have triggered the formation of this system. Supernova production of heavy elements over astronomic periods of time ultimately made the chemistry of life on Earth possible.

A near-Earth supernova is a supernova close enough to the Earth to have noticeable effects on its biosphere. Depending upon the type and energy of the supernova, it could be as far as 3000 light-years away. Gamma rays from a supernova would induce a chemical reaction in the upper atmosphere converting molecular nitrogen into nitrogen oxides, depleting the ozone layer enough to expose the surface to harmful ultraviolet solar radiation. This has been proposed as the cause of the Ordovician–Silurian extinction, which resulted in the death of nearly 60% of the oceanic life on Earth.
In 1996 it was theorized that traces of past supernovae might be detectable on Earth in the form of metal isotope signatures in rock strata. Iron-60 enrichment was later reported in deep-sea rock of the Pacific Ocean. In 2009, elevated levels of nitrate ions were found in Antarctic ice, which coincided with the 1006 and 1054 supernovae. Gamma rays from these supernovae could have boosted levels of nitrogen oxides, which became trapped in the ice.

Type Ia supernovae are thought to be potentially the most dangerous if they occur close enough to the Earth. Because these supernovae arise from dim, common white dwarf stars in binary systems, it is likely that a supernova that can affect the Earth will occur unpredictably and in a star system that is not well studied. The closest known candidate is IK Pegasi (see below). Recent estimates predict that a Type II supernova would have to be closer than eight parsecs (26 light-years) to destroy half of the Earth's ozone layer, and there are no such candidates closer than about 500 light years.

The next supernova in the Milky Way will likely be detectable even if it occurs on the far side of the galaxy. It is likely to be produced by the collapse of an unremarkable red supergiant and it is very probable that it will already have been catalogued in infrared surveys such as 2MASS. There is a smaller chance that the next core collapse supernova will be produced by a different type of massive star such as a yellow hypergiant, luminous blue variable, or Wolf–Rayet. The chances of the next supernova being a Type Ia produced by a white dwarf are calculated to be about a third of those for a core collapse supernova. Again it should be observable wherever it occurs, but it is less likely that the progenitor will ever have been observed. It isn't even known exactly what a Type Ia progenitor system looks like, and it is difficult to detect them beyond a few parsecs. The total supernova rate in our galaxy is estimated to be between 2 and 12 per century, although we haven't actually observed one for several centuries.
Statistically, the next supernova is likely to be produced from an otherwise unremarkable red supergiant, but it is difficult to identify which of those supergiants are in the final stages of heavy element fusion in their cores and which have millions of years left. The most-massive red supergiants are expected to shed their atmospheres and evolve to Wolf–Rayet stars before their cores collapse. All Wolf–Rayet stars are expected to end their lives from the Wolf–Rayet phase within a million years or so, but again it is difficult to identify those that are closest to core collapse. One class that is expected to have no more than a few thousand years before exploding are the WO Wolf–Rayet stars, which are known to have exhausted their core helium. Only eight of them are known, and only four of those are in the Milky Way.

A number of close or well known stars have been identified as possible core collapse supernova candidates: the red supergiants Antares and Betelgeuse; the yellow hypergiant Rho Cassiopeiae; the luminous blue variable Eta Carinae that has already produced a supernova impostor; and the brightest component, a Wolf–Rayet star, in the Regor or Gamma Velorum system, Others have gained notoriety as possible, although not very likely, progenitors for a gamma-ray burst; for example WR 104.

Identification of candidates for a Type Ia supernova is much more speculative. Any binary with an accreting white dwarf might produce a supernova although the exact mechanism and timescale is still debated. These systems are faint and difficult to identify, but the novae and recurrent novae are such systems that conveniently advertise themselves. One example is U Scorpii. The nearest known Type Ia supernova candidate is IK Pegasi (HR 8210), located at a distance of 150 light-years, but observations suggest it will be several million years before the white dwarf can accrete the critical mass required to become a Type Ia supernova.




</doc>
<doc id="27681" url="https://en.wikipedia.org/wiki?curid=27681" title="Sergei Prokofiev">
Sergei Prokofiev

Sergei Sergeyevich Prokofiev (; ; 27 April 1891 – 5 March 1953) was a Russian Soviet composer, pianist and conductor. As the creator of acknowledged masterpieces across numerous musical genres, he is regarded as one of the major composers of the 20th century. His works include such widely heard works as the March from "The Love for Three Oranges," the suite "Lieutenant Kijé", the ballet "Romeo and Juliet"—from which "Dance of the Knights" is taken—and "Peter and the Wolf." Of the established forms and genres in which he worked, he created – excluding juvenilia – seven completed operas, seven symphonies, eight ballets, five piano concertos, two violin concertos, a cello concerto, a symphony-concerto for cello and orchestra, and nine completed piano sonatas.

A graduate of the St Petersburg Conservatory, Prokofiev initially made his name as an iconoclastic composer-pianist, achieving notoriety with a series of ferociously dissonant and virtuosic works for his instrument, including his first two piano concertos. In 1915, Prokofiev made a decisive break from the standard composer-pianist category with his orchestral "Scythian Suite", compiled from music originally composed for a ballet commissioned by Sergei Diaghilev of the Ballets Russes. Diaghilev commissioned three further ballets from Prokofiev—"Chout," "Le pas d'acier" and "The Prodigal Son"—which at the time of their original production all caused a sensation among both critics and colleagues. Prokofiev's greatest interest, however, was opera, and he composed several works in that genre, including "The Gambler" and "The Fiery Angel". Prokofiev's one operatic success during his lifetime was "The Love for Three Oranges," composed for the Chicago Opera and subsequently performed over the following decade in Europe and Russia.

After the Revolution of 1917, Prokofiev left Russia with the official blessing of the Soviet minister Anatoly Lunacharsky, and resided in the United States, then Germany, then Paris, making his living as a composer, pianist and conductor. During that time, he married a Spanish singer, Carolina (Lina) Codina, with whom he had two sons. In the early 1930s, the Great Depression diminished opportunities for Prokofiev's ballets and operas to be staged in America and western Europe. Prokofiev, who regarded himself as composer foremost, resented the time taken by touring as a pianist, and increasingly turned to the Soviet Union for commissions of new music; in 1936, he finally returned to his homeland with his family. He enjoyed some success there – notably with "Lieutenant Kijé," "Peter and the Wolf," "Romeo and Juliet," and perhaps above all with "Alexander Nevsky."

The Nazi invasion of the USSR spurred him to compose his most ambitious work, an operatic version of Leo Tolstoy's "War and Peace". In 1948, Prokofiev was attacked for producing "anti-democratic formalism." Nevertheless, he enjoyed personal and artistic support from a new generation of Russian performers, notably Sviatoslav Richter and Mstislav Rostropovich: he wrote his ninth piano sonata for the former and his Symphony-Concerto for the latter.

Prokofiev was born in 1891 in Sontsovka (now Sontsivka, Pokrovsk Raion, Donetsk Oblast, Ukraine), a remote rural estate in the Yekaterinoslav Governorate of the Russian Empire. His father, Sergei Alexeyevich Prokofiev, was an agronomist. Prokofiev's mother, Maria (née Zhitkova), came from a family of former serfs who had been owned by the Sheremetev family, under whose patronage serf-children were taught theatre and arts from an early age. She was described by Reinhold Glière (Prokofiev's first composition teacher) as "a tall woman with beautiful, clever eyes … who knew how to create an atmosphere of warmth and simplicity about her." After their wedding in the summer of 1877, the Prokofievs moved to a small estate in the Smolensk governorate. Eventually, Sergei Alexeyevich found employment as a soil engineer, employed by one of his former fellow-students, Dmitri Sontsov, to whose estate in the Ukrainian steppes the Prokofievs moved.

By the time of Prokofiev's birth, Maria—having previously lost two daughters—had devoted her life to music; during her son's early childhood, she spent two months a year in Moscow or St Petersburg taking piano lessons. Sergei Prokofiev was inspired by hearing his mother practising the piano in the evenings, mostly works by Chopin and Beethoven, and wrote his first piano composition at the age of five, an "Indian Gallop", which was written down by his mother: it was in the F Lydian mode (a major scale with a raised 4th scale degree), as the young Prokofiev felt "reluctance to tackle the black notes". By seven, he had also learned to play chess. Chess would remain a passion of his, and he became acquainted with world chess champions José Raúl Capablanca, whom he beat in a simultaneous exhibition match in 1914, and Mikhail Botvinnik, with whom he played several matches in the 1930s. At the age of nine, he was composing his first opera, "The Giant", as well as an overture and various other pieces.
In 1902, Prokofiev's mother met Sergei Taneyev, director of the Moscow Conservatory, who initially suggested that Prokofiev should start lessons in piano and composition with Alexander Goldenweiser. Unable to arrange that, Taneyev instead arranged for composer and pianist Reinhold Glière to spend the summer of 1902 in Sontsovka teaching Prokofiev. The first series of lessons culminated, at the 11-year-old Prokofiev's insistence, with the budding composer making his first attempt to write a symphony. The following summer, Glière revisited Sontsovka to give further tuition. When, decades later, Prokofiev wrote about his lessons with Glière, he gave due credit to his teacher's sympathetic method but complained that Glière had introduced him to "square" phrase structure and conventional modulations, which he subsequently had to unlearn. Nonetheless, equipped with the necessary theoretical tools, Prokofiev started experimenting with dissonant harmonies and unusual time signatures in a series of short piano pieces he called "ditties" (after the so-called "song form", more accurately ternary form, on which they were based), laying the basis for his own musical style.

Despite his growing talent, Prokofiev's parents hesitated over starting their son on a musical career at such an early age, and considered the possibility of his attending a good high school in Moscow. By 1904, his mother had decided instead on Saint Petersburg, and she and Prokofiev visited the then capital to explore the possibility of moving there for his education. They were introduced to composer Alexander Glazunov, a professor at the Saint Petersburg Conservatory, who asked to see Prokofiev and his music; Prokofiev had composed two more operas, "Desert Islands" and "The Feast during the Plague", and was working on his fourth, "Undina". Glazunov was so impressed that he urged Prokofiev's mother to have her son apply for admission to the Conservatory. He passed the introductory tests and enrolled that year.

Several years younger than most of his class, Prokofiev was viewed as eccentric and arrogant, and annoyed a number of his classmates by keeping statistics on their errors. During that period, he studied under, among others, Alexander Winkler for piano, Anatoly Lyadov for harmony and counterpoint, Nikolai Tcherepnin for conducting, and Nikolai Rimsky-Korsakov for orchestration (though when Rimsky-Korsakov died in 1908, Prokofiev noted that he had only studied with him "after a fashion"—he was just one of many students in a heavily attended class—and regretted that he otherwise "never had the opportunity to study with him"). He also shared classes with the composers Boris Asafyev and Nikolai Myaskovsky, the latter becoming a relatively close and lifelong friend.

As a member of the Saint Petersburg music scene, Prokofiev developed a reputation as a musical rebel, while getting praise for his original compositions, which he performed himself on the piano. In 1909, he graduated from his class in composition with unimpressive marks. He continued at the Conservatory, studying piano under Anna Yesipova and continuing his conducting lessons under Tcherepnin.

In 1910, Prokofiev's father died and Sergei's financial support ceased. Fortunately, he had started making a name for himself as a composer and pianist outside the Conservatory, making appearances at the St Petersburg Evenings of Contemporary Music. There he performed several of his more adventurous piano works, such as his highly chromatic and dissonant Etudes, Op. 2 (1909). His performance of it impressed the organisers of the Evenings sufficiently for them to invite Prokofiev to give the Russian premiere of Arnold Schoenberg's Drei Klavierstücke, Op. 11. Prokofiev's harmonic experimentation continued with "Sarcasms" for piano, Op. 17 (1912), which makes extensive use of polytonality. He composed his first two piano concertos around then, the latter of which caused a scandal at its premiere (23 August 1913, Pavlovsk). According to one account, the audience left the hall with exclamations of "'To hell with this futuristic music! The cats on the roof make better music!'", but the modernists were in rapture.

In 1911, help arrived from renowned Russian musicologist and critic Alexander Ossovsky, who wrote a supportive letter to music publisher Boris P. Jurgenson (son of publishing-firm founder Peter Jurgenson [1836–1904]); thus a contract was offered to the composer. Prokofiev made his first foreign trip in 1913, travelling to Paris and London where he first encountered Sergei Diaghilev's Ballets Russes.

In 1914, Prokofiev finished his career at the Conservatory by entering the 'battle of the pianos', a competition open to the five best piano students for which the prize was a Schreder grand piano: Prokofiev won by performing his own Piano Concerto No. 1.

Soon afterwards, he journeyed to London where he made contact with the impresario Sergei Diaghilev. Diaghilev commissioned Prokofiev's first ballet, "Ala and Lolli"; but when Prokofiev brought the work in progress to him in Italy in 1915 he rejected it as "non-Russian". Urging Prokofiev to write "music that was national in character", Diaghilev then commissioned the ballet "Chout" ("The Buffoon"). (The original Russian-language full title was Сказка про шута, семерых шутов перешутившего, meaning "The Tale of the Buffoon who Outwits Seven Other Buffoons".) Under Diaghilev's guidance, Prokofiev chose his subject from a collection of folktales by the ethnographer Alexander Afanasyev; the story, concerning a buffoon and a series of confidence tricks, had been previously suggested to Diaghilev by Igor Stravinsky as a possible subject for a ballet, and Diaghilev and his choreographer Léonide Massine helped Prokofiev to shape it into a ballet scenario. Prokofiev's inexperience with ballet led him to revise the work extensively in the 1920s, following Diaghilev's detailed critique, prior to its first production.

The ballet's premiere in Paris on 17 May 1921 was a huge success and was greeted with great admiration by an audience that included Jean Cocteau, Igor Stravinsky and Maurice Ravel. Stravinsky called the ballet "the single piece of modern music he could listen to with pleasure," while Ravel called it "a work of genius."

During World War I, Prokofiev returned to the Conservatory and studied organ to avoid conscription. He composed "The Gambler" based on Fyodor Dostoyevsky's novel of the same name, but rehearsals were plagued by problems, and the scheduled 1917 première had to be cancelled because of the February Revolution. In the summer of that year, Prokofiev composed his first symphony, the "Classical". The name was Prokofiev's own; the music is in a style that, according to Prokofiev, Joseph Haydn would have used if he were alive at the time. The music is more or less Classical in style but incorporates more modern musical elements (see Neoclassicism).

The symphony was also an exact contemporary of Prokofiev's Violin Concerto No. 1 in D major, Op. 19, which was scheduled to premiere in November 1917. The first performances of both works had to wait until 21 April 1918 and 18 October 1923, respectively. Prokofiev stayed briefly with his mother in Kislovodsk in the Caucasus.

After completing the score of "Seven, They Are Seven", a "Chaldean invocation" for chorus and orchestra, Prokofiev was "left with nothing to do and time hung heavily on my hands". Believing that Russia "had no use for music at the moment", Prokofiev decided to try his fortunes in America until the turmoil in his homeland had passed. He set out for Moscow and Petersburg in March 1918 to sort out financial matters and to arrange for his passport. In May, he headed for the US, having obtained official permission to do so from Anatoly Lunacharsky, the People's Commissar for Education, who told him: "You are a revolutionary in music, we are revolutionaries in life. We ought to work together. But if you want to go to America I shall not stand in your way."

Arriving in San Francisco after having been released from questioning by immigration officials on Angel Island on 11 August 1918, Prokofiev was soon compared to other famous Russian exiles, such as Sergei Rachmaninoff. His debut solo concert in New York led to several further engagements. He also received a contract from the music director of the Chicago Opera Association, Cleofonte Campanini, for the production of his new opera "The Love for Three Oranges"; however, due to Campanini's illness and death, the premiere was postponed. The delay was another example of Prokofiev's bad luck in operatic matters. The failure also cost him his American solo career since the opera took too much time and effort. He soon found himself in financial difficulties, and in April 1920, he left for Paris, not wanting to return to Russia as a failure.

In Paris, Prokofiev reaffirmed his contacts with Diaghilev's Ballets Russes. He also completed some of his older, unfinished works, such as his Third Piano Concerto. "The Love for Three Oranges" finally premièred in Chicago, under the composer's baton, on 30 December 1921. Diaghilev became sufficiently interested in the opera to request Prokofiev play the vocal score to him in June 1922, while they were both in Paris for a revival of "Chout", so he could consider it for a possible production. Stravinsky, who was present at the audition, refused to listen to more than the first act. When he then accused Prokofiev of "wasting time composing operas", Prokofiev retorted that Stravinsky "was in no position to lay down a general artistic direction, since he is himself not immune to error". According to Prokofiev, Stravinsky "became incandescent with rage" and "we almost came to blows and were separated only with difficulty". As a result, "our relations became strained and for several years Stravinsky's attitude toward me was critical."

In March 1922, Prokofiev moved with his mother to the town of Ettal in the Bavarian Alps, where for over a year he concentrated on an opera project, "The Fiery Angel", based on the novel by Valery Bryusov. His later music had acquired a following in Russia, and he received invitations to return there, but decided to stay in Europe. In 1923, Prokofiev married the Spanish singer Carolina Codina (1897–1989, stage name Lina Llubera) before moving back to Paris.

In Paris, several of his works, including the Second Symphony, were performed, but their reception was lukewarm and Prokofiev sensed that he "was evidently no longer a sensation". Still, the Symphony appeared to prompt Diaghilev to commission "Le pas d'acier" ("The Steel Step"), a "modernist" ballet score intended to portray the industrialisation of the Soviet Union. It was enthusiastically received by Parisian audiences and critics.

Around 1924, Prokofiev was introduced to Christian Science. He began to practice its teachings, which he believed to be beneficial to his health and to his fiery temperament and to which he remained faithful for the rest of his life, according to biographer Simon Morrison.

Prokofiev and Stravinsky restored their friendship, though Prokofiev particularly disliked Stravinsky's "stylization of Bach" in such recent works as the Octet and the Concerto for Piano and Wind Instruments. For his part, Stravinsky described Prokofiev as the greatest Russian composer of his day, after himself.

In 1927, Prokofiev made his first concert tour in the Soviet Union. Over the course of more than two months, he spent time in Moscow and Leningrad (as St Petersburg had been renamed), where he enjoyed a very successful staging of "The Love for Three Oranges" in the Mariinsky Theatre. In 1928, Prokofiev completed his Third Symphony, which was broadly based on his unperformed opera "The Fiery Angel". The conductor Serge Koussevitzky characterized the Third as "the greatest symphony since Tchaikovsky's Sixth."

In the meantime, however, Prokofiev, under the influence of the teachings of Christian Science, had turned against the expressionist style and the subject matter of "The Fiery Angel". He now preferred what he called a "new simplicity", which he believed more sincere than the "contrivances and complexities" of so much modern music of the 1920s. During 1928–29, Prokofiev composed what was to be his last ballet for Diaghilev, "The Prodigal Son". When first staged in Paris on 21 May 1929, choreographed by George Balanchine with Serge Lifar in the title role, the audience and critics were particularly struck by the final scene in which the prodigal son drags himself across the stage on his knees to be welcomed by his father. Diaghilev had recognised that in the music to the scene, Prokofiev had "never been more clear, more simple, more melodious, and more tender". Only months later, Diaghilev died.

That summer, Prokofiev completed the Divertimento, Op. 43 (which he had started in 1925) and revised his Sinfonietta, Op. 5/48, a work started in his days at the Conservatory. In October of that year, he had a car crash while driving his family back to Paris from their holiday: as the car turned over, Prokofiev pulled some muscles on his left hand. Prokofiev was therefore unable to perform in Moscow during his tour shortly after the accident, but he was able to enjoy watching performances of his music from the audience. Prokofiev also attended the Bolshoi Theatre's "audition" of his ballet "Le pas d'acier", and was interrogated by members of the Russian Association of Proletarian Musicians (RAPM) about the work: he was asked whether the factory portrayed "a capitalist factory, where the worker is a slave, or a Soviet factory, where the worker is the master? If it is a Soviet factory, when and where did Prokofiev examine it, since from 1918 to the present he has been living abroad and came here for the first time in 1927 for two weeks [sic]?" Prokofiev replied, "That concerns politics, not music, and therefore I won't answer." The RAPM condemned the ballet as a "flat and vulgar anti-Soviet anecdote, a counter-revolutionary composition bordering on Fascism". The Bolshoi had no option but to reject the ballet.

With his left hand healed, Prokofiev toured the United States successfully at the start of 1930, propped up by his recent European success. That year, Prokofiev began his first non-Diaghilev ballet "On the Dnieper", Op. 51, a work commissioned by Serge Lifar, who had been appointed "maitre de ballet" at the Paris Opéra. In 1931 and 1932, he completed his fourth and fifth piano concertos. The following year saw the completion of the Symphonic Song, Op. 57, which Prokofiev's friend Myaskovsky—thinking of its potential audience in the Soviet Union—told him "isn't quite for us… it lacks that which we mean by monumentalism—a familiar simplicity and broad contours, of which you are extremely capable, but temporarily are carefully avoiding."

By the early 1930s, both Europe and America were suffering from the Great Depression, which inhibited both new opera and ballet productions, though audiences for Prokofiev's appearances as a pianist were, in Europe at least, undiminished. However, Prokofiev, who saw himself as a composer first and foremost, increasingly resented the amount of time that was lost to composition through his appearances as a pianist. Having been homesick for some time, Prokofiev began to build substantial bridges with the Soviet Union.

Following the dissolution of the RAPM in 1932, he acted increasingly as a musical ambassador between his homeland and western Europe, and his premieres and commissions were increasingly under the auspices of the Soviet Union. One such was "Lieutenant Kijé", which was commissioned as the score to a Soviet film.

Another commission, from the Kirov Theatre (as the Mariinsky had now been renamed) in Leningrad, was the ballet "Romeo and Juliet", composed to a scenario created by Adrian Piotrovsky and Sergei Radlov following the precepts of "drambalet" (dramatised ballet, officially promoted at the Kirov to replace works based primarily on choreographic display and innovation). Following Radlov's acrimonious resignation from the Kirov in June 1934, a new agreement was signed with the Bolshoi Theatre in Moscow on the understanding that Piotrovsky would remain involved. However, the ballet's original happy ending (contrary to Shakespeare) provoked controversy among Soviet cultural officials; the ballet's production was then postponed indefinitely when the staff of the Bolshoi was overhauled at the behest of the chairman of the Committee on Arts Affairs, Platon Kerzhentsev. Nikolai Myaskovsky, one of his closest friends, mentioned in a number of letters how he would like Prokofiev to stay in Russia.

In 1936, Prokofiev and his family settled permanently in Moscow, after shifting back and forth between Moscow and Paris for the previous four years. That year, he composed one of his most famous works, "Peter and the Wolf", for Natalya Sats's Central Children's Theatre. Sats also persuaded Prokofiev to write two songs for children, "Sweet Song", and "Chatterbox"; they were eventually joined by "The Little Pigs" and published as "Three Children's Songs", Op. 68. Prokofiev also composed the gigantic "Cantata for the 20th Anniversary of the October Revolution," originally intended for performance during the anniversary year but effectively blocked by Kerzhentsev, who demanded at the work's audition before the Committee on Arts Affairs, "Just what do you think you're doing, Sergey Sergeyevich, taking texts that belong to the people and setting them to such incomprehensible music?" The Cantata had to wait until 5 April 1966 for a partial premiere, just over 13 years after the composer's death.

Forced to adapt to the new circumstances (whatever private misgivings he had about them), Prokofiev wrote a series of "mass songs" (Opp. 66, 79, 89), using the lyrics of officially approved Soviet poets. In 1938, Prokofiev collaborated with Eisenstein on the historical epic "Alexander Nevsky", some of his most inventive and dramatic music. Although the film had very poor sound recording, Prokofiev adapted much of his score into a large-scale cantata for mezzo-soprano, orchestra and chorus, which was extensively performed and recorded. In the wake of "Alexander Nevsky"'s success, Prokofiev composed his first Soviet opera "Semyon Kotko", which was intended to be produced by the director Vsevolod Meyerhold. However, the première of the opera was postponed because Meyerhold was arrested on 20 June 1939 by the NKVD (Joseph Stalin's Secret Police), and shot on 2 February 1940. Only months after Meyerhold's arrest, Prokofiev was 'invited' to compose "Zdravitsa" (literally translated 'Cheers!', but more often given the English title "Hail to Stalin") (Op. 85) to celebrate Joseph Stalin's 60th birthday.

Later in 1939, Prokofiev composed his Piano Sonatas Nos. 6, 7, and 8, Opp. 82–84, widely known today as the "War Sonatas." Premiered respectively by Prokofiev (No. 6: 8 April 1940), Sviatoslav Richter (No. 7: Moscow, 18 January 1943) and Emil Gilels (No. 8: Moscow, 30 December 1944), they were subsequently championed in particular by Richter. Biographer Daniel Jaffé argued that Prokofiev, "having forced himself to compose a cheerful evocation of the nirvana Stalin wanted everyone to believe he had created" (i.e. in "Zdravitsa") then subsequently, in the three sonatas, "expressed his true feelings". As evidence, Jaffé has pointed out that the central movement of Sonata No. 7 opens with a theme based on a Robert Schumann lied "Wehmut" ("Sadness", which appears in Schumann's "Liederkreis", Op. 39): its words translate, "I can sometimes sing as if I were glad, yet secretly tears well and so free my heart. Nightingales… sing their song of longing from their dungeon's depth… everyone delights, yet no one feels the pain, the deep sorrow in the song." Ironically (it appears no one noticed his allusion), Sonata No. 7 received a Stalin Prize (Second Class), and No. 8 a Stalin Prize (First Class).

In the meantime, "Romeo and Juliet" was finally staged by the Kirov Ballet, choreographed by Leonid Lavrovsky, on 11 January 1940. To the surprise of all of its participants, the dancers having struggled to cope with the music's syncopated rhythms and almost having boycotted the production, the ballet was an instant success, and became recognised as the crowning achievement of Soviet dramatic ballet.

Prokofiev had been considering making an opera out of Leo Tolstoy's epic novel "War and Peace", when news of the German invasion of Russia on 22 June 1941 made the subject seem all the more timely. Prokofiev took two years to compose his original version of "War and Peace". Because of the war, he was evacuated together with a large number of other artists, initially to the Caucasus, where he composed his Second String Quartet. By now, his relationship with the 25-year-old writer and librettist Mira Mendelssohn (1915–1968) had finally led to his separation from his wife Lina, although they never divorced; indeed, Prokofiev had tried to persuade Lina and their sons to accompany him as evacuees out of Moscow, but Lina opted to stay.

During the war years, restrictions on style and the demand that composers write in a 'socialist realist' style were slackened, and Prokofiev was generally able to compose in his own way. The Violin Sonata No. 1, Op. 80, "The Year 1941", Op. 90, and the "Ballade for the Boy Who Remained Unknown", Op. 93 all came from this period. In 1943, Prokofiev joined Eisenstein in Alma-Ata, the largest city in Kazakhstan, to compose more film music ("Ivan the Terrible"), and the ballet "Cinderella" (Op. 87), one of his most melodious and celebrated compositions. Early that year, he also played excerpts from "War and Peace" to members of the Bolshoi Theatre collective, but the Soviet government had opinions about the opera that resulted in many revisions. In 1944, Prokofiev composed his Fifth Symphony (Op. 100) at a composer's colony outside Moscow. He conducted its first performance on 13 January 1945, just a fortnight after the triumphant premieres on 30 December 1944 of his Eighth Piano Sonata and, on the same day, the first part of Eisenstein's "Ivan the Terrible".

With the premiere of his Fifth Symphony, which was programmed alongside "Peter and the Wolf" and the "Classical" Symphony (conducted by Nikolai Anosov), Prokofiev appeared to reach the peak of his celebrity as a leading composer of the Soviet Union. Shortly afterward, he suffered a concussion after a fall due to chronic high blood pressure. He never fully recovered from the injury, and he was forced on medical advice to restrict his composing activity.

Prokofiev had time to write his postwar Sixth Symphony and his Ninth Piano Sonata (for Sviatoslav Richter) before the so-called "Zhdanov Decree". In early 1948, following a meeting of Soviet composers convened by Andrei Zhdanov, the Politburo issued a resolution denouncing Prokofiev, Dmitri Shostakovich, Myaskovsky, and Khachaturian of the crime of "formalism", described as a "renunciation of the basic principles of classical music" in favour of "muddled, nerve-racking" sounds that "turned music into cacophony". Eight of Prokofiev's works were banned from performance: "The Year 1941", "Ode to the End of the War", "Festive Poem", "Cantata for the Thirtieth Anniversary of October", "Ballad of an Unknown Boy", the 1934 piano cycle "Thoughts", and Piano Sonatas Nos. 6 and 8. Such was the perceived threat behind the banning of the works that even works that had avoided censure were no longer programmed: by August 1948, Prokofiev was in severe financial straits, his personal debt amounting to 180,000 rubles.

Meanwhile, on 20 February 1948, Prokofiev's estranged wife Lina was arrested for 'espionage', as she had tried to send money to her mother in Spain. After nine months of interrogation, she was sentenced by a three-member Military Collegium of the Supreme Court of the USSR to 20 years of hard labour. She was eventually released after Stalin's death in 1953 and in 1974 left the Soviet Union.

Prokofiev's latest opera projects, among them his desperate attempt to appease the cultural authorities, "The Story of a Real Man", were quickly cancelled by the Kirov Theatre. The snub, in combination with his declining health, caused Prokofiev to progressively withdraw from public life and from various activities, even his beloved chess, and increasingly devote himself to his own work. After a serious relapse in 1949, his doctors ordered him to limit his composing to an hour a day.

In spring 1949, he wrote his Cello Sonata in C, Op. 119, for the 22-year-old Mstislav Rostropovich, who gave the first performance in 1950, with Sviatoslav Richter. For Rostropovich, Prokofiev also extensively recomposed his Cello Concerto, transforming it into a Symphony-Concerto, a landmark in the cello and orchestra repertory today. The last public performance he attended, on 11 October 1952, was the première of the Seventh Symphony, his final masterpiece and last completed work. The symphony was written for the Children's Radio Division.
Prokofiev died at the age of 61 on 5 March 1953, the same day as Joseph Stalin. He had lived near Red Square, and for three days the throngs gathered to mourn Stalin, making it impossible to carry Prokofiev's body out for the funeral service at the headquarters of the Soviet Composers' Union. He is buried in the Novodevichy Cemetery in Moscow.

The leading Soviet musical periodical reported Prokofiev's death as a brief item on page 116. (The first 115 pages were devoted to the death of Stalin.) Prokofiev's death is usually attributed to cerebral hemorrhage. He had been chronically ill for the prior eight years; the precise nature of Prokofiev's terminal illness remains uncertain.

Lina Prokofiev outlived her estranged husband by many years, dying in London in early 1989. Royalties from her late husband's music provided her with a modest income, and she acted as storyteller for a recording of her husband's "Peter and the Wolf" (currently released on CD by Chandos Records) with Neeme Järvi conducting the Scottish National Orchestra. Their sons Sviatoslav (1924–2010), an architect, and Oleg (1928–1998), an artist, painter, sculptor and poet, dedicated a large part of their lives to the promotion of their father's life and work.

Arthur Honegger proclaimed that Prokofiev would "remain for us the greatest figure of contemporary music," and the American scholar Richard Taruskin has recognised Prokofiev's "gift, virtually unparalleled among 20th-century composers, for writing distinctively original diatonic melodies." Yet for some time Prokofiev's reputation in the West suffered as a result of Cold War antipathies, and his music has never won from Western academics and critics the kind of esteem enjoyed by Igor Stravinsky and Arnold Schoenberg, composers purported to have had a greater influence on younger generations of musicians.

Today Prokofiev may well be the most popular composer of 20th-century music. His orchestral music alone is played more frequently in the United States than that of any other composer of the last hundred years save Richard Strauss, while his operas, ballets, chamber works, and piano music appear regularly throughout major concert halls worldwide.

The composer was honoured in his native Donetsk Oblast when the Donetsk International Airport was renamed "Donetsk Sergey Prokofiev International Airport" and the Donetsk Musical and Pedagogical Institute was renamed the "S.S. Prokofiev State Music Academy of Donetsk" in 1988.


Important works include (in chronological order):

Prokofiev was a soloist with the London Symphony Orchestra, conducted by Piero Coppola, in the first recording of his Piano Concerto No. 3, recorded in London by His Master's Voice in June 1932. Prokofiev also recorded some of his solo piano music for HMV in Paris in February 1935; these recordings were issued on CD by Pearl and Naxos. In 1938, he conducted the Moscow Philharmonic Orchestra in a recording of the second suite from his "Romeo and Juliet" ballet; this performance was later released on LP and CD. Another reported recording with Prokofiev and the Moscow Philharmonic was of the First Violin Concerto with David Oistrakh as soloist; Everest Records later released this recording on an LP. Despite the attribution, the conductor was Aleksandr Gauk. A short sound film of Prokofiev playing some of the music from his opera "War and Peace" and then explaining the music has been discovered.





</doc>
<doc id="27683" url="https://en.wikipedia.org/wiki?curid=27683" title="Satellite">
Satellite

In the context of spaceflight, a satellite is an artificial object which has been intentionally placed into orbit. Such objects are sometimes called artificial satellites to distinguish them from natural satellites such as Earth's Moon.

In 1957 the Soviet Union launched the world's first artificial satellite, Sputnik 1. Since then, about 6,600 satellites from more than 40 countries have been launched. According to a 2013 estimate, 3,600 remained in orbit. Of those, about 1,000 were operational; while the rest have lived out their useful lives and become space debris. Approximately 500 operational satellites are in low-Earth orbit, 50 are in medium-Earth orbit (at 20,000 km), and the rest are in geostationary orbit (at 36,000 km). A few large satellites have been launched in parts and assembled in orbit. Over a dozen space probes have been placed into orbit around other bodies and become artificial satellites to the Moon, Mercury, Venus, Mars, Jupiter, Saturn, a few asteroids, a comet and the Sun.

Satellites are used for many purposes. Common types include military and civilian Earth observation satellites, communications satellites, navigation satellites, weather satellites, and space telescopes. Space stations and human spacecraft in orbit are also satellites. Satellite orbits vary greatly, depending on the purpose of the satellite, and are classified in a number of ways. Well-known (overlapping) classes include low Earth orbit, polar orbit, and geostationary orbit.

A launch vehicle is a rocket that places a satellite into orbit. Usually, it lifts off from a launch pad on land. Some are launched at sea from a submarine or a mobile maritime platform, or aboard a plane (see air launch to orbit).

Satellites are usually semi-independent computer-controlled systems. Satellite subsystems attend many tasks, such as power generation, thermal control, telemetry, attitude control and orbit control.

"Newton's cannonball", presented as a "thought experiment" in "A Treatise of the System of the World", by Isaac Newton was the first published mathematical study of the possibility of an artificial satellite.

The first fictional depiction of a satellite being launched into orbit was a short story by Edward Everett Hale, "The Brick Moon". The idea surfaced again in Jules Verne's "The Begum's Fortune" (1879).

In 1903, Konstantin Tsiolkovsky (1857–1935) published "Exploring Space Using Jet Propulsion Devices" (in Russian: "Исследование мировых пространств реактивными приборами"), which is the first academic treatise on the use of rocketry to launch spacecraft. He calculated the orbital speed required for a minimal orbit, and that a multi-stage rocket fuelled by liquid propellants could achieve this.

In 1928, Herman Potočnik (1892–1929) published his sole book, "The Problem of Space Travel — The Rocket Motor" (German: "Das Problem der Befahrung des Weltraums — der Raketen-Motor"). He described the use of orbiting spacecraft for observation of the ground and described how the special conditions of space could be useful for scientific experiments.

In a 1945 "Wireless World" article, the English science fiction writer Arthur C. Clarke (1917–2008) described in detail the possible use of communications satellites for mass communications. He suggested that three geostationary satellites would provide coverage over the entire planet.

The US military studied the idea of what was referred to as the "earth satellite vehicle" when Secretary of Defense James Forrestal made a public announcement on 29 December 1948, that his office was coordinating that project between the various services.

The first artificial satellite was Sputnik 1, launched by the Soviet Union on 4 October 1957, and initiating the Soviet Sputnik program, with Sergei Korolev as chief designer. This in turn triggered the Space Race between the Soviet Union and the United States.

Sputnik 1 helped to identify the density of high atmospheric layers through measurement of its orbital change and provided data on radio-signal distribution in the ionosphere. The unanticipated announcement of Sputnik 1's success precipitated the Sputnik crisis in the United States and ignited the so-called Space Race within the Cold War.

Sputnik 2 was launched on 3 November 1957 and carried the first living passenger into orbit, a dog named Laika.

In May, 1946, Project RAND had released the Preliminary Design of an Experimental World-Circling Spaceship, which stated, "A satellite vehicle with appropriate instrumentation can be expected to be one of the most potent scientific tools of the Twentieth Century."
The United States had been considering launching orbital satellites since 1945 under the Bureau of Aeronautics of the United States Navy. The United States Air Force's Project RAND eventually released the report, but considered the satellite to be a tool for science, politics, and propaganda, rather than a potential military weapon. In 1954, the Secretary of Defense stated, "I know of no American satellite program." In February 1954 Project RAND released "Scientific Uses for a Satellite Vehicle," written by R.R. Carhart. This expanded on potential scientific uses for satellite vehicles and was followed in June 1955 with "The Scientific Use of an Artificial Satellite," by H.K. Kallmann and W.W. Kellogg.

In the context of activities planned for the International Geophysical Year (1957–58), the White House announced on 29 July 1955 that the U.S. intended to launch satellites by the spring of 1958. This became known as Project Vanguard. On 31 July, the Soviets announced that they intended to launch a satellite by the fall of 1957.

Following pressure by the American Rocket Society, the National Science Foundation, and the International Geophysical Year, military interest picked up and in early 1955 the Army and Navy were working on Project Orbiter, two competing programs: the army's which involved using a Jupiter C rocket, and the civilian/Navy Vanguard Rocket, to launch a satellite. At first, they failed: initial preference was given to the Vanguard program, whose first attempt at orbiting a satellite resulted in the explosion of the launch vehicle on national television. But finally, three months after Sputnik 2, the project succeeded; Explorer 1 became the United States' first artificial satellite on 31 January 1958.

In June 1961, three-and-a-half years after the launch of Sputnik 1, the Air Force used resources of the United States Space Surveillance Network to catalog 115 Earth-orbiting satellites.

Early satellites were constructed as "one-off" designs. With growth in geosynchronous (GEO) satellite communication, multiple satellites began to be built on single model platforms called satellite buses. The first standardized satellite bus design was the HS-333 GEO commsat, launched in 1972.

Currently the largest artificial satellite ever is the International Space Station.
The United States Space Surveillance Network (SSN), a division of the United States Strategic Command, has been tracking objects in Earth's orbit since 1957 when the Soviet Union opened the Space Age with the launch of Sputnik I. Since then, the SSN has tracked more than 26,000 objects. The SSN currently tracks more than 8,000 man-made orbiting objects. The rest have re-entered Earth's atmosphere and disintegrated, or survived re-entry and impacted the Earth. The SSN tracks objects that are 10 centimeters in diameter or larger; those now orbiting Earth range from satellites weighing several tons to pieces of spent rocket bodies weighing only 10 pounds. About seven percent are operational satellites (i.e. ~560 satellites), the rest are space debris. The United States Strategic Command is primarily interested in the active satellites, but also tracks space debris which upon reentry might otherwise be mistaken for incoming missiles.

There are three basic categories of non-military satellite services:

Fixed satellite services handle hundreds of billions of voice, data, and video transmission tasks across all countries and continents between certain points on the Earth's surface.

Mobile satellite systems help connect remote regions, vehicles, ships, people and aircraft to other parts of the world and/or other mobile or stationary communications units, in addition to serving as navigation systems.

Scientific research satellites provide meteorological information, land survey data (e.g. remote sensing), Amateur (HAM) Radio, and other different scientific research applications such as earth science, marine science, and atmospheric research.



The first satellite, Sputnik 1, was put into orbit around Earth and was therefore in geocentric orbit. By far this is the most common type of orbit with approximately 1,459 artificial satellites orbiting the Earth. Geocentric orbits may be further classified by their altitude, inclination and eccentricity.

The commonly used altitude classifications of geocentric orbit are Low Earth orbit (LEO), Medium Earth orbit (MEO) and High Earth orbit (HEO). Low Earth orbit is any orbit below 2,000 km. Medium Earth orbit is any orbit between 2,000 and 35,786 km. High Earth orbit is any orbit higher than 35,786 km.

The general structure of a satellite is that it is connected to the earth stations that are present on the ground and connected through terrestrial links.






The satellite's functional versatility is imbedded within its technical components and its operations characteristics. Looking at the "anatomy" of a typical satellite, one discovers two modules. Note that some novel architectural concepts such as Fractionated spacecraft somewhat upset this taxonomy.

The bus module consists of the following subsystems:

The structural subsystem provides the mechanical base structure with adequate stiffness to withstand stress and vibrations experienced during launch, maintain structural integrity and stability while on station in orbit, and shields the satellite from extreme temperature changes and micro-meteorite damage.

The telemetry subsystem (aka Command and Data Handling, C&DH) monitors the on-board equipment operations, transmits equipment operation data to the earth control station, and receives the earth control station's commands to perform equipment operation adjustments.

The power subsystem consists of solar panels to convert solar energy into electrical power, regulation and distribution functions, and batteries that store power and supply the satellite when it passes into the Earth's shadow. Nuclear power sources (Radioisotope thermoelectric generator) have also been used in several successful satellite programs including the Nimbus program (1964–1978).

The thermal control subsystem helps protect electronic equipment from extreme temperatures due to intense sunlight or the lack of sun exposure on different sides of the satellite's body (e.g. optical solar reflector)

The attitude and orbit control subsystem consists of sensors to measure vehicle orientation, control laws embedded in the flight software, and actuators (reaction wheels, thrusters). These apply the torques and forces needed to re-orient the vehicle to a desired attitude, keep the satellite in the correct orbital position, and keep antennas pointed in the right directions.

The second major module is the communication payload, which is made up of transponders. A transponder is capable of :

When satellites reach the end of their mission (this normally occurs within 3 or 4 years after launch), satellite operators have the option of de-orbiting the satellite, leaving the satellite in its current orbit or moving the satellite to a graveyard orbit. Historically, due to budgetary constraints at the beginning of satellite missions, satellites were rarely designed to be de-orbited. One example of this practice is the satellite Vanguard 1. Launched in 1958, Vanguard 1, the 4th manmade satellite put in Geocentric orbit, was still in orbit , as well as the upper stage of its launch rocket.

Instead of being de-orbited, most satellites are either left in their current orbit or moved to a graveyard orbit. As of 2002, the FCC requires all geostationary satellites to commit to moving to a graveyard orbit at the end of their operational life prior to launch. In cases of uncontrolled de-orbiting, the major variable is the solar flux, and the minor variables the components and form factors of the satellite itself, and the gravitational perturbations generated by the Sun and the Moon (as well as those exercised by large mountain ranges, whether above or below sea level). The nominal breakup altitude due to aerodynamic forces and temperatures is 78 km, with a range between 72 and 84 km. Solar panels, however, are destroyed before any other component at altitudes between 90 and 95 km.

This list includes countries with an independent capability to place satellites in orbit, including production of the necessary launch vehicle. Note: many more countries have the capability to design and build satellites but are unable to launch them, instead relying on foreign launch services. This list does not consider those numerous countries, but only lists those capable of launching satellites indigenously, and the date this capability was first demonstrated. The list includes the European Space Agency, a multi-national state organization, but does not include private consortiums.



A few other private companies are capable of sub-orbital launches.

While Canada was the third country to build a satellite which was launched into space, it was launched aboard an American rocket from an American spaceport. The same goes for Australia, who launched first satellite involved a donated U.S. Redstone rocket and American support staff as well as a joint launch facility with the United Kingdom. The first Italian satellite San Marco 1 launched on 15 December 1964 on a U.S. Scout rocket from Wallops Island (Virginia, United States) with an Italian launch team trained by NASA. By similar occasions, almost all further first national satellites was launched by foreign rockets.

†-note: Both Chile and Belarus used Russian companies as principal contractors to build their satellites, they used Russian-Ukrainian manufactured rockets and launched either from Russia or Kazakhstan.


In recent times, satellites have been hacked by militant organizations to broadcast propaganda and to pilfer classified information from military communication networks.

For testing purposes, satellites in low earth orbit have been destroyed by ballistic missiles launched from earth. Russia, the United States and China have demonstrated the ability to eliminate satellites. In 2007 the Chinese military shot down an aging weather satellite, followed by the US Navy shooting down a defunct spy satellite in February 2008.

Due to the low received signal strength of satellite transmissions, they are prone to jamming by land-based transmitters. Such jamming is limited to the geographical area within the transmitter's range. GPS satellites are potential targets for jamming, but satellite phone and television signals have also been subjected to jamming.

Also, it is very easy to transmit a carrier radio signal to a geostationary satellite and thus interfere with the legitimate uses of the satellite's transponder. It is common for Earth stations to transmit at the wrong time or on the wrong frequency in commercial satellite space, and dual-illuminate the transponder, rendering the frequency unusable. Satellite operators now have sophisticated monitoring that enables them to pinpoint the source of any carrier and manage the transponder space effectively. 



</doc>
<doc id="27684" url="https://en.wikipedia.org/wiki?curid=27684" title="Steampunk">
Steampunk

Steampunk is a subgenre of science fiction or science fantasy that incorporates technology and aesthetic designs inspired by 19th-century industrial steam-powered machinery. Although its literary origins are sometimes associated with the cyberpunk genre, steampunk works are often set in an alternative history of the 19th century's British Victorian era or American "Wild West", in a future during which steam power has maintained mainstream usage, or in a fantasy world that similarly employs steam power. However, steampunk and Neo-Victorian are different in that the Neo-Victorian movement does not extrapolate on technology and embraces the positive aspects of the Victorian era's culture and philosophy.

Steampunk most recognizably features anachronistic technologies or retro-futuristic inventions as people in the 19th century might have envisioned them, and is likewise rooted in the era's perspective on fashion, culture, architectural style, and art. Such technology may include fictional machines like those found in the works of H. G. Wells and Jules Verne, or of the modern authors Philip Pullman, Scott Westerfeld, Stephen Hunt, and China Miéville. Other examples of steampunk contain alternative-history-style presentations of such technology as steam cannons, lighter-than-air airships, analogue computers, or such digital mechanical computers as Charles Babbage's Analytical Engine.

Steampunk may also incorporate additional elements from the genres of fantasy, horror, historical fiction, alternate history, or other branches of speculative fiction, making it often a hybrid genre. The first known appearance of the term "steampunk" was in 1987, though it now retroactively refers to many works of fiction created as far back as the 1950s or 1960s.

Steampunk also refers to any of the artistic styles, clothing fashions, or subcultures that have developed from the aesthetics of steampunk fiction, Victorian-era fiction, art nouveau design, and films from the mid-20th century. Various modern utilitarian objects have been modded by individual artisans into a pseudo-Victorian mechanical "steampunk" style, and a number of visual and musical artists have been described as steampunk.

Steampunk is influenced by and often adopts the style of the 19th-century scientific romances of Jules Verne, H. G. Wells, Mary Shelley, and Edward S. Ellis's "The Steam Man of the Prairies". Several more modern works of art and fiction significant to the development of the genre were produced before the genre had a name. "Titus Alone" (1959), by Mervyn Peake, is widely regarded by scholars as the first novel in the genre proper, while others point to Michael Moorcock's 1971 novel "The Warlord of the Air", which was heavily influenced by Peake's work. The film "Brazil" (1985) was an important early cinematic influence that helped codify the aesthetics of the genre. "The Adventures of Luther Arkwright" was an early (1970s) comic version of the Moorcock-style mover between timestreams.

In fine art, Remedios Varo's paintings combine elements of Victorian dress, fantasy, and technofantasy imagery. In television, one of the earliest manifestations of the steampunk ethos in the mainstream media was the CBS television series "The Wild Wild West" (1965–1969), which inspired the later film.

Although many works now considered seminal to the genre were published in the 1960s and 1970s, the term "steampunk" originated in the late 1980s as a tongue-in-cheek variant of "cyberpunk". It was coined by science fiction author K. W. Jeter, who was trying to find a general term for works by Tim Powers ("The Anubis Gates", 1983), James Blaylock ("Homunculus", 1986), and himself ("Morlock Night", 1979, and "Infernal Devices", 1987)—all of which took place in a 19th-century (usually Victorian) setting and imitated conventions of such actual Victorian speculative fiction as H. G. Wells' "The Time Machine". In a letter to science fiction magazine "Locus", printed in the April 1987 issue, Jeter wrote:

While Jeter's "Morlock Night" and "Infernal Devices", Powers' "The Anubis Gates", and Blaylock's "Lord Kelvin's Machine" were the first novels to which Jeter's neologism would be applied, the three authors gave the term little thought at the time. They were far from the first modern science fiction writers to speculate on the development of steam-based technology or alternative histories. Keith Laumer's "Worlds of the Imperium" (1962) and Ronald W. Clark's "Queen Victoria's Bomb" (1967) apply modern speculation to past-age technology and society. Michael Moorcock's "Warlord of the Air" (1971) is another early example. Harry Harrison's novel "A Transatlantic Tunnel, Hurrah!" (1973) portrays a British Empire of an alternative year 1973, full of atomic locomotives, coal-powered flying boats, ornate submarines, and Victorian dialogue. The Adventures of Luther Arkwright (mid-1970s) was the first steampunk comic. In February 1980, Richard A. Lupoff and Steve Stiles published the first "chapter" of their 10-part comic strip "The Adventures of Professor Thintwhistle and His Incredible Aether Flyer".

The first use of the word in a title was in Paul Di Filippo's 1995 "Steampunk Trilogy", consisting of three short novels: "Victoria", "Hottentots", and "Walt and Emily", which, respectively, imagine the replacement of Queen Victoria by a human/newt clone, an invasion of Massachusetts by Lovecraftian monsters, and a love affair between Walt Whitman and Emily Dickinson.
Superficially, steampunk may resemble retrofuturism. Indeed, both sensibilities recall "the older but still modern eras in which technological change seemed to anticipate a better world, one remembered as relatively innocent of industrial decline."

One of steampunk’s most significant contributions is the way in which it mixes digital media with traditional handmade art forms. As scholars Rachel Bowser and Brian Croxall put it, "the tinkering and tinker-able technologies within steampunk invite us to roll up our sleeves and get to work re-shaping our contemporary world." In this respect, steampunk bears more in common with DIY craft and making.

Many of the visualisations of steampunk have their origins with, among others, Walt Disney's film "20,000 Leagues Under the Sea" (1954), including the design of the story's submarine the "Nautilus", its interiors, and the crew's underwater gear; and George Pal's film "The Time Machine" (1960), especially the design of the time machine itself. This theme is also carried over to Six Flags Magic Mountain and Disney parks, in the themed area the "Screampunk District" at Six Flags Magic Mountain and in the designs of The Mysterious Island section of Tokyo DisneySea theme park and Disneyland Paris' Discoveryland area. 

Aspects of steampunk design emphasise a balance between form and function. In this it is like the Arts and Crafts Movement. But John Ruskin, William Morris, and the other reformers in the late nineteenth century rejected machines and industrial production. On the other hand, steampunk enthusiasts present a "non-luddite critique of technology".

Various modern utilitarian objects have been modified by enthusiasts into a pseudo-Victorian mechanical "steampunk" style. Examples include computer keyboards and electric guitars. The goal of such redesigns is to employ appropriate materials (such as polished brass, iron, wood, and leather) with design elements and craftsmanship consistent with the Victorian era, rejecting the aesthetic of industrial design.

In 1994, the Paris Metro station at Arts et Métiers was redesigned by Belgian artist Francois Schuiten in steampunk style, to honor the works of Jules Verne. The station is reminiscent of a submarine, sheathed in brass with giant cogs in the ceiling and portholes that look out onto fanciful scenes.
The artist group "Kinetic Steam Works" brought a working steam engine to the Burning Man festival in 2006 and 2007. The group's founding member, Sean Orlando, created a Steampunk Tree House (in association with a group of people who would later form the "Five Ton Crane Arts Group") that has been displayed at a number of festivals. The Steampunk Tree House is now permanently installed at the Dogfish Head Brewery in Milton, Delaware.

The Neverwas Haul is a three-story, self-propelled mobile art vehicle built to resemble a Victorian house on wheels. Designed by Shannon O’Hare, it was built by volunteers in 2006 and presented at the Burning Man festival from 2006 through 2015. When fully built, the Haul propelled itself at a top speed of 5 miles per hour and required a crew of ten people to operate safely. Currently, the Neverwas Haul makes her home at Obtainium Works, an "art car factory" in Vallejo, CA, owned by O’Hare and home to several other self-styled "contraptionists".

In May–June 2008, multimedia artist and sculptor Paul St George exhibited outdoor interactive video installations linking London and Brooklyn, New York, in a Victorian era-styled telectroscope. Utilizing this device, New York promoter Evelyn Kriete organised a transatlantic wave between steampunk enthusiasts from both cities, prior to White Mischief's "Around the World in 80 Days" steampunk-themed event.

In 2009, for Questacon, artist Tim Wetherell created a large wall piece that represented the concept of the clockwork universe. This steel artwork contains moving gears, a working clock, and a movie of the moon's terminator in action. The 3D moon movie was created by Antony Williams.

From October 2009 through February 2010, the Museum of the History of Science, Oxford, hosted the first major exhibition of steampunk art objects, curated and developed by New York artist and designer Art Donovan, who also exhibited his own "electro-futuristic" lighting sculptures, and presented by Dr. Jim Bennett, museum director. From redesigned practical items to fantastical contraptions, this exhibition showcased the work of eighteen steampunk artists from across the globe. The exhibition proved to be the most successful and highly attended in the museum's history and attracted more than eighty thousand visitors. The event was detailed in the official artist's journal "The Art of Steampunk", by curator Donovan.

In November 2010, The Libratory Steampunk Art Gallery was opened by Damien McNamara in Oamaru, New Zealand. Created from papier-mâché to resemble a large subterranean cave and filled with industrial equipment from yesteryear, rayguns, and general steampunk quirks, its purpose is to provide a place for steampunkers in the region to display artwork for sale all year long. A year later, a more permanent gallery, Steampunk HQ, was opened in the former Meeks Grain Elevator Building across the road from The Woolstore, and has since become a notable tourist attraction for Oamaru.

In 2012, the "Mobilis in Mobili: An Exhibition of Steampunk Art and Appliance" made its debut. Originally located at New York City's Wooster Street Social Club (itself the subject of the television series "NY Ink"), the exhibit featured working steampunk tattoo systems designed by Bruce Rosenbaum, of ModVic and owner of the Steampunk House, Joey "Dr. Grymm" Marsocci, and Christopher Conte. with different approaches. "[B]icycles, cell phones, guitars, timepieces and entertainment systems" rounded out the display. The opening night exhibition featured a live performance by steampunk band Frenchy and the Punk.

Steampunk fashion has no set guidelines but tends to synthesize modern styles with influences from the Victorian era. Such influences may include bustles, corsets, gowns, and petticoats; suits with waistcoats, coats, top hats and bowler hats (themselves originating in 1850 England), tailcoats and spats; or military-inspired garments. Steampunk-influenced outfits are usually accented with several technological and "period" accessories: timepieces, parasols, flying/driving goggles, and ray guns. Modern accessories like cell phones or music players can be found in steampunk outfits, after being modified to give them the appearance of Victorian-era objects. Post-apocalyptic elements, such as gas masks, ragged clothing, and tribal motifs, can also be included. Aspects of steampunk fashion have been anticipated by mainstream high fashion, the Lolita and aristocrat styles, neo-Victorianism, and the romantic goth subculture.
In 2005, Kate Lambert, known as "Kato", founded the first steampunk clothing company, "Steampunk Couture", mixing Victorian and post-apocalyptic influences. In 2013, IBM predicted, based on an analysis of more than a half million public posts on message boards, blogs, social media sites, and news sources, "that 'steampunk,' a subgenre inspired by the clothing, technology and social mores of Victorian society, will be a major trend to bubble up and take hold of the retail industry". Indeed, high fashion lines such as Prada, Dolce & Gabbana, Versace, Chanel, and Christian Dior had already been introducing steampunk styles on the fashion runways. And in episode 7 of "Lifetime"'s "Project Runway: Under the Gunn" reality series, contestants were challenged to create "avant-garde" "steampunk chic" looks. America's Next Top Model tackled Steampunk fashion in a 2012 episode where models competed in a Steampunk themed photo shoot, posing in front of a steam train while holding a live owl.

The educational book "Elementary BASIC - Learning to Program Your Computer in BASIC with Sherlock Holmes" (1981), by Henry Singer and Andrew Ledgar, may have been the first fictional work to depict the use of Charles Babbage's Analytical Engine in an adventure story. The instructional book, aimed at young programming students, depicts Holmes using the engine as an aid in his investigations, and lists programs that perform simple data processing tasks required to solve the fictional cases. The book even describes a device that allows the engine to be used remotely, over telegraph lines, as a possible enhancement to Babbage's machine. Companion volumes—"Elementary Pascal - Learning to Program Your Computer in Pascal with Sherlock Holmes" and "From Baker Street to Binary - An Introduction to Computers and Computer Programming with Sherlock Holmes"—were also written.
In 1988, the first version of the science fiction roleplaying game "" was published. The game is set in an alternative history in which certain now discredited Victorian scientific theories were probable and led to new technologies. Contributing authors included Frank Chadwick, Loren Wiseman, and Marcus Rowland.

William Gibson and Bruce Sterling's novel "The Difference Engine" (1990) is often credited with bringing about widespread awareness of steampunk. This novel applies the principles of Gibson and Sterling's cyberpunk writings to an alternative Victorian era where Ada Lovelace and Charles Babbage's proposed steam-powered mechanical computer, which Babbage called a difference engine (a later, more general-purpose version was known as an analytical engine), was actually built, and led to the dawn of the information age more than a century "ahead of schedule". This setting was different from most steampunk settings in that it takes a dim and dark view of this future, rather than the more prevalent utopian versions.

Nick Gevers's original anthology "Extraordinary Engines" (2008) features newer steampunk stories by some of the genre's writers, as well as other science fiction and fantasy writers experimenting with neo-Victorian conventions. A retrospective reprint anthology of steampunk fiction was released, also in 2008, by Tachyon Publications. Edited by Ann and Jeff VanderMeer and appropriately entitled "Steampunk", it is a collection of stories by James Blaylock, whose "Narbondo" trilogy is typically considered steampunk; Jay Lake, author of the novel "Mainspring", sometimes labeled "clockpunk"; the aforementioned Michael Moorcock; as well as Jess Nevins, known for his annotations to "The League of Extraordinary Gentlemen" (first published in 1999).

Younger readers have also been targeted by steampunk themes, by authors such as Philip Reeve and Scott Westerfeld. Reeve's quartet "Mortal Engines" is set far in Earth's future where giant moving cities consume each other in a battle for resources, a concept Reeve coined as "Municipal Darwinism". Westerfeld's "Leviathan" trilogy is set during an alternate First World War fought between the "clankers" (Central Powers), who use steam technology, and "darwinists" (Allied Powers), who use genetically engineered creatures instead of machines.

"Mash-ups" are also becoming increasingly popular in books aimed at younger readers, mixing steampunk with other genres. Suzanne Lazear's "Aether Chronicles" series mixes steampunk with faeries, and "The Unnaturalists", by Tiffany Trent, combines steampunk with mythological creatures and alternate history.

While most of the original steampunk works had a historical setting, later works often place steampunk elements in a fantasy world with little relation to any specific historic era. Historical steampunk tends to be science fiction that presents an alternate history; it also contains real locales and persons from history with alternative fantasy technology. "Fantasy-world steampunk", such as China Miéville's "Perdido Street Station", Alan Campbell's "Scar Night", and Stephen Hunt's Jackelian novels, on the other hand, presents steampunk in a completely imaginary fantasy realm, often populated by legendary creatures coexisting with steam-era and other anachronistic technologies. However, the works of China Miéville and similar authors are sometimes referred to as belonging to the "New Weird" rather than steampunk.

Self-described author of "far-fetched fiction" Robert Rankin has increasingly incorporated elements of steampunk into narrative worlds that are both Victorian and re-imagined contemporary. In 2009, he was made a Fellow of the Victorian Steampunk Society.

The comic book series "Hellboy", created by Mike Mignola, and the two Hellboy films featuring Ron Perlman and directed by Guillermo del Toro, all have steampunk elements. In the comic book and the first (2004) film, Karl Ruprecht Kroenen is a Nazi SS scientist who has an addiction to having himself surgically altered, and who has many mechanical prostheses, including a clockwork heart. The character Johann Krauss is featured in the comic and in the second film, "" (2008), as an ectoplasmic medium (a gaseous form in a partly mechanical suit). This second film also features the Golden Army itself, which is a collection of 4,900 mechanical steampunk warriors.

Since the 1990s, the application of the steampunk label has expanded beyond works set in recognisable historical periods, to works set in fantasy worlds that rely heavily on steam- or spring-powered technology. One of the earliest short stories relying on steam-powered flying machines is "The Aerial Burglar" of 1844. An example from juvenile fiction is "The Edge Chronicles" by Paul Stewart and Chris Riddell.

Fantasy steampunk settings abound in tabletop and computer role-playing games. Notable examples include "Skies of Arcadia", "", and "".

One of the first steampunk novels set in a Middle Earth-like world was the "Forest of Boland Light Railway" by BB, about gnomes who build a steam locomotive. 50 years later, Terry Pratchett wrote the Discworld novel "Raising Steam," about the ongoing industrial revolution and railway mania in Ankh-Morpork.

The gnomes and goblins in "World of Warcraft" also have technological societies that could be described as steampunk, as they are vastly ahead of the technologies of men, but still run on steam and mechanical power. 

The Dwarves of the "Elder Scrolls" series, described therein as a race of Elves called the Dwemer, also use steam powered machinery, with gigantic brass-like gears, throughout their underground cities. However, magical means are used to keep ancient devices in motion despite the Dwemer's ancient disappearance.

The 1998 game , as well as the other sequels including its 2014 reboot, feature heavy steampunk-inspired architecture, setting, and technology.

Amidst the historical and fantasy subgenres of steampunk is a type that takes place in a hypothetical future or a fantasy equivalent of our future involving the domination of steampunk-style technology and aesthetics. Examples include Jean-Pierre Jeunet and Marc Caro's "The City of Lost Children" (1995), "Turn A Gundam" (1999–2000), "Trigun", and Disney's film "Treasure Planet" (2002). In 2011, musician Thomas Dolby heralded his return to music after a 20-year hiatus with an online steampunk alternate fantasy world called the Floating City, to promote his album "A Map of the Floating City".

Another setting is "Western" steampunk, which overlaps with both the Weird West and Science fiction Western subgenres. One of the earliest steampunk books set in America was "The Steam Man of the Prairies" by Edward S. Ellis. Several other categories have arisen, sharing similar names, including dieselpunk, clockworkpunk, and others. Most of these terms were coined as supplements to the GURPS role playing game, and are not used in other contexts.

Kaja Foglio introduced the term "Gaslight Romance", gaslamp fantasy, which John Clute and John Grant define as "steampunk stories ... most commonly set in a romanticised, smoky, 19th-century London, as are Gaslight Romances. But the latter category focuses nostalgically on icons from the late years of that century and the early years of the 20th century—on Dracula, Jekyll and Hyde, Jack the Ripper, Sherlock Holmes and even Tarzan—and can normally be understood as combining supernatural fiction and recursive fantasy, though some gaslight romances can be read as fantasies of history." Author/artist James Richardson-Brown coined the term "steamgoth" to refer to steampunk expressions of fantasy and horror with a "darker" bent.

Mary Shelley's "The Last Man", set near the end of the 21st century after a plague had brought down civilization, was probably the ancestor of post-apocalyptic steampunk literature. Post-apocalyptic steampunk is set in a world where some cataclysm has precipitated the fall of civilization and steam power is once again ascendant, such as in Hayao Miyazaki's post-apocalyptic anime "Future Boy Conan" (1978), where a war fought with superweapons has devastated the planet. Robert Brown's novel, "The Wrath of Fate" (as well as much of Abney Park's music) is set in A Victorianesque world where an apocalypse was set into motion by a time-traveling mishap. Cherie Priest's Boneshaker series is set in a world where a zombie apocalypse happened during the Civil War era. The Peshawar Lancers by S.M. Stirling is set in a post-apocalyptic future in which a meteor shower in 1878 caused the collapse of Industrialized civilization. The movie 9 (which might be better classified as "stitchpunk" but was largely influenced by steampunk) is also set in a post-apocalyptic world after a self-aware war machine ran amok. "Steampunk Magazine" even published a book called "A Steampunk's Guide to the Apocalypse", about how steampunks could survive should such a thing actually happen.

In general, this category includes any recent science fiction that takes place in a recognizable historical period (sometimes an alternate history version of an actual historical period) in which the Industrial Revolution has already begun, but electricity is not yet widespread, "usually Britain of the early to mid-nineteenth century or the fantasized Wild West-era United States", with an emphasis on steam- or spring-propelled gadgets. The most common historical steampunk settings are the Victorian and Edwardian eras, though some in this "Victorian steampunk" category are set as early as the beginning of the Industrial Revolution and as late as the end of World War I.

Some examples of this type include the novel "The Difference Engine", the comic book series "League of Extraordinary Gentlemen", the Disney animated film "", Scott Westerfeld's "Leviathan" trilogy, and the roleplaying game "." The anime film "Steamboy" (2004) is another good example of Victorian steampunk, taking place in an alternate 1866 where steam technology is more advanced than it ever was in real life. Some, such as the comic series "Girl Genius", have their own unique times and places despite partaking heavily of the flavor of historic settings. Other comic series are set in a more familiar London, as in the "Victorian Undead", which has Sherlock Holmes, Doctor Watson, and others taking on zombies, Doctor Jekyll and Mister Hyde, and Count Dracula, with advanced weapons and devices.

Karel Zeman's film "The Fabulous World of Jules Verne" (1958) is a very early example of cinematic steampunk. Based on Jules Verne novels, Zeman's film imagines a past that never was, based on those novels. Other early examples of historical steampunk in cinema include Hayao Miyazaki's anime films such as "" (1986) and "Howl's Moving Castle" (2004), which contain many archetypal anachronisms characteristic of the steampunk genre.

"Historical" steampunk usually leans more towards science fiction than fantasy, but a number of historical steampunk stories have incorporated magical elements as well. For example, "Morlock Night", written by K. W. Jeter, revolves around an attempt by the wizard Merlin to raise King Arthur to save the Britain of 1892 from an invasion of Morlocks from the future.

Paul Guinan's "Boilerplate", a "biography" of a robot in the late 19th century, began as a website that garnered international press coverage when people began believing that Photoshop images of the robot with historic personages were real. The site was adapted into the illustrated hardbound book "Boilerplate: History's Mechanical Marvel", which was published by Abrams in October 2009. Because the story was not set in an alternative history, and in fact contained accurate information about the Victorian era, some booksellers referred to the tome as "historical steampunk".

Fictional settings inspired by Asian rather than Western history have been called "silkpunk". The term appears to originate with the author Ken Liu, who defined it as "a blend of science fiction and fantasy [that] draws inspiration from classical East Asian antiquity", with a "technology vocabulary (...) based on organic materials historically important to East Asia (bamboo, paper, silk) and seafaring cultures of the Pacific (coconut, feathers, coral)", rather than the brass and leather associated with steampunk. Other authors whose work has been described as silkpunk are JY Yang and Elizabeth Bear.

Steampunk music is very broadly defined. Abney Park’s lead singer Robert Brown defined it as "mixing Victorian elements and modern elements". There is a broad range of musical influences that make up the Steampunk sound, from industrial dance and world music to folk rock, dark cabaret to straightforward punk, Carnatic to industrial, hip-hop to opera (and even industrial hip-hop opera), darkwave to progressive rock, barbershop to big band.

Joshua Pfeiffer (of Vernian Process) is quoted as saying, "As for Paul Roland, if anyone deserves credit for spearheading Steampunk music, it is him. He was one of the inspirations I had in starting my project. He was writing songs about the first attempt at manned flight, and an Edwardian airship raid in the mid-80s long before almost anyone else..." Thomas Dolby is also considered one of the early pioneers of retro-futurist (i.e., Steampunk and Dieselpunk) music. Amanda Palmer was once quoted as saying, "Thomas Dolby is to Steampunk what Iggy Pop was to Punk!"

Steampunk has also appeared in the work of musicians who do not specifically identify as Steampunk. For example, the music video of "Turn Me On", by David Guetta and featuring Nicki Minaj, takes place in a Steampunk universe where Guetta creates human droids. Another music video is "The Ballad of Mona Lisa", by Panic! at the Disco, which has a distinct Victorian Steampunk theme. A continuation of this theme has in fact been used throughout the 2011 album "Vices & Virtues", in the music videos, album art, and tour set and costumes. In addition, the album "Clockwork Angels" (2012) and its supporting tour by progressive rock band Rush contain lyrics, themes, and imagery based around Steampunk. Similarly, Abney Park headlined the first "Steamstock" outdoor steampunk music festival in Richmond, California, which also featured Thomas Dolby, Frenchy and the Punk, Lee Presson and the Nails, Vernian Process, and others.

The music video for the Lindsey Stirling song "Roundtable Rival", has a Western Steampunk setting.

"The Fabulous World of Jules Verne" (1958), directed by Karel Zeman.

"The Fabulous Baron Munchausen" (1962), directed by Karel Zeman.

The 1965 television series "The Wild Wild West", as well as the 1999 film of the same name, features many elements of advanced steam-powered technology set in the Wild West time period of the United States.

"Two Years' Vacation" (or "The Stolen Airship") (1967) directed by Karel Zeman

The BBC series "Doctor Who" also incorporates steampunk elements. During season 14 of the show (in 1976), the formerly futuristic looking interior set was replaced with a Victorian-styled wood-panel and brass affair. In the 1996 American co-production, the TARDIS interior was re-designed to resemble an almost Victorian library with the central control console made up of an eclectic array of anachronistic objects. Modified and streamlined for the 2005 revival of the series, the TARDIS console continued to incorporate steampunk elements, including a Victorian typewriter and gramophone. Several storylines can be classed as steampunk, for example: "The Evil of the Daleks" (1966), wherein Victorian scientists invent a time travel device.

"Dinner for Adele" (1977), directed by Oldřich Lipský

The 1979 film "Time After Time" has Herbert George "H.G." Wells following a surgeon named John Leslie Stevenson into the future, as John is suspected of being Jack the Ripper. Both separately use Wells's time machine to travel.

"The Mysterious Castle in the Carpathians" (1981), directed by Oldřich Lipský

The 1982 American TV series "Q.E.D." is set in Edwardian England, stars Sam Waterston as Professor Quentin Everett Deverill (from whose initials, by which he is primarily known, the series title is derived, initials which also stand for the Latin phrase "quod erat demonstrandum", which translates as "which was to be demonstrated"). The Professor is an inventor and scientific detective, in the mold of Sherlock Holmes.

The plot of the Soviet film "Kin-dza-dza!" (1986) centers on a desert planet, depleted of its resources, where an impoverished dog-eat-dog society uses steam-punk machines, the movements and functions of which defy earthly logic.

In making his 1986 Japanese film "Castle in the Sky", Hayao Miyazaki was heavily influenced by steampunk culture, the film featuring various air ships and steam-powered contraptions as well as a mysterious island that floats through the sky, accomplished not through magic as in most stories, but instead by harnessing the physical properties of a rare crystal—analogous to the lodestone used in the Laputa of Swift's Gulliver's Travels—augmented by massive propellers, as befitting the Victorian motif.

The first "Wallace & Grommet" animation "A Grand Day Out" (1989) features a space rocket in the steampunk style.

"The Adventures of Brisco County, Jr.", a 1993 Fox Network TV science fiction-western set in the 1890s, features elements of steampunk as represented by the character Professor Wickwire, whose inventions were described as "the coming thing".

The short-lived 1995 TV show "Legend", on UPN, set in 1876 Arizona, features such classic inventions as a steam-driven "quadrovelocipede" and night-vision goggles, and stars John de Lancie as a thinly disguised Nikola Tesla.

Alan Moore's and Kevin O'Neill's 1999 "The League of Extraordinary Gentlemen" graphic novel series (and the subsequent 2003 film adaption) greatly popularised the steampunk genre.

Steamboy (2004) is a Japanese animated action film directed and co-written by Katsuhiro Otomo ("Akira"). It is a retro science-fiction epic set in a Steampunk Victorian England. It features steamboats, trains, airships and inventors.

The 2007 Syfy miniseries "Tin Man" incorporates a considerable number of steampunk-inspired themes into a re-imagining of L. Frank Baum's "The Wonderful Wizard of Oz".

Despite leaning more towards gothic influences, the "parallel reality" of Meanwhile City, within the 2009 film "Franklyn", contains many steampunk themes, such as costumery, architecture, minimal use of electricity (with a preference for gaslight), and absence of modern technology (such as there being no motorised vehicles or advanced weaponry, and the manual management of information with no use of computers).

The 2009–2014 Syfy television series "Warehouse 13" features many steampunk-inspired objects and artifacts, including computer designs created by steampunk artisan Richard Nagy, a.k.a. "Datamancer".

The 2010 episode of the TV series "Castle" entitled "Punked" (which first aired on October 11, 2010) prominently features the steampunk subculture and uses Los Angeles-area steampunks (such as the League of STEAM) as extras.

The 2011 film "The Three Musketeers" has many steampunk elements, including gadgets and airships. 

"The Legend of Korra", a 2012–2014 Nickelodeon animated series, incorporates steampunk elements in an industrialized world with East Asian themes.
The "Penny Dreadful" (2014) television series is a Gothic Victorian fantasy series with steampunk props and costumes.

The 2015 GSN reality television game show "Steampunk'd" features a competition to create steampunk-inspired art and designs which are judged by notable Steampunks Thomas Willeford, Kato, and Matt King.

Based on the work of cartoonist Jacques Tardi, "April and the Extraordinary World" (2015) is an animated movie set in a steampunk Paris. It features airships, trains, submarines, and various other steam-powered contraptions.

Tim Burton's 2016 film "Alice Through the Looking Glass" features steampunk costumes, props, and vehicles.

A variety of styles of video games have used steampunk settings.

"The Chaos Engine" (1993) is a run and gun video game inspired by the Gibson/Sterling novel "The Difference Engine" (1990), set in a Victorian steampunk age. Developed by the Bitmap Brothers, it was first released on the Amiga in 1993; a sequel was released in 1996.

The graphic adventure puzzle video games "Myst" (1993), "Riven" (1997), and "" (2001) (all produced by Cyan Worlds) take place in an alternate steampunk universe, where elaborate infrastructures have been built to run on steam power.

The "SteamWorld" series of games has the player controlling steam-powered robots.

Both "" and its sequel, "Thief II" are set in a steampunk metropolis.

"Guns of Icarus Online" (2012) is multiplayer game with steampunk thematic.

"BioShock Infinite" (2013) is a FPS game set in 1912, in a fictional city called Columbia, which uses technology to float in the sky and has many historical and religious scenes.

"" (2014), a Japanese otome game for the PS Vita is set in a steampunk Victorian London, and features a cast with several historical figures with steampunk aesthetics.

"Dishonored" and "Dishonored 2" are set within a fictional world with heavy steampunk influences, wherein whale oil, as opposed to coal, served as catalyst of their industrial revolution.

Mattel's "Monster High" dolls Rebecca Steam and Hexiciah Steam. 

The "Pullip Dolls" by Japanese manufacturer Dal have a steampunk range.

Because of the popularity of steampunk, there is a growing movement of adults that want to establish steampunk as a culture and lifestyle. Some fans of the genre adopt a steampunk aesthetic through fashion, home decor, music, and film. While Steampunk is considered the amalgamation of Victorian aesthetic principles with modern sensibilities and technologies, it can be more broadly categorised as neo-Victorianism, described by scholar Marie-Luise Kohlke as "the afterlife of the nineteenth century in the cultural imaginary". The subculture has its own magazine, blogs, and online shops.

In September 2012, a panel, chaired by steampunk entertainer Veronique Chevalier and with panelists including magician Pop Hadyn and members of the steampunk performance group the League of STEAM, was held at Stan Lee's Comikaze Expo. The panel suggested that because steampunk was inclusive of and incorporated ideas from various other subcultures such as goth, neo-Victorian, and cyberpunk, as well as a growing number of fandoms, it was fast becoming a "super-culture" rather than a mere subculture. Other steampunk notables such as Professor Elemental have expressed similar views about steampunk's inclusive diversity.
Some have proposed a steampunk philosophy that incorporates punk-inspired anti-establishment sentiments typically bolstered by optimism about human potential.

Steampunk became a common descriptor for homemade objects sold on the craft network Etsy between 2009 and 2011, though many of the objects and fashions bear little resemblance to earlier established descriptions of steampunk. Thus the craft network may not strike observers as "sufficiently steampunk" to warrant its use of the term. Comedian April Winchell, author of the book "Regretsy: Where DIY meets WTF", cataloged some of the most egregious and humorous examples on her website "Regretsy". The blog was popular among steampunks and even inspired a music video that went viral in the community and was acclaimed by steampunk "notables".

2006 saw the first "SalonCon", a neo-Victorian/steampunk convention. It ran for three consecutive years and featured artists, musicians (Voltaire and Abney Park), authors (Catherynne M. Valente, Ekaterina Sedia, and G. D. Falksen), salons led by people prominent in their respective fields, workshops and panels on steampunk—as well as a seance, ballroom dance instruction, and the Chrononauts' Parade. The event was covered by MTV and "The New York Times". Since then, a number of popular steampunk conventions have sprung up the world over, with names like Steamcon (Seattle, WA), the Steampunk World's Fair (Piscataway, NJ), Up in the Aether: The Steampunk Convention (Dearborn, MI), Steampunk NZ (Oamaru, New Zealand), Steampunk Unlimited (Strasburg Railroad, Lancaster, PA). Each year, on Mother's Day weekend, the city of Waltham, MA, turns over its city center and surrounding areas to host the Watch City Steampunk Festival, a US outdoor steampunk festival.
In recent years, steampunk has also become a regular feature at San Diego Comic-Con International, with the Saturday of the four-day event being generally known among steampunks as "Steampunk Day", and culminating with a photo-shoot for the local press. In 2010, this was recorded in the Guinness Book of World Records as the world's largest steampunk photo shoot. In 2013, Comic-Con announced four official 2013 T-shirts, one of them featuring the official Rick Geary Comic-Con toucan mascot in steampunk attire. The Saturday steampunk "after-party" has also become a major event on the steampunk social calendar: in 2010, the headliners included The Slow Poisoner, Unextraordinary Gentlemen, and Voltaire, with Veronique Chevalier as Mistress of Ceremonies and special appearance by the League of STEAM; in 2011, UXG returned with Abney Park.

Steampunk has also sprung up recently at Renaissance Festivals and Renaissance Faires, in the US. Some festivals have organised events or a "Steampunk Day", while others simply support an open environment for donning steampunk attire. The Bristol Renaissance Faire in Kenosha, Wisconsin, on the Wisconsin/Illinois border, featured a Steampunk costume contest during the 2012 season, the previous two seasons having seen increasing participation in the phenomenon.

Steampunk also has a growing following in the UK and Europe. The largest European event is "Weekend at the Asylum", held at The Lawn, Lincoln, every September since 2009. Organised as a not-for-profit event by the Victorian Steampunk Society, the Asylum is a dedicated steampunk event which takes over much of the historical quarter of Lincoln, England, along with Lincoln Castle. In 2011, there were over 1000 steampunks in attendance. The event features the Empire Ball, Majors Review, Bazaar Eclectica, and the international Tea Duelling final.





</doc>
<doc id="27686" url="https://en.wikipedia.org/wiki?curid=27686" title="Spreadsheet">
Spreadsheet

A spreadsheet is an interactive computer application for organization, analysis and storage of data in tabular form. Spreadsheets are developed as computerized simulations of paper accounting worksheets. The program operates on data entered in cells of a table. Each cell may contain either numeric or text data, or the results of formulas that automatically calculate and display a value based on the contents of other cells. A spreadsheet may also refer to one such electronic document.

Spreadsheet users can adjust any stored value and observe the effects on calculated values. This makes the spreadsheet useful for "what-if" analysis since many cases can be rapidly investigated without manual recalculation. Modern spreadsheet software can have multiple interacting sheets, and can display data either as text and numerals, or in graphical form.

Besides performing basic arithmetic and mathematical functions, modern spreadsheets provide built-in functions for common financial and statistical operations. Such calculations as net present value or standard deviation can be applied to tabular data with a pre-programmed function in a formula. Spreadsheet programs also provide conditional expressions, functions to convert between text and numbers, and functions that operate on strings of text.

Spreadsheets have replaced paper-based systems throughout the business world. Although they were first developed for accounting or bookkeeping tasks, they now are used extensively in any context where tabular lists are built, sorted, and shared.

LANPAR, available in 1969, was the first electronic spreadsheet on mainframe and time sharing computers. LANPAR was an acronym: LANguage for Programming Arrays at Random. VisiCalc was the first electronic spreadsheet on a microcomputer, and it helped turn the Apple II computer into a popular and widely used system. Lotus 1-2-3 was the leading spreadsheet when DOS was the dominant operating system. Excel now has the largest market share on the Windows and Macintosh platforms. A spreadsheet program is a standard feature of an office productivity suite; since the advent of web apps, office suites now also exist in web app form. Web based spreadsheets are a relatively new category.

A spreadsheet consists of a table of "cells" arranged into rows and columns and referred to by the X and Y locations. X locations, the columns, are normally represented by letters, "A", "B", "C", etc., while rows are normally represented by numbers, 1, 2, 3, etc. A single cell can be referred to by addressing its row and column, "C10" for instance. This electronic concept of cell references was first introduced in LANPAR (Language for Programming Arrays at Random) (co-invented by Rene Pardo and Remy Landau) and a variant used in VisiCalc, and known as "A1 notation". Additionally, spreadsheets have the concept of a "range", a group of cells, normally contiguous. For instance, one can refer to the first ten cells in the first column with the range "A1:A10". LANPAR innovated forward referencing/natural order calculation which didn't re-appear until Lotus 123 and Microsoft's MultiPlan Version 2.

In modern spreadsheet applications, several spreadsheets, often known as "worksheets" or simply "sheets", are gathered together to form a "workbook". A workbook is physically represented by a file, containing all the data for the book, the sheets and the cells with the sheets. Worksheets are normally represented by tabs that flip between pages, each one containing one of the sheets, although Numbers changes this model significantly. Cells in a multi-sheet book add the sheet name to their reference, for instance, "Sheet 1!C10". Some systems extend this syntax to allow cell references to different workbooks.

Users interact with sheets primarily through the cells. A given cell can hold data by simply entering it in, or a formula, which is normally created by preceding the text with an equals sign. Data might include the string of text codice_1, the number codice_2 or the date codice_3. A formula would begin with the equals sign, codice_4, but this would normally be invisible because the display shows the "result" of the calculation, codice_5 in this case, not the formula itself. This may lead to confusion in some cases.

The key feature of spreadsheets is the ability for a formula to refer to the contents of other cells, which may in turn be the result of a formula. To make such a formula, one simply replaces a number with a cell reference. For instance, the formula codice_6 would produce the result of multiplying the value in cell C10 by the number 5. If C10 holds the value codice_7 the result will be codice_5. But C10 might also hold its own formula referring to other cells, and so on.

The ability to chain formulas together is what gives a spreadsheet its power. Many problems can be broken down into a series of individual mathematical steps, and these can be assigned to individual formulas in cells. Some of these formulas can apply to ranges as well, like the codice_9 function that adds up all the numbers within a range.

Spreadsheets share many principles and traits of databases, but spreadsheets and databases are not the same thing. A spreadsheet is essentially just one table, whereas a database is a collection of many tables with machine-readable semantic relationships between them. While it is true that a workbook that contains three sheets is indeed a file containing multiple tables that can interact with each other, it lacks the relational structure of a database. Spreadsheets and databases are interoperable—sheets can be imported into databases to become tables within them, and database queries can be exported into spreadsheets for further analysis.

A spreadsheet program is one of the main components of an office productivity suite, which usually also contains a word processor, a presentation program, and a database management system. Programs within a suite use similar commands for similar functions. Usually sharing data between the components is easier than with a non-integrated collection of functionally equivalent programs. This was particularly an advantage at a time when many personal computer systems used text-mode displays and commands, instead of a graphical user interface.

The word "spreadsheet" came from "spread" in its sense of a newspaper or magazine item (text or graphics) that covers two facing pages, extending across the center fold and treating the two pages as one large one. The compound word "spread-sheet" came to mean the format used to present book-keeping ledgers—with columns for categories of expenditures across the top, invoices listed down the left margin, and the amount of each payment in the cell where its row and column intersect—which were, traditionally, a "spread" across facing pages of a bound ledger (book for keeping accounting records) or on oversized sheets of paper (termed "analysis paper") ruled into rows and columns in that format and approximately twice as wide as ordinary paper.

A batch "spreadsheet" is indistinguishable from a batch compiler with added input data, producing an output report, "i.e.", a 4GL or conventional, non-interactive, batch computer program. However, this concept of an electronic spreadsheet was outlined in the 1961 paper "Budgeting Models and System Simulation" by Richard Mattessich. The subsequent work by Mattessich (1964a, Chpt. 9, "Accounting and Analytical Methods") and its companion volume, Mattessich (1964b, "Simulation of the Firm through a Budget Computer Program") applied computerized spreadsheets to accounting and budgeting systems (on mainframe computers programmed in FORTRAN IV). These batch Spreadsheets dealt primarily with the addition or subtraction of entire columns or rows (of input variables), rather than individual "cells".

In 1962 this concept of the spreadsheet, called BCL for Business Computer Language, was implemented on an IBM 1130 and in 1963 was ported to an IBM 7040 by R. Brian Walsh at Marquette University, Wisconsin. This program was written in Fortran. Primitive timesharing was available on those machines. In 1968 BCL was ported by Walsh to the IBM 360/67 timesharing machine at Washington State University. It was used to assist in the teaching of finance to business students. Students were able to take information prepared by the professor and manipulate it to represent it and show ratios etc. In 1964, a book entitled "Business Computer Language" was written by Kimball, Stoffells and Walsh and both the book and program were copyrighted in 1966 and years later that copyright was renewed

Applied Data Resources had a FORTRAN preprocessor called Empires.

In the late 1960s Xerox used BCL to develop a more sophisticated version for their timesharing system.

A key invention in the development of electronic spreadsheets was made by Rene K. Pardo and Remy Landau, who filed in 1970 on a spreadsheet automatic natural order calculation algorithm. While the patent was initially rejected by the patent office as being a purely mathematical invention, following 12 years of appeals, Pardo and Landau won a landmark court case at the Predecessor Court of the Federal Circuit (CCPA), overturning the Patent Office in 1983 — establishing that "something does not cease to become patentable merely because the point of novelty is in an algorithm." However, in 1995 the United States Court of Appeals for the Federal Circuit ruled the patent unenforceable.

The actual software was called LANPAR3 — LANguage for Programming Arrays at Random. This was conceived and entirely developed in the summer of 1969, following Pardo and Landau's recent graduation from Harvard University. Co-inventor Rene Pardo recalls that he felt that one manager at Bell Canada should not have to depend on programmers to program and modify budgeting forms, and he thought of letting users type out forms in any order and having an electronic computer calculate results in the right order ("Forward Referencing/Natural Order Calculation"). Pardo and Landau developed and implemented the software in 1969.

LANPAR was used by Bell Canada, AT&T and the 18 operating telephone companies nationwide for their local and national budgeting operations. LANPAR was also used by General Motors. Its uniqueness was Pardo's co-invention incorporating forward referencing/natural order calculation (one of the first "non-procedural" computer languages) as opposed to left-to-right, top to bottom sequence for calculating the results in each cell that was used by VisiCalc, SuperCalc, and the first version of Multiplan. Without forward referencing/natural order calculation, the user had to manually recalculate the spreadsheet as many times as necessary until the values in all the cells had stopped changing. Forward referencing/natural order calculation by a compiler was the cornerstone functionality required for any spreadsheet to be practical and successful.

The LANPAR system was implemented on GE400 and Honeywell 6000 online timesharing systems, enabling users to program remotely via computer terminals and modems. Data could be entered dynamically either by paper tape, specific file access, on line, or even external data bases. Sophisticated mathematical expressions, including logical comparisons and "if/then" statements, could be used in any cell, and cells could be presented in any order.

In 1968, three former employees from the General Electric computer company headquartered in Phoenix, Arizona set out to start their own software development house. A. Leroy Ellison, Harry N. Cantrell, and Russell E. Edwards found themselves doing a large number of calculations when making tables for the business plans that they were presenting to venture capitalists. They decided to save themselves a lot of effort and wrote a computer program that produced their tables for them. This program, originally conceived as a simple utility for their personal use, would turn out to be the first software product offered by the company that would become known as Capex Corporation. "AutoPlan" ran on GE’s Time-sharing service; afterward, a version that ran on IBM mainframes was introduced under the name "AutoTab". (National CSS offered a similar product, CSSTAB, which had a moderate timesharing user base by the early 1970s. A major application was opinion research tabulation.)

AutoPlan/AutoTab was not a WYSIWYG interactive spreadsheet program, it was a simple scripting language for spreadsheets. The user defined the names and labels for the rows and columns, then the formulas that defined each row or column. In 1975, Autotab-II was advertised as extending the original to a maximum of ""1,500 rows and columns, combined in any proportion the user requires...""

The IBM Financial Planning and Control System was developed in 1976, by Brian Ingham at IBM Canada. It was implemented by IBM in at least 30 countries. It ran on an IBM mainframe and was among the first applications for financial planning developed with APL that completely hid the programming language from the end-user. Through IBM's VM operating system, it was among the first programs to auto-update each copy of the application as new versions were released. Users could specify simple mathematical relationships between rows and between columns. Compared to any contemporary alternatives, it could support very large spreadsheets. It loaded actual financial data drawn from the legacy batch system into each user's spreadsheet on a monthly basis. It was designed to optimize the power of APL through object kernels, increasing program efficiency by as much as 50 fold over traditional programming approaches.

An example of an early "industrial weight" spreadsheet was APLDOT, developed in 1976 at the United States Railway Association on an IBM 360/91, running at The Johns Hopkins University Applied Physics Laboratory in Laurel, MD. The application was used successfully for many years in developing such applications as financial and costing models for the US Congress and for Conrail. APLDOT was dubbed a "spreadsheet" because financial analysts and strategic planners used it to solve the same problems they addressed with paper spreadsheet pads.

Because of Dan Bricklin and Bob Frankston's implementation of VisiCalc on the Apple II in 1979 and the IBM PC in 1981, the spreadsheet concept became widely known in the late 1970s and early 1980s. VisiCalc was the first spreadsheet that combined all essential features of modern spreadsheet applications (except for forward referencing/natural order recalculation), such as WYSIWYG interactive user interface, automatic recalculation, status and formula lines, range copying with relative and absolute references, formula building by selecting referenced cells. Unaware of LANPAR at the time "PC World" magazine called VisiCalc the first electronic spreadsheet.

Bricklin has spoken of watching his university professor create a table of calculation results on a blackboard. When the professor found an error, he had to tediously erase and rewrite a number of sequential entries in the table, triggering Bricklin to think that he could replicate the process on a computer, using the blackboard as the model to view results of underlying formulas. His idea became VisiCalc, the first application that turned the personal computer from a hobby for computer enthusiasts into a business tool.

VisiCalc went on to become the first "killer application", an application that was so compelling, people would buy a particular computer just to use it. VisiCalc was in no small part responsible for the Apple II's success. The program was later ported to a number of other early computers, notably CP/M machines, the Atari 8-bit family and various Commodore platforms. Nevertheless, VisiCalc remains best known as an Apple II program.

SuperCalc was a spreadsheet application published by Sorcim in 1980, and originally bundled (along with WordStar) as part of the CP/M software package included with the Osborne 1 portable computer. It quickly became the de facto standard spreadsheet for CP/M and was ported to MS-DOS in 1982.

The acceptance of the IBM PC following its introduction in August, 1981, began slowly, because most of the programs available for it were translations from other computer models. Things changed dramatically with the introduction of Lotus 1-2-3 in November, 1982, and release for sale in January, 1983. Since it was written especially for the IBM PC, it had good performance and became the killer app for this PC. Lotus 1-2-3 drove sales of the PC due to the improvements in speed and graphics compared to VisiCalc on the Apple II.

Lotus 1-2-3, along with its competitor Borland Quattro, soon displaced VisiCalc. Lotus 1-2-3 was released on January 26, 1983, started outselling then-most-popular VisiCalc the very same year, and for a number of years was the leading spreadsheet for DOS.

Microsoft released the first version of Excel for the Macintosh on September 30, 1985, and then ported it to Windows, with the first version being numbered 2.05 (to synchronize with the Macintosh version 2.2) and released in November 1987. The Windows 3.x platforms of the early 1990s made it possible for Excel to take market share from Lotus. By the time Lotus responded with usable Windows products, Microsoft had begun to assemble their Office suite. By 1995, Excel was the market leader, edging out Lotus 1-2-3, and in 2013, IBM discontinued Lotus 1-2-3 altogether.

With the advent of advanced web technologies such as Ajax circa 2005, a new generation of online spreadsheets has emerged. Equipped with a rich Internet application user experience, the best web based online spreadsheets have many of the features seen in desktop spreadsheet applications. Some of them such as EditGrid, Google Sheets, Microsoft Excel Online, Smartsheet, or Zoho Office Suite also have strong multi-user collaboration features or offer real time updates from remote sources such as stock prices and currency exchange rates.

Gnumeric is a free, cross-platform spreadsheet program that is part of the GNOME Free Software Desktop Project. OpenOffice.org Calc and the closely related LibreOffice Calc (using the LGPL license) are free and open-source spreadsheets.

Notable current spreadsheet software:
Discontinued spreadsheet software:

A number of companies have attempted to break into the spreadsheet market with programs based on very different paradigms. Lotus introduced what is likely the most successful example, Lotus Improv, which saw some commercial success, notably in the financial world where its powerful data mining capabilities remain well respected to this day.

Spreadsheet 2000 attempted to dramatically simplify formula construction, but was generally not successful.

The main concepts are those of a grid of cells, called a sheet, with either raw data, called values, or formulas in the cells. Formulas say how to mechanically compute new values from existing values. Values are generally numbers, but can also be pure text, dates, months, etc. Extensions of these concepts include logical spreadsheets. Various tools for programming sheets, visualizing data, remotely connecting sheets, displaying cells' dependencies, etc. are commonly provided.

A "cell" can be thought of as a box for holding data. A single cell is usually referenced by its column and row (A2 would represent the cell containing the value 10 in the example table below). Usually rows, representing the dependent variables, are referenced in decimal notation starting from 1, while columns representing the independent variables use 26-adic bijective numeration using the letters A-Z as numerals. Its physical size can usually be tailored to its content by dragging its height or width at box intersections (or for entire columns or rows by dragging the column- or row-headers).

An array of cells is called a "sheet" or "worksheet". It is analogous to an array of variables in a conventional computer program (although certain unchanging values, once entered, could be considered, by the same analogy, constants). In most implementations, many worksheets may be located within a single spreadsheet. A worksheet is simply a subset of the spreadsheet divided for the sake of clarity. Functionally, the spreadsheet operates as a whole and all cells operate as global variables within the spreadsheet (each variable having 'read' access only except its own containing cell).

A cell may contain a value or a formula, or it may simply be left empty.
By convention, formulas usually begin with = sign.

A value can be entered from the computer keyboard by directly typing into the cell itself. Alternatively, a value can be based on a formula (see below), which might perform a calculation, display the current date or time, or retrieve external data such as a stock quote or a database value.

The Spreadsheet "Value Rule"

Computer scientist Alan Kay used the term "value rule" to summarize a spreadsheet's operation: a cell's value relies solely on the formula the user has typed into the cell. The formula may rely on the value of other cells, but those cells are likewise restricted to user-entered data or formulas. There are no 'side effects' to calculating a formula: the only output is to display the calculated result inside its occupying cell. There is no natural mechanism for permanently modifying the contents of a cell unless the user manually modifies the cell's contents. In the context of programming languages, this yields a limited form of first-order functional programming.
A standard of spreadsheets since the 1980s, this optional feature eliminates the need to manually request the spreadsheet program to recalculate values (nowadays typically the default option unless specifically 'switched off' for large spreadsheets, usually to improve performance). Some earlier spreadsheets required a manual request to recalculate, since recalculation of large or complex spreadsheets often reduced data entry speed. Many modern spreadsheets still retain this option.

Recalculation generally requires that there are no circular dependencies in a spreadsheet. A dependency graph is a graph that has a vertex for each object to be updated, and an edge connecting two objects whenever one of them needs to be updated earlier than the other. Dependency graphs without circular dependencies form directed acyclic graphs, representations of partial orderings (in this case, across a spreadsheet) that can be relied upon to give a definite result.

This feature refers to updating a cell's contents periodically with a value from an external source—such as a cell in a "remote" spreadsheet. For shared, Web-based spreadsheets, it applies to "immediately" updating cells another user has updated. All dependent cells must be updated also.

Once entered, selected cells (or the entire spreadsheet) can optionally be "locked" to prevent accidental overwriting. Typically this would apply to cells containing formulas but might be applicable to cells containing "constants" such as a kilogram/pounds conversion factor (2.20462262 to eight decimal places). Even though individual cells are marked as locked, the spreadsheet data are not protected until the feature is activated in the file preferences.

A cell or range can optionally be defined to specify how the value is displayed. The default display format is usually set by its initial content if not specifically previously set, so that for example "31/12/2007" or "31 Dec 2007" would default to the cell format of "date".
Similarly adding a % sign after a numeric value would tag the cell as a percentage cell format. The cell contents are not changed by this format, only the displayed value.

Some cell formats such as "numeric" or "currency" can also specify the number of decimal places.

This can allow invalid operations (such as doing multiplication on a cell containing a date), resulting in illogical results without an appropriate warning.

Depending on the capability of the spreadsheet application, each cell (like its counterpart the "style" in a word processor) can be separately formatted using the attributes of either the content (point size, color, bold or italic) or the cell (border thickness, background shading, color). To aid the readability of a spreadsheet, cell formatting may be conditionally applied to data; for example, a negative number may be displayed in red.

A cell's formatting does not typically affect its content and depending on how cells are referenced or copied to other worksheets or applications, the formatting may not be carried with the content.

In most implementations, a cell, or group of cells in a column or row, can be "named" enabling the user to refer to those cells by a name rather than by a grid reference. Names must be unique within the spreadsheet, but when using multiple sheets in a spreadsheet file, an identically named cell range on each sheet can be used if it is distinguished by adding the sheet name. One reason for this usage is for creating or running macros that repeat a command across many sheets. Another reason is that formulas with named variables are readily checked against the algebra they are intended to implement (they resemble Fortran expressions). Use of named variables and named functions also makes the spreadsheet structure more transparent.

In place of a named cell, an alternative approach is to use a cell (or grid) reference. Most cell references indicate another cell in the same spreadsheet, but a cell reference can also refer to a cell in a different sheet within the same spreadsheet, or (depending on the implementation) to a cell in another spreadsheet entirely, or to a value from a remote application.

A typical cell reference in "A1" style consists of one or two case-insensitive letters to identify the column (if there are up to 256 columns: A–Z and AA–IV) followed by a row number (e.g., in the range 1–65536). Either part can be relative (it changes when the formula it is in is moved or copied), or absolute (indicated with $ in front of the part concerned of the cell reference). The alternative "R1C1" reference style consists of the letter R, the row number, the letter C, and the column number; relative row or column numbers are indicated by enclosing the number in square brackets. Most current spreadsheets use the A1 style, some providing the R1C1 style as a compatibility option.

When the computer calculates a formula in one cell to update the displayed value of that cell, cell reference(s) in that cell, naming some other cell(s), cause the computer to fetch the value of the named cell(s).

A cell on the same "sheet" is usually addressed as:

A cell on a different sheet of the same spreadsheet is usually addressed as:

Some spreadsheet implementations in Excel allow a cell references to another spreadsheet (not the current open and active file) on the same computer or a local network. It may also refer to a cell in another open and active spreadsheet on the same computer or network that is defined as shareable. These references contain the complete filename, such as:

In a spreadsheet, references to cells automatically update when new rows or columns are inserted or deleted. Care must be taken, however, when adding a row immediately before a set of column totals to ensure that the totals reflect the additional rows values—which they often do not.

A circular reference occurs when the formula in one cell refers—directly, or indirectly through a chain of cell references—to another cell that refers back to the first cell. Many common errors cause circular references. However, some valid techniques use circular references. These techniques, after many spreadsheet recalculations, (usually) converge on the correct values for those cells.

Likewise, instead of using a named range of cells, a range reference can be used. Reference to a range of cells is typically of the form (A1:A6), which specifies all the cells in the range A1 through to A6. A formula such as "=SUM(A1:A6)" would add all the cells specified and put the result in the cell containing the formula itself.

In the earliest spreadsheets, cells were a simple two-dimensional grid. Over time, the model has expanded to include a third dimension, and in some cases a series of named grids, called sheets. The most advanced examples allow inversion and rotation operations which can slice and project the data set in various ways.

A formula identifies the calculation needed to place the result in the cell it is contained within. A cell containing a formula therefore has two display components; the formula itself and the resulting value. The formula is normally only shown when the cell is selected by "clicking" the mouse over a particular cell; otherwise it contains the result of the calculation.

A formula assigns values to a cell or range of cells, and typically has the format:

where the expression consists of:

When a cell contains a formula, it often contains references to other cells. Such a cell reference is a type of variable. Its value is the value of the referenced cell or some derivation of it. If that cell in turn references other cells, the value depends on the values of those. References can be relative (e.g., codice_13, or codice_14), absolute (e.g., codice_25, or codice_26) or mixed row– or column-wise absolute/relative (e.g., codice_27 is column-wise absolute and codice_28 is row-wise absolute).

The available options for valid formulas depends on the particular spreadsheet implementation but, in general, most arithmetic operations and quite complex nested conditional operations can be performed by most of today's commercial spreadsheets. Modern implementations also offer functions to access custom-build functions, remote data, and applications.

A formula may contain a condition (or nested conditions)—with or without an actual calculation—and is sometimes used purely to identify and highlight errors. In the example below, it is assumed the sum of a column of percentages (A1 through A6) is tested for validity and an explicit message put into the adjacent right-hand cell.

Further examples:

The best way to build up conditional statements is step by step composing followed by trial and error testing and refining code.

A spreadsheet does not, in fact, have to contain any formulas at all, in which case it could be considered merely a collection of data arranged in rows and columns (a database) like a calendar, timetable or simple list. Because of its ease of use, formatting and hyperlinking capabilities, many spreadsheets are used solely for this purpose.

Spreadsheets usually contain a number of supplied functions, such as arithmetic operations (for example, summations, averages and so forth), trigonometric functions, statistical functions, and so forth. In addition there is often a provision for "user-defined functions". In Microsoft Excel these functions are defined using Visual Basic for Applications in the supplied Visual Basic editor, and such functions are automatically accessible on the worksheet. In addition, programs can be written that pull information from the worksheet, perform some calculations, and report the results back to the worksheet. In the figure, the name "sq" is user-assigned, and function "sq" is introduced using the "Visual Basic" editor supplied with Excel. "Name Manager" displays the spreadsheet definitions of named variables "x" & "y".

Functions themselves cannot write into the worksheet, but simply return their evaluation. However, in Microsoft Excel, subroutines can write values or text found within the subroutine directly to the spreadsheet. The figure shows the Visual Basic code for a subroutine that reads each member of the named column variable "x", calculates its square, and writes this value into the corresponding element of named column variable "y". The "y" column contains no formula because its values are calculated in the subroutine, not on the spreadsheet, and simply are written in.

Whenever a reference is made to a cell or group of cells that are not located within the current physical spreadsheet file, it is considered as accessing a "remote" spreadsheet. The contents of the referenced cell may be accessed either on first reference with a manual update or more recently in the case of web based spreadsheets, as a near real time value with a specified automatic refresh interval.

Many spreadsheet applications permit charts, graphs or histograms to be generated from specified groups of cells that are dynamically re-built as cell contents change. The generated graphic component can either be embedded within the current sheet or added as a separate object.

In the late 1980s and early 1990s, first Javelin Software and Lotus Improv appeared. Unlike models in a conventional spreadsheet, they utilized models built on objects called variables, not on data in cells of a report. These multi-dimensional spreadsheets enabled viewing data and algorithms in various self-documenting ways, including simultaneous multiple synchronized views. For example, users of Javelin could move through the connections between variables on a diagram while seeing the logical roots and branches of each variable. This is an example of what is perhaps its primary contribution of the earlier Javelin—the concept of traceability of a user's logic or model structure through its twelve views. A complex model can be dissected and understood by others who had no role in its creation.

In these programs, a time series, or any variable, was an object in itself, not a collection of cells that happen to appear in a row or column. Variables could have many attributes, including complete awareness of their connections to all other variables, data references, and text and image notes. Calculations were performed on these objects, as opposed to a range of cells, so adding two time series automatically aligns them in calendar time, or in a user-defined time frame. Data were independent of worksheets—variables, and therefore data, could not be destroyed by deleting a row, column or entire worksheet. For instance, January's costs are subtracted from January's revenues, regardless of where or whether either appears in a worksheet. This permits actions later used in pivot tables, except that flexible manipulation of report tables was but one of many capabilities supported by variables. Moreover, if costs were entered by week and revenues by month, the program could allocate or interpolate as appropriate. This object design enabled variables and whole models to reference each other with user-defined variable names, and to perform multidimensional analysis and massive, but easily editable consolidations.

Trapeze, a spreadsheet on the Mac, went further and explicitly supported
not just table columns, but also matrix operators.

Spreadsheets that have a formula language based upon logical expressions, rather than arithmetic expressions are known as logical spreadsheets. Such spreadsheets can be used to reason deductively about their cell values.

Just as the early programming languages were designed to generate spreadsheet printouts, programming techniques themselves have evolved to process tables (also known as spreadsheets or matrices) of data more efficiently in the computer itself.

Spreadsheets are a popular end-user development tool. EUD denotes activities or techniques in which people who are not professional developers create automated behavior and complex data objects without significant knowledge of a programming language. Many people find it easier to perform calculations in spreadsheets than by writing the equivalent sequential program. This is due to several traits of spreadsheets.

A "spreadsheet program" is designed to perform general computation tasks using spatial relationships rather than time as the primary organizing principle.

It is often convenient to think of a spreadsheet as a mathematical graph, where the nodes are spreadsheet cells, and the edges are references to other cells specified in formulas. This is often called the dependency graph of the spreadsheet. References between cells can take advantage of spatial concepts such as relative position and absolute position, as well as named locations, to make the spreadsheet formulas easier to understand and manage.

Spreadsheets usually attempt to automatically update cells when the cells they depend on change. The earliest spreadsheets used simple tactics like evaluating cells in a particular order, but modern spreadsheets calculate following a minimal recomputation order from the dependency graph. Later spreadsheets also include a limited ability to propagate values in reverse, altering source values so that a particular answer is reached in a certain cell. Since spreadsheet cells formulas are not generally invertible, though, this technique is of somewhat limited value.

Many of the concepts common to sequential programming models have analogues in the spreadsheet world. For example, the sequential model of the indexed loop is usually represented as a table of cells, with similar formulas (normally differing only in which cells they reference).

Spreadsheets have evolved to use scripting programming languages like VBA as a tool for extensibility beyond what the spreadsheet language makes easy.

While spreadsheets represented a major step forward in quantitative modeling, they have deficiencies. Their shortcomings include the perceived unfriendliness of alpha-numeric cell addresses.




Other problems associated with spreadsheets include:


While there are built-in and third-party tools for desktop spreadsheet applications that address some of these shortcomings, awareness and use of these is generally low. A good example of this is that 55% of Capital market professionals "don't know" how their spreadsheets are audited; only 6% invest in a third-party solution

Spreadsheet risk is the risk associated with deriving a materially incorrect value from a spreadsheet application that will be utilised in making a related (usually numerically-based) decision. Examples include the valuation of an asset, the determination of financial accounts, the calculation of medicinal doses or the size of load-bearing beam for structural engineering. The risk may arise from inputting erroneous or fraudulent data values, from mistakes (or incorrect changes) within the logic of the spreadsheet or the omission of relevant updates (e.g., out of date exchange rates). Some single-instance errors have exceeded US$1 billion. Because spreadsheet risk is principally linked to the actions (or inaction) of individuals it is defined as a sub-category of operational risk.

In the report into the 2012 JPMorgan Chase trading loss, a lack of control over spreadsheets used for critical financial functions was cited as a factor in the trading losses of more than six billion dollars which were reported as a result of derivatives trading gone bad.

Despite this, research carried out by ClusterSeven revealed that around half (48%) of c-level executives and senior managers at firms reporting annual revenues over £50m said there were either no usage controls at all or poorly applied manual processes over the use of spreadsheets at the firms.

In 2013 Thomas Herndon, a graduate student of economics at the University of Massachusetts Amherst found major coding flaws in the spreadsheet used by the economists Carmen Reinhart and Kenneth Rogoff in a very influential 2010 journal article. The Reinhart and Rogoff article was widely used as justification to drive 2010–2013 European austerity programs.




</doc>
<doc id="27687" url="https://en.wikipedia.org/wiki?curid=27687" title="St. Louis">
St. Louis

St. Louis () is an independent city and major U.S. port in the state of Missouri, built along the western bank of the Mississippi River, which marks Missouri's border with Illinois. The city had an estimated March 22, 2018 population of 308,626 and is the cultural and economic center of the Greater St. Louis Metropolitan area (home to nearly 3,000,000 people ), which is the largest metropolitan area in Missouri and the 19th-largest in the United States.

Prior to European settlement, the area was a major regional center of Native American Mississippian culture. The city of St. Louis was founded in 1764 by French fur traders Pierre Laclède and Auguste Chouteau, and named after Louis IX of France. In 1764, following France's defeat in the Seven Years' War, the area was ceded to Spain and retroceded back to France in 1800. In 1803, the United States acquired the territory as part of the Louisiana Purchase. During the 19th century, St. Louis developed as a major port on the Mississippi River. In the 1870 Census, St. Louis was ranked as the 4th-largest city in the United States. It separated from St. Louis County in 1877, becoming an independent city and limiting its own political boundaries. In 1904, it hosted the Louisiana Purchase Exposition and the Summer Olympics.

The economy of metropolitan St. Louis relies on service, manufacturing, trade, transportation of goods, and tourism. Its metro area is home to major corporations, including Anheuser-Busch, Express Scripts, Centene, Boeing Defense, Emerson, Energizer, Panera, Enterprise, Peabody Energy, Ameren, Post Holdings, Monsanto, Edward Jones, Go Jet, Purina and Sigma-Aldrich. Nine of the ten Fortune 500 companies based in Missouri are located within the St. Louis metropolitan area. This city has also become known for its growing medical, pharmaceutical and research presence. St. Louis has two professional sports teams: the St. Louis Cardinals of Major League Baseball and the St. Louis Blues of the National Hockey League. The city is commonly identified with the tall Gateway Arch in the city's downtown.

The area that would become St. Louis was a center of the Native American Mississippian culture, which built numerous temple and residential earthwork mounds on both sides of the Mississippi River. Their major regional center was at Cahokia Mounds, active from 900 AD to 1500 AD. Due to numerous major earthworks within St. Louis boundaries, the city was nicknamed as the "Mound City." These mounds were mostly demolished during the city's development. Historic Native American tribes in the area included the Siouan-speaking Osage people, whose territory extended west, and the Illiniwek.

European exploration of the area was first recorded in 1673, when French explorers Louis Jolliet and Jacques Marquette traveled through the Mississippi River valley. Five years later, La Salle claimed the region for France as part of "La Louisiane."
The earliest European settlements in the area were built in Illinois Country (also known as Upper Louisiana) on the east side of the Mississippi River during the 1690s and early 1700s at Cahokia, Kaskaskia, and Fort de Chartres. Migrants from the French villages on the opposite side of the Mississippi River (e.g. Kaskaskia) founded Ste. Genevieve in the 1730s.

In early 1764, after France lost the 7 Years' War, Pierre Laclède and his stepson Auguste Chouteau founded what was to become the city of St. Louis. (French lands east of the Mississippi had been ceded to Great Britain and the lands west of the Mississippi to Spain; France and Spain were 18th-century allies and both were Catholic nations.) The early French families built the city's economy on the fur trade with the Osage, as well as with more distant tribes along the Missouri River. The Chouteau brothers gained a monopoly from Spain on the fur trade with Santa Fe. French colonists used African slaves as domestic servants and workers in the city.

France, alarmed that Britain would demand French possessions west of the Mississippi and the Missouri River basin after the losing New France to them in 1759-60, transferred these to Spain as part of the Viceroyalty of New Spain. These areas remained in Spanish possession until 1803. In 1780 during the American Revolutionary War, St. Louis was attacked by British forces, mostly Native American allies, in the Battle of St. Louis.

The founding of St. Louis began in 1763. Pierre Laclede led an expedition to set up a fur-trading post farther up the Mississippi River. Before then, Laclede had been a very successful merchant. For this reason, he and his trading partner Gilbert Antoine de St. Maxent were offered monopolies for six years of the fur trading in that area.

Though they were originally only granted rights to set-up a trading post, Laclede and other members of his expedition quickly set up a settlement. Some historians believe that Laclede's determination to create this settlement was the result of his affair with a married woman Marie-Thérèse Bourgeois Chouteau in New Orleans.

Laclede on his initial expedition was accompanied by his young stepson, Auguste Chouteau. Some historians still debate which of the two men was the true founder of St. Louis. The reason for this lingering question is that all the documentation of the founding was loaned and subsequently destroyed in a fire.
For the first few years of St. Louis's existence, the city was not recognized by any of the governments. Though originally thought to be under the control of the Spanish government, no one asserted any authority over the settlement, and thus St. Louis had no local government. This led Laclede to assume a position of civil control, and all problems were disposed in public settings, such as communal meetings. In addition, Laclede granted new settlers lots in town and the surrounding countryside. In hindsight, many of these original settlers thought of these first few years as "the golden age of St. Louis."
By 1765, the city began receiving visits from representatives of the English, French, and Spanish governments. The Indians in the area expressed dissatisfaction at being under the control of British forces. One of the great Ottawa chieftains, Pontiac, was angered by the change of power and the potential for the British to come into their lands. He desired to fight against them but many of the St. Louis inhabitants refused.

St. Louis was transferred to the French First Republic in 1800 (although all of the colonial lands continued to be administered by Spanish officials), then sold by the French to the U.S. in 1803 as part of the Louisiana Purchase. St. Louis became the capital of, and gateway to, the new territory. Shortly after the official transfer of authority was made, the Lewis and Clark Expedition was commissioned by President Thomas Jefferson. The expedition departed from St. Louis in May 1804 along the Missouri River to explore the vast territory. There were hopes of finding a water route to the Pacific Ocean, but the party had to go overland in the Upper West. They reached the Pacific Ocean via the Columbia River in summer 1805. They returned, reaching St. Louis on September 23, 1806. Both Lewis and Clark lived in St. Louis after the expedition. Many other explorers, settlers, and trappers (such as Ashley's Hundred) would later take a similar route to the West.

The city elected its first municipal legislators (called trustees) in 1808. Steamboats first arrived in St. Louis in 1818, improving connections with New Orleans and eastern markets. Missouri was admitted as a state in 1821. St. Louis was incorporated as a city in 1822, and continued to develop largely due to its busy port and trade connections. Slaves worked in many jobs on the waterfront as well as on the riverboats. Given the city's location close to the free state of Illinois and others, some slaves escaped to freedom. Others, especially women with children, sued in court in freedom suits, and several prominent local attorneys aided slaves in these suits. About half the slaves achieved freedom in hundreds of suits before the American Civil War.

Immigrants from Ireland and Germany arrived in St. Louis in significant numbers starting in the 1840s, and the population of St. Louis grew from less than 20,000 in 1840, to 77,860 in 1850, to more than 160,000 by 1860. By the mid-1800s, St. Louis had a greater population than New Orleans.

Settled by many Southerners in a slave state, the city was split in political sympathies and became polarized during the American Civil War. In 1861, 28 civilians were killed in a clash with Union troops. The war hurt St. Louis economically, due to the Union blockade of river traffic to the south on the Mississippi River. The St. Louis Arsenal constructed ironclads for the Union Navy.

After the war, St. Louis profited via trade with the West, aided by the 1874 completion of the Eads Bridge, named for its architect. Industrial developments on both banks of the river were linked by the bridge, the first in the mid-west over the Mississippi River. The bridge connects St. Louis, Missouri to East St. Louis, Illinois. The Eads Bridge became an iconic image of the city of St. Louis, from the time of its erection until 1965 when the Gateway Arch Bridge was constructed. The bridge crosses the St. Louis riverfront between Laclede's Landing, to the north, and the grounds of the Gateway Arch, to the south. Today the road deck has been restored, allowing vehicular and pedestrian traffic to cross the river. The St. Louis MetroLink light rail system has used the rail deck since 1993. An estimated 8,500 vehicles pass through it daily.

On August 22, 1876, the city of St. Louis voted to secede from St. Louis County and become an independent city. Industrial production continued to increase during the late 19th century. Major corporations such as the Anheuser-Busch brewery and Ralston-Purina company were established. St. Louis also was home to Desloge Consolidated Lead Company and several brass era automobile companies, including the Success Automobile Manufacturing Company; St. Louis is the site of the Wainwright Building, an early skyscraper built in 1892 by noted architect Louis Sullivan.

In 1904, the city hosted the 1904 World's Fair and the 1904 Summer Olympics, becoming the first non-European city to host the Olympics. Permanent facilities and structures remaining from the fair are Forest Park and associated structures within its boundaries: the Saint Louis Art Museum, the St. Louis Zoo and the Missouri History Museum, as well as Tower Grove Park and the Botanical Gardens.

In the aftermath of emancipation of slaves following the Civil War, social and racial discrimination in housing and employment were common in St. Louis. Starting in the 1910s, many property deeds included racial or religious restrictive covenants against new immigrants and migrants. In the first half of the 20th century, St. Louis was a destination for many African Americans in the Great Migration from the rural South seeking better opportunities. During World War II, the NAACP campaigned to integrate war factories, and restrictive covenants were prohibited in 1948 by the "Shelley v. Kraemer" U.S. Supreme Court decision, which case originated as a lawsuit in St. Louis. In 1964 civil rights activists protested at the construction of the Gateway Arch to publicize their effort to gain entry for African Americans into the skilled trade unions, where they were underrepresented. The Department of Justice filed the first suit against the unions under the Civil Rights Act of 1964.

In the first part of the century, St. Louis had some of the worst air pollution in the United States. In April 1940 the city banned the use of soft coal mined in nearby states. The city hired inspectors to ensure only hard anthracite was burned. By 1946 the city had reduced air pollution by about three-quarters.
"De jure" educational segregation continued into the 1950s, and "de facto" segregation continued into the 1970s, leading to a court challenge and interdistrict desegregation agreement. Students have been bussed mostly from the city to county school districts to have opportunities for integrated classes, although the city has created magnet schools to attract students.

St. Louis, like many Midwestern cities, expanded in the early 20th century due to industrialization, which provided jobs to new generations of immigrants and migrants from the South. It reached its peak population of 856,796 at the 1950 census. Suburbanization from the 1950s through the 1990s dramatically reduced the city's population, as did restructuring of industry and loss of jobs. The effects of suburbanization were exacerbated by the relatively small geographical size of St. Louis due to its earlier decision to become an independent city, and it lost much of its tax base. During the 19th and 20th century, most major cities aggressively annexed surrounding areas as residential development occurred away from the central city; however, St. Louis was unable to do so.

In the 21st century, the city of St. Louis contains only 11% of its total metropolitan population, while among the top 20 metro areas in the United States, the central cities contain an average of 24% of total metropolitan area population. Although small increases in population have taken place in St. Louis during the early 2000s, overall the city lost population from 2000 to 2010. Immigration has continued, with the city attracting Vietnamese, Latinos from Mexico and Central America, and Bosnians, the latter forming the largest Bosnian community outside of Bosnia.

Several urban renewal projects were built in the 1950s, as the city worked to replace old and substandard housing. Some of these were poorly designed and resulted in problems. One prominent example, Pruitt-Igoe, became a symbol of failure in public housing, and was torn down less than two decades after it was built.

Since the 1980s, several revitalization efforts have focused on downtown St. Louis.

Urban revitalization continued in the new century. Gentrification has taken place in the Washington Avenue Historic District. In 2006, St. Louis received the World Leadership Award for urban renewal. In 2013 the US Census Bureau estimated that St. Louis had a population of 318,416.

On December 20, 2011 a 24-year-old African American man from St. Louis, Missouri was shot and killed by a St. Louis police officer. When the police officer was found not guilty in September 2017, several protests erupted.

The architecture of St. Louis exhibits a variety of commercial, residential, and monumental architecture. St. Louis is known for the Gateway Arch, the tallest monument constructed in the United States at . The Arch pays homage to Thomas Jefferson and St. Louis's position as the gateway to the West. Architectural influences reflected in the area include French Colonial, German, early American, and modern architectural styles.

Some notable post-modern commercial skyscrapers were built downtown in the 1970s and 1980s, including the One US Bank Plaza (1976), the AT&T Center (1986), and One Metropolitan Square (1989), which is the tallest building in St. Louis. One US Bank Plaza, the local headquarters for US Bancorp, was constructed for the Mercantile Bancorporation in the Structural expressionist style, emphasizing the steel structure of the building.

During the 1990s, St. Louis saw the construction of the largest United States courthouse by area, the Thomas F. Eagleton United States Courthouse (completed in 2000). The Eagleton Courthouse is home to the United States District Court for the Eastern District of Missouri and the United States Court of Appeals for the Eighth Circuit. The most recent high-rise buildings in St. Louis include two residential towers: the Park East Tower in the Central West End and the Roberts Tower located in downtown.

Several examples of religious structures are extant from the pre-Civil War period, and most reflect the common residential styles of the time. Among the earliest is the Basilica of St. Louis, King of France (locally referred to as the "Old Cathedral"). The Basilica was built between 1831 and 1834 in the Federal style. Other religious buildings from the period include SS. Cyril and Methodius Church (1857) in the Romanesque Revival style and Christ Church Cathedral (completed in 1867, designed in 1859) in the Gothic Revival style.

Only a few civic buildings were constructed during the early 19th century. The original St. Louis courthouse was built in 1826 and featured a Federal style stone facade with a rounded portico. However, this courthouse was replaced during renovation and expansion of the building in the 1850s. The Old St. Louis County Courthouse (locally known as the "Old Courthouse") was completed in 1864 and was notable for having an early cast iron dome and for being the tallest structure in Missouri until 1894. Finally, a customs house was constructed in the Greek Revival style in 1852, but was demolished and replaced in 1873 by the U.S. Customhouse and Post Office.

Because much of the city's early commercial and industrial development was centered along the riverfront, many pre-Civil War buildings were demolished during construction of the Gateway Arch. The city's remaining architectural heritage of the era includes a multi-block district of cobblestone streets and brick and cast-iron warehouses called Laclede's Landing. Now popular for its restaurants and nightclubs, the district is located north of Gateway Arch along the riverfront. Other industrial buildings from the era include some portions of the Anheuser-Busch Brewery, which date to the early 1860s.

St. Louis saw a vast expansion in variety and number of religious buildings during the late 19th century and early 20th century. The largest and most ornate of these is the Cathedral Basilica of St. Louis, designed by Thomas P. Barnett and constructed between 1907 and 1914 in the Neo-Byzantine style. The St. Louis Cathedral, as it is known, has one of the largest mosaic collections in the world. Another landmark in religious architecture of St. Louis is the St. Stanislaus Kostka, which is an example of the Polish Cathedral style. Among the other major designs of the period were St. Alphonsus Liguori (locally known as "The Rock Church") (1867) in the Gothic Revival and Second Presbyterian Church of St. Louis (1900) in Richardsonian Romanesque.

By the 1900 census, St. Louis was the fourth largest city in the country. In 1904, the city hosted a world's fair at Forest Park called the Louisiana Purchase Exposition. Its architectural legacy is somewhat scattered. Among the fair-related cultural institutions in the park are the Saint Louis Art Museum designed by Cass Gilbert, part of the remaining lagoon at the foot of Art Hill, and the Flight Cage at the St. Louis Zoo. The Missouri History Museum was built afterward, with the profit from the fair. But 1904 left other assets to the city, like Theodore Link's 1894 St. Louis Union Station, and an improved Forest Park.

The city is divided into 79 government-designated neighborhoods. The neighborhood divisions have no legal standing, although some neighborhood associations administer grants or hold veto power over historic-district development.

Several neighborhoods are lumped together in categories such as "North City," "South City," and "The Central West End."

The following is a list of neighborhoods of the city of St. Louis, Missouri.

According to the United States Census Bureau, St. Louis has a total area of , of which is land and (6.2%) is water. (Not shown on simple maps of the city, the land at its airport is owned by the city, served by its fire department and others, and is an exclave of St. Louis.) The city is built primarily on bluffs and terraces that rise 100–200 feet above the western banks of the Mississippi River, in the Midwestern United States just south of the Missouri-Mississippi confluence. Much of the area is a fertile and gently rolling prairie that features low hills and broad, shallow valleys. Both the Mississippi River and the Missouri River have cut large valleys with wide flood plains.

Limestone and dolomite of the Mississippian epoch underlie the area, and parts of the city are karst in nature. This is particularly true of the area south of downtown, which has numerous sinkholes and caves. Most of the caves in the city have been sealed, but many springs are visible along the riverfront. Coal, brick clay, and millerite ore were once mined in the city. The predominant surface rock, known as "St. Louis limestone", is used as dimension stone and rubble for construction.

Near the southern boundary of the city of St. Louis (separating it from St. Louis County) is the River des Peres, practically the only river or stream within the city limits that is not entirely underground. Most of River des Peres was confined to a channel or put underground in the 1920s and early 1930s. The lower section of the river was the site of some of the worst flooding of the Great Flood of 1993.

The city's eastern boundary is the Mississippi River, which separates Missouri from Illinois. The Missouri River forms the northern line of St. Louis County, except for a few areas where the river has changed its course. The Meramec River forms most of its southern line.

St. Louis lies in the transitional zone between the humid continental climate type and the humid subtropical climate type (Köppen "Dfa" and "Cfa", respectively), with neither large mountains nor large bodies of water to moderate its temperature. The city experiences hot, humid summers, and chilly to cold winters. It is subject to both cold Arctic air and hot, humid tropical air from the Gulf of Mexico. The average annual temperature recorded at nearby Lambert–St. Louis International Airport, is . Both temperatures can be seen on an average 2 or 3 days per year. Average annual precipitation is about , but annual precipitation has ranged from in 1953 to in 2015.

St. Louis experiences thunderstorms 48 days a year on average. Especially in the spring, these storms can often be severe, with high winds, large hail and tornadoes. Lying within the hotbed of Tornado Alley, St. Louis reigns as one of the most frequently tornadic metropolitan areas, and has an extensive history of particularly damaging tornadoes. Severe flooding, such as the Great Flood of 1993, may occur in spring and summer; the (often rapid) melting of thick snow cover upstream either the Missouri or Mississippi Rivers can contribute to springtime flooding.

Before the founding of the city, the area was mostly prairie and open forest. Native Americans maintained this environment, good for hunting, by burning underbrush. Trees are mainly oak, maple, and hickory, similar to the forests of the nearby Ozarks; common understory trees include eastern redbud, serviceberry, and flowering dogwood. Riparian areas are forested with mainly American sycamore.

Most of the residential areas of the city are planted with large native shade trees. The largest native forest area is found in Forest Park. In autumn, the changing color of the trees is notable. Most species here are typical of the eastern woodland, although numerous decorative non-native species are found. The most notable invasive species is Japanese honeysuckle, which officials are trying to manage because of its damage to native trees. It is removed from some parks.

Large mammals found in the city include urbanized coyotes and white-tailed deer. Eastern gray squirrel, cottontail rabbit, and other rodents are abundant, as well as the nocturnal Virginia opossum. Large bird species are abundant in parks and include Canada goose, mallard duck, as well as shorebirds, including the great egret and great blue heron. Gulls are common along the Mississippi River; these species typically follow barge traffic.

Winter populations of bald eagles are found along the Mississippi River around the Chain of Rocks Bridge. The city is on the Mississippi Flyway, used by migrating birds, and has a large variety of small bird species, common to the eastern US. The Eurasian tree sparrow, an introduced species, is limited in North America to the counties surrounding St. Louis. The city has special sites for birdwatching of migratory species, including Tower Grove Park.

Frogs are commonly found in the springtime, especially after extensive wet periods. Common species include the American toad and species of chorus frogs commonly called spring peepers, which are found in nearly every pond. Some years have outbreaks of cicadas or ladybugs. Mosquitoes, no-see-ums, and houseflies are common insect nuisances, especially in July and August; because of this, windows are nearly universally fitted with screens. Invasive populations of honeybees have sharply declined in recent years. Numerous native species of pollinator insects have recovered to fill their ecological niche, and armadillos have been regularly seen throughout the St. Louis area, especially since 2005.

St. Louis grew slowly until the American Civil War, when industrialization and immigration sparked a boom. Mid-19th century immigrants included many Irish and Germans; later there were immigrants from southern and eastern Europe. In the early 20th century, African American and white migrants came from the South; the former as part of the Great Migration out of rural areas of the Deep South. Many came from Mississippi and Arkansas.

After years of immigration, migration, and expansion, the city reached its peak population in 1950. That year, the Census Bureau reported St. Louis's population as 82% White and 17.9% African American. After World War II, St. Louis began losing population to the suburbs, first because of increased demand for new housing, unhappiness with city services, ease of commuting by subsidized highways, and later, white flight. St. Louis's population decline has resulted in a significant increase of abandoned residential housing units and vacant lots throughout the city proper; this blight has attracted much wildlife (such as deer and coyotes) to the many abandoned overgrown lots.

St. Louis has lost 64.0% of its population since the 1950 United States Census, the highest percent of any city that had a population of 100,000 or more at the time of the 1950 Census. Detroit, Michigan and Youngstown, Ohio are the only other cities that have had population declines of at least 60% in the same time frame. The population of the city of St. Louis has been in decline since the 1950 census; during this period the population of the St. Louis Metropolitan Area, which includes more than one county, has grown every year and continues to do so. A big factor in the decline has been the rapid increase in suburbanization.

According to the 2010 United States Census, St. Louis had 319,294 people living in 142,057 households, of which 67,488 households were families. The population density was 5,158.2 people per square mile (1,990.6/km²). About 24% of the population was 19 or younger, 9% were 20 to 24, 31% were 25 to 44, 25% were 45 to 64, and 11% were 65 or older. The median age was about 34 years.

The population was about 49.2% African American, 43.9% White (42.2% Non-Hispanic White), 2.9% Asian, 0.3% Native American/Alaska Native, and 2.4% reporting two or more races. Hispanic or Latino of any race were 3.5% of the population.

The African-American population is concentrated in the north side of the city (the area north of Delmar Boulevard is 94.0% black, compared with 35.0% in the central corridor and 26.0% in the south side of St. Louis). Among the Asian-American population in the city, the largest ethnic group is Vietnamese (0.9%), followed by Chinese (0.6%) and Indians (0.5%). The Vietnamese community has concentrated in the Dutchtown neighborhood of south St. Louis; Chinese are concentrated in the Central West End. People of Mexican descent are the largest Latino group, and make up 2.2% of St. Louis's population. They have the highest concentration in the Dutchtown, Benton Park West (Cherokee Street), and Gravois Park neighborhoods.

In 2000, the median income for a household in the city was $29,156, and the median income for a family was $32,585. Males had a median income of $31,106; females, $26,987. Per capita income was $18,108.

Some 19% of the city's housing units were vacant, and slightly less than half of these were vacant structures not for sale or rent.

In 2010, St. Louis's per-capita rates of online charitable donations and volunteerism were among the highest among major U.S. cities.

As of 2010, 91.05% (270,934) of St. Louis city residents age 5 and older spoke English at home as a primary language, while 2.86% (8,516) spoke Spanish, 0.91% (2,713) Serbo-Croat, 0.74% (2,200) Vietnamese, 0.50% (1,495) African languages, 0.50% (1,481) Chinese, and French was spoken as a main language by 0.45% (1,341) of the population over the age of five. In total, 8.95% (26,628) of St. Louis's population age 5 and older spoke a mother language other than English.

About 15 families from Bosnia settled in St. Louis between 1960 and 1970. After the Bosnian War started in 1992, more Bosnian refugees began arriving and by 2000, tens of thousands of Bosnian refugees settled in St. Louis with the help of Catholic aid societies. Many of them were professionals and skilled workers who had to take any job opportunity to be able to support their families. Most Bosnian refugees are Muslim, ethnically Bosniaks (87%); they have settled primarily in south St. Louis and South County. Bosnian-Americans are well integrated into the city, developing many businesses and ethnic/cultural organizations.

An estimated 70,000 Bosnians live in the metro area, the largest population of Bosnians in the United States and the largest Bosnian population outside their homeland. The highest concentration of Bosnians is in the neighborhood of Bevo Mill and in Affton, Mehlville, and Oakville of south St. Louis County.

According to stltoday.com in 2016 GDP of the St. Louis metro area was $160 billion and in 2015 GDP was $155 billion. 2014 Gross Metropolitan Product (GMP) of St. Louis was $145.958 billion up from $144.03 in 2013, $138.403 in 2012 and $133.1 in 2011 making it the 21st-highest in the country. The St. Louis Metropolitan Area had a Per capita GDP of $48,738 in 2014 up 1.6% from 2013. This signals the growth of the St. Louis economy. According to the 2007 Economic Census, manufacturing in the city conducted nearly $11 billion in business, followed by the health care and social service industry with $3.5 billion, professional or technical services with $3.1 billion, and the retail trade with $2.5 billion. The health care sector was the biggest employer in the area with 34,000 workers, followed by administrative and support jobs, 24,000; manufacturing, 21,000, and food service, 20,000.

As of 2018, the St. Louis Metropolitan Area is home to ten Fortune 500 companies, the 7th largest Fortune 500 city in the United States. Some of St. Louis' largest Fortune 500 companies are: Express Scripts, Emerson Electric, Monsanto, Reinsurance Group of America, Centene, Graybar Electric, and Edward Jones Investments. 
Other notable corporations headquartered in the region include Arch Coal, Wells Fargo Advisors (formerly A.G. Edwards), Energizer Holdings, Patriot Coal, Post Foods, United Van Lines, and Mayflower Transit, Post Holdings, Olin, and Enterprise Holdings (a parent company of several car rental companies). Notable corporations with operations in St. Louis include Cassidy Turley, Kerry Group, MasterCard, TD Ameritrade, and BMO Harris Bank.

Health care and biotechnology institutions with operations in St. Louis include Pfizer, the Donald Danforth Plant Science Center, the Solae Company, Sigma-Aldrich, and Multidata Systems International. General Motors manufactures automobiles in Wentzville, while an earlier plant, known as the St. Louis Truck Assembly, built GMC automobiles from 1920 until 1987. Chrysler closed its St. Louis Assembly production facility in nearby Fenton, Missouri and Ford closed the St. Louis Assembly Plant in Hazelwood.

Several once-independent pillars of the local economy have been purchased by other corporations. Among them are Anheuser-Busch, purchased by Belgium-based InBev; Missouri Pacific Railroad, which was headquartered in St. Louis, merged with the Omaha, Nebraska-based Union Pacific Railroad in 1982; McDonnell Douglas, whose operations are now part of Boeing Defense, Space & Security; Mallinckrodt, purchased by Tyco International; and Ralston Purina, now a wholly owned subsidiary of Nestlé. The May Department Stores Company (which owned Famous-Barr and Marshall Field's stores) was purchased by Federated Department Stores, which has its regional headquarters in the area. The Federal Reserve Bank of St. Louis in downtown is one of two federal reserve banks in Missouri. Most of the assets of Furniture Brands International were sold to Heritage Home Group in 2013, and while that company remained in the area for a brief time, it has moved to North Carolina.
St. Louis is a center of medicine and biotechnology. The Washington University School of Medicine is affiliated with Barnes-Jewish Hospital, the fifth-largest hospital in the world. Both institutions operate the Alvin J. Siteman Cancer Center. The School of Medicine also is affiliated with St. Louis Children's Hospital, one of the country's top pediatric hospitals. Both hospitals are owned by BJC HealthCare. The McDonnell Genome Institute at Washington University played a major role in the Human Genome Project. St. Louis University Medical School is affiliated with SSM Health's Cardinal Glennon Children's Hospital and St. Louis University Hospital. It also has a cancer center, vaccine research center, geriatric center, and a bioethics institute. Several different organizations operate hospitals in the area, including BJC HealthCare, Mercy, SSM Health Care, and Tenet.

Boeing employs nearly 15,000 people in its north St. Louis campus, headquarters to its defense unit. In 2013, the company said it would move about 600 jobs from Seattle, where labor costs have risen, to a new IT center in St. Louis. Other companies, such as LaunchCode and LockerDome, see the city's potential to become the next major tech hub. Programs such as Arch Grants are attracting new startups to the region.

According to the "St. Louis Business Journal", the top employers in the St. Louis metropolitan area as of May 1, 2017, are as follows:

According to St. Louis's 2017 Comprehensive Annual Financial Report (June 30), the top employers in the city only, as of June 30, 2016 are:

The city is home to three national research universities, University of Missouri-St. Louis, Washington University in St. Louis and Saint Louis University, as classified under the Carnegie Classification of Institutions of Higher Education. Washington University School of Medicine in St. Louis has been ranked among the top 10 medical schools in the country by US News & World Report for as long as the list has been published, and as high as second, in 2003 and 2004.

In addition to Catholic theological institutions such as Kenrick-Glennon Seminary, St. Louis is home to three Protestant seminaries: Eden Theological Seminary of the United Church of Christ, Covenant Theological Seminary of the Presbyterian Church in America, and Concordia Seminary of the St. Louis-based Lutheran Church–Missouri Synod.

The St. Louis Public Schools (SLPS) operate more than 75 schools, attended by more than 25,000 students, including several magnet schools. SLPS operates under provisional accreditation from the state of Missouri and is under the governance of a state-appointed school board called the Special Administrative Board, although a local board continues to exist without legal authority over the district. Since 2000, charter schools have operated in the city of St. Louis using authorization from Missouri state law. These schools are sponsored by local institutions or corporations and take in students from kindergarten through high school. In addition, several private schools exist in the city, and the Archdiocese of St. Louis operates dozens of parochial schools in the city, including parochial high schools. The city also has several private high schools, including secular, Catholic and Lutheran schools. St. Louis University High School - a Jesuit preparatory high school founded in 1818 - is the oldest secondary educational institution in the U.S. west of the Mississippi River.

With its French past and waves of Catholic immigrants in the 19th and 20th centuries, from Ireland, Germany and Italy, St. Louis is a major center of Roman Catholicism in the United States. St. Louis also boasts the largest Ethical Culture Society in the United States, and consistently ranks as one of the most generous cities in the United States, ranking ninth in 2013. Several places of worship in the city are noteworthy, such as the Cathedral Basilica of St. Louis, home of the world's largest mosaic installation.
Other locally notable churches include the Basilica of St. Louis, King of France, the oldest Roman Catholic cathedral west of the Mississippi River and the oldest church in St. Louis; the St. Louis Abbey, whose distinctive architectural style garnered multiple awards at the time of its completion in 1962; and St. Francis de Sales Oratory, a neo-Gothic church completed in 1908 in South St. Louis and the second-largest church in the city.

The city is defined by music and the performing arts, especially its association with blues, jazz, and ragtime. St. Louis is home to the St. Louis Symphony, the second-oldest symphony orchestra in the United States, which has toured nationally and internationally to strong reviews. Until 2010, it was also home to KFUO-FM, one of the oldest classical music FM radio stations west of the Mississippi River.

The Gateway Arch marks downtown St. Louis and a historic center that includes the Federal courthouse where the Dred Scott case was first argued, a newly renovated and expanded public library, major churches and businesses, and retail. An increasing downtown residential population has taken to adapted office buildings and other historic structures. In nearby University City is the Delmar Loop, ranked by the American Planning Association as a "great American street" for its variety of shops and restaurants, and the Tivoli Theater, all within walking distance.

Unique city and regional cuisine reflecting various immigrant groups include toasted ravioli, gooey butter cake, provel cheese, the slinger, the Gerber sandwich, the St. Paul sandwich, and St. Louis-style pizza, featuring thin crust and provel cheese. Some St. Louis chefs have begun emphasizing use of local produce, meats and fish, and neighborhood farmers' markets have become increasingly popular, as well as one downtown. Artisan bakeries, salumeria, and chocolatiers also operate in the city.

Also unique to St. Louis are St. Louis style pizza (extremely thin crust, Provel cheese, and cut in small squares, marketed by one leading purveyor as "The Square Beyond Compare") and the Ted Drewes "Concrete", which is frozen custard blended with any combination of dozens of ingredients, served in a large yellow cup with a spoon and straw. The mixture is so thick that a spoon inserted into the custard does not fall if the cup is inverted.

St. Louis is home to Major League Baseball and the National Hockey League, notable collegiate-level soccer teams, and has hosted several collegiate sports tournaments. It is one of three American cities to have hosted an Olympic Games.

St. Louis is home to two major league sports teams. The St. Louis Cardinals are one of the most successful franchises in Major League Baseball. The Cardinals have won 19 National League (NL) titles (the most pennants for the league franchise in one city) and 11 World Series titles (second only to the New York Yankees and the most by any NL franchise), most recently in 2011. They play at Busch Stadium. Previously, the St. Louis Browns played in the American League (AL) from 1902 to 1953, before moving to Baltimore, Maryland to become the current incarnation of the Orioles. The 1944 World Series was an all-St. Louis World Series, matching up the St. Louis Cardinals and St. Louis Browns at Sportsman's Park.
The St. Louis Blues of the National Hockey League (NHL) play at the Scottrade Center. They were one of the six teams added to the NHL in the 1967 expansion. The Blues have never won the Stanley Cup, and are the oldest team not to do so. Prior to the Blues, the city was home to the St. Louis Eagles. The team only played in the 1934-35 season.

St. Louis has been home to four different National Football League (NFL) teams. The St. Louis All-Stars played in the city in 1923, the St. Louis Gunners in 1934, the St. Louis Cardinals from 1960 to 1987, and the St. Louis Rams from 1995 to 2015. The football Cardinals advanced to the NFL playoffs just three times (1974, 1975 and 1982), never hosting or winning in any appearance. The Cardinals moved to Phoenix, Arizona, in 1988. The Rams played at the Edward Jones Dome from 1995 to 2015 and went on to win Super Bowl XXXIV. The Rams returned to Los Angeles, California in 2016.

The St. Louis Hawks of the National Basketball Association (NBA) played at Kiel Auditorium from 1955 to 1968. They won the NBA championship in 1958 and played in two other NBA Finals: 1957 and 1960. In 1968 the Hawks moved to Atlanta, Georgia.

St. Louis also hosts several minor league sports teams. The Gateway Grizzlies and the River City Rascals of the Frontier League (which are not affiliated with Major League Baseball) play in the area. The St. Louis Trotters of the Independent Basketball Association play at Matthews Dickey. St. Louis FC of the United Soccer League play at World Wide Technology Soccer Park and both River City Raiders and St. Louis Ambush play at the Family Arena. The region hosts INDYCAR, NHRA drag racing, and NASCAR events at the Gateway International Raceway in Madison, Illinois. St. Louis Slam play at the Harlen C. Hunter Stadium

At the collegiate level, St. Louis has hosted the Final Four of both the women's and men's college basketball NCAA Division I championship tournaments, and the Frozen Four collegiate ice hockey tournament. Although the area does not have a National Basketball Association team, it hosts an American Basketball Association team called the St. Louis Phoenix. St. Louis University has won 10 NCAA Men's Soccer Championships, and the city has hosted the College Cup several times. In addition to collegiate soccer, many St. Louisans have played for the United States men's national soccer team, and 20 St. Louisans have been elected into the National Soccer Hall of Fame. St. Louis also is the origin of the sport of corkball, a type of baseball in which there is no base running.

St. Louis is home to the Chess Club and Scholastic Center of St. Louis (CCSCSL) where the U.S. Chess Championship is held. St. Louisan Rex Sinquefield founded the CCSCSL and moved the World Chess Hall of Fame to St. Louis in 2011. The Sinquefield Cup Tournament started at St. Louis in 2013. In 2014 the Sinquefield Cup was the highest rated chess tournament of all time. Former U.S. Chess Champions Caruana and Hikaru Nakamura have lived in St. Louis. Women's chess champion Susan Polgar also resides in St. Louis.

The city operates more than 100 parks, with amenities that include sports facilities, playgrounds, concert areas, picnic areas, and lakes. Forest Park, located on the western edge of city, is the largest, occupying 1,400 acres of land, making it almost twice as large as Central Park in New York City. The park is home to five major institutions, including the Saint Louis Art Museum, the St. Louis Zoo, the St. Louis Science Center, the Missouri History Museum, and the Muny amphitheatre. Another significant park in the city is the Gateway Arch National Park, a National Memorial which was known as the Jefferson National Expansion Memorial until 2018 and is located on the riverfront in downtown St. Louis. The centerpiece of the park is the tall Gateway Arch, designed by noted architect Eero Saarinen and completed on October 28, 1965. Also part of the historic park is the Old Courthouse, where the first two trials of "Dred Scott v. Sandford" were held in 1847 and 1850.
Other notable parks in the city include the Missouri Botanical Garden, Tower Grove Park, Carondelet Park and Citygarden. The Missouri Botanical Garden, a private garden and botanical research facility, is a National Historic Landmark and one of the oldest botanical gardens in the United States. The Garden features 79 acres of horticultural displays from around the world. This includes a Japanese strolling garden, Henry Shaw's original 1850 estate home and a geodesic dome called the Climatron. Immediately south of the Missouri Botanical Garden is Tower Grove Park, a gift to the city by Henry Shaw. Citygarden is an urban sculpture park located in downtown St. Louis, with art from Fernand Léger, Aristide Maillol, Julian Opie, Tom Otterness, Niki de Saint Phalle, and Mark di Suvero. The park is divided into three sections, each of which represent a different theme: river bluffs; flood plains; and urban gardens. The park also has a restaurant – Death in the Afternoon. Another downtown sculpture park is the Serra Sculpture Park, with the 1982 Richard Serra sculpture "Twain".

The city of St. Louis has a strong mayor-council government with legislative authority vested in the Board of Aldermen of the City of St. Louis and with executive authority in the Mayor of St. Louis and six other separately elected officials. The Board of Aldermen is made up of 28 members (one elected from each of the city's wards) plus a board president who is elected citywide. 
The 2014 fiscal year budget topped $1 billion for the first time, a 1.9% increase over the $985.2 million budget in 2013. 238,253 registered voters lived in the city in 2012, down from 239,247 in 2010, and 257,442 in 2008.

Municipal elections in St. Louis are held in odd numbered years, with the primary elections in March and the general election in April. The mayor is elected in odd numbered years following the United States Presidential Election, as are the aldermen representing odd-numbered wards. The President of the Board of Aldermen and the aldermen from even-numbered wards are elected in the off-years. The Democratic Party has dominated St. Louis city politics for decades. The city has not had a Republican mayor since 1949 and the last time a Republican was elected to another citywide office was in the 1970s. As of 2015, all 28 of the city's Aldermen are Democrats. Forty-Six individuals have held the office of mayor of St. Louis, four of whom—William Carr Lane, John Fletcher Darby, John Wimer, and John How—served non-consecutive terms. The most terms served by a mayor was by Lane who served 8 full terms plus the unexpired term of Darby. The current mayor is Lyda Krewson, who took office April 18, 2017 The second-longest serving mayor was Henry Kiel, who took office April 15, 1913 and left office April 21, 1925, a total of 12 years and 9 days over three terms in office. Two others—Raymond Tucker, and Vincent C. Schoemehl—also served three terms as mayor, but served seven fewer days. The shortest serving mayor was Arthur Barret who died 11 days after taking office.

Although St. Louis separated from St. Louis County in 1876, some mechanisms have been put in place for joint funding management and funding of regional assets. The St. Louis Zoo-Museum district collects property taxes from residents of both St. Louis City and County and the funds are used to support cultural institutions including the St. Louis Zoo, Saint Louis Art Museum and the Missouri Botanical Gardens. Similarly, the Metropolitan Sewer District provides sanitary and storm sewer service to the city and much of St. Louis County. The Bi-State Development Agency (now known as Metro) runs the region's MetroLink light rail system and bus system.

St. Louis is split between 11 districts in the Missouri House of Representatives: all of the 76th, 77th, 78th, 79th, 80th, 81st, 82nd, and 84th, and parts of the 66th, 83rd, and 93rd, which are shared with St. Louis County. The 5th Missouri Senate district is entirely within the city, while the 4th is shared with St. Louis County.

At the federal level, St. Louis is the heart of , which also includes part of northern St. Louis County. A Republican has not represented a significant portion of St. Louis in the U.S. House since 1953.

The United States Court of Appeals for the Eighth Circuit and the United States District Court for the Eastern District of Missouri are based in the Thomas F. Eagleton United States Courthouse in downtown St. Louis. St. Louis is also home to a Federal Reserve System branch, the Federal Reserve Bank of St. Louis. The National Geospatial-Intelligence Agency (NGA) also maintains major facilities in the St. Louis area.

The Military Personnel Records Center (NPRC-MPR) located at 9700 Page Avenue in St. Louis, is a branch of the National Personnel Records Center and is the repository of over 56 million military personnel records and medical records pertaining to retired, discharged, and deceased veterans of the U.S. armed forces.

The city of St. Louis has, as of April 2017, the highest murder rate, per capita, in the United States, with 188 homicides in 2015 (59.3 homicides per 100,000) and ranks No. 13 of the most dangerous cities in the world by homicide rate. Detroit, Flint, Memphis, Oakland, and some smaller cities with fewer than 100,000 population (ex. Camden) have higher overall violent crime rates than St. Louis, when comparing other crimes such as rape, robbery, and aggravated assault. St. Louis index crime rates have declined almost every year since the peak in 1993 (16,648), to the 2014 level of 7,931 (which is the sum of violent crimes and property crimes) per 100,000. In 2015, the index crime rate reversed the 2005-2014 decline to a level of 8,204. Between 2005 and 2014, violent crime has declined by 20%, although rates of violent crime remains 6 times higher than the United States national average and property crime in the city remains 2 times the national average. St. Louis has a significantly higher homicide rate than the rest of the U.S. for both whites and blacks and a higher proportion committed by males. As of October 2016, 7 of the homicide suspects were white, 95 black, 0 Hispanic, 0 Asian and only 1 female out of the 102 suspects. In 2016, St. Louis was the most dangerous city in the United States with populations of 100,000 or more, ranking 1st in violent crime and 2nd in property crime. It was also ranked 6th of the most dangerous of all establishments in the United States, and East St. Louis, a suburb of the city itself, was ranked 1st. The St. Louis Police Department at the end of 2016 reported a total of 188 murders for the year, the same number of homicides that had occurred in the city in 2015. At the end of 2017, St. Louis had 205 murders (+9%) in a city of fewer than 315,000. The new Chief of Police, John Hayden said two-thirds (67%) of all the murders and one-half of all the assaults are concentrated in a triangular area in the North part of the city.

Greater St. Louis commands the 19th-largest media market in the United States, a position roughly unchanged for over a decade. All of the major U.S. television networks have affiliates in St. Louis, including KTVI 2 (Fox), KMOV 4 (CBS), KSDK 5 (NBC), KETC 9 (PBS), KPLR-TV 11 (CW), KDNL 30 (ABC), WRBU 46 (Ion), and WPXS 51 Daystar Television Network. Among the area's most popular radio stations are KMOX (AM sports and talk, notable as the longtime flagship station for St. Louis Cardinals broadcasts), KLOU (FM oldies), WIL-FM (FM country), WARH (FM adult hits), and KSLZ (FM Top 40 mainstream). St. Louis also supports public radio's KWMU, an NPR affiliate, and community radio's KDHX. KZQZ is a popular Oldies station. All-sports stations, such as KFNS 590 AM "The Fan", WXOS "101.1 ESPN", and KSLG are also popular.

The "St. Louis Post-Dispatch" is the region's major daily newspaper. Others in the region include the "Suburban Journals", which serve parts of St. Louis County, while the primary alternative newspaper is the "Riverfront Times". Three weeklies serve the African-American community: the "St. Louis Argus", the "St. Louis American", and the "St. Louis Sentinel". "St. Louis Magazine", a local monthly magazine, covers topics such as local history, cuisine, and lifestyles, while the weekly "St. Louis Business Journal" provides coverage of regional business news. St. Louis was served by an online newspaper, the "St. Louis Beacon", but that publication merged with KWMU in 2013.

Many books and movies have been written about St. Louis. A few of the most influential and prominent films are "Meet Me in St. Louis" and "American flyers", and novels include "The Killing Dance", "Meet Me in St. Louis", "The Runaway Soul" (novel), "The Rose of Old St. Louis", and "Circus of the Damned".

As St. Louis was a prime location for immigrants to move to, much of the early social work depicting immigrant life was based on St. Louis, such as in the book "The Immigrant of St. Louis".

Road, rail, ship, and air transportation modes connect the city with surrounding communities in Greater St. Louis, national transportation networks, and international locations. St. Louis also supports a public transportation network that includes bus and light rail service.

Four interstate highways connect the city to a larger regional highway system. Interstate 70, an east-west highway, runs roughly from the northwest corner of the city to downtown St. Louis. The north-south Interstate 55 enters the city at the south near the Carondelet neighborhood and runs toward the center of the city, and both Interstate 64 and Interstate 44 enter the city on the west, running parallel to the east. Two of the four interstates (Interstates 55 and 64) merge south of the Gateway Arch National Park and leave the city on the Poplar Street Bridge into Illinois, while Interstate 44 terminates at Interstate 70 at its new interchange near N Broadway and Cass Ave.

The 563-mile Avenue of the Saints links St. Louis with St. Paul, Minnesota.

Major roadways include the north-south Memorial Drive, located on the western edge of the Gateway Arch National Park and parallel to Interstate 70, the north-south streets of Grand Boulevard and Jefferson Avenue, both of which run the length of the city, and Gravois Road, which runs from the southeastern portion of the city to downtown and used to be signed as U.S. Route 66. An east-west roadway that connects the city with surrounding communities is Martin Luther King, Jr. Drive, which carries traffic from the western edge of the city to downtown.

The St. Louis metro area is served by Metrolink (known as Metro) and is the 11th-largest light rail system in the country with 46 mi (74 km) of double track light rail. The Red Line and The Blue Line both serve all the stations in the inner city, and branch to different destinations beyond in the suburban areas. Both lines enter the city north of Forest Park on the western edge of the city or on the Eads Bridge in downtown St. Louis to Illinois. All of the system track is in independent right of way, with both surface level and underground subways track in the city. All stations are independent entry, while all platforms are flush-level with trains. Rail service is provided by the Bi-State Development Agency (also known as Metro), which is funded by a sales taxes levied in the city and other counties in the region. The Gateway Multimodal Transportation Center acts as the hub station in the city of St. Louis, linking the city's light rail system, local bus system, passenger rail service, and national bus service. It is located just east of the historic grand St. Louis Union Station.

St. Louis is served by two passenger airports. St. Louis Lambert International Airport, owned and operated by the City of St. Louis, is 11 miles northwest of downtown along highway I-70 between I-170 and I-270 in St. Louis County. It is the largest and busiest airport in the state. In 2016, the airport has over 255 daily departures to about 90 domestic and international locations and a total of over 13 million passengers. The airport serves as a focus hub city for Southwest Airlines and was a former hub for Trans World Airlines and former focus-city for American Airlines and AmericanConnection. MidAmerica St. Louis Airport is the secondary passenger airport serving the metropolitan area. Located 17 miles east of the city downtown core, the airport serves domestic passengers. Air cargo transportation is available at Lambert International and at other nearby regional airports, including MidAmerica St. Louis Airport, Spirit of St. Louis Airport, and St. Louis Downtown Airport.

The airport has two terminals with a total of five concourses. International flights and passengers use Terminal 2, whose lower level holds the Immigration and Customs gates. Passengers can move between the terminals on complimentary buses that run continuously, or via MetroLink for a fee. It was possible to walk between the terminals until Concourse D was closed in 2008.

River transportation is available through the Port of St. Louis, which is 19.3 miles of riverbank on the Mississippi River that handles more than 32 million tons of freight annually. The Port is the 2nd largest inland port by trip-ton miles, and the 3rd largest by tonnage in the United States, with more than one hundred docking facilities for barge shipping and 16 public terminals on the river. The Port Authority added 2 new small fire and rescue craft in 2012 and 2013.

Inter-city rail passenger train service in the city is provided by Amtrak. All Amtrak trains serving St. Louis use the Gateway Multimodal Transportation Center downtown. Amtrak trains terminating in the city include the "Lincoln Service" to Chicago and the "Missouri River Runner" to Kansas City, Missouri. St. Louis is an intermediate stop on the "Texas Eagle" route which provides long-distance passenger service between San Antonio, Texas, and Chicago.

St. Louis is the nation's third-largest freight rail hub, moving Missouri exports such as fertilizer, gravel, crushed stone, prepared foodstuffs, fats, oils, nonmetallic mineral products, grain, alcohol, tobacco products, automobiles, and automobile parts. Freight rail service in St. Louis is provided on tracks owned by Union Pacific Railroad, Norfolk Southern Railway, Foster Townsend Rail Logistics – formerly Manufacturers Railway (St. Louis), Terminal Railroad Association of St. Louis, Affton Trucking, and the BNSF Railway.

The Terminal Railroad Association of St. Louis (reporting mark: TRRA) is a switching and terminal railroad jointly owned by all the major rail carriers in St. Louis. The company operates 30 diesel-electric locomotives to move railcars around the classification yards, deliver railcars to local industries, and ready trains for departure. The TRRA processes and dispatches a significant portion of railroad traffic moving through the city and owns and operates a network of rail bridges and tunnels including the MacArthur Bridge (St. Louis) and the Merchants Bridge. This infrastructure is also used by inter-city rail and long-distance passenger trains serving St. Louis.

Local bus service in the city of St. Louis is provided by the Bi-State Development Agency via MetroBus, with more than 75 routes connecting to MetroLink commuter rail transit and stops in the city and region. The city is also served by Madison County Transit, which connects downtown St. Louis to Madison County, Illinois. National bus service in the city is offered by Greyhound Lines, Burlington Trailways and Amtrak Thruway Motorcoach, with a station at the Gateway Multimodal Transportation Center, and Megabus, with a stop at St. Louis Union Station.

Taxicab service in the city is provided by private companies regulated by the Metropolitan Taxicab Commission. Rates vary by vehicle type, size, passengers and distance, and by regulation all taxicab fares must be calculated using a taximeter and be payable in cash or credit card. Solicitation by a driver is prohibited, although a taxicab may be hailed on the street or at a stand.
During the early 21st century, St. Louis has 16 sister cities.

With informal relations with Tuguegarao, Philippines.





</doc>
<doc id="27691" url="https://en.wikipedia.org/wiki?curid=27691" title="Social security">
Social security

Social security is "any government system that provides monetary assistance to people with an inadequate or no income."

Social security is enshrined in Article 22 of the Universal Declaration of Human Rights, which states:

Everyone, as a member of society, has the right to social security and is entitled to realization, through national effort and international co-operation and in accordance with the organization and resources of each State, of the economic, social and cultural rights indispensable for his dignity and the free development of his personality.

In simple terms, the signatories agree that the society in which a person lives should help them to develop and to make the most of all the advantages (culture, work, social welfare) which are offered to them in the country.

Social security may also refer to the action programs of an organization intended to promote the welfare of the population through assistance measures guaranteeing access to sufficient resources for food and shelter and to promote health and well-being for the population at large and potentially vulnerable segments such as children, the elderly, the sick and the unemployed. Services providing social security are often called social services.

Terminology in this area is somewhat different in the United States from in the rest of the English-speaking world. The general term for an action program in support of the well being of poor people in the United States is "welfare program", and the general term for all such programs is simply "welfare". In American society, the term "welfare" arguably has negative connotations. In the United States, the term "Social Security" refers to the US social insurance program for all retired and disabled people. Elsewhere the term is used in a much broader sense, referring to the economic security society offers when people are faced with certain risks. In its 1952 Social Security (Minimum Standards) Convention (nr. 102), the International Labour Organization (ILO) defined the traditional contingencies covered by social security as including:


People who cannot reach a guaranteed social minimum for other reasons may be eligible for "social assistance" (or welfare, in American English).

Modern authors often consider the ILO approach too narrow. In their view, social security is not limited to the provision of cash transfers, but also aims at security of work, health, and social participation; and new social risks (single parenthood, the reconciliation of work and family life) should be included in the list as well.

Social security may refer to:

A report published by the ILO in 2014 estimated that only 27% of the world's population has access to comprehensive social security.

While several of the provisions to which the concept refers have a long history (especially in poor relief), the notion of "social security" itself is a fairly recent one. The earliest examples of use date from the 19th century. In a speech to mark the independence of Venezuela, Simón Bolívar (1819) pronounced: "El sistema de gobierno más perfecto es aquel que produce mayor suma de felicidad posible, mayor suma de "seguridad social" y mayor suma de estabilidad política" (which translates to "The most perfect system of government is that which produces the greatest amount of happiness, the greatest amount of social security and the greatest amount of political stability").

In the Roman Empire, the Emperor Trajan (reigned A.D. 98–117) distributed gifts of money and free grain to the poor in the city of Rome, and returned the gifts of gold sent to him upon his accession by cities in Italy and the provinces of the Empire. Trajan's program brought acclaim from many, including Pliny the Younger.

In Jewish tradition, charity (represented by tzedakah) is a matter of religious obligation rather than benevolence. Contemporary charity is regarded as a continuation of the Biblical Maaser Ani, or poor-tithe, as well as Biblical practices, such as permitting the poor to glean the corners of a field and harvest during the Shmita (Sabbatical year). Voluntary charity, along with prayer and repentance, is befriended to ameliorate the consequences of bad acts.

The Song dynasty (c.1000 AD) government supported multiple forms of social assistance programs, including the establishment of retirement homes, public clinics, and pauper's graveyards.

According to economist Robert Henry Nelson, "The medieval Roman Catholic Church operated a far-reaching and comprehensive welfare system for the poor..."

The concepts of welfare and pension were put into practice in the early Islamic law of the Caliphate as forms of "Zakat" (charity), one of the Five Pillars of Islam, since the time of the Rashidun caliph Umar in the 7th century. The taxes (including "Zakat" and "Jizya") collected in the treasury of an Islamic government were used to provide income for the needy, including the poor, elderly, orphans, widowed persons, and the disabled. According to the Islamic jurist Al-Ghazali (Algazel, 1058–1111), the government was also expected to store up food supplies in every region in case a disaster or famine occurred. (See Bayt al-mal for further information.)

There is relatively little statistical data on transfer payments before the High Middle Ages. In the medieval period and until the Industrial Revolution, the function of welfare payments in Europe was principally achieved through private giving or charity. In those early times, there was a much broader group considered to be in poverty as compared to the 21st century.

Early welfare programs in Europe included the English Poor Law of 1601, which gave parishes the responsibility for providing poverty relief assistance to the poor. This system was substantially modified by the 19th-century Poor Law Amendment Act, which introduced the system of workhouses.

It was predominantly in the late 19th and early 20th centuries that an organized system of state welfare provision was introduced in many countries. Otto von Bismarck, Chancellor of Germany, introduced one of the first welfare systems for the working classes in 1883. In Great Britain the Liberal government of Henry Campbell-Bannerman and David Lloyd George introduced the National Insurance system in 1911, a system later expanded by Clement Attlee. The United States did not have an organized welfare system until the Great Depression, when emergency relief measures were introduced under President Franklin D. Roosevelt. Even then, Roosevelt's New Deal focused predominantly on a program of providing work and stimulating the economy through public spending on projects, rather than on cash payment.

This policy is usually applied through various programs designed to provide a population with income at times when they are unable to care for themselves. Income maintenance is based in a combination of five main types of program:

Social protection refers to a set of benefits available (or not available) from the state, market, civil society and households, or through a combination of these agencies, to the individual/households to reduce multi-dimensional deprivation. This multi-dimensional deprivation could be affecting less active poor persons (such as the elderly or the disabled) and active poor persons (such as the unemployed).

This broad framework makes this concept more acceptable in developing countries than the concept of social security. Social security is more applicable in the conditions, where large numbers of citizens depend on the formal economy for their livelihood. Through a defined contribution, this social security may be managed.

But, in the context of widespread informal economy, formal social security arrangements are almost absent for the vast majority of the working population. Besides, in developing countries, the state's capacity to reach the vast majority of the poor people may be limited because of its limited infrastructure and resources. In such a context, multiple agencies that could provide for social protection, including health care, is critical for policy consideration. The framework of social protection is thus holds the state responsible for providing for the poorest populations by regulating non-state agencies.

Collaborative research from the Institute of Development Studies debating Social Protection from a global perspective, suggests that advocates for social protection fall into two broad categories: "instrumentalists" and "activists". Instrumentalists argue that extreme poverty, inequality, and vulnerability is dysfunctional in the achievement of development targets (such as the MDGs). In this view, social protection is about putting in place risk management mechanisms that will compensate for incomplete or missing insurance (and other) markets, until a time that private insurance can play a more prominent role in that society. Activist arguments view the persistence of extreme poverty, inequality, and vulnerability as symptoms of social injustice and structural inequality and see social protection as a right of citizenship. Targeted welfare is a necessary step between humanitarianism and the ideal of a "guaranteed minimum income" where entitlement extends beyond cash or food transfers and is based on citizenship, not philanthropy.

The following proposals were made by the American Association of Retired Persons on its website in 2015:







</doc>
<doc id="27692" url="https://en.wikipedia.org/wiki?curid=27692" title="Steam engine">
Steam engine

A steam engine is a heat engine that performs mechanical work using steam as its working fluid. In simple terms, the steam engine uses the expansion principle of chemistry, where heat applied to water evaporates the water into steam, and the force generated pushes a piston back and forth inside a cylinder. This pushing force is typically transformed, by way of a connecting rod and flywheel, into rotational force for work. The term "steam engine" is generally applied only to reciprocating engines as just described, not to the steam turbine.

Steam engines are external combustion engines, where the working fluid is separated from the combustion products. Non-combustion heat sources such as solar power, nuclear power or geothermal energy may be used. The ideal thermodynamic cycle used to analyze this process is called the Rankine cycle. In the cycle, water is heated and changes into steam in a boiler operating at a high pressure. When expanded using pistons or turbines mechanical work is done. The reduced-pressure steam is then exhausted to the atmosphere, or condensed and pumped back into the boiler.

In general usage, the term "steam engine" can refer to either complete steam plants (including boilers etc.) such as railway steam locomotives and portable engines, or may refer to the piston or turbine machinery alone, as in the beam engine and stationary steam engine. However, a more detailed look at the steam locomotive referred to the engine as only that part where the heat in the steam was turned into motion of the piston, and hence enabled separate statements for boiler efficiency and engine efficiency. Specialized devices such as steam hammers and steam pile drivers are dependent on the steam pressure supplied from a separate boiler.

The use of boiling water to produce mechanical motion goes back over 2000 years, but early devices were not practical. The Spanish inventor Jerónimo de Ayanz y Beaumont obtained a patent for a rudimentary steam-powered water pump in 1606. In 1698 Thomas Savery patented a steam pump that used steam in direct contact with the water being pumped. Savery's steam pump used condensing steam to create a partial vacuum and draw water into a chamber, and then applied pressurized steam to further pump the water.

Thomas Newcomen's "atmospheric engine" was the first commercial true steam engine using a piston, and was used in 1712 for removing flood water from a mine. 104 were in use by 1733. Eventually over two thousand of them were installed.

In 1781 Scottish engineer James Watt patented a steam engine that produced continuous rotary motion. Watt's ten-horsepower engines enabled a wide range of manufacturing machinery to be powered. The engines could be sited anywhere that water and coal or wood fuel could be obtained. By 1883, engines that could provide 10,000 hp had become feasible. The stationary steam engine was a key component of the Industrial Revolution, allowing factories to locate where water power was unavailable. The atmospheric engines of Newcomen and Watt were large compared to the amount of power they produced, but high-pressure steam engines were light enough to be applied to vehicles such as traction engines and railway locomotives.

Reciprocating piston type steam engines remained the dominant source of power until the early 20th century, when advances in the design of electric motors and internal combustion engines gradually resulted in the replacement of reciprocating (piston) steam engines in commercial usage, and the ascendancy of steam turbines in power generation. Considering that the great majority of worldwide electric generation is produced by turbine type steam engines, the "steam age" is continuing with energy levels far beyond those of the turn of the 19th and 20th century.

Although steam powered devices were developed before the first practical piston steam engine, they were not directly connected to the Newcomen atmospheric engine. The Newcomen engine owes its development to the discovery of atmospheric pressure and to shared technical information whose path is traceable. For development of the commercial steam engine see: History of the steam engine#Development of the commercial steam engine


Since the early 18th century, steam power has been applied to a variety of practical uses. At first it powered reciprocating pumps, but from the 1780s rotative engines (those converting reciprocating motion into rotary motion) began to appear, driving factory machinery such as power looms. Speed control in response to changing load made direct application of a steam engine to spinning machinery impractical until the invention of the Corliss engine in 1848. Until then steam engines were used to pump water to turn a water wheel, which powered the spinning machinery. At the turn of the 19th century, steam-powered transport on both sea and land began to make its appearance, becoming more dominant as the century progressed.
Steam engines can be said to have been the moving force behind the Industrial Revolution and saw widespread commercial use driving machinery in factories, mills and mines; powering pumping stations; and propelling transport appliances such as railway locomotives, ships, steamboats and road vehicles. Their use in agriculture led to an increase in the land available for cultivation. There have at one time or another been steam-powered farm tractors, motorcycles (without much success) and even automobiles as the Stanley Steamer.

The weight of boilers and condensers generally makes the power-to-weight ratio of a steam plant lower than for internal combustion engines. For mobile applications steam has been largely superseded by internal combustion engines or electric motors. However, most electric power is generated using steam turbine plant, so that indirectly the world's industry is still dependent on steam power. Recent concerns about fuel sources and pollution have incited a renewed interest in steam both as a component of cogeneration processes and as a prime mover. This is becoming known as the Advanced Steam movement.

The history of the steam engine stretches back as far as the first century; the first recorded rudimentary steam-powered "engine" being the aeolipile described by Hero of Alexandria, a mathematician and engineer in Roman Egypt. In the following centuries, the few steam-powered "engines" known were, like the aeolipile, essentially experimental devices used by inventors to demonstrate the properties of steam. A rudimentary steam turbine device was described by Taqi al-Din in Ottoman Egypt in 1551 and by Giovanni Branca in Italy in 1629. Jerónimo de Ayanz y Beaumont received patents in 1606 for fifty steam powered inventions, including a water pump for draining inundated mines. Denis Papin, a Huguenot refugee, did some useful work on the steam digester in 1679, and first used a piston to raise weights in 1690.

The first commercial steam-powered device was a water pump, developed in 1698 by Thomas Savery. It used condensing steam to create a vacuum which was used to raise water from below, then it used steam pressure to raise it higher. Small engines were effective though larger models were problematic. They proved only to have a limited lift height and were prone to boiler explosions. It received some use in mines, pumping stations and for supplying water wheels used to power textile machinery. An attractive feature of the Savery engine was its low cost. Bento de Moura Portugal introduced an ingenious improvement of Savery's construction "to render it capable of working itself", as described by John Smeaton in the Philosophical Transactions published in 1751. It continued to be manufactured until the late 18th century. One engine was still known to be operating in 1820.

The first commercially successful true engine, in that it could generate power and transmit it to a machine, was the atmospheric engine, invented by Thomas Newcomen around 1712. It was an improvement over Savery's steam pump, using a piston as proposed by Papin. Newcomen's engine was relatively inefficient, and in most cases was used for pumping water. It worked by creating a partial vacuum by condensing steam under a piston within a cylinder. It was employed for draining mine workings at depths hitherto impossible, and also for providing a reusable water supply for driving waterwheels at factories sited away from a suitable "head". Water that had passed over the wheel was pumped back up into a storage reservoir above the wheel.

In 1720 Jacob Leupold described a two-cylinder high-pressure steam engine. The invention was published in his major work "Theatri Machinarum Hydraulicarum". The engine used two heavy pistons to provide motion to a water pump. Each piston was raised by the steam pressure and returned to its original position by gravity. The two pistons shared a common four way rotary valve connected directly to a steam boiler.
The next major step occurred when James Watt developed (1763–1775) an improved version of Newcomen's engine, with a separate condenser. Boulton and Watt's early engines used half as much coal as John Smeaton's improved version of Newcomen's. Newcomen's and Watt's early engines were "atmospheric". They were powered by air pressure pushing a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam. The engine cylinders had to be large because the only usable force acting on them was due to atmospheric pressure.

Watt proceeded to develop his engine further, modifying it to provide a rotary motion suitable for driving factory machinery. This enabled factories to be sited away from rivers, and further accelerated the pace of the Industrial Revolution.

The meaning of high pressure, together with an actual value above ambient, depends on the era in which the term was used. For early use of the term Van Reimsdijk refers to steam being at a sufficiently high pressure that it could be exhausted to atmosphere without reliance on a vacuum to enable it to perform useful work. Ewing states that Watt's condensing engines were known, at the time, as low pressure compared to high pressure, non-condensing engines of the same period.

Watt's patent prevented others from making high pressure and compound engines. Shortly after Watt's patent expired in 1800, Richard Trevithick and, separately, Oliver Evans in 1801 introduced engines using high-pressure steam; Trevithick obtained his high-pressure engine patent in 1802, and Evans had made several working models before then. These were much more powerful for a given cylinder size than previous engines and could be made small enough for transport applications. Thereafter, technological developments and improvements in manufacturing techniques (partly brought about by the adoption of the steam engine as a power source) resulted in the design of more efficient engines that could be smaller, faster, or more powerful, depending on the intended application.

The Cornish engine was developed by Trevithick and others in the 1810s. It was a compound cycle engine that used high-pressure steam expansively, then condensed the low-pressure steam, making it relatively efficient. The Cornish engine had irregular motion and torque though the cycle, limiting it mainly to pumping. Cornish engines were used in mines and for water supply until the late 19th century.

Early builders of stationary steam engines considered that horizontal cylinders would be subject to excessive wear. Their engines were therefore arranged with the piston axis vertical. In time the horizontal arrangement became more popular, allowing compact, but powerful engines to be fitted in smaller spaces.

The acme of the horizontal engine was the Corliss steam engine, patented in 1849, which was a four-valve counter flow engine with separate steam admission and exhaust valves and automatic variable steam cutoff. When Corliss was given the Rumford Medal, the committee said that "no one invention since Watt's time has so enhanced the efficiency of the steam engine". In addition to using 30% less steam, it provided more uniform speed due to variable steam cut off, making it well suited to manufacturing, especially cotton spinning.

The first experimental road going steam powered vehicles were built in the late 18th century, but it was not until after Richard Trevithick had developed the use of high-pressure steam, around 1800, that mobile steam engines became a practical proposition. The first half of the 19th century saw great progress in steam vehicle design, and by the 1850s it was becoming viable to produce them on a commercial basis. This progress was dampened by legislation which limited or prohibited the use of steam powered vehicles on roads. Improvements in vehicle technology continued from the 1860s to the 1920s. Steam road vehicles were used for many applications. In the 20th century, the rapid development of internal combustion engine technology led to the demise of the steam engine as a source of propulsion of vehicles on a commercial basis, with relatively few remaining in use beyond the Second World War. Many of these vehicles were acquired by enthusiasts for preservation, and numerous examples are still in existence. In the 1960s the air pollution problems in California gave rise to a brief period of interest in developing and studying steam powered vehicles as a possible means of reducing the pollution. Apart from interest by steam enthusiasts, the occasional replica vehicle, and experimental technology no steam vehicles are in production at present.

Near the end of the 19th century compound engines came into widespread use. Compound engines exhausted steam in to successively larger cylinders to accommodate the higher volumes at reduced pressures, giving improved efficiency. These stages were called expansions, with double- and triple-expansion engines being common, especially in shipping where efficiency was important to reduce the weight of coal carried. Steam engines remained the dominant source of power until the early 20th century, when advances in the design of electric motors and internal combustion engines gradually resulted in the replacement of reciprocating (piston) steam engines, with shipping in the 20th-century relying upon the steam turbine.

As the development of steam engines progressed through the 18th century, various attempts were made to apply them to road and railway use. In 1784, William Murdoch, a Scottish inventor, built a prototype steam road locomotive. An early working model of a steam rail locomotive was designed and constructed by steamboat pioneer John Fitch in the United States probably during the 1780s or 1790s.
His steam locomotive used interior bladed wheels guided by rails or tracks.

The first full-scale working railway steam locomotive was built by Richard Trevithick in the United Kingdom and, on 21 February 1804, the world's first railway journey took place as Trevithick's unnamed steam locomotive hauled a train along the tramway from the Pen-y-darren ironworks, near Merthyr Tydfil to Abercynon in south Wales. The design incorporated a number of important innovations that included using high-pressure steam which reduced the weight of the engine and increased its efficiency. Trevithick visited the Newcastle area later in 1804 and the colliery railways in north-east England became the leading centre for experimentation and development of steam locomotives.

Trevithick continued his own experiments using a trio of locomotives, concluding with the Catch Me Who Can in 1808. Only four years later, the successful twin-cylinder locomotive "Salamanca" by Matthew Murray was used by the edge railed rack and pinion Middleton Railway. In 1825 George Stephenson built the "Locomotion" for the Stockton and Darlington Railway. This was the first public steam railway in the world and then in 1829, he built "The Rocket" which was entered in and won the Rainhill Trials. The Liverpool and Manchester Railway opened in 1830 making exclusive use of steam power for both passenger and freight trains.

Steam locomotives continued to be manufactured until the late twentieth century in places such as China and the former East Germany (where the DR Class 52.80 was produced).

The final major evolution of the steam engine design was the use of steam turbines starting in the late part of the 19th century. Steam turbines are generally more efficient than reciprocating piston type steam engines (for outputs above several hundred horsepower), have fewer moving parts, and provide rotary power directly instead of through a connecting rod system or similar means. Steam turbines virtually replaced reciprocating engines in electricity generating stations early in the 20th century, where their efficiency, higher speed appropriate to generator service, and smooth rotation were advantages. Today most electric power is provided by steam turbines. In the United States 90% of the electric power is produced in this way using a variety of heat sources. Steam turbines were extensively applied for propulsion of large ships throughout most of the 20th century.

Although the reciprocating steam engine is no longer in widespread commercial use, various companies are exploring or exploiting the potential of the engine as an alternative to internal combustion engines. The company Energiprojekt AB in Sweden has made progress in using modern materials for harnessing the power of steam. The efficiency of Energiprojekt's steam engine reaches some 27-30% on high-pressure engines. It is a single-step, 5-cylinder engine (no compound) with superheated steam and consumes approx. of steam per kWh.

There are two fundamental components of a steam plant: the boiler or steam generator, and the "motor unit", referred to itself as a "steam engine". Stationary steam engines in fixed buildings may have the boiler and engine in separate buildings some distance apart. For portable or mobile use, such as steam locomotives, the two are mounted together.

The widely used reciprocating engine typically consisted of a cast iron cylinder, piston, connecting rod and beam or a crank and flywheel, and miscellaneous linkages. Steam was alternately supplied and exhausted by one or more valves. Speed control was either automatic, using a governor, or by a manual valve. The cylinder casting contained steam supply and exhaust ports.

Engines equipped with a condenser are a separate type than those that exhaust to the atmosphere.

Other components are often present; pumps (such as an injector) to supply water to the boiler during operation, condensers to recirculate the water and recover the latent heat of vaporisation, and superheaters to raise the temperature of the steam above its saturated vapour point, and various mechanisms to increase the draft for fireboxes. When coal is used, a chain or screw stoking mechanism and its drive engine or motor may be included to move the fuel from a supply bin (bunker) to the firebox. See: Mechanical stoker

The heat required for boiling the water and raising the temperature of the steam can be derived from various sources, most commonly from burning combustible materials with an appropriate supply of air in a closed space (called variously combustion chamber, firebox, furnace). In some cases the heat source is a nuclear reactor, geothermal energy, solar energy or waste heat from an internal combustion engine or industrial process. In the case of model or toy steam engines, the heat source can be an electric heating element.

Boilers are pressure vessels that contain water to be boiled, and features that transfer the heat to the water as effectively as possible.

The two most common types are:

Fire tube boilers were the main type used for early high-pressure steam (typical steam locomotive practice), but they were to a large extent displaced by more economical water tube boilers in the late 19th century for marine propulsion and large stationary applications.

Many boilers raise the temperature of the steam after it has left that part of the boiler where it is in contact with the water. Known as superheating it turns 'wet steam' into 'superheated steam'. It avoids the steam condensing in the engine cylinders, and gives a significantly higher efficiency.

In a steam engine, a piston or steam turbine or any other similar device for doing mechanical work takes a supply of steam at high pressure and temperature and gives out a supply of steam at lower pressure and temperature, using as much of the difference in steam energy as possible to do mechanical work.

These "motor units" are often called 'steam engines' in their own right. Engines using compressed air or other gases differ from steam engines only in details that depend on the nature of the gas although compressed air has been used in steam engines without change.

As with all heat engines, the majority of primary energy must be emitted as waste heat at relatively low temperature.

The simplest cold sink is to vent the steam to the environment. This is often used on steam locomotives, as the released steam is vented up the chimney so as to increase the draw on the fire, which greatly increases engine power, but reduces efficiency.

Sometimes the waste heat is useful itself, and in those cases very high overall efficiency can be obtained. For example, combined heat and power (CHP) systems use the waste steam for district heating, exceeding 80% combined efficiency.

Where CHP is not used, steam turbines in stationary power plants use surface condensers as a cold sink. The condensers are cooled by water flow from oceans, rivers, lakes, and often by cooling towers which evaporate water to provide cooling energy removal. The resulting condensed hot water, is then pumped back up to pressure and sent back to the boiler. A dry type cooling tower is similar to an automobile radiator and is used in locations where water is costly. Waste heat can also be ejected by evaporative (wet) cooling towers use pass the rejected to external water cycle that evaporates some of flow to the air. Cooling towers often have visible plumes due to the evaporated water condensing into droplets carried up by the warm air. Evaporative cooling towers need less water flow than "once-through" cooling by river or lake water; a 700 megawatt coal-fired power plant may use about 3600 cubic metres of make-up water every hour for evaporative cooling, but would need about twenty times as much if cooled by river water. Evaporative water cannot be used for subsequent purposes (other than rain somewhere), whereas river water can be re-used. In all cases, the steam plant water, which must be kept pure, is kept separate from the cooling water or air, and once the low-pressure steam condenses into water, it is returned to the boiler.

The Rankine cycle and most practical steam engines have a water pump to recycle or top up the boiler water, so that they may be run continuously. Utility and industrial boilers commonly use multi-stage centrifugal pumps; however, other types are used. Another means of supplying lower-pressure boiler feed water is an injector, which uses a steam jet usually supplied from the boiler. Injectors became popular in the 1850s but are no longer widely used, except in applications such as steam locomotives. It is the pressurization of the water that circulates through the steam boiler that allows the water to be raised to temperatures well above 100 °C boiling point of water at one atmospheric pressure, and by that means to increase the efficiency of the steam cycle.

For safety reasons, nearly all steam engines are equipped with mechanisms to monitor the boiler, such as a pressure gauge and a sight glass to monitor the water level.

Many engines, stationary and mobile, are also fitted with a governor to regulate the speed of the engine without the need for human interference.

The most useful instrument for analyzing the performance of steam engines is the steam engine indicator. Early versions were in use by 1851, but the most successful indicator was developed for the high speed engine inventor and manufacturer Charles Porter by Charles Richard and exhibited at London Exhibition in 1862. The steam engine indicator traces on paper the pressure in the cylinder throughout the cycle, which can be used to spot various problems and calculate developed horsepower. It was routinely used by engineers, mechanics and insurance inspectors. The engine indicator can also be used on internal combustion engines. See image of indicator diagram below (in "Types of motor units" section).

The centrifugal governor was adopted by James Watt for use on a steam engine in 1788 after Watt's partner Boulton saw one on the equipment of a flour mill Boulton & Watt were building. The governor could not actually hold a set speed, because it would assume a new constant speed in response to load changes. The governor was able to handle smaller variations such as those caused by fluctuating heat load to the boiler. Also, there was a tendency for oscillation whenever there was a speed change. As a consequence, engines equipped only with this governor were not suitable for operations requiring constant speed, such as cotton spinning. The governor was improved over time and coupled with variable steam cut off, good speed control in response to changes in load was attainable near the end of the 19th century.

In a simple engine, or "single expansion engine" the charge of steam passes through the entire expansion process in an individual cylinder, although a simple engine may have one or more individual cylinders. It is then exhausted directly into the atmosphere or into a condenser. As steam expands in passing through a high-pressure engine, its temperature drops because no heat is being added to the system; this is known as adiabatic expansion and results in steam entering the cylinder at high temperature and leaving at lower temperature. This causes a cycle of heating and cooling of the cylinder with every stroke, which is a source of inefficiency.

The dominant efficiency loss in reciprocating steam engines is cylinder condensation and re-evaporation. The steam cylinder and adjacent metal parts/ports operate at a temperature about half way between the steam admission saturation temperature and the saturation temperature corresponding to the exhaust pressure. As high pressure steam is admitted into the working cylinder, much of the high temperature steam is condensed as water droplets onto the metal surfaces, significantly reducing the steam available for expansive work. When the expanding steam reaches low pressure (especially during the exhaust stroke), the previously deposited water droplets that had just been formed within the cylinder/ports now boil away (re-evaporation) and this steam does no further work in the cylinder.

There are practical limits on the expansion ratio of a steam engine cylinder, as increasing cylinder surface area tends to exacerbate the cylinder condensation and re-evaporation issues. This negates the theoretical advantages associated with a high ratio of expansion in an individual cylinder.

A method to lessen the magnitude of energy loss to a very long cylinder was invented in 1804 by British engineer Arthur Woolf, who patented his "Woolf high-pressure compound engine" in 1805. In the compound engine, high-pressure steam from the boiler expands in a high-pressure (HP) cylinder and then enters one or more subsequent lower-pressure (LP) cylinders. The complete expansion of the steam now occurs across multiple cylinders, with the overall temperature drop within each cylinder reduced considerably. By expanding the steam in steps with smaller temperature range (within each cylinder) the condensation and re-evaporation efficiency issue (described above) is reduced. This reduces the magnitude of cylinder heating and cooling, increasing the efficiency of the engine. By staging the expansion in multiple cylinders, variations of torque can be reduced. To derive equal work from lower-pressure cylinder requires a larger cylinder volume as this steam occupies a greater volume. Therefore, the bore, and in rare cases the stroke, are increased in low-pressure cylinders, resulting in larger cylinders.

Double-expansion (usually known as compound) engines expanded the steam in two stages. The pairs may be duplicated or the work of the large low-pressure cylinder can be split with one high-pressure cylinder exhausting into one or the other, giving a three-cylinder layout where cylinder and piston diameter are about the same, making the reciprocating masses easier to balance.

Two-cylinder compounds can be arranged as:


With two-cylinder compounds used in railway work, the pistons are connected to the cranks as with a two-cylinder simple at 90° out of phase with each other ("quartered"). When the double-expansion group is duplicated, producing a four-cylinder compound, the individual pistons within the group are usually balanced at 180°, the groups being set at 90° to each other. In one case (the first type of Vauclain compound), the pistons worked in the same phase driving a common crosshead and crank, again set at 90° as for a two-cylinder engine. With the three-cylinder compound arrangement, the LP cranks were either set at 90° with the HP one at 135° to the other two, or in some cases all three cranks were set at 120°.

The adoption of compounding was common for industrial units, for road engines and almost universal for marine engines after 1880; it was not universally popular in railway locomotives where it was often perceived as complicated. This is partly due to the harsh railway operating environment and limited space afforded by the loading gauge (particularly in Britain, where compounding was never common and not employed after 1930). However, although never in the majority, it was popular in many other countries.

It is a logical extension of the compound engine (described above) to split the expansion into yet more stages to increase efficiency. The result is the multiple-expansion engine. Such engines use either three or four expansion stages and are known as "triple-" and "quadruple-expansion engines" respectively. These engines use a series of cylinders of progressively increasing diameter. These cylinders are designed to divide the work into equal shares for each expansion stage. As with the double-expansion engine, if space is at a premium, then two smaller cylinders may be used for the low-pressure stage. Multiple-expansion engines typically had the cylinders arranged inline, but various other formations were used. In the late 19th century, the Yarrow-Schlick-Tweedy balancing "system" was used on some marine triple-expansion engines. Y-S-T engines divided the low-pressure expansion stages between two cylinders, one at each end of the engine. This allowed the crankshaft to be better balanced, resulting in a smoother, faster-responding engine which ran with less vibration. This made the four-cylinder triple-expansion engine popular with large passenger liners (such as the "Olympic" class), but this was ultimately replaced by the virtually vibration-free turbine engine. It is noted however that triple expansion reciprocating steam engines were used to drive the WWII Liberty ships, by far the largest number of identical ships ever built. 2700 ships were built, in the USA, from a British original design. 

The image to the right shows an animation of a triple-expansion engine. The steam travels through the engine from left to right. The valve chest for each of the cylinders is to the left of the corresponding cylinder.

Land-based steam engines could exhaust their steam to atmosphere, as feed water was usually readily available. Prior to and during World War I, the expansion engine dominated marine applications, where high vessel speed was not essential. It was, however, superseded by the British invention steam turbine where speed was required, for instance in warships, such as the dreadnought battleships, and ocean liners. of 1905 was the first major warship to replace the proven technology of the reciprocating engine with the then-novel steam turbine.

In most reciprocating piston engines, the steam reverses its direction of flow at each stroke (counterflow), entering and exhausting from the same end of the cylinder. The complete engine cycle occupies one rotation of the crank and two piston strokes; the cycle also comprises four "events" – admission, expansion, exhaust, compression. These events are controlled by valves often working inside a "steam chest" adjacent to the cylinder; the valves distribute the steam by opening and closing steam "ports" communicating with the cylinder end(s) and are driven by valve gear, of which there are many types.

The simplest valve gears give events of fixed length during the engine cycle and often make the engine rotate in only one direction. Many however have a reversing mechanism which additionally can provide means for saving steam as speed and momentum are gained by gradually "shortening the cutoff" or rather, shortening the admission event; this in turn proportionately lengthens the expansion period. However, as one and the same valve usually controls both steam flows, a short cutoff at admission adversely affects the exhaust and compression periods which should ideally always be kept fairly constant; if the exhaust event is too brief, the totality of the exhaust steam cannot evacuate the cylinder, choking it and giving excessive compression (""kick back"").

In the 1840s and 50s, there were attempts to overcome this problem by means of various patent valve gears with a separate, variable cutoff expansion valve riding on the back of the main slide valve; the latter usually had fixed or limited cutoff. The combined setup gave a fair approximation of the ideal events, at the expense of increased friction and wear, and the mechanism tended to be complicated. The usual compromise solution has been to provide "lap" by lengthening rubbing surfaces of the valve in such a way as to overlap the port on the admission side, with the effect that the exhaust side remains open for a longer period after cut-off on the admission side has occurred. This expedient has since been generally considered satisfactory for most purposes and makes possible the use of the simpler Stephenson, Joy and Walschaerts motions. Corliss, and later, poppet valve gears had separate admission and exhaust valves driven by trip mechanisms or cams profiled so as to give ideal events; most of these gears never succeeded outside of the stationary marketplace due to various other issues including leakage and more delicate mechanisms.

Before the exhaust phase is quite complete, the exhaust side of the valve closes, shutting a portion of the exhaust steam inside the cylinder. This determines the compression phase where a cushion of steam is formed against which the piston does work whilst its velocity is rapidly decreasing; it moreover obviates the pressure and temperature shock, which would otherwise be caused by the sudden admission of the high-pressure steam at the beginning of the following cycle.

The above effects are further enhanced by providing "lead": as was later discovered with the internal combustion engine, it has been found advantageous since the late 1830s to advance the admission phase, giving the valve "lead" so that admission occurs a little before the end of the exhaust stroke in order to fill the "clearance volume" comprising the ports and the cylinder ends (not part of the piston-swept volume) before the steam begins to exert effort on the piston.

Uniflow engines attempt to remedy the difficulties arising from the usual counterflow cycle where, during each stroke, the port and the cylinder walls will be cooled by the passing exhaust steam, whilst the hotter incoming admission steam will waste some of its energy in restoring working temperature. The aim of the uniflow is to remedy this defect and improve efficiency by providing an additional port uncovered by the piston at the end of each stroke making the steam flow only in one direction. By this means, the simple-expansion uniflow engine gives efficiency equivalent to that of classic compound systems with the added advantage of superior part-load performance, and comparable efficiency to turbines for smaller engines below one thousand horsepower. However, the thermal expansion gradient uniflow engines produce along the cylinder wall gives practical difficulties.. The Quasiturbine is a uniflow rotary steam engine where steam intakes in hot areas, while exhausting in cold areas.

A steam turbine consists of one or more "rotors" (rotating discs) mounted on a drive shaft, alternating with a series of "stators" (static discs) fixed to the turbine casing. The rotors have a propeller-like arrangement of blades at the outer edge. Steam acts upon these blades, producing rotary motion. The stator consists of a similar, but fixed, series of blades that serve to redirect the steam flow onto the next rotor stage. A steam turbine often exhausts into a surface condenser that provides a vacuum. The stages of a steam turbine are typically arranged to extract the maximum potential work from a specific velocity and pressure of steam, giving rise to a series of variably sized high- and low-pressure stages. Turbines are only efficient if they rotate at relatively high speed, therefore they are usually connected to reduction gearing to drive lower speed applications, such as a ship's propeller. In the vast majority of large electric generating stations, turbines are directly connected to generators with no reduction gearing. Typical speeds are 3600 revolutions per minute (RPM) in the USA with 60 Hertz power, and 3000 RPM in Europe and other countries with 50 Hertz electric power systems. In nuclear power applications the turbines typically run at half these speeds, 1800 RPM and 1500 RPM. A turbine rotor is also only capable of providing power when rotating in one direction. Therefore, a reversing stage or gearbox is usually required where power is required in the opposite direction.

Steam turbines provide direct rotational force and therefore do not require a linkage mechanism to convert reciprocating to rotary motion. Thus, they produce smoother rotational forces on the output shaft. This contributes to a lower maintenance requirement and less wear on the machinery they power than a comparable reciprocating engine.

The main use for steam turbines is in electricity generation (in the 1990s about 90% of the world's electric production was by use of steam turbines) however the recent widespread application of large gas turbine units and typical combined cycle power plants has resulted in reduction of this percentage to the 80% regime for steam turbines. In electricity production, the high speed of turbine rotation matches well with the speed of modern electric generators, which are typically direct connected to their driving turbines. In marine service, (pioneered on the "Turbinia"), steam turbines with reduction gearing (although the Turbinia has direct turbines to propellers with no reduction gearbox) dominated large ship propulsion throughout the late 20th century, being more efficient (and requiring far less maintenance) than reciprocating steam engines. In recent decades, reciprocating Diesel engines, and gas turbines, have almost entirely supplanted steam propulsion for marine applications.

Virtually all nuclear power plants generate electricity by heating water to provide steam that drives a turbine connected to an electrical generator. Nuclear-powered ships and submarines either use a steam turbine directly for main propulsion, with generators providing auxiliary power, or else employ turbo-electric transmission, where the steam drives a turbo generator set with propulsion provided by electric motors. A limited number of steam turbine railroad locomotives were manufactured. Some non-condensing direct-drive locomotives did meet with some success for long haul freight operations in Sweden and for express passenger work in Britain, but were not repeated. Elsewhere, notably in the U.S.A., more advanced designs with electric transmission were built experimentally, but not reproduced. It was found that steam turbines were not ideally suited to the railroad environment and these locomotives failed to oust the classic reciprocating steam unit in the way that modern diesel and electric traction has done.

An oscillating cylinder steam engine is a variant of the simple expansion steam engine which does not require valves to direct steam into and out of the cylinder. Instead of valves, the entire cylinder rocks, or oscillates, such that one or more holes in the cylinder line up with holes in a fixed port face or in the pivot mounting (trunnion). These engines are mainly used in toys and models, because of their simplicity, but have also been used in full size working engines, mainly on ships where their compactness is valued.

It is possible to use a mechanism based on a pistonless rotary engine such as the Wankel engine in place of the cylinders and valve gear of a conventional reciprocating steam engine. Many such engines have been designed, from the time of James Watt to the present day, but relatively few were actually built and even fewer went into quantity production; see link at bottom of article for more details. The major problem is the difficulty of sealing the rotors to make them steam-tight in the face of wear and thermal expansion; the resulting leakage made them very inefficient. Lack of expansive working, or any means of control of the cutoff, is also a serious problem with many such designs.

By the 1840s, it was clear that the concept had inherent problems and rotary engines were treated with some derision in the technical press. However, the arrival of electricity on the scene, and the obvious advantages of driving a dynamo directly from a high-speed engine, led to something of a revival in interest in the 1880s and 1890s, and a few designs had some limited success.. The Quasiturbine is a new type of uniflow rotary steam engine.

Of the few designs that were manufactured in quantity, those of the Hult Brothers Rotary Steam Engine Company of Stockholm, Sweden, and the spherical engine of Beauchamp Tower are notable. Tower's engines were used by the Great Eastern Railway to drive lighting dynamos on their locomotives, and by the Admiralty for driving dynamos on board the ships of the Royal Navy. They were eventually replaced in these niche applications by steam turbines.

The aeolipile represents the use of steam by the rocket-reaction principle, although not for direct propulsion.

In more modern times there has been limited use of steam for rocketry – particularly for rocket cars. Steam rocketry works by filling a pressure vessel with hot water at high pressure and opening a valve leading to a suitable nozzle. The drop in pressure immediately boils some of the water and the steam leaves through a nozzle, creating a propulsive force.

Steam engines possess boilers and other components that are pressure vessels that contain a great deal of potential energy. Steam escapes and boiler explosions (typically BLEVEs) can and have in the past caused great loss of life. While variations in standards may exist in different countries, stringent legal, testing, training, care with manufacture, operation and certification is applied to ensure safety.

Failure modes may include:

Steam engines frequently possess two independent mechanisms for ensuring that the pressure in the boiler does not go too high; one may be adjusted by the user, the second is typically designed as an ultimate fail-safe. Such safety valves traditionally used a simple lever to restrain a plug valve in the top of a boiler. One end of the lever carried a weight or spring that restrained the valve against steam pressure. Early valves could be adjusted by engine drivers, leading to many accidents when a driver fastened the valve down to allow greater steam pressure and more power from the engine. The more recent type of safety valve uses an adjustable spring-loaded valve, which is locked such that operators may not tamper with its adjustment unless a seal illegally is broken. This arrangement is considerably safer. 

Lead fusible plugs may be present in the crown of the boiler's firebox. If the water level drops, such that the temperature of the firebox crown increases significantly, the lead melts and the steam escapes, warning the operators, who may then manually suppress the fire. Except in the smallest of boilers the steam escape has little effect on dampening the fire. The plugs are also too small in area to lower steam pressure significantly, depressurizing the boiler. If they were any larger, the volume of escaping steam would itself endanger the crew.

The Rankine cycle is the fundamental thermodynamic underpinning of the steam engine. The cycle is an arrangement of components as is typically used for simple power production, and utilizes the phase change of water (boiling water producing steam, condensing exhaust steam, producing liquid water)) to provide a practical heat/power conversion system. The heat is supplied externally to a closed loop with some of the heat added being converted to work and the waste heat being removed in a condenser. The Rankine cycle is used in virtually all steam power production applications. In the 1990s, Rankine steam cycles generated about 90% of all electric power used throughout the world, including virtually all solar, biomass, coal and nuclear power plants. It is named after William John Macquorn Rankine, a Scottish polymath.

The Rankine cycle is sometimes referred to as a practical Carnot cycle because, when an efficient turbine is used, the TS diagram begins to resemble the Carnot cycle. The main difference is that heat addition (in the boiler) and rejection (in the condenser) are isobaric (constant pressure) processes in the Rankine cycle and isothermal (constant temperature) processes in the theoretical Carnot cycle. In this cycle a pump is used to pressurize the working fluid which is received from the condenser as a liquid not as a gas. Pumping the working fluid in liquid form during the cycle requires a small fraction of the energy to transport it compared to the energy needed to compress the working fluid in gaseous form in a compressor (as in the Carnot cycle). The cycle of a reciprocating steam engine differs from that of turbines because of condensation and re-evaporation occurring in the cylinder or in the steam inlet passages.

The working fluid in a Rankine cycle can operate as a closed loop system, where the working fluid is recycled continuously, or may be an "open loop" system, where the exhaust steam is directly released to the atmosphere, and a separate source of water feeding the boiler is supplied. Normally water is the fluid of choice due to its favourable properties, such as non-toxic and unreactive chemistry, abundance, low cost, and its thermodynamic properties. Mercury is the working fluid in the mercury vapor turbine. Low boiling hydrocarbons can be used in a binary cycle.

The steam engine contributed much to the development of thermodynamic theory; however, the only applications of scientific theory that influenced the steam engine were the original concepts of harnessing the power of steam and atmospheric pressure and knowledge of properties of heat and steam. The experimental measurements made by Watt on a model steam engine led to the development of the separate condenser. Watt independently discovered latent heat, which was confirmed by the original discoverer Joseph Black, who also advised Watt on experimental procedures. Watt was also aware of the change in the boiling point of water with pressure. Otherwise, the improvements to the engine itself were more mechanical in nature. The thermodynamic concepts of the Rankine cycle did give engineers the understanding needed to calculate efficiency which aided the development of modern high-pressure and -temperature boilers and the steam turbine.

The efficiency of an engine cycle can be calculated by dividing the energy output of mechanical work that the engine produces by the energy input to the engine by the burning fuel.

The historical measure of a steam engine's energy efficiency was its "duty". The concept of duty was first introduced by Watt in order to illustrate how much more efficient his engines were over the earlier Newcomen designs. Duty is the number of foot-pounds of work delivered by burning one bushel (94 pounds) of coal. The best examples of Newcomen designs had a duty of about 7 million, but most were closer to 5 million. Watt's original low-pressure designs were able to deliver duty as high as 25 million, but averaged about 17. This was a three-fold improvement over the average Newcomen design. Early Watt engines equipped with high-pressure steam improved this to 65 million.

No heat engine can be more efficient than the Carnot cycle, in which heat is moved from a high temperature reservoir to one at a low temperature, and the efficiency depends on the temperature difference. For the greatest efficiency, steam engines should be operated at the highest steam temperature possible (superheated steam), and release the waste heat at the lowest temperature possible.

The efficiency of a Rankine cycle is usually limited by the working fluid. Without the pressure reaching supercritical levels for the working fluid, the temperature range the cycle can operate over is quite small; in steam turbines, turbine entry temperatures are typically 565 °C (the creep limit of stainless steel) and condenser temperatures are around 30 °C. This gives a theoretical Carnot efficiency of about 63% compared with an actual efficiency of 42% for a modern coal-fired power station. This low turbine entry temperature (compared with a gas turbine) is why the Rankine cycle is often used as a bottoming cycle in combined-cycle gas turbine power stations.

One of the principal advantages the Rankine cycle holds over others is that during the compression stage relatively little work is required to drive the pump, the working fluid being in its liquid phase at this point. By condensing the fluid, the work required by the pump consumes only 1% to 3% of the turbine (or reciprocating engine)power and contributes to a much higher efficiency for a real cycle. The benefit of this is lost somewhat due to the lower heat addition temperature. Gas turbines, for instance, have turbine entry temperatures approaching 1500 °C. Nonetheless, the efficiencies of actual large steam cycles and large modern gas turbines are fairly well matched.

In practice, a reciprocating steam engine cycle exhausting the steam to atmosphere will typically have an efficiency (including the boiler) in the range of 1-10%, but with the addition of a condenser and multiple expansion, and high steam pressure/temperature, it may be greatly improved, historically into the regime of 10-20%, and very rarely slightly higher.

A modern large electrical power station (producing several hundred megawatts of electrical output) with steam reheat, economizer etc. will achieve efficiency in the mid 40% range, with the most efficient units approaching 50% thermal efficiency.

It is also possible to capture the waste heat using cogeneration in which the waste heat is used for heating a lower boiling point working fluid or as a heat source for district heating via saturated low-pressure steam.




</doc>
<doc id="27694" url="https://en.wikipedia.org/wiki?curid=27694" title="Satan">
Satan

Satan, also known as the Devil, is an entity in the Abrahamic religions that seduces humans into sin. In Christianity and Islam, he is usually seen as a fallen angel, or a jinni, who used to possess great piety and beauty, but rebelled against God, who nevertheless allows him temporary power over the fallen world and a host of demons. In Judaism, Satan is typically regarded as a metaphor for the "yetzer hara", or "evil inclination", or as an agent subservient to God.

A figure known as "the satan" first appears in the Tanakh as a heavenly prosecutor, a member of the sons of God subordinate to Yahweh, who prosecutes the nation of Judah in the heavenly court and tests the loyalty of Yahweh's followers by forcing them to suffer. During the intertestamental period, possibly due to influence from the Zoroastrian figure of Angra Mainyu, the satan developed into a malevolent entity with abhorrent qualities in dualistic opposition to God. In the apocryphal Book of Jubilees, Yahweh grants the satan (referred to as Mastema) authority over a group of fallen angels to tempt humans to sin and punish them. In the Synoptic Gospels, Satan tempts Jesus in the desert and is identified as the cause of illness and temptation. In the Book of Revelation, Satan appears as a Great Red Dragon, who is defeated by Michael the Archangel and cast down from Heaven. He is later bound for one thousand years, but is briefly set free before being ultimately defeated and cast into the Lake of Fire.

In Christianity, Satan is also known as the Devil and, although the Book of Genesis does not mention him, he is often identified as the serpent in the Garden of Eden. In medieval times, Satan played a minimal role in Christian theology and was used as a comic relief figure in mystery plays. During the early modern period, Satan's significance greatly increased as beliefs such as demonic possession and witchcraft became more prevalent. During the Age of Enlightenment, belief in the existence of Satan became harshly criticized. Nonetheless, belief in Satan has persisted, particularly in the Americas. In the Quran, Shaitan, also known as Iblis, is an entity made of fire who was cast out of Heaven because he refused to bow before the newly-created Adam and incites humans and jinn to sin by infecting their minds with "waswās" ("evil suggestions"). Although Satan is generally viewed as evil, some groups have very different beliefs.

In Theistic Satanism, Satan is considered a deity who is either worshipped or revered. In LaVeyan Satanism, Satan is a symbol of virtuous characteristics and liberty. Satan's appearance is never described in the Bible, but, since the ninth century, he has often been shown in Christian art with horns, cloven hooves, unusually hairy legs, and a tail, often naked and holding a pitchfork. These are an amalgam of traits derived from various pagan deities, including Pan, Poseidon, and Bes. Satan appears frequently in Christian literature, most notably in Dante Alighieri's "Inferno", variants of the Faust legend, John Milton's "Paradise Lost" and "Paradise Regained", and the poems of William Blake. He continues to appear in film, television, and music.

The original Hebrew term "satan" is a generic noun meaning "accuser" or "adversary", which is used throughout the Hebrew Bible to refer to ordinary human adversaries, as well as a specific supernatural entity. The word is derived from a verb meaning primarily "to obstruct, oppose". When it is used without the definite article (simply "satan"), the word can refer to any accuser, but when it is used with the definite article ("ha-satan"), it usually refers specifically to the heavenly accuser: the satan.

"Ha-Satan" with the definite article occurs 13 times in the Masoretic Text, in two books of the Hebrew Bible: Job ch. 1–2 (10×) and Zechariah 3:1–2 (3×). "Satan" without the definite article is used in 10 instances, of which two are translated "diabolos" in the Septuagint and "Satan" in the King James Version (KJV):

The word "satan" does not occur in the Book of Genesis, which mentions only a talking serpent and does not identify the serpent with any supernatural entity. The first occurrence of the word "satan" in the Hebrew Bible in reference to a supernatural figure comes from , which describes the Angel of Yahweh confronting Balaam on his donkey: "Balaam's departure aroused the wrath of Elohim, and the Angel of Yahweh stood in the road as a satan against him." In , Yahweh sends the "Angel of Yahweh" to inflict a plague against Israel for three days, killing 70,000 people as punishment for David having taken a census without his approval. repeats this story, but replaces the "Angel of Yahweh" with an entity referred to as "a satan".

Some passages clearly refer to the satan, without using the word itself. describes the sons of Eli as "sons of Belial"; the later usage of this word makes it clearly a synonym for "satan". In Yahweh sends a "troubling spirit" to torment King Saul as a mechanism to ingratiate David with the king. In , the prophet Micaiah describes to King Ahab a vision of Yahweh sitting on his throne surrounded by the Host of Heaven. Yahweh asks the Host which of them will lead Ahab astray. A "spirit", whose name is not specified, but who is analogous to the satan, volunteers to be "a Lying Spirit in the mouth of all his Prophets".

The satan appears in the Book of Job, a poetic dialogue set within a prose framework, which may have been written around the time of the Babylonian captivity. In the text, Job is a righteous man favored by Yahweh. describes the "sons of God" ("bənê hāʼĕlōhîm") presenting themselves before Yahweh. Yahweh asks one of them, "the satan", where he has been, to which he replies that he has been roaming around the earth. Yahweh asks, "Have you considered My servant Job?" The satan replies by urging Yahweh to let him torture Job, promising that Job will abandon his faith at the first tribulation. Yahweh consents; the satan destroys Job's servants and flocks, yet Job refuses to condemn Yahweh. The first scene repeats itself, with the satan presenting himself to Yahweh alongside the other "sons of God". Yahweh points out Job's continued faithfulness, to which the satan insists that more testing is necessary; Yahweh once again gives him permission to test Job. In the end, Job remains faithful and righteous, and it is implied that the satan is shamed in his defeat.

 contains a description of a vision dated to the middle of February of 519 BC, in which an angel shows Zechariah a scene of Joshua the High Priest dressed in filthy rags, representing the nation of Judah and its sins, on trial with Yahweh as the judge and the satan standing as the prosecutor. Yahweh rebukes the satan and orders for Joshua to be given clean clothes, representing Yahweh's forgiveness of Judah's sins.

During the Second Temple Period, when Jews were living in the Achaemenid Empire, Judaism was heavily influenced by Zoroastrianism, the religion of the Achaemenids. Jewish conceptions of Satan were impacted by Angra Mainyu, the Zoroastrian god of evil, darkness, and ignorance. In the Septuagint, the Hebrew "ha-Satan" in Job and Zechariah is translated by the Greek word "diabolos" (slanderer), the same word in the Greek New Testament from which the English word "devil" is derived. Where "satan" is used to refer to human enemies in the Hebrew Bible, such as Hadad the Edomite and Rezon the Syrian, the word is left untranslated but transliterated in the Greek as "satan", a neologism in Greek.

The idea of Satan as an opponent of God and a purely evil figure seems to have taken root in Jewish pseudepigrapha during the Second Temple Period, particularly in the "apocalypses". The Book of Enoch, which the Dead Sea Scrolls have revealed to have been nearly as popular as the Torah, describes a group of 200 angels known as the "Watchers", who are assigned to supervise the earth, but instead abandon their duties and have sexual intercourse with human women. The leader of the Watchers is Semjâzâ and another member of the group, known as Azazel, spreads sin and corruption among humankind. The Watchers are ultimately sequestered in isolated caves across the earth and are condemned to face judgement at the end of time. The Book of Jubilees, written in around 150 BC, retells the story of the Watchers' defeat, but, in deviation from the Book of Enoch, Mastema, the "Chief of Spirits", intervenes before they are all sealed away, requesting for Yahweh to let him keep some of them to become his workers. Yahweh acquiesces this request and Mastema uses them to tempt humans into committing more sins, so that he may punish them for their wickedness. Later, Mastema induces Yahweh to test Abraham by ordering him to sacrifice Isaac.

The Second Book of Enoch, also called the Slavonic Book of Enoch, contains references to a Watcher called Satanael. It is a pseudepigraphic text of an uncertain date and unknown authorship. The text describes Satanael as being the prince of the Grigori who was cast out of heaven and an evil spirit who knew the difference between what was "righteous" and "sinful". In the Book of Wisdom, the devil is represented as the being who brought death into the world. The name Samael, which is used in reference to one of the fallen angels, later became a common name for Satan in Jewish Midrash and Kabbalah.

Most Jews do not believe in the existence of a supernatural omnimalevolent figure. Traditionalists and philosophers in medieval Judaism adhered to rational theology, rejecting any belief in rebel or fallen angels, and viewing evil as abstract. The Rabbis usually interpreted the word "satan" as it is used in the Tanakh as referring strictly to "human" adversaries and rejected all of the Enochian writings mentioning Satan as a literal, heavenly figure from the Biblical canon, making every attempt to root them out. Nonetheless, the word "satan" has occasionally been metaphorically applied to evil influences, such as the Jewish exegesis of the "yetzer hara" ("evil inclination") mentioned in . Rabbinical scholarship on the Book of Job generally follows the Talmud and Maimonides in identifying "the satan" from the prologue as a metaphor for the "yetzer hara" and not an actual entity. Satan is rarely mentioned in Tannaitic literature, but is found in Babylonian aggadah. According to a narration, the sound of the shofar, which is primarily intended to remind Jews of the importance of "teshuva", is also intended symbolically to "confuse the accuser" (Satan) and prevent him from rendering any litigation to God against the Jews. In Hasidic Judaism, the Kabbalah presents Satan as an agent of God whose function is to tempt humans into sinning so that he may accuse them in the heavenly court. The Hasidic Jews of the 18th century associated ha-Satan with "Baal Davar".

Each sect of Judaism has its own interpretation of Satan's identity. Conservative Judaism generally rejects the Talmudic interpretation of Satan as a metaphor for the "yetzer hara", and regard him as a literal agent of God. Orthodox Judaism, on the other hand, outwardly embraces Talmudic teachings on Satan, and involves Satan in religious life far more inclusively than other sects. Satan is mentioned explicitly in some daily prayers, including during Shacharit and certain post-meal benedictions, as described in Talmud and the Jewish Code of Law. In Reform Judaism, Satan is generally seen in his Talmudic role as a metaphor for the "yetzer hara" and the symbolic representation of innate human qualities such as selfishness.

The most common English synonym for "Satan" is "devil", which descends from Middle English "devel", from Old English "dēofol," that in turn represents an early Germanic borrowing of Latin "diabolus" (also the source of "diabolical"). This in turn was borrowed from Greek "diabolos" "slanderer", from "diaballein" "to slander": "dia-" "across, through" + "ballein" "to hurl". In the New Testament, the words "Satan" and "diabolos" are used interchangeably as synonyms. Beelzebub, meaning "Lord of Flies", is the contemptuous name given in the Hebrew Bible and New Testament to a Philistine god whose original name has been reconstructed as most probably "Ba'al Zabul", meaning "Baal the Prince". The Synoptic Gospels identify Satan and Beelzebub as the same. The name Abaddon (meaning "place of destruction") is used six times in the Old Testament, mainly as a name for one the regions of Sheol. describes Abaddon, whose name is translated into Greek as "Apollyon", meaning "the destroyer", as an angel who rules the Abyss. In modern usage, Abaddon is sometimes equated with Satan.

The three Synoptic Gospels all describe the temptation of Christ by Satan in the desert (, , and ). Satan first shows Jesus a stone and tells him to turn it into bread. He also takes him to the pinnacle of the Temple in Jerusalem and commands Jesus to throw himself down so that the angels will catch him. Satan takes Jesus to the top of a tall mountain as well; there, he shows him the kingdoms of the earth and promises to give them all to him if he will bow down and worship him. Each time Jesus rebukes Satan and, after the third temptation, he is administered by the angels. Satan's promise in and to give Jesus all the kingdoms of the earth implies that all those kingdoms belong to him. The fact that Jesus does not dispute Satan's promise indicates that the authors of those gospels believed this to be true.

Satan plays a role in some of the parables of Jesus, namely the Parable of the Sower, the Parable of the Weeds, Parable of the Sheep and the Goats, and the Parable of the Strong Man. According to the Parable of the Sower, Satan "profoundly influences" those who fail to understand the gospel. The latter two parables say that Satan's followers will be punished on Judgement Day, with the Parable of the Sheep and the Goats stating that the Devil, his angels, and the people who follow him will be consigned to "eternal fire". When the Pharisees accused Jesus of exorcising demons through the power of Beelzebub, Jesus responds by telling the Parable of the Strongman, saying: "how can someone enter a strong man's house and plunder his goods, unless he first binds the strong man? Then indeed he may plunder his house" (). The strong man in this parable represents Satan.

The Synoptic Gospels identify Satan and his demons as the causes of illness, including fever (), leprosy (), and arthritis (), while the Epistle to the Hebrews describes the Devil as "him who holds the power of death" (). The author of Luke-Acts attributes more power to Satan than both Matthew and Mark. In , Jesus grants Satan the authority to test Peter and the other apostles. states that Judas Iscariot betrayed Jesus because "Satan entered" him and, in , Peter describes Satan as "filling" Ananias's heart and causing him to sin. The Gospel of John only uses the name "Satan" three times. In , Jesus says that the Jews are the children of the Devil rather than the children of Abraham; the same verse describes the Devil as "a man-killer from the beginning" and "a liar and the father of lying." describes the Devil as inspiring Judas to betray Jesus and identifies Satan as "the Archon of this Cosmos", who is destined to be overthrown through Jesus's death and resurrection. promises that the Holy Spirit will "accuse the World concerning sin, justice, and judgement", a role resembling that of the satan in the Old Testament.

The Book of Revelation represents Satan as the supernatural ruler of the Roman Empire and the ultimate cause of all evil in the world. In , as part of the letter to the church at Smyrna, John of Patmos refers to the Jews of Smyrna as "a synagogue of Satan" and warns that "the Devil is about to cast some of you into prison as a test ["peirasmos"], and for ten days you will have affliction." In , in the letter to the church of Pergamum, John warns that Satan lives among the members of the congregation and declares that "Satan's throne" is in their midst. Pergamum was the capital of the Roman Province of Asia and "Satan's throne" may be referring to the monumental Pergamon Altar in the city, which was dedicated to the Greek god Zeus, or to a temple dedicated to the Roman emperor Augustus.

In , Satan is bound with a chain and hurled into the Abyss, where he is imprisoned for one thousand years. In , he is set free and gathers his armies along with Gog and Magog to wage war against the righteous, but is defeated with fire from Heaven, and cast into the lake of fire. Some Christians associate Satan with the number 666, which describes as the Number of the Beast. However, the beast mentioned in Revelation 13 is not Satan, and the use of 666 in the Book of Revelation has been interpreted as a reference to the Roman Emperor Nero, as 666 is the numeric value of his name in Hebrew.

Despite the fact that the Book of Genesis never mentions Satan, Christians have traditionally interpreted the serpent in the Garden of Eden as Satan due to , which calls Satan "that ancient serpent". This verse, however, is probably intended to identify Satan with the Leviathan, a monstrous sea-serpent whose destruction by Yahweh is prophesied in . The first recorded individual to identify Satan with the serpent from the Garden of Eden was the second-century AD Christian apologist Justin Martyr, in chapters 45 and 79 of his "Dialogue with Trypho". Other early church fathers to mention this identification include Theophilus and Tertullian. The early Christian Church, however, encountered opposition from pagans such as Celsus, who claimed in his treatise "The True Word" that "it is blasphemy... to say that the greatest God... has an adversary who constrains his capacity to do good" and said that Christians "impiously divide the kingdom of God, creating a rebellion in it, as if there were opposing factions within the divine, including one that is hostile to God".

The name "Heylel", meaning "morning star" (or, in Latin, "Lucifer"), was a name for Attar, the god of the planet Venus in Canaanite mythology, who attempted to scale the walls of the heavenly city, but was vanquished by the god of the sun. The name is used in in metaphorical reference to the king of Babylon. uses a description of a cherub in Eden as a polemic against Ithobaal II, the king of Tyre. The Church Father Origen of Alexandria ( 184 – 253), who was only aware of the actual text of these passages and not the original myths to which they refer, concluded in his treatise "On the First Principles", which is preserved in a Latin translation by Tyrannius Rufinus, that neither of these verses could literally refer to a human being and must therefore be alluding to "a certain Angel who had received the office of governing the nation of the Tyrians," but was hurled down to Earth after he was found to be corrupt.

In his apologetic treatise "Contra Celsum", however, Origen changed his interpretations of Isaiah 14:12 and Ezekiel 28:12-15, now interpreting both of them as referring to Satan. According to Henry Ansgar Kelly, Origen seems to have adopted this new interpretation to refute unnamed persons who, perhaps under the influence of Zoroastrian radical dualism, believed "that Satan's original nature was Darkness." The later Church Father Jerome ( 347 – 420), translator of the Latin Vulgate, accepted Origen's theory of Satan as a fallen angel and wrote about it in his commentary on the Book of Isaiah. In Christian tradition ever since, both Isaiah 14:12 and Ezekiel 28:12-15 have been understood as allegorically referring to Satan. For most Christians, Satan has been regarded as an angel who rebelled against God.

According to the ransom theory of atonement, which was popular among early Christian theologians, Satan gained power over humanity through Adam and Eve's sin and Christ's death on the cross was a ransom to Satan in exchange for humanity's liberation. This theory holds that Satan was tricked by God because Christ was not only free of sin, but also the incarnate Deity, whom Satan lacked the ability to enslave. Irenaeus of Lyons described a prototypical form of the ransom theory, but Origen was the first to propose it in its fully developed form. The theory was later expanded by theologians such as Gregory of Nyssa and Rufinus of Aquileia. In the eleventh century, Anselm of Canterbury criticized the ransom theory, along with the associated Christus Victor theory, resulting in the theory's decline in western Europe. The theory has nonetheless retained some of its popularity in the Eastern Orthodox Church.

Most early Christians firmly believed that Satan and his demons had the power to possess humans and exorcisms were widely practiced by Jews, Christians, and pagans alike. Belief in demonic possession continued through the Middle Ages into the early modern period. Exorcisms were seen as a display of God's power over Satan. The vast majority of people who thought they were possessed by the Devil did not suffer from hallucinations or other "spectacular symptoms", but "complained of anxiety, religious fears, and evil thoughts."

Satan had minimal role in medieval Christian theology, but he frequently appeared as a recurring comedic stock character in late medieval mystery plays, in which he was portrayed as a comic relief figure who "frolicked, fell, and farted in the background". Jeffrey Burton Russell describes the medieval conception of Satan as "more pathetic and repulsive than terrifying" and he was seen as little more than a nuisance to God's overarching plan. The "Golden Legend", a collection of saints' lives compiled in around 1260 by the Dominican Friar Jacobus da Varagine, contains numerous stories about encounters between saints and Satan, in which Satan is constantly duped by the saints' cleverness and by the power of God. Henry Ansgar Kelly remarks that Satan "comes across as the opposite of fearsome." The "Golden Legend" was the most popular book during the High and Late Middle Ages and more manuscripts of it have survived from the period than for any other book, including even the Bible itself.

The "Canon Episcopi", written in the eleventh century AD, condemns belief in witchcraft as heretical, but also documents that many people at the time apparently believed in it. Witches were believed to fly through the air on broomsticks, consort with demons, perform in "lurid sexual rituals" in the forests, murder human infants and eat them as part of Satanic rites, and engage in conjugal relations with demons. In 1326, Pope John XXII issued the papal bull "Super illius Specula", which condemned folk divination practices as consultation with Satan. By the 1430s, the Catholic Church began to regard witchcraft as part of a vast conspiracy led by Satan himself.

During the Early Modern Period, Christians gradually began to regard Satan as increasingly powerful and the fear of Satan's power became a dominant aspect of the worldview of Christians across Europe. During the Protestant Reformation, Martin Luther taught that, rather than trying to argue with Satan, Christians should avoid temptation altogether by seeking out pleasant company; Luther especially recommended music as a safeguard against temptation, since the Devil "cannot endure gaiety." John Calvin repeated a maxim from Saint Augustine that "Man is like a horse, with either God or the devil as rider."

In the late fifteenth century, a series of witchcraft panics erupted in France and Germany. The German Inquisitors Heinrich Kramer and Jacob Sprenger argued in their book "Malleus Maleficarum", published in 1487, that all "maleficia" ("sorcery") was rooted in the work of Satan. In the mid-sixteenth century, the panic spread to England and Switzerland. Both Protestants and Catholics alike firmly believed in witchcraft as a real phenomenon and supported its prosecution. In the late 1500s, the Dutch demonologist Johann Weyer argued in his treatise "De praestigiis daemonum" that witchcraft did not exist, but that Satan promoted belief in it to lead Christians astray. The panic over witchcraft intensified in the 1620s and continued until the end of the 1600s. Brian Levack estimates that around 60,000 people were executed for witchcraft during the entire span of the witchcraft hysteria.

The early English settlers of North America, especially the Puritans of New England, believed that Satan "visibly and palpably" reigned in the New World. John Winthrop claimed that the Devil made rebellious Puritan women give birth to stillborn monsters with claws, sharp horns, and "on each foot three claws, like a young fowl." Cotton Mather wrote that devils swarmed around Puritan settlements "like the frogs of Egypt". The Puritans believed that the Native Americans were worshippers of Satan and described them as "children of the Devil". Some settlers claimed to have seen Satan himself appear in the flesh at native ceremonies. During the First Great Awakening, the "new light" preachers portrayed their "old light" critics as ministers of Satan. By the time of the Second Great Awakening, Satan's primary role in American evangelicalism was as the opponent of the evangelical movement itself, who spent most of his time trying to hinder the ministries of evangelical preachers, a role he has largely retained among present-day American fundamentalists.

By the early 1600s, skeptics in Europe, including the English author Reginald Scot and the Anglican bishop John Bancroft, had begun to criticize the belief that demons still had the power to possess people. This skepticism was bolstered by the belief that miracles only occurred during the Apostolic Age, which had long since ended. Later, Enlightenment thinkers, such as David Hume, Denis Diderot, and Voltaire, attacked the notion of Satan's existence altogether. Voltaire labelled John Milton's "Paradise Lost" a "disgusting fantasy" and declared that belief in Hell and Satan were among the many lies propagated by the Catholic Church to keep humanity enslaved. By the eighteenth century, trials for witchcraft had ceased in most western countries, with the notable exceptions of Poland and Hungary, where they continued. Belief in the power of Satan, however, remained strong among traditional Christians.

Mormonism developed its own views on Satan. According to the Book of Moses, the Devil offered to be the redeemer of mankind for the sake of his own glory. Conversely, Jesus offered to be the redeemer of mankind so that his father's will would be done. After his offer was rejected, Satan became rebellious and was subsequently cast out of heaven. In the Book of Moses, Cain is said to have "loved Satan more than God" and conspired with Satan to kill Abel. It was through this pact that Cain became a Master Mahan. The Book of Moses also says that Moses was tempted by Satan before calling upon the name of the "Only Begotten", which caused Satan to depart. Douglas Davies asserts that this text "reflects" the temptation of Jesus in the Bible.

Belief in Satan and demonic possession remains strong among Christians in the United States and Latin America. According to a 2013 poll conducted by YouGov, fifty-seven percent of people in the United States believe in a literal Devil, compared to eighteen percent of people in Britain. Fifty-one percent of Americans believe that Satan has the power to possess people. W. Scott Poole, author of "Satan in America: The Devil We Know", has opined that "In the United States over the last forty to fifty years, a composite image of Satan has emerged that borrows from both popular culture and theological sources" and that most American Christians do not "separate what they know [about Satan] from the movies from what they know from various ecclesiastical and theological traditions." The Catholic Church generally played down Satan and exorcism during late twentieth and early twenty-first centuries, but Pope Francis brought renewed focus on the Devil in the early 2010s, stating, among many other pronouncements, that "The devil is intelligent, he knows more theology than all the theologians together." According to the "Encyclopædia Britannica", liberal Christianity tends to view Satan "as a [figurative] mythological attempt to express the reality and extent of evil in the universe, existing outside and apart from humanity but profoundly influencing the human sphere."

Bernard McGinn describes multiple traditions detailing the relationship between the Antichrist and Satan. In the dualist approach, Satan will become incarnate in the Antichrist, just as God became incarnate in Jesus. However, in Orthodox Christian thought, this view is problematic because it is too similar to Christ's incarnation. Instead, the "indwelling" view has become more accepted, which stipulates that the Antichrist is a human figure inhabited by Satan, since the latter's power is not to be seen as equivalent to God's.

The Arabic equivalent of the word "Satan" is "Shaitan" (شيطان, from the root šṭn شط⁬ن). The word itself is an adjective (meaning "astray" or "distant", sometimes translated as "devil") that can be applied to both man ("al-ins", الإنس) and "al-jinn" (الجن), but it is also used in reference to Satan in particular. In the Quran, Satan's name is Iblis (), probably a derivative of the Greek word "diabolos". Muslims do not regard Satan as the cause of evil, but as a tempter, who takes advantage of humans' inclinations toward self-centeredness.

Seven suras in the Quran describe how God ordered all the angels and Iblis to bow before the newly-created Adam. All the angels bowed, but Iblis refused, claiming to be superior to Adam because he was made from fire; whereas Adam was made from clay (). Consequently, God expelled him from Paradise and condemned him to Jahannam. Iblis thereafter became a "kafir", "an ungrateful disbeliever", whose sole mission is to lead humanity astray. God allows Iblis to do this, because He knows that the righteous will be able to resist Iblis's attempts to misguide them. On Judgement Day, while the lot of Satan remains in question, those who followed him will be thrown into the fires of Jahannam. After his banishment from Paradise, Iblis, who thereafter became known as "Al-Shaitan" ("the Demon"), lured Adam and Eve into eating the fruit from the forbidden tree.

The primary characteristic of Satan, aside from his hubris and despair, is his ability to cast evil suggestions ("waswās") into men and women. states that Satan has no influence over the righteous, but that those who fall in error are under his power. implies that those who obey God's laws are immune to the temptations of Satan. warns that Satan tries to keep Muslims from reading the Quran and recommends reciting the Quran as an antidote against Satan. refers to Satan as the enemy of humanity and forbids humans from worshipping him. In the Quranic retelling of the story of Job, Job knows that Satan is the one tormenting him.

In the Quran, Satan is apparently an angel, but, in , he is described as "from the jinns". This, combined with the fact that he describes himself as having been made from fire, posed a major problem for Muslims exegetes of the Quran, who disagree on whether Satan is a fallen angel or the leader of a group of evil jinn. According to a hadith from Ibn Abbas, Iblis was actually an angel whom God created out of fire. Ibn Abbas asserts that the word "jinn" could be applied to earthly jinn, but also to "fiery angels" like Satan.

Hasan of Basra, an eminent Muslim theologian who lived in the seventh century AD, was quoted as saying: "Iblis was not an angel even for the time of an eye wink. He is the origin of Jinn as Adam is of Mankind." The medieval Persian scholar Abu Al-Zamakhshari states that the words "angels" and "jinn" are synonyms. Another Persian scholar, Al-Baydawi, instead argues that Satan "hoped" to be an angel, but that his actions made him a jinn. Other Islamic scholars argue that Satan was a jinn who was admitted into Paradise as a reward for his righteousness and, unlike the angels, was given the choice to obey or disobey God. When he was expelled from Paradise, Satan blamed humanity for his punishment. Concering the fiery origin of Iblis, Zakariya al-Qazwini and Muḥammad ibn Aḥmad Ibshīhī state that all supernatural creatures originated from fire but the angels from its light and the jinn from its blaze, thus fire denotes a disembodiment origin of all spiritual entities.

The Muslim historian Al-Tabari, who died in around 923 AD, writes that, before Adam was created, earthly jinn made of smokeless fire roamed the earth and spread corruption. He further relates that Iblis was originally an angel named "Azazil" or "Al-Harith", from a group of angels, in contrast to the jinn, created from the "fires of simoom", who was sent by God to confront the earthly jinn. Azazil defeated the jinn in battle and drove them into the mountains, but he became convinced that he was superior to humans and all the other angels, leading to his downfall. In this account, Azazil's group of angels were called "jinn" because they guarded Jannah (Paradise). In another tradition recorded by Al-Tabari, Satan was one of the earthly jinn, who was taken captive by the angels and brought to Heaven as a prisoner. God appointed him as judge over the other jinn and he became known as "Al-Hakam". He fulfilled his duty for a thousand years before growing negligent, but was rehabilitated again and resumed his position until his refusal to bow before Adam.

During the first two centuries of Islam, Muslims almost unanimously accepted the historicity of a tradition known as the Satanic Verses. According to this narrative, Muhammad was told by Satan to add words to the Quran which would allow Muslims to pray for the intercession of pagan goddesses. He mistook the words of Satan for divine inspiration. Modern Muslims almost universally reject this story as heretical, as it calls the integrity of the Quran into question.

On the third day of the Hajj, Muslim pilgrims to Mecca throw seven stones at a pillar known as the "Jamrah al-’Aqabah", symbolizing the stoning of the Devil. This ritual is based on the Islamic tradition that, when God ordered Abraham to sacrifice his son Ishmael, Satan tempted him three times not to do it, and, each time, Abraham responded by throwing seven stones at him.

The hadith teach that newborn babies cry because Satan touches them while they are being born, and that this touch causes people to have an aptitude for sin. This doctrine bears some similarities to the doctrine of original sin. Muslim tradition holds that only Jesus and Mary were not touched by Satan at birth. However, when he was a boy, Muhammad's heart was literally opened by an angel, who removed a black clot that symbolized sin.

Muslim tradition preserves a number of stories involving dialogues between Jesus and Iblis, all of which are intended to demonstrate Jesus's virtue and Satan's depravity. Ahmad ibn Hanbal records an Islamic retelling of Jesus's temptation by Satan in the desert from the Synoptic Gospels. Ahmad quotes Jesus as saying, "The greatest sin is love of the world. Women are the ropes of Satan. Wine is the key to every evil." Abu Uthman al-Jahiz credits Jesus with saying, "The world is Satan's farm, and its people are his plowmen." Al-Ghazali tells an anecdote about how Jesus went out one day and saw Satan carrying ashes and honey; when he asked what they were for, Satan replied, "The honey I put on the lips of backbiters so that they achieve their aim. The ashes I put on the faces of orphans, so that people come to dislike them." The thirteenth-century scholar Sibt ibn al-Jawzi states that, when Jesus asked him what truly broke his back, Satan replied, "The neighing of horses in the cause of Allah."

According to Sufi mysticism, Iblis refused to bow to Adam because he was fully devoted to God alone and refused to bow to anyone else. For this reason, Sufi masters regard Satan and Muhammad as the two most perfect monotheists. Sufis reject the concept of dualism and instead believe in the unity of existence. In the same way that Muhammad was the instrument of God's mercy, Sufis regard Satan as the instrument of God's wrath.

Muslims believe that Satan is also the cause of deceptions originating from the mind and desires for evil. He is regarded as a cosmic force for separation, despair and spiritual envelopment. Muslims do distinguish between the satanic temptations and the murmurings of the bodily lower self (Nafs). The lower self commands the person to do a specific task or to fulfill a specific desire; whereas the inspirations of Satan tempt the person to do evil in general and, after a person successfully resists his first suggestion, Satan returns with new ones. If a Muslim feels that Satan is inciting him to sin, he is advised to seek refuge with God by reciting: "In the name of Allah, I seek refuge in you, from Satan the outcast." Muslims are also obliged to "seek refuge" before reciting the Quran.

In the Bahá'í Faith, Satan is not regarded as an independent evil power as he is in some faiths, but signifies the "lower nature" of humans. `Abdu'l-Bahá explains: "This lower nature in man is symbolized as Satan — the evil ego within us, not an evil personality outside." All other evil spirits described in various faith traditions—such as fallen angels, demons, and jinns—are also metaphors for the base character traits a human being may acquire and manifest when he turns away from God. Actions, that are described as "satanic" in some Bahá'í writings, denote humans deeds caused by selfish desires.

Theistic Satanism, commonly referred to as "devil worship", views Satan as a deity, whom individuals may supplicate to. It consists of loosely affiliated or independent groups and cabals, which all agree that Satan is a real entity.

Atheistic Satanism, as practiced by the Satanic Temple and by followers of LaVeyan Satanism, holds that Satan does not exist as a literal anthropomorphic entity, but rather as a symbol of a cosmos which Satanists perceive to be permeated and motivated by a force that has been given many names by humans over the course of time. In this religion, "Satan" is not viewed or depicted as a hubristic, irrational, and fraudulent creature, but rather is revered with Prometheus-like attributes, symbolizing liberty and individual empowerment. To adherents, he also serves as a conceptual framework and an external metaphorical projection of the Satanist's highest personal potential. In his essay "Satanism: The Feared Religion", the current High Priest of the Church of Satan, Peter H. Gilmore, further expounds that "...Satan is a symbol of Man living as his prideful, carnal nature dictates. The reality behind Satan is simply the dark evolutionary force of entropy that permeates all of nature and provides the drive for survival and propagation inherent in all living things. Satan is not a conscious entity to be worshiped, rather a reservoir of power inside each human to be tapped at will".

LaVeyan Satanists embrace the original etymological meaning of the word "Satan" (Hebrew: שָּׂטָן "satan", meaning "adversary"). According to Peter H. Gilmore, "The Church of Satan has chosen Satan as its primary symbol because in Hebrew it means adversary, opposer, one to accuse or question. We see ourselves as being these Satans; the adversaries, opposers and accusers of all spiritual belief systems that would try to hamper enjoyment of our life as a human being."

The main deity in the tentatively Indo-European pantheon of the Yazidis, Melek Taus, is similar to the devil in Christian and Islamic traditions, as he refused to bow down before humanity. Therefore Christians and Muslims often consider Melek Taus to be Satan. However, rather than being Satanic, Yazidism can be understood as a remnant of a pre-Islamic Middle Eastern Indo-European religion, and/or a ghulat Sufi movement founded by Shaykh Adi. In fact, there is no entity in Yazidism which represents evil in opposition to God; such dualism is rejected by Yazidis.

In the Middle Ages, the Cathars, practitioners of a dualistic religion, were accused of worshipping Satan by the Catholic Church. Pope Gregory IX stated in his work "Vox in Rama" that the Cathars believed that God had erred in casting Lucifer out of heaven and that Lucifer would return to reward his faithful. On the other hand, according to Catharism, the creator-god of the material world worshipped by the Catholic Church is actually Satan.

Wicca is a modern, syncretic Neopagan religion, whose practitioners many Christians have incorrectly assumed to worship Satan. In actuality, Wiccans do not believe in the existence of Satan or any analogous figure and have repeatedly and emphatically rejected the notion that they venerate such an entity. The cult of the skeletal figure of Santa Muerte, which has grown exponentially in Mexico, has been denounced by the Catholic Church as Devil-worship. However, devotees of Santa Muerte view her as an angel of death created by God, and many of them identify as Catholic.

Much modern folklore about Satanism does not originate from the actual beliefs or practices of theistic or atheistic Satanists, but rather from a mixture of medieval Christian folk beliefs, political or sociological conspiracy theories, and contemporary urban legends. An example is the Satanic ritual abuse scare of the 1980s — beginning with the memoir "Michelle Remembers" — which depicted Satanism as a vast conspiracy of elites with a predilection for child abuse and human sacrifice. This genre frequently describes Satan as physically incarnating in order to receive worship.

In Dante Alighieri's "Inferno", Satan appears as a giant demon, frozen mid-breast in ice at the center of the Ninth Circle of Hell. Satan has three faces and a pair of bat-like wings affixed under each chin. In his three mouths, Satan gnaws on Brutus, Judas Iscariot, and Cassius, whom Dante regarded as having betrayed the "two greatest heroes of the human race": Julius Caesar, the founder of the new order of government, and Jesus, the founder of the new order of religion. As Satan beats his wings, he creates a cold wind that continues to freeze the ice surrounding him and the other sinners in the Ninth Circle. Dante and Virgil climb up Satan's shaggy legs until gravity is reversed and they fall through the earth into the southern hemisphere.

Satan appears in several stories from "The Canterbury Tales" by Geoffrey Chaucer, including "The Summoner's Prologue", in which a friar arrives in Hell and sees no other friars, but is told there are millions. Then Satan lifts his tail to reveal that all of the friars live inside his anus. Chaucer's description of Satan's appearance is clearly based on Dante's. The legend of Faust, recorded in the 1589 chapbook "The History of the Damnable Life and the Deserved Death of Doctor John Faustus", concerns a pact allegedly made by the German scholar Johann Georg Faust with a demon named Mephistopheles agreeing to sell his soul to Satan in exchange for twenty-four years of earthly pleasure. This chapbook became the source for Christopher Marlowe's "The Tragical History of the Life and Death of Doctor Faustus".

John Milton's epic poem "Paradise Lost" features Satan as its main protagonist. Milton portrays Satan as a tragic antihero destroyed by his own hubris. The poem, which draws extensive inspiration from Greek tragedy, recreates Satan as a complex literary character, who dares to rebel against the "tyranny" of God, in spite of God's own omnipotence. The English poet and painter William Blake famously quipped that "The reason Milton wrote in fetters when he wrote of Angels & God, and at liberty when of Devils & Hell, is because he was a true poet and of the Devils party without knowing it." "Paradise Regained", the sequel to "Paradise Lost", is a retelling of Satan's temptation of Jesus in the desert.

William Blake regarded Satan as a model of rebellion against unjust authority and features him in many of his poems and illustrations, including his 1780 book "The Marriage of Heaven and Hell", in which Satan is celebrated as the ultimate rebel, the incarnation of human emotion and the epitome of freedom from all forms of reason and orthodoxy. Based on the Biblical passages portraying Satan as the accuser of sin, Blake interpreted Satan as "a promulgator of moral laws."

Satan's appearance is never described in the Bible or any early Christian writings, though Paul the Apostle does write that "Satan disguises himself as an angel of light" (). The Devil was never shown in early Christian artwork and first appears in medieval art of the ninth century, where he is shown with cloven hooves, hairy legs, the tail of a goat, pointed ears, a beard, a flat nose, and a set of horns. Some art historians claim that Satan was depicted earlier in the sixth century in one of the mosaics of the Basilica of Sant'Apollinare Nuovo. The mosaic "Christ the Good Sheppard" features a blue angel which appears to the left hand side of Jesus behind three goats. Satan may have first become associated with goats through the Parable of the Sheep and the Goats, recorded in , in which Jesus separates sheep (representing the saved) from goats (representing the damned).

Medieval Christians were known to adapt previously existing pagan iconography to suit depictions of Christian figures. Much of Satan's traditional iconography in Christianity appears to be derived from Pan, a rustic, goat-legged fertility god in ancient Greek religion. Early Christian writers such as Saint Jerome equated the Greek satyrs and the Roman fauns, whom Pan resembled, with demons. The Devil's pitchfork appears to have been adapted from the trident wielded by the Greek god Poseidon and Satan's flame-like hair seems to have originated from the Egyptian god Bes. By the High Middle Ages, Satan and devils appear in all works of Christian art: in paintings, sculptures, and on cathedrals. Satan is usually depicted naked, but his genitals are rarely shown and are often covered by animal furs. The goat-like portrayal of Satan was especially closely associated with him in his role as the object of worship by sorcerers and as the incubus, a demon believed to rape human women in their sleep.

Italian frescoes from the late Middle Ages onward frequently show Satan chained in Hell, feeding on the bodies of the perpetually damned. These frescoes are early enough to have inspired Dante's portrayal in his "Inferno". As the serpent in the Garden of Eden, Satan is often shown as a snake with arms and legs as well the head and full-breasted upper torso of a woman. Satan and his demons could take any form in medieval art, but, when appearing in their true form, they were often shown as short, hairy, black-skinned humanoids with clawed and bird feet and extra faces on their chests, bellies, genitals, buttocks, and tails. The modern popular culture image of Satan as a well-dressed gentleman with small horns and a tail originates from portrayals of Mephistopheles in the operas "La damnation de Faust" (1846) by Hector Berlioz, "Mefistofele" (1868) by Arrigo Boito, and "Faust" by Charles Gounod.

The Devil is depicted as a vampire bat in Georges Méliès' "The Haunted Castle" (1896), which is often considered the first horror film. So-called "Black Masses" have been portrayed in sensationalist B-movies since the 1960s. One of the first films to portray such a ritual was the 1965 film "Eye of the Devil", also known as "13". Alex Sanders, a former black magician, served as a consultant on the film to ensure that the rituals portrayed in it were depicted accurately. Over the next thirty years, the novels of Dennis Wheatley and the films of Hammer Film Productions both played a major role in shaping the popular image of Satanism.

The film version of Ira Levin's "Rosemary's Baby" established made Satanic themes a staple of mainstream horror fiction. Later films such as "The Exorcist" (1973), "The Omen" (1976) and "Angel Heart" (1987) feature Satan as an antagonist.

References to Satan in music can be dated back to the Middle Ages. During the fifth century, a musical interval called the tritone became known as "the devil in Music" and was banned by the Catholic Church. Giuseppe Tartini was inspired to write his most famous work, the Violin Sonata in G minor, also known as "The Devil's Trill", after dreaming of the Devil playing the violin. Tartini claimed that the sonata was a lesser imitation of what the Devil had played in his dream. Niccolò Paganini was believed to have derived his musical talent from a deal with the Devil. Charles Gounod's "Faust" features a narrative that involves Satan.

In the early 1900s, jazz and blues became known as the "Devil's Music" as they were considered "dangerous and unholy". According to legend, blues musician Tommy Johnson was a terrible guitarist before exchanging his soul to the Devil for a guitar. Later, Robert Johnson claimed that he had sold his soul in return for becoming a great blues guitarist. Satanic symbolism appears in rock music from the 1960s. Mick Jagger assumes the role of Lucifer in the Rolling Stones' "Sympathy for the Devil" (1968), while Black Sabbath portrayed the Devil in numerous songs, including "War Pigs" (1970) and "N.I.B." (1970).





</doc>
<doc id="27695" url="https://en.wikipedia.org/wiki?curid=27695" title="Structured programming">
Structured programming

Structured programming is a programming paradigm aimed at improving the clarity, quality, and development time of a computer program by making extensive use of the structured control flow constructs of selection (if/then/else) and repetition (while and for), block structures, and subroutines in contrast to using simple tests and jumps such as the "go to" statement, which can lead to "spaghetti code" that is potentially difficult to follow and maintain.

It emerged in the late 1950s with the appearance of the ALGOL 58 and ALGOL 60 programming languages, with the latter including support for block structures. Contributing factors to its popularity and widespread acceptance, at first in academia and later among practitioners, include the discovery of what is now known as the structured program theorem in 1966, and the publication of the influential "Go To Statement Considered Harmful" open letter in 1968 by Dutch computer scientist Edsger W. Dijkstra, who coined the term "structured programming".

Structured programming is most frequently used with deviations that allow for clearer programs in some particular cases, such as when exception handling has to be performed.

Following the structured program theorem, all programs are seen as composed of control structures:

Subroutines; callable units such as procedures, functions, methods, or subprograms are used to allow a sequence to be referred to by a single statement.

Blocks are used to enable groups of statements to be treated as if they were one statement. "Block-structured" languages have a syntax for enclosing structures in some formal way, such as an if-statement bracketed by codice_6 as in ALGOL 68, or a code section bracketed by codice_7, as in PL/I and Pascal, whitespace indentation as in Python - or the curly braces codice_8 of C and many later languages.

It is possible to do structured programming in any programming language, though it is preferable to use something like a procedural programming language. Some of the languages initially used for structured programming include: ALGOL, Pascal, PL/I and Ada, but most new procedural programming languages since that time have included features to encourage structured programming, and sometimes deliberately left out features – notably GOTO – in an effort to make unstructured programming more difficult.
"Structured programming" (sometimes known as modular programming) enforces a logical structure on the program being written to make it more efficient and easier to understand and modify.

The structured program theorem provides the theoretical basis of structured programming. It states that three ways of combining programs—sequencing, selection, and iteration—are sufficient to express any computable function. This observation did not originate with the structured programming movement; these structures are sufficient to describe the instruction cycle of a central processing unit, as well as the operation of a Turing machine. Therefore, a processor is always executing a "structured program" in this sense, even if the instructions it reads from memory are not part of a structured program. However, authors usually credit the result to a 1966 paper by Böhm and Jacopini, possibly because Dijkstra cited this paper himself. The structured program theorem does not address how to write and analyze a usefully structured program. These issues were addressed during the late 1960s and early 1970s, with major contributions by Dijkstra, Robert W. Floyd, Tony Hoare, Ole-Johan Dahl, and David Gries.

P. J. Plauger, an early adopter of structured programming, described his reaction to the structured program theorem:

Donald Knuth accepted the principle that programs must be written with provability in mind, but he disagreed (and still disagrees) with abolishing the GOTO statement. In his 1974 paper, "Structured Programming with Goto Statements", he gave examples where he believed that a direct jump leads to clearer and more efficient code without sacrificing provability. Knuth proposed a looser structural constraint: It should be possible to draw a program's flow chart with all forward branches on the left, all backward branches on the right, and no branches crossing each other. Many of those knowledgeable in compilers and graph theory have advocated allowing only reducible flow graphs.

Structured programming theorists gained a major ally in the 1970s after IBM researcher Harlan Mills applied his interpretation of structured programming theory to the development of an indexing system for "The New York Times" research file. The project was a great engineering success, and managers at other companies cited it in support of adopting structured programming, although Dijkstra criticized the ways that Mills's interpretation differed from the published work.

As late as 1987 it was still possible to raise the question of structured programming in a computer science journal. Frank Rubin did so in that year with an open letter titled ""GOTO considered harmful" considered harmful". Numerous objections followed, including a response from Dijkstra that sharply criticized both Rubin and the concessions other writers made when responding to him.

By the end of the 20th century nearly all computer scientists were convinced that it is useful to learn and apply the concepts of structured programming. High-level programming languages that originally lacked programming structures, such as FORTRAN, COBOL, and BASIC, now have them.

While goto has now largely been replaced by the structured constructs of selection (if/then/else) and repetition (while and for), few languages are purely structured. The most common deviation, found in many languages, is the use of a return statement for early exit from a subroutine. This results in multiple exit points, instead of the single exit point required by structured programming. There are other constructions to handle cases that are awkward in purely structured programming.

The most common deviation from structured programming is early exit from a function or loop. At the level of functions, this is a codice_9 statement. At the level of loops, this is a codice_10 statement (terminate the loop) or codice_11 statement (terminate the current iteration, proceed with next iteration). In structured programming, these can be replicated by adding additional branches or tests, but for returns from nested code this can add significant complexity. C is an early and prominent example of these constructs. Some newer languages also have "labeled breaks", which allow breaking out of more than just the innermost loop. Exceptions also allow early exit, but have further consequences, and thus are treated below.

Multiple exits can arise for a variety of reasons, most often either that the subroutine has no more work to do (if returning a value, it has completed the calculation), or has encountered "exceptional" circumstances that prevent it from continuing, hence needing exception handling.

The most common problem in early exit is that cleanup or final statements are not executed – for example, allocated memory is not deallocated, or open files are not closed, causing memory leaks or resource leaks. These must be done at each return site, which is brittle and can easily result in bugs. For instance, in later development, a return statement could be overlooked by a developer, and an action which should be performed at the end of a subroutine (e.g., a trace statement) might not be performed in all cases. Languages without a return statement, such as standard Pascal, do not have this problem.

Most modern languages provide language-level support to prevent such leaks; see detailed discussion at resource management. Most commonly this is done via unwind protection, which ensures that certain code is guaranteed to be run when execution exits a block; this is a structured alternative to having a cleanup block and a codice_12. This is most often known as codice_13 and considered a part of exception handling. Various techniques exist to encapsulate resource management. An alternative approach, found primarily in C++, is Resource Acquisition Is Initialization, which uses normal stack unwinding (variable deallocation) at function exit to call destructors on local variables to deallocate resources.

Kent Beck, Martin Fowler and co-authors have argued in their refactoring books that nested conditionals may be harder to understand than a certain type of flatter structure using multiple exits predicated by guard clauses. Their 2009 book flatly states that "one exit point is really not a useful rule. Clarity is the key principle: If the method is clearer with one exit point, use one exit point; otherwise don’t". They offer a cookbook solution for transforming a function consisting only of nested conditionals into a sequence of guarded return (or throw) statements, followed by a single unguarded block, which is intended to contain the code for the common case, while the guarded statements are supposed to deal with the less common ones (or with errors). Herb Sutter and Andrei Alexandrescu also argue in their 2004 C++ tips book that the single-exit point is an obsolete requirement.

In his 2004 textbook, David Watt writes that "single-entry multi-exit control flows are often desirable". Using Tennent's framework notion of sequencer, Watt uniformly describes the control flow constructs found in contemporary programming languages and attempts to explain why certain types of sequencers are preferable to others in the context of multi-exit control flows. Watt writes that unrestricted gotos (jump sequencers) are bad because the destination of the jump is not self-explanatory to the reader of a program until the reader finds and examines the actual label or address that is the target of the jump. In contrast, Watt argues that the conceptual intent of a return sequencer is clear from its own context, without having to examine its destination. Watt writes that a class of sequencers known as "escape sequencers", defined as a "sequencer that terminates execution of a textually enclosing command or procedure", encompasses both breaks from loops (including multi-level breaks) and return statements. Watt also notes that while jump sequencers (gotos) have been somewhat restricted in languages like C, where the target must be an inside the local block or an encompassing outer block, that restriction alone is not sufficient to make the intent of gotos in C self-describing and so they can still produce "spaghetti code". Watt also examines how exception sequencers differ from escape and jump sequencers; this is explained in the next section of this article.

In contrast to the above, Bertrand Meyer wrote in his 2009 textbook that instructions like codice_10 and codice_11 "are just the old codice_12 in sheep's clothing" and strongly advised against their use.

Based on the coding error from the Ariane 501 disaster, software developer Jim Bonang argues that any exceptions thrown from a function violate the single-exit paradigm, and proposes that all inter-procedural exceptions should be forbidden. In C++ syntax, this is done by declaring all function signatures as codice_17 (since C++11) or codice_18. Bonang proposes that all single-exit conforming C++ should be written along the lines of:

Peter Ritchie also notes that, in principle, even a single codice_19 right before the codice_9 in a function constitutes a violation of the single-exit principle, but argues that Dijkstra's rules were written in a time before exception handling became a paradigm in programming languages, so he proposes to allow any number of throw points in addition to a single return point. He notes that solutions which wrap exceptions for the sake of creating a single-exit have higher nesting depth and thus are more difficult to comprehend, and even accuses those who propose to apply such solutions to programming languages which support exceptions of engaging in cargo cult thinking.

David Watt also analyzes exception handling in the framework of sequencers (introduced in this article in the previous section on early exits.) Watt notes that an abnormal situation (generally exemplified with arithmetic 
overflows or input/output failures like file not found) is a kind of error that "is detected in some low-level program unit, but [for which] a handler is more naturally located in a high-level program unit". For example, a program might contain several calls to read files, but the action to perform when a file is not found depends on the meaning (purpose) of the file in question to the program and thus a handling routine for this abnormal situation cannot be located in low-level system code. Watts further notes that introducing status flags testing in the caller, as single-exit structured programming or even (multi-exit) return sequencers would entail, results in a situation where "the application code tends to get cluttered by tests of status flags" and that "the programmer might forgetfully or lazily omit to test a status flag. In fact, abnormal situations represented by status flags are by default ignored!" He notes that in contrast to status flags testing, exceptions have the opposite default behavior, causing the program to terminate unless the programmer explicitly deals with the exception in some way, possibly by adding code to willfully ignore it. Based on these arguments, Watt concludes that jump sequencers or escape sequencers (discussed in the previous section) aren't as suitable as a dedicated exception sequencer with the semantics discussed above.

The textbook by Louden and Lambert emphasizes that exception handling differs from structured programming constructs like codice_2 loops because the transfer of control "is set up at a different point in the program than that where the actual transfer takes place. At the point where the transfer actually occurs, there may be no syntactic indication that control will in fact be transferred." Computer science professor Arvind Kumar Bansal also notes that in languages which implement exception handling, even control structures like codice_4, which have the single-exit property in absence of exceptions, no longer have it in presence of exceptions, because an exception can prematurely cause an early exit in any part of the control structure; for instance if codice_23 throws an exception in codice_24, then the usual exit point after check() is not reached. Citing multiple prior studies by others (1999-2004) and their own results, Westley Weimer and George Necula wrote that a significant problem with exceptions is that they "create hidden control-flow paths that are difficult for programmers to reason about".

The necessity to limit code to single-exit points appears in some contemporary programming environments focused on parallel computing, such as OpenMP. The various parallel constructs from OpenMP, like codice_25, do not allow early exits from inside to the outside of the parallel construct; this restriction includes all manner of exits, from codice_10 to C++ exceptions, but all of these are permitted inside the parallel construct if the jump target is also inside it.

More rarely, subprograms allow multiple "entry." This is most commonly only "re"-entry into a coroutine (or generator/semicoroutine), where a subprogram yields control (and possibly a value), but can then be resumed where it left off. There are a number of common uses of such programming, notably for streams (particularly input/output), state machines, and concurrency. From a code execution point of view, yielding from a coroutine is closer to structured programming than returning from a subroutine, as the subprogram has not actually terminated, and will continue when called again – it is not an early exit. However, coroutines mean that multiple subprograms have execution state – rather than a single call stack of subroutines – and thus introduce a different form of complexity.

It is very rare for subprograms to allow entry to an arbitrary position in the subprogram, as in this case the program state (such as variable values) is uninitialized or ambiguous, and this is very similar to a goto.

Some programs, particularly parsers and communications protocols, have a number of states that follow each other in a way that is not easily reduced to the basic structures, and some programmers implement the state-changes with a jump to the new state. This type of state-switching is often used in the Linux kernel.

However, it is possible to structure these systems by making each state-change a separate subprogram and using a variable to indicate the active state (see trampoline). Alternatively, these can be implemented via coroutines, which dispense with the trampoline.





</doc>
<doc id="27696" url="https://en.wikipedia.org/wiki?curid=27696" title="Semiconductor device fabrication">
Semiconductor device fabrication

Semiconductor device fabrication is the process used to create the integrated circuits that are present in everyday electrical and electronic devices. It is a multiple-step sequence of photolithographic and chemical processing steps during which electronic circuits are gradually created on a wafer made of pure semiconducting material. Silicon is almost always used, but various compound semiconductors are used for specialized applications. By industry standard, each generation of the semiconductor manufacturing process, also known as "technology node", "process technology" or simply "node", is designated by the process' minimum feature size (indicated in micrometers or nanometers), which was originally given by the process' gate length.

The entire manufacturing process, from start to packaged chips ready for shipment, takes six to eight weeks and is performed in highly specialized facilities referred to as fabs. In more advanced semiconductor devices, such as modern 14/10/7 nm nodes, fabrication can take up to 15 weeks with 11–13 weeks being the industry average.

When feature widths were far greater than about 10 micrometres, purity was not the issue that it is today in device manufacturing. As devices became more integrated, cleanrooms became even cleaner. Today, the fabs are pressurized with filtered air to remove even the smallest particles, which could come to rest on the wafers and contribute to defects. The workers in a semiconductor fabrication facility are required to wear cleanroom suits to protect the devices from human contamination.

Semiconductor device manufacturing has spread from Texas and California in the 1960s to the rest of the world, including Europe, the Middle East, and Asia. It is a global business today. The leading semiconductor manufacturers typically have facilities all over the world. Intel, the world's largest manufacturer, has facilities in Europe and Asia as well as the U.S. Samsung, Qualcomm, and Broadcom, among the biggest semiconductor manufacturers, also have facilities spread in different countries.

A typical wafer is made out of extremely pure silicon that is grown into mono-crystalline cylindrical ingots (boules) up to 300 mm (slightly less than 12 inches) in diameter using the Czochralski process. These ingots are then sliced into wafers about 0.75 mm thick and polished to obtain a very regular and flat surface.

In semiconductor device fabrication, the various processing steps fall into four general categories: deposition, removal, patterning, and modification of electrical properties.
Modern chips have up to eleven metal levels produced in over 300 sequenced processing steps.

FEOL processing refers to the formation of the transistors directly in the silicon. The raw wafer is engineered by the growth of an ultrapure, virtually defect-free silicon layer through epitaxy. In the most advanced logic devices, "prior" to the silicon epitaxy step, tricks are performed to improve the performance of the transistors to be built. One method involves introducing a "straining step" wherein a silicon variant such as silicon-germanium (SiGe) is deposited. Once the epitaxial silicon is deposited, the crystal lattice becomes stretched somewhat, resulting in improved electronic mobility. Another method, called "silicon on insulator" technology involves the insertion of an insulating layer between the raw silicon wafer and the thin layer of subsequent silicon epitaxy. This method results in the creation of transistors with reduced parasitic effects.

Front-end surface engineering is followed by growth of the gate dielectric (traditionally silicon dioxide), patterning of the gate, patterning of the source and drain regions, and subsequent implantation or diffusion of dopants to obtain the desired complementary electrical properties. In dynamic random-access memory (DRAM) devices, storage capacitors are also fabricated at this time, typically stacked above the access transistor (the now defunct DRAM manufacturer Qimonda implemented these capacitors with trenches etched deep into the silicon surface).

Once the various semiconductor devices have been created, they must be interconnected to form the desired electrical circuits. This occurs in a series of wafer processing steps collectively referred to as BEOL (not to be confused with "back end" of chip fabrication, which refers to the packaging and testing stages). BEOL processing involves creating metal interconnecting wires that are isolated by dielectric layers. The insulating material has traditionally been a form of SiO or a silicate glass, but recently new low dielectric constant materials are being used (such as silicon oxycarbide), typically providing dielectric constants around 2.7 (compared to 3.82 for SiO), although materials with constants as low as 2.2 are being offered to chipmakers.

Historically, the metal wires have been composed of aluminum. In this approach to wiring (often called "subtractive aluminum"), blanket films of aluminum are deposited first, patterned, and then etched, leaving isolated wires. Dielectric material is then deposited over the exposed wires. The various metal layers are interconnected by etching holes (called ""vias")" in the insulating material and then depositing tungsten in them with a CVD technique; this approach is still used in the fabrication of many memory chips such as dynamic random-access memory (DRAM), because the number of interconnect levels is small (currently no more than four).

More recently, as the number of interconnect levels for logic has substantially increased due to the large number of transistors that are now interconnected in a modern microprocessor, the timing delay in the wiring has become so significant as to prompt a change in wiring material (from aluminum to copper interconnect layer) and a change in dielectric material (from silicon dioxides to newer low-K insulators). This performance enhancement also comes at a reduced cost via damascene processing, which eliminates processing steps. As the number of interconnect levels increases, planarization of the previous layers is required to ensure a flat surface prior to subsequent lithography. Without it, the levels would become increasingly crooked, extending outside the depth of focus of available lithography, and thus interfering with the ability to pattern. CMP (chemical-mechanical planarization) is the primary processing method to achieve such planarization, although dry "etch back" is still sometimes employed when the number of interconnect levels is no more than three.

The highly serialized nature of wafer processing has increased the demand for metrology in between the various processing steps. For example, thin film metrology based on ellipsometry or reflectometry is used to tightly control the thickness of gate oxide, as well as the thickness, refractive index and extinction coefficient of photoresist and other coatings. Wafer test metrology equipment is used to verify that the wafers haven't been damaged by previous processing steps up until testing; if too many dies on one wafer have failed, the entire wafer is scrapped to avoid the costs of further processing. Virtual metrology has been used to predict wafer properties based on statistical methods without performing the physical measurement itself.

Once the front-end process has been completed, the semiconductor devices are subjected to a variety of electrical tests to determine if they function properly. The proportion of devices on the wafer found to perform properly is referred to as the yield. Manufacturers are typically secretive about their yields, but it can be as low as 30%. Process variation is one among many reasons for low yield.

The fab tests the chips on the wafer with an electronic tester that presses tiny probes against the chip. The machine marks each bad chip with a drop of dye. Currently, electronic dye marking is possible if wafer test data is logged into a central computer database and chips are "binned" (i.e. sorted into virtual bins) according to the predetermined test limits. The resulting binning data can be graphed, or logged, on a wafer map to trace manufacturing defects and mark bad chips. This map can also be used during wafer assembly and packaging.

Chips are also tested again after packaging, as the bond wires may be missing, or analog performance may be altered by the package. This is referred to as the "final test".

Usually, the fab charges for testing time, with prices in the order of cents per second. Testing times vary from a few milliseconds to a couple of seconds, and the test software is optimized for reduced testing time. Multiple chip (multi-site) testing is also possible, because many testers have the resources to perform most or all of the tests in parallel.

Chips are often designed with "testability features" such as scan chains or a "built-in self-test" to speed testing, and reduce testing costs. In certain designs that use specialized analog fab processes, wafers are also laser-trimmed during the testing, in order to achieve tightly-distributed resistance values as specified by the design.

Good designs try to test and statistically manage "corners" (extremes of silicon behavior caused by a high operating temperature combined with the extremes of fab processing steps). Most designs cope with at least 64 corners.

Once tested, a wafer is typically reduced in thickness before the wafer is scored and then broken into individual dice, a process known as wafer dicing. Only the good, unmarked chips are packaged.

Plastic or ceramic packaging involves mounting the die, connecting the die pads to the pins on the package, and sealing the die. Tiny wires are used to connect the pads to the pins. In the old days, wires were attached by hand, but now specialized machines perform the task. Traditionally, these wires have been composed of gold, leading to a lead frame (pronounced "leed frame") of solder-plated copper; lead is poisonous, so lead-free "lead frames" are now mandated by RoHS.

Chip scale package (CSP) is another packaging technology. A plastic dual in-line package, like most packages, is many times larger than the actual die hidden inside, whereas CSP chips are nearly the size of the die; a CSP can be constructed for each die "before" the wafer is diced.

The packaged chips are retested to ensure that they were not damaged during packaging and that the die-to-pin interconnect operation was performed correctly. A laser then etches the chip's name and numbers on the package.

This is a list of processing techniques that are employed numerous times throughout the construction of a modern electronic device; this list does not necessarily imply a specific order.

Many toxic materials are used in the fabrication process. These include:


It is vital that workers should not be directly exposed to these dangerous substances. The high degree of automation common in the IC fabrication industry helps to reduce the risks of exposure. Most fabrication facilities employ exhaust management systems, such as wet scrubbers, combustors, heated absorber cartridges, etc., to control the risk to workers and to the environment.





</doc>
<doc id="27698" url="https://en.wikipedia.org/wiki?curid=27698" title="Sanskrit">
Sanskrit

Sanskrit (; , Sanskrit: संस्कृतम्) is a language of ancient India with a documented history of nearly 3,500 years. It is the primary liturgical language of Hinduism; the predominant language of most works of Hindu philosophy as well as some of the principal texts of Buddhism and Jainism. Sanskrit, in its various variants and dialects, was the "lingua franca" of ancient and medieval India. In the early 1st millennium CE, along with Buddhism and Hinduism, Sanskrit migrated to Southeast Asia, parts of East Asia and Central Asia, emerging as a language of high culture and of local ruling elites in these regions.

Sanskrit is an Old Indo-Aryan language. As one of the oldest documented members of the Indo-European family of languages, Sanskrit holds a prominent position in Indo-European studies. It is related to Greek and Latin, as well as Hittite, Luwian, Old Avestan and many other extinct languages with historical significance to Europe, West Asia and Central Asia. It traces its linguistic ancestry to the Proto-Indo-Aryan language, Proto-Indo-Iranian and the Proto-Indo-European languages. Sanskrit is traceable to the 2nd millennium BCE in a form known as the Vedic Sanskrit, with the "Rigveda" as the earliest surviving text. A more refined and an exact grammatical form called the Classical Sanskrit emerged in mid-1st millennium BCE with the "Aṣṭādhyāyī" treatise of Pāṇini. Sanskrit, though not necessarily Classical Sanskrit, is the root language of many Prakrit languages. Examples include numerous modern daughter Northern Indian subcontinental languages such as Hindi, Nepali, Bengali, Punjabi and Marathi.

The body of Sanskrit literature encompasses a rich tradition of philosophical and religious texts, as well as poetry, music, drama, scientific, technical and other texts. In the ancient era, Sanskrit compositions were orally transmitted by methods of memorisation of exceptional complexity, rigour and fidelity. The earliest known inscriptions in Sanskrit are from the 1st-century BCE, such as the few discovered in Ayodhya and Ghosundi-Hathibada (Chittorgarh). Sanskrit texts dated to the 1st millennium CE were written in the Brahmi script, the Nāgarī script, the historic South Indian scripts and their derivative scripts. Sanskrit is one of the 22 languages listed in the Eighth Schedule of the Constitution of India. It continues to be widely used as a ceremonial and ritual language in Hinduism and some Buddhist practices such as hymns and chants.

The Sanskrit verbal adjective "" is a compound word consisting of "sams" (together, good, well, perfected) and "krta-" (made, formed, work). It connotes a work that has been "well prepared, pure and perfect, polished, sacred". According to Biderman, the perfection contextually being referred to in the etymological origins of the word is its tonal qualities, rather than semantic. Sound and oral transmission were highly valued quality in ancient India, and its sages refined the alphabet, the structure of words and its exacting grammar into a "collection of sounds, a kind of sublime musical mold", states Biderman, as an integral language they called Sanskrit. From late Vedic period onwards, state Annette Wilke and Oliver Moebus, resonating sound and its musical foundations attracted an "exceptionally large amount of linguistic, philosophical and religious literature" in India. The sound was visualized as "pervading all creation", another representation of the world itself, the "mysterious magnum" of the Hindu thought. The search for perfection in thought and of salvation was one of the dimensions of sacred sound, and the common thread to weave all ideas and inspirations became the quest for what the ancient Indians believed to be a perfect language, the "phonocentric episteme" of Sanskrit.

Sanskrit as a language competed with numerous less exact vernacular Indian languages called Prakritic languages (""). The term "prakrta" literally means "original, natural, normal, artless", states Franklin Southworth. The relationship between Prakrit and Sanskrit is found in the Indian texts dated to the 1st millennium CE. Patanjali acknowledged that Prakrit is the first language, one instinctively adopted by every child with all its imperfections and later leads to the problems of interpretation and misunderstanding. The purifying structure of the Sanskrit language removes these imperfections. The early Sanskrit grammarian Dandin states, for example, that much in the Prakrit languages is etymologically rooted in Sanskrit but involve "loss of sounds" and corruptions that result from a "disregard of the grammar". Dandin acknowledged that there are words and confusing structures in Prakrit that thrive independent of Sanskrit. This view is found in the writing of Bharata Muni, the author of the ancient "Natyasastra" text. The early Jain scholar Namisadhu acknowledged the difference, but disagreed that the Prakrit language was a corruption of Sanskrit. Namisadhu stated that the Prakrit language was the "purvam" (came before, origin) and they came naturally to women and children, that Sanskrit was a refinement of the Prakrit through a "purification by grammar".

Sanskrit belongs to the Indo-European family of languages. It is one of the three ancient documented languages that likely arose from a common root language now referred to as the Proto-Indo-European language:

Other Indo-European languages related to Sanskrit include archaic and classical Latin (c. 600 BCE – 100 CE, old Italian), Gothic (archaic Germanic language, c. 350 CE), Old Norse (c. 200 CE and after), Old Avestan (c. late 2nd millennium BCE) and Younger Avestan (c. 900 BCE). The closest ancient relatives of Vedic Sanskrit in the Indo-European languages are the Nuristani language found in the remote Hindu Kush region of the northeastern Afghanistan and northwestern Himalayas, as well as the extinct Avestan and Old Persian – both Iranian languages.

Colonial era scholars familiar with Latin and Greek were struck by the resemblance of the Sanskrit language, both its vocabulary and grammar, to the classical languages of Europe. It suggested a common root and historical links between some of the major distant ancient languages of the world. William Jones remarked:

In order to explain the common features shared by Sanskrit and other Indo-European languages, the Indo-Aryan migration theory states that the original speakers of what became Sanskrit arrived in the Indian subcontinent from the north-west sometime during the early second millennium BCE. Evidence for such a theory includes the close relationship between the Indo-Iranian tongues and the Baltic and Slavic languages, vocabulary exchange with the non-Indo-European Uralic languages, and the nature of the attested Indo-European words for flora and fauna. The pre-history of Indo-Aryan languages which preceded Vedic Sanskrit is unclear and various hypotheses place it over a fairly wide limit. According to Thomas Burrow, based on the relationship between various Indo-European languages, the origin of all these languages may possibly be in what is now Central or Eastern Europe, while the Indo-Iranian group possibly arose in Central Russia. The Iranian and Indo-Aryan branches separated quite early. It is the Indo-Aryan branch that moved into eastern Iran and the south into the Indian subcontinent in the first half of the 2nd millennium BCE. Once in ancient India, the Indo-Aryan language underwent rapid linguistic change and morphed into the Vedic Sanskrit language.

The pre-Classical form of Sanskrit is known as Vedic Sanskrit. The earliest attested Sanskrit text is the Rigveda, a Hindu scripture, from the mid-to-late second millennium BCE. No written records from such an early period survive if they ever existed. However, scholars are confident that the oral transmission of the texts is reliable: they were ceremonial literature where the exact phonetic expression and its preservation were a part of the historic tradition.

The "Rigveda" is a collection of books, created by multiple authors from distant parts of ancient India. These authors represented different generations, and the mandalas 2 to 7 are the oldest while the mandalas 1 and 10 are relatively the youngest. Yet, the Vedic Sanskrit in these books of the "Rigveda" "hardly presents any dialectical diversity", states Louis Renou – an Indologist known for his scholarship of the Sanskrit literature and the "Rigveda" in particular. According to Renou, this implies that the Vedic Sanskrit language had a "set linguistic pattern" by the second half of the 2nd-millennium BCE. Beyond the "Rigveda", the ancient literature in Vedic Sanskrit that has survived into the modern age include the "Samaveda", "Yajurveda", "Atharvaveda" along with the embedded and layered Vedic texts such as the Brahmanas, Aranyakas and the early Upanishads. These Vedic documents reflect the dialects of Sanskrit found in the various parts of the northwestern, northern and eastern Indian subcontinent.

Vedic Sanskrit was both a spoken and literary language of ancient India. According to Michael Witzel, Vedic Sanskrit was a spoken language of the semi-nomadic Aryas who temporarily settled in one place, maintained cattle herds, practiced limited agriculture and after some time moved by wagon train they called "grama". The Vedic Sanskrit language or a closely related Indo-European variant was recognized beyond ancient India as evidenced by the "Mitanni Treaty" between the ancient Hittite and Mitanni people, carved into a rock, in a region that are now parts of Syria and Turkey. Parts of this treaty such as the names of the Mitannian princes and technical terms related to horse training, for reasons not understood, are in early forms of Vedic Sanskrit. The treaty also invokes the gods Varuna, Mitra, Indra and Nasatya found in the earliest layers of the Vedic literature.
The Vedic Sanskrit found in the "Rigveda" is distinctly more archaic than other Vedic texts, and in many respects, the Rigvedic language is notably more similar to those found in the archaic texts of Old Avestan Zoroastrian "Gathas" and Homer's "Iliad" and "Odyssey". According to Stephanie W. Jamison and Joel P. Brereton – Indologists known for their translation of the "Rigveda", the Vedic Sanskrit literature "clearly inherited" from Indo-Iranian and Indo-European times, the social structures such as the role of the poet and the priests, the patronage economy, the phrasal equations and some of the poetic meters. While there are similarities, state Jamison and Brereton, there are also differences between Vedic Sanskrit, the Old Avestan, and the Mycenaean Greek literature. For example, unlike the Sanskrit similes in the "Rigveda", the Old Avestan "Gathas" lack simile entirely, and it is rare in the later version of the language. The Homerian Greek, like Rigvedic Sanskrit, deploys simile extensively, but they are structurally very different.

The early Vedic form of the Sanskrit language was far less homogenous, and it evolved over time into a more structured and homogeneous language, ultimately into the Classical Sanskrit by about the mid-1st-millennium BCE. According to Richard Gombrich – an Indologist and a scholar of Sanskrit Pāli and Buddhist Studies, the archaic Vedic Sanskrit found in the "Rigveda" had already evolved in the Vedic period, as evidenced in the later Vedic literature. The language in the early Upanishads of Hinduism and the late Vedic literature approaches Classical Sanskrit, while the archaic Vedic Sanskrit had by the Buddha's time become unintelligible to all except ancient Indian sages, states Gombrich.

The formalization of the Sanskrit language is credited to , along with Patanjali's "Mahabhasya" and Katyayana's commentary that preceded Patanjali's work. Panini composed "" ("Eight-Chapter Grammar"). The century in which he lived is unclear and debated, but his work is generally accepted to be from sometime between 6th and 4th centuries BCE.

The was not the first description of Sanskrit grammar, but it is the earliest that has survived in full. Pāṇini cites ten scholars on the phonological and grammatical aspects of the Sanskrit language before him, as well as the variants in the usage of Sanskrit in different regions of India. The ten Vedic scholars he quotes are Apisali, Kashyapa, Gargya, Galava, Cakravarmana, Bharadvaja, Sakatayana, Sakalya, Senaka and Sphotayana. The of Panini became the foundation of Vyākaraṇa, a Vedanga. In the , language is observed in a manner that has no parallel among Greek or Latin grammarians. Pāṇini's grammar, according to Renou and Filliozat, defines the linguistic expression and a classic that set the standard for the Sanskrit language. Pāṇini made use of a technical metalanguage consisting of a syntax, morphology and lexicon. This metalanguage is organised according to a series of meta-rules, some of which are explicitly stated while others can be deduced.
Pāṇini's comprehensive and scientific theory of grammar is conventionally taken to mark the start of Classical Sanskrit. His systematic treatise inspired and made Sanskrit the preeminent Indian language of learning and literature for two millennia. It is unclear whether Pāṇini wrote his treatise on Sanskrit language or he orally created the detailed and sophisticated treatise then transmitted it through his students. Modern scholarship generally accepts that he knew of a form of writing, based on references to words such as "lipi" ("script") and "lipikara" ("scribe") in section 3.2 of the " Aṣṭādhyāyī".

The Classical Sanskrit language formalized by Panini, states Renou, is "not an impoverished language", rather it is "a controlled and a restrained language from which archaisms and unnecessary formal alternatives were excluded". The Classical form of the language simplified the "sandhi" rules but retained various aspects of the Vedic language, while adding rigor and flexibilities, so that it had sufficient means to express thoughts as well as being "capable of responding to the future increasing demands of an infinitely diversified literature", according to Renou. Panini included numerous "optional rules" beyond the Vedic Sanskrit's "bahulam" framework, to respect liberty and creativity so that individual writers separated by geography or time would have the choice to express facts and their views in their own way, where tradition followed competitive forms of the Sanskrit language.

The phonetic differences between Vedic Sanskrit and Classical Sanskrit are negligible when compared to the intense change that must have occurred in the pre-Vedic period between Indo-Aryan language and the Vedic Sanskrit. The noticeable differences between the Vedic and the Classical Sanskrit include the much-expanded grammar and grammatical categories as well as the differences in the accent, the semantics and the syntax. There are also some differences between how some of the nouns and verbs end, as well as the "sandhi" rules, both internal and external. Quite many words found in the early Vedic Sanskrit language are never found in late Vedic Sanskrit or Classical Sanskrit literature, while some words have different and new meanings in Classical Sanskrit when contextually compared to the early Vedic Sanskrit literature.

Arthur Macdonell was among the early colonial era scholars who summarized some of the differences between the Vedic and Classical Sanskrit. Louis Renou published in 1956, in French, a more extensive discussion of the similarities, the differences and the evolution of the Vedic Sanskrit within the Vedic period and then to the Classical Sanskrit along with his views on the history. This work has been translated by Jagbans Balbir.

Sanskrit co-existed with numerous other Prakrit languages of ancient India. The Prakrit languages of India also have ancient roots and some Sanskrit scholars have called these "Apabhramsa", literally "spoiled". The Vedic literature includes words whose phonetic equivalent are not found in other Indo European languages but which are found in the regional Prakrit languages, which makes it likely that the interaction, the sharing of words and ideas began early in the Indian history. As the Indian thought diversified and challenged earlier beliefs of Hinduism, particularly in the form of Buddhism and Jainism, the Prakrit languages such as Pali in Theravada Buddhism and Ardhamagadhi in Jainism competed with Sanskrit in the ancient times. However, states Paul Dundas – a scholar of Jainism, these ancient Prakrit languages had "roughly the same relationship to Sanskrit as medieval Italian does to Latin." The Indian tradition states that the Buddha and the Mahavira preferred Prakrit language so that everyone could understand it. However, scholars such as Dundas have questioned this hypothesis. They state that there is no evidence for this and whatever evidence is available suggests that by the start of the common era, hardly anybody other than learned monks had the capacity to understand the old Prakrit languages such as Ardhamagadhi.

Colonial era scholars questioned whether Sanskrit was ever a spoken language, or was it only a literary language? Scholars disagree in their answers. A section of Western scholars state that Sanskrit was never a spoken language, while others and particularly most Indian scholars state the opposite. Those who affirm Sanskrit to have been a vernacular language point to the necessity of Sanskrit being a spoken language for the oral tradition that preserved the vast number of Sanskrit manuscripts from ancient India. Secondly, they state that the textual evidence in the works of Yaksa, Panini and Patanajali affirms that the Classical Sanskrit in their era was a language that is spoken ("bhasha") by the cultured and educated. Some "sutras" expound upon the variant forms of spoken Sanskrit versus written Sanskrit. The 7th-century Chinese Buddhist pilgrim Xuanzang mentioned in his memoir that official philosophical debates in India were held in Sanskrit, not in the vernacular language of that region.

According to Sanskrit linguist Madhav Deshpande, Sanskrit was a spoken language in a colloquial form by the mid 1st millennium BCE which coexisted with a more formal, grammatical correct form of literary Sanskrit. This, states Deshpande, is true for modern languages where colloquial incorrect approximations and dialects of a language are spoken and understood, along with more refined, sophisticated and grammatically accurate forms of the same language being found in the literary works. The Indian tradition, states Moriz Winternitz, has favored the learning and the usage of multiple languages from the ancient times. Sanskrit was a spoken language in the educated and the elite classes, but it was also a language that must have been understood in a more wider circle of society because the widely popular folk epics and stories such as the "Ramayana", the "Mahabharata", the "Bhagavata Purana", the "Panchatantra" and many other texts are all in the Sanskrit language. The Classical Sanskrit with its exacting grammar was thus the language of the Indian scholars and the educated classes, while others communicated with approximate or ungrammatical variants of it as well as other natural Indian languages. Sanskrit, as the learned language of Ancient India, thus existed alongside the vernacular Prakrits. Many Sanskrit dramas indicate that the language coexisted with the vernacular Prakrits. Centres in Varanasi, Paithan, Pune and Kanchipuram were centers of classical Sanskrit learning and public debates until the arrival of the colonial era.

According to Étienne Lamotte – an Indologist and Buddhism scholar, Sanskrit became the dominant literary and inscriptional language because of its precision in communication. It was, states Lamotte, an ideal instrument for presenting ideas and as knowledge in Sanskrit multiplied so did its spread and influence. Sanskrit was adopted voluntarily as a vehicle of high culture, arts, and profound ideas. Pollock disagrees with Lamotte, but concurs that Sanskrit's influence grew into what he terms as "Sanskrit Cosmopolis" over a region that included all of South Asia and much of southeast Asia. The Sanskrit language cosmopolis thrived beyond India between 300 and 1300 CE.

Sanskrit has been the predominant language of Hindu texts encompassing a rich tradition of philosophical and religious texts, as well as poetry, music, drama, scientific, technical and others. It is the predominant language of one of the largest collection of historic manuscripts. The earliest known inscriptions in Sanskrit are from the 1st-century BCE, such as the Ayodhya Inscription of Dhana and Ghosundi-Hathibada (Chittorgarh).

Though developed and nurtured by scholars of orthodox schools of Hinduism, Sanskrit has been the language for some of the key literary works and theology of heterodox schools of Indian philosophies such as Buddhism and Jainism. The structure and capabilities of the Classical Sanskrit language launched ancient Indian speculations about "the nature and function of language", what is the relationship between words and their meanings in the context of a community of speakers, whether this relationship is objective or subjective, discovered or is created, how individuals learn and relate to the world around them through language, and about the limits of language? They speculated on the role of language, the ontological status of painting word-images through sound, and the need for rules so that it can serve as a means for a community of speakers, separated by geography or time, to share and understand profound ideas from each other. These speculations became particularly important to the Mimamsa and the Nyaya schools of Hindu philosophy, and later to Vedanta and Mahayana Buddhism, states Frits Staal – a scholar of Linguistics with a focus on Indian philosophies and Sanskrit. Though written in a number of different scripts, the dominant language of Hindu texts has been Sanskrit. It or a hybrid form of Sanskrit became the preferred language of Mahayana Buddhism scholarship. One of the early and influential Buddhist philosopher Nagarjuna (~200 CE), for example, used Classical Sanskrit as the language for his texts. According to Renou, Sanskrit had a limited role in the Theravada tradition (formerly known as the Hinayana) but the Prakrit works that have survived are of doubtful authenticity. Some of the canonical fragments of the early Buddhist traditions, discovered in the 20th-century, suggest the early Buddhist traditions did use of imperfect and reasonably good Sanskrit, sometimes with a Pali syntax, states Renou. The Mahāsāṃghika and Mahavastu, in their late Hinayana forms, used hybrid Sanskrit for their literature. Sanskrit was also the language of some of the oldest surviving, authoritative and much followed philosophical works of Jainism such as the "Tattvartha Sutra" by Umaswati.
The Sanskrit language has been one of the major means for the transmission of knowledge and ideas in Asian history. Indian texts in Sanskrit were already in China by 402 CE, carried by the influential Buddhist pilgrim Faxian who translated them into Chinese by 418 CE. Xuanzang, another Chinese Buddhist pilgrim, learnt Sanskrit in India and carried 657 Sanskrit texts to China in the 7th-century where he established a major center of learning and language translation under the patronage of Emperor Taizong. By the early 1st millennium CE, Sanskrit had spread Buddhist and Hindu ideas to Southeast Asia, parts of the East Asia and the Central Asia. It was accepted as a language of high culture and the preferred language by some of the local ruling elites in these regions. According to the Dalai Lama, the Sanskrit language is a parent language that is at the foundation of many modern languages of India and the one that promoted Indian thought to other distant countries. In Tibetan Buddhism, states the Dalai Lama, Sanskrit language has been a revered one and called "legjar lhai-ka" or "elegant language of the gods". It has been the means of transmitting the "profound wisdom of Buddhist philosophy" to Tibet.

The Sanskrit language created a pan-Indic accessibility to information and knowledge in the ancient and medieval times, in contrast to the Prakrit languages which were understood just regionally. It created a cultural bond across the subcontinent. As local languages and dialects evolved and diversified, Sanskrit served as the common language. It connected scholars from distant parts of the Indian subcontinent such as Tamil Nadu and Kashmir, states Deshpande, as well as those from different fields of studies, though there must have been differences in its pronunciation given the first language of the respective speakers. The Sanskrit language brought Indic people together, particularly its elite scholars. Some of these scholars of Indian history regionally produced vernacularized Sanskrit to reach wider audiences, as evidenced by texts discovered in Rajasthan, Gujarat, and Maharashtra. Once the audience became familiar with the easier to understand vernacularized version of Sanskrit, those interested could graduate from colloquial Sanskrit to the more advanced Classical Sanskrit. Rituals and the rites-of-passage ceremonies have been and continue to be the other occasions where a wide spectrum of people hear Sanskrit, and occasionally join in to speak some Sanskrit words such as ""namah"".

Classical Sanskrit is the standard register as laid out in the grammar of , around the fourth century BCE. Its position in the cultures of Greater India is akin to that of Latin and Ancient Greek in Europe. Sanskrit has significantly influenced most modern languages of the Indian subcontinent, particularly the languages of the northern, western, central and eastern Indian subcontinent.

Sanskrit declined starting about and after the 13th-century. This coincides with the beginning of Islamic invasions of the Indian subcontinent to create, thereafter expand the Muslim rule in the form of Sultanates and later the Mughal Empire. With the fall of Kashmir around the 13th-century, a premier center of Sanskrit literary creativity, Sanskrit literature there disappeared, perhaps in the "fires that periodically engulfed the capital of Kashmir" or the "Mongol invasion of 1320" states Sheldon Pollock. The Sanskrit literature which was once widely disseminated out of the northwest regions of the subcontinent, stopped after the 12th-century. As Hindu kingdoms fell in the eastern and the South India, such as the great Vijayanagara Empire, so did Sanskrit. There were exceptions and short periods of imperial support for Sanskrit, mostly concentrated during the reign of the tolerant Mughal emperor Akbar. Muslim rulers patronized the Middle Eastern language and scripts found in Persia and Arabia, and the Indians linguistically adapted to this Persianization to gain employment with the Muslim rulers. Hindu rulers such as Shivaji of the Maratha Empire, reversed the process, by re-adopting Sanskrit and re-asserting their socio-linguistic identity. After Islamic rule disintegrated in the Indian subcontinent and the colonial rule era began, Sanskrit re-emerged but in the form of a "ghostly existence" in regions such as Bengal. This decline was the result of "political institutions and civic ethos" that did not support the historic Sanskrit literary culture.

Scholars are divided on whether or when Sanskrit died. Western authors such as John Snelling state that Sanskrit and Pali are both dead Indian languages. Indian authors such as M Ramakrishnan Nair state that Sanskrit was a dead language by the 1st millennium BCE. Sheldon Pollock states that in some crucial way, "Sanskrit is dead". After the 12th-century, the Sanskrit literary works were reduced to "reinscription and restatements" of ideas already explored, and any creativity was restricted to hymns and verses. This contrasted with the previous 1,500 years when "great experiments in moral and aesthetic imagination" marked the Indian scholarship using Classical Sanskrit, states Pollock.

Other scholars state that Sanskrit language did not die, only declined. Hanneder disagrees with Pollock, finding his arguments elegant but "often arbitrary". According to Hanneder, a decline or regional absence of creative and innovative literature constitutes a negative evidence to Pollock's hypothesis, but it is not positive evidence. A closer look at Sanskrit in the Indian history after the 12th-century suggests that Sanskrit survived despite the odds. According to Hanneder,
The Sanskrit language, states Moriz Winternitz, was never a dead language and it is still alive though its prevalence is lesser than ancient and medieval times. Sanskrit remains an integral part of Hindu journals, festivals, Ramlila plays, drama, rituals and the rites-of-passage. Similarly, Brian Hatcher states that the "metaphors of historical rupture" by Pollock are not valid, that there is ample proof that Sanskrit was very much alive in the narrow confines of surviving Hindu kingdoms between the 13th and 18th-century, and its reverence and tradition continues.

Hanneder states that modern works in Sanskrit are either ignored or their "modernity" contested. According to Robert Goldman and Sally Sutherland, Sanskrit is neither "dead" nor "living" in the conventional sense. It is a special, timeless language that lives in the numerous manuscripts, daily chants and ceremonial recitations, a heritage language that Indians contextually prize and some practice.

When the British introduced English to India in the 19th century, knowledge of Sanskrit and ancient literature continued to flourish as the study of Sanskrit changed from a more traditional style into a form of analytical and comparative scholarship mirroring that of Europe.

The relationship of Sanskrit to the Prakrit languages, particularly the modern form of Indian languages, is complex and spans about 3,500 years, states Colin Masica – a linguist specializing in South Asian languages. A part of the difficulty is the lack of sufficient textual, archaeological and epigraphical evidence for the ancient Prakrit languages with rare exceptions such as Pali, leading to a tendency of anachronistic errors. Sanskrit and Prakrit languages may be divided into Old Indo-Aryan (1500 BCE-600 BCE), Middle Indo-Aryan (600 BCE-1000 CE) and New Indo-Aryan (1000 CE-current), each can further be subdivided in early, middle or second, and late evolutionary substages.

Vedic Sanskrit belongs to the early Old Indo-Aryan while Classical Sanskrit to the later Old Indo-Aryan stage. The evidence for Prakrits such as Pali (Theravada Buddhism) and Ardhamagadhi (Jainism), along with Magadhi, Maharashtri, Sinhala, Sauraseni and Niya (Gandhari), emerge in the Middle Indo-Aryan stage in two versions – archaic and more formalized – that may be placed in early and middle substages of the 600 BCE-1000 CE period. Two literary Indic languages can be traced to the late Middle Indo-Aryan stage and these are Apabhramsa and Elu (a form of literary Sinhalese). Numerous North, Central, Eastern and Western Indian languages, such as Hindi, Gujarati, Sindhi, Punjabi, Kashmiri, Nepali, Braj, Awadhi, Bengali, Assamese, Oriya, Marathi, and others belong to the New Indo-Aryan stage.

There is an extensive overlap in the vocabulary, phonetics and other aspects of these New Indo-Aryan languages with Sanskrit, but it is neither universal nor identical across the languages. They likely emerged from a synthesis of the ancient Sanskrit language traditions and an admixture of various regional dialects. Each language has some unique and regionally creative aspects, with unclear origins. Prakrit languages do have a grammatical structure, but like the Vedic Sanskrit, it is far less rigorous than Classical Sanskrit. The roots of all Prakrit languages may be in the Vedic Sanskrit and ultimately the Indo-Aryan language, their structural details vary from the Classical Sanskrit. It is generally accepted by scholars and widely believed in India that numerous major modern Indic languages such as Punjabi, Hindi, Gujarati and Bengali have roots in Sanskrit in the broadest sense of the term, and are the descendants of the Sanskrit language. Sanskrit, states Burjor Avari, can be described as "the mother language of almost all the languages of north India".

The Sanskrit language's historic presence is attested across a wide geography beyond the Indian subcontinent. Inscriptions and literary evidence suggests that Sanskrit language was already being adopted in Southeast Asia and Central Asia in the 1st-millennium CE, through monks, religious pilgrims and merchants.

The Indian subcontinent has been the geographic range of the largest collection of the ancient and pre-18th century Sanskrit manuscripts and inscriptions. Beyond ancient India, significant collections of Sanskrit manuscripts and inscriptions have been found in China (particularly the Tibetan monasteries), Myanmar, Indonesia, Cambodia, Laos, Vietnam, Thailand, and Malaysia. Sanskrit inscriptions, manuscripts or its remnants, including some of the oldest known Sanskrit written texts, have been discovered in dry high deserts and mountainous terrains such as in Nepal, Tibet, Afghanistan, Mongolia, Uzbekistan, Turkmenistan, Tajikistan, and Kazakhstan. Some Sanskrit texts and inscriptions have also been discovered in Korea and Japan.

Sanskrit is a studied school subject, but scarcely spoken in contemporary India. In the 2001 Census of India, 14,135 Indians reported Sanskrit to be their first language. In the 2011 census, 24,821 people reported Sanskrit to be their first language.

According to the 2011 national census of Nepal, 1,669 people use Sanskrit as their first language.

In India, Sanskrit is among the 22 official languages of India in the Eighth Schedule to the Constitution. The state of Uttarakhand in India lists Sanskrit as its second official language.

Sanskrit shares many Proto-Indo-European language family features. The consonantal system is the same, though it systematically enlarged the inventory of distinct sounds. For example, Sanskrit added a voiceless aspirated "Th", to the voiceless "T", voiced "D" and voiced aspirated "Dh" found in PIE languages.

The most significant and distinctive phonological development in Sanskrit is vowel-merger, states Stephanie Jamison – an Indo-European linguist specializing in Sanskrit literature. The short ∗e, ∗o and *a, all merge as "a" (अ) in Sanskrit, while long ∗ē, ∗ō and *ā, all merge as long "ā" (आ). These mergers occurred very early and significantly impacted Sanskrit's morphological system. Some phonological developments in it mirror those in other PIE languages. For example, the labiovelars merged with the plain velars as in other satem languages. However, the secondary palatalization of the resulting segments is more thorough and systematic within Sanskrit, states Jamison. A series of retroflex dental stops were invented in Sanskrit to more thoroughly articulate sounds for clarity. For example, unlike the loss of the morphological clarity from vowel contraction that is found in early Greek and related southeast European languages, Sanskrit deployed ∗y, ∗w, and ∗s intervocalically to provide morphological clarity.

The cardinal vowels ("svaras") "i" (इ), "u" (उ), "a" (अ) distinguish length in Sanskrit, states Jamison. The short "a" (अ) in Sanskrit is a closer vowel than ā, equivalent to schwa. The mid vowels ē (ए) and ō (ओ) in Sanskrit are monophthongizations of the Indo-Iranian diphthongs ∗ai and ∗au. The Old Iranian language preserved *ai and ∗au. In contrast, in Sanskrit, they are inherently long. The vocalic liquid *r̥ in Sanskrit is a merger of PIE ∗r̥ and ∗l̥. Additionally, Sanskrit invents a long-r̥ and uses it in a few analogically generated morphological categories.

According to Masica, Sanskrit has four traditional semivowels, with which were classed, "for morphophonemic reasons, the liquids: y, r, l, and v; that is, as y and v were the non-syllabics corresponding to i, u, so were r, l in relation to r̥ and l̥". The northwestern, the central and the eastern Sanskrit dialects have had a historic confusion between "r" and "l". The Paninian system that followed the central dialect preserved the distinction, likely out of reverence for the Vedic Sanskrit that distinguished the "r" and "l". However, the northwestern dialect only had "r", while the eastern dialect probably only had "l", states Masica. Thus literary works from different parts of ancient India appear inconsistent in their use of "r" and "l", resulting in doublets that is occasionally semantically differentiated.

Sanskrit invented symmetric consonantal phoneme structure based on how the sound is articulated, though the actual usage of these sounds conceals the lack of parallelism in the apparent symmetry possibly from historical changes within the language. The glides and liquids regularly alternate with vowels in Sanskrit, for example, i ≈ y; u ≈ v ([w]); r̥ ≈ r ; l̥ ≈ l, states Jamison.

Sanskrit created a series of retroflex stops. All the retroflexes in Sanskrit are in "origin conditioned alternants of dentals, though from the beginning of the language they have a qualified independence", states Jamison.

The palatals are affricates in Sanskrit, not stops. The palatal nasal is a conditioned variant of n occurring next to palatal obstruents. The "anusvara" that Sanskrit deploys is a conditioned alternant of postvocalic nasals, under certain sandhi conditions. Its "visarga" is a word-final or morpheme-final conditioned alternant of s and r under certain sandhi conditions.

The voiceless aspirated series is also an invention in Sanskrit but significantly rarer in use than the other three.

While Sanskrit invented and organized sounds for expression beyond those found in the PIE language, it retained many features found in the Iranian and Balto-Slavic languages. An example of a similar process in all three, states Jamison, is the retroflex sibilant .s being the automatic product of dental s following i, u, r, and k (mnemonically “ruki”).

Sanskrit deploys extensive phonological alternations on different linguistic levels through "sandhi" rules (literally, the rules of "putting together, union, connection, alliance"). This is similar to the English alteration of "going to" as "gonna", states Jamison. The Sanskrit language accepts such alterations within it, but offers formal rules for the "sandhi" of any two words next to each other in the same sentence or linking two sentences. The external "sandhi" rules state that similar short vowels coalesce into a single long vowel, while dissimilar vowels form glides or undergo diphthongization. Among the consonants, most external "sandhi" rules recommend regressive assimilation for clarity when they are voiced. According to Jamison, these rules ordinarily apply at compound seams and morpheme boundaries. In Vedic Sanskrit, the external "sandhi" rules are more variable than in Classical Sanskrit.

The internal "sandhi" rules are more intricate and account for the root and the canonical structure of the Sanskrit word. These rules anticipate what are now known as the Bartholomae's law and Grassmann's law. For example, states Jamison, the "voiceless, voiced, and voiced aspirated obstruents of a positional series regularly alternate with each other (p ≈ b ≈ bh; t ≈ d ≈ dh, etc.; note, however, c ≈ j ≈ h), such that, for example, a morpheme with an underlying voiced aspirate final may show alternants with all three stops under differing internal sandhi conditions". The velar series (k, g, gh) alternate with the palatal series (c, j, h), while the structural position of the palatal series is modified into a retroflex cluster when followed by dental. This rule create two morphophonemically distinct series from a single palatal series.

Vocalic alternations in the Sanskrit morphological system is termed "strengthening", and called "guna" and "vriddhi" in the preconsonantal versions. There is an equivalence to terms deployed in Indo-European descriptive grammars, wherein Sanskrit's unstrengthened state is same as the zero-grade, "guna" corresponds to normal-grade, while "vriddhi" is same as the lengthened-state. The qualitative ablaut is not found in Sanskrit just like it is absent in Iranian, but Sanskrit retains quantitative ablaut through vowel strengthening. The transformations between unstrengthened to "guna" is prominent in the morphological system, states Jamison, while "vriddhi" is a particularly significant rule when adjectives of origin and appurtenance are derived. The manner in which this is done slightly differs between the Vedic and the Classical Sanskrit.

Sanskrit grants a very flexible syllable structure, where they may begin or end with vowels, be single consonants or clusters. Similarly, the syllable may have an internal vowel of any weight. The Vedic Sanskrit shows traces of following the Sievers-Edgerton Law, but Classical Sanskrit doesn't. Vedic Sanskrit has a pitch accent system, states Jamison, which were acknowledged by Panini, but in his Classical Sanskrit the accents disappear. Most Vedic Sanskrit words have one accent. However, this accent is not phonologically predictable, states Jamison. It can fall anywhere in the word and its position often conveys morphological and syntactic information. According to Masica, the presence of an accent system in Vedic Sanskrit is evidenced from the markings in the Vedic texts. This is important because of Sanskrit's connection to the PIE languages and comparative Indo-European linguistics.

Sanskrit, like most early Indo-European languages, lost the so-called "laryngeal consonants (cover-symbol ∗H) present in the Proto-Indo-European", states Jamison. This significantly impacted the evolutionary path of the Sanskrit phonology and morphology, particularly in the variant forms of roots.

The basis of Sanskrit morphology is the root, states Jamison, "a morpheme bearing lexical meaning". The verbal and nominal stems of Sanskrit words are derived from this root through the phonological vowel-gradation processes, the addition of affixes, verbal and nominal stems. It then adds an ending to establish the grammatical and syntactic identity of the stem. According to Jamison, the "three major formal elements of the morphology are (i) root, (ii) affix, and (iii) ending; and they are roughly responsible for (i) lexical meaning, (ii) derivation, and (iii) inflection respectively". The systematic method by which the Sanskrit language derives words has attracted morphological analysis studies using modern computing tools.

A Sanskrit word has the following canonical structure:

The root structure has certain phonological constraints. Two of the most important constraints of a "root" is that it does not end in a short "a" (अ) and that it is monosyllabic. In contrast, the affixes and endings commonly do. The affixes in Sanskrit are almost always suffixes, with exceptions being the augment "a-" added as prefix to past tense verb forms and the "-na/n-" infix in single verbal present class, states Jamison.

According to Ruppel, verbs in Sanskrit express the same information as other Indo-European languages such as English. Sanskrit verbs describe an action or occurrence or state, its embedded morphology informs as to "who is doing it" (person or persons), "when it is done" (tense) and "how it is done" (mood, voice). The Indo-European languages differ in the detail. For example, the Sanskrit language attaches the affixes and ending to the verb root, while the English language adds small independent words before the verb. In Sanskrit, these elements co-exist within the word.

Sanskrit originated in an oral society, and the oral tradition was maintained through the development of early classical Sanskrit literature. Some scholars such as Jack Goody suggest that the Vedic Sanskrit texts are not the product of an oral society, basing this view by comparing inconsistencies in the transmitted versions of literature from various oral societies such as the Greek, Serbian and other cultures, then noting that the Vedic literature is too consistent and vast to have been composed and transmitted orally across generations, without being written down. These scholars add that the Vedic texts likely involved both a written and oral tradition, calling it "parallel products of a literate society".

Sanskrit has no native script of its own, and historical evidence suggests that it has been written in various scripts on a variety of media such as palm leaves, cloth, paper, rock and metal sheets, at least by the time of arrival of Alexander the Great in northwestern Indian subcontinent in 1st millennium BCE.

The earliest known rock inscriptions in Sanskrit date to the first century BCE, and the Junagadh rock inscription of Rudradaman I (c. 150 AD) "represents a turning point" as it is a more "extensive record in the poetic style" of "high Classical Sanskrit". They are in the "Brāhmī" script, which was originally used for Prakrit, not Sanskrit. It has been described as a paradox that the first evidence of written Sanskrit occurs centuries later than that of the Prakrit languages which are its linguistic descendants. In northern India, there are "Brāhmī" inscriptions dating from the third century BCE onwards, the oldest appearing on the famous Prakrit pillar inscriptions of king Ashoka. The earliest South Indian Sanskrit inscriptions in Tamil Brahmi, written in Pallava script and later Grantha, belong to the first half of the first millennium CE. The rounded shapes of the Pallava script along with Sanskrit migrated with the Tamils to southeast Asia, where it spread and ultimately evolved into Mon-Burmese, Khmer, Thai, Laos, Sumatran, Celebes, Javanese and Balinese scripts.

Brahmi evolved into a multiplicity of Brahmic scripts, many of which were used to write Sanskrit. Roughly contemporary with the Brahmi, Kharosthi was used in the northwest of the subcontinent. Sometime between the fourth and eighth centuries, the Gupta script, derived from Brahmi, became prevalent. Around the eighth century, the "Śāradā" script evolved out of the Gupta script. The latter was displaced in its turn by Devanagari in the 11th or 12th century, with intermediary stages such as the Siddhaṃ script. In East India, the Odia alphabet, and the Bengali alphabet, were used.

In the south, where Dravidian languages predominate, scripts used for Sanskrit include the Kannada, Telugu, the Malayalam and Grantha alphabets.

Since the late 18th century, Sanskrit has been transliterated using the Latin alphabet. The system most commonly used today is the IAST (International Alphabet of Sanskrit Transliteration), which has been the academic standard since 1888. ASCII-based transliteration schemes have also evolved because of difficulties representing Sanskrit characters in computer systems. These include Harvard-Kyoto and ITRANS, a transliteration scheme that is used widely on the Internet, especially in Usenet and in email, for considerations of speed of entry as well as rendering issues. With the wide availability of Unicode-aware web browsers, IAST has become common online. It is also possible to type using an alphanumeric keyboard and transliterate to Devanagari using software like Mac OS X's international support.
European scholars in the 19th century generally preferred Devanagari for the transcription and reproduction of whole texts and lengthy excerpts. However, references to individual words and names in texts composed in European Languages were usually represented with Roman transliteration. From the 20th century onwards, because of production costs, textual editions edited by Western scholars have mostly been in Romanised transliteration.

The Sanskrit grammatical tradition, Vyākaraṇa, one of the six Vedangas, began in the late Vedic period and culminated in the "Aṣṭādhyāyī" of Pāṇini, which consists of 3990 sutras (ca. fifth century BCE). About a century after Pāṇini (around 400 BCE), Kātyāyana composed "Vārtika"s on the Pāṇini sũtras. Patanjali, who lived three centuries after Pāṇini, wrote the "Mahābhāṣya", the "Great Commentary" on the "Aṣṭādhyāyī" and "Vārtika"s. Because of these three ancient Vyākaraṇins (grammarians), this grammar is called "Trimuni Vyākarana". To understand the meaning of the sutras, Jayaditya and Vāmana wrote a commentary, the "Kāsikā", in 600 CE. Pāṇinian grammar is based on 14 Shiva sutras (aphorisms), where the whole "mātrika" (alphabet) is abbreviated. This abbreviation is called the "Pratyāhara".

Sanskrit verbs are categorized into ten classes, which can be conjugated to form the present, imperfect, imperative, optative, perfect, aorist, future, and conditional moods and tenses. Before Classical Sanskrit, older forms also included a subjunctive mood. Each conjugational ending conveys person, number, and voice.

Nouns are highly inflected, including three grammatical genders, three numbers, and eight cases. Nominal compounds are common, and can include over 10 word stems.

Word order is free, though there is a strong tendency toward subject–object–verb, the original system of Vedic prose.

More than 3,000 Sanskrit works have been composed since India's independence in 1947. Much of this work has been judged of high quality, in comparison to both classical Sanskrit literature and modern literature in other Indian languages.

The Sahitya Akademi has given an award for the best creative work in Sanskrit every year since 1967. In 2009, Satya Vrat Shastri became the first Sanskrit author to win the Jnanpith Award, India's highest literary award.

Sanskrit is used extensively in the Carnatic and Hindustani branches of classical music. Kirtanas, bhajans, stotras, and shlokas of Sanskrit are popular throughout India. The samaveda uses musical notations in several of its recessions.

In Mainland China, musicians such as Sa Dingding have written pop songs in Sanskrit.

Numerous loan Sanskrit words are found in other major Asian languages. For example, Filipino, Cebuano, Lao, Khmer Thai and its alphabets, Malay, Indonesian (old Javanese-English dictionary by P.J. Zoetmulder contains over 25,500 entries), and even in English.

Over 90 weeklies, fortnightlies and quarterlies are published in Sanskrit. Sudharma, a daily newspaper in Sanskrit, has been published out of Mysore, India, since 1970, while Sanskrit Vartman Patram and Vishwasya Vrittantam started in Gujarat during the last five years. Since 1974, there has been a short daily news broadcast on state-run All India Radio. These broadcasts are also made available on the internet on AIR's website. Sanskrit news is broadcast on TV and on the internet through the DD National channel at 6:55 AM IST.

Sanskrit is the sacred language of various Hindu, Buddhist, and Jain traditions. It is used during worship in Hindu temples throughout the world. In Newar Buddhism, it is used in all monasteries, while Mahayana and Tibetan Buddhist religious texts and sutras are in Sanskrit as well as vernacular languages. Jain texts are written in Sanskrit, including the Tattvartha sutra, Ratnakaranda śrāvakācāra, the Bhaktamara Stotra and the Agamas.
It is also popular amongst the many practitioners of yoga in the West, who find the language helpful for understanding texts such as the Yoga Sutras of Patanjali.

In Nepal, India and Indonesia, Sanskrit phrases are widely used as mottoes for various national, educational and social organisations:

Attempts at reviving the Sanskrit language have been undertaken in the Republic of India since its foundation in 1947 (it was included in the 14 original languages of the Eighth Schedule to the Constitution).

Samskrita Bharati is an organisation working for Sanskrit revival. The "All-India Sanskrit Festival" (since 2002) holds composition contests. The 1991 Indian census reported 49,736 fluent speakers of Sanskrit. Sanskrit learning programmes also feature on the lists of most AIR broadcasting centres. The Mattur village in central Karnataka claims to have native speakers of Sanskrit among its population. Inhabitants of all castes learn Sanskrit starting in childhood and converse in the language. Even the local Muslims converse in Sanskrit. Historically, the village was given by king Krishnadevaraya of the Vijayanagara Empire to Vedic scholars and their families, while people in his kingdom spoke Kannada and Telugu. Another effort concentrates on preserving and passing along the oral tradition of the Vedas, is one such organisation based out of Hyderabad that has been digitising the Vedas by recording recitations of Vedic Pandits.

Haryana state has over 24 Sanskrit colleges offering education equivalent to bachelors degree, additionally masters and doctoral level degrees are also offered by the Kurukshetra University and Maharshi Dayanand University.

The Central Board of Secondary Education of India (CBSE), along with several other state education boards, has made Sanskrit an alternative option to the state's own official language as a second or third language choice in the schools it governs. In such schools, learning Sanskrit is an option for grades 5 to 8 (Classes V to VIII). This is true of most schools affiliated with the Indian Certificate of Secondary Education (ICSE) board, especially in states where the official language is Hindi. Sanskrit is also taught in traditional gurukulas throughout India.

St James Junior School in London, England, offers Sanskrit as part of the curriculum. In the United States, since September 2009, high school students have been able to receive credits as Independent Study or toward Foreign Language requirements by studying Sanskrit, as part of the "SAFL: Samskritam as a Foreign Language" program coordinated by Samskrita Bharati. In Australia, the Sydney private boys' high school Sydney Grammar School offers Sanskrit from years 7 through to 12, including for the Higher School Certificate.

A list of Sanskrit universities is given below in chronological order of establishment:

Many universities throughout the world train and employ Sanskrit scholars, either within a separate Sanskrit department or as part of a broader focus area, such as South Asian studies or Linguistics. For example, Delhi university has about 400 Sanskrit students, about half of which are in post-graduate programmes.

European scholarship in Sanskrit, begun by Heinrich Roth (1620–1668) and Johann Ernst Hanxleden (1681–1731), is considered responsible for the discovery of an Indo-European language family by Sir William Jones (1746–1794). This research played an important role in the development of Western philology, or historical linguistics.

Sir William Jones was one of the most influential philologists of his time. He told The Asiatic Society in Calcutta on 2 February 1786:
The Sanskrit language, whatever be its antiquity, is of a wonderful structure; more perfect than the Greek, more copious than the Latin, and more exquisitely refined than either, yet bearing to both of them a stronger affinity, both in the roots of verbs and in the forms of grammar, than could have been produced by accident; so strong, indeed, that no philologer could examine them all three, without believing them to have sprung from some common source, which, perhaps, no longer exists.

Orientalist scholars of the 18th century like Sir William Jones marked a wave of enthusiasm for Indian culture and for Sanskrit. According to Thomas Trautmann, after this period of "Indomania", a certain hostility to Sanskrit and to Indian culture in general began to assert itself in early 19th century Britain, manifested by a neglect of Sanskrit in British academia. This was the beginning of a general push in favour of the idea that India should be culturally, religiously and linguistically assimilated to Britain as far as possible. Trautmann considers two separate and logically opposite sources for the growing hostility: one was "British Indophobia", which he calls essentially a developmentalist, progressivist, liberal, and non-racial-essentialist critique of Hindu civilisation as an aid for the improvement of India along European lines; the other was scientific racism, a theory of the English "common-sense view" that Indians constituted a "separate, inferior and unimprovable race".

For nearly 2,000 years, Sanskrit was the language of a cultural order that exerted influence across South Asia, Inner Asia, Southeast Asia, and to a certain extent East Asia. A significant form of post-Vedic Sanskrit is found in the Sanskrit of Indian epic poetry—the "Ramayana" and "Mahabharata". The deviations from in the epics are generally considered to be on account of interference from Prakrits, or innovations, and not because they are pre-Paninian. Traditional Sanskrit scholars call such deviations "ārṣa" (आर्ष), meaning 'of the ṛṣis', the traditional title for the ancient authors. In some contexts, there are also more "prakritisms" (borrowings from common speech) than in Classical Sanskrit proper. Buddhist Hybrid Sanskrit is a literary language heavily influenced by the Middle Indo-Aryan languages, based on early Buddhist Prakrit texts which subsequently assimilated to the Classical Sanskrit standard in varying degrees.

Sanskrit has greatly influenced the languages of India that grew from its vocabulary and grammatical base; for instance, Hindi is a "Sanskritised register" of Hindustani. All modern Indo-Aryan languages, as well as Munda and Dravidian languages have borrowed many words either directly from Sanskrit ("tatsama" words), or indirectly via middle Indo-Aryan languages ("tadbhava" words). Words originating in Sanskrit are estimated at roughly fifty percent of the vocabulary of modern Indo-Aryan languages, as well as the literary forms of Malayalam and Kannada. Literary texts in Telugu are lexically Sanskrit or Sanskritised to an enormous extent, perhaps seventy percent or more. Marathi is another prominent language in Western India, that derives most of its words and Marathi grammar from Sanskrit. Sanskrit words are often preferred in the literary texts in Marathi over corresponding colloquial Marathi word.

Buddhist Sanskrit has had a considerable influence on East Asian languages such as Chinese, state William Wang and Chaofen Sun. Many words have been adopted from Sanskrit into the Chinese, both in its historic religious discourse and everyday use. This process likely started about 200 CE and continued through about 1400 CE, with the efforts of monks such as Yuezhi, Anxi, Kangju, Tianzhu, Yan Fodiao, Faxian, Xuanzang and Yijing. Further, as the Chinese language and culture influenced the rest of East Asia, the ideas in Sanskrit texts and some of its linguistic elements migrated further.

Sanskrit has also influenced Sino-Tibetan languages, mostly through translations of Buddhist Hybrid Sanskrit. Many terms were transliterated directly and added to the Chinese vocabulary. Chinese words like "chànà" (Devanagari: क्षण "" 'instantaneous period') were borrowed from Sanskrit. Many Sanskrit texts survive only in Tibetan collections of commentaries to the Buddhist teachings, the Tengyur.

Sanskrit was a language for religious purposes and for the political elite in parts of medieval era Southeast Asia, Central Asia and East Asia. In Southeast Asia, languages such as Thai and Lao contain many loanwords from Sanskrit, as do Khmer. For example, in Thai, Ravana, the emperor of Lanka, is called "Thosakanth", a derivation of his Sanskrit name "Dāśakaṇṭha" "having ten necks".

Many Sanskrit loanwords are also found in Austronesian languages, such as Javanese, particularly the older form in which nearly half the vocabulary is borrowed. Other Austronesian languages, such as traditional Malay and modern Indonesian, also derive much of their vocabulary from Sanskrit. Similarly, Philippine languages such as Tagalog have some Sanskrit loanwords, although more are derived from Spanish. A Sanskrit loanword encountered in many Southeast Asian languages is the word "bhāṣā", or spoken language, which is used to refer to the names of many languages. English also has words of Sanskrit origin.
Sanskrit has also influenced the religious register of Japanese mostly through transliterations.These were borrowed from Chinese transliterations.

"Satyagraha", an opera by Philip Glass, uses texts from the "Bhagavad Gita", sung in Sanskrit. The closing credits of "The Matrix Revolutions" has a prayer from the "Brihadaranyaka Upanishad". The song "Cyber-raga" from Madonna's album "Music" includes Sanskrit chants, and "Shanti/Ashtangi" from her 1998 album "Ray of Light", which won a Grammy, is the ashtanga vinyasa yoga chant. The lyrics include the mantra Om shanti. Composer John Williams featured choirs singing in Sanskrit for "Indiana Jones and the Temple of Doom" and in "". The theme song of "Battlestar Galactica 2004" is the Gayatri Mantra, taken from the Rigveda. The lyrics of "The Child In Us" by Enigma also contains Sanskrit verses.




</doc>
<doc id="27699" url="https://en.wikipedia.org/wiki?curid=27699" title="Sign language">
Sign language

Sign languages (also known as signed languages) are languages that use manual communication to convey meaning. This can include simultaneously employing hand gestures, movement, orientation of the fingers, arms or body, and facial expressions to convey a speaker's ideas. Sign languages often share significant similarities with their respective spoken language, such as American Sign Language (ASL) with American English. Grammar and sentence structure, however, may vary to encourage efficiency and fluidity in speaking. It is important to note that just because a spoken language is intelligible transnationally, such as English in the United States and the United Kingdom, does not mean that the sign languages from those regions are as well; ASL and British Sign Language (BSL) were formed independently and are therefore not mutually intelligible.

Linguists consider both spoken and signed communication to be types of natural language, meaning that both emerged through an abstract, protracted aging process and evolved over time without meticulous planning. Sign language should not be confused with "body language", a type of nonverbal communication.

Wherever communities of deaf people exist, sign languages have developed, and are at the cores of local deaf cultures. Although signing is used primarily by the deaf and hard of hearing, it is also used by hearing individuals, such as those unable to physically speak, or those who have trouble with spoken language due to a disability or condition (augmentative and alternative communication).

It is unclear how many sign languages currently exist worldwide. A common misconception is that all sign languages are the same worldwide or that sign language is international. Each country generally has its own, native sign language, and some have more than one (although there are also substantial similarities among all sign languages). The 2013 edition of Ethnologue lists 137 sign languages. Some sign languages have obtained some form of legal recognition, while others have no status at all.

Linguists distinguish natural sign languages from other systems that are precursors to them or derived from them, such as invented manual codes for spoken languages, home sign, "baby sign", and signs learned by non-human primates.

Groups of deaf people have used sign languages throughout history. One of the earliest written records of a sign language is from the fifth century BC, in Plato's "Cratylus", where Socrates says: "If we hadn't a voice or a tongue, and wanted to express things to one another, wouldn't we try to make signs by moving our hands, head, and the rest of our body, just as dumb people do at present?"

Until the 19th century, most of what is known about historical sign languages is limited to the manual alphabets (fingerspelling systems) that were invented to facilitate transfer of words from a spoken language to a sign language, rather than documentation of the language itself. Pedro Ponce de León (1520–1584) is said to have developed the first manual alphabet.

In 1620, Juan Pablo Bonet published (‘Reduction of letters and art for teaching mute people to speak’) in Madrid. It is considered the first modern treatise of sign language phonetics, setting out a method of oral education for deaf people and a manual alphabet.
In Britain, manual alphabets were also in use for a number of purposes, such as secret communication, public speaking, or communication by deaf people. In 1648, John Bulwer described "Master Babington", a deaf man proficient in the use of a manual alphabet, "contryved on the joynts of his fingers", whose wife could converse with him easily, even in the dark through the use of tactile signing.

In 1680, George Dalgarno published "Didascalocophus, or, The deaf and dumb mans tutor", in which he presented his own method of deaf education, including an "arthrological" alphabet, where letters are indicated by pointing to different joints of the fingers and palm of the left hand. Arthrological systems had been in use by hearing people for some time; some have speculated that they can be traced to early Ogham manual alphabets.

The vowels of this alphabet have survived in the contemporary alphabets used in British Sign Language, Auslan and New Zealand Sign Language. The earliest known printed pictures of consonants of the modern two-handed alphabet appeared in 1698 with "Digiti Lingua" (Latin for "Language" [or "Tongue"] "of the Finger"), a pamphlet by an anonymous author who was himself unable to speak. He suggested that the manual alphabet could also be used by mutes, for silence and secrecy, or purely for entertainment. Nine of its letters can be traced to earlier alphabets, and 17 letters of the modern two-handed alphabet can be found among the two sets of 26 handshapes depicted.

Charles de La Fin published a book in 1692 describing an alphabetic system where pointing to a body part represented the first letter of the part (e.g. Brow=B), and vowels were located on the fingertips as with the other British systems. He described such codes for both English and Latin.

By 1720, the British manual alphabet had found more or less its present form. Descendants of this alphabet have been used by deaf communities (or at least in classrooms) in former British colonies India, Australia, New Zealand, Uganda and South Africa, as well as the republics and provinces of the former Yugoslavia, Grand Cayman Island in the Caribbean, Indonesia, Norway, Germany and the United States.

Frenchman Charles-Michel de l'Épée published his manual alphabet in the 18th century, which has survived basically unchanged in France and North America until the present time. In 1755, Abbé de l'Épée founded the first school for deaf children in Paris; Laurent Clerc was arguably its most famous graduate. Clerc went to the United States with Thomas Hopkins Gallaudet to found the American School for the Deaf in Hartford, Connecticut, in 1817. Gallaudet's son, Edward Miner Gallaudet, founded a school for the deaf in 1857 in Washington, D.C., which in 1864 became the National Deaf-Mute College. Now called Gallaudet University, it is still the only liberal arts university for deaf people in the world.

International Sign, formerly known as Gestuno, is used mainly at international deaf events such as the Deaflympics and meetings of the World Federation of the Deaf. While recent studies claim that International Sign is a kind of a pidgin, they conclude that it is more complex than a typical pidgin and indeed is more like a full sign language.
While the more commonly used term is International Sign, it is sometimes referred to as Gestuno, or International Sign Pidgin and International Gesture (IG). International Sign is a term used by the World Federation of the Deaf and other international organisations.

In linguistic terms, sign languages are as rich and complex as any spoken language, despite the common misconception that they are not "real languages". Professional linguists have studied many sign languages and found that they exhibit the fundamental properties that exist in all languages.

Sign languages are not mime—in other words, signs are conventional, often arbitrary and do not necessarily have a visual relationship to their referent, much as most spoken language is not onomatopoeic. While iconicity is more systematic and widespread in sign languages than in spoken ones, the difference is not categorical. The visual modality allows the human preference for close connections between form and meaning, present but suppressed in spoken languages, to be more fully expressed. This does not mean that sign languages are a visual rendition of a spoken language. They have complex grammars of their own and can be used to discuss any topic, from the simple and concrete to the lofty and abstract.

Sign languages, like spoken languages, organize elementary, meaningless units called phonemes into meaningful semantic units. (These were once called cheremes (from the Greek word for "hand") in the case of sign languages, by analogy to the phonemes (from Greek for "voice") of spoken languages, but now also called phonemes, since the function is the same.) This is often called duality of patterning. As in spoken languages, these meaningless units are represented as (combinations of) features, although often also crude distinctions are made in terms of handshape (or "handform"), orientation, location (or "place of articulation"), movement, and non-manual expression.

Common linguistic features of many sign languages are the occurrence of classifiers, a high degree of inflection by means of changes of movement, and a topic-comment syntax. More than spoken languages, sign languages can convey meaning by simultaneous means, e.g. by the use of space, two manual articulators, and the signer's face and body. Though there is still much discussion on the topic of iconicity in sign languages, classifiers are generally considered to be highly iconic, as these complex constructions "function as predicates that may express any or all of the following: motion, position, stative-descriptive, or handling information". It needs to be noted that the term classifier is not used by everyone working on these constructions. Across the field of sign language linguistics the same constructions are also referred with other terms.

Today, linguists study sign languages as true languages, part of the field of linguistics. However, the category "sign languages" was not added to the "Linguistic Bibliography / Bibliographie Linguistique" until the 1988 volume, when it appeared with 39 entries.

Always there is a common misconception that sign languages are somehow dependent on spoken languages: that they are spoken language expressed in signs, or that they were invented by hearing people. Similarities in language processing in the brain between signed and spoken languages further perpetuated this misconception. Hearing teachers in deaf schools, such as Charles-Michel de l'Épée or Thomas Hopkins Gallaudet, are often incorrectly referred to as "inventors" of sign language. Instead, sign languages, like all natural languages, are developed by the people who use them, in this case, deaf people, who may have little or no knowledge of any spoken language.

As a sign language develops, it sometimes borrows elements from spoken languages, just as all languages borrow from other languages that they are in contact with. Sign languages vary in how and how much they borrow from spoken languages. In many sign languages, a manual alphabet (fingerspelling) may be used in signed communication to borrow a word from a spoken language, by spelling out the letters. This is most commonly used for proper names of people and places; it is also used in some languages for concepts for which no sign is available at that moment, particularly if the people involved are to some extent bilingual in the spoken language. Fingerspelling can sometimes be a source of new signs, such as initialized signs, in which the handshape represents the first letter of a spoken word with the same meaning.

On the whole, though, sign languages are independent of spoken languages and follow their own paths of development. For example, British Sign Language (BSL) and American Sign Language (ASL) are quite different and mutually unintelligible, even though the hearing people of the United Kingdom and the United States share the same spoken language. The grammars of sign languages do not usually resemble those of spoken languages used in the same geographical area; in fact, in terms of syntax, ASL shares more with spoken Japanese than it does with English.

Similarly, countries which use a single spoken language throughout may have two or more sign languages, or an area that contains more than one spoken language might use only one sign language. South Africa, which has 11 official spoken languages and a similar number of other widely used spoken languages, is a good example of this. It has only one sign language with two variants due to its history of having two major educational institutions for the deaf which have served different geographic areas of the country.

Sign languages exploit the unique features of the visual medium (sight), but may also exploit tactile features (tactile sign languages). Spoken language is by and large linear; only one sound can be made or received at a time. Sign language, on the other hand, is visual and, hence, can use a simultaneous expression, although this is limited articulatorily and linguistically. Visual perception allows processing of simultaneous information.

One way in which many sign languages take advantage of the spatial nature of the language is through the use of classifiers. Classifiers allow a signer to spatially show a referent's type, size, shape, movement, or extent.

The large focus on the possibility of simultaneity in sign languages in contrast to spoken languages is sometimes exaggerated, though. The use of two manual articulators is subject to motor constraints, resulting in a large extent of symmetry or signing with one articulator only. Further, sign languages, just like spoken languages, depend on linear sequencing of signs to form sentences; the greater use of simultaneity is mostly seen in the morphology (internal structure of individual signs).

Sign languages convey much of their prosody through non-manual signs. Postures or movements of the body, head, eyebrows, eyes, cheeks, and mouth are used in various combinations to show several categories of information, including lexical distinction, grammatical structure, adjectival or adverbial content, and discourse functions. While the content of a signed sentence is produced manually, most of the grammatical relations are produced non-manually (i.e., with the face and the torso).

In ASL (American Sign Language), some signs have required facial components that distinguish them from other signs. An example of this sort of lexical distinction is the sign translated 'not yet', which requires that the tongue touch the lower lip and that the head rotate from side to side, in addition to the manual part of the sign. Without these features it would be interpreted as 'late'.

Grammatical structure that is shown through non-manual signs includes questions, negation, relative clauses, boundaries between sentences, and the argument structure of some verbs. ASL and BSL use similar non-manual marking for yes/no questions, for example. They are shown through raised eyebrows and a forward head tilt.

Some adjectival and adverbial information is conveyed through non-manual signs, but what these signs are varies from language to language. For instance, in ASL a slightly open mouth with the tongue relaxed and visible in the corner of the mouth means 'carelessly', but a similar sign in BSL means 'boring' or 'unpleasant'.

Discourse functions such as turn taking are largely regulated through head movement and eye gaze. Since the addressee in a signed conversation must be watching the signer, a signer can avoid letting the other person have a turn by not looking at them, or can indicate that the other person may have a turn by making eye contact.

The first studies on iconicity in ASL were published in the late 1970s, and early 1980s. Many early sign language linguists rejected the notion that iconicity was an important aspect of the language. Though they recognized that certain aspects of the language seemed iconic, they considered this to be merely extralinguistic, a property which did not influence the language. However, mimetic aspects of sign language (signs that imitate, mimic, or represent) are found in abundance across a wide variety of sign languages. For example, deaf children learning sign language try to express something but do not know the associated sign, they will often invent an iconic sign that displays mimetic properties. Though it never disappears from a particular sign language, iconicity is gradually weakened as forms of sign languages become more customary and are subsequently grammaticized. As a form becomes more conventional, it becomes disseminated in a methodical way phonologically to the rest of the sign language community. Frishberg (1975) wrote a very influential paper addressing the relationship between arbitrariness and iconicity in ASL. She concluded that though originally present in many signs, iconicity is degraded over time through the application of grammatical processes. In other words, over time, the natural processes of regularization in the language obscures any iconically motivated features of the sign.

Some researchers have suggested that the properties of ASL give it a clear advantage in terms of learning and memory. Psychologist Roger Brown was one of the first to document this benefit. In his study, Brown found that when children were taught signs that had high levels of iconic mapping they were significantly more likely to recall the signs in a later memory task than when they were taught signs that had little or no iconic properties.

A central task for the pioneers of sign language linguistics was trying to prove that ASL was a real language and not merely a collection of gestures or "English on the hands." One of the prevailing beliefs at this time was that 'real languages' must consist of an arbitrary relationship between form and meaning. Thus, if ASL consisted of signs that had iconic form-meaning relationship, it could not be considered a real language. As a result, iconicity as a whole was largely neglected in research of sign languages.

The cognitive linguistics perspective rejects a more traditional definition of iconicity as a relationship between linguistic form and a concrete, real-world referent. Rather it is a set of selected correspondences between the form and meaning of a sign. In this view, iconicity is grounded in a language user's mental representation ("construal" in cognitive grammar). It is defined as a fully grammatical and central aspect of a sign language rather than a peripheral phenomenon.

The cognitive linguistics perspective allows for some signs to be fully iconic or partially iconic given the number of correspondences between the possible parameters of form and meaning. In this way, the Israeli Sign Language (ISL) sign for "ask" has parts of its form that are iconic ("movement away from the mouth" means "something coming from the mouth"), and parts that are arbitrary (the handshape, and the orientation).

Many signs have metaphoric mappings as well as iconic or metonymic ones. For these signs there are three way correspondences between a form, a concrete source and an abstract target meaning. The ASL sign LEARN has this three way correspondence. The abstract target meaning is "learning". The concrete source is putting objects into the head from books. The form is a grasping hand moving from an open palm to the forehead. The iconic correspondence is between form and concrete source. The metaphorical correspondence is between concrete source and abstract target meaning. Because the concrete source is connected to two correspondences linguistics refer to metaphorical signs as "double mapped".
Although sign languages have emerged naturally in deaf communities alongside or among spoken languages, they are unrelated to spoken languages and have different grammatical structures at their core.

Sign languages may be classified by how they arise.

In non-signing communities, home sign is not a full language, but closer to a pidgin. Home sign is amorphous and generally idiosyncratic to a particular family, where a deaf child does not have contact with other deaf children and is not educated in sign. Such systems are not generally passed on from one generation to the next. Where they are passed on, creolization would be expected to occur, resulting in a full language. However, home sign may also be closer to full language in communities where the hearing population has a gestural mode of language; examples include various Australian Aboriginal sign languages and gestural systems across West Africa, such as Mofu-Gudur in Cameroon.

A village sign language is a local indigenous language that typically arises over several generations in a relatively insular community with a high incidence of deafness, and is used both by the deaf and by a significant portion of the hearing community, who have deaf family and friends. The most famous of these is probably the extinct Martha's Vineyard Sign Language of the US, but there are also numerous village languages scattered throughout Africa, Asia, and America.

Deaf-community sign languages, on the other hand, arise where deaf people come together to form their own communities. These include school sign, such as Nicaraguan Sign Language, which develop in the student bodies of deaf schools which do not use sign as a language of instruction, as well as community languages such as Bamako Sign Language, which arise where generally uneducated deaf people congregate in urban centers for employment. At first, Deaf-community sign languages are not generally known by the hearing population, in many cases not even by close family members. However, they may grow, in some cases becoming a language of instruction and receiving official recognition, as in the case of ASL.

Both contrast with speech-taboo languages such as the various Aboriginal Australian sign languages, which are developed by the hearing community and only used secondarily by the deaf. It is doubtful whether most of these are languages in their own right, rather than manual codes of spoken languages, though a few such as Yolngu Sign Language are independent of any particular spoken language. Hearing people may also develop sign to communicate with speakers of other languages, as in Plains Indian Sign Language; this was a contact signing system or pidgin that was evidently not used by deaf people in the Plains nations, though it presumably influenced home sign.

Language contact and creolization is common in the development of sign languages, making clear family classifications difficult – it is often unclear whether lexical similarity is due to borrowing or a common parent language, or whether there was one or several parent languages, such as several village languages merging into a Deaf-community language. Contact occurs between sign languages, between sign and spoken languages (contact sign, a kind of pidgin), and between sign languages and gestural systems used by the broader community. One author has speculated that Adamorobe Sign Language, a village sign language of Ghana, may be related to the "gestural trade jargon used in the markets throughout West Africa", in vocabulary and areal features including prosody and phonetics.

The only comprehensive classification along these lines going beyond a simple listing of languages dates back to 1991. The classification is based on the 69 sign languages from the 1988 edition of Ethnologue that were known at the time of the 1989 conference on sign languages in Montreal and 11 more languages the author added after the conference.

In his classification, the author distinguishes between primary and auxiliary sign languages as well as between single languages and names that are thought to refer to more than one language. The prototype-A class of languages includes all those sign languages that seemingly cannot be derived from any other language. Prototype-R languages are languages that are remotely modelled on a prototype-A language (in many cases thought to have been French Sign Language) by a process Kroeber (1940) called "stimulus diffusion". The families of BSL, DGS, JSL, LSF (and possibly LSG) were the products of creolization and relexification of prototype languages. Creolization is seen as enriching overt morphology in sign languages, as compared to reducing overt morphology in spoken languages.

Linguistic typology (going back to Edward Sapir) is based on word structure and distinguishes morphological classes such as agglutinating/concatenating, inflectional, polysynthetic, incorporating, and isolating ones.

Sign languages vary in word-order typology. For example, Austrian Sign Language, Japanese Sign Language and Indo-Pakistani Sign Language are Subject-object-verb while ASL is Subject-verb-object. Influence from the surrounding spoken languages is not improbable.

Sign languages tend to be incorporating classifier languages, where a classifier handshape representing the object is incorporated into those transitive verbs which allow such modification. For a similar group of intransitive verbs (especially motion verbs), it is the subject which is incorporated. Only in a very few sign languages (for instance Japanese Sign Language) are agents ever incorporated. in this way, since subjects of intransitives are treated similarly to objects of transitives, incorporation in sign languages can be said to follow an ergative pattern.

Brentari classifies sign languages as a whole group determined by the medium of communication (visual instead of auditory) as one group with the features monosyllabic and polymorphemic. That means, that one syllable (i.e. one word, one sign) can express several morphemes, e.g., subject and object of a verb determine the direction of the verb's movement (inflection).

Another aspect of typology that has been studied in sign languages is their systems for cardinal numbers. Typologically significant differences have been found between sign languages.

Children who are exposed to a sign language from birth will acquire it, just as hearing children acquire their native spoken language.

The Critical Period hypothesis suggests that language, spoken or signed, is more easily acquired as a child at a young age versus an adult because of the plasticity of the child's brain. In a study done at the University of McGill, they found that American Sign Language users who acquired the language natively (from birth) performed better when asked to copy videos of ASL sentences than ASL users who acquired the language later in life. They also found that there are differences in the grammatical morphology of ASL sentences between the two groups, all suggesting that there is a very important critical period in learning signed languages.

The acquisition of non-manual features follows an interesting pattern: When a word that always has a particular non-manual feature associated with it (such as a wh- question word) is learned, the non-manual aspects are attached to the word but don’t have the flexibility associated with adult use. At a certain point, the non-manual features are dropped and the word is produced with no facial expression. After a few months, the non-manuals reappear, this time being used the way adult signers would use them.

Sign languages do not have a traditional or formal written form. Many deaf people do not see a need to write their own language.

Several ways to represent sign languages in written form have been developed.


So far, there is no consensus regarding the written form of sign language. Except for SignWriting, none are widely used. Maria Galea writes that SignWriting "is becoming widespread, uncontainable and untraceable. In the same way that works written in and about a well developed writing system such as the Latin script, the time has arrived where SW is so widespread, that it is impossible in the same way to list all works that have been produced using this writing system and that have been written about this writing system." In 2015, the Federal University of Santa Catarina accepted a dissertation written in Brazilian Sign Language using Sutton SignWriting for a master's degree in linguistics. The dissertation "The Writing of Grammatical Non-Manual Expressions in Sentences in LIBRAS Using the SignWriting System" by João Paulo Ampessan states that "the data indicate the need for [non-manual expressions] usage in writing sign language".

For a native signer, sign perception influences how the mind makes sense of their visual language experience. For example, a handshape may vary based on the other signs made before or after it, but these variations are arranged in perceptual categories during its development. The mind detects handshape contrasts but groups similar handshapes together in one category. Different handshapes are stored in other categories. The mind ignores some of the similarities between different perceptual categories, at the same time preserving the visual information within each perceptual category of handshape variation.

When Deaf people constitute a relatively small proportion of the general population, Deaf communities often develop that are distinct from the surrounding hearing community.
These Deaf communities are very widespread in the world, associated especially with sign languages used in urban areas and throughout a nation, and the cultures they have developed are very rich.

One example of sign language variation in the Deaf community is Black ASL. This sign language was developed in the Black Deaf community as a variant during the American era of segregation and racism, where young Black Deaf students were forced to attend separate schools than their white Deaf peers.

On occasion, where the prevalence of deaf people is high enough, a deaf sign language has been taken up by an entire local community, forming what is sometimes called a "village sign language" or "shared signing community". Typically this happens in small, tightly integrated communities with a closed gene pool. Famous examples include:
In such communities deaf people are generally well integrated in the general community and not socially disadvantaged, 
so much so that it is difficult to speak of a separate "Deaf" community.

Many Australian Aboriginal sign languages arose in a context of extensive speech taboos, such as during mourning and initiation rites. They are or were especially highly developed among the Warlpiri, Warumungu, Dieri, Kaytetye, Arrernte, and Warlmanpa, and are based on their respective spoken languages.

A pidgin sign language arose among tribes of American Indians in the Great Plains region of North America (see Plains Indian Sign Language). It was used by hearing people to communicate among tribes with different spoken languages, as well as by deaf people. There are especially users today among the Crow, Cheyenne, and Arapaho. Unlike Australian Aboriginal sign languages, it shares the spatial grammar of deaf sign languages.
In the 1500s, a Spanish expeditionary, Cabeza de Vaca, observed natives in the western part of modern-day Florida using sign language, and in the mid-16th century Coronado mentioned that communication with the Tonkawa using signs was possible without a translator. Whether or not these gesture systems reached the stage at which they could properly be called languages is still up for debate. There are estimates indicating that as many as 2% of Native Americans are seriously or completely deaf, a rate more than twice the national average.

Signs may also be used by hearing people for manual communication in secret situations, such as hunting, in noisy environments, underwater, through windows or at a distance.

Some sign languages have obtained some form of legal recognition, while others have no status at all. Sarah Batterbury has argued that sign languages should be recognized and supported not merely as an accommodation for the disabled, but as the communication medium of language communities.

One of the first demonstrations of the ability for telecommunications to help sign language users communicate with each other occurred when AT&T's videophone (trademarked as the "Picturephone") was introduced to the public at the 1964 New York World's Fair – two deaf users were able to freely communicate with each other between the fair and another city. However, video communication did not become widely available until sufficient bandwidth for the high volume of video data became available in the early 2000s.

The Internet now allows deaf people to talk via a video link, either with a special-purpose videophone designed for use with sign language or with "off-the-shelf" video services designed for use with broadband and an ordinary computer webcam. The special videophones that are designed for sign language communication may provide better quality than 'off-the-shelf' services and may use data compression methods specifically designed to maximize the intelligibility of sign languages. Some advanced equipment enables a person to remotely control the other person's video camera, in order to zoom in and out or to point the camera better to understand the signing.

In order to facilitate communication between deaf and hearing people, sign language interpreters are often used. Such activities involve considerable effort on the part of the interpreter, since sign languages are distinct natural languages with their own syntax, different from any spoken language.

The interpretation flow is normally between a sign language and a spoken language that are customarily used in the same country, such as French Sign Language (LSF) and spoken French in France, Spanish Sign Language (LSE) to spoken Spanish in Spain, British Sign Language (BSL) and spoken English in the U.K., and American Sign Language (ASL) and spoken English in the USA and most of anglophone Canada (since BSL and ASL are distinct sign languages both used in English-speaking countries), etc. Sign language interpreters who can translate between signed and spoken languages that are not normally paired (such as between LSE and English), are also available, albeit less frequently.

With recent developments in artificial intelligence in computer science, some recent deep learning based machine translation algorithms have been developed which automatically translate short videos containing sign language sentences (often simple sentence consists of only one clause) directly to written language. 

Interpreters may be physically present with both parties to the conversation but, since the technological advancements in the early 2000s, provision of interpreters in remote locations has become available. In video remote interpreting (VRI), the two clients (a sign language user and a hearing person who wish to communicate with each other) are in one location, and the interpreter is in another. The interpreter communicates with the sign language user via a video telecommunications link, and with the hearing person by an audio link. VRI can be used for situations in which no on-site interpreters are available.

However, VRI cannot be used for situations in which all parties are speaking via telephone alone. With video relay service (VRS), the sign language user, the interpreter, and the hearing person are in three separate locations, thus allowing the two clients to talk to each other on the phone through the interpreter.

Sign language is sometimes provided for television programmes. The signer usually appears in the bottom corner of the screen, with the programme being broadcast full size or slightly shrunk away from that corner. Typically for press conferences such as those given by the Mayor of New York City, the signer appears to stage left or right of the public official to allow both the speaker and signer to be in frame at the same time.

Paddy Ladd initiated deaf programming on British television in the 1980s and is credited with getting sign language on television and enabling deaf children to be educated in sign.

In traditional analogue broadcasting, many programmes are repeated, often in the early hours of the morning, with the signer present rather than have them appear at the main broadcast time. This is due to the distraction they cause to those not wishing to see the signer. On the BBC, many programmes that broadcast late at night or early in the morning are signed. Some emerging television technologies allow the viewer to turn the signer on and off in a similar manner to subtitles and closed captioning.

Legal requirements covering sign language on television vary from country to country. In the United Kingdom, the Broadcasting Act 1996 addressed the requirements for blind and deaf viewers, but has since been replaced by the Communications Act 2003.

As with any spoken language, sign languages are also vulnerable to becoming endangered. For example, a sign language used by a small community may be endangered and even abandoned as users shift to a sign language used by a larger community, as has happened with Hawai'i Sign Language, which is almost extinct except for a few elderly signers. Even national sign languages can be endangered; for example, New Zealand Sign Language is losing users. Methods are being developed to assess the language vitality of sign languages.

There are a number of communication systems that are similar in some respects to sign languages, while not having all the characteristics of a full sign language, particularly its grammatical structure. Many of these are either precursors to natural sign languages or are derived from them.

When Deaf and Hearing people interact, signing systems may be developed that use signs drawn from a natural sign language but used according to the grammar of the spoken language. In particular, when people devise one-for-one sign-for-word correspondences between spoken words (or even morphemes) and signs that represent them, the system that results is a manual code for a spoken language, rather than a natural sign language. Such systems may be invented in an attempt to help teach Deaf children the spoken language, and generally are not used outside an educational context.

It has become popular for hearing parents to teach signs (from ASL or some other sign language) to young hearing children. Since the muscles in babies' hands grow and develop quicker than their mouths, signs can be a beneficial option for better communication. Babies can usually produce signs before they can speak. This reduces the confusion between parents when trying to figure out what their child wants. When the child begins to speak, signing is usually abandoned, so the child does not progress to acquiring the grammar of the sign language.

This is in contrast to hearing children who grow up with Deaf parents, who generally acquire the full sign language natively, the same as Deaf children of Deaf parents.

Informal, rudimentary sign systems are sometimes developed within a single family. For instance, when hearing parents with no sign language skills have a deaf child, the child may develop a system of signs naturally, unless repressed by the parents. The term for these mini-languages is home sign (sometimes "home sign" or "kitchen sign").

Home sign arises due to the absence of any other way to communicate. Within the span of a single lifetime and without the support or feedback of a community, the child naturally invents signs to help meet his or her communication needs, and may even develop a few grammatical rules for combining short sequences of signs. Still, this kind of system is inadequate for the intellectual development of a child and it comes nowhere near meeting the standards linguists use to describe a complete language. No type of home sign is recognized as a full language.

There have been several notable examples of scientists teaching signs to non-human primates in order to communicate with humans, such as 
common chimpanzees, 
gorillas and 
orangutans. However, linguists generally point out that this does not constitute knowledge of a human "language" (as a complete system, rather than simply signs/words). Notable examples of animals who have learned signs include:

One theory of the evolution of human language states that it developed first as a gestural system, which later shifted to speech. An important question for this gestural theory is what caused the shift to vocalization.






"Note: the articles for specific sign languages (e.g. ASL or BSL) may contain further external links, e.g. for learning those languages."


</doc>
<doc id="27701" url="https://en.wikipedia.org/wiki?curid=27701" title="String (computer science)">
String (computer science)

In computer programming, a string is traditionally a sequence of characters, either as a literal constant or as some kind of variable. The latter may allow its elements to be mutated and the length changed, or it may be fixed (after creation). A string is generally considered a data type and is often implemented as an array data structure of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding. "String" may also denote more general arrays or other sequence (or list) data types and structures.

Depending on programming language and precise data type used, a variable declared to be a string may either cause storage in memory to be statically allocated for a predetermined maximum length or employ dynamic allocation to allow it to hold a variable number of elements.

When a string appears literally in source code, it is known as a string literal or an anonymous string.

In formal languages, which are used in mathematical logic and theoretical computer science, a string is a finite sequence of symbols that are chosen from a set called an alphabet.

Let Σ be a non-empty finite set of symbols (alternatively called characters), called the alphabet. No assumption is made about the nature of the symbols. A string (or word) over Σ is any finite sequence of symbols from Σ. For example, if Σ = {0, 1}, then "01011" is a string over Σ.

The "length" of a string "s" is the number of symbols in "s" (the length of the sequence) and can be any non-negative integer; it is often denoted as |"s"|. The "empty string" is the unique string over Σ of length 0, and is denoted "ε" or "λ".

The set of all strings over Σ of length "n" is denoted Σ. For example, if Σ = {0, 1}, then Σ = {00, 01, 10, 11}. Note that Σ = {ε} for any alphabet Σ.

The set of all strings over Σ of any length is the Kleene closure of Σ and is denoted Σ. In terms of Σ,
For example, if Σ = {0, 1}, then Σ = {ε, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, ...}. Although the set Σ itself is countably infinite, each element of Σ is a string of finite length.

A set of strings over Σ (i.e. any subset of Σ) is called a "formal language" over Σ. For example, if Σ = {0, 1}, the set of strings with an even number of zeros, {ε, 1, 00, 11, 001, 010, 100, 111, 0000, 0011, 0101, 0110, 1001, 1010, 1100, 1111, ...}, is a formal language over Σ.

"Concatenation" is an important binary operation on Σ. For any two strings "s" and "t" in Σ, their concatenation is defined as the sequence of symbols in "s" followed by the sequence of characters in "t", and is denoted "st". For example, if Σ = {a, b, ..., z}, "s" = bear, and "t" = hug, then "st" = bearhug and "ts" = hugbear.

String concatenation is an associative, but non-commutative operation. The empty string ε serves as the identity element; for any string "s", ε"s" = "s"ε = "s". Therefore, the set Σ and the concatenation operation form a monoid, the free monoid generated by Σ. In addition, the length function defines a monoid homomorphism from Σ to the non-negative integers (that is, a function formula_2, such that formula_3).

A string "s" is said to be a "substring" or "factor" of "t" if there exist (possibly empty) strings "u" and "v" such that "t" = "usv". The relation "is a substring of" defines a partial order on Σ, the least element of which is the empty string.

A string "s" is said to be a prefix of "t" if there exists a string "u" such that "t" = "su". If "u" is nonempty, "s" is said to be a "proper" prefix of "t". Symmetrically, a string "s" is said to be a suffix of "t" if there exists a string "u" such that "t" = "us". If "u" is nonempty, "s" is said to be a "proper" suffix of "t". Suffixes and prefixes are substrings of "t". Both the relations "is a prefix of" and "is a suffix of" are prefix orders.

A string "s" = "uv" is said to be a rotation of "t" if "t" = "vu". For example, if Σ = {0, 1} the string 0011001 is a rotation of 0100110, where u = 00110 and v = 01.

The reverse of a string is a string with the same symbols but in reverse order. For example, if "s" = abc (where a, b, and c are symbols of the alphabet), then the reverse of "s" is cba. A string that is the reverse of itself (e.g., "s" = madam) is called a palindrome, which also includes the empty string and all strings of length 1.

It is often useful to define an ordering on a set of strings. If the alphabet Σ has a total order (cf. alphabetical order) one can define a total order on Σ called lexicographical order. For example, if Σ = {0, 1} and 0 < 1, then the lexicographical order on Σ includes the relationships ε < 0 < 00 < 000 < ... < 0001 < 001 < 01 < 010 < 011 < 0110 < 01111 < 1 < 10 < 100 < 101 < 111 < 1111 < 11111 ... The lexicographical order is total if the alphabetical order is, but isn't well-founded for any nontrivial alphabet, even if the alphabetical order is.

See Shortlex for an alternative string ordering that preserves well-foundedness.

A number of additional operations on strings commonly occur in the formal theory. These are given in the article on string operations.

Strings admit the following interpretation as nodes on a graph:

The natural topology on the set of fixed-length strings or variable-length strings is the discrete topology, but the natural topology on the set of infinite strings is the limit topology, viewing the set of infinite strings as the inverse limit of the sets of finite strings. This is the construction used for the "p"-adic numbers and some constructions of the Cantor set, and yields the same topology.

Isomorphisms between string representations of topologies can be found by normalizing according to the lexicographically minimal string rotation.

A string datatype is a datatype modeled on the idea of a formal string. Strings are such an important and useful datatype that they are implemented in nearly every programming language. In some languages they are available as primitive types and in others as composite types. The syntax of most high-level programming languages allows for a string, usually quoted in some way, to represent an instance of a string datatype; such a meta-string is called a "literal" or "string literal".

Although formal strings can have an arbitrary but finite length, the length of strings in real languages is often constrained to an artificial maximum. In general, there are two types of string datatypes: "fixed-length strings", which have a fixed maximum length to be determined at compile time and which use the same amount of memory whether this maximum is needed or not, and "variable-length strings", whose length is not arbitrarily fixed and which can use varying amounts of memory depending on the actual requirements at run time. Most strings in modern programming languages are variable-length strings. Of course, even variable-length strings are limited in length – by the number of bits available to a pointer, and by the size of available computer memory. The string length can be stored as a separate integer (which may put an artificial limit on the length) or implicitly through a termination character, usually a character value with all bits zero such as in C programming language. See also "Null-terminated" below.

String datatypes have historically allocated one byte per character, and, although the exact character set varied by region, character encodings were similar enough that programmers could often get away with ignoring this, since characters a program treated specially (such as period and space and comma) were in the same place in all the encodings a program would encounter. These character sets were typically based on ASCII or EBCDIC. If text in one encoding was displayed on a system using a different encoding, text was often mangled, though often somewhat readable and some computer users learned to read the mangled text.

Logographic languages such as Chinese, Japanese, and Korean (known collectively as CJK) need far more than 256 characters (the limit of a one 8-bit byte per-character encoding) for reasonable representation. The normal solutions involved keeping single-byte representations for ASCII and using two-byte representations for CJK ideographs. Use of these with existing code led to problems with matching and cutting of strings, the severity of which depended on how the character encoding was designed. Some encodings such as the EUC family guarantee that a byte value in the ASCII range will represent only that ASCII character, making the encoding safe for systems that use those characters as field separators. Other encodings such as ISO-2022 and Shift-JIS do not make such guarantees, making matching on byte codes unsafe. These encodings also were not "self-synchronizing", so that locating character boundaries required backing up to the start of a string, and pasting two strings together could result in corruption of the second string.

Unicode has simplified the picture somewhat. Most programming languages now have a datatype for Unicode strings. Unicode's preferred byte stream format UTF-8 is designed not to have the problems described above for older multibyte encodings. UTF-8, UTF-16 and UTF-32 require the programmer to know that the fixed-size code units are different than the "characters", the main difficulty currently is incorrectly designed APIs that attempt to hide this difference (UTF-32 does make "code points" fixed-sized, but these are not "characters" due to composing codes).

Some languages like C++ implement strings as templates that can be used with any datatype, but this is the exception, not the rule.

Some languages, such as C++ and Ruby, normally allow the contents of a string to be changed after it has been created; these are termed "mutable" strings. In other languages, such as Java and Python, the value is fixed and a new string must be created if any alteration is to be made; these are termed "immutable" strings.

Strings are typically implemented as arrays of bytes, characters, or code units, in order to allow fast access to individual units or substrings—including characters when they have a fixed length. A few languages such as Haskell implement them as linked lists instead.

Some languages, such as Prolog and Erlang, avoid implementing a dedicated string datatype at all, instead adopting the convention of representing strings as lists of character codes.

Representations of strings depend heavily on the choice of character repertoire and the method of character encoding. Older string implementations were designed to work with repertoire and encoding defined by ASCII, or more recent extensions like the ISO 8859 series. Modern implementations often use the extensive repertoire defined by Unicode along with a variety of complex encodings such as UTF-8 and UTF-16.

The term "byte string" usually indicates a general-purpose string of bytes, rather than strings of only (readable) characters, strings of bits, or such. Byte strings often imply that bytes can take any value and any data can be stored as-is, meaning that there should be no value interpreted as a termination value.

Most string implementations are very similar to variable-length arrays with the entries storing the character codes of corresponding characters. The principal difference is that, with certain encodings, a single logical character may take up more than one entry in the array. This happens for example with UTF-8, where single codes (UCS code points) can take anywhere from one to four bytes, and single characters can take an arbitrary number of codes. In these cases, the logical length of the string (number of characters) differs from the physical length of the array (number of bytes in use). UTF-32 avoids the first part of the problem.

The length of a string can be stored implicitly by using a special terminating character; often this is the null character (NUL), which has all bits zero, a convention used and perpetuated by the popular C programming language. Hence, this representation is commonly referred to as a C string. This representation of an "n"-character string takes "n" + 1 space (1 for the terminator), and is thus an implicit data structure.

In terminated strings, the terminating code is not an allowable character in any string. Strings with "length" field do not have this limitation and can also store arbitrary binary data.

An example of a "null-terminated string" stored in a 10-byte buffer, along with its ASCII (or more modern UTF-8) representation as 8-bit hexadecimal numbers is:

The length of the string in the above example, "codice_1", is 5 characters, but it occupies 6 bytes. Characters after the terminator do not form part of the representation; they may be either part of other data or just garbage. (Strings of this form are sometimes called "ASCIZ strings", after the original assembly language directive used to declare them.)

Using a special byte other than null for terminating strings has historically appeared in both hardware and software, though sometimes with a value that was also a printing character. codice_2 was used by many assembler systems, codice_3 used by CDC systems (this character had a value of zero), and the ZX80 used codice_4 since this was the string delimiter in its BASIC language.

Somewhat similar, "data processing" machines like the IBM 1401 used a special word mark bit to delimit strings at the left, where the operation would start at the right. This bit had to be clear in all other parts of the string. This meant that, while the IBM 1401 had a seven-bit word, almost no-one ever thought to use this as a feature, and override the assignment of the seventh bit to (for example) handle ASCII codes.

Early microcomputer software relied upon the fact that ASCII codes do not use the high-order bit, and set it to indicate the end of a string. It must be reset to 0 prior to output.

The length of a string can also be stored explicitly, for example by prefixing the string with the length as a byte value. This convention is used in many Pascal dialects; as a consequence, some people call such a string a Pascal string or P-string. Storing the string length as byte limits the maximum string length to 255. To avoid such limitations, improved implementations of P-strings use 16-, 32-, or 64-bit words to store the string length. When the "length" field covers the address space, strings are limited only by the available memory.

If the length is bounded, then it can be encoded in constant space, typically a machine word, thus leading to an implicit data structure, taking "n" + "k" space, where "k" is the number of characters in a word (8 for 8-bit ASCII on a 64-bit machine, 1 for 32-bit UTF-32/UCS-4 on a 32-bit machine, etc.).
If the length is not bounded, encoding a length "n" takes log("n") space (see fixed-length code), so length-prefixed strings are a succinct data structure, encoding a string of length "n" in log("n") + "n" space.

In the latter case, the length-prefix field itself doesn't have fixed length, therefore the actual string data needs to be moved when the string grows such that the length field needs to be increased.

Here is a Pascal string stored in a 10-byte buffer, along with its ASCII / UTF-8 representation:

Many languages, including object-oriented ones, implement strings as records with an internal structure like:

However, since the implementation is usually hidden, the string must be accessed and modified through member functions. codice_5 is a pointer to a dynamically allocated memory area, which might be expanded as needed. See also string (C++).

Both character termination and length codes limit strings: For example, C character arrays that contain null (NUL) characters cannot be handled directly by C string library functions: Strings using a length code are limited to the maximum value of the length code.

Both of these limitations can be overcome by clever programming.

It is possible to create data structures and functions that manipulate them that do not have the problems associated with character termination and can in principle overcome length code bounds. It is also possible to optimize the string represented using techniques from run length encoding (replacing repeated characters by the character value and a length) and Hamming encoding.

While these representations are common, others are possible. Using ropes makes certain string operations, such as insertions, deletions, and concatenations more efficient.

The core data structure in a text editor is the one that manages the string (sequence of characters) that represents the current state of the file being edited.
While that state could be stored in a single long consecutive array of characters, a typical text editor instead uses an alternative representation as its sequence data structure—a gap buffer, a linked list of lines, a piece table, or a rope—which makes certain string operations, such as insertions, deletions, and undoing previous edits, more efficient.

The differing memory layout and storage requirements of strings can affect the security of the program accessing the string data. String representations requiring a terminating character are commonly susceptible to buffer overflow problems if the terminating character is not present, caused by a coding error or an attacker deliberately altering the data. String representations adopting a separate length field are also susceptible if the length can be manipulated. In such cases, program code accessing the string data requires bounds checking to ensure that it does not inadvertently access or change data outside of the string memory limits.

String data is frequently obtained from user-input to a program. As such, it is the responsibility of the program to validate the string to ensure that it represents the expected format. Performing limited or no validation of user-input can cause a program to be vulnerable to code injection attacks.

Sometimes, strings need to be embedded inside a text file that is both human-readable and intended for consumption by a machine. This is needed in, for example, source code of programming languages, or in configuration files. In this case, the NUL character doesn't work well as a terminator since it is normally invisible (non-printable) and is difficult to input via a keyboard. Storing the string length would also be inconvenient as manual computation and tracking of the length is tedious and error-prone.

Two common representations are:

While character strings are very common uses of strings, a string in computer science may refer generically to any sequence of homogeneously typed data. A bit string or byte string, for example, may be used to represent non-textual binary data retrieved from a communications medium. This data may or may not be represented by a string-specific datatype, depending on the needs of the application, the desire of the programmer, and the capabilities of the programming language being used. If the programming language's string implementation is not 8-bit clean, data corruption may ensue.

C programmers draw a sharp distinction between a "string", aka a "string of characters", which by definition is always null terminated, vs. a "byte string" or "pseudo string" which may be stored in the same array but is often not null terminated.
Using C string handling functions on such a "byte string" often seems to work, but later leads to security problems.

There are many algorithms for processing strings, each with various trade-offs. Some categories of algorithms include:

Advanced string algorithms often employ complex mechanisms and data structures, among them suffix trees and finite state machines.

The name "stringology" was coined in 1984 by computer scientist Zvi Galil for the issue of algorithms and data structures used for string processing.

Character strings are such a useful datatype that several languages have been designed in order to make string processing applications easy to write. Examples include the following languages:

Many Unix utilities perform simple string manipulations and can be used to easily program some powerful string processing algorithms. Files and finite streams may be viewed as strings.

Some APIs like Multimedia Control Interface, embedded SQL or printf use strings to hold commands that will be interpreted.

Recent scripting programming languages, including Perl, Python, Ruby, and Tcl employ regular expressions to facilitate text operations. Perl is particularly noted for its regular expression use, and many other languages and applications implement Perl compatible regular expressions.

Some languages such as Perl and Ruby support string interpolation, which permits arbitrary expressions to be evaluated and included in string literals.

String functions are used to manipulate a string or change or edit the contents of a string. They also are used to query information about a string. They are usually used within the context of a computer programming language.

The most basic example of a string function is the string length function – the function that returns the length of a string (not counting any terminator characters or any of the string's internal structural information) and does not modify the string. This function is often named codice_6 or codice_7. For example, codice_8 would return 11.

In some programming languages, a string buffer is an alternative to a string. It has the ability to be altered through adding or appending, whereas a String is normally fixed or immutable.

Java's standard way to handle text is to use its codice_9 class. Any given codice_10 in Java is an immutable object, which means its state cannot be changed. A codice_10 has an array of characters. Whenever a codice_10 must be manipulated, any changes require the creation of a new codice_10 (which, in turn, involves the creation of a new array of characters, and copying of the original array). This happens even if the original codice_10's value or intermediate codice_10s used for the manipulation are not kept.

Java provides two alternate classes for string manipulation, called and . Both of these, like codice_10, each has an array to hold characters. They, however, are mutable (its state can be altered). Their array of characters is not necessarily completely filled (as opposed to a String, whose array is always the exact required length for its contents). Thus, a codice_17 or codice_18 has the capability to add, remove, or change its state without creating a new object (and without the creation of a new array, and array copying). The exception to this is when its array is no longer of suitable length to hold its content (a case which rarely happens because of the default Dynamic memory allocation provided by the JVM). In this case, it is required to create a new array, and copy the contents.

For these reasons, Java would handle an expression like

like this:
Generally, a codice_17 is more efficient than a String in string handling. However, this is not necessarily the case, since a StringBuffer will be required to recreate its character array when it runs out of space. Theoretically, this is possible to happen the same number of times as a new String would be required, although this is unlikely (and the programmer can provide length hints to prevent this). Either way, the effect is not noticeable in modern desktop computers.

As well, the shortcomings of arrays are inherent in a codice_17. In order to insert or remove characters at arbitrary positions, whole sections of arrays must be moved.

The method by which a codice_17 is attractive in an environment with low processing power takes this ability by using too much memory, which is likely also at a premium in this environment. This point, however, is trivial, considering the space required for creating many instances of Strings in order to process them. As well, the StringBuffer can be optimized to "waste" as little memory as possible.

The class, introduced in J2SE 5.0, differs from codice_17 in that it is unsynchronized. When only a single thread at a time will access the object, using a codice_18 processes more efficiently than using a codice_17.

codice_17 and codice_18 are included in the package.

Microsoft's .NET Framework has a codice_18 class in its Base Class Library.


Some microprocessor's instruction set architectures contain direct support for string operations, such as block copy (e.g. In intel x86m codice_30).



</doc>
<doc id="27706" url="https://en.wikipedia.org/wiki?curid=27706" title="Satanism">
Satanism

Satanism is a group of ideological and philosophical beliefs based on Satan. Contemporary religious practice of Satanism began with the founding of the Church of Satan in 1966, although a few historical precedents exist. Prior to the public practice, Satanism existed primarily as an accusation by various Christian groups toward perceived ideological opponents, rather than a self-identity. Satanism, and the concept of Satan, has also been used by artists and entertainers for symbolic expression.

Accusations that various groups have been practicing Satanism have been made throughout much of Christian history. During the Middle Ages, the Inquisition attached to the Roman Catholic Church alleged that various heretical Christian sects and groups, such as the Knights Templar and the Cathars, performed secret Satanic rituals. In the subsequent Early Modern period, belief in a widespread Satanic conspiracy of witches resulted in mass trials of alleged witches across Europe and the North American colonies. Accusations that Satanic conspiracies were active and that they were behind events such as Protestantism (and conversely, the Protestant claim that the Pope was the Antichrist) and the French Revolution continued to be made in Christendom during the eighteenth to the twentieth century. The idea of a vast Satanic conspiracy reached new heights with the influential Taxil hoax of France in the 1890s, which claimed that Freemasonry worshiped Satan, Lucifer, and Baphomet in their rituals. In the 1980s and 1990s, the Satanic ritual abuse hysteria spread through the United States and United Kingdom, amid fears that groups of Satanists were regularly sexually abusing and murdering children in their rites. In most of these cases, there is no corroborating evidence that any of those accused of Satanism were actually practitioners of a Satanic religion or guilty of the allegations levelled at them.

Since the 19th century, various small religious groups have emerged that identify as Satanists or use Satanic iconography. Satanist groups that appeared after the 1960s are widely diverse, but two major trends are theistic Satanism and atheistic Satanism. Theistic Satanists venerate Satan as a supernatural deity, viewing him not as omnipotent but rather as a patriarch. In contrast, atheistic Satanists regard Satan as merely a symbol of certain human traits.
Contemporary religious Satanism is predominantly an American phenomenon, the ideas spreading elsewhere with the effects of globalization and the Internet. The Internet spreads awareness of other Satanists, and is also the main battleground for Satanist disputes. Satanism started to reach Central and Eastern Europe in the 1990s, in time with the fall of the Soviet Union, and most noticeably in Poland and Lithuania, predominantly Roman Catholic countries.

In their study of Satanism, the religious studies scholars Asbjørn Dyrendal, James R. Lewis, and Jesper Aa. Petersen stated that the term "Satanism" "has a history of being a designation made by people against those whom they dislike; it is a term used for 'othering'". The concept of Satanism is an invention of Christianity, for it relies upon the figure of Satan, a character deriving from Christian mythology.

Elsewhere, Petersen noted that "Satanism as something others do is very different from Satanism as a self-designation".
Eugene Gallagher noted that, as commonly used, "Satanism" was usually "a polemical, not a descriptive term".

The word "Satan" was not originally a proper name but rather an ordinary noun meaning "the adversary"; in this context it appears at several points in the Old Testament. For instance, in the Book of Samuel, David is presented as the satan ("adversary") of the Philistines, while in the Book of Numbers the term appears as a verb, when God sent an angel to "satan" ("to oppose") Balaam. Prior to the composition of the New Testament, the idea developed within Jewish communities that Satan was the name of an angel who had rebelled against God and had been cast out of Heaven along with his followers; this account would be incorporated into contemporary texts like the Book of Enoch. This Satan was then featured in parts of the New Testament, where he was presented as a figure who tempted humans to commit sin; in the Book of Matthew and the Book of Luke, he attempted to tempt Jesus of Nazareth as the latter fasted in the wilderness.

The word "Satanism" was adopted into English from the French "satanisme". The terms "Satanism" and "Satanist" are first recorded as appearing in the English and French languages during the sixteenth century, when they were used by Christian groups to attack other, rival Christian groups. In a Roman Catholic tract of 1565, the author condemns the "heresies, blasphemies, and sathanismes [sic]" of the Protestants. In an Anglican work of 1559, Anabaptists and other Protestant sects are condemned as "swarmes of Satanistes [sic]". As used in this manner, the term "Satanism" was not used to claim that people literally worshipped Satan, but rather presented the view that through deviating from what the speaker or writer regarded as the true variant of Christianity, they were regarded as being essentially in league with the Devil. During the nineteenth century, the term "Satanism" began to be used to describe those considered to lead a broadly immoral lifestyle, and it was only in the late nineteenth century that it came to be applied in English to individuals who were believed to consciously and deliberately venerate Satan. This latter meaning had appeared earlier in the Swedish language; the Lutheran Bishop Laurentius Paulinus Gothus had described devil-worshipping sorcerers as "Sathanister" in his "Ethica Christiana", produced between 1615 and 1630.

Historical and anthropological research suggests that nearly all societies have developed the idea of a sinister and anti-human force that can hide itself within society. This commonly involves a belief in witches, a group of individuals who invert the norms of their society and seek to harm their community, for instance by engaging in incest, murder, and cannibalism. Allegations of witchcraft may have different causes and serve different functions within a society. For instance, they may serve to uphold social norms, to heighten the tension in existing conflicts between individuals, or to scapegoat certain individuals for various social problems. 

Another contributing factor to the idea of Satanism is the concept that there is an agent of misfortune and evil who operates on a cosmic scale, something usually associated with a strong form of ethical dualism that divides the world clearly into forces of good and forces of evil. The earliest such entity known is Angra Mainyu, a figure that appears in the Persian religion of Zoroastrianism. This concept was also embraced by Judaism and early Christianity, and although it was soon marginalised within Jewish thought, it gained increasing importance within early Christian understandings of the cosmos. While the early Christian idea of the Devil was not well developed, it gradually adapted and expanded through the creation of folklore, art, theological treatises, and morality tales, thus providing the character with a range of extra-Biblical associations.

As Christianity expanded throughout the Middle East, North Africa, and Europe, it came into contact with a variety of other religions, which it regarded as "pagan". Christian theologians claimed that the gods and goddesses venerated by these "pagans" were not genuine divinities, but were actually demons. However, they did not believe that "pagans" were deliberately devil-worshippers, instead claiming that they were simply misguided. In Christian iconography, the Devil and demons were given the physical traits of figures from Classical mythology such as the god Pan, fauns, and satyrs.

Those Christian groups regarded as heretics by the Roman Catholic Church were treated differently, with theologians arguing that they were deliberately worshipping the Devil. This was accompanied by claims that such individuals engaged in incestuous sexual orgies, murdered infants, and committed acts of cannibalism, all stock accusations that had previously been levelled at Christians themselves in the Roman Empire.
The first recorded example of such an accusation being made within Western Christianity took place in Toulouse in 1022, when two clerics were tried for allegedly venerating a demon. Throughout the middle ages, this accusation would be applied to a wide range of Christian heretical groups, including the Paulicians, Bogomils, Cathars, Waldensians, and the Hussites. The Knights Templar were accused of worshipping an idol known as Baphomet, with Lucifer having appeared at their meetings in the form of a cat. As well as these Christian groups, these claims were also made about Europe's Jewish community. In the thirteenth century, there were also references made to a group of "Luciferians" led by a woman named Lucardis which hoped to see Satan rule in Heaven. References to this group continued into the fourteenth century, although historians studying the allegations concur that these Luciferians were likely a fictitious invention.

Within Christian thought, the idea developed that certain individuals could make a pact with Satan. This may have emerged after observing that pacts with gods and goddesses played a role in various pre-Christian belief systems, or that such pacts were also made as part of the Christian cult of saints. Another possibility is that it derives from a misunderstanding of Augustine of Hippo's condemnation of augury in his "On the Christian Doctrine", written in the late fourth century. Here, he stated that people who consulted augurs were entering ""quasi pacts"" (covenants) with demons. The idea of the diabolical pact made with demons was popularised across Europe in the story of Faust, likely based in part on the real life Johann Georg Faust.

As the late medieval gave way to the early modern period, European Christendom experienced a schism between the established Roman Catholic Church and the breakaway Protestant movement. In the ensuing Reformation and Counter-Reformation, both Catholics and Protestants accused each other of deliberately being in league with Satan. It was in this context that the terms "Satanist" and "Satanism" emerged.

The early modern period also saw fear of Satanists reach its "historical apogee" in the form of the witch trials of the fifteenth to the eighteenth centuries. This came about as the accusations which had been levelled at medieval heretics, among them that of devil-worship, were applied to the pre-existing idea of the witch, or practitioner of malevolent magic. The idea of a conspiracy of Satanic witches was developed by educated elites, although the concept of malevolent witchcraft was a widespread part of popular belief and folkloric ideas about the night witch, the wild hunt, and the dance of the fairies were incorporated into it. The earliest trials took place in Northern Italy and France, before spreading it out to other areas of Europe and to Britain's North American colonies, being carried out by the legal authorities in both Catholic and Protestant regions.
Between 30,000 and 50,000 individuals were executed as accused Satanic witches.
Most historians agree that the majority of those persecuted in these witch trials were innocent of any involvement in Devil worship. However, in their summary of the evidence for the trials, the historians Geoffrey Scarre and John Callow thought it "without doubt" that some of those accused in the trials had been guilty of employing magic in an attempt to harm their enemies, and were thus genuinely guilty of witchcraft.

In seventeenth-century Sweden, a number of highway robbers and other outlaws living in the forests informed judges that they venerated Satan because he provided more practical assistance than God.
The historian of religion Massimo Introvigne regarded these practices as "folkloric Satanism".

During the eighteenth century, gentleman's social clubs became increasingly prominent in Britain and Ireland, among the most secretive of which were the Hellfire Clubs, which were first reported in the 1720s. The most famous of these groups was the Order of the Knights of Saints Francis, which was founded circa 1750 by the aristocrat Sir Francis Dashwood and which assembled first at his estate at West Wycombe and later in Medmenham Abbey. A number of contemporary press sources portrayed these as gatherings of atheist rakes where Christianity was mocked and toasts were made to the Devil. Beyond these sensationalist accounts, which may not be accurate portrayals of actual events, little is known about the activities of the Hellfire Clubs. Introvigne suggested that they may have engaged in a form of "playful Satanism" in which Satan was invoked "to show a daring contempt for conventional morality" by individuals who neither believed in his literal existence nor wanted to pay homage to him.

The French Revolution of 1789 dealt a blow to the hegemony of the Roman Catholic Church in parts of Europe, and soon a number of Catholic authors began making claims that it had been masterminded by a conspiratorial group of Satanists. Among the first to do so was French Catholic priest Jean-Baptiste Fiard, who publicly claimed that a wide range of individuals, from the Jacobins to tarot card readers, were part of a Satanic conspiracy. Fiard's ideas were furthered by Alexis-Vincent-Charles Berbiguier, who devoted a lengthy book to this conspiracy theory; he claimed that Satanists had supernatural powers allowing them to curse people and to shapeshift into both cats and fleas. Although most of his contemporaries regarded Berbiguier as mad, his ideas gained credence among many occultists, including Stanislas de Guaita, a Cabalist who used them for the basis of his book, "The Temple of Satan".

In the early 20th century, the British novelist Dennis Wheatley produced a range of influential novels in which his protagonists battled Satanic groups. At the same time, non-fiction authors like Montague Summers and Rollo Ahmed published books claiming that Satanic groups practicing black magic were still active across the world, although they provided no evidence that this was the case. During the 1950s, various British tabloid newspapers repeated such claims, largely basing their accounts on the allegations of one woman, Sarah Jackson, who claimed to have been a member of such a group. In 1973, the British Christian Doreen Irvine published "From Witchcraft to Christ", in which she claimed to have been a member of a Satanic group that gave her supernatural powers, such as the ability to levitate, before she escaped and embraced Christianity.
In the United States during the 1960s and 1970s, various Christian preachers—the most famous being Mike Warnke in his 1972 book "The Satan-Seller"—claimed that they had been members of Satanic groups who carried out sex rituals and animal sacrifices before discovering Christianity. According to Gareth Medway in his historical examination of Satanism, these stories were "a series of inventions by insecure people and hack writers, each one based on a previous story, exaggerated a little more each time".

Other publications made allegations of Satanism against historical figures. The 1970s saw the publication of the Romanian Protestant preacher Richard Wurmbrand's book in which he argued—without corroborating evidence—that the socio-political theorist Karl Marx had been a Satanist.

At the end of the twentieth century, a moral panic developed around claims regarding a Devil-worshipping cult that made use of sexual abuse, murder, and cannibalism in its rituals, with children being among its victims. Initially, the alleged perpetrators of such crimes were labelled "witches", although the term "Satanist" was soon adopted as a favoured alternative, and the phenomenon itself came to be called "the Satanism Scare". Promoters of the claims alleged that there was a conspiracy of organised Satanists who occupied prominent positions throughout society, from the police to politicians, and that they had been powerful enough to cover up their crimes.

One of the primary sources for the scare was "Michelle Remembers", a 1980 book by the Canadian psychiatrist Lawrence Pazder in which he detailed what he claimed were the repressed memories of his patient (and wife) Michelle Smith. Smith had claimed that as a child she had been abused by her family in Satanic rituals in which babies were sacrificed and Satan himself appeared. In 1983, allegations were made that the McMartin family—owners of a preschool in California—were guilty of sexually abusing the children in their care during Satanic rituals. The allegations resulted in a lengthy and expensive trial, in which all of the accused would eventually be cleared. The publicity generated by the case resulted in similar allegations being made in various other parts of the United States.

A prominent aspect of the Satanic Scare was the claim by those in the developing "anti-Satanism" movement that any child's claim about Satanic ritual abuse must be true, because children would not lie. Although some involved in the anti-Satanism movement were from Jewish and secular backgrounds, a central part was played by fundamentalist and evangelical forms of Christianity, in particular Pentecostalism, with Christian groups holding conferences and producing books and videotapes to promote belief in the conspiracy. Various figures in law enforcement also came to be promoters of the conspiracy theory, with such "cult cops" holding various conferences to promote it. The scare was later imported to the United Kingdom through visiting evangelicals and became popular among some of the country's social workers, resulting in a range of accusations and trials across Britain.

The Satanic ritual abuse hysteria died down between 1990 and 1994. In the late 1980s, the Satanic Scare had lost its impetus following increasing scepticism about such allegations, and a number of those who had been convicted of perpetrating Satanic ritual abuse saw their convictions overturned.
In 1990, an agent of the U.S. Federal Bureau of Investigation, Ken Lanning, revealed that he had investigated 300 allegations of Satanic ritual abuse and found no evidence for Satanism or ritualistic activity in any of them. In the UK, the Department of Health commissioned the anthropologist Jean La Fontaine to examine the allegations of SRA. She noted that while approximately half did reveal evidence of genuine sexual abuse of children, none revealed any evidence that Satanist groups had been involved or that any murders had taken place. She noted three examples in which lone individuals engaged in child molestation had created a ritual performance to facilitate their sexual acts, with the intent of frightening their victims and justifying their actions, but that none of these child molestors were involved in wider Satanist groups. By the 21st century, hysteria about Satanism has waned in most Western countries, although allegations of Satanic ritual abuse continued to surface in parts of continental Europe and Latin America.

From the late seventeenth through to the nineteenth century, the character of Satan was increasingly rendered unimportant in Western philosophy and ignored in Christian theology, while in folklore he came to be seen as a foolish rather than a menacing figure. The development of new values in the Age of Enlightenment—in particular those of reason and individualism—contributed to a shift in how many Europeans viewed Satan. In this context, a number of individuals took Satan out of the traditional Christian narrative and "reread and reinterpreted" him "in light of their own time and their own interests", in turn generating "new and different portraits of Satan".

The shifting view of Satan owes many of its origins to John Milton's epic poem "Paradise Lost" (1667), in which Satan features as the protagonist. Milton was a Puritan and had never intended for his depiction of Satan to be a sympathetic one. However, in portraying Satan as a victim of his own pride who rebelled against God he humanized him and also allowed him to be interpreted as a rebel against tyranny. This was how Milton's Satan was understood by later readers like the publisher Joseph Johnson, and the anarchist philosopher William Godwin, who reflected it in his 1793 book "Enquiry Concerning Political Justice". "Paradise Lost" gained a wide readership in the eighteenth century, both in Britain and in continental Europe, where it had been translated into French by Voltaire. Milton thus became "a central character in rewriting Satanism" and would be viewed by many later religious Satanists as a ""de facto" Satanist". 

The nineteenth century saw the emergence of what has been termed "literary Satanism" or "romantic Satanism". According to Van Luijk, this cannot be seen as a "coherent movement with a single voice, but rather as a "post factum" identified group of sometimes widely divergent authors among whom a similar theme is found". For the literary Satanists, Satan was depicted as a benevolent and sometimes heroic figure, with these more sympathetic portrayals proliferating in the art and poetry of many romanticist and decadent figures. For these individuals, Satanism was not a religious belief or ritual activity, but rather a "strategic use of a symbol and a character as part of artistic and political expression". 

Among the romanticist poets to adopt this view of Satan was the English poet Percy Bysshe Shelley, who had been influenced by Milton. In his poem "Laon and Cythna", Shelley praised the "Serpent", a reference to Satan, as a force for good in the universe.
Another was Shelley's fellow British poet Lord Byron, who included Satanic themes in his 1821 play "Cain", which was a dramatization of the Biblical story of Cain and Abel. These more positive portrayals also developed in France; one example was the 1823 work "Eloa" by Alfred de Vigny. Satan was also adopted by the French poet Victor Hugo, who made the character's fall from Heaven a central aspect of his "La Fin de Satan", in which he outlined his own cosmogony.
Although the likes of Shelley and Byron promoted a positive image of Satan in their work, there is no evidence that any of them performed religious rites to venerate him, and thus it is problematic to regard them as religious Satanists.

Radical left-wing political ideas had been spread by the American Revolution of 1765–83 and the French Revolution of 1789–99, and the figure of Satan, who was interpreted as having rebelled against the tyranny imposed by God, was an appealing one for many of the radical leftists of the period. For them, Satan was "a symbol for the struggle against tyranny, injustice, and oppression... a mythical figure of rebellion for an age of revolutions, a larger-than-life individual for an age of individualism, a free thinker in an age struggling for free thought". The French anarchist Pierre-Joseph Proudhon, who was a staunch critic of Christianity, embraced Satan as a symbol of liberty in several of his writings. Another prominent 19th century anarchist, the Russian Mikhail Bakunin, similarly described the figure of Satan as "the eternal rebel, the first freethinker and the emancipator of worlds" in his book "God and the State". These ideas likely inspired the American feminist activist Moses Harman to name his anarchist periodical "Lucifer the Lightbearer". The idea of this "Leftist Satan" declined during the twentieth century, although it was used on occasion by authorities within the Soviet Union, who portrayed Satan as a symbol of freedom and equality.

During the 1960s and 1970s, several rock bands—namely the American Coven and the British Black Widow—employed the imagery of Satanism and witchcraft in their work. References to Satan also appeared in the work of those rock bands which were pioneering the heavy metal genre in Britain during the 1970s. Black Sabbath for instance made mention of Satan in their lyrics, although several of the band's members were practicing Christians and other lyrics affirmed the power of the Christian God over Satan. In the 1980s, greater use of Satanic imagery was made by heavy metal bands like Slayer, Kreator, Sodom, and Destruction. Bands active in the subgenre of death metal—among them Deicide, Morbid Angel, and Entombed—also adopted Satanic imagery, combining it with other morbid and dark imagery, such as that of zombies and serial killers.

Satanism would come to be more closely associated with the subgenre of black metal, in which it was foregrounded over the other themes that had been used in death metal. A number of black metal performers incorporated self-injury into their act, framing this as a manifestation of Satanic devotion. The first black metal band, Venom, proclaimed themselves to be Satanists, although this was more an act of provocation than an expression of genuine devotion to the Devil. Satanic themes were also used by the black metal bands Bathory and Hellhammer. However, the first black metal act to more seriously adopt Satanism was Mercyful Fate, whose vocalist, King Diamond, joined the Church of Satan.
More often than not musicians associating themselves with black metal say they do not believe in legitimate Satanic ideology and often profess to being atheists, agnostics, or religious skeptics.

In contrast to King Diamond, various black metal Satanists sought to distance themselves from LaVeyan Satanism, for instance by referring to their beliefs as "devil worship". These individuals regarded Satan as a literal entity, and in contrast to LaVey's views, they associated Satanism with criminality, suicide, and terror. For them, Christianity was regarded as a plague which required eradication. Many of these individuals—such as Varg Vikernes and Euronymous—were Norwegian, and influenced by the strong anti-Christian views of this milieu, between 1992 and 1996 around fifty Norwegian churches were destroyed in arson attacks. Within the black metal scene, a number of musicians later replaced Satanic themes with those deriving from Heathenry, a form of modern Paganism.

Rather than being one single form of religious Satanism, there are instead multiple different religious Satanisms, each with different ideas about what being a Satanist entails. The historian of religion Ruben van Luijk used a "working definition" in which Satanism was regarded as "the intentional, religiously motivated veneration of Satan".

Dyrendal, Lewis, and Petersen believed that it was not a single movement, but rather a milieu. They and others have nevertheless referred to it as a new religious movement. They believed that there was a family resemblance that united all of the varying groups in this milieu, and that most of them were self religions. They argued that there were a set of features that were common to the groups in this Satanic milieu: these were the positive use of the term "Satanist" as a designation, an emphasis on individualism, a genealogy that connects them to other Satanic groups, a transgressive and antinomian stance, a self-perception as an elite, and an embrace of values such as pride, self-reliance, and productive non-conformity.

Dyrendal, Lewis, and Petersen argued that the groups within the Satanic milieu could be divided into three groups: reactive Satanists, rationalist Satanists, and esoteric Satanists. They saw reactive Satanism as encompassing "popular Satanism, inverted Christianity, and symbolic rebellion" and noted that it situates itself in opposition to society while at the same time conforming to society's perspective of evil. Rationalist Satanism is used to describe the trend in the Satanic milieu which is atheistic, sceptical, materialistic, and epicurean. Esoteric Satanism instead applied to those forms which are theistic and draw upon ideas from other forms of Western esotericism, Modern Paganism, Buddhism, and Hinduism.

The first person to promote a Satanic philosophy was the Pole Stanislaw Przybyszewski, who promoted a Social Darwinian ideology. 

The use of the term "Lucifer" was also taken up by the French ceremonial magician Eliphas Levi, who has been described as a "Romantic Satanist". During his younger days, Levi used "Lucifer" in much the same manner as the literary romantics. As he moved toward a more politically conservative outlook in later life, he retained the use of the term, but instead applied it as to what he believed was a morally neutral facet of the Absolute. In his book "Dogma and Ritual of High Magic", published in two volumes between 1854 and 1856, Levi offered the symbol of Baphomet. He claimed that this was a figure who had been worshipped by the Knights Templar.
According to Introvigne, this image gave "the Satanists their most popular symbol ever".

Levi was not the only occultist who wanted to use the term "Lucifer" without adopting the term "Satan" in a similar way. The early Theosophical Society held to the view that "Lucifer" was a force that aided humanity's awakening to its own spiritual nature. In keeping with this view, the Society began production of a journal titled "Lucifer".

"Satan" was also used within the esoteric system propounded by Danish occultist Carl William Hansen, who used the pen name "Ben Kadosh". Hansen was involved in a variety of esoteric groups, including Martinism, Freemasonry, and the Ordo Templi Orientis, drawing on ideas from various groups to establish his own philosophy. In one pamphlet, he provided a "Luciferian" interpretation of Freemasonry. Kadosh's work left little influence outside of Denmark.

Both during his life and after it, the British occultist Aleister Crowley has been widely described as a Satanist, usually by detractors. Crowley stated he did not consider himself a Satanist, nor did he worship Satan, as he did not accept the Christian world view in which Satan was believed to exist. He nevertheless used Satanic imagery, for instance by describing himself as "the Beast 666" and referring to the Whore of Babylon in his work, while in later life he sent "Antichristmas cards" to his friends. Dyrendel, Lewis, and Petersen noted that despite the fact that Crowley was not a Satanist, he "in many ways embodies the pre-Satanist esoteric discourse on Satan and Satanism through his lifestyle and his philosophy", with his "image and thought" becoming an "important influence" on the later development of religious Satanism.

In 1928 the Fraternitas Saturni (FS) was established in Germany; its founder, Eugen Grosche, published "Satanische Magie" ("Satanic Magic") that same year. The group connected Satan to Saturn, claiming that the planet related to the Sun in the same manner that Lucifer relates to the human world. 

In 1932 an esoteric group known as the Brotherhood of the Golden Arrow was established in Paris, France by Maria de Naglowska, a Russian occultist who had fled to France following the Russian Revolution. She promoted a theology centred on what she called the Third Term of the Trinity consisting of Father, Son, and Sex, the latter of which she deemed to be most important. Her early disciples, who underwent what she called "Satanic Initiations", included models and art students recruited from bohemian circles. The Golden Arrow disbanded after Naglowska abandoned it in 1936. According to Introvigne, hers was "a quite complicated Satanism, built on a complex philosophical vision of the world, of which little would survive its initiator".

In 1969 a Satanic group based in Toledo, Ohio, part of the United States, came to public attention. Called the Our Lady of Endor Coven, it was led by a man named Herbert Sloane, who described his Satanic tradition as the Ophite Cultus Sathanas and alleged that it had been established in the 1940s. The group offered a Gnostic interpretation of the world in which the creator God was regarded as evil and the Biblical Serpent presented as a force for good who had delivered salvation to humanity in the Garden of Eden. Sloane's claims that his group had a 1940s origin remain unproven; it may be that he falsely claimed older origins for his group to make it appear older than Anton LaVey's Church of Satan which had been established in 1966.

None of these groups had any real impact on the emergence of the later Satanic milieu in the 1960s.

Anton LaVey, who has been referred to as "The Father of Satanism", synthesized his religion through the establishment of the Church of Satan in 1966 and the publication of "The Satanic Bible" in 1969. LaVey's teachings promoted "indulgence", "vital existence", "undefiled wisdom", "kindness to those who deserve it", "responsibility to the responsible" and an "eye for an eye" code of ethics, while shunning "abstinence" based on guilt, "spirituality", "unconditional love", "pacifism", "equality", "herd mentality" and "scapegoating". In LaVey's view, the Satanist is a carnal, physical and pragmatic being, where enjoyment of physical existence and an undiluted view of this-worldly truth are promoted as the core values of Satanism, propagating a naturalistic worldview that sees mankind as animals existing in an amoral universe. 

LaVey believed that the ideal Satanist should be individualistic and non-conformist, rejecting what he called the "colorless existence" that mainstream society sought to impose on those living within it. He praised the human ego for encouraging an individual's pride, self-respect, and self-realization and accordingly believed in satisfying the ego's desires. He expressed the view that self-indulgence was a desirable trait, and that hate and aggression were not wrong or undesirable emotions but that they were necessary and advantageous for survival. Accordingly, he praised the seven deadly sins as virtues which were beneficial for the individual. The anthropologist Jean La Fontaine highlighted an article that appeared in "The Black Flame", in which one writer described "a true Satanic society" as one in which the population consists of "free-spirited, well-armed, fully-conscious, self-disciplined individuals, who will neither need nor tolerate any external entity 'protecting' them or telling them what they can and cannot do."

Sociologist James R. Lewis noted that "LaVey was directly responsible for the genesis of Satanism as a serious religious (as opposed to a purely literary) movement". Scholars agree that there is no reliably documented case of Satanic continuity prior to the founding of the Church of Satan. It was the first organized church in modern times to be devoted to the figure of Satan, and according to Faxneld and Petersen, the Church represented "the first public, highly visible, and long-lasting organization which propounded a coherent satanic discourse". LaVey's book, "The Satanic Bible", has been described as the most important document to influence contemporary Satanism. The book contains the core principles of Satanism, and is considered the foundation of its philosophy and dogma. Petersen noted that it is "in many ways "the" central text of the Satanic milieu", with Lap similarly testifying to its dominant position within the wider Satanic movement. David G. Bromley calls it "iconoclastic" and "the best-known and most influential statement of Satanic theology." Eugene V. Gallagher says that Satanists use LaVey's writings "as lenses through which they view themselves, their group, and the cosmos." He also states: "With a clear-eyed appreciation of true human nature, a love of ritual and pageantry, and a flair for mockery, LaVey's "Satanic Bible" promulgated a gospel of self-indulgence that, he argued, anyone who dispassionately considered the facts would embrace."

A number of religious studies scholars have described LaVey's Satanism as a form of "self-religion" or "self-spirituality", with religious studies scholar Amina Olander Lap arguing that it should be seen as being both part of the "prosperity wing" of the self-spirituality New Age movement and a form of the Human Potential Movement. The anthropologist Jean La Fontaine described it as having "both elitist and anarchist elements", also citing one occult bookshop owner who referred to the Church's approach as "anarchistic hedonism". In "The Invention of Satanism", Dyrendal and Petersen theorized that LaVey viewed his religion as "an antinomian self-religion for productive misfits, with a cynically carnivalesque take on life, and no supernaturalism". The sociologist of religion James R. Lewis even described LaVeyan Satanism as "a blend of Epicureanism and Ayn Rand's philosophy, flavored with a pinch of ritual magic." The historian of religion Mattias Gardell described LaVey's as "a rational ideology of egoistic hedonism and self-preservation", while Nevill Drury characterised LaVeyan Satanism as "a religion of self-indulgence". It has also been described as an "institutionalism of Machiavellian self-interest". 

Prominent Church leader Blanche Barton described Satanism as "an alignment, a lifestyle". LaVey and the Church espoused the view that "Satanists are born, not made"; that they are outsiders by their nature, living as they see fit, who are self-realized in a religion which appeals to the would-be Satanist's nature, leading them to realize they are Satanists through finding a belief system that is in line with their own perspective and lifestyle. Adherents to the philosophy have described Satanism as a non-spiritual religion of the flesh, or "...the world's first religion". LaVey used Christianity as a negative mirror for his new faith, with LaVeyan Satanism rejecting the basic principles and theology of Christian belief. It views Christianity – alongside other major religions, and philosophies such as humanism and liberal democracy – as a largely negative force on humanity; LaVeyan Satanists perceive Christianity as a lie which promotes idealism, self-denigration, herd behavior, and irrationality. LaVeyans view their religion as a force for redressing this balance by encouraging materialism, egoism, stratification, carnality, atheism, and social Darwinism. LaVey's Satanism was particularly critical of what it understands as Christianity's denial of humanity's animal nature, and it instead calls for the celebration of, and indulgence in, these desires. In doing so, it places an emphasis on the carnal rather than the spiritual.

Practitioners do not believe that Satan literally exists and do not worship him. Instead, Satan is viewed as a positive archetype embracing the Hebrew root of the word "Satan" as "adversary", who represents pride, carnality, and enlightenment, and of a cosmos which Satanists perceive to be motivated by a "dark evolutionary force of entropy that permeates all of nature and provides the drive for survival and propagation inherent in all living things". The Devil is embraced as a symbol of defiance against the Abrahamic faiths which LaVey criticized for what he saw as the suppression of humanity's natural instincts. Moreover, Satan also serves as a metaphorical external projection of the individual's godhood. LaVey espoused the view that "god" is a creation of man, rather than man being a creation of "god". In his book, "The Satanic Bible", the Satanist's view of god is described as the Satanist's true "self"—a projection of his or her own personality—not an external deity. Satan is used as a representation of personal liberty and individualism. LaVey explained that the gods worshiped by other religions are also projections of man's true self. He argues that man's unwillingness to accept his own ego has caused him to externalize these gods so as to avoid the feeling of narcissism that would accompany self-worship. The current High Priest of the Church of Satan, Peter H. Gilmore, further expounds that "...Satan is a symbol of Man living as his prideful, carnal nature dictates [...] Satan is not a conscious entity to be worshiped, rather a reservoir of power inside each human to be tapped at will. The Church of Satan has chosen Satan as its primary symbol because in Hebrew it means adversary, opposer, one to accuse or question. We see ourselves as being these Satans; the adversaries, opposers and accusers of all spiritual belief systems that would try to hamper enjoyment of our life as a human being." The term "Theistic Satanism" has been described as "oxymoronic" by the church and its High Priest. The Church of Satan rejects the legitimacy of any other organizations who claim to be Satanists, dubbing them reverse-Christians, pseudo-Satanists or Devil worshipers, atheistic or otherwise, and maintains a purist approach to Satanism as expounded by LaVey.

After LaVey's death in 1997, the Church of Satan was taken over by a new administration and its headquarters were moved to New York. LaVey's daughter, the High Priestess Karla LaVey, felt this to be a disservice to her father's legacy. The First Satanic Church was re-founded on October 31, 1999 by Karla LaVey to carry on the legacy of her father. She continues to run it out of San Francisco, California.

The Satanic Temple is an American religious and political activist organization based in Salem, Massachusetts. The organization actively participates in public affairs that have manifested in several public political actions and efforts at lobbying, with a focus on the separation of church and state and using satire against Christian groups that it believes interfere with personal freedom. According to Dyrendal, Lewis, and Petersen, the group were "rationalist, political pranksters". Their pranks are designed to highlight religious hypocrisy and advance the cause of secularism. In one of their actions, they performed a "Pink Mass" over the grave of the mother of the evangelical Christian and prominent anti-LGBT preacher Fred Phelps; the Temple claimed that the mass converted the spirit of Phelps' mother into a lesbian.

The Satanic Temple does not believe in a supernatural Satan, as they believe that this encourages superstition that will keep them from being "malleable to the best current scientific understandings of the material world". The Temple uses the literary Satan as metaphor to construct a cultural narrative which promotes pragmatic skepticism, rational reciprocity, personal autonomy, and curiosity. Satan is thus used as a symbol representing "the eternal rebel" against arbitrary authority and social norms.

Theistic Satanism (also known as traditional Satanism, Spiritual Satanism or Devil worship) is a form of Satanism with the primary belief that Satan is an actual deity or force to revere or worship. Other characteristics of theistic Satanism may include a belief in magic, which is manipulated through ritual, although that is not a defining criterion, and theistic Satanists may focus solely on devotion.

Luciferianism can be understood best as a belief system or intellectual creed that venerates the essential and inherent characteristics that are affixed and commonly given to Lucifer. Luciferianism is often identified as an auxiliary creed or movement of Satanism, due to the common identification of Lucifer with Satan. Some Luciferians accept this identification and/or consider Lucifer as the "light bearer" and illuminated aspect of Satan, giving them the name of Satanists and the right to bear the title. Others reject it, giving the argument that Lucifer is a more positive and easy-going ideal than Satan. They are inspired by the ancient myths of Egypt, Rome and Greece, Gnosticism and traditional Western occultism.

According to the group's own claims, the Order of Nine Angles was established in Shropshire, Western England during the late 1960s, when a Grand Mistress united a number of ancient pagan groups active in the area.
This account states that when the Order's Grand Mistress migrated to Australia, a man known as "Anton Long" took over as the new Grand Master. From 1976 onward he authored an array of texts for the tradition, codifying and extending its teachings, mythos, and structure.
Various academics have argued that Long is the pseudonym of British Neo-Nazi activist David Myatt, an allegation that Myatt has denied.
The ONA arose to public attention in the early 1980s, spreading its message through magazine articles over the following two decades. In 2000, it established a presence on the internet, later adopting social media to promote its message.

The ONA is a secretive organization, and lacks any central administration, instead operating as a network of allied Satanic practitioners, which it terms the "kollective". It consists largely of autonomous cells known as "nexions". The majority of these are located in Britain, Ireland, and Germany, although others are located elsewhere in Europe, and in Russia, Egypt, South Africa, Brazil, Australia, and the United States.

The ONA describe their occultism as "Traditional Satanism". The ONA's writings encourage human sacrifice, referring to their victims as "opfers". According to the Order's teachings, such opfers must demonstrate character faults that mark them out as being worthy of death, and accordingly the ONA insists that children must never be victims. No ONA cell have admitted to carrying out a sacrifice in a ritualised manner, but rather Order members have joined the police and military in order to carry out such killings. Faxneld described the Order as "a dangerous and extreme form of Satanism", while religious studies scholar Graham Harvey claimed that the ONA fit the stereotype of the Satanist "better than other groups" by embracing "deeply shocking" and illegal acts.

The Temple of Set is an initiatory occult society claiming to be the world's leading left-hand path religious organization. It was established in 1975 by Michael A. Aquino and certain members of the priesthood of the Church of Satan, who left because of administrative and philosophical disagreements. ToS deliberately self-differentiates from CoS in several ways, most significantly in theology and sociology. The philosophy of the Temple of Set may be summed up as "enlightened individualism"—enhancement and improvement of oneself by personal education, experiment and initiation. This process is necessarily different and distinctive for each individual. The members do not agree on whether Set is "real" or not, and they're not expected to.

The Temple presents the view that the name "Satan" was originally a corruption of the name "Set". The Temple teaches that Set is a real entity, the only real god in existence, with all others created by the human imagination. Set is described as having given humanity—through the means of non-natural evolution—the "Black Flame" or the "Gift of Set", a questioning intellect which sets the species apart from other animals. While Setians are expected to revere Set, they do not worship him. Central to Setian philosophy is the human individual, with self-deification presented as the ultimate goal.

In 2005 Petersen noted that academic estimates for the Temple's membership varied from between 300 and 500, and Granholm suggested that in 2007 the Temple contained circa 200 members.

Dyrendal, Lewis, and Petersen used the term "reactive Satanism" to describe one form of modern religious Satanism. They described this as an adolescent and anti-social means of rebelling in a Christian society, by which an individual transgresses cultural boundaries. They believed that there was two tendencies within reactive Satanism: one, "Satanic tourism", was characterised by the brief period of time in which an individual was involved, while the other, the "Satanic quest", was typified by a longer and deeper involvement.

The researcher Gareth Medway noted that in 1995 he encountered a British woman who stated that she had been a practicing Satanist during her teenage years. She had grown up in a small mining village, and had come to believe that she had psychic powers. After hearing about Satanism in some library books, she declared herself a Satanist and formulated a belief that Satan was the true god. After her teenage years she abandoned Satanism and became a chaos magickian.

Some reactive Satanist are teenagers or mentally disturbed individuals who have engaged in criminal activities. During the 1980s and 1990s, several groups of teenagers were apprehended after sacrificing animals and vandalising both churches and graveyards with Satanic imagery. Introvigne expressed the view that these incidents were "more a product of juvenile deviance and marginalization than Satanism". In a few cases the crimes of these reactive Satanists have included murder. In 1970, two separate groups of teenagers—one led by Stanley Baker in Big Sur and the other by Steven Hurd in Los Angeles—killed a total of three people and consumed parts of their corpses in what they later claimed were sacrifices devoted to Satan. In 1984, a U.S. group called the Knights of the Black Circle killed one of its own members, Gary Lauwers, over a disagreement regarding the group's illegal drug dealing; group members later related that Lauwers' death was a sacrifice to Satan.
The American serial killer Richard Ramirez for instance claimed that he was a Satanist; during his 1980s killing spree he left an inverted pentagram at the scene of each murder and at his trial called out "Hail Satan!".

Dyrendal, Lewis, and Petersen observed that from surveys of Satanists conducted in the early 21st century, it was clear that the Satanic milieu was "heavily dominated by young males". They nevertheless noted that census data from New Zealand suggested that there may be a growing proportion of women becoming Satanists. In comprising more men than women, Satanism differs from most other religious communities, including most new religious communities. Most Satanists came to their religion through reading, either online or books, rather than through being introduced to it through personal contacts. Many practitioners do not claim that they converted to Satanism, but rather state that they were born that way, and only later in life confirmed that Satanism served as an appropriate label for their pre-existing worldviews. Others have stated that they had experiences with supernatural phenomena that led them to embracing Satanism. A number reported feelings of anger at the hypocrisy of many practicing Christians and expressed the view that the monotheistic Gods of Christianity and other religions are unethical, citing issues such as the problem of evil. For some practitioners, Satanism gave a sense of hope, including for those who had been physically and sexually abused.

The surveys revealed that atheistic Satanists appeared to be in the majority, although the numbers of theistic Satanists appeared to grow over time. Beliefs in the afterlife varied, although the most popular afterlife views were reincarnation and the idea that consciousness survives bodily death. The surveys also demonstrated that most recorded Satanists practiced magic, although there were differing opinions as to whether magical acts operated according to etheric laws or whether the effect of magic was purely psychological. A number described performing cursing, in most cases as a form of vigilante justice.
Most practitioners conduct their religious observances in a solitary manner, and never or rarely meet fellow Satanists for rituals. Rather, the primary interaction that takes place between Satanists is online, on websites or via email.
From their survey data, Dyrendal, Lewis, and Petersen noted that the average length of involvement in the Satanic milieu was seven years. A Satanist's involvement in the movement tends to peak in the early twenties and drops off sharply in their thirties. A small proportion retain their allegiance to the religion into their elder years. When asked about their political views, the largest proportion of Satanists identified as apolitical or non-aligned, while only a small percentage identified as conservative despite the conservative views of prominent Satanists like LaVey and Marilyn Manson. A small minority of Satanists expressed support for the far right; conversely, over two-thirds expressed negative or extremely negative views about Nazism and neo-Nazism.

In 2004 it was claimed that Satanism was allowed in the Royal Navy of the British Armed Forces, despite opposition from Christians. In 2016, under a Freedom of Information request, the Navy Command Headquarters stated that "we do not recognise satanism as a formal religion, and will not grant facilities or make specific time available for individual worship".

In 2005, the Supreme Court of the United States debated in the case of Cutter v. Wilkinson over protecting minority religious rights of prison inmates after a lawsuit challenging the issue was filed to them. The court ruled that facilities that accept federal funds cannot deny prisoners accommodations that are necessary to engage in activities for the practice of their own religious beliefs.




</doc>
<doc id="27707" url="https://en.wikipedia.org/wiki?curid=27707" title="Socialist law">
Socialist law

Socialist law or Soviet law denotes a general type of legal system which has been used in socialist and formerly socialist states. It is based on the civil law system, with major modifications and additions from Marxist-Leninist ideology. There is controversy as to whether socialist law ever constituted a separate legal system or not. If so, prior to the end of the Cold War, "socialist law" would be ranked among the major legal systems of the world.

While civil law systems have traditionally put great pains in defining the notion of private property, how it may be acquired, transferred, or lost, socialist law systems provide for most property to be owned by the state or by agricultural co-operatives, and having special courts and laws for state enterprises.

Many scholars argue that socialist law was not a separate legal classification. Although the command economy approach of the communist states meant that most types of property could not be owned, the Soviet Union always had a civil code, courts that interpreted this civil code, and a civil law approach to legal reasoning (thus, both legal process and legal reasoning were largely analogous to the French or German civil code system). Legal systems in all socialist states preserved formal criteria of the Romano-Germanic civil law; for this reason, law theorists in post-socialist states usually consider the Socialist law as a particular case of the Romano-Germanic civil law. Cases of development of common law into Socialist law are unknown because of incompatibility of basic principles of these two systems (common law presumes influential rule-making role of courts while courts in socialist states play a dependent role).

Soviet law displayed many special characteristics that derived from the socialist nature of the Soviet state and reflected Marxist-Leninist ideology. Vladimir Lenin accepted the Marxist conception of the law and the state as instruments of coercion in the hands of the bourgeoisie and postulated the creation of popular, informal tribunals to administer revolutionary justice. One of the main theoreticians of Soviet socialist legality in this early phase was Pēteris Stučka.

Alongside this utopian trend was one more critical of the concept of "proletarian justice", represented by Evgeny Pashukanis. A dictatorial trend developed that advocated the use of law and legal institutions to suppress all opposition to the regime. This trend reached its zenith under Joseph Stalin with the ascendancy of Andrey Vyshinsky, when the administration of justice was carried out mainly by the security police in special tribunals.

During the de-Stalinization of the Nikita Khrushchev era, a new trend developed, based on socialist legality, that stressed the need to protect the procedural and statutory rights of citizens, while still calling for obedience to the state. New legal codes, introduced in 1960, were part of the effort to establish legal norms in administering laws. Although socialist legality remained in force after 1960, the dictatorial and utopian trends continued to influence the legal process. Persecution of political and religious dissenters continued, but at the same time there was a tendency to decriminalize lesser offenses by handing them over to people's courts and administrative agencies and dealing with them by education rather than by incarceration.
By late 1986, the Mikhail Gorbachev era was stressing anew the importance of individual rights in relation to the state and criticizing those who violated procedural law in implementing Soviet justice. This signaled a resurgence of socialist legality as the dominant trend. Socialist legality itself still lacked features associated with Western jurisprudence.

Socialist law is similar to common law or civil law but with a greatly increased public law sector and decreased private law sector.


A specific institution characteristic to Socialist law was the so-called burlaw court (or, verbally, "court of comrades", Russian товарищеский суд) which decided on minor offences.

Among the remaining communist governments, some (most notably the People's Republic of China) have added extensive modifications to their legal systems. In general, this is a result of their market-oriented economic changes. However, some communist influence can still be seen. For example, in Chinese real estate law there is no unified concept of real property; the state owns all land but often not the structures that sit on that land. A rather complex "ad hoc" system of use rights to land property has developed, and these use rights are the things being officially traded (rather than the property itself). In some cases (for example in the case of urban residential property), the system results in something that resembles real property transactions in other legal systems.

In other cases, the Chinese system results in something quite different. For example, it is a common misconception that reforms under Deng Xiaoping resulted in the privatization of agricultural land and a creation of a land tenure system similar to those found in Western countries. In actuality, the village committee owns the land and contracts the right to use this land to individual farmers who may use the land to make money from agriculture. Hence the rights that are normally unified in Western economies are split up between the individual farmer and the village committee.

This has a number of consequences. One of them is that, because the farmer does not have an absolute right to transfer the land, he cannot borrow against his use rights. On the other hand, there is some insurance against risk in the system, in that the farmer can return his land to the village committee if he wants to stop farming and start some other sort of business. Then, if this business does not work, he can get a new contract with the village committee and return to farming. The fact that the land is redistributable by the village committee also ensures that no one is left landless; this creates a form of social welfare.

There have been a number of proposals to reform this system and they have tended to be in the direction of fully privatizing rural land for the alleged purpose of increasing efficiency. These proposals have usually not received any significant support, largely because of the popularity of the current system among the farmers themselves. There is little risk that the village committee will attempt to impose a bad contract on the farmers, since this would reduce the amount of money the village committee receives. At the same time, the farmer has some flexibility to decide to leave farming for other ventures and to return at a later time.



</doc>
<doc id="27709" url="https://en.wikipedia.org/wiki?curid=27709" title="Semiconductor">
Semiconductor

A semiconductor material has an electrical conductivity value falling between that of a conductor – such as copper, gold etc. – and an insulator, such as glass. Their resistance decreases as their temperature increases, which is behaviour opposite to that of a metal. Their conducting properties may be altered in useful ways by the deliberate, controlled introduction of impurities ("doping") into the crystal structure. Where two differently-doped regions exist in the same crystal, a semiconductor junction is created. The behavior of charge carriers which include electrons, ions and electron holes at these junctions is the basis of diodes, transistors and all modern electronics.

Semiconductor devices can display a range of useful properties such as passing current more easily in one direction than the other, showing variable resistance, and sensitivity to light or heat. Because the electrical properties of a semiconductor material can be modified by doping, or by the application of electrical fields or light, devices made from semiconductors can be used for amplification, switching, and energy conversion.

The conductivity of silicon is increased by adding a small amount of pentavalent (antimony, phosphorus, or arsenic) or trivalent (boron, gallium, indium) atoms (~ part in 10). This process is known as doping and resulting semiconductors are known as doped or extrinsic semiconductors.

The modern understanding of the properties of a semiconductor relies on quantum physics to explain the movement of charge carriers in a crystal lattice. Doping greatly increases the number of charge carriers within the crystal. When a doped semiconductor contains mostly free holes it is called "p-type", and when it contains mostly free electrons it is known as "n-type". The semiconductor materials used in electronic devices are doped under precise conditions to control the concentration and regions of p- and n-type dopants. A single semiconductor crystal can have many p- and n-type regions; the p–n junctions between these regions are responsible for the useful electronic behavior.

Although some pure elements and many compounds display semiconductor properties, silicon, germanium, and compounds of gallium are the most widely used in electronic devices. Elements near the so-called "metalloid staircase", where the metalloids are located on the periodic table, are usually used as semiconductors.

Some of the properties of semiconductor materials were observed throughout the mid 19th and first decades of the 20th century. The first practical application of semiconductors in electronics was the 1904 development of the cat's-whisker detector, a primitive semiconductor diode widely used in early radio receivers. Developments in quantum physics in turn allowed the development of the transistor in 1947 and the integrated circuit in 1958.


A large number of elements and compounds have semiconducting properties, including:

Most common semiconducting materials are crystalline solids, but amorphous and liquid semiconductors are also known. These include hydrogenated amorphous silicon and mixtures of arsenic, selenium and tellurium in a variety of proportions. These compounds share with better known semiconductors the properties of intermediate conductivity and a rapid variation of conductivity with temperature, as well as occasional negative resistance. Such disordered materials lack the rigid crystalline structure of conventional semiconductors such as silicon. They are generally used in thin film structures, which do not require material of higher electronic quality, being relatively insensitive to impurities and radiation damage.

Almost all of today's electronic technology involves the use of semiconductors, with the most important aspect being the integrated circuit (IC), which are found in laptops, scanners, cell-phones, etc. Semiconductors for ICs are mass-produced. To create an ideal semiconducting material, chemical purity is paramount. Any small imperfection can have a drastic effect on how the semiconducting material behaves due to the scale at which the materials are used.

A high degree of crystalline perfection is also required, since faults in crystal structure (such as dislocations, twins, and stacking faults) interfere with the semiconducting properties of the material. Crystalline faults are a major cause of defective semiconductor devices. The larger the crystal, the more difficult it is to achieve the necessary perfection. Current mass production processes use crystal ingots between in diameter which are grown as cylinders and sliced into wafers.

There is a combination of processes that is used to prepare semiconducting materials for ICs. One process is called thermal oxidation, which forms silicon dioxide on the surface of the silicon. This is used as a gate insulator and field oxide. Other processes are called photomasks and photolithography. This process is what creates the patterns on the circuity in the integrated circuit. Ultraviolet light is used along with a photoresist layer to create a chemical change that generates the patterns for the circuit.

Etching is the next process that is required. The part of the silicon that was not covered by the photoresist layer from the previous step can now be etched. The main process typically used today is called plasma etching. Plasma etching usually involves an etch gas pumped in a low-pressure chamber to create plasma. A common etch gas is chlorofluorocarbon, or more commonly known Freon. A high radio-frequency voltage between the cathode and anode is what creates the plasma in the chamber. The silicon wafer is located on the cathode, which causes it to be hit by the positively charged ions that are released from the plasma. The end result is silicon that is etched anisotropically.

The last process is called diffusion. This is the process that gives the semiconducting material its desired semiconducting properties. It is also known as doping. The process introduces an impure atom to the system, which creates the p-n junction. In order to get the impure atoms embedded in the silicon wafer, the wafer is first put in a 1,100 degree Celsius chamber. The atoms are injected in and eventually diffuse with the silicon. After the process is completed and the silicon has reached room temperature, the doping process is done and the semiconducting material is ready to be used in an integrated circuit.

Semiconductors are defined by their unique electric conductive behavior, somewhere between that of a conductor and an insulator.
The differences between these materials can be understood in terms of the quantum states for electrons, each of which may contain zero or one electron (by the Pauli exclusion principle). These states are associated with the electronic band structure of the material.
Electrical conductivity arises due to the presence of electrons in states that are delocalized (extending through the material), however in order to transport electrons a state must be "partially filled", containing an electron only part of the time. If the state is always occupied with an electron, then it is inert, blocking the passage of other electrons via that state.
The energies of these quantum states are critical, since a state is partially filled only if its energy is near the Fermi level (see Fermi–Dirac statistics).

High conductivity in a material comes from it having many partially filled states and much state delocalization.
Metals are good electrical conductors and have many partially filled states with energies near their Fermi level.
Insulators, by contrast, have few partially filled states, their Fermi levels sit within band gaps with few energy states to occupy.
Importantly, an insulator can be made to conduct by increasing its temperature: heating provides energy to promote some electrons across the band gap, inducing partially filled states in both the band of states beneath the band gap (valence band) and the band of states above the band gap (conduction band).
An (intrinsic) semiconductor has a band gap that is smaller than that of an insulator and at room temperature significant numbers of electrons can be excited to cross the band gap.

A pure semiconductor, however, is not very useful, as it is neither a very good insulator nor a very good conductor.
However, one important feature of semiconductors (and some insulators, known as "semi-insulators") is that their conductivity can be increased and controlled by doping with impurities and gating with electric fields. Doping and gating move either the conduction or valence band much closer to the Fermi level, and greatly increase the number of partially filled states.

Some wider-band gap semiconductor materials are sometimes referred to as semi-insulators. When undoped, these have electrical conductivity nearer to that of electrical insulators, however they can be doped (making them as useful as semiconductors). Semi-insulators find niche applications in micro-electronics, such as substrates for HEMT. An example of a common semi-insulator is gallium arsenide. Some materials, such as titanium dioxide, can even be used as insulating materials for some applications, while being treated as wide-gap semiconductors for other applications.

The partial filling of the states at the bottom of the conduction band can be understood as adding electrons to that band.
The electrons do not stay indefinitely (due to the natural thermal recombination) but they can move around for some time.
The actual concentration of electrons is typically very dilute, and so (unlike in metals) it is possible to think of the electrons in the conduction band of a semiconductor as a sort of classical ideal gas, where the electrons fly around freely without being subject to the Pauli exclusion principle. In most semiconductors the conduction bands have a parabolic dispersion relation, and so these electrons respond to forces (electric field, magnetic field, etc.) much like they would in a vacuum, though with a different effective mass.
Because the electrons behave like an ideal gas, one may also think about conduction in very simplistic terms such as the Drude model, and introduce concepts such as electron mobility.

For partial filling at the top of the valence band, it is helpful to introduce the concept of an electron hole.
Although the electrons in the valence band are always moving around, a completely full valence band is inert, not conducting any current.
If an electron is taken out of the valence band, then the trajectory that the electron would normally have taken is now missing its charge.
For the purposes of electric current, this combination of the full valence band, minus the electron, can be converted into a picture of a completely empty band containing a positively charged particle that moves in the same way as the electron.
Combined with the "negative" effective mass of the electrons at the top of the valence band, we arrive at a picture of a positively charged particle that responds to electric and magnetic fields just as a normal positively charged particle would do in vacuum, again with some positive effective mass.
This particle is called a hole, and the collection of holes in the valence band can again be understood in simple classical terms (as with the electrons in the conduction band).

When ionizing radiation strikes a semiconductor, it may excite an electron out of its energy level and consequently leave a hole. This process is known as "electron–hole pair generation". Electron-hole pairs are constantly generated from thermal energy as well, in the absence of any external energy source.

Electron-hole pairs are also apt to recombine. Conservation of energy demands that these recombination events, in which an electron loses an amount of energy larger than the band gap, be accompanied by the emission of thermal energy (in the form of phonons) or radiation (in the form of photons).

In some states, the generation and recombination of electron–hole pairs are in equipoise. The number of electron-hole pairs in the steady state at a given temperature is determined by quantum statistical mechanics. The precise quantum mechanical mechanisms of generation and recombination are governed by conservation of energy and conservation of momentum.

As the probability that electrons and holes meet together is proportional to the product of their numbers, the product is in steady state nearly constant at a given temperature, providing that there is no significant electric field (which might "flush" carriers of both types, or move them from neighbour regions containing more of them to meet together) or externally driven pair generation. The product is a function of the temperature, as the probability of getting enough thermal energy to produce a pair increases with temperature, being approximately exp(−"E"/"kT"), where "k" is Boltzmann's constant, "T" is absolute temperature and "E" is band gap.

The probability of meeting is increased by carrier traps—impurities or dislocations which can trap an electron or hole and hold it until a pair is completed. Such carrier traps are sometimes purposely added to reduce the time needed to reach the steady state.

The conductivity of semiconductors may easily be modified by introducing impurities into their crystal lattice. The process of adding controlled impurities to a semiconductor is known as "doping". The amount of impurity, or dopant, added to an "intrinsic" (pure) semiconductor varies its level of conductivity. Doped semiconductors are referred to as "extrinsic". By adding impurity to the pure semiconductors, the electrical conductivity may be varied by factors of thousands or millions.

A 1 cm specimen of a metal or semiconductor has of the order of 10 atoms. In a metal, every atom donates at least one free electron for conduction, thus 1 cm of metal contains on the order of 10 free electrons, whereas a 1 cm sample of pure germanium at 20 °C contains about atoms, but only free electrons and holes. The addition of 0.001% of arsenic (an impurity) donates an extra 10 free electrons in the same volume and the electrical conductivity is increased by a factor of 10,000.

The materials chosen as suitable dopants depend on the atomic properties of both the dopant and the material to be doped. In general, dopants that produce the desired controlled changes are classified as either electron acceptors or donors. Semiconductors doped with "donor" impurities are called "n-type", while those doped with "acceptor" impurities are known as "p-type". The n and p type designations indicate which charge carrier acts as the material's majority carrier. The opposite carrier is called the minority carrier, which exists due to thermal excitation at a much lower concentration compared to the majority carrier.

For example, the pure semiconductor silicon has four valence electrons which bond each silicon atom to its neighbors. In silicon, the most common dopants are "group III" and "group V" elements. Group III elements all contain three valence electrons, causing them to function as acceptors when used to dope silicon. When an acceptor atom replaces a silicon atom in the crystal, a vacant state (an electron "hole") is created, which can move around the lattice and functions as a charge carrier. Group V elements have five valence electrons, which allows them to act as a donor; substitution of these atoms for silicon creates an extra free electron. Therefore, a silicon crystal doped with boron creates a p-type semiconductor whereas one doped with phosphorus results in an n-type material.

During manufacture, dopants can be diffused into the semiconductor body by contact with gaseous compounds of the desired element, or ion implantation can be used to accurately position the doped regions.

The history of the understanding of semiconductors begins with experiments on the electrical properties of materials. The properties of negative temperature coefficient of resistance, rectification, and light-sensitivity were observed starting in the early 19th century.

Thomas Johann Seebeck was the first to notice an effect due to semiconductors, in 1821. In 1833, Michael Faraday reported that the resistance of specimens of silver sulfide decreases when they are heated. This is contrary to the behavior of metallic substances such as copper. In 1839, Alexandre Edmond Becquerel reported observation of a voltage between a solid and a liquid electrolyte when struck by light, the photovoltaic effect. In 1873 Willoughby Smith observed that selenium resistors exhibit decreasing resistance when light falls on them. In 1874 Karl Ferdinand Braun observed conduction and rectification in metallic sulfides, although this effect had been discovered much earlier by Peter Munck af Rosenschold () writing for the Annalen der Physik und Chemie in 1835, and Arthur Schuster found that a copper oxide layer on wires has rectification properties that ceases when the wires are cleaned. William Grylls Adams and Richard Evans Day observed the photovoltaic effect in selenium in 1876.

A unified explanation of these phenomena required a theory of solid-state physics which developed greatly in the first half of the 20th Century. In 1878 Edwin Herbert Hall demonstrated the deflection of flowing charge carriers by an applied magnetic field, the Hall effect. The discovery of the electron by J.J. Thomson in 1897 prompted theories of electron-based conduction in solids. Karl Baedeker, by observing a Hall effect with the reverse sign to that in metals, theorized that copper iodide had positive charge carriers. Johan Koenigsberger classified solid materials as metals, insulators and "variable conductors" in 1914 although his student Josef Weiss already introduced the term "Halbleiter" (semiconductor in modern meaning) in PhD thesis in 1910. Felix Bloch published a theory of the movement of electrons through atomic lattices in 1928. In 1930, B. Gudden stated that conductivity in semiconductors was due to minor concentrations of impurities. By 1931, the band theory of conduction had been established by Alan Herries Wilson and the concept of band gaps had been developed. Walter H. Schottky and Nevill Francis Mott developed models of the potential barrier and of the characteristics of a metal-semiconductor junction. By 1938, Boris Davydov had developed a theory of the copper-oxide rectifier, identifying the effect of the p–n junction and the importance of minority carriers and surface states.

Agreement between theoretical predictions (based on developing quantum mechanics) and experimental results was sometimes poor. This was later explained by John Bardeen as due to the extreme "structure sensitive" behavior of semiconductors, whose properties change dramatically based on tiny amounts of impurities. Commercially pure materials of the 1920s containing varying proportions of trace contaminants produced differing experimental results. This spurred the development of improved material refining techniques, culminating in modern semiconductor refineries producing materials with parts-per-trillion purity.

Devices using semiconductors were at first constructed based on empirical knowledge, before semiconductor theory provided a guide to construction of more capable and reliable devices.

Alexander Graham Bell used the light-sensitive property of selenium to transmit sound over a beam of light in 1880. A working solar cell, of low efficiency, was constructed by Charles Fritts in 1883 using a metal plate coated with selenium and a thin layer of gold; the device became commercially useful in photographic light meters in the 1930s. Point-contact microwave detector rectifiers made of lead sulfide were used by Jagadish Chandra Bose in 1904; the cat's-whisker detector using natural galena or other materials became a common device in the development of radio. However, it was somewhat unpredictable in operation and required manual adjustment for best performance. In 1906 H.J. Round observed light emission when electric current passed through silicon carbide crystals, the principle behind the light-emitting diode. Oleg Losev observed similar light emission in 1922 but at the time the effect had no practical use. Power rectifiers, using copper oxide and selenium, were developed in the 1920s and became commercially important as an alternative to vacuum tube rectifiers.

In the years preceding World War II, infra-red detection and communications devices prompted research into lead-sulfide and lead-selenide materials. These devices were used for detecting ships and aircraft, for infrared rangefinders, and for voice communication systems. The point-contact crystal detector became vital for microwave radio systems, since available vacuum tube devices could not serve as detectors above about 4000 MHz; advanced radar systems relied on the fast response of crystal detectors. Considerable research and development of silicon materials occurred during the war to develop detectors of consistent quality.

Detector and power rectifiers could not amplify a signal. Many efforts were made to develop a solid-state amplifier and were successful in developing a device called the point contact transistor which could amplify 20db or more. In 1922 Oleg Losev developed two-terminal, negative resistance amplifiers for radio, and he perished in the Siege of Leningrad after successful completion. In 1926 Julius Edgar Lilienfeld patented a device resembling a modern field-effect transistor, but it was not practical. R. Hilsch and R. W. Pohl in 1938 demonstrated a solid-state amplifier using a structure resembling the control grid of a vacuum tube; although the device displayed power gain, it had a cut-off frequency of one cycle per second, too low for any practical applications, but an effective application of the available theory. At Bell Labs, William Shockley and A. Holden started investigating solid-state amplifiers in 1938. The first p–n junction in silicon was observed by Russell Ohl about 1941, when a specimen was found to be light-sensitive, with a sharp boundary between p-type impurity at one end and n-type at the other. A slice cut from the specimen at the p–n boundary developed a voltage when exposed to light.

In France, during the war, Herbert Mataré had observed amplification between adjacent point contacts on a germanium base. After the war, Mataré's group announced their "Transistron" amplifier only shortly after Bell Labs announced the "transistor".





</doc>
<doc id="27711" url="https://en.wikipedia.org/wiki?curid=27711" title="Starch">
Starch

Starch or amylum is a polymeric carbohydrate consisting of a large number of glucose units joined by glycosidic bonds. This polysaccharide is produced by most green plants as energy storage. It is the most common carbohydrate in human diets and is contained in large amounts in staple foods like potatoes, wheat, maize (corn), rice, and cassava.

Pure starch is a white, tasteless and odorless powder that is insoluble in cold water or alcohol. It consists of two types of molecules: the linear and helical amylose and the branched amylopectin. Depending on the plant, starch generally contains 20 to 25% amylose and 75 to 80% amylopectin by weight. Glycogen, the glucose store of animals, is a more highly branched version of amylopectin.

In industry, starch is converted into sugars, for example by malting, and fermented to produce ethanol in the manufacture of beer, whisky and biofuel. It is processed to produce many of the sugars used in processed foods. Mixing most starches in warm water produces a paste, such as wheatpaste, which can be used as a thickening, stiffening or gluing agent. The biggest industrial non-food use of starch is as an adhesive in the papermaking process. Starch can be applied to parts of some garments before ironing, to stiffen them.

The word "starch" is from a Germanic root with the meanings "strong, stiff, strengthen, stiffen". Modern German "Stärke" (starch) is related. The Greek term for starch, "amylon" (ἄμυλον), is also related. It provides the root amyl, which is used as a prefix in biochemistry for several 5-carbon compounds related to or derived from starch (e.g. amyl alcohol).

Starch grains from the rhizomes of "Typha" (cattails, bullrushes) as flour have been identified from grinding stones in Europe dating back to 30,000 years ago. Starch grains from sorghum were found on grind stones in caves in Ngalue, Mozambique dating up to 100,000 years ago.

Pure extracted wheat starch paste was used in Ancient Egypt possibly to glue papyrus. The extraction of starch is first described in the "Natural History" of Pliny the Elder around AD 77–79. Romans used it also in cosmetic creams, to powder the hair and to thicken sauces. Persians and Indians used it to make dishes similar to gothumai wheat halva. Rice starch as surface treatment of paper has been used in paper production in China since 700 CE.

In addition to starchy plants consumed directly, 66 million tonnes of starch were being produced per year worldwide by 2008. In the EU this was around 8.5 million tonnes, with around
40% being used for industrial applications and 60% for food uses, most of the latter as glucose syrups.

Most green plants use starch as their energy store.The extra glucose is changed into starch which is more complex than glucose(by plants). An exception is the family Asteraceae (asters, daisies and sunflowers), where starch is replaced by the fructan inulin. Inulin-like fructans are also present in grasses such as wheat, in onions and garlic, bananas, and asparagus.

In photosynthesis, plants use light energy to produce glucose from carbon dioxide. The glucose is used to generate the chemical energy required for general metabolism, to make organic compounds such as nucleic acids, lipids, proteins and structural polysaccharides such as cellulose, or is stored in the form of starch granules, in amyloplasts. Toward the end of the growing season, starch accumulates in twigs of trees near the buds. Fruit, seeds, rhizomes, and tubers store starch to prepare for the next growing season.

Glucose is soluble in water, hydrophilic, binds with water and then takes up much space and is osmotically active; glucose in the form of starch, on the other hand, is not soluble, therefore osmotically inactive and can be stored much more compactly.

Glucose molecules are bound in starch by the easily hydrolyzed alpha bonds. The same type of bond is found in the animal reserve polysaccharide glycogen. This is in contrast to many structural polysaccharides such as chitin, cellulose and peptidoglycan, which are bound by beta bonds and are much more resistant to hydrolysis.

Plants produce starch by first converting glucose 1-phosphate to ADP-glucose using the enzyme glucose-1-phosphate adenylyltransferase. This step requires energy in the form of ATP. The enzyme starch synthase then adds the ADP-glucose via a 1,4-alpha glycosidic bond to a growing chain of glucose residues, liberating ADP and creating amylose. The ADP-glucose is almost certainly added to the non-reducing end of the amylose polymer, as the UDP-glucose is added to the non-reducing end of glycogen during glycogen synthesis.

Starch branching enzyme introduces 1,6-alpha glycosidic bonds between the amylose chains, creating the branched amylopectin. The starch debranching enzyme isoamylase removes some of these branches. Several isoforms of these enzymes exist, leading to a highly complex synthesis process.

Glycogen and amylopectin have similar structure, but the former has about one branch point per ten 1,4-alpha bonds, compared to about one branch point per thirty 1,4-alpha bonds in amylopectin. Amylopectin is synthesized from ADP-glucose while mammals and fungi synthesize glycogen from UDP-glucose; for most cases, bacteria synthesize glycogen from ADP-glucose (analogous to starch).

In addition to starch synthesis in plants, starch can be synthesized from non-food starch mediated by an enzyme cocktail. In this cell-free biosystem, beta-1,4-glycosidic bond-linked cellulose is partially hydrolyzed to cellobiose. Cellobiose phosphorylase cleaves to glucose 1-phosphate and glucose; the other enzyme—potato alpha-glucan phosphorylase can add a glucose unit from glucose 1-phosphorylase to the non-reducing ends of starch. In it, phosphate is internally recycled. The other product, glucose, can be assimilated by a yeast. This cell-free bioprocessing does not need any costly chemical and energy input, can be conducted in aqueous solution, and does not have sugar losses.

Starch is synthesized in plant leaves during the day and stored as granules; it serves as an energy source at night. The insoluble, highly branched starch chains have to be phosphorylated in order to be accessible for degrading enzymes. The enzyme glucan, water dikinase (GWD) phosphorylates at the C-6 position of a glucose molecule, close to the chains 1,6-alpha branching bonds. A second enzyme, phosphoglucan, water dikinase (PWD) phosphorylates the glucose molecule at the C-3 position. A loss of these enzymes, for example a loss of the GWD, leads to a starch excess (sex) phenotype, and because starch cannot be phosphorylated, it accumulates in the plastids.

After the phosphorylation, the first degrading enzyme, beta-amylase (BAM) can attack the glucose chain at its non-reducing end. Maltose is released as the main product of starch degradation. If the glucose chain consists of three or fewer molecules, BAM cannot release maltose. A second enzyme, disproportionating enzyme-1 (DPE1), combines two maltotriose molecules. From this chain, a glucose molecule is released. Now, BAM can release another maltose molecule from the remaining chain. This cycle repeats until starch is degraded completely. If BAM comes close to the phosphorylated branching point of the glucose chain, it can no longer release maltose. In order for the phosphorylated chain to be degraded, the enzyme isoamylase (ISA) is required.

The products of starch degradation are predominantly maltose and smaller amounts of glucose. These molecules are exported from the plastid to the cytosol, maltose via the maltose transporter, which if mutated (MEX1-mutant) results in maltose accumulation in the plastid. Glucose is exported via the plastidic glucose translocator (pGlcT). These two sugars act as a precursor for sucrose synthesis. Sucrose can then be used in the oxidative pentose phosphate pathway in the mitochondria, to generate ATP at night.

While amylose was thought to be completely unbranched, it is now known that some of its molecules contain a few branch points.
Amylose is a much smaller molecule than amylopectin. About one quarter of the mass of starch granules in plants consist of amylose, although there are about 150 times more amylose than amylopectin molecules.

Starch molecules arrange themselves in the plant in semi-crystalline granules. Each plant species has a unique starch granular size: rice starch is relatively small (about 2 μm) while potato starches have larger granules (up to 100 μm).

Starch becomes soluble in water when heated. The granules swell and burst, the semi-crystalline structure is lost and the smaller amylose molecules start leaching out of the granule, forming a network that holds water and increasing the mixture's viscosity. This process is called starch gelatinization. During cooking, the starch becomes a paste and increases further in viscosity. During cooling or prolonged storage of the paste, the semi-crystalline structure partially recovers and the starch paste thickens, expelling water. This is mainly caused by retrogradation of the amylose. This process is responsible for the hardening of bread or staling, and for the water layer on top of a starch gel (syneresis).

Some cultivated plant varieties have pure amylopectin starch without amylose, known as "waxy starches". The most used is waxy maize, others are glutinous rice and waxy potato starch. Waxy starches have less retrogradation, resulting in a more stable paste. High amylose starch, amylomaize, is cultivated for the use of its gel strength and for use as a resistant starch (a starch that resists digestion) in food products.

Synthetic amylose made from cellulose has a well-controlled degree of polymerization. Therefore, it can be used as a potential drug deliver carrier.

Certain starches, when mixed with water, will produce a non-newtonian fluid sometimes nicknamed "oobleck".

The enzymes that break down or hydrolyze starch into the constituent sugars are known as amylases.

Alpha-amylases are found in plants and in animals. Human saliva is rich in amylase, and the pancreas also secretes the enzyme. Individuals from populations with a high-starch diet tend to have more amylase genes than those with low-starch diets;

Beta-amylase cuts starch into maltose units. This process is important in the digestion of starch and is also used in brewing, where amylase from the skin of seed grains is responsible for converting starch to maltose (Malting, Mashing).

Given a heat of combustion of glucose of whereas that of starch is per mole of glucose monomer, hydrolysis releases about per mole, or per gram of glucose product.

If starch is subjected to dry heat, it breaks down to form dextrins, also called "pyrodextrins" in this context. This break down process is known as dextrinization. (Pyro)dextrins are mainly yellow to brown in color and dextrinization is partially responsible for the browning of toasted bread.

A triiodide (I) solution formed by mixing iodine and iodide (usually from potassium iodide) is used to test for starch; a dark blue color indicates the presence of starch. The details of this reaction are not fully known, but recent scientific work using single crystal x-ray crystallography and comparative Raman spectroscopy suggests that the final starch-iodine structure is similar to an infinite polyiodide chain like one found in a pyrroloperylene-iodine complex. The strength of the resulting blue color depends on the amount of amylose present. Waxy starches with little or no amylose present will color red. Benedict's test and Fehling's test is also done to indicate the presence of starch.

Starch indicator solution consisting of water, starch and iodide is often used in redox titrations: in the presence of an oxidizing agent the solution turns blue, in the presence of reducing agent the blue color disappears because triiodide (I) ions break up into three iodide ions, disassembling the starch-iodine complex.
A 0.3% w/w solution is the standard concentration for a starch indicator. It is made by adding 3 grams of soluble starch to 1 liter of heated water; the solution is cooled before use (starch-iodine complex becomes unstable at temperatures above 35 °C).

Each species of plant has a unique type of starch granules in granular size, shape and crystallization pattern. Under the microscope, starch grains stained with iodine illuminated from behind with polarized light show a distinctive Maltese cross effect (also known as extinction cross and birefringence).

Starch is the most common carbohydrate in the human diet and is contained in many staple foods. The major sources of starch intake worldwide are the cereals (rice, wheat, and maize) and the root vegetables (potatoes and cassava). Many other starchy foods are grown, some only in specific climates, including acorns, arrowroot, arracacha, bananas, barley, breadfruit, buckwheat, canna, colacasia, katakuri, kudzu, malanga, millet, oats, oca, polynesian arrowroot, sago, sorghum, sweet potatoes, rye, taro, chestnuts, water chestnuts and yams, and many kinds of beans, such as favas, lentils, mung beans, peas, and chickpeas.

Widely used prepared foods containing starch are bread, pancakes, cereals, noodles, pasta, porridge and tortilla.

Digestive enzymes have problems digesting crystalline structures. Raw starch is digested poorly in the duodenum and small intestine, while bacterial degradation takes place mainly in the colon. When starch is cooked, the digestibility is increased.

Starch gelatinization during cake baking can be impaired by sugar competing for water, preventing gelatinization and improving texture.

Before the advent of processed foods, people consumed large amounts of uncooked and unprocessed starch-containing plants, which contained high amounts of resistant starch. Microbes within the large intestine fermented the starch, produced short-chain fatty acids, which are used as energy, and support the maintenance and growth of the microbes. More highly processed foods are more easily digested and release more glucose in the small intestine—less starch reaches the large intestine and more energy is absorbed by the body. It is thought that this shift in energy delivery (as a result of eating more processed foods) may be one of the contributing factors to the development of metabolic disorders of modern life, including obesity and diabetes.

The starch industry extracts and refines starches from seeds, roots and tubers, by wet grinding, washing, sieving and drying. Today, the main commercial refined starches are cornstarch, tapioca, arrowroot, and wheat, rice, and potato starches. To a lesser extent, sources of refined starch are sweet potato, sago and mung bean. To this day, starch is extracted from more than 50 types of plants.

Untreated starch requires heat to thicken or gelatinize. When a starch is pre-cooked, it can then be used to thicken instantly in cold water. This is referred to as a pregelatinized starch.

Starch can be hydrolyzed into simpler carbohydrates by acids, various enzymes, or a combination of the two. The resulting fragments are known as dextrins. The extent of conversion is typically quantified by dextrose equivalent (DE), which is roughly the fraction of the glycosidic bonds in starch that have been broken.

These starch sugars are by far the most common starch based food ingredient and are used as sweeteners in many drinks and foods. They include:


A modified starch is a starch that has been chemically modified to allow the starch to function properly under conditions frequently encountered during processing or storage, such as high heat, high shear, low pH, freeze/thaw and cooling.

The modified food starches are E coded according to the International Numbering System for Food Additives (INS):

INS 1400, 1401, 1402, 1403 and 1405 are in the EU food ingredients without an E-number. Typical modified starches for technical applications are cationic starches, hydroxyethyl starch and carboxymethylated starches.

As an additive for food processing, food starches are typically used as thickeners and stabilizers in foods such as puddings, custards, soups, sauces, gravies, pie fillings, and salad dressings, and to make noodles and pastas. Function as thickeners, extenders, emulsion stabilizers and are exceptional binders in processed meats.

Gummed sweets such as jelly beans and wine gums are not manufactured using a mold in the conventional sense. A tray is filled with native starch and leveled. A positive mold is then pressed into the starch leaving an impression of 1,000 or so jelly beans. The jelly mix is then poured into the impressions and put onto a stove to set. This method greatly reduces the number of molds that must be manufactured.

In the pharmaceutical industry, starch is also used as an excipient, as tablet disintegrant, and as binder.

Resistant starch is starch that escapes digestion in the small intestine of healthy individuals.
High amylose starch from corn has a higher gelatinization temperature than other types of starch and retains its resistant starch content through baking, mild extrusion and other food processing techniques. It is used as an insoluble dietary fiber in processed foods such as bread, pasta, cookies, crackers, pretzels and other low moisture foods. It is also utilized as a dietary supplement for its health benefits. Published studies have shown that resistant starch helps to improve insulin sensitivity, increases satiety and improves markers of colonic function.
It has been suggested that resistant starch contributes to the health benefits of intact whole grains.

Papermaking is the largest non-food application for starches globally, consuming millions of metric tons annually. In a typical sheet of copy paper for instance, the starch content may be as high as 8%. Both chemically modified and unmodified starches are used in papermaking. In the wet part of the papermaking process, generally called the "wet-end", the starches used are cationic and have a positive charge bound to the starch polymer. These starch derivatives associate with the anionic or negatively charged paper fibers / cellulose and inorganic fillers. Cationic starches together with other retention and internal sizing agents help to give the necessary strength properties to the paper web formed in the papermaking process (wet strength), and to provide strength to the final paper sheet (dry strength).

In the dry end of the papermaking process, the paper web is rewetted with a starch based solution. The process is called surface sizing. Starches used have been chemically, or enzymatically depolymerized at the paper mill or by the starch industry (oxidized starch). The size/starch solutions are applied to the paper web by means of various mechanical presses (size presses). Together with surface sizing agents the surface starches impart additional strength to the paper web and additionally provide water hold out or "size" for superior printing properties. Starch is also used in paper coatings as one of the binders for the coating formulations which include a mixture of pigments, binders and thickeners. Coated paper has improved smoothness, hardness, whiteness and gloss and thus improves printing characteristics.

Corrugated board adhesives are the next largest application of non-food starches globally. Starch glues are mostly based on unmodified native starches, plus some additive such as borax and caustic soda. Part of the starch is gelatinized to carry the slurry of uncooked starches and prevent sedimentation. This opaque glue is called a SteinHall adhesive. The glue is applied on tips of the fluting. The fluted paper is pressed to paper called liner. This is then dried under high heat, which causes the rest of the uncooked starch in glue to swell/gelatinize. This gelatinizing makes the glue a fast and strong adhesive for corrugated board production.

Clothing or laundry starch is a liquid prepared by mixing a vegetable starch in water (earlier preparations also had to be boiled), and is used in the laundering of clothes. Starch was widely used in Europe in the 16th and 17th centuries to stiffen the wide collars and ruffs of fine linen which surrounded the necks of the well-to-do. During the 19th and early 20th century it was stylish to stiffen the collars and sleeves of men's shirts and the ruffles of women's petticoats by applying starch to them as the clean clothes were being ironed. Starch gave clothing smooth, crisp edges, and had an additional practical purpose: dirt and sweat from a person's neck and wrists would stick to the starch rather than to the fibers of the clothing. The dirt would wash away along with the starch; after laundering, the starch would be reapplied. Today, starch is sold in aerosol cans for home use.

Another large non-food starch application is in the construction industry, where starch is used in the gypsum wall board manufacturing process. Chemically modified or unmodified starches are added to the stucco containing primarily gypsum. Top and bottom heavyweight sheets of paper are applied to the formulation, and the process is allowed to heat and cure to form the eventual rigid wall board. The starches act as a glue for the cured gypsum rock with the paper covering, and also provide rigidity to the board.

Starch is used in the manufacture of various adhesives or glues for book-binding, wallpaper adhesives, paper sack production, tube winding, gummed paper, envelope adhesives, school glues and bottle labeling. Starch derivatives, such as yellow dextrins, can be modified by addition of some chemicals to form a hard glue for paper work; some of those forms use borax or soda ash, which are mixed with the starch solution at to create a very good adhesive. Sodium silicate can be added to reinforce these formula.


The Occupational Safety and Health Administration (OSHA) has set the legal limit (Permissible exposure limit) for starch exposure in the workplace as 15 mg/m total exposure and 5 mg/m respiratory exposure over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a Recommended exposure limit (REL) of 10 mg/m total exposure and 5 mg/m respiratory exposure over an 8-hour workday.




</doc>
<doc id="27712" url="https://en.wikipedia.org/wiki?curid=27712" title="Sugar">
Sugar

Sugar is the generic name for sweet-tasting, soluble carbohydrates, many of which are used in food. There are various types of sugar derived from different sources. Simple sugars are called monosaccharides and include glucose (also known as dextrose), fructose, and galactose. The "table sugar" or "granulated sugar" most customarily used as food is sucrose, a disaccharide of glucose and fructose. Sugar is used in prepared foods (e.g., cookies and cakes) and is added to some foods and beverages (e.g., coffee and tea). In the body, sucrose is hydrolysed into the simple sugars fructose and glucose. Other disaccharides include maltose from malted grain, and lactose from milk. Longer chains of sugars are called oligosaccharides or polysaccharides. Some other chemical substances, such as glycerol and sugar alcohols may also have a sweet taste, but are not classified as sugars. Diet food substitutes for sugar include aspartame and sucralose, a chlorinated derivative of sucrose.

Sugars are found in the tissues of most plants and are present in sugarcane and sugar beet in sufficient concentrations for efficient commercial extraction. In 2017–18, the world production of sugar was 185 million tonnes. The average person consumes about of sugar each year (33.1 kg in developed countries), equivalent to over 260 food calories per person per day. Since the latter part of the twentieth century, it has been questioned whether a diet high in sugars, especially refined sugars, is good for human health. Over-consumption of sugar has been implicated in the occurrence of obesity, diabetes, cardiovascular disease, dementia, and tooth decay. Numerous studies have been undertaken to try to clarify the position, but with varying results, mainly because of the difficulty of finding populations for use as controls that do not consume or are largely free of any sugar consumption.

The etymology reflects the spread of the commodity. From Sanskrit ("śarkarā"), meaning "ground or candied sugar," originally "grit, gravel", came Persian "shakar", whence Arabic ("sukkar"), whence Medieval Latin "succarum", whence 12th-century French "sucre", whence the English word "sugar". Italian "zucchero", Spanish "azúcar", and Portuguese "açúcar" came directly from Arabic, the Spanish and Portuguese words retaining the Arabic definite article. The earliest Greek word attested is ("sákkʰaris").

The English word "jaggery", a coarse brown sugar made from date palm sap or sugarcane juice, has a similar etymological origin: Portuguese "jágara" from the Malayalam ("cakkarā"), which is itself from the Sanskrit ("śarkarā").

Sugar has been produced in the Indian subcontinent since ancient times and spread from there into modern-day Afghanistan through the Khyber Pass. It was not plentiful or cheap in early times and honey was more often used for sweetening in most parts of the world. Originally, people chewed raw sugarcane to extract its sweetness. Sugarcane was a native of tropical South Asia and Southeast Asia. Different species seem to have originated from different locations with "Saccharum barberi" originating in India and "S. edule" and "S. officinarum" coming from New Guinea. One of the earliest historical references to sugarcane is in Chinese manuscripts dating back to 8th century BCE that state that the use of sugarcane originated in India.

In the tradition of Indian medicine (āyurveda), the sugarcane is known by the name Ikṣu and the sugarcane juice is known as Phāṇita. Its varieties, synonyms and characterics are defined in nighaṇṭus such as the Bhāvaprakāśa (1.6.23, group of sugarcanes).

The Greek physician Pedanius Dioscorides in the 1st century CE described sugar in his medical treatise De Materia Medica, and Pliny the Elder, a 1st-century CE Roman, described sugar in his Natural History: "Sugar is made in Arabia as well, but Indian sugar is better. It is a kind of honey found in cane, white as gum, and it crunches between the teeth. It comes in lumps the size of a hazelnut. Sugar is used only for medical purposes."

Sugar was found in Europe by the 1st century CE. Sugar remained relatively unimportant until the Indians discovered methods of turning sugarcane juice into granulated crystals that were easier to store and to transport. Crystallized sugar was discovered by the time of the Imperial Guptas, around the 5th century CE. In the local Indian language, these crystals were called "khanda" (Devanagari: खण्ड, ), which is the source of the word "candy". Indian sailors, who carried clarified butter and sugar as supplies, introduced knowledge of sugar on the various trade routes they travelled. Buddhist monks, as they travelled around, brought sugar crystallization methods to China. During the reign of Harsha (r. 606–647) in North India, Indian envoys in Tang China taught methods of cultivating sugarcane after Emperor Taizong of Tang (r. 626–649) made known his interest in sugar. China then established its first sugarcane plantations in the seventh century. Chinese documents confirm at least two missions to India, initiated in 647 CE, to obtain technology for sugar refining. In South Asia, the Middle East and China, sugar became a staple of cooking and desserts.

Crusaders brought sugar home with them to Europe after their campaigns in the Holy Land, where they encountered caravans carrying "sweet salt". Early in the 12th century, Venice acquired some villages near Tyre and set up estates to produce sugar for export to Europe, where it supplemented honey, which had previously been the only available sweetener.
Crusade chronicler William of Tyre, writing in the late 12th century, described sugar as "very necessary for the use and health of mankind". In the 15th century, Venice was the chief sugar refining and distribution centre in Europe.

In August 1492, Christopher Columbus stopped at La Gomera in the Canary Islands, for wine and water, intending to stay only four days. He became romantically involved with the governor of the island, Beatriz de Bobadilla y Ossorio, and stayed a month. When he finally sailed, she gave him cuttings of sugarcane, which became the first to reach the New World. The first sugar cane harvest was conducted in Hispaniola in 1501, and many sugar mills had been constructed in Cuba and Jamaica by the 1520s. The Portuguese took sugar cane to Brazil. By 1540, there were 800 cane sugar mills in Santa Catarina Island and another 2,000 on the north coast of Brazil, Demarara, and Surinam.

Sugar was a luxury in Europe until the 18th century, when it became more widely available. It then became popular and by the 19th century, sugar came to be considered a necessity. This evolution of taste and demand for sugar as an essential food ingredient unleashed major economic and social changes. It drove, in part, colonization of tropical islands and nations where labor-intensive sugarcane plantations and sugar manufacturing could thrive. The demand for cheap labor to perform the hard work involved in its cultivation and processing increased the demand for the slave trade from Africa (in particular West Africa). After slavery was abolished, there was high demand for indentured laborers from South Asia (in particular India). Millions of slave and indentured laborers were brought into the Caribbean and the Americas, Indian Ocean colonies, southeast Asia, Pacific Islands, and East Africa and Natal. The modern ethnic mix of many nations that have been settled in the last two centuries has been influenced by the demand for sugar.

Sugar also led to some industrialization of areas where sugar cane was grown. For example, Lieutenant J. Paterson, of the Bengal establishment, persuaded the British Government that sugar cane could be cultivated in British India with many advantages and at less expense than in the West Indies; as a result, sugar factories were established in Bihar in eastern India. During the Napoleonic Wars, sugar beet production increased in continental Europe because of the difficulty of importing sugar when shipping was subject to blockade. By 1880, the sugar beet was the main source of sugar in Europe. It was cultivated in Lincolnshire and other parts of England, although the United Kingdom continued to import the main part of its sugar from its colonies.

Until the late nineteenth century, sugar was purchased in loaves, which had to be cut using implements called sugar nips. In later years, granulated sugar was more usually sold in bags. Sugar cubes were produced in the nineteenth century. The first inventor of a process to make sugar in cube form was the Moravian Jakub Kryštof Rad, director of a sugar company in Dačice. He began sugar cube production after being granted a five-year patent for the process on January 23, 1843. Henry Tate of Tate & Lyle was another early manufacturer of sugar cubes at his refineries in Liverpool and London. Tate purchased a patent for sugar cube manufacture from German Eugen Langen, who in 1872 had invented a different method of processing of sugar cubes.

Scientifically, "sugar" loosely refers to a number of carbohydrates, such as monosaccharides, disaccharides, or oligosaccharides. Monosaccharides are also called "simple sugars," the most important being glucose. Most monosaccharides have a formula that conforms to with n between 3 and 7 (deoxyribose being an exception). Glucose has the molecular formula . The names of typical sugars end with -"ose", as in "glucose" and "fructose". Sometimes such words may also refer to any types of carbohydrates soluble in water. The acyclic mono- and disaccharides contain either aldehyde groups or ketone groups. These carbon-oxygen double bonds (C=O) are the reactive centers. All saccharides with more than one ring in their structure result from two or more monosaccharides joined by glycosidic bonds with the resultant loss of a molecule of water () per bond.

Monosaccharides in a closed-chain form can form glycosidic bonds with other monosaccharides, creating disaccharides (such as sucrose) and polysaccharides (such as starch). Enzymes must hydrolyze or otherwise break these glycosidic bonds before such compounds become metabolized. After digestion and absorption the principal monosaccharides present in the blood and internal tissues include glucose, fructose, and galactose. Many pentoses and hexoses can form ring structures. In these closed-chain forms, the aldehyde or ketone group remains non-free, so many of the reactions typical of these groups cannot occur. Glucose in solution exists mostly in the ring form at equilibrium, with less than 0.1% of the molecules in the open-chain form.

Biopolymers of sugars are common in nature. Through photosynthesis, plants produce glyceraldehyde-3-phosphate (G3P), a phosphated 3-carbon sugar that is used by the cell to make monosaccharides such as glucose () or (as in cane and beet) sucrose (). Monosaccharides may be further converted into structural polysaccharides such as cellulose and pectin for cell wall construction or into energy reserves in the form of storage polysaccharides such as starch or inulin. Starch, consisting of two different polymers of glucose, is a readily degradable form of chemical energy stored by cells, and can be converted to other types of energy. Another polymer of glucose is cellulose, which is a linear chain composed of several hundred or thousand glucose units. It is used by plants as a structural component in their cell walls. Humans can digest cellulose only to a very limited extent, though ruminants can do so with the help of symbiotic bacteria in their gut. DNA and RNA are built up of the monosaccharides deoxyribose and ribose, respectively. Deoxyribose has the formula and ribose the formula .

Because sugars burn easily when exposed to flame, the handling of sugars risks dust explosion. The risk of explosion is higher when the sugar has been milled to superfine texture, such as for use in chewing gum. The 2008 Georgia sugar refinery explosion, which killed 14 people and injured 40, and destroyed most of the refinery, was caused by the ignition of sugar dust.

In its culinary use, exposing sugar to heat causes caramelization. As the process occurs, volatile chemicals such as diacetyl are released, producing the characteristic caramel flavor.

Fructose, galactose, and glucose are all simple sugars, monosaccharides, with the general formula CHO. They have five hydroxyl groups (−OH) and a carbonyl group (C=O) and are cyclic when dissolved in water. They each exist as several isomers with dextro- and laevo-rotatory forms that cause polarized light to diverge to the right or the left.

Lactose, maltose, and sucrose are all compound sugars, disaccharides, with the general formula CHO. They are formed by the combination of two monosaccharide molecules with the exclusion of a molecule of water.

The sugar contents of common fruits and vegetables are presented in Table 1.
The fructose to fructose plus glucose ratio is calculated by including the fructose and glucose coming from the sucrose.

Due to rising demand, sugar production in general increased some 14% over the period 2009 to 2018. The largest importers were China, Indonesia, and the United States.

In 2016, global production of sugar beets was 277 million tonnes, led by Russia with 19% of the world total (table). 

Sugar beet became a major source of sugar in the 19th century when methods for extracting the sugar became available. It is a biennial plant, a cultivated variety of "Beta vulgaris" in the family Amaranthaceae, the tuberous root of which contains a high proportion of sucrose. It is cultivated as a root crop in temperate regions with adequate rainfall and requires a fertile soil. The crop is harvested mechanically in the autumn and the crown of leaves and excess soil removed. The roots do not deteriorate rapidly and may be left in a clamp in the field for some weeks before being transported to the processing plant. Here the crop is washed and sliced and the sugar extracted by diffusion. Milk of lime is added to the raw juice and carbonatated in a number of stages in order to purify it. Water is evaporated by boiling the syrup under a vacuum. The syrup is then cooled and seeded with sugar crystals. The white sugar that crystallizes out can be separated in a centrifuge and dried. It requires no further refining.

Global production of sugarcane in 2016 was 1.9 billion tonnes, with Brazil producing 41% of the world total and India 18% (table). 

Sugarcane refers to any of several species, or their hybrids, of giant grasses in the genus "Saccharum" in the family Poaceae. They have been cultivated in tropical climates in South Asia and Southeast Asia since ancient times for the sucrose that is found in their stems. A great expansion in sugarcane production took place in the 18th century with the establishment of slave plantations in the Americas. The use of slavery meant that this was the first time that sugar became cheap enough for most people, who previously had to rely on honey to sweeten foods. It requires a frost-free climate with sufficient rainfall during the growing season to make full use of the plant's great growth potential. The crop is harvested mechanically or by hand, chopped into lengths and conveyed rapidly to the processing plant (commonly known as a sugar mill). Here, it is either milled and the juice extracted with water or extracted by diffusion. The juice is then clarified with lime and heated to destroy enzymes. The resulting thin syrup is concentrated in a series of evaporators, after which further water is removed by evaporation in vacuum containers. The resulting supersaturated solution is seeded with sugar crystals and the sugar crystallizes out and is separated from the fluid and dried. Molasses is a by-product of the process and the fiber from the stems, known as bagasse, is burned to provide energy for the sugar extraction process. The crystals of raw sugar have a sticky brown coating and either can be used as they are or can be bleached by sulfur dioxide or can be treated in a carbonatation process to produce a whiter product. About of irrigation water is needed for every one kilogram (2.2 pounds) of sugar produced.

Refined sugar is made from raw sugar that has undergone a refining process to remove the molasses. Raw sugar is sucrose which is extracted from sugarcane or sugar beet. While raw sugar can be consumed, the refining process removes unwanted tastes and results in refined sugar or white sugar.

The sugar may be transported in bulk to the country where it will be used and the refining process often takes place there. The first stage is known as affination and involves immersing the sugar crystals in a concentrated syrup that softens and removes the sticky brown coating without dissolving them. The crystals are then separated from the liquor and dissolved in water. The resulting syrup is treated either by a carbonatation or by a phosphatation process. Both involve the precipitation of a fine solid in the syrup and when this is filtered out, many of the impurities are removed at the same time. Removal of color is achieved by using either a granular activated carbon or an ion-exchange resin. The sugar syrup is concentrated by boiling and then cooled and seeded with sugar crystals, causing the sugar to crystallize out. The liquor is spun off in a centrifuge and the white crystals are dried in hot air and ready to be packaged or used. The surplus liquor is made into refiners' molasses.

The International Commission for Uniform Methods of Sugar Analysis sets standards for the measurement of the purity of refined sugar, known as ICUMSA numbers; lower numbers indicate a higher level of purity in the refined sugar.

Refined sugar is widely used for industrial needs for higher quality. Refined sugar is purer (ICUMSA below 300) than raw sugar (ICUMSA over 1,500). The level of purity associated with the colors of sugar, expressed by standard number ICUMSA, the smaller ICUMSA numbers indicate the higher purity of sugar.


In most parts of the world, sugar is an important part of the human diet, making food more palatable and providing food energy. After cereals and vegetable oils, sugar derived from sugarcane and beet provided more kilocalories per capita per day on average than other food groups. According to one source, per capita consumption of sugar in 2016 was highest in the United States, followed by Germany and the Netherlands.

Brown and white granulated sugar are 97% to nearly 100% carbohydrates, respectively, with less than 2% water, and no dietary fiber, protein or fat (table). Brown sugar contains a moderate amount of iron (15% of the Reference Daily Intake in a 100 gram amount, see table), but a typical serving of 4 grams (one teaspoon), would provide 15 calories and a negligible amount of iron or any other nutrient. Because brown sugar contains 5–10% molasses reintroduced during processing, its value to some consumers is a richer flavor than white sugar.

A 2003 WHO technical report provided evidence that high intake of sugary drinks (including fruit juice) increased the risk of obesity by adding to overall energy intake. The 'empty calories' argument states that a diet high in added sugar will reduce consumption of foods that contain essential nutrients.

By itself, sugar is not a factor causing obesity and metabolic syndrome, but rather – when over-consumed – is a component of unhealthy dietary behavior. Controlled trials showed that overconsumption of sugar-sweetened beverages increases body weight and body fat, and that replacement of sugar by artificial sweeteners reduces weight. Other studies showed correlation between refined sugar ("free sugar") consumption and the onset of diabetes, and negative correlation with the consumption of fiber.

From systematic reviews published in 2016, there is no evidence that sugar intake at normal levels increases the risk of cardiovascular diseases. Sugar, particularly fructose, does not have unique effects causing injury to the cardiovascular system, but rather excess total energy intake increases risk of cardiovascular and metabolic diseases.

Reviews published in 2014 and 2016 suggest that sugar addiction does not occur in humans.

Some studies report evidence of causality between refined sugar and hyperactivity. The 2003 WHO report suggests that inconclusive evidence for sugar as a cause of hyperactivity is expected when studies do not control for intake of free sugars versus unrefined sugars.

The 2003 WHO report stated that "Sugars are undoubtedly the most important dietary factor in the development of dental caries". For tooth decay, there is "convincing evidence from human intervention studies, epidemiological studies, animal studies and experimental studies, for an association between the amount and frequency of free sugars intake and dental caries" while other sugars (complex carbohydrate) consumption is normally associated with a lower rate of dental caries. Also, studies have shown that the consumption of sugar and starch have different impacts on oral health, with the ingestion of starchy foods and fresh fruit being associated with lower incidence of dental caries.

Claims have been made of a sugar–Alzheimer's disease connection, but there is inconclusive evidence that cognitive decline is related to dietary fructose or overall energy intake.

The World Health Organization recommends that both adults and children reduce the intake of free sugars to less than 10% of total energy intake. A reduction to below 5% of total energy intake brings additional health benefits, especially in what regards dental caries (cavities in the teeth). These recommendations were based on the totality of available evidence reviewed regarding the relationship between free sugars intake and body weight and dental caries. Free sugars include monosaccharides and disaccharides added to foods and beverages by the manufacturer, cook or consumer, and sugars naturally present in honey, syrups, fruit juices and fruit juice concentrates.

On May 20, 2016 the U.S. Food and Drug Administration announced changes to the Nutrition Facts panel displayed on all foods, to be effective by July 2018. New to the panel is a requirement to list "Added sugars" by weight and as a percent of Daily Value (DV). For vitamins and minerals the intent of DVs is to indicate how much should be consumed. For added sugars, the guidance is that 100% DV should not be exceeded. 100% DV is defined as 50 grams. For a person consuming 2000 calories a day, 50 grams, the amount to not exceed, is the same as 200 calories, and thus 10% of total calories – same guidance as the World Health Organization. To put this into context, most 12 ounce (335 mL) cans of soda contain 39 grams of sugar. In the United States, a recently published government survey on food consumption reported that for men and women ages 20 and older the average total sugar intakes – naturally occurring in foods and added – were, respectively, 125 and 99 g/day.

Various culinary sugars have different densities due to differences in particle size and inclusion of moisture.

Domino Sugar gives the following weight to volume conversions (in United States customary units):

The "Engineering Resources – Bulk Density Chart" published in "Powder and Bulk" gives different values for the bulk densities:
Manufacturers of sugary products, such as soft drinks and candy, and the Sugar Research Foundation have been accused of trying to influence consumers and medical associations in the 1960s and 1970s by creating doubt about the potential health hazards of sucrose overconsumption, while promoting saturated fat as the main dietary risk factor in cardiovascular diseases. In 2016, the criticism led to recommendations that diet policymakers emphasize the need for high-quality research that accounts for multiple biomarkers on development of cardiovascular diseases.




</doc>
<doc id="27715" url="https://en.wikipedia.org/wiki?curid=27715" title="Saint Louis">
Saint Louis

Saint Louis, Saint-Louis or St. Louis may refer to a number of things, the great majority named after Saint Louis IX (1214–1270), a King of France and Catholic saint, and including the American city of St. Louis.















</doc>
<doc id="27717" url="https://en.wikipedia.org/wiki?curid=27717" title="Salma Hayek">
Salma Hayek

Salma Hayek Pinault (born Hayek Jiménez; September 2, 1966), is a Mexican and American film actress, producer, and former model. She began her career in Mexico starring in the telenovela "Teresa" and starred in the film "El Callejón de los Milagros" ("Miracle Alley") for which she was nominated for an Ariel Award. In 1991 Hayek moved to Hollywood and came to prominence with roles in films such as "Desperado" (1995), "From Dusk till Dawn" (1996), "Dogma" (1999), and "Wild Wild West" (1999).

Her breakthrough role was in the 2002 film "Frida" as Mexican painter Frida Kahlo for which she was nominated in the category of Best Actress for an Academy Award, BAFTA Award, Screen Actors Guild Award, and Golden Globe Award. This movie received widespread attention and was a critical and commercial success. She won a Daytime Emmy Award for Outstanding Directing in a Children/Youth/Family Special in 2004 for "The Maldonado Miracle" and received an Emmy Award nomination for Outstanding Guest Actress in a Comedy Series in 2007 after guest-starring in the ABC television comedy-drama "Ugly Betty." She also guest-starred on the NBC comedy series "30 Rock" from 2009 to 2013.

Hayek's recent films include "Grown Ups" (2010), "Puss in Boots" (2011), "Grown Ups 2" (2013), "Tale of Tales" (2015) and "The Hitman's Bodyguard" (2017).

Hayek was born Salma Hayek Jiménez in Coatzacoalcos, Veracruz, Mexico. Her younger brother, Sami (born 1972), is a furniture designer. Her mother, Diana Jiménez Medina, is an opera singer and talent scout. Her father, Sami Hayek Domínguez, is an oil company executive and owner of an industrial-equipment firm, who once ran for mayor of Coatzacoalcos. Her father is Mexican of Lebanese descent, with his family being from the city of Baabdat, Lebanon, a city Salma and her father visited in 2015 to promote her movie "Kahlil Gibran's The Prophet". Her mother is Mexican of Spanish descent. 

In an interview in 2015 with "Un Nuevo Día" while visiting Madrid, Hayek described herself as fifty-percent Lebanese and fifty-percent Spanish, stating that her grandmother/maternal great-grandparents were from Spain. Raised in a wealthy, devout Roman Catholic family, she was sent to the Academy of the Sacred Heart in Grand Coteau, Louisiana, USA, at the age of twelve. In school, she was diagnosed with dyslexia. She attended university in Mexico City, where she studied International Relations at the Universidad Iberoamericana.

At the age of 23, Hayek landed the title role in "Teresa" (1989), a successful Mexican telenovela that made her a star in Mexico. In 1994, Hayek starred in the film "El Callejón de los Milagros" ("Miracle Alley"), which has won more awards than any other movie in the history of Mexican cinema. For her performance, Hayek was nominated for an Ariel Award.

Hayek moved to Los Angeles, California, in 1991 to study acting under Stella Adler. She had limited fluency in English, and dyslexia. Robert Rodriguez, and his producer and then-wife, Elizabeth Avellan, soon gave Hayek a starring role opposite Antonio Banderas in 1995's "Desperado". She followed her role in "Desperado" with a brief role as a vampire queen in "From Dusk till Dawn", in which she performed a table-top snake dance.

Hayek had a starring role opposite Matthew Perry in the 1997 romantic comedy "Fools Rush In". In 1999 she co-starred in Will Smith's big-budget "Wild Wild West", and played a supporting role in Kevin Smith's "Dogma". In 2000 Hayek had an uncredited acting part opposite Benicio del Toro in "Traffic". In 2003, she reprised her role from "Desperado" by appearing in "Once Upon a Time in Mexico", the final film of the "Mariachi Trilogy".

Around 2000, Hayek founded film production company Ventanarosa, through which she produces film and television projects. Her first feature as a producer was 1999's "El Coronel No Tiene Quien Le Escriba", Mexico's official selection for submission for Best Foreign Film at the Oscars.

"Frida", co-produced by Hayek, was released in 2002. Starring Hayek as Frida Kahlo, and Alfred Molina as her unfaithful husband, Diego Rivera, the film was directed by Julie Taymor and featured an entourage of stars in supporting and minor roles (Valeria Golino, Ashley Judd, Edward Norton, Geoffrey Rush) and cameos (Antonio Banderas). She earned a Best Actress Academy Award nomination for her performance.

"In the Time of the Butterflies" is a 2001 feature film based on the Julia Álvarez book of the same name, covering the lives of the Mirabal sisters. In the movie, Salma Hayek plays one of the sisters, Minerva, and Edward James Olmos plays the Dominican dictator Rafael Leónidas Trujillo whom the sisters opposed.

In 2003, Hayek produced and directed "The Maldonado Miracle", a Showtime movie based on the book of the same name, winning her a Daytime Emmy Award for Outstanding Directing in a Children/Youth/Family Special. In December 2005, she directed a music video for Prince, titled "Te Amo Corazon" ("I love you, sweetheart") that featured Mía Maestro.
Hayek was an executive producer of "Ugly Betty", a television series that aired around the world from 2006 to 2010. Hayek adapted the series for American television with Ben Silverman, who acquired the rights and scripts from the Colombian telenovela "Yo Soy Betty La Fea" in 2001. Originally intended as a half-hour sitcom for NBC in 2004, the project would later be picked up by ABC for the 2006–2007 season with Silvio Horta also producing. Hayek guest-starred on "Ugly Betty" as Sofia Reyes, a magazine editor. She also had a cameo playing an actress in the telenovela within the show. The show won a Golden Globe Award for Best Comedy Series in 2007. Hayek's performance as Sofia resulted in a nomination for Outstanding Guest Actress in a Comedy Series at the 59th Primetime Emmy Awards.

In April 2007, Hayek finalized negotiations with MGM to become the CEO of her own Latin-themed film production company, Ventanarosa. The following month, she signed a two-year deal with ABC for Ventanarosa to develop projects for the network.

Hayek played the wife of Adam Sandler's character in the buddy comedy "Grown Ups", which also co-starred Chris Rock and Kevin James. At his insistence, Hayek co-starred with Antonio Banderas in the "Shrek" spin-off film "Puss in Boots" as the voice of the character Kitty Softpaws, who serves as Puss's female counterpart and love interest. In 2012, Hayek directed Jada Pinkett Smith in the music video "Nada se compara." She reprised her role in "Grown Ups 2", which was released in July 2013.

Hayek has been a spokeswoman for Avon cosmetics since February 2004. She was a spokeswoman for Revlon in 1998. In 2001, she modeled for Chopard and was featured in 2006 Campari ads, photographed by Mario Testino. On April 3, 2009, she helped introduce La Doña, a watch by Cartier inspired by fellow Mexican actress María Félix.

Hayek has worked with the Procter & Gamble Company and UNICEF to promote the funding (through disposable diaper sales) of vaccines against maternal and neonatal tetanus. She is a global spokesperson for the Pampers/UNICEF "partnership" 1 Pack = 1 Vaccine to help raise awareness of the program. This "partnership" involves Procter & Gamble donating the cost of one tetanus vaccination (approximately 24 cents) for every pack of Pampers sold.

In 2008, Hayek co-founded Juice Generation's juice delivery program Cooler Cleanse. In 2017, she and Juice Generation founder Eric Helms launched the beauty subscription delivery service Blend It Yourself, based on Hayek's personal beauty elixirs. It supplies subscribers with prepared organic frozen smoothie and acai bowl ingredients, some of which can also be applied as face masks. She also wrote the foreword to Helms' 2014 book "The Juice Generation: 100 Recipes for Fresh Juices and Superfood Smoothies".

In 2011, Hayek launched her own line of cosmetics, skincare, and haircare products called Nuance by Salma Hayek, to be sold at CVS stores in North America. Hayek was also featured in a series of Spanish language commercials for Lincoln cars.

In spring 2006, the Blue Star Contemporary Art Center in San Antonio, Texas displayed sixteen portrait paintings by muralist George Yepes and filmmaker Robert Rodriguez of Hayek as Aztec goddess Itzpapalotl.

Hayek is a naturalized United States citizen. She studied at Ramtha's School of Enlightenment and is a practitioner of yoga. Hayek, who was raised Catholic, has said she is not very devout anymore and does not believe in the institution (Church), but still believes in Jesus Christ and God.

On March 9, 2007, Hayek confirmed her engagement to French billionaire and Kering CEO François-Henri Pinault as well as her pregnancy. She gave birth to daughter Valentina Paloma Pinault in September 2007 at Cedars-Sinai Medical Center in Los Angeles, California. They were married on Valentine's Day 2009 in Paris. On April 25, 2009, they had a second ceremony in Venice.

In July 2011, Hayek's husband was named in a paternity case. According to reports, Pinault is the father of supermodel Linda Evangelista's son, Augustin James, who was born in October 2006. He denied all allegations, although he later reached a settlement with Evangelista.

On December 13, 2017, Hayek published an op-ed in "The New York Times" stating that she had been harassed and abused by Harvey Weinstein during the production of "Frida".<ref name="Hayek_12/13/2017"></ref>

Hayek's charitable work includes increasing awareness on violence against women and discrimination against immigrants. On July 19, 2005, Hayek testified before the U.S. Senate Committee on the Judiciary supporting reauthorizing the Violence Against Women Act. In February 2006, she donated $25,000 to a Coatzacoalcos, Mexico, shelter for battered women and another $50,000 to Monterrey based anti-domestic violence groups. Hayek is a board member of V-Day, the charity founded by playwright Eve Ensler. Nonetheless, Hayek has stated that she is not a feminist. She later revised her stance on this, stating: "I am a feminist because a lot of amazing women have made me who I am today. (...) But – it should not be just because I am a woman".

Hayek also advocates breastfeeding. During a UNICEF fact-finding trip to Sierra Leone, she breastfed a hungry week-old baby whose mother could not produce milk. She said she did it to reduce the stigma associated with breastfeeding and to encourage infant nutrition.

In 2010 Hayek's humanitarian work earned her a nomination for the VH1 Do Something Awards. In 2013 Hayek launched with Beyoncé and Frida Giannini a Gucci campaign, "Chime for Change", that aims to spread female empowerment.

For International Women's Day 2014 Hayek was one of the artist signatories of Amnesty International's letter, to then British Prime Minister David Cameron, campaigning for women's rights in Afghanistan. Following her visit to Lebanon in 2015, she criticised the discrimination against women there.


In July 2007, "The Hollywood Reporter" ranked Hayek fourth in their inaugural Latino Power 50, a list of the most powerful members of the Hollywood Latino community. That same month, a poll found Hayek to be the "sexiest celebrity" out of a field of 3,000 celebrities (male and female); according to the poll, "65 percent of the U.S. population would use the term 'sexy' to describe her". In 2008, she was awarded the Women in Film Lucy Award in recognition of her excellence and innovation in her creative works that have enhanced the perception of women through the medium of television In December of that year, "Entertainment Weekly" ranked Hayek number 17 in their list of the "25 Smartest People in TV."




</doc>
<doc id="27718" url="https://en.wikipedia.org/wiki?curid=27718" title="Super Bowl">
Super Bowl

The Super Bowl is the annual championship game of the National Football League (NFL). The game is the culmination of a regular season that begins in the late summer of the previous calendar year. Normally, Roman numerals are used to identify each game, rather than the year in which it is held. For example, Super Bowl I was played on January 15, 1967, following the 1966 regular season. The sole exception to this naming convention tradition occurred with Super Bowl 50, which was played on February 7, 2016, following the 2015 regular season, and the following year, the nomenclature returned to Roman numerals for Super Bowl LI, following the 2016 regular season. The most recent Super Bowl was Super Bowl LII, on February 4, 2018, following the 2017 regular season.

The game was created as part of a merger agreement between the NFL and its then-rival league, the American Football League (AFL). It was agreed that the two leagues' champion teams would play in the AFL–NFL World Championship Game until the merger was to officially begin in 1970. After the merger, each league was redesignated as a "conference", and the game has since been played between the conference champions to determine the NFL's league champion. Currently, the National Football Conference (NFC) leads the league with 27 wins to 25 wins for the American Football Conference (AFC). The Pittsburgh Steelers have the most Super Bowl championship titles, with six. The New England Patriots have the most Super Bowl appearances, with ten. Charles Haley and Tom Brady both have five Super Bowl rings, which is the record for the most rings won by a single player.

The day on which the Super Bowl is played, now considered by some as an unofficial American national holiday, is called "Super Bowl Sunday". It is the second-largest day for U.S. food consumption, after Thanksgiving Day. In addition, the Super Bowl has frequently been the most-watched American television broadcast of the year; the seven most-watched broadcasts in U.S. television history are Super Bowls. In 2015, Super Bowl XLIX became the most-watched American television program in history with an average audience of 114.4 million viewers, the fifth time in six years the game had set a record, starting with the 2010 Super Bowl, which itself had taken over the number-one spot held for 27 years by the final episode of "M*A*S*H". The Super Bowl is also among the most-watched sporting events in the world, almost all audiences being North American, and is second to soccer's UEFA Champions League final as the most watched "annual" sporting event worldwide.

The NFL restricts the use of its "Super Bowl" trademark; it is frequently called the Big Game or other generic terms by non-sponsoring corporations. Because of the high viewership, commercial airtime during the Super Bowl broadcast is the most expensive of the year, leading to companies regularly developing their most expensive advertisements for this broadcast. As a result, watching and discussing the broadcast's commercials has become a significant aspect of the event. In addition, popular singers and musicians including Mariah Carey, Michael Jackson, Madonna, Prince, Beyoncé, Paul McCartney, The Rolling Stones, The Who, Whitney Houston, and Lady Gaga have performed during the event's pre-game and halftime ceremonies.

For four decades after its 1920 inception, the NFL successfully fended off several rival leagues. However, in 1960, it encountered its most serious competitor when the American Football League (AFL) was formed. The AFL vied heavily with the NFL for both players and fans, but by the middle of the decade, the strain of competition led to serious merger talks between the two leagues. Prior to the 1966 season, the NFL and AFL reached a merger agreement that was to take effect for the 1970 season. As part of the merger, the champions of the two leagues agreed to meet in a world championship game for professional American football until the merger was effected.

A bowl game is a post-season college football game. The original "bowl game" was the Rose Bowl Game in Pasadena, California, which was first played in 1902 as the "Tournament East-West football game" as part of the Pasadena Tournament of Roses and moved to the new Rose Bowl Stadium in 1923. The stadium got its name from the fact that the game played there was part of the Tournament of Roses and that it was shaped like a bowl, much like the Yale Bowl in New Haven, Connecticut; the Tournament of Roses football game itself eventually came to be known as the Rose Bowl Game. Exploiting the Rose Bowl Game's popularity, post-season college football contests were created for Miami (the Orange Bowl), New Orleans (the Sugar Bowl), and El Paso, Texas (the Sun Bowl) in 1935, and for Dallas (the Cotton Bowl) in 1937. By the time the first Super Bowl was played, the term "bowl" for any major American football game was well established.
Lamar Hunt, owner of the AFL's Kansas City Chiefs, first used the term "Super Bowl" to refer to the NFL-AFL championship game in the merger meetings. Hunt later said the name was likely in his head because his children had been playing with a Super Ball toy; a vintage example of the ball is on display at the Pro Football Hall of Fame in Canton, Ohio. In a July 25, 1966, letter to NFL commissioner Pete Rozelle, Hunt wrote, "I have kiddingly called it the 'Super Bowl,' which obviously can be improved upon."

The leagues' owners chose the name "AFL–NFL Championship Game", but in July 1966 the "Kansas City Star" quoted Hunt in discussing "the Super Bowl — that's my term for the championship game between the two leagues", and the media immediately began using the term. Although the league stated in 1967 that "not many people like it", asking for suggestions and considering alternatives such as "Merger Bowl" and "The Game", the Associated Press reported that "Super Bowl" "grew and grew and grew-until it reached the point that there was Super Week, Super Sunday, Super Teams, Super Players, ad infinitum". "Super Bowl" became official beginning with the third annual game. Roman numerals were first affixed for the fifth edition, in January 1971.
After the NFL's Green Bay Packers won the first two Super Bowls, some team owners feared for the future of the merger. At the time, many doubted the competitiveness of AFL teams compared with their NFL counterparts, though that perception changed when the AFL's New York Jets defeated the NFL's Baltimore Colts in Super Bowl III in Miami. One year later, the AFL's Kansas City Chiefs defeated the NFL's Minnesota Vikings 23–7 in Super Bowl IV in New Orleans, which was the final AFL-NFL World Championship Game played before the merger. Beginning with the 1970 season, the NFL realigned into two conferences; the former AFL teams plus three NFL teams (the Colts, Pittsburgh Steelers, and Cleveland Browns) would constitute the American Football Conference (AFC), while the remaining NFL clubs would form the National Football Conference (NFC). The champions of the two conferences would play each other in the Super Bowl.

The winning team receives the Vince Lombardi Trophy, named after the coach of the Green Bay Packers, who won the first two Super Bowl games and three of the five preceding NFL championships in 1961, 1962, and 1965. Following Lombardi's death in September 1970, the trophy was named the Vince Lombardi Trophy. The first trophy awarded under the new name was presented to the Baltimore Colts following their win in Super Bowl V in Miami.

The Super Bowl is currently played on the first Sunday in February. This is due to the current NFL schedule which consists of the opening weekend of the season being held immediately after Labor Day (the first Monday in September), the 17-week regular season (where teams each play 16 games and have one bye), the first three rounds of the playoffs, and the Super Bowl two weeks after the two Conference Championship Games. This schedule has been in effect since Super Bowl XXXVIII in February 2004. The date of the Super Bowl can thus be determined from the date of the preceding Labor Day. For example, Labor Day in 2015 occurred on September 7; therefore the next Super Bowl was scheduled exactly five months later on February 7, 2016.

Originally, the game took place in early to mid-January. For Super Bowl I there was only one round of playoffs: the pre-merger NFL and AFL Championship Games. The addition of two playoff rounds (first in 1967 and then in 1978), an increase in regular season games from 14 to 16 (1978), and the establishment of one bye-week per team (1990) have caused the Super Bowl to be played later. Partially offsetting these season-lengthening effects, simultaneous with the addition of two regular season games in 1978, the season was started earlier. Prior to 1978, the season started as late as September 21. Now, since Labor Day is always the first Monday of September, September 13 is the latest possible date for the first full Sunday set of games (Since 2002, the regular season has started with the Kickoff Game on the first Thursday after Labor Day). The earliest possible season start date is September 7.

The Pittsburgh Steelers have won six Super Bowls, the most of any team; the Dallas Cowboys, New England Patriots and San Francisco 49ers have five victories each, while the Green Bay Packers and New York Giants have four Super Bowl championships. Fourteen other NFL franchises have won at least one Super Bowl. Eight teams have appeared in Super Bowl games without a win. The Minnesota Vikings were the first team to have appeared a record four times without a win. The Buffalo Bills played in a record four Super Bowls in a row and lost every one. Four teams (the Cleveland Browns, Detroit Lions, Jacksonville Jaguars, and Houston Texans) have never appeared in a Super Bowl. The Browns and Lions both won NFL Championships prior to the creation of the Super Bowl, while the Jaguars (1995) and Texans (2002) are both recent NFL expansion teams. (Detroit, Houston, and Jacksonville, however, have hosted a Super Bowl, leaving the Browns the only team to date who has neither played in nor whose city has hosted the game.) The Minnesota Vikings won the last NFL Championship before the merger but lost to the AFL champion Kansas City Chiefs in Super Bowl IV.

The Green Bay Packers won the first two Super Bowls (Known as the AFL-NFL World Championship Game for these first two contests), defeating the Kansas City Chiefs and Oakland Raiders following the and seasons, respectively. The Packers were led by quarterback, Bart Starr, who was named the Most Valuable Player (MVP) for both games. These two championships, coupled with the Packers' NFL championships in , , and , amount to the most successful stretch in NFL History; five championships in seven years, and the only threepeat in NFL history (1965, 1966, and 1967).

In Super Bowl III, the AFL's New York Jets defeated the eighteen-point favorite Baltimore Colts of the NFL, 16–7. The Jets were led by quarterback Joe Namath, who had famously guaranteed a Jets win prior to the game, and former Colts head coach Weeb Ewbank, and their victory proved that the AFL was the NFL's competitive equal. This was reinforced the following year when the AFL's Kansas City Chiefs defeated the NFL's Minnesota Vikings 23–7 in Super Bowl IV.

After the AFL–NFL merger was completed in 1970, three franchises – the Dallas Cowboys, Miami Dolphins, and Pittsburgh Steelers – would go on to dominate the 1970s, winning a combined eight Super Bowls in the decade.

The Baltimore Colts, now a member of the AFC, would start the decade by defeating the Cowboys in Super Bowl V, a game which is notable as being the only Super Bowl to date in which a player from the losing team won the Super Bowl MVP (Cowboys' linebacker Chuck Howley). Beginning with this Super Bowl, all Super Bowls have served as the NFL's league championship game.
The Cowboys, coming back from a loss the previous season, won Super Bowl VI over the Dolphins. However, this would be the Dolphins' final loss in over a year, as the next year, the Dolphins would go 14–0 in the regular season and eventually win all of their playoff games, capped off with a 14–7 victory in Super Bowl VII, becoming the first and only team to finish an entire perfect regular and postseason. The Dolphins would repeat as league champions by winning Super Bowl VIII a year later.

In the late 1970s, the Steelers became the first NFL dynasty of the post-merger era by winning four Super Bowls (IX, X, XIII, and XIV) in six years. They were led by head coach Chuck Noll, the play of offensive stars Terry Bradshaw, Franco Harris, Lynn Swann, John Stallworth, and Mike Webster, and their dominant "Steel Curtain" defense, led by "Mean" Joe Greene, L. C. Greenwood, Ernie Holmes, Mel Blount, Jack Ham, and Jack Lambert. The coaches and administrators also were part of the dynasty's greatness as evidenced by the team's "final pieces" being part of the famous 1974 draft. The selections in that class have been considered the best by any pro franchise ever, as Pittsburgh selected four future Hall of Famers, the most for any team in any sport in a single draft. The Steelers were the first team to win three and then four Super Bowls and appeared in six AFC Championship Games during the decade, making the playoffs in eight straight seasons. Nine players and three coaches and administrators on the team have been inducted into the Pro Football Hall of Fame. Pittsburgh still remains the only team to win back-to-back Super Bowls twice and four Super Bowls in a six-year period.

The Steelers' dynasty was interrupted only by the Oakland Raiders' Super Bowl XI win and the Cowboys winning their second Super Bowl of the decade.

In the 1980s and 1990s, the tables turned for the AFC, as the NFC dominated the Super Bowls of the new decade and most of those in the 1990s. The NFC won 16 of the 20 Super Bowls during these two decades, including 13 straight from Super Bowl XIX to Super Bowl XXXI. The NFC's winning streak was only interrupted when the Los Angeles Raiders routed the Washington Redskins, 38-9 in Super Bowl XVIII.
The most successful team of the 1980s was the San Francisco 49ers, which featured the West Coast offense of Hall of Fame head coach Bill Walsh. This offense was led by three-time Super Bowl MVP and Hall of Fame quarterback Joe Montana, Super Bowl MVP and Hall of Fame wide receiver Jerry Rice, running back Roger Craig, and defensive safety/cornerback Ronnie Lott. Under their leadership, the 49ers won four Super Bowls in the decade (XVI, XIX, XXIII, and XXIV) and made nine playoff appearances between 1981 and 1990, including eight division championships, becoming the second dynasty of the post-merger NFL.

The 1980s also produced the 1985 Chicago Bears, who posted an 18–1 record under head coach Mike Ditka; quarterback Jim McMahon; and Hall of Fame running back Walter Payton. Their team won Super Bowl XX in dominant fashion. The Washington Redskins and New York Giants were also top teams of this period; the Redskins won Super Bowls XVII, XXII, and XXVI. The Giants claimed Super Bowls XXI and XXV. As in the 1970s, the Oakland Raiders were the only team to interrupt the Super Bowl dominance of other teams; they won Super Bowls XV and XVIII (the latter as the Los Angeles Raiders).

Following several seasons with poor records in the 1980s, the Dallas Cowboys rose back to prominence in the 1990s. During this decade, the Cowboys made post-season appearances every year except for the seasons of 1990 and 1997. From 1992 to 1996, the Cowboys won their division championship each year. In this same period, the Buffalo Bills had made their mark reaching the Super Bowl for a record four consecutive years, only to lose all four. After Super Bowl championships by division rivals New York (1990) and Washington (1991), the Cowboys won three of the next four Super Bowls (XXVII, XXVIII, and XXX) led by quarterback Troy Aikman, running back Emmitt Smith, and wide receiver Michael Irvin. All three of these players went to the Hall of Fame. The Cowboys' streak was interrupted by the 49ers, who won their league-leading fifth title overall with Super Bowl XXIX in dominating fashion under Super Bowl MVP and Hall of Fame quarterback Steve Young, Hall of Fame wide receiver Jerry Rice, and Hall of Fame cornerback Deion Sanders; however, the Cowboys' victory in Super Bowl XXX the next year also gave them five titles overall and they did so with Sanders after he won the Super Bowl the previous year with the 49ers. The NFC's winning streak was continued by the Green Bay Packers who, under Hall of Fame quarterback Brett Favre, won Super Bowl XXXI, their first championship since Super Bowl II in the late 1960s.

Super Bowl XXXII saw quarterback John Elway and running back Terrell Davis lead the Denver Broncos to an upset victory over the defending champion Packers, snapping the NFC's 13-year winning streak. The following year, the Broncos defeated the Atlanta Falcons in Super Bowl XXXIII, Elway's fifth Super Bowl appearance, his second NFL championship, and his final NFL game. The back-to-back victories heralded a change in momentum in which AFC teams would win nine out of 12 Super Bowls. In the years between 1995 and 2016, five teams – the Steelers, New England Patriots, Broncos, Baltimore Ravens, and Indianapolis Colts – accounted for 20 of the 22 AFC Super Bowl appearances (including the last 14), with those same teams often meeting each other earlier in the playoffs. In contrast, the NFC saw a different representative in the Super Bowl every season from 2001 through 2010.

The year following the Broncos' second victory, however, a surprising St. Louis Rams team led by the undrafted quarterback, Kurt Warner, who would close out the 1990s in a wild battle against the Tennessee Titans in Super Bowl XXXIV. The tense game came down to the final play in which Tennessee had the opportunity to tie the game and send it to overtime. The Titans nearly pulled it off, but the tackle of receiver Kevin Dyson by linebacker Mike Jones kept the ball out of the end zone by a matter of inches. In 2007, ESPN would rank "The Tackle" as the 2nd greatest moment in Super Bowl history.

Super Bowl XXXV was played by the AFC's Baltimore Ravens and the NFC's New York Giants. The Ravens defeated the Giants by the score of 34–7. The game was played on January 28, 2001, at Raymond James Stadium in Tampa, Florida.

The New England Patriots became the dominant team throughout the early 2000s, winning the championship three out of four years early in the decade. They would become only the second team in the history of the NFL to do so (after the 1990s Dallas Cowboys). In Super Bowl XXXVI, first-year starting quarterback Tom Brady led his team to a 20–17 upset victory over the St. Louis Rams. Brady would go on to win the MVP award for this game. The Patriots also won Super Bowls XXXVIII and XXXIX defeating the Carolina Panthers and the Philadelphia Eagles respectively. This four-year stretch of Patriot dominance was interrupted by the Tampa Bay Buccaneers' 48–21 Super Bowl XXXVII victory over the Oakland Raiders.

The Pittsburgh Steelers and Indianapolis Colts continued the era of AFC dominance by winning Super Bowls XL and XLI in 2005–06 and 2006–07, respectively defeating the Seattle Seahawks and Chicago Bears.

In the 2007 season, the Patriots became the fourth team in NFL history to have a perfect unbeaten and untied regular season record, the second in the Super Bowl era after the 1972 Miami Dolphins, and the first to finish 16–0. They easily marched through the AFC playoffs and were heavy favorites in Super Bowl XLII. However, they lost that game to Eli Manning and the New York Giants 17–14, leaving the Patriots' 2007 record at 18–1.

The following season, the Steelers logged their record sixth Super Bowl title (XLIII) in a 27–23, final-minute victory against the Arizona Cardinals.

The 2009 season saw the New Orleans Saints defeat the Indianapolis Colts in Super Bowl XLIV by a score of 31–17 to take home their first Championship. With this victory, the Saints joined the Tampa Bay Buccaneers and New York Jets as the only teams to have won in their sole Super Bowl appearance.

The 2010s have seen parity between the two conferences, but not within them. Since the start of 2010, five of the nine Super Bowl winners hailed from the NFC, the other four from the AFC.

Following up the Saints' win in Super Bowl XLIV, the 2010 season brought the Green Bay Packers their fourth Super Bowl (XLV) victory and record thirteenth NFL championship overall with the defeat of the Pittsburgh Steelers in February 2011. The Giants won another title after the 2011 season, again defeating the Patriots in Super Bowl XLVI.

The Baltimore Ravens snapped the NFC's three-game winning streak by winning Super Bowl XLVII in a 34–31 victory over the San Francisco 49ers. Super Bowl XLVIII, played at New Jersey's MetLife Stadium in February 2014, was the first Super Bowl held outdoors in a cold weather environment. The Seattle Seahawks won their first NFL title with a 43–8 defeat of the Denver Broncos, in a highly touted matchup that pitted Seattle's top-ranked defense against a Peyton Manning-led Denver offense that had broken the NFL's single-season scoring record.

In Super Bowl XLIX, the Patriots beat the defending Super Bowl champions, the Seahawks, 28–24 as Malcolm Butler intercepted a Seattle pass in the end zone with the Seahawks poised to take the lead. In Super Bowl 50, the Broncos, led by the league's top-ranked defense, defeated the Carolina Panthers, who had the league's top-ranked offense, in what became the final game of quarterback Peyton Manning's career.

In Super Bowl LI, the Atlanta Falcons had a 28–3 lead late in the third quarter, but lost to the Patriots, 34–28, in the first Super Bowl to ever end in overtime.

In Super Bowl LII, the Philadelphia Eagles defeated the New England Patriots, 41–33. It was the Eagles' third Super Bowl appearance, and their first win in franchise history. It was the Patriots' tenth Super Bowl appearance, and their fourth appearance in ten years; had the Patriots won, they would have tied the Pittsburgh Steelers with the most Super Bowl wins (six).

The Super Bowls of the late 2000s and 2010s are notable for the performances (and the pedigrees) of several of the participating quarterbacks, and stagnation (especially on the AFC side) in repeated appearances by the same teams and players. In particular, Tom Brady, Ben Roethlisberger, or Peyton Manning appeared as the AFC team's quarterback in all but two of the Super Bowls between 2001 and 2018.

The Super Bowl is one of the most watched annual sporting events in the world, with viewership overwhelmingly domestic. The only other annual event that gathers more viewers is the UEFA Champions League final. For many years, the Super Bowl has possessed a large US and global television viewership, and it is often the most watched United States originating television program of the year. The game tends to have high Nielsen television ratings, which is usually around a 40 rating and 60 shares. This means that on average, more than 100 million people from the United States alone are tuned into the Super Bowl at any given moment.

In press releases preceding each year's event, the NFL typically claims that that year's Super Bowl will have a potential worldwide audience of around one billion people in over 200 countries. This figure refers to the number of people "able" to watch the game, not the number of people "actually" watching. However, the statements have been frequently misinterpreted in various media as referring to the latter figure, leading to a common misperception about the game's actual global audience. The New York-based media research firm Initiative measured the global audience for the 2005 Super Bowl at 93 million people, with 98 percent of that figure being viewers in North America, which meant roughly 2 million people outside North America watched the Super Bowl that year.

The 2015 Super Bowl XLIX holds the record for average number of U.S. viewers, with a final number of 114.4 million, making the game the most-viewed television broadcast of any kind in American history. The halftime show was the most watched ever with 118.5 million viewers tuning in, and an all-time high of 168 million viewers in the United States had watched several portions of the Super Bowl 2015 broadcast. The game set a record for total viewers for the fifth time in six years.

The highest-rated game according to Nielsen was Super Bowl XVI in 1982, which was watched in 49.1 percent of households (73 shares), or 40,020,000 households at the time. Ratings for that game, a San Francisco victory over Cincinnati, may have been aided by a large blizzard that had affected much of the northeastern United States on game day, leaving residents to stay at home more than usual. Super Bowl XVI still ranks fourth on Nielsen's list of top-rated programs of all time, and three other Super Bowls, XII, XVII, and XX, made the top ten.

Famous commercial campaigns include the Budweiser "Bud Bowl" campaign, the 1984 introduction of Apple's Macintosh computer, and the 1999 and 2000 dot-com ads. As the television ratings of the Super Bowl have steadily increased over the years, prices have also increased every year, with advertisers paying as much as $3.5 million for a thirty-second spot during Super Bowl XLVI in 2012. A segment of the audience tunes into the Super Bowl solely to view commercials. In 2010, Nielsen reported that 51 percent of Super Bowl viewers tune in for the commercials. The Super Bowl halftime show has spawned another set of alternative entertainment such as the Lingerie Bowl, the Beer Bottle Bowl, and others.

Since 1991, the Super Bowl has begun between 6:19 and 6:40 PM EST so that most of the game is played during the primetime hours on the East Coast.

<br>

Super Bowls I–VI were blacked out in the television markets of the host cities, due to league restrictions then in place.

The Super Bowl provides an extremely strong lead-in to programming following it on the same channel, the effects of which can last for several hours. For instance, in discussing the ratings of a local TV station, Buffalo television critic Alan Pergament noted on the coattails from Super Bowl XLVII, which aired on CBS: "A paid program that ran on Channel 4 (WIVB-TV) at 2:30 in the morning had a 1.3 rating. That's higher than some CW prime time shows get on WNLO-TV, Channel 4's sister station."

Because of this strong coattail effect, the network that airs the Super Bowl typically takes advantage of the large audience to air an episode of a hit series, or to premiere the pilot of a promising new one in the lead-out slot, which immediately follows the Super Bowl and post-game coverage.

Early Super Bowls featured a halftime show consisting of marching bands from local colleges or high schools; but as the popularity of the game increased, a trend where popular singers and musicians performed during its pre-game ceremonies and the halftime show, or simply sang the national anthem of the United States, emerged. Unlike regular season or playoff games, thirty minutes are allocated for the Super Bowl halftime. After a special live episode of the Fox sketch comedy series "In Living Color" caused a drop in viewership for the Super Bowl XXVI halftime show, the NFL sought to increase the Super Bowl's audience by hiring A-list talent to perform. They approached Michael Jackson, whose performance the following year drew higher figures than the game itself. Another notable performance came during Super Bowl XXXVI in 2002, when U2 performed; during their third song, "Where the Streets Have No Name", the band played under a large projection screen which scrolled through names of the victims of the September 11 attacks.

For many years, Whitney Houston's performance of the national anthem at Super Bowl XXV in 1991, during the Gulf War, had long been regarded as one of the best renditions of the anthem in history. Then, in an historic, groundbreaking, and emotional performance prior to Super Bowl XLVIII, soprano Renee Fleming became the first opera singer to perform the anthem, propelling FOX to the highest ratings of any program in its history, and remains so today.

The halftime show of Super Bowl XXXVIII attracted controversy, following an incident in which Justin Timberlake removed a piece of Janet Jackson's top, briefly exposing one of her breasts before the broadcast quickly cut away from the shot. The incident led to fines being issued by the FCC (and a larger crackdown over "indecent" content broadcast on television), and MTV (then a sister to the game's broadcaster that year, CBS, under Viacom) being banned by the NFL from producing the Super Bowl halftime show in the future. In an effort to prevent a repeat of the incident, the NFL held a moratorium on Super Bowl halftime shows featuring pop performers, and instead invited a single, headlining veteran act, such as Paul McCartney, The Who, Prince, and Bruce Springsteen. This practice ended at Super Bowl XLV, which returned to using current pop acts such as The Black Eyed Peas and Katy Perry.

Excluding Super Bowl XXXIX, the famous "I'm going to Disney World!" advertising campaign took place in every Super Bowl since Super Bowl XXI when quarterback Phil Simms from the New York Giants became the first player to say the tagline.

As of Super Bowl LII, 27 of 52 Super Bowls have been played in three cities: New Orleans (ten times), the Greater Miami area (ten times), and the Greater Los Angeles area (seven times). No market or region without an active NFL franchise has ever hosted a Super Bowl, and the presence of an NFL team in a market or region is now a "de jure" requirement for bidding on the game. The winning market is not, however, required to host the Super Bowl in the same stadium that its NFL team uses, and nine Super Bowls have been held in a stadium other than the one the NFL team in that city was using at the time. For example, Los Angeles's last five Super Bowls were all played at the Rose Bowl, which has never been used by any NFL franchise outside of the Super Bowl.

No team has ever played the Super Bowl in its home stadium. The closest any team has come was the 2017 Minnesota Vikings, who were within one win of playing Super Bowl LII in U.S. Bank Stadium, but lost the NFC Championship game to the Philadelphia Eagles. Two teams have played the Super Bowl in their home market: the San Francisco 49ers, who played Super Bowl XIX in Stanford Stadium instead of Candlestick Park; and the Los Angeles Rams, who played Super Bowl XIV in the Rose Bowl instead of the Los Angeles Memorial Coliseum. In both cases, the stadium in which the Super Bowl was held was perceived to be a better stadium for a large, high-profile event than the stadiums the Rams and 49ers were playing in at the time; this situation has not arisen since 1993, in part because the league has traditionally awarded the Super Bowl in modern times to the newest stadiums. Besides those two, the only other Super Bowl venue that was not the home stadium to an NFL team at the time was Rice Stadium in Houston: the Houston Oilers had played there previously, but moved to the Astrodome several years prior to Super Bowl VIII. The Orange Bowl was the only AFL stadium to host a Super Bowl and the only stadium to host consecutive Super Bowls, hosting Super Bowls II and III.

Traditionally, the NFL does not award Super Bowls to stadiums that are located in climates with an expected average daily temperature less than 50 °F (10 °C) on game day unless the field can be completely covered by a fixed or retractable roof. Six Super Bowls have been played in northern cities: two in the Detroit area—Super Bowl XVI at Pontiac Silverdome in Pontiac, Michigan and Super Bowl XL at Ford Field in Detroit, two in Minneapolis—Super Bowl XXVI at the Hubert H. Humphrey Metrodome and Super Bowl LII at the U.S. Bank Stadium, one in Indianapolis at Lucas Oil Stadium for Super Bowl XLVI, and one in the New York area—Super Bowl XLVIII at MetLife Stadium. Only MetLife Stadium did not have a roof (be it fixed or retractable) but it was still picked as the host stadium for Super Bowl XLVIII in an apparent waiver of the warm-climate rule.

There have been a few instances where the league has rescinded the Super Bowl from cities. Super Bowl XXVII in 1993 was originally awarded to Sun Devil Stadium in Tempe, Arizona, but after Arizona voters elected not to recognize Martin Luther King, Jr. Day as a paid state-employee's holiday in 1990, the NFL moved the game to the Rose Bowl in Pasadena, California. When voters in Arizona opted to create such a legal holiday in 1992, Super Bowl XXX in 1996 was awarded to Tempe. Super Bowl XXXIII was awarded first to Candlestick Park in San Francisco, but when plans to renovate the stadium fell through, the game was moved to Pro Player Stadium in greater Miami. Super Bowl XXXVII was awarded to a new stadium not yet built in San Francisco, when that stadium failed to be built, the game was moved to San Diego. Super Bowl XLIV, slated for February 7, 2010, was withdrawn from New York City's proposed West Side Stadium, because the city, state, and proposed tenants New York Jets could not agree on funding. Super Bowl XLIV was then eventually awarded to Sun Life Stadium in Miami Gardens, Florida. Super Bowl XLIX in 2015 was originally given to Arrowhead Stadium in Kansas City, Missouri, but after two sales taxes failed to pass at the ballot box, and opposition by local business leaders and politicians increased, Kansas City eventually withdrew its request to host the game. Super Bowl XLIX was then eventually awarded to University of Phoenix Stadium in Glendale, Arizona.

In 2011, Texas Attorney General Greg Abbott said, "[The Super Bowl is] commonly known as the single largest human trafficking incident in the United States." According to "Forbes", 10,000 prostitutes were brought to Miami in 2010 for the Super Bowl. "Snopes" research in 2015 determined that the actual number of prostitutes involved in a typical Super Bowl weekend is less than 100, not statistically higher than any other time of the year, and that the notion of mass increases in human trafficking around the Super Bowl was a politician's myth.

The location of the Super Bowl is chosen by the NFL well in advance, usually three to five years before the game. Cities place bids to host a Super Bowl and are evaluated in terms of stadium renovation and their ability to host. In 2014, a document listing the specific requirements of Super Bowl hosts was leaked, giving a clear list of what was required for a Super Bowl host. Much of the cost of the Super Bowl is to be assumed by the host community, although some costs are enumerated within the requirements to be assumed by the NFL. Some of the host requirements include:

The NFL owners meet to make a selection on the site, usually three to five years prior to the event. In 2007, NFL commissioner Roger Goodell suggested that a Super Bowl might be played in London, perhaps at Wembley Stadium. The game has never been played in a region that lacks an NFL franchise; seven Super Bowls have been played in Los Angeles, but none were held there in the 21-year period when the league had no team in the area. New Orleans, the site of the 2013 Super Bowl, invested more than $1 billion in infrastructure improvements in the years leading up to the game.

Through Super Bowl LVI, teams were allowed to bid for the rights to host Super Bowls. The league rescinded this privilege in 2018 and will make all decisions regarding hosting sites from Super Bowl LVII onward; the league will choose a potential venue unilaterally, the chosen team will put together a hosting proposal, and the league will vote upon it to determine if it is acceptable.

The designated "home team" alternates between the NFC team in odd-numbered games and the AFC team in even-numbered games. This alternation was initiated with the first Super Bowl, when the Green Bay Packers were the designated home team. Regardless of being the home or away team of record, each team has their team logo and wordmark painted in one of the end zones. Designated away teams have won 30 of 51 Super Bowls to date (approximately 59 percent).
Since Super Bowl XIII in January 1979, the home team is given the choice of wearing their colored or white jerseys. Originally, the designated home team had to wear their colored jerseys, which resulted in Dallas donning their less exposed dark blue jerseys for Super Bowl V. While most of the home teams in the Super Bowl have chosen to wear their colored jerseys, there have been six (6) exceptions: the Dallas Cowboys during Super Bowl XIII and XXVII, the Washington Redskins during Super Bowl XVII, the Pittsburgh Steelers during Super Bowl XL, the Denver Broncos during Super Bowl 50, and the New England Patriots in Super Bowl LII. The Cowboys, since , have worn white jerseys at home. The Redskins wore white at home under coach Joe Gibbs starting in through , continued by Richie Petitbon and Norv Turner through , then again when Gibbs returned from through . Meanwhile, the Steelers, who have always worn their black jerseys at home since the AFL–NFL merger in , opted for the white jerseys after winning three consecutive playoff games on the road, wearing white. The Steelers' decision was compared with the New England Patriots in Super Bowl XX; the Patriots had worn white jerseys at home during the season, but after winning road playoff games against the New York Jets and Miami Dolphins wearing red jerseys, New England opted to switch to crimson for the Super Bowl as the designated home team. For the Broncos in Super Bowl 50, Denver general manager John Elway simply stated, "We've had Super Bowl success in our white uniforms"; they previously had been in Super Bowls when wearing their orange jerseys. The Broncos' decision is also perceived to be made out of superstition, losing all Super Bowl games with the orange jerseys in terrible fashion. It is unclear why the Patriots chose to wear their white jerseys for Super Bowl LII. During the pairing of Bill Belichick and Tom Brady, New England has mostly worn their blue jerseys for home games, but have worn white for a home game in the , , and seasons. The New England Patriots were 3-0 in their white uniforms in Super Bowls prior to Super Bowl LII with Belichick and Brady, and they may have been going on recent trends of teams who wear white for the Super Bowl game. White-shirted teams have won 33 of 52 Super Bowls to date (63 percent). The only teams to win in their dark-colored uniform in more recent years are the Green Bay Packers against the Pittsburgh Steelers in Super Bowl XLV and the Philadelphia Eagles against the New England Patriots in Super Bowl LII, with teams in white winning 12 of the last 14 Super Bowls.

The 49ers, as part of the league's 75th Anniversary celebration, used their 1957 throwback uniform in Super Bowl XXIX, which for that year was their regular home jersey. No team has yet worn a third jersey or Color Rush uniform for the Super Bowl.

Fifteen different regions have hosted Super Bowls.

A total of twenty-six different stadiums, six of which no longer exist and one of which does not yet exist, have hosted or are scheduled to host Super Bowls. Years listed in the table below are the years the game was actually played ("will be played") rather than what NFL season it is considered to have been.

Future venues:

The game has never been played in a region that lacked an NFL or AFL franchise. London, England has occasionally been mentioned as a host city for a Super Bowl in the near future. Wembley Stadium has hosted several NFL games as part of the NFL International Series and is specifically designed for large, individual events. NFL Commissioner Roger Goodell has openly discussed the possibility on different occasions. Time zone complications are a significant obstacle to a Super Bowl in London; a typical 6:30 p.m. Eastern Time start would result in the game beginning at 11:30 p.m. local time in London, an unusually late hour to be holding spectator sports (the NFL has never in its history started a game later than 9:15 p.m. local time). As bids have been submitted for all Super Bowls through Super Bowl LVI, the soonest that any stadium outside the NFL's footprint could serve as host would be Super Bowl LVII in 2023.

The NFL is very active on stopping what it says is unauthorized commercial use of its trademarked terms "NFL", "Super Bowl", and "Super Sunday". As a result, many events and promotions tied to the game, but not sanctioned by the NFL, are asked to refer to it with euphemisms such as "The Big Game", or other generic descriptions. A radio spot for Planters nuts parodied this, by saying "it would be "super"...to have a "bowl"...of Planters nuts while watching the big game!" and comedian Stephen Colbert began referring to the game in 2014 as the "Superb Owl". In 2015, the NFL filed opposition with the USPTO Trademark Trial and Appeal Board to a trademark application submitted by an Arizona-based nonprofit for "Superb Owl". The NFL claims that the use of the phrase "Super Bowl" implies an NFL affiliation, and on this basis the league asserts broad rights to restrict how the game may be shown publicly; for example, the league says Super Bowl showings are prohibited in churches or at other events that "promote a message", while venues that do not regularly show sporting events cannot show the Super Bowl on any television screen larger than 55 inches. Some critics say the NFL is exaggerating its ownership rights by stating that "any use is prohibited", as this contradicts the broad doctrine of fair use in the United States. Legislation was proposed by Utah Senator Orrin Hatch in 2008 "to provide an exemption from exclusive rights in copyright for certain nonprofit organizations to display live football games", and "for other purposes".

In 2004, the NFL started issuing Cease and Desist letters to casinos in Las Vegas that were hosting Super Bowl parties. "Super Bowl" is a registered trademark, owned by the NFL, and any other business using that name for profit-making ventures is in violation of federal law, according to the letters. In reaction to the letters, many Vegas resorts, rather than discontinue the popular and lucrative parties, started referring to them as "Big Game Parties".

In 2006, the NFL made an attempt to trademark "The Big Game" as well; however, it withdrew the application in 2007 due to growing commercial and public-relations opposition to the move, mostly from Stanford University and the University of California, Berkeley and their fans, as the Stanford Cardinal football and California Golden Bears football teams compete in the "Big Game", which has been played since 1892 (28 years before the formation of the NFL and 75 years before Super Bowl I). Additionally, the Mega Millions lottery game was known as The Big Game from 1996 to 2002.

Like the other major professional leagues in the United States, the winner of the Super Bowl is usually declared "world champions", a title that has been mocked by non-American journalists. Others feel the title is fitting, since it is the only professional league of its kind.

The practice by the U.S. major leagues of using the "World Champion" moniker originates from the World Series of professional baseball, and it was later used during the first three Super Bowls when they were referred to as AFL-NFL World Championship Games. The phrase is still engraved on the Super Bowl rings.






</doc>
<doc id="27725" url="https://en.wikipedia.org/wiki?curid=27725" title="Surface area">
Surface area

The surface area of a solid object is a measure of the total area that the surface of the object occupies. The mathematical definition of surface area in the presence of curved surfaces is considerably more involved than the definition of arc length of one-dimensional curves, or of the surface area for polyhedra (i.e., objects with flat polygonal faces), for which the surface area is the sum of the areas of its faces. Smooth surfaces, such as a sphere, are assigned surface area using their representation as parametric surfaces. This definition of surface area is based on methods of infinitesimal calculus and involves partial derivatives and double integration.

A general definition of surface area was sought by Henri Lebesgue and Hermann Minkowski at the turn of the twentieth century. Their work led to the development of geometric measure theory, which studies various notions of surface area for irregular objects of any dimension. An important example is the Minkowski content of a surface.

While the areas of many simple surfaces have been known since antiquity, a rigorous mathematical "definition" of area requires a great deal of care.
This should provide a function

which assigns a positive real number to a certain class of surfaces that satisfies several natural requirements. The most fundamental property of the surface area is its additivity: "the area of the whole is the sum of the areas of the parts". More rigorously, if a surface "S" is a union of finitely many pieces "S", …, "S" which do not overlap except at their boundaries, then 

Surface areas of flat polygonal shapes must agree with their geometrically defined area. Since surface area is a geometric notion, areas of congruent surfaces must be the same and the area must depend only on the shape of the surface, but not on its position and orientation in space. This means that surface area is invariant under the group of Euclidean motions. These properties uniquely characterize surface area for a wide class of geometric surfaces called "piecewise smooth". Such surfaces consist of finitely many pieces that can be represented in the parametric form

with a continuously differentiable function formula_4 The area of an individual piece is defined by the formula

Thus the area of "S" is obtained by integrating the length of the normal vector formula_6 to the surface over the appropriate region "D" in the parametric "uv" plane. The area of the whole surface is then obtained by adding together the areas of the pieces, using additivity of surface area. The main formula can be specialized to different classes of surfaces, giving, in particular, formulas for areas of graphs "z" = "f"("x","y") and surfaces of revolution.

One of the subtleties of surface area, as compared to arc length of curves, is that surface area cannot be defined simply as the limit of areas of polyhedral shapes approximating a given smooth surface. It was demonstrated by Hermann Schwarz that already for the cylinder, different choices of approximating flat surfaces can lead to different limiting values of the area (this example is known as ).

Various approaches to a general definition of surface area were developed in the late nineteenth and the early twentieth century by Henri Lebesgue and Hermann Minkowski. While for piecewise smooth surfaces there is a unique natural notion of surface area, if a surface is very irregular, or rough, then it may not be possible to assign an area to it at all. A typical example is given by a surface with spikes spread throughout in a dense fashion. Many surfaces of this type occur in the study of fractals. Extensions of the notion of area which partially fulfill its function and may be defined even for very badly irregular surfaces are studied in geometric measure theory. A specific example of such an extension is the Minkowski content of the surface.

The below given formulas can be used to show that the surface area of a sphere and cylinder of the same radius and height are in the ratio 2 : 3, as follows.

Let the radius be "r" and the height be "h" (which is 2"r" for the sphere).

formula_7

The discovery of this ratio is credited to Archimedes.

Surface area is important in chemical kinetics. Increasing the surface area of a substance generally increases the rate of a chemical reaction. For example, iron in a fine powder will combust, while in solid blocks it is stable enough to use in structures. For different applications a minimal or maximal surface area may be desired.
The surface area of an organism is important in several considerations, such as regulation of body temperature and digestion. Animals use their teeth to grind food down into smaller particles, increasing the surface area available for digestion. The epithelial tissue lining the digestive tract contains microvilli, greatly increasing the area available for absorption. Elephants have large ears, allowing them to regulate their own body temperature. In other instances, animals will need to minimize surface area; for example, people will fold their arms over their chest when cold to minimize heat loss.

The surface area to volume ratio (SA:V) of a cell imposes upper limits on size, as the volume increases much faster than does the surface area, thus limiting the rate at which substances diffuse from the interior across the cell membrane to interstitial spaces or to other cells. Indeed, representing a cell as an idealized sphere of radius "r", the volume and surface area are, respectively, "V" = 4/3 π "r"; "SA" = 4 π "r". The resulting surface area to volume ratio is therefore 3/"r". Thus, if a cell has a radius of 1 μm, the SA:V ratio is 3; whereas if the radius of the cell is instead 10 μm, then the SA:V ratio becomes 0.3. With a cell radius of 100, SA:V ratio is 0.03. Thus, the surface area falls off steeply with increasing volume.




</doc>
<doc id="27727" url="https://en.wikipedia.org/wiki?curid=27727" title="Solid state">
Solid state

Solid state, or solid matter, is one of the four fundamental states of matter. It may also refer to:





</doc>
<doc id="27730" url="https://en.wikipedia.org/wiki?curid=27730" title="Serbo-Croatian">
Serbo-Croatian

Serbo-Croatian , also called Serbo-Croat , Serbo-Croat-Bosnian (SCB), Bosnian-Croatian-Serbian (BCS), or Bosnian-Croatian-Montenegrin-Serbian (BCMS), is a South Slavic language and the primary language of Serbia, Croatia, Bosnia and Herzegovina, and Montenegro. It is a pluricentric language with four mutually intelligible standard varieties.

South Slavic dialects historically formed a continuum. The turbulent history of the area, particularly due to expansion of the Ottoman Empire, resulted in a patchwork of dialectal and religious differences. Due to population migrations, Shtokavian became the most widespread dialect in the western Balkans, intruding westwards into the area previously occupied by Chakavian and Kajkavian (which further blend into Slovenian in the northwest). Bosniaks, Croats and Serbs differ in religion and were historically often part of different cultural circles, although a large part of the nations have lived side by side under foreign overlords. During that period, the language was referred to under a variety of names, such as "Slavic", "Illyrian", or according to region, "Bosnian", "Serbian" and "Croatian", the latter often in combination with "Slavonian" or "Dalmatian".

Serbo-Croatian was standardized in the mid-19th-century Vienna Literary Agreement by Croatian and Serbian writers and philologists, decades before a Yugoslav state was established. From the very beginning, there were slightly different literary Serbian and Croatian standards, although both were based on the same Shtokavian subdialect, Eastern Herzegovinian. In the 20th century, Serbo-Croatian served as the official language of the Kingdom of Yugoslavia (when it was called "Serbo-Croato-Slovenian"), and later as one of the official languages of the Socialist Federal Republic of Yugoslavia. The breakup of Yugoslavia affected language attitudes, so that social conceptions of the language separated on ethnic and political lines. Since the breakup of Yugoslavia, Bosnian has likewise been established as an official standard in Bosnia and Herzegovina, and there is an ongoing movement to codify a separate Montenegrin standard. Serbo-Croatian thus generally goes by the ethnic names Serbian, Croatian, Bosnian, and sometimes Montenegrin and Bunjevac.

Like other South Slavic languages, Serbo-Croatian has a simple phonology, with the common five-vowel system and twenty-five consonants. Its grammar evolved from Common Slavic, with complex inflection, preserving seven grammatical cases in nouns, pronouns, and adjectives. Verbs exhibit imperfective or perfective aspect, with a moderately complex tense system. Serbo-Croatian is a pro-drop language with flexible word order, subject–verb–object being the default. It can be written in Serbian Cyrillic or Gaj's Latin alphabet, whose thirty letters mutually map one-to-one, and the orthography is highly phonemic in all standards.

Throughout the history of the South Slavs, the vernacular, literary, and written languages (e.g. Chakavian, Kajkavian, Shtokavian) of the various regions and ethnicities developed and diverged independently. Prior to the 19th century, they were collectively called "Illyric", "Slavic", "Slavonian", "Bosnian", "Dalmatian", "Serbian" or "Croatian". As such, the term "Serbo-Croatian" was first used by Jacob Grimm in 1824, popularized by the Viennese philologist Jernej Kopitar in the following decades, and accepted by Croatian Zagreb grammarians in 1854 and 1859. At that time, Serb and Croat lands were still part of the Ottoman and Austrian Empires. Officially, the language was called variously "Serbo-Croat, Croato-Serbian, Serbian and Croatian, Croatian and Serbian, Serbian or Croatian, Croatian or Serbian." Unofficially, Serbs and Croats typically called the language "Serbian" or "Croatian", respectively, without implying a distinction between the two, and again in independent Bosnia and Herzegovina, "Bosnian", "Croatian", and "Serbian" were considered to be three names of a single official language. Croatian linguist Dalibor Brozović advocated the term "Serbo-Croatian" as late as 1988, claiming that in an analogy with Indo-European, Serbo-Croatian does not only name the two components of the same language, but simply charts the limits of the region in which it is spoken and includes everything between the limits (‘Bosnian’ and ‘Montenegrin’). Today, use of the term "Serbo-Croatian" is controversial due to the prejudice that nation and language must match. It is still used for lack of a succinct alternative, though alternative names have emerged, such as "Bosnian/Croatian/Serbian" (BCS), which is often seen in political contexts such as the International Criminal Tribunal for the former Yugoslavia.

Old Church Slavonic was adopted as the language of the liturgy. This language was gradually adapted to non-liturgical purposes and became known as the Croatian version of Old Slavonic. The two variants of the language, liturgical and non-liturgical, continued to be a part of the Glagolitic service as late as the middle of the 19th century. The earliest known Croatian Church Slavonic Glagolitic manuscripts are the "Glagolita Clozianus" and the "Vienna Folia" from the 11th century.

The beginning of written Serbo-Croatian can be traced from the 10th century and on when Serbo-Croatian medieval texts were written in five scripts: Latin, Glagolitic, Early Cyrillic, Bosnian Cyrillic ("bosančica/bosanica"), and Arebica, the last principally by Bosniak nobility. Serbo-Croatian competed with the more established literary languages of Latin and Old Slavonic in the west and Persian and Arabic in the east.

Old Slavonic developed into the Serbo-Croatian variant of Church Slavonic between the 12th and 16th centuries.

Among the earliest attestations of Serbo-Croatian are the Humac tablet, dating from the 10th or 11th century, written in Bosnian Cyrillic and Glagolitic; the Plomin tablet, dating from the same era, written in Glagolitic; the Valun tablet, dated to the 11th century, written in Glagolitic and Latin; and the Inscription of Župa Dubrovačka, a Glagolitic tablet dated to the 11th century.

The Baška tablet from the late 11th century was written in Glagolitic. It is a large stone tablet found in the small Church of St. Lucy, Jurandvor on the Croatian island of Krk that contains text written mostly in Chakavian in the Croatian script. It is also important in the history of the nation as it mentions Zvonimir, the king of Croatia at the time.

The Charter of Ban Kulin of 1189, written by Ban Kulin of Bosnia, was an early Shtokavian text, written in Bosnian Cyrillic.

The luxurious and ornate representative texts of Serbo-Croatian Church Slavonic belong to the later era, when they coexisted with the Serbo-Croatian vernacular literature. The most notable are the "Missal of Duke Novak" from the Lika region in northwestern Croatia (1368), "Evangel from Reims" (1395, named after the town of its final destination), Hrvoje's Missal from Bosnia and Split in Dalmatia (1404), and the first printed book in Serbo-Croatian, the Glagolitic Missale Romanum Glagolitice (1483).

During the 13th century Serbo-Croatian vernacular texts began to appear, the most important among them being the "Istrian land survey" of 1275 and the "Vinodol Codex" of 1288, both written in the Chakavian dialect.

The Shtokavian dialect literature, based almost exclusively on Chakavian original texts of religious provenance (missals, breviaries, prayer books) appeared almost a century later. The most important purely Shtokavian vernacular text is the Vatican Croatian Prayer Book (c. 1400).

Both the language used in legal texts and that used in Glagolitic literature gradually came under the influence of the vernacular, which considerably affected its phonological, morphological, and lexical systems. From the 14th and the 15th centuries, both secular and religious songs at church festivals were composed in the vernacular.

Writers of early Serbo-Croatian religious poetry ("začinjavci") gradually introduced the vernacular into their works. These "začinjavci" were the forerunners of the rich literary production of the 16th-century literature, which, depending on the area, was Chakavian-, Kajkavian-, or Shtokavian-based. The language of religious poems, translations, miracle and morality plays contributed to the popular character of medieval Serbo-Croatian literature.

One of the earliest dictionaries, also in the Slavic languages as a whole, was the "Bosnian–Turkish Dictionary" of 1631 authored by Muhamed Hevaji Uskufi and was written in the Arebica script.

In the mid-19th century, Serbian (led by self-taught writer and folklorist Vuk Stefanović Karadžić) and most Croatian writers and linguists (represented by the Illyrian movement and led by Ljudevit Gaj and Đuro Daničić), proposed the use of the most widespread dialect, Shtokavian, as the base for their common standard language. Karadžić standardised the Serbian Cyrillic alphabet, and Gaj and Daničić standardized the Croatian Latin alphabet, on the basis of vernacular speech phonemes and the principle of phonological spelling. In 1850 Serbian and Croatian writers and linguists signed the Vienna Literary Agreement, declaring their intention to create a unified standard. Thus a complex bi-variant language appeared, which the Serbs officially called "Serbo-Croatian" or "Serbian or Croatian" and the Croats "Croato-Serbian", or "Croatian or Serbian". Yet, in practice, the variants of the conceived common literary language served as different literary variants, chiefly differing in lexical inventory and stylistic devices. The common phrase describing this situation was that Serbo-Croatian or "Croatian or Serbian" was a single language. During the Austro-Hungarian occupation of Bosnia and Herzegovina, the language of all three nations was called "Bosnian" until the death of administrator von Kállay in 1907, at which point the name was changed to "Serbo-Croatian".

With unification of the first the Kingdom of the Serbs, Croats, and Slovenes – the approach of Karadžić and the Illyrians became dominant. The official language was called "Serbo-Croato-Slovenian" ("srpsko-hrvatsko-slovenački") in the 1921 constitution. In 1929, the constitution was suspended, and the country was renamed the Kingdom of Yugoslavia, while the official language of Serbo-Croato-Slovene was reinstated in the 1931 constitution.

In June 1941, the Nazi puppet Independent State of Croatia began to rid the language of "Eastern" (Serbian) words, and shut down Serbian schools.

On January 15, 1944, the Anti-Fascist Council of the People's Liberation of Yugoslavia (AVNOJ) declared Croatian, Serbian, Slovene, and Macedonian to be equal in the entire territory of Yugoslavia. In 1945 the decision to recognize Croatian and Serbian as separate languages was reversed in favor of a single Serbo-Croatian or Croato-Serbian language. In the Communist-dominated second Yugoslavia, ethnic issues eased to an extent, but the matter of language remained blurred and unresolved.

In 1954, major Serbian and Croatian writers, linguists and literary critics, backed by Matica srpska and Matica hrvatska signed the Novi Sad Agreement, which in its first conclusion stated: "Serbs, Croats and Montenegrins share a single language with two equal variants that have developed around Zagreb (western) and Belgrade (eastern)". The agreement insisted on the equal status of Cyrillic and Latin scripts, and of Ekavian and Ijekavian pronunciations. It also specified that "Serbo-Croatian" should be the name of the language in official contexts, while in unofficial use the traditional "Serbian" and "Croatian" were to be retained. Matica hrvatska and Matica srpska were to work together on a dictionary, and a committee of Serbian and Croatian linguists was asked to prepare a "pravopis". During the sixties both books were published simultaneously in Ijekavian Latin in Zagreb and Ekavian Cyrillic in Novi Sad. Yet Croatian linguists claim that it was an act of unitarianism. The evidence supporting this claim is patchy: Croatian linguist Stjepan Babić complained that the television transmission from Belgrade always used the Latin alphabet— which was true, but was not proof of unequal rights, but of frequency of use and prestige. Babić further complained that the Novi Sad Dictionary (1967) listed side by side words from both the Croatian and Serbian variants wherever they differed, which one can view as proof of careful respect for both variants, and not of unitarism. Moreover, Croatian linguists criticized those parts of the Dictionary for being unitaristic that were written by Croatian linguists. And finally, Croatian linguists ignored the fact that the material for the "Pravopisni rječnik" came from the Croatian Philological Society. Regardless of these facts, Croatian intellectuals brought the Declaration on the Status and Name of the Croatian Literary Language in 1967. On occasion of the publication’s 45th anniversary, the Croatian weekly journal Forum published the Declaration again in 2012, accompanied by a critical analysis.

West European scientists judge the Yugoslav language policy as an exemplary one: although three-quarters of the population spoke one language, no single language was official on a federal level. Official languages were declared only at the level of constituent republics and provinces, and very generously: Vojvodina had five (among them Slovak and Romanian, spoken by 0.5 per cent of the population), and Kosovo four (Albanian, Turkish, Romany and Serbo-Croatian). Newspapers, radio and television studios used sixteen languages, fourteen were used as languages of tuition in schools, and nine at universities. Only the Yugoslav Army used Serbo-Croatian as the sole language of command, with all other languages represented in the army’s other activities—however, this is not different from other armies of multilingual states, or in other specific institutions, such as international air traffic control where English is used worldwide. All variants of Serbo-Croatian were used in state administration and republican and federal institutions. Both Serbian and Croatian variants were represented in respectively different grammar books, dictionaries, school textbooks and in books known as pravopis (which detail spelling rules). Serbo-Croatian was a kind of soft standardisation. However, legal equality could not dampen the prestige Serbo-Croatian had: since it was the language of three quarters of the population, it functioned as an unofficial lingua franca. And within Serbo-Croatian, the Serbian variant, with twice as many speakers as the Croatian, enjoyed greater prestige, reinforced by the fact that Slovene and Macedonian speakers preferred it to the Croatian variant because their languages are also Ekavian. This is a common situation in other pluricentric languages, e.g. the variants of German differ according to their prestige, the variants of Portuguese too. Moreover, all languages differ in terms of prestige: "the fact is that languages (in terms of prestige, learnability etc.) are not equal, and the law cannot make them equal".

In 2017, the "Declaration of the Common Language" ("Deklaracija o zajedničkom jeziku"), signed by a group of NGOs and linguists from former Yugoslavia, argues that all variants belong to a common polycentric language.

The total number of persons who declared their native language as either 'Bosnian', 'Croatian', 'Serbian', 'Montenegrin', or 'Serbo-Croatian' in countries of the region is about 16 million.

Serbian is spoken by about 9.5 million, mostly in Serbia (6.7m), Bosnia and Herzegovina (1.4m), and Montenegro (0.4m). Serbian minorities are found in the Republic of Macedonia and in Romania. In Serbia, there are about 760,000 second-language speakers of Serbian, including Hungarians in Vojvodina and the 400,000 estimated Roma. Familiarity of Kosovo Albanians with Serbian in Kosovo varies depending on age and education, and exact numbers are not available.

Croatian is spoken by roughly 4.8 million, including some 575,000 in Bosnia and Herzegovina. A small Croatian minority that lives in Italy, known as Molise Croats, have somewhat preserved traces of the Croatian language. In Croatia, 170,000, mostly Italians and Hungarians, use it as a second language.

Bosnian is spoken by 2.2 million people, chiefly Bosniaks, including about 220,000 in Serbia and Montenegro.

The notion of Montenegrin as a separate standard from Serbian is relatively recent. In the 2003 census, around 150,000 Montenegrins, of the country's 620,000, declared Montenegrin as their native language. That figure is likely to increase, due to the country's independence and strong institutional backing of Montenegrin language.

Serbo-Croatian is also a second language of many Slovenians and Macedonians, especially those born during the time of Yugoslavia. According to the 2002 Census, Serbo-Croatian and its variants have the largest number of speakers of the minority languages in Slovenia.

Outside the Balkans, there are over 2 million native speakers of the language(s), especially in countries which are frequent targets of immigration, such as Australia, Austria, Brazil, Canada, Chile, Germany, Hungary, Italy, Sweden and the United States.

Serbo-Croatian is a highly inflected language. Traditional grammars list seven cases for nouns and adjectives: nominative, genitive, dative, accusative, vocative, locative, and instrumental, reflecting the original seven cases of Proto-Slavic, and indeed older forms of Serbo-Croatian itself. However, in modern Shtokavian the locative has almost merged into dative (the only difference is based on accent in some cases), and the other cases can be shown declining; namely:

Like most Slavic languages, there are mostly three genders for nouns: masculine, feminine, and neuter, a distinction which is still present even in the plural (unlike Russian and, in part, the Čakavian dialect). They also have two numbers: singular and plural. However, some consider there to be three numbers (paucal or "dual," too), since (still preserved in closely related Slovene) after two ("dva", "dvije"/"dve"), three ("tri") and four ("četiri"), and all numbers ending in them (e.g. twenty-two, ninety-three, one hundred four) the genitive singular is used, and after all other numbers five ("pet") and up, the genitive plural is used. (The number one ["jedan"] is treated as an adjective.) Adjectives are placed in front of the noun they modify and must agree in both case and number with it.

There are seven tenses for verbs: past, present, future, exact future, aorist, imperfect, and plusquamperfect; and three moods: indicative, imperative, and conditional. However, the latter three tenses are typically used only in Shtokavian writing, and the time sequence of the exact future is more commonly formed through an alternative construction.

In addition, like most Slavic languages, the Shtokavian verb also has one of two aspects: perfective or imperfective. Most verbs come in pairs, with the perfective verb being created out of the imperfective by adding a prefix or making a stem change. The imperfective aspect typically indicates that the action is unfinished, in progress, or repetitive; while the perfective aspect typically denotes that the action was completed, instantaneous, or of limited duration. Some Štokavian tenses (namely, aorist and imperfect) favor a particular aspect (but they are rarer or absent in Čakavian and Kajkavian). Actually, aspects "compensate" for the relative lack of tenses, because aspect of the verb determines whether the act is completed or in progress in the referred time.

The Serbo-Croatian vowel system is simple, with only five vowels in Shtokavian. All vowels are monophthongs. The oral vowels are as follows:

The vowels can be short or long, but the phonetic quality does not change depending on the length. In a word, vowels can be long in the stressed syllable and the syllables following it, never in the ones preceding it.

The consonant system is more complicated, and its characteristic features are series of affricate and palatal consonants. As in English, voice is phonemic, but aspiration is not.

In consonant clusters all consonants are either voiced or voiceless. All the consonants are voiced if the last consonant is normally voiced or voiceless if the last consonant is normally voiceless. This rule does not apply to approximantsa consonant cluster may contain voiced approximants and voiceless consonants; as well as to foreign words ("Washington" would be transcribed as "VašinGton"), personal names and when consonants are not inside of one syllable.

Apart from Slovene, Serbo-Croatian is the only Slavic language with a pitch accent (simple tone) system. This feature is present in some other Indo-European languages, such as Swedish, Norwegian, and Ancient Greek. Neo-Shtokavian Serbo-Croatian, which is used as the basis for standard Bosnian, Croatian, Montenegrin, and Serbian, has four "accents", which involve either a rising or falling tone on either long or short vowels, with optional post-tonic lengths:

The tone stressed vowels can be approximated in English with "set" vs. "setting?" said in isolation for a short tonic "e," or "leave" vs. "leaving?" for a long tonic "i," due to the prosody of final stressed syllables in English.

General accent rules in the standard language:


There are no other rules for accent placement, thus the accent of every word must be learned individually; furthermore, in inflection, accent shifts are common, both in type and position (the so-called "mobile paradigms"). The second rule is not strictly obeyed, especially in borrowed words.

Comparative and historical linguistics offers some clues for memorising the accent position: If one compares many standard Serbo-Croatian words to e.g. cognate Russian words, the accent in the Serbo-Croatian word will be one syllable before the one in the Russian word, with the rising tone. Historically, the rising tone appeared when the place of the accent shifted to the preceding syllable (the so-called "Neoshtokavian retraction"), but the quality of this new accent was different – its melody still "gravitated" towards the original syllable. Most Shtokavian dialects (Neoshtokavian) dialects underwent this shift, but Chakavian, Kajkavian and the Old Shtokavian dialects did not.

Accent diacritics are not used in the ordinary orthography, but only in the linguistic or language-learning literature (e.g. dictionaries, orthography and grammar books). However, there are very few minimal pairs where an error in accent can lead to misunderstanding.

Serbo-Croatian orthography is almost entirely phonetic. Thus, most words should be spelled as they are pronounced. In practice, the writing system does not take into account allophones which occur as a result of interaction between words:

Also, there are some exceptions, mostly applied to foreign words and compounds, that favor morphological/etymological over phonetic spelling:

One systemic exception is that the consonant clusters ds and dš do not change into ts and tš (although "d" tends to be unvoiced in normal speech in such clusters):

Only a few words are intentionally "misspelled", mostly in order to resolve ambiguity:

Through history, this language has been written in a number of writing systems:

The oldest texts since the 11th century are in Glagolitic, and the oldest preserved text written completely in the Latin alphabet is "Red i zakon sestara reda Svetog Dominika", from 1345. The Arabic alphabet had been used by Bosniaks; Greek writing is out of use there, and Arabic and Glagolitic persisted so far partly in religious liturgies.

Today, it is written in both the Latin and Cyrillic scripts. Serbian and Bosnian variants use both alphabets, while Croatian uses the Latin only.

The Serbian Cyrillic alphabet was revised by Vuk Stefanović Karadžić in the 19th century.

The Croatian Latin alphabet ("Gajica") followed suit shortly afterwards, when Ljudevit Gaj defined it as standard Latin with five extra letters that had diacritics, apparently borrowing much from Czech, but also from Polish, and inventing the unique digraphs "lj", "nj" and "dž". These digraphs are represented as "ļ, ń and ǵ" respectively in the "Rječnik hrvatskog ili srpskog jezika", published by the former Yugoslav Academy of Sciences and Arts in Zagreb. The latter digraphs, however, are unused in the literary standard of the language. All in all, this makes Serbo-Croatian the only Slavic language to officially use both the Latin and Cyrillic scripts, albeit the Latin version is more commonly used.

In both cases, spelling is phonetic and spellings in the two alphabets map to each other one-to-one:

Latin to Cyrillic

Cyrillic to Latin

The digraphs "Lj", "Nj" and "Dž" represent distinct phonemes and are considered to be single letters. In crosswords, they are put into a single square, and in sorting, lj follows l and nj follows n, except in a few words where the individual letters are pronounced separately. For instance, "nadživ(j)eti" "to outlive" is composed of the prefix "nad-" "out, over" and the verb "živ(j)eti" "to live". The Cyrillic alphabet avoids such ambiguity by providing a single letter for each phoneme.

"Đ" used to be commonly written as "Dj" on typewriters, but that practice led to too many ambiguities. It is also used on car license plates. Today "Dj" is often used again in place of "Đ" on the Internet as a replacement due to the lack of installed Serbo-Croat keyboard layouts.

Unicode has separate characters for the digraphs lj (Ǉ, ǈ, ǉ), nj (Ǌ, ǋ, ǌ) and dž (Ǆ, ǅ, ǆ).

South Slavic historically formed a dialect continuum, i.e. each dialect has some similarities with the neighboring one, and differences grow with distance. However, migrations from the 16th to 18th centuries resulting from the spread of Ottoman Empire on the Balkans have caused large-scale population displacement that broke the dialect continuum into many geographical pockets. Migrations in the 20th century, primarily caused by urbanization and wars, also contributed to the reduction of dialectal differences.

The primary dialects are named after the most common question word for "what": Shtokavian uses the pronoun "što" or "šta", Chakavian uses "ča" or "ca", Kajkavian ("kajkavski"), "kaj" or "kej". In native terminology they are referred to as "nar(j)ečje", which would be equivalent of "group of dialects", whereas their many subdialects are referred to as "dijalekti ""dialects" or "govori ""speeches".

The pluricentric Serbo-Croatian standard language and all four contemporary standard variants are based on the Eastern Herzegovinian subdialect of Neo-Shtokavian. Other dialects are not taught in schools or used by the state media. The Torlakian dialect is often added to the list, though sources usually note that it is a transitional dialect between Shtokavian and the Bulgaro-Macedonian dialects.

The Serbo-Croatian dialects differ not only in the question word they are named after, but also heavily in phonology, accentuation and intonation, case endings and tense system (morphology) and basic vocabulary. In the past, Chakavian and Kajkavian dialects were spoken on a much larger territory, but have been replaced by Štokavian during the period of migrations caused by Ottoman Turkish conquest of the Balkans in the 15th and the 16th centuries. These migrations caused the koinéisation of the Shtokavian dialects, that used to form the West Shtokavian (more closer and transitional towards the neighbouring Chakavian and Kajkavian dialects) and East Shtokavian (transitional towards the Torlakian and the whole Bulgaro-Macedonian area) dialect bundles, and their subsequent spread at the expense of Chakavian and Kajkavian. As a result, Štokavian now covers an area larger than all the other dialects combined, and continues to make its progress in the enclaves where non-literary dialects are still being spoken.

The differences among the dialects can be illustrated on the example of Schleicher's fable. Diacritic signs are used to show the difference in accents and prosody, which are often quite significant, but which are not reflected in the usual orthography.

A basic distinction among the dialects is in the reflex of the long Common Slavic vowel "jat", usually transcribed as *ě. Depending on the reflex, the dialects are divided into Ikavian, Ekavian, and Ijekavian, with the reflects of "jat" being /i/, /e/, and /ije/ or /je/ respectively. The long and short "jat" is reflected as long or short */i/ and /e/ in Ikavian and Ekavian, but Ijekavian dialects introduce a "ije"/"je" alternation to retain a distinction.

Standard Croatian and Bosnian are based on Ijekavian, whereas Serbian uses both Ekavian and Ijekavian forms (Ijekavian for Bosnian Serbs, Ekavian for most of Serbia). Influence of standard language through state media and education has caused non-standard varieties to lose ground to the literary forms.

The jat-reflex rules are not without exception. For example, when short "jat" is preceded by "r", in most Ijekavian dialects developed into /re/ or, occasionally, /ri/. The prefix "prě-" ("trans-, over-") when long became "pre-" in eastern Ijekavian dialects but to "prije-" in western dialects; in Ikavian pronunciation, it also evolved into "pre-" or "prije-" due to potential ambiguity with "pri-" ("approach, come close to"). For verbs that had "-ěti " in their infinitive, the past participle ending "-ěl" evolved into "-io" in Ijekavian Neoštokavian.

The following are some examples:

Enisa Kafadar argues that there is only one Serbo-Croatian language with several varieties. This has made it possible to include all four varieties in a new grammar book. Daniel Bunčić concludes that it is a pluricentric language, with four standard variants spoken in Serbia, Croatia, Montenegro and Bosnia and Herzegovina. The mutual intelligibility between their speakers "exceeds that between the standard variants of English, French, German, or Spanish". Other linguists have argued that the differences between the variants of Serbo-Croatian are less significant than those between the variants of English, German, Dutch, and Hindi–Urdu.

Among pluricentric languages, Serbo-Croatian was the only one with a pluricentric standardisation within one state. The dissolution of Yugoslavia has made Serbo-Croatian even more of a typical pluricentric language, since the variants of other pluricentric languages are also spoken in different states.

The current Serbian constitution of 2006 refers to the official language as "Serbian", while the Montenegrin constitution of 2007 proclaimed "Montenegrin" as the primary official language, but also grants other languages the right of official use.

The International Organization for Standardization (ISO) has specified different Universal Decimal Classification (UDC) numbers for Croatian "(UDC 862," abbreviation hr) and Serbian "(UDC 861", abbreviation sr), while the cover term "Serbo-Croatian" is used to refer to the combination of original signs ("UDC 861/862," abbreviation sh). Furthermore, the "ISO 639" standard designates the Bosnian language with the abbreviations bos and bs.

The International Criminal Tribunal for the former Yugoslavia considers what it calls "BCS" (Bosnian-Croatian-Serbian) to be the main language of all Bosnian, Croatian, and Serbian defendants. The indictments, documents, and verdicts of the ICTY are not written with any regard for consistently following the grammatical prescriptions of any of the three standardsbe they Serbian, Croatian, or Bosnian.

For utilitarian purposes, the Serbo-Croatian language is often called ""naš jezik"" ("our language") or ""naški"" (sic. "Ourish" or "Ourian") by native speakers. This politically correct term is frequently used to describe the Serbo-Croatian language by those who wish to avoid nationalistic and linguistic discussions.

The majority of mainstream Serbian linguists consider Serbian and Croatian to be one language, that is called Serbo-Croatian ("srpskohrvatski") or Croato-Serbian ("hrvatskosrpski"). A minority of Serbian linguists are of the opinion that Serbo-Croatian did exist, but has, in the meantime, dissolved.

The opinion of the majority of Croatian linguists is that there has never been a Serbo-Croatian language, but two different standard languages that overlapped sometime in the course of history. However, Croatian linguist Snježana Kordić has been leading an academic discussion on this issue in the Croatian journal "Književna republika" from 2001 to 2010. In the discussion, she shows that linguistic criteria such as mutual intelligibility, the huge overlap in the linguistic system, and the same dialect basis of the standard language are evidence that Croatian, Serbian, Bosnian and Montenegrin are four national variants of the pluricentric Serbo-Croatian language. Igor Mandić states: "During the last ten years, it has been the longest, the most serious and most acrid discussion (…) in 21st-century Croatian culture". Inspired by that discussion, a monograph on language and nationalism has been published.

The view of the majority of Croatian linguists that there is no single Serbo-Croatian language but several different standard languages has been sharply criticized by German linguist Bernhard Gröschel in his monograph "Serbo-Croatian Between Linguistics and Politics".

A more detailed overview, incorporating arguments from Croatian philology and contemporary linguistics, would be as follows:

The linguistic debate in this region is more about politics than about linguistics per se.

The topic of language for writers from Dalmatia and Dubrovnik prior to the 19th century made a distinction only between speakers of Italian or Slavic, since those were the two main groups that inhabited Dalmatian city-states at that time. Whether someone spoke Croatian or Serbian was not an important distinction then, as the two languages were not distinguished by most speakers. This has been used as an argument to state that Croatian literature Croatian per se, but also includes Serbian and other languages that are part of Serbo-Croatian, These facts undermine the Croatian language proponents' argument that modern-day Croatian is based on a language called Old Croatian.

However, most intellectuals and writers from Dalmatia who used the Štokavian dialect and practiced the Catholic faith saw themselves as part of a Croatian nation as far back as the mid-16th to 17th centuries, some 300 years before Serbo-Croatian ideology appeared. Their loyalty was first and foremost to Catholic Christendom, but when they professed an ethnic identity, they referred to themselves as "Slovin" and "Illyrian" (a sort of forerunner of Catholic baroque pan-Slavism) and Croatthese 30-odd writers over the span of c. 350 years always saw themselves as Croats first and never as part of a Serbian nation. It should also be noted that, in the pre-national era, Catholic religious orientation did not necessarily equate with Croat ethnic identity in Dalmatia. A Croatian follower of Vuk Karadžić, Ivan Broz, noted that for a Dalmatian to identify oneself as a Serb was seen as foreign as identifying oneself as Macedonian or Greek. Vatroslav Jagić pointed out in 1864:

On the other hand, the opinion of Jagić from 1864 is argued not to have firm grounds. When Jagić says "Croatian", he refers to a few cases referring to the Dubrovnik vernacular as "ilirski" (Illyrian). This was a common name for all Slavic vernaculars in Dalmatian cities among the Roman inhabitants. In the meantime, other written monuments are found that mention "srpski", "lingua serviana" (= Serbian), and some that mention Croatian. By far the most competent Serbian scientist on the Dubrovnik language issue, Milan Rešetar, who was born in Dubrovnik himself, wrote behalf of language characteristics: "The one who thinks that Croatian and Serbian are two separate languages must confess that Dubrovnik always (linguistically) used to be Serbian."

Finally, the former "medieval" texts from Dubrovnik and Montenegro dating before the 16th century were neither true Štokavian nor Serbian, but mostly specific a Jekavian-Čakavian that was nearer to actual Adriatic islanders in Croatia.

Nationalists have conflicting views about the language(s). The nationalists among the Croats conflictingly claim either that they speak an entirely separate language from Serbs and Bosniaks or that these two peoples have, due to the longer lexicographic tradition among Croats, somehow "borrowed" their standard languages from them. Bosniak nationalists claim that both Croats and Serbs have "appropriated" the Bosnian language, since Ljudevit Gaj and Vuk Karadžić preferred the Neoštokavian-Ijekavian dialect, widely spoken in Bosnia and Herzegovina, as the basis for language standardization, whereas the nationalists among the Serbs claim either that any divergence in the language is artificial, or claim that the Štokavian dialect is theirs and the Čakavian Croats'— in more extreme formulations Croats have "taken" or "stolen" their language from the Serbs. 

Proponents of unity among Southern Slavs claim that there is a single language with normal dialectal variations. The term "Serbo-Croatian" (or synonyms) is not officially used in any of the successor countries of former Yugoslavia.

In Serbia, the Serbian standard has an official status countrywide, while both Serbian and Croatian are official in the province of Vojvodina. A large Bosniak minority is present in the southwest region of Sandžak, but the "official recognition" of Bosnian language is moot. Bosnian is an optional course in 1st and 2nd grade of the elementary school, while it is also in official use in the municipality of Novi Pazar. However, its nomenclature is controversial, as there is incentive that it is referred to as "Bosniak" ("bošnjački") rather than "Bosnian" ("bosanski") (see Bosnian language#Controversy and recognition for details).

Croatian is the official language of Croatia, while Serbian is also official in municipalities with significant Serb population.

In Bosnia and Herzegovina, all three standard languages are recorded as official but in practice and media, mostly Bosnian and Serbian are applied. Confrontations have on occasion been absurd. The academic Muhamed Filipović, in an interview to Slovenian television, told of a local court in a Croatian district requesting a paid translator to translate from Bosnian to Croatian before the trial could proceed.







</doc>
