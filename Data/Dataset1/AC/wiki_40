<doc id="29374" url="https://en.wikipedia.org/wiki?curid=29374" title="Steam turbine">
Steam turbine

A steam turbine is a device that extracts thermal energy from pressurized steam and uses it to do mechanical work on a rotating output shaft. Its modern manifestation was invented by Sir Charles Parsons in 1884.

Because the turbine generates rotary motion, it is particularly suited to be used to drive an electrical generator – about 90% of all electricity generation in the United States in the year 1996 was by use of steam turbines. The steam turbine is a form of heat engine that derives much of its improvement in thermodynamic efficiency from the use of multiple stages in the expansion of the steam, which results in a closer approach to the ideal reversible expansion process.

The first device that may be classified as a reaction steam turbine was little more than a toy, the classic Aeolipile, described in the 1st century by Hero of Alexandria in Roman Egypt. In 1551, Taqi al-Din in Ottoman Egypt described a steam turbine with the practical application of rotating a spit. Steam turbines were also described by the Italian Giovanni Branca (1629) and John Wilkins in England (1648). The devices described by Taqi al-Din and Wilkins are today known as steam jacks. In 1672 an impulse steam turbine driven car was designed by Ferdinand Verbiest. A more modern version of this car was produced some time in the late 18th century by an unknown German mechanic. In 1775 at Soho James Watt designed a reaction turbine that was put to work there. In 1827 the Frenchmen Real and Pichon patented and constructed a compound impulse turbine.

The modern steam turbine was invented in 1884 by Sir Charles Parsons, whose first model was connected to a dynamo that generated 7.5 kW (10 hp) of electricity. The invention of Parsons' steam turbine made cheap and plentiful electricity possible and revolutionized marine transport and naval warfare. Parsons' design was a reaction type. His patent was licensed and the turbine scaled-up shortly after by an American, George Westinghouse. The Parsons turbine also turned out to be easy to scale up. Parsons had the satisfaction of seeing his invention adopted for all major world power stations, and the size of generators had increased from his first 7.5 kW set up to units of 50,000 kW capacity. Within Parson's lifetime, the generating capacity of a unit was scaled up by about 10,000 times, and the total output from turbo-generators constructed by his firm C. A. Parsons and Company and by their licensees, for land purposes alone, had exceeded thirty million horse-power.

A number of other variations of turbines have been developed that work effectively with steam. The "de Laval turbine" (invented by Gustaf de Laval) accelerated the steam to full speed before running it against a turbine blade. De Laval's impulse turbine is simpler, less expensive and does not need to be pressure-proof. It can operate with any pressure of steam, but is considerably less efficient. Auguste Rateau developed a pressure compounded impulse turbine using the de Laval principle as early as 1896, obtained a US patent in 1903, and applied the turbine to a French torpedo boat in 1904. He taught at the École des mines de Saint-Étienne for a decade until 1897, and later founded a successful company that was incorporated into the Alstom firm after his death. One of the founders of the modern theory of steam and gas turbines was Aurel Stodola, a Slovak physicist and engineer and professor at the Swiss Polytechnical Institute (now ETH) in Zurich. His work "Die Dampfturbinen und ihre Aussichten als Wärmekraftmaschinen" (English: The Steam Turbine and its prospective use as a Heat Engine) was published in Berlin in 1903. A further book "Dampf und Gas-Turbinen" (English: Steam and Gas Turbines) was published in 1922.

The "Brown-Curtis turbine", an impulse type, which had been originally developed and patented by the U.S. company International Curtis Marine Turbine Company, was developed in the 1900s in conjunction with John Brown & Company. It was used in John Brown-engined merchant ships and warships, including liners and Royal Navy warships.

The present-day manufacturing industry for steam turbines is dominated by Chinese power equipment makers. Harbin Electric, Shanghai Electric, and Dongfang Electric, the top three power equipment makers in China, collectively hold a majority stake in the worldwide market share for steam turbines in 2009-10 according to Platts. Other manufacturers with minor market share include Bharat Heavy Electricals Limited, Siemens, Alstom, General Electric, Doosan Škoda Power, Mitsubishi Heavy Industries, and Toshiba. The consulting firm Frost & Sullivan projects that manufacturing of steam turbines will become more consolidated by 2020 as Chinese power manufacturers win increasing business outside of China.

Steam turbines are made in a variety of sizes ranging from small <0.75 kW (<1 hp) units (rare) used as mechanical drives for pumps, compressors and other shaft driven equipment, to turbines used to generate electricity. There are several classifications for modern steam turbines.

Turbine blades are of two basic types, blades and nozzles. Blades move entirely due to the impact of steam on them and their profiles do not converge. This results in a steam velocity drop and essentially no pressure drop as steam moves through the blades. A turbine composed of blades alternating with fixed nozzles is called an impulse turbine, , Rateau turbine, or Brown-Curtis turbine. Nozzles appear similar to blades, but their profiles converge near the exit. This results in a steam pressure drop and velocity increase as steam moves through the nozzles. Nozzles move due to both the impact of steam on them and the reaction due to the high-velocity steam at the exit. A turbine composed of moving nozzles alternating with fixed nozzles is called a reaction turbine or Parsons turbine.

Except for low-power applications, turbine blades are arranged in multiple stages in series, called compounding, which greatly improves efficiency at low speeds. A reaction stage is a row of fixed nozzles followed by a row of moving nozzles. Multiple reaction stages divide the pressure drop between the steam inlet and exhaust into numerous small drops, resulting in a pressure-compounded turbine. Impulse stages may be either pressure-compounded, velocity-compounded, or pressure-velocity compounded. A pressure-compounded impulse stage is a row of fixed nozzles followed by a row of moving blades, with multiple stages for compounding. This is also known as a Rateau turbine, after its inventor. A velocity-compounded impulse stage (invented by Curtis and also called a "Curtis wheel") is a row of fixed nozzles followed by two or more rows of moving blades alternating with rows of fixed blades. This divides the velocity drop across the stage into several smaller drops. A series of velocity-compounded impulse stages is called a pressure-velocity compounded turbine.

By 1905, when steam turbines were coming into use on fast ships (such as ) and in land-based power applications, it had been determined that it was desirable to use one or more Curtis wheels at the beginning of a multi-stage turbine (where the steam pressure is highest), followed by reaction stages. This was more efficient with high-pressure steam due to reduced leakage between the turbine rotor and the casing. This is illustrated in the drawing of the German 1905 AEG marine steam turbine. The steam from the boilers enters from the right at high pressure through a throttle, controlled manually by an operator (in this case a sailor known as the throttleman). It passes through five Curtis wheels and numerous reaction stages (the small blades at the edges of the two large rotors in the middle) before exiting at low pressure, almost certainly to a condenser. The condenser provides a vacuum that maximizes the energy extracted from the steam, and condenses the steam into feedwater to be returned to the boilers. On the left are several additional reaction stages (on two large rotors) that rotate the turbine in reverse for astern operation, with steam admitted by a separate throttle. Since ships are rarely operated in reverse, efficiency is not a priority in astern turbines, so only a few stages are used to save cost.

A major challenge facing turbine design was reducing the creep experienced by the blades. Because of the high temperatures and high stresses of operation, steam turbine materials become damaged through these mechanisms. As temperatures are increased in an effort to improve turbine efficiency, creep becomes significant. To limit creep, thermal coatings and superalloys with solid-solution strengthening and grain boundary strengthening are used in blade designs.

Protective coatings are used to reduce the thermal damage and to limit oxidation. These coatings are often stabilized zirconium dioxide-based ceramics. Using a thermal protective coating limits the temperature exposure of the nickel superalloy. This reduces the creep mechanisms experienced in the blade. Oxidation coatings limit efficiency losses caused by a buildup on the outside of the blades, which is especially important in the high-temperature environment.

The nickel-based blades are alloyed with aluminum and titanium to improve strength and creep resistance. The microstructure of these alloys is composed of different regions of composition. A uniform dispersion of the gamma-prime phase – a combination of nickel, aluminum, and titanium – promotes the strength and creep resistance of the blade due to the microstructure.

Refractory elements such as rhenium and ruthenium can be added to the alloy to improve creep strength. The addition of these elements reduces the diffusion of the gamma prime phase, thus preserving the fatigue resistance, strength, and creep resistance.

These types include condensing, non-condensing, reheat, extraction and induction.

Condensing turbines are most commonly found in electrical power plants. These turbines receive steam from a boiler and exhaust it to a condenser. The exhausted steam is at a pressure well below atmospheric, and is in a partially condensed state, typically of a quality near 90%.

Non-condensing or back pressure turbines are most widely used for process steam applications. The exhaust pressure is controlled by a regulating valve to suit the needs of the process steam pressure. These are commonly found at refineries, district heating units, pulp and paper plants, and desalination facilities where large amounts of low pressure process steam are needed.

Reheat turbines are also used almost exclusively in electrical power plants. In a reheat turbine, steam flow exits from a high-pressure section of the turbine and is returned to the boiler where additional superheat is added. The steam then goes back into an intermediate pressure section of the turbine and continues its expansion. Using reheat in a cycle increases the work output from the turbine and also the expansion reaches conclusion before the steam condenses, thereby minimizing the erosion of the blades in last rows. In most of the cases, maximum number of reheats employed in a cycle is 2 as the cost of super-heating the steam negates the increase in the work output from turbine.

Extracting type turbines are common in all applications. In an extracting type turbine, steam is released from various stages of the turbine, and used for industrial process needs or sent to boiler feedwater heaters to improve overall cycle efficiency. Extraction flows may be controlled with a valve, or left uncontrolled. Extracted steam results in a loss of power in the downstream stages of the turbine.

Induction turbines introduce low pressure steam at an intermediate stage to produce additional power.

These arrangements include single casing, tandem compound and cross compound turbines. Single casing units are the most basic style where a single casing and shaft are coupled to a generator. Tandem compound are used where two or more casings are directly coupled together to drive a single generator. A cross compound turbine arrangement features two or more shafts not in line driving two or more generators that often operate at different speeds. A cross compound turbine is typically used for many large applications. A typical 1930s-1960s naval installation is illustrated below; this shows high- and low-pressure turbines driving a common reduction gear, with a geared cruising turbine on one high-pressure turbine.

The moving steam imparts both a tangential and axial thrust on the turbine shaft, but the axial thrust in a simple turbine is unopposed. To maintain the correct rotor position and balancing, this force must be counteracted by an opposing force. Thrust bearings can be used for the shaft bearings, the rotor can use dummy pistons, it can be double flow- the steam enters in the middle of the shaft and exits at both ends, or a combination of any of these. In a double flow rotor, the blades in each half face opposite ways, so that the axial forces negate each other but the tangential forces act together. This design of rotor is also called two-flow, double-axial-flow, or double-exhaust. This arrangement is common in low-pressure casings of a compound turbine.

An ideal steam turbine is considered to be an isentropic process, or constant entropy process, in which the entropy of the steam entering the turbine is equal to the entropy of the steam leaving the turbine. No steam turbine is truly isentropic, however, with typical isentropic efficiencies ranging from 20–90% based on the application of the turbine. The interior of a turbine comprises several sets of blades or "buckets". One set of stationary blades is connected to the casing and one set of rotating blades is connected to the shaft. The sets intermesh with certain minimum clearances, with the size and configuration of sets varying to efficiently exploit the expansion of steam at each stage.

Practical thermal efficiency of a steam turbine varies with turbine size, load condition, gap losses and friction losses. They reach top values up to about 50% in a 1200 MW turbine; smaller ones have a lower efficiency. To maximize turbine efficiency the steam is expanded, doing work, in a number of stages. These stages are characterized by how the energy is extracted from them and are known as either impulse or reaction turbines. Most steam turbines use a mixture of the reaction and impulse designs: each stage behaves as either one or the other, but the overall turbine uses both. Typically, lower pressure sections are reaction type and higher pressure stages are impulse type.

An impulse turbine has fixed nozzles that orient the steam flow into high speed jets. These jets contain significant kinetic energy, which is converted into shaft rotation by the bucket-like shaped rotor blades, as the steam jet changes direction. A pressure drop occurs across only the stationary blades, with a net increase in steam velocity across the stage.
As the steam flows through the nozzle its pressure falls from inlet pressure to the exit pressure (atmospheric pressure, or more usually, the condenser vacuum). Due to this high ratio of expansion of steam, the steam leaves the nozzle with a very high velocity. The steam leaving the moving blades has a large portion of the maximum velocity of the steam when leaving the nozzle. The loss of energy due to this higher exit velocity is commonly called the carry over velocity or leaving loss.

The law of moment of momentum states that the sum of the moments of external forces acting on a fluid which is temporarily occupying the control volume is equal to the net time change of angular momentum flux through the control volume.

The swirling fluid enters the control volume at radius formula_1 with tangential velocity formula_2 and leaves at radius formula_3 with tangential velocity formula_4.

A velocity triangle paves the way for a better understanding of the relationship between the various velocities. In the adjacent figure we have:

Then by the law of moment of momentum, the torque on the fluid is given by:

formula_17

For an impulse steam turbine: formula_18. Therefore, the tangential force on the blades is formula_19. The work done per unit time or power developed: formula_20.

When ω is the angular velocity of the turbine, then the blade speed is formula_21. The power developed is then formula_22.

Blade efficiency

Blade efficiency (formula_23) can be defined as the ratio of the work done on the blades to kinetic energy supplied to the fluid, and is given by

formula_24

Stage efficiency
A stage of an impulse turbine consists of a nozzle set and a moving wheel. The stage efficiency defines a relationship between enthalpy drop in the nozzle and work done in the stage.

formula_25

Where formula_26 is the specific enthalpy drop of steam in the nozzle.

By the first law of thermodynamics: formula_27

Assuming that formula_5 is appreciably less than formula_6, we get formula_30 ≈ formula_31
Furthermore, stage efficiency is the product of blade efficiency and nozzle efficiency, or formula_32

Nozzle efficiency is given by formula_33 = formula_34, where the enthalpy (in J/Kg) of steam at the entrance of the nozzle is formula_35 and the enthalpy of steam at the exit of the nozzle is formula_36.
formula_37
formula_38
formula_39
formula_40

The ratio of the cosines of the blade angles at the outlet and inlet can be taken and denoted formula_41.
The ratio of steam velocities relative to the rotor speed at the outlet to the inlet of the blade is defined by the friction coefficient formula_42.

formula_43 and depicts the loss in the relative velocity due to friction as the steam flows around the blades (formula_44 for smooth blades).

formula_45

The ratio of the blade speed to the absolute steam velocity at the inlet is termed the blade speed ratio formula_46 = formula_47

formula_23 is maximum when formula_49 or, formula_50. That implies formula_51 and therefore formula_52. Now formula_53 (for a single stage impulse turbine)

Therefore, the maximum value of stage efficiency is obtained by putting the value of formula_52 in the expression of formula_23/

We get: formula_56.

For equiangular blades, formula_57, therefore formula_58, and we get formula_59. If the friction due to the blade surface is neglected then formula_60.

Conclusions on maximum efficiency

formula_60

1. For a given steam velocity work done per kg of steam would be maximum when formula_62 or formula_63.

2. As formula_64 increases, the work done on the blades reduces, but at the same time surface area of the blade reduces, therefore there are less frictional losses.

In the "reaction turbine", the rotor blades themselves are arranged to form convergent nozzles. This type of turbine makes use of the reaction force produced as the steam accelerates through the nozzles formed by the rotor. Steam is directed onto the rotor by the fixed vanes of the stator. It leaves the stator as a jet that fills the entire circumference of the rotor. The steam then changes direction and increases its speed relative to the speed of the blades. A pressure drop occurs across both the stator and the rotor, with steam accelerating through the stator and decelerating through the rotor, with no net change in steam velocity across the stage but with a decrease in both pressure and temperature, reflecting the work performed in the driving of the rotor.

Blade efficiency

Energy input to the blades in a stage:

formula_65 is equal to the kinetic energy supplied to the fixed blades (f) + the kinetic energy supplied to the moving blades (m).

Or, formula_66 = enthalpy drop over the fixed blades, formula_67 + enthalpy drop over the moving blades, formula_68.

The effect of expansion of steam over the moving blades is to increase the relative velocity at the exit. Therefore, the relative velocity at the exit formula_12 is always greater than the relative velocity at the inlet formula_11.

In terms of velocities, the enthalpy drop over the moving blades is given by:

formula_71

The enthalpy drop in the fixed blades, with the assumption that the velocity of steam entering the fixed blades is equal to the velocity of steam leaving the previously moving blades is given by:

formula_67 = formula_73 where V is the inlet velocity of steam in the nozzle

formula_74 is very small and hence can be neglected

Therefore, formula_67 = formula_76

formula_77

formula_78

A very widely used design has half degree of reaction or 50% reaction and this is known as Parson’s turbine. This consists of symmetrical rotor and stator blades.
For this turbine the velocity triangle is similar and we have:

formula_79, formula_80

formula_81, formula_82

Assuming "Parson’s turbine" and obtaining all the expressions we get

formula_83

From the inlet velocity triangle we have formula_84

formula_85

formula_86

Work done (for unit mass flow per second): formula_87

Therefore, the blade efficiency is given by

formula_88

Condition of maximum blade efficiency

If formula_89, then

formula_90

For maximum efficiency formula_91, we get

formula_92

and this finally gives formula_93

Therefore, formula_94 is found by putting the value of formula_95 in the expression of blade efficiency

formula_96

formula_97

Because of the high pressures used in the steam circuits and the materials used, steam turbines and their casings have high thermal inertia. When warming up a steam turbine for use, the main steam stop valves (after the boiler) have a bypass line to allow superheated steam to slowly bypass the valve and proceed to heat up the lines in the system along with the steam turbine. Also, a turning gear is engaged when there is no steam to slowly rotate the turbine to ensure even heating to prevent uneven expansion. After first rotating the turbine by the turning gear, allowing time for the rotor to assume a straight plane (no bowing), then the turning gear is disengaged and steam is admitted to the turbine, first to the astern blades then to the ahead blades slowly rotating the turbine at 10–15 RPM (0.17–0.25 Hz) to slowly warm the turbine. The warm-up procedure for large steam turbines may exceed ten hours.

During normal operation, rotor imbalance can lead to vibration, which, because of the high rotation velocities, could lead to a blade breaking away from the rotor and through the casing. To reduce this risk, considerable efforts are spent to balance the turbine. Also, turbines are run with high-quality steam: either superheated (dry) steam, or saturated steam with a high dryness fraction. This prevents the rapid impingement and erosion of the blades which occurs when condensed water is blasted onto the blades (moisture carry over). Also, liquid water entering the blades may damage the thrust bearings for the turbine shaft. To prevent this, along with controls and baffles in the boilers to ensure high-quality steam, condensate drains are installed in the steam piping leading to the turbine.

Maintenance requirements of modern steam turbines are simple and incur low costs (typically around $0.005 per kWh); their operational life often exceeds 50 years.

The control of a turbine with a governor is essential, as turbines need to be run up slowly to prevent damage and some applications (such as the generation of alternating current electricity) require precise speed control. Uncontrolled acceleration of the turbine rotor can lead to an overspeed trip, which causes the governor and throttle valves that control the flow of steam to the turbine to close. If these valves fail then the turbine may continue accelerating until it breaks apart, often catastrophically. Turbines are expensive to make, requiring precision manufacture and special quality materials.

During normal operation in synchronization with the electricity network, power plants are governed with a five percent droop speed control. This means the full load speed is 100% and the no-load speed is 105%. This is required for the stable operation of the network without hunting and drop-outs of power plants. Normally the changes in speed are minor. Adjustments in power output are made by slowly raising the droop curve by increasing the spring pressure on a centrifugal governor. Generally this is a basic system requirement for all power plants because the older and newer plants have to be compatible in response to the instantaneous changes in frequency without depending on outside communication.

The steam turbine operates on basic principles of thermodynamics using the part 3-4 of the Rankine cycle shown in the adjoining diagram. Superheated steam (or dry saturated steam, depending on application) leaves the boiler at high temperature and high pressure. At entry to the turbine, the steam gains kinetic energy by passing through a nozzle (a fixed nozzle in an impulse type turbine or the fixed blades in a reaction type turbine). When the steam leaves the nozzle it is moving at high velocity towards the blades of the turbine rotor. A force is created on the blades due to the pressure of the vapor on the blades causing them to move. A generator or other such device can be placed on the shaft, and the energy that was in the steam can now be stored and used. The steam leaves the turbine as a saturated vapor (or liquid-vapor mix depending on application) at a lower temperature and pressure than it entered with and is sent to the condenser to be cooled. The first law enables us to find a formula for the rate at which work is developed per unit mass. Assuming there is no heat transfer to the surrounding environment and that the changes in kinetic and potential energy are negligible compared to the change in specific enthalpy we arrive at the following equation

where

To measure how well a turbine is performing we can look at its isentropic efficiency. This compares the actual performance of the turbine with the performance that would be achieved by an ideal, isentropic, turbine. When calculating this efficiency, heat lost to the surroundings is assumed to be zero. Steam's starting pressure and temperature is the same for both the actual and the ideal turbines, but at turbine exit, steam's energy content ('specific enthalpy') for the actual turbine is greater than that for the ideal turbine because of irreversibility in the actual turbine. The specific enthalpy is evaluated at the same steam pressure for the actual and ideal turbines in order to give a good comparison between the two.

The isentropic efficiency is found by dividing the actual work by the ideal work.

where

Electrical power stations use large steam turbines driving electric generators to produce most (about 80%) of the world's electricity. The advent of large steam turbines made central-station electricity generation practical, since reciprocating steam engines of large rating became very bulky, and operated at slow speeds. Most central stations are fossil fuel power plants and nuclear power plants; some installations use geothermal steam, or use concentrated solar power (CSP) to create the steam. Steam turbines can also be used directly to drive large centrifugal pumps, such as feedwater pumps at a thermal power plant.

The turbines used for electric power generation are most often directly coupled to their generators. As the generators must rotate at constant synchronous speeds according to the frequency of the electric power system, the most common speeds are 3,000 RPM for 50 Hz systems, and 3,600 RPM for 60 Hz systems. Since nuclear reactors have lower temperature limits than fossil-fired plants, with lower steam quality, the turbine generator sets may be arranged to operate at half these speeds, but with four-pole generators, to reduce erosion of turbine blades.

In steamships, advantages of steam turbines over reciprocating engines are smaller size, lower maintenance, lighter weight, and lower vibration. A steam turbine is only efficient when operating in the thousands of RPM, while the most effective propeller designs are for speeds less than 300 RPM; consequently, precise (thus expensive) reduction gears are usually required, although numerous early ships through World War I, such as "Turbinia", had direct drive from the steam turbines to the propeller shafts. Another alternative is turbo-electric transmission, in which an electrical generator run by the high-speed turbine is used to run one or more slow-speed electric motors connected to the propeller shafts; precision gear cutting may be a production bottleneck during wartime. Turbo-electric drive was most used in large US warships designed during World War I and in some fast liners, and was used in some troop transports and mass-production destroyer escorts in World War II.

The higher cost of turbines and the associated gears or generator/motor sets is offset by lower maintenance requirements and the smaller size of a turbine when compared to a reciprocating engine having an equivalent power, although the fuel costs are higher than a diesel engine because steam turbines have lower thermal efficiency. To reduce fuel costs the thermal efficiency of both types of engine have been improved over the years. Today, propulsion steam turbine cycle efficiencies have yet to break 50%, yet diesel engines routinely exceed 50%, especially in marine applications. Diesel power plants also have lower operating costs since fewer operators are required. Thus, conventional steam power is used in very few new ships. An exception is LNG carriers which often find it more economical to use boil-off gas with a steam turbine than to re-liquify it.

Nuclear-powered ships and submarines use a nuclear reactor to create steam for turbines. Nuclear power is often chosen where diesel power would be impractical (as in submarine applications) or the logistics of refuelling pose significant problems (for example, icebreakers). It has been estimated that the reactor fuel for the Royal Navy's s is sufficient to last 40 circumnavigations of the globe – potentially sufficient for the vessel's entire service life. Nuclear propulsion has only been applied to a very few commercial vessels due to the expense of maintenance and the regulatory controls required on nuclear systems and fuel cycles. 

The development of steam turbine marine propulsion from 1894-1935 was dominated by the need to reconcile the high efficient speed of the turbine with the low efficient speed (less than 300 rpm) of the ship's propeller at an overall cost competitive with reciprocating engines. In 1894, efficient reduction gears were not available for the high powers required by ships, so direct drive was necessary. In "Turbinia", which has direct drive to each propeller shaft, the efficient speed of the turbine was reduced after initial trials by directing the steam flow through all three direct drive turbines (one on each shaft) in series, probably totaling around 200 turbine stages operating in series. Also, there were three propellers on each shaft for operation at high speeds. The high shaft speeds of the era are represented by one of the first US turbine-powered destroyers, , launched in 1909, which had direct drive turbines and whose three shafts turned at 724 rpm at 28.35 knots. The use of turbines in several casings exhausting steam to each other in series became standard in most subsequent marine propulsion applications, and is a form of cross-compounding. The first turbine was called the high pressure (HP) turbine, the last turbine was the low pressure (LP) turbine, and any turbine in between was an intermediate pressure (IP) turbine. A much later arrangement than "Turbinia" can be seen on in Long Beach, California, launched in 1934, in which each shaft is powered by four turbines in series connected to the ends of the two input shafts of a single-reduction gearbox. They are the HP, 1st IP, 2nd IP, and LP turbines.

The quest for economy was even more important when cruising speeds were considered. Cruising speed is roughly 50% of a warship's maximum speed and 20-25% of its maximum power level. This would be a speed used on long voyages when fuel economy is desired. Although this brought the propeller speeds down to an efficient range, turbine efficiency was greatly reduced, and early turbine ships had poor cruising ranges. A solution that proved useful through most of the steam turbine propulsion era was the cruising turbine. This was an extra turbine to add even more stages, at first attached directly to one or more shafts, exhausting to a stage partway along the HP turbine, and not used at high speeds. As reduction gears became available around 1911, some ships, notably the battleship , had them on cruising turbines while retaining direct drive main turbines. Reduction gears allowed turbines to operate in their efficient range at a much higher speed than the shaft, but were expensive to manufacture.

Cruising turbines competed at first with reciprocating engines for fuel economy. An example of the retention of reciprocating engines on fast ships was the famous of 1911, which along with her sisters and had triple-expansion engines on the two outboard shafts, both exhausting to an LP turbine on the center shaft. After adopting turbines with the s launched in 1909, the United States Navy reverted to reciprocating machinery on the s of 1912, then went back to turbines on "Nevada" in 1914. The lingering fondness for reciprocating machinery was because the US Navy had no plans for capital ships exceeding 21 knots until after World War I, so top speed was less important than economical cruising. The United States had acquired the Philippines and Hawaii as territories in 1898, and lacked the British Royal Navy's worldwide network of coaling stations. Thus, the US Navy in 1900-1940 had the greatest need of any nation for fuel economy, especially as the prospect of war with Japan arose following World War I. This need was compounded by the US not launching any cruisers 1908-1920, so destroyers were required to perform long-range missions usually assigned to cruisers. So, various cruising solutions were fitted on US destroyers launched 1908-1916. These included small reciprocating engines and geared or ungeared cruising turbines on one or two shafts. However, once fully geared turbines proved economical in initial cost and fuel they were rapidly adopted, with cruising turbines also included on most ships. Beginning in 1915 all new Royal Navy destroyers had fully geared turbines, and the United States followed in 1917.

In the Royal Navy, speed was a priority until the Battle of Jutland in mid-1916 showed that in the battlecruisers too much armour had been sacrificed in its pursuit. The British used exclusively turbine-powered warships from 1906. Because they recognized that a significant cruising range would be desirable given their worldwide empire, some warships, notably the s, were fitted with cruising turbines from 1912 onwards following earlier experimental installations.

In the US Navy, the s, launched 1935–36, introduced double-reduction gearing. This further increased the turbine speed above the shaft speed, allowing smaller turbines than single-reduction gearing. Steam pressures and temperatures were also increasing progressively, from 300 psi/425 F (2.07 MPa/218 C – saturation temperature) on the World War I-era to 615 psi/850 F (4.25 MPa/454 C) superheated steam on some World War II s and later ships. A standard configuration emerged of an axial-flow high-pressure turbine (sometimes with a cruising turbine attached) and a double-axial-flow low-pressure turbine connected to a double-reduction gearbox. This arrangement continued throughout the steam era in the US Navy and was also used in some Royal Navy designs. Machinery of this configuration can be seen on many preserved World War II-era warships in several countries. When US Navy warship construction resumed in the early 1950s, most surface combatants and aircraft carriers used 1,200 psi/950 F (8.28 MPa/510 C) steam. This continued until the end of the US Navy steam-powered warship era with the s of the early 1970s. Amphibious and auxiliary ships continued to use 600 psi (4.14 MPa) steam post-World War II, with , launched in 2001, possibly the last non-nuclear steam-powered ship built for the US Navy.

Turbo-electric drive was introduced on the battleship , launched in 1917. Over the next eight years the US Navy launched five additional turbo-electric-powered battleships and two aircraft carriers (initially ordered as s). Ten more turbo-electric capital ships were planned, but cancelled due to the limits imposed by the Washington Naval Treaty. Although "New Mexico" was refitted with geared turbines in a 1931-33 refit, the remaining turbo-electric ships retained the system throughout their careers. This system used two large steam turbine generators to drive an electric motor on each of four shafts. The system was less costly initially than reduction gears and made the ships more maneuverable in port, with the shafts able to reverse rapidly and deliver more reverse power than with most geared systems. Some ocean liners were also built with turbo-electric drive, as were some troop transports and mass-production destroyer escorts in World War II. However, when the US designed the "treaty cruisers", beginning with launched in 1927, geared turbines were used to conserve weight, and remained in use for all fast steam-powered ships thereafter.

Since the 1980s, steam turbines have been replaced by gas turbines on fast ships and by diesel engines on other ships; exceptions are nuclear-powered ships and submarines and LNG carriers. Some auxiliary ships continue to use steam propulsion. In the U.S. Navy, the conventionally powered steam turbine is still in use on all but one of the Wasp-class amphibious assault ships. The Royal Navy decommissioned its last conventional steam-powered surface warship class, the , in 2002. In 2013, the French Navy ended its steam era with the decommissioning of its last . Amongst the other blue-water navies, the Russian Navy currently operates steam-powered s and s. The Indian Navy currently operates INS "Vikramaditya", a modified ; it also operates three s commissioned in the early 2000s and one scheduled for decommissioning. The Chinese Navy currently operates steam-powered s, s along with s and the lone Type 051B destroyer.

Most other naval forces either retired or re-engined their steam-powered warships by 2010. As of 2017, the Mexican Navy operates four steam-powered former U.S. s and two steam-powered former U.S. s. The Egyptian Navy and the Republic of China Navy respectively operate two and six former U.S. s. The Ecuadorian Navy currently operates two steam-powered s (modified s).

A steam turbine locomotive engine is a steam locomotive driven by a steam turbine.

The main advantages of a steam turbine locomotive are better rotational balance and reduced hammer blow on the track. However, a disadvantage is less flexible output power so that turbine locomotives were best suited for long-haul operations at a constant output power.

The first steam turbine rail locomotive was built in 1908 for the Officine Meccaniche Miani Silvestri Grodona Comi, Milan, Italy. In 1924 Krupp built the steam turbine locomotive T18 001, operational in 1929, for Deutsche Reichsbahn.

British, German, other national and international test codes are used to standardize the procedures and definitions used to test steam turbines. Selection of the test code to be used is an agreement between the purchaser and the manufacturer, and has some significance to the design of the turbine and associated systems. In the United States, ASME has produced several performance test codes on steam turbines. These include ASME PTC 6-2004, Steam Turbines, ASME PTC 6.2-2011, Steam Turbines in Combined Cycles, PTC 6S-1988, Procedures for Routine Performance Test of Steam Turbines. These ASME performance test codes have gained international recognition and acceptance for testing steam turbines. The single most important and differentiating characteristic of ASME performance test codes, including PTC 6, is that the test uncertainty of the measurement indicates the quality of the test and is not to be used as a commercial tolerance.






</doc>
<doc id="29376" url="https://en.wikipedia.org/wiki?curid=29376" title="Sardinia">
Sardinia

Sardinia ( ; , /, Sassarese: "Sardhigna", Gallurese: "Saldigna", Catalan: "Sardenya", Tabarchino: "Sardegna") is the second-largest island in the Mediterranean Sea (after Sicily and before Cyprus). It is located in the Western Mediterranean, to the immediate south of the French island of Corsica.

Sardinia is politically a region of Italy, whose official name is "Regione Autonoma della Sardegna" / "Regione Autònoma de Sardigna" (Autonomous Region of Sardinia), and enjoys some degree of domestic autonomy granted by a specific Statute. It is divided into four provinces and a metropolitan city, with Cagliari being the region's capital and its largest city as well. Sardinia's indigenous language and the other minority languages (Sassarese, Corsican Gallurese, Algherese Catalan and Ligurian Tabarchino) spoken on the island are recognized by the regional law and enjoy "equal dignity" with Italian.

The name Sardinia is from the pre-Roman noun *"s(a)rd-", later romanised as ' (feminine '). It makes its first appearance on the Nora Stone, where the word "Šrdn" testifies to the name's existence when the Phoenician merchants first arrived. According to "Timaeus", one of Plato's dialogues, Sardinia and its people as well might have been named after Sardò ("Σαρδώ"), a legendary woman born in Sardis ("Σάρδεις"), capital of the ancient Kingdom of Lydia. There has also been speculation that identifies the ancient Nuragic Sards with the Sherden, one of the Sea Peoples. It is suggested that the name had a religious connotation from its use also as the adjective for the ancient Sardinian mythological hero-god Sardus Pater "Sardinian Father" (in modern times misunderstood as being "Father Sardus"), as well as being the stem of the adjective "sardonic". In Classical antiquity, Sardinia was called (the Latinised form of ), "Sandal", "Sardinia" and "Sardó" ().

Sardinia is the second-largest island in the Mediterranean Sea (after Sicily and before Cyprus), with an area of . It is situated between 38° 51' and 41° 18' latitude north (respectively Isola del Toro and Isola La Presa) and 8° 8' and 9° 50' east longitude (respectively Capo dell'Argentiera and Capo Comino). To the west of Sardinia is the Sea of Sardinia, a unit of the Mediterranean Sea; to Sardinia's east is the Tyrrhenian Sea, which is also an element of the Mediterranean Sea.

The nearest land masses are (clockwise from north) the island of Corsica, the Italian Peninsula, Sicily, Tunisia, the Balearic Islands, and Provence. The Tyrrhenian Sea portion of the Mediterranean Sea is directly to the east of Sardinia between the Sardinian east coast and the west coast of the Italian mainland peninsula. The Strait of Bonifacio is directly north of Sardinia and separates Sardinia from the French island of Corsica.

The coasts of Sardinia [ long] are generally high and rocky, with long, relatively straight stretches of coastline, many outstanding headlands, a few wide, deep bays, rias, many inlets and with various smaller islands off the coast.

The island has an ancient geoformation and, unlike Sicily and mainland Italy, is not earthquake-prone. Its rocks date in fact from the Palaeozoic Era (up to 500 million years old). Due to long erosion processes, the island's highlands, formed of granite, schist, trachyte, basalt (called "jaras" or "gollei"), sandstone and dolomite limestone (called "tonneri" or "heels"), average at between . The highest peak is Punta La Marmora ("Perdas Carpìas" in Sardinian language)(), part of the Gennargentu Ranges in the centre of the island. Other mountain chains are Monte Limbara () in the northeast, the Chain of Marghine and Goceano () running crosswise for towards the north, the Monte Albo (), the Sette Fratelli Range in the southeast, and the Sulcis Mountains and the Monte Linas (). The island's ranges and plateaux are separated by wide alluvial valleys and flatlands, the main ones being the Campidano in the southwest between Oristano and Cagliari and the Nurra in the northwest.

Sardinia has few major rivers, the largest being the Tirso, long, which flows into the Sea of Sardinia, the Coghinas (115 km) and the Flumendosa (127 km). There are 54 artificial lakes and dams that supply water and electricity. The main ones are Lake Omodeo and Lake Coghinas. The only natural freshwater lake is Lago di Baratz. A number of large, shallow, salt-water lagoons and pools are located along the of the coastline.

The climate of the island is variable from area to area, due to several factors including the extension in latitude and the elevation. It can be classified in two different macrobioclimates (Mediterranean pluviseasonal oceanic and Temperate oceanic), one macrobioclimatic variant, called Submediterranean, and four classes of continentality (from weak semihyperoceanic to weak semicontinental), eight thermotypic horizons (from lower thermomediterranean to upper supratemperate) and seven ombrotypic horizons (from lower dry to lower hyperhumid), resulting in a combination of 43 different isobioclimates.

During the year there is a major concentration of rainfall in the winter and autumn, some heavy showers in the spring and snowfalls in the highlands. The average temperature is between , with mild winters and hot summers on the coasts ( in January, in July), and cold winters and cool summers on the mountains ( in January, in July).

Rainfall has a Mediterranean distribution all over the island, with almost totally rainless summers and wet autumns, winters and springs. However, in summer, the rare rainfalls can be characterized by short but severe thunderstorms, which can cause flash floods. The climate is also heavily influenced by the vicinity of the Gulf of Genoa (barometric low) and the relative proximity of the Atlantic Ocean. Low pressures in autumn can generate the formation of the so-called "Medicanes", extratropical cyclones which affect the Mediterranean basin. In 2013, the island was hit by several cyclones, included the Cyclone Cleopatra, which dumped almost 18 inches (450 mm) of rainfall within an hour and a half. Sardinia being relatively large and hilly, weather is not uniform; in particular the East is drier, but paradoxically it suffers the worst rainstorms: in autumn 2009, it rained more than in a single day in Siniscola, and 19 November 2013, locations in Sardinia were reported to have received more than 431 mm (17 inches) within two hours. The western coast has a higher distribution of rainfalls even for modest elevations (for instance Iglesias, elevation , average annual precipitation ). The driest part of the island is the coast of Cagliari gulf, with less than per year, the minimum is at Capo Carbonara at the extreme south-east of the island , and the wettest is the top of the Gennargentu mountain with almost per year. The average for the entire island is about per year, which is more than enough for the needs of the population and vegetation.
The Mistral from the northwest is the dominant wind on and off throughout the year, though it is most prevalent in winter and spring. It can blow quite strongly, but it is usually dry and cool.

Sardinia is one of the most geologically ancient bodies of land in Europe.
The island was populated in various waves of immigration from prehistory until recent times.

The first people to settle in Sardinia during the Upper Paleolithic and the Mesolithic came from Continental Europe; the Paleolithic colonization of the island is demonstrated by the evidences in Oliena's "Corbeddu Cave"; in the Mesolithic some populations, particularly from present-day Tyrrhenian coast of Italy, managed to move to northern Sardinia via Corsica. The Neolithic Revolution was introduced in the 6th millennium BC by the Cardial culture coming from the Italian Peninsula. In the mid-Neolithic period, the Ozieri culture, probably of Aegean origin, flourished on the island spreading the hypogeum tombs known as domus de Janas, while the Arzachena culture of Gallura built the first megaliths: circular tombs. In the early 3rd millennium BC, the metallurgy of copper and silver began to develop.

During the late Chalcolithic, the so-called Beaker culture, coming from various parts of Continental Europe, appeared in Sardinia. These new people predominantly settled on the west coast, where the majority of the sites attributed to them had been found. The Beaker culture was followed in the early Bronze Age by the Bonnanaro culture which showed both reminiscences of the Beaker and influences by the Polada culture.

As time passed, the different Sardinian populations appear to have become united in customs, yet remained politically divided into various small, tribal groupings, at times banding together, and at others waging war against each other. Habitations consisted of round thatched stone huts.

From about 1500 BC onwards, villages were built around round tower-fortresses called nuraghi (singular form "Nuraghe", usually pluralized in English as "Nuraghes"). These towers were often reinforced and enlarged with battlements. Tribal boundaries were guarded by smaller lookout Nuraghes erected on strategic hills commanding a view of other territories.

Today, some 7,000 Nuraghes dot the Sardinian landscape. While initially these Nuraghes had a relatively simple structure, with time they became extremely complex and monumental (see for example Nuraghe Santu Antine, Su Nuraxi, or Nuraghe Arrubiu). The scale, complexity and territorial spread of these buildings attest to the level of wealth accumulated by the Nuragic people, their advances in technology and the complexity of their society, which was able to coordinate large numbers of people with different roles for the purpose of building the monumental Nuraghes.

The Nuraghes are not the only Nuragic buildings that survive, as there are several sacred wells around Sardinia and other buildings that had religious purposes such as the Giants' grave (monumental collective tombs) and collections of religious buildings that probably served as destinations for pilgrimage and mass religious rites (e.g. Su Romanzesu near Bitti).

Sardinia was at the time at the centre of several commercial routes and it was an important provider of raw materials such as copper and lead, which were pivotal for the manufacture of the time. By controlling the extraction of these raw materials and by commercing them with other countries, the Nuragic civilisation was able to accumulate wealth and reach a level of sophistication that is not only reflected in the complexity of its surviving buildings, but also in its artworks (e.g. the votive bronze statuettes found across Sardinia or the statues of Mont'e Prama).

According to some scholars, the Nuragic people(s) are identifiable with the Sherden, a tribe of the "Sea Peoples".

The Nuragic civilization was linked with other contemporaneous megalithic civilization of the western Mediterranean, such as the Talaiotic culture of the Balearic Islands and the Torrean civilization of South Corsica. Evidence of trade with the other civilizations of the time is attested by several artefacts (e.g. pots), coming from as far as Cyprus, Crete, Mainland Greece, Spain and Italy, that have been found in Nuragic sites, bearing witness to the scope of commercial relations between the Nuragic people and other peoples in Europe and beyond.

Around the 9th century BC the Phoenicians began visiting Sardinia with increasing frequency, presumably initially needing safe overnight and all-weather anchorages along their trade routes from the coast of modern-day Lebanon as far afield as the African and European Atlantic coasts and beyond. The most common ports of call were Caralis, Nora, Bithia, Sulci, and Tharros. Claudian, a 4th-century Latin poet, in his poem "De bello Gildonico", stated that Caralis was founded by people from Tyre, probably in the same time of the foundation of Carthage, in the 9th or 8th century BC.

In the 6th century BC, after the conquest of western Sicily, the Carthaginians planned to annex Sardinia. A first invasion attempt led by Malco was foiled by the victorious Nuraghic resistance. However, from 510 BC, the southern and west-central part of the island was invaded a second time and came under Carthaginian rule.
In 238 BC, taking advantage of Carthage having to face a rebellion of her mercenaries (the Mercenary War) after the First Punic War (264–241 BC), the Romans annexed Corsica and Sardinia from the Carthaginians. The two islands became the province of Corsica and Sardinia. They were not given a provincial governor until 227 BC. The Romans faced many rebellions, and it took them many years to pacify both islands. The existing coastal cities were enlarged and embellished, and Roman colonies such as Turris Lybissonis and Feronia were founded. These were populated by Roman immigrants. The Roman military occupation brought the Nuragic civilization to an end, except for the mountainous interior of the island, which the Romans called "Barbaria", meaning "Barbarian land". Roman rule in Sardinia lasted 694 years, during which time the province was an important source of grain for the capital. Latin came to be the dominant spoken language during this period, though Roman culture was slower to take hold, and Roman rule was often contested by the Sardinian tribes from the mountainous regions.

The east Germanic tribe of the Vandals conquered Sardinia in 456. Their rule lasted for 78 years up to 534, when 400 eastern Roman troops led by Cyril, one of the officers of the "foederati", retook the island. It is known that the Vandal government continued the forms of the existing Roman Imperial structure. The governor of Sardinia continued to be called the "praeses" and apparently continued to manage military, judicial, and civil governmental functions via imperial procedures. The only Vandal governor of Sardinia about whom there is substantial record is the last, Godas, a Visigoth noble. In AD 530, a coup d'état in Carthage removed King Hilderic, a convert to Nicene Christianity, in favor of his cousin Gelimer, an Arian Christian like most of the élite in his kingdom. Godas was sent to take charge and ensure the loyalty of Sardinia. He did the exact opposite, declaring the island's independence from Carthage and opening negotiations with Emperor Justinian I, who had declared war on Hilderic's behalf. In AD 533 Gelimer sent the bulk of his army and navy (120 vessels and 5,000 men) to Sardinia to subdue Godas, with the catastrophic result that the Vandal Kingdom was overwhelmed when Justinian's own army under Belisarius arrived at Carthage in their absence. The Vandal Kingdom ended and Sardinia was returned to Roman rule.

In 533, Sardinia returned to the rule of the Byzantine Empire when the Vandals were defeated by the armies of Justinian I under the General Belisarius in the Battle of Tricamarum, in their African kingdom; Belisarius sent his general Cyril to Sardinia to retake the island. Sardinia remained in Byzantine hands for the next 300 years aside from a short period in which it was invaded by the Ostrogoths in 551.

Under Byzantine rule, the island was divided into districts called "mereíai" (μερείαι) in Byzantine Greek, which were governed by a judge residing in Caralis and garrisoned by an army stationed in "Forum Traiani" (today Fordongianus) under the command of a "dux". During this time, Christianity took deeper root on the island, supplanting the Paganism which had survived into the early Middle Ages in the culturally conservative hinterlands. Along with lay Christianity, the followers of monastic figures such as Basil of Caesarea became established in Sardinia. While Christianity penetrated the majority of the population, the region of Barbagia remained largely pagan and, probably, partially non-Latin speaking. They re-established a short-lived independent domain with Sardinian-heathen lay and religious traditions, one of its kings being Hospito. Pope Gregory I wrote a letter to Hospito defining him "Dux Barbaricinorum" and, being Christian, the leader and best of his people. In this unique letter about Hospito, the Pope prompts him to convert his people who "living all like irrational animals, ignore the true God and worship wood and stone" ("Barbaricini omnes, ut insensata animalia vivant, Deum verum nesciant, ligna autem et lapides adorent").
The dates and circumstances of the end of Byzantine rule in Sardinia are not known. Direct central control was maintained at least through c. 650, after which local legates were empowered in the face of the rebellion of Gregory the Patrician, Exarch of Africa and the first invasion of the Muslim conquest of the Maghreb. There is some evidence that senior Byzantine administration in the Exarchate of Africa retreated to Caralis following the final fall of Carthage to the Arabs in 697. The loss of imperial control in Africa led to escalating raids by Moors and Berbers on the island, the first of which is documented in 705, forcing increased military self-reliance in the province. Communication with the central government became daunting if not impossible during and after the Muslim conquest of Sicily between 827 and 902. A letter by Pope Nicholas I as early as 864 mentions the "Sardinian judges", without reference to the empire and a letter by Pope John VIII (reigned 872–882) refers to them as "principes" ("princes"). By the time of "De Administrando Imperio", completed in 952, the Byzantine authorities no longer listed Sardinia as an imperial province, suggesting they considered it lost.
In all likelihood a local noble family, the Lacon-Gunale, acceded to the power of Archon, still identifying themselves as vassals of the Byzantines, but "de facto" independent as communications with Constantinople were very difficult. We know only two names of those rulers, Salusios (Σαλούσιος) and the protospatharios Turcoturios (Tουρκοτούριος) from two inscriptions), who probably reigned between the 10th and the 11th century. These rulers were still closely linked to the Byzantines, both for a pact of ancient vassalage, and from the ideological point of view, with the use of the Byzantine Greek language (in a Romance country), and the use of art of Byzantine inspiration.
In the early 11th century, an attempt to conquer the island was made by the Moors based in the Iberian Peninsula. The only records of that war are from Pisan and Genoese chronicles. The Christians won, but after that, the previous Sardinian kingdom was totally undermined and divided into four small states: Cagliari ("Calari"), Arborea ("Arbaree"), Gallura, Torres or Logudoro.

Whether this final transformation from imperial civil servant to independent sovereign bodies resulted from imperial abandonment or local assertion, by the 10th century, the so-called giudici ( / , literally "judges", a Byzantine administrative title) had emerged as the autonomous rulers of Sardinia. The title of "iudice" changed with the language and local understanding of the position, becoming the Sardinian "judike", essentially a king or sovereign, while "giudicato" (), literally "judgedom", came to mean "state". A letter by Mieszko I of Poland to Pope John XV proves that the Judgedoms were known even in Poland (see Dagome iudex), and that they played an important role in Medieval Europe.

Early medieval Sardinian political institutions evolved from the millennium-old Roman imperial structures with relatively little Germanic influence.

Although the Judgedoms were hereditary lordships, the old Byzantine imperial notion that personal title or honor was separate from the state still remained, so the Giudicato was not regarded as the personal property of the monarch as was common in later European feudalism. Like the imperial systems, the new order also preserved "semi-democratic" forms, with national assemblies called the Crown of the Realm. Each Giudicato saw to its own defense, maintained its own laws and administration, and looked after its own foreign and trading affairs.

The history of the four Judgedoms would be defined by the contest for influence between the rival rising sea powers of Genoa and Pisa, and later the ambitions of the Kingdom of Aragon.

The Giudicato of Cagliari (or "Pluminos"), during the regency of Torchitorio V of Cagliari and his successor, William III, was allied with the Republic of Genoa. Because of this it was brought to an end in 1258, when its capital, Santa Igia, was stormed and destroyed by an alliance of Sardinian and Pisan forces. The territory then was divided between the Republic of Pisa, the Della Gherardesca family from Italy, and the Sardinian Judgedoms of Arborea and Gallura. Pisa maintained the control over the fortress of Castel di Cagliari founded by Pisan merchants in 1216/1217 east of Santa Igia; in the south-west the count Ugolino della Gherardesca promoted the birth of the town of Villa di Chiesa (today Iglesias) to exploit the nearby rich silver deposits.

The Giudicato of Logudoro (also called "Torres") was also allied to the Republic of Genoa and came to an end in 1259 after the death of the "judikessa" (queen) Adelasia. The territory was divided up between the Doria and Malaspina families of Genoa and the Bas-Serra family of Arborea, while the city of Sassari became a small republic, along the lines of the Italian city-states ("comuni"), confederated firstly with Pisa and then with Genoa.

The Giudicato of Gallura ended in the year 1288, when the last giudice, Nino Visconti (a friend of Dante Alighieri), was driven out by the Pisans, who occupied the territory.

The Giudicato of Arborea, having Oristano as its capital, had the longest life compared to the other kingdoms. Its later history is entwined with the attempt to unify the island into a single Sardinian state ("Republica sardisca" "Sardinian Republic" in Sardinian, "Nació sarda" or "sardesca" "Sardinian Nation" in Catalan) against their relatives and former Aragonese allies.

In 1297, Pope Boniface VIII established on his own initiative ("motu proprio") a hypothetical "regnum Sardiniae et Corsicae" ("Kingdom of Sardinia and Corsica") in order to settle the War of the Sicilian Vespers diplomatically. This had broken out in 1282 between the Capetian House of Anjou and Catalans over the possession of Sicily. Despite the existence of the indigenous states, the Pope offered this newly created crown to James II of Aragon, promising him support should he wish to conquer Pisan Sardinia in exchange for Sicily.
In 1324, in alliance with the Kingdom of Arborea and following a military campaign that lasted a year or so, the Aragon Crown Prince Alfonso led a Catalan army that occupied the Pisan territories of Cagliari and Gallura along with the allied city of Sassari, naming them ""The Kingdom of Sardinia and Corsica"". The kingdom was to remain a dominion of the Crown of Aragon (under the Kings of Spain) until the Treaty of Utrecht.

During this period, the Giudicato of Arborea promulgated the legal code of the kingdom in the "Carta de Logu" ('Charter of the Land'). The Carta de Logu was originally compiled by Marianus IV of Arborea, and was amended and updated by Mariano's daughter, Female Judge ("judikessa" or "juighissa") Eleanor of Arborea. The legal code was written in Sardinian and established a whole range of citizens' rights. Among the revolutionary concepts in this Carta de Logu was the right of women to refuse marriage and to own property. In terms of civil liberties, the code made provincial 14th century Sardinia one of the most developed societies in all of Europe.

In 1353, Peter IV of Aragon, following Aragonese customs, granted a parliament to the kingdom of Sardinia and Corsica, which was followed by some degree of self-government under a viceroy and judicial independence. This parliament, however, had limited powers. It consisted of high-ranking military commanders, the clergy and the nobility. The kingdom of Aragon also introduced the feudal system into the areas of Sardinia that it ruled.

The Sardinian Giudicati never adopted feudalism, and Arborea maintained its parliament, called the "Corona de Logu" "Crown of the Realm". In this parliament, apart from the nobles and military commanders, also sat the representatives of each township and village. The Corona de Logu exercised some control over the king: under the rule of the "bannus consensus" the king could be deposed or even executed if he did not follow the rules of the kingdom.

Broken the alliance with the Crown of Aragon, from 1353 to 1409, the Arborean giudici Marianus IV, Hugh III and Brancaleone Doria (husband of Eleanor of Arborea), succeeded in occupying all of Sardinia except the heavily fortified towns of the Castle of Cagliari and Alghero, which for years remained as the only Aragonese dominions in Sardinia (Sardinian-Catalan War).

In 1409, Martin I of Sicily, king of Sicily and heir to the crown of Aragon, defeated the Sardinians at the Battle of Sanluri. The battle was fought by about 20,000 Sardinian, Genoese and French knights, enrolled from their kingdom at a time when the population of Sardinia had been greatly depleted by the plague. Despite the Sardinian army outnumbering the Aragonese army, they were defeated.

The giudicato of Arborea disappeared in 1420, when its rights were sold by the last king for 100,000 gold florins, and after some of its most notable men switched sides in exchange for privileges. For example, Leonardo Cubello, with some claim to the crown being from a family related to the Kings of Arborea, was granted the title of Marquis of Oristano and feudal rights on a territory that partly overlapped with the original extension of the Kingdom of Arborea in exchange for his subjection to the Aragonese monarchs.

The conquest of Sardinia by the Kingdom of Aragon meant the introduction of the feudal system throughout Sardinia. Thus Sardinia is probably the only European country where feudalism was introduced in the transition period from the Middle Ages to the early modern period, at a time when feudalism had already been abandoned by many other European countries.

In 1469, the heir to Sardinia, Ferdinand II of Aragon, married Isabel of Castile, and the "Kingdom of Sardinia" (which was separated from Corsica) was to be inherited by their Habsburg grandson, Charles I of Spain, with the state symbol of the Four Moors. The successors of Charles I of Spain, in order to defend their Mediterranean territories from raids of the Barbary pirates, fortified the Sardinian shores with a system of coastal lookout towers, allowing the gradual resettlement of some coastal areas.

The Kingdom of Sardinia remained Aragonese-Spanish for about 400 years, from 1323 to 1708, assimilating a number of Spanish traditions, customs and linguistic expressions, nowadays vividly portrayed in the folklore parades of Saint Efisio in Cagliari (1 May), the Cavalcade on Sassari (last but one Sunday in May), and the Redeemer in Nuoro (28 August). To this day Catalan is still spoken in the north-western city of Alghero (l'Alguer).

Many famines have been reported in Sardinia. According to Stephen L. Dyson and Robert J. Rowland, "The Jesuits of Cagliari recorded years during the late 16th century "of such hunger and so sterile that the majority of the people could sustain life only with wild ferns and other weeds" ... During the terrible famine of 1680, some 80,000 people, out of a total population of 250,000, are said to have died, and entire villages were devastated..."

In 1708, as a consequence of the Spanish War of Succession, the rule of the Kingdom of Sardinia passed from King Philip V of Spain into the hands of the Austrians, who occupied the island. The Treaty of Utrecht granted Sardinia to the Austrians, but in 1717, Cardinal Giulio Alberoni, minister of Philip V of Spain, reoccupied Sardinia.

In 1718, with the Treaty of London, Sardinia was handed over to the House of Savoy, that would impose the Italian language on the island in 1760.
In 1793, Sardinians repelled the French "Expédition de Sardaigne" during the French Revolutionary Wars. On 23 February 1793, Domenico Millelire, commanding the Sardinian fleet, defeated the fleets of the French Republic near the Maddalena archipelago, of which then-lieutenant Napoleon Bonaparte was a leader. Millelire became the first recipient of the Gold Medal of Military Valor of the Italian Armed Forces. In the same month, Sardinians stopped the attempted French landing on the beach of Quartu Sant'Elena, near the Capital of Cagliari. Because of these successes, the representatives of the nobility and clergy ("Stamenti") formulated five requests addressed to the King Victor Amadeus III of Sardinia, but these were rejected, Because of this discontent, on 28 April 1794, during an uprising in Cagliari, two Savoyard officials were killed. That was the start of a revolt (called the "Sardinian Vespers") in the island, which culminated on 28 April 1794 (commemorated today as "sa die de sa Sardigna") with the expulsion or even execution of the Piedmontese officers for a few days from the Capital Cagliari. 

On 28 December 1795 Sassari insurgents demonstrating against feudalism, mainly from the region of Logudoro, occupied the city. On 13 February 1796, in order to prevent the spread of the revolt, the viceroy Filippo Vivalda gave the Sardinian magistrate Giovanni Maria Angioy the role of Alternos, which meant a substitute of the viceroy himself. Angioy moved from Cagliari to Sassari, and during his journey almost all the villages joined the uprising, demanding an end to feudalism and aiming to declare the island to be an independent republic, but once he was outnumbered by loyalist forces he fled to Paris and sought support for a French annexation of the island.

In 1798 the islet near Sardinia was attacked by the Tunisians and over 900 inhabitants were taken away as slaves. The final Muslim attack on the island was on Sant'Antioco on 16 October 1815, over a millennium since the first.

In 1799, as a consequence of the Napoleonic Wars in Italy, the Savoy royal family left Turin and took refuge in Cagliari for some fifteen years. In 1847, the Sardinian parliaments ("Stamenti"), in order to get the Piedmontese liberal reforms they could not afford due to their separated legal system, renounced their state autonomy and agreed to form a union with Piedmont, Savoy, Nice and Liguria in order to have a single parliament, a single magistracy and a single government in Turin; this move aggravated the island's peripheral condition and most of the pro-union supporters, including its leader Giovanni Siotto Pintor, would later regret it.
In 1820, the Savoyards imposed the "Enclosures Act" ("Editto delle Chiudende") on the island, aimed at turning the land's traditional collective ownership, a cultural and economic cornerstone of Sardinia since the Nuragic times, to private property. This gave rise to many abuses, as the reform favoured the landholders while excluding the poor Sardinian farmers and shepherds, who witnessed the abolition of the communal rights and the sale of the land. Many local rebellions like the Nuorese "Su Connottu" ("The Already Known" in Sardinian) riot in 1868, all repressed by the King's army, resulted in an attempt to return to the past and reaffirm the right to use the once common land. However the common lands (called "ademprivios") were never completely abolished, and they are still present in large number to this day (500,000 hectares of common lands were counted in 1956, of which 345,000 constituted by woods).

In 1848, the confederation of states powered by the Savoyard kings of Sardinia became a unitarian and constitutional state and moved to the Italian Wars of Independence for the Unification of Italy, that were led for thirteen years. In 1861, being Italy united by a debated war campaign, the parliament of the Kingdom of Sardinia decided by law to change its name and the title of its king in Kingdom of Italy and King of Italy. Most Sardinian forests were cut down at this time, in order to provide the Piedmontese with raw materials, like wood, used to make railway sleepers on the mainland. The extension of the primary natural forests, praised by every traveller visiting Sardinia, would in fact be reduced to 1/5 of their original number, being little more than 100.000 hectares at the end of the century.

During the First World War, the Sardinian soldiers of the Brigata Sassari distinguished themselves. It was the first and only regional military unit in Italy, since the people enrolled were only Sardinians. The brigade suffered heavy losses and earned four Gold Medals of Military Valor. Sardinia lost more young people than any other Italian region on the front, with 138 casualties per 1000 soldiers compared to the Italian average of 100 casualties.

During the Fascist period, with the implementation of the policy of autarky, several swamps around the island were reclaimed and agrarian communities founded. The main communities were the village of Mussolinia (now called Arborea), populated by farmers from Veneto and Friuli, in the area of Oristano and Fertilia, populated at first by settlers from the Ferrara area, followed, after World War II, by a notable number of Istrian Italians and Dalmatian Italians hailing from territories lost to Yugoslavia, in the area adjacent the city of Alghero, within the region of Nurra . Also established during that time (1938) was the city of Carbonia, which became the main centre of coal mining activity, that attracted thousand of workers from the rest of the Island and the Italian mainland. The Sardinian writer Grazia Deledda won the Nobel Prize for Literature in 1926.
During the Second World War, Sardinia was an important air and naval base and was heavily bombed by the Allies, especially the city of Cagliari. German troops left the island on 8 September 1943, a few days after the Armistice of Cassibile, and retired to Corsica without fighting and bloodshed, after a bilateral agreement between the general Antonio Basso (Commander of the Armed Forces of Sardinia) and the German Karl Hans Lungerhausen, general of the 90th Panzergrenadier Division.

In 1946, by popular referendum, Italy became a republic, with Sardinia being administered since 1948 by a special statute of autonomy. By 1951, malaria was successfully eliminated by the ERLAAS, Anti-malaric Regional Authority, and the support of the Rockefeller Foundation, which facilitated the commencement of the Sardinian tourist boom. With the increase in tourism, coal decreased in importance but Sardinia followed the Italian economic miracle.

In the early 1960s, an industrialisation effort was commenced, the so-called "Piani di Rinascita" (rebirth plans), with the initiation of major infrastructure projects on the island. These included the construction of new dams and roads, reforestation, agricultural zones on reclaimed marshland, and large industrial complexes (primarily oil refineries and related petrochemical operations). With the creation of petrochemical industries, thousands of ex-farmers became industrial workers. The 1973 oil crisis caused the termination of employment for thousands of workers employed in the petrochemical industries, which aggravated the emigration already present in the 1950s and 1960s.

Sardinia faced the creation of military bases on the island, like Decimomannu Air Base and Salto di Quirra (the biggest scientific military base in Europe) in the same decades. Even now, around 60% of all Italian and NATO military installations in Italy are on Sardinia, whose area is less than one-tenth of all the Italian territory and whose population is little more than the 2.5%; furthermore, they comprise over 35.000 hectares used for experimental weapons testing, where 80% of the military explosives in Italy are used.

Sardinian nationalism and local protest movements became stronger in the 1970s, and a number of bandits ("anonima sarda") started a long series of kidnappings, which ended only in the 1990s. This also gave rise to various militant groups that blended separatist and communist ideas, the most famous being "Barbagia Rossa" and the Sardinian Armed Movement, which perpetrated several bombings and terrorist actions between the 1970s and the 1980s. In the span of just two years (1987–1988), 224 bombing attacks were reported.

In 1983 a prominent activist of a separatist party, the Sardinian Action Party ("Partidu Sardu – Partito Sardo d'Azione"), was elected president of the regional parliament, and in the 1980s several other movements calling for independence from Italy were born; in the 1990s some of them became political parties, even if in a rather disjointed manner. It was not until 1999 that the island's languages (Sardinian, Sassarese, Gallurese, Algherese and Tabarchino) were recognised, even if just formally, together with Italian. The 35th G8 summit was planned by Prodi II Cabinet to be held in Sardinia, on the island of La Maddalena, in July 2009; however, in April 2009, the Italian Prime Minister, Silvio Berlusconi, decided, without convoking the Italian parliament or consulting the Sardinian governor of his own party, to move the summit, even though the works were almost completed, to L'Aquila, provoking heavy protests.

Today Sardinia is phasing in as an EU region, with a diversified economy focused on tourism and the tertiary sector. The economic efforts of the last twenty years have reduced the handicap of insularity, especially in the fields of low-cost air travel and advanced information technology. For example, the CRS4 (Center for Advanced Studies, Research and Development in Sardinia) developed the second European website and 1st in Italy in 1991 and webmail in 1995. CRS4 allowed several telecommunication companies and internet service providers based on the island to flourish, such as Videonline in 1994, Tiscali in 1998 and Andala Umts in 1999.

According to the ISTAT census of 2001, the literacy rate in Sardinia among people below 64 years old is 99.5 percent. Total literacy rate (including people over 65) is 98.2 percent.
Illiteracy rate among males below 65 years old is 0.24 percent and among women 0.25 percent; the number of women that annually graduate at secondary high schools and universities is about 10–20 percent higher than men. Sardinia has the 2nd highest rate of school drop-out in Italy.

Sardinia has two public universities: the University of Sassari and the University of Cagliari, founded in the 16th and 17th century. 48,979 students were enrolled at universities in 2007–08.

Taken as a whole, Sardinia's economic conditions are such that the island is in the best position among Italian regions located south of Rome. The greatest economic development had taken place inland, in the provinces of Cagliari and Sassari, characterized by a certain amount of enterprise. According to Eurostat, the 2014 nominal GDP was €33,356 million, €33,085 million in purchasing power parity, resulting in a GDP per capita of €19,900, which is 72% of the EU average. The per capita income in Sardinia is the highest of the southern half of Italy. The most populated provincial chief towns have higher incomes: in Cagliari the income per capita is €27,545, in Sassari €24,006, in Oristano €23,887, in Nuoro is €23,316 and in Olbia is €20,827.

The Sardinian economy is, however, constrained due to the high costs of the transportation of goods and electricity, which is twice that of the continental Italian regions, and triple that of the EU average. Sardinia is the only Italian region that produces a surplus of electricity, and exports electricity to Corsica and the Italian mainland: in 2009, the new submarine power cable Sapei entered into operation. It links the Fiume Santo Power Station, in Sardinia, to the converter stations in Latina, in the Italian peninsula. The SACOI is another submarine power cable that links Sardinia to Italy, crossing Corsica, from 1965.
Small scale LNG terminals and a 404-km gas pipeline are under construction, and will be operative in 2018. They will decrease the current high cost of the electric power in the island.

Three main banks are headquartered in Sardinia. However, the Banco di Sardegna and the Banca di Sassari, both originally from Sassari, are controlled by the mainland-based Banca Popolare dell'Emilia-Romagna; likewise the Banca di Credito Sardo, originally based in Cagliari, has been absorbed by the parent company Intesa Sanpaolo in 2014.

The unemployment rate for the fourth quarter of 2008 was 8.6%; by 2012, the unemployment rate had increased to 14.6%. Its rise was due to the global financial crisis that hit Sardinian exports, mainly focused on refined oil, chemical products, and also mining and metallurgical products.

There are chances for Sardinia to become a tax haven, the whole island territory being free by custom duties, vat and excise taxes on fuel; since February 2013, the town of Portoscuso has become the first free trade zone. According to the article 12 of the Sardinian Statute modified by the regional parliament in October 2013: ""The Territory of the Autonomous Region of Sardinia is located off the customs line and constitutes a Free Trade Zone enclosed by the surrounding sea; the access points consist of the seaports and the airports. The Sardinian Free Trade Zone is regulated by the laws of the European Union and Italy that are in force also in Livigno, Campione D'Italia, Gorizia, Savogna d'Isonzo and the Region of Aosta Valley"".

This table shows the sectors of the Sardinian economy in 2011:

Sardinia's land is dedicated 60% to livestock, 20% to agriculture and the rest is occupied by closed forests, urban areas and areas that are not exploitable. Sardinia is home to nearly 4 million sheep, almost half of the entire Italian assets and that makes the island one of the areas of the world with the highest density of sheep along with some parts of UK and New Zealand (135 sheep every square kilometer versus 129 in UK and 116 in New Zealand). Sardinia has been for thousands of years specializing in sheep breeding, and, to a lesser extent, goats and cattle that is less productive of agriculture in relation to land use. It is probably in breeding and cattle ownership the economic base of the early proto-historic and monumental Sardinian civilization from Neolithic to the Iron Age.
Even agriculture has played a very important role in the economic history of the island, especially in the great plain of Campidano, particularly suitable for wheat farming. The Sardinian soils, even those plains are slightly permeable, with aquifers of lacking and sometimes brackish water and very small natural reserves. Water scarcity was the first problem that was faced for the modernization of the sector, with the construction of a great barrier system of dams, which today contains nearly 2 billion cubic meters of water. The Sardinian agriculture is now linked to specific products such as cheese, wine, olive oil, artichoke, tomato for a growing product export. The reclamations have helped to extend the crops and to introduce other ones such as vegetables and fruit, next to the historical ones, olive and grapes that are present in the hilly areas. The Campidano plain, the largest lowland Sardinian produces oats, barley and durum, of which is one of the most important Italian producers. Among the vegetables, as well as artichokes, has a certain weight the production of oranges, and, before the reform of the sugar sector from the European Union, the cultivation of sugar beet. 

In the forests there is the cork oak, which grows naturally; Sardinia produces about 80% of Italian. The cork district, in the northern part of the Gallura region, around Calangianus and Tempio Pausania, is composed of 130 companies. Every year in Sardinia 200,000 quintals (20,000 tonnes) of cork are carved, and 40% of the end products are exported.

In fresh food, as well as artichokes, the production of tomatoes (including Camoni tomato) and citrus fruit are of a certain weight. Sardinia is the 5th Italian region for rice production, the main paddy fields are located in the Arborea Plain.

In addition to meat, Sardinia produces a wide variety of cheese, considering that half of the sheep milk produced in Italy is produced in Sardinia, and is largely worked by the cooperatives of the shepherds and small industries. Sardinia also produces most of the pecorino romano, a non-original product of the island, much of which is traditionally addressed to the Italian overseas communities. Sardinia boasts a centuries-old tradition of horse breeding since the Aragonese domination, whose cavalry drew from equine heritage of the island to strengthen their own army or to make a gift to the other sovereigns of Europe. Today the Island boasts the highest number of horse herds in Italy.

There is little fishing (and no real maritime tradition), Portoscuso tunas are exported worldwide, but primarily to Japan.

The once prosperous mining industry is still active though restricted to coal (Nuraxi Figus, hamlet of Gonnesa), antimony (Villasalto), gold (Furtei), bauxite (Olmedo) and lead and zinc (Iglesiente, Nurra). The granite extraction represents one of the most flourishing industries in the northern part of the island. The Gallura granite district is composed of 260 companies that work in 60 quarries, where 75% of the Italian granite is extracted.
The principal industries are chemicals (Porto Torres, Cagliari, Villacidro, Ottana), petrochemicals (Porto Torres, Sarroch), metalworking (Portoscuso, Portovesme, Villacidro), cement (Cagliari), pharmaceutical (Sassari), shipbuilding (Arbatax, Olbia, Porto Torres), oil rig construction (Arbatax), rail industry (Villacidro), arms industries at Domusnovas and food (sugar refineries at Villasor and Oristano, dairy at Arborea, Macomer and Thiesi, fish factory at Olbia.

In Sardinia is located the DASS ("Distretto Aerospaziale della Sardegna"), a consortium of companies, research centers and universities focused on aerospace industry and research. The aerospace manufacturer Vitrociset, in Villaputzu, is involved in the production of the stealth multirole fighter Lockheed Martin F-35 Lightning II.

Plans related to industrial conversion are in progress in the main industrial sites, like in Porto Torres, where seven research centres are developing the transformation from traditional fossil fuel related industry to an integrated production chain from vegetable oil using oleaginous seeds to bio-plastics.matrica green chemicals

Sardinia is involved in the industrial production of the AIRPod, an innovative car powered by compressed air, with the first factory being built in Bolotana.

Craft industries include rugs, jewelry, textile, lacework, basket making and coral.

The Sardinian economy is today focused on the overdeveloped tertiary sector (67.8% of employment), with commerce, services, information technology, public administration and especially on tourism (mainly seaside tourism), which represents the main industry of the island with 2,721 active companies and 189,239 rooms. In 2008 there were 2,363,496 arrivals (up 1.4% on 2007). In the same year, the airports of the island registered 11,896,674 passengers (up 1.24% on 2007).

Due to its isolated and insular location, Sardinia focused part of its economy on the development of digital technologies since the dawn of internet era: the first Italian website, one of the first webmail system and one of the first and largest internet providers (Video On Line) were realised by the CRS4, the first European online newspaper was developed by L'Unione Sarda and also the first Italian UMTS company was founded on the island. Today Sardinia is the second Italian region, after Lombardy, for investments in startups (owning the 20% of the Italian venture capital).

On the island are headquartered some telecommunication companies and internet service providers, such as Tiscali and the Mediterranean Skylogic Teleport, a ground station controlled by satellite provider Eutelsat. Sardinia is the Italian region with the highest e-intensity index, after the Aosta Valley (index measuring the relative maturity of Internet economies on the basis of three factors: enablement, engagement, and expenditure) and the region with the highest internet performances, such as fastest broadband connection in Italy.
Sardinia is also the Italian region with the highest percentage (41%) of 4G LTE users.
The Chinese multinational telecommunications equipment and systems companies ZTE and Huawei have development centers and innovation labs in Sardinia.

Sardinia has become Europe's first region to fully adopt the new Digital Terrestrial Television broadcasting standard. From 1 November 2008 TV channels are broadcast only in digital.

Sardinia has three international airports (Alghero-Fertilia/Riviera del Corallo Airport, Olbia-Costa Smeralda Airport and Cagliari-Elmas Airport) connected with the principal Italian cities and many European destinations, mainly in the United Kingdom, Scandinavia, Spain and Germany, and two regional airports (Oristano-Fenosu Airport and Tortolì-Arbatax Airport). Internal air connections between Sardinian airports are limited to a daily Cagliari-Olbia flight. Sardinian citizens benefit from special sales on plane tickets, and several low-cost air companies operate on the island.

Air Italy (formerly known as Meridiana) is an airline headquartered in the airport of Olbia; it was founded as Alisarda in 1963 by the Aga Khan IV. The development of Alisarda followed the development of Costa Smeralda in the north east part of the island, a well known vacation spot among billionaires and film actors worldwide.

The ferry companies operating on the island are Tirrenia di Navigazione, Moby Lines, Corsica Ferries - Sardinia Ferries, Grandi Navi Veloci, SNAV, SNCM and CMN; they link the Sardinian seaports of Porto Torres, Olbia, Golfo Aranci, Arbatax, Santa Teresa Gallura, Palau and Cagliari with Civitavecchia, Genoa, Livorno, Naples, Palermo, Trapani, Piombino in Italy, Marseille, Toulon, Bonifacio, Propriano and Ajaccio in France and Barcelona in Spain.

A regional ferry company, the Saremar, links the main island to the islands of La Maddalena and San Pietro, and from 2011, also the port of Olbia with Civitavecchia and Porto Torres with Savona.

About 40 tourist harbours are located along the Sardinian coasts.

Sardinia is the only Italian region without Autostrade, but the road network is well developed with a system of no-toll roads with dual carriageway, called "superstrade" (en: super roads), that connect the principal towns and the main airports and seaports; the speed limit is /. The principal road is the SS131 "Carlo Felice", linking the north with the south of the island, crossing the most historic regions of Porto Torres and Cagliari; it is part of European route E25. The SS 131 d.c.n links Oristano with Olbia, crossing the hinterland Nuoro region. Other roads designed for high-capacity traffic link Sassari with Alghero, Sassari with Tempio Pausania, Sassari – Olbia, Cagliari – Tortolì, Cagliari – Iglesias, Nuoro – Lanusei. A work in progress is converting the main routes to highway standards, with the elimination of all intersections. The secondary inland and mountain roads are generally narrow with many hairpin turns, so the speed limits are very low.

Public transport buses reach every town and village at least once a day; however, due to the low density of population, the smallest territories are reachable only by car. The Azienda Regionale Sarda Trasporti (ARST) is the public regional bus transport agency. Networks of city buses serve the main towns.

In Sardinia 1,295,462 vehicles circulate, equal to 613 per 1,000 inhabitants.

The Sardinian railway system was developed starting from the 19th century by the Welsh engineer Benjamin Piercy.

Today there are two different railway operators:

The "Trenino Verde" ("Little Green Train") is a railway tourism service operated by ARST. Vintage railcars and steam locomotives run through the wildest parts of the island. They allow the traveller to have scenic views impossible to see from the main roads.

With a population density of 69/km, slightly more than a third of the national average, Sardinia is the fourth least populated region in Italy. In the recent past the population distribution was anomalous compared to that of other Italian regions lying on the sea. In fact, contrary to the general trend, most urban settlement, with the exception of the fortified cities of Cagliari, Alghero, Castelsardo and few others, has not taken place primarily along the coast but in the subcoastal areas and towards the centre of the island. Historical reasons for this include the repeated Saracen raids during the Middle Ages and then Barbary raids until the early 19th century (making the coast unsafe), widespread pastoral activities inland, and the swampy nature of the coastal plains (reclaimed definitively only in the 20th century). The situation has been reversed with the expansion of seaside tourism; today all Sardinia's major urban centres are located near the coasts, while the island's interior is very sparsely populated.

It is the region with the lowest total fertility rate (1.087 births per woman) and the second-lowest birth rate of Italy (which is already one of the lowest in the world). Combined with the aging of population going rather fast (in 2009, people older than 65 were 18,7%), rural depopulation is quite a big issue: between 1991 and 2001, 71,4% of Sardinian villages have lost population (32 more than 20% and 115 between 10% and 20%), with over 30 of them being at risk to become ghost towns. Nonetheless, the overall population has been increasing because of a considerable immigration flow, mainly from the Italian mainland, Eastern Europe (esp. Romania), Africa and Asia.
Average life expectancy is slightly over 82 years (85 for women and 79.7 for men). Sardinia shares with the Japanese island of Okinawa the highest rate of centenarians in the world (22 centenarians/100,000 inhabitants).
Sardinia is the first discovered Blue Zone, a demographic and/or geographic area in the world with an oversize concentration of centenarians and supercentenarians.

In 2016 there were 50,346 foreign national residents, forming 3% of the total Sardinian population. The most represented nationalities were:



Sardinia's most populated cities are Cagliari and Sassari. The Metropolitan City of Cagliari has 431,302 inhabitants, or about ¼ of the population of the entire island. Eurostat has identified in Sardinia two Functional Urban Areas: Cagliari, with 477,000 inhabitants, and Sassari, with 222,000 inhabitants.

Sardinia is one of the five Italian autonomous regions, along with the Aosta Valley, Trentino-Alto Adige/Südtirol, Friuli Venezia Giulia and Sicily. Its particular statute, which in itself is a constitutional law, gives the region a limited degree of autonomy, entailing the right to carry out the administrative functions of the local body and to create its own laws in a strictly defined number of domains.

The regional administration is constituted by three authorities:

Since 2016, Sardinia is divided into four provinces (Nuoro, Oristano, Sassari, South Sardinia) and the metropolitan city of Cagliari.

Around 65% of all the Italian and NATO military installations in Italy are on Sardinia, whose area is less than one-tenth of all the Italian territory and whose population is little more than the 2,5%. The bases, used for manufacturing plants and military testing grounds, totally take up more than 350 km² of the island's land, making Sardinia the most militarized region in Italy and the most militarized island in Europe. Besides the land-occupying installations, where 80% of the military explosives in Italy are used, there are also other military structures located on the sea and along the coastline, roughly equivalent to 20000 km² (little less than the island's surface), being made inaccessible to the civil population when military exercises are being held. Among the others, the most notable military bases on the island are the Interagency Polygons in Quirra, Capo Teulada and Capo Frasca, used by Italian and NATO forces to test-fire ballistic missiles and weapons and by Italian and European Space Agency to test space vehicles and for orbital launches. Until 2008, the US navy had also a nuclear submarine base in the Maddalena Archipelago.

Sardinia is the only autonomous region in Italy where its special Statute uses the term "popolo" (distinct people) to refer to its inhabitants. While this formula is also used by Veneto, which unlike Sardinia is an ordinary region, the Sardinian Statute is adopted with a constitutional law. In both cases, such term is not meant to imply any legal difference between Sardinians and any other citizen of the country.

Of the prehistoric architecture in Sardinia there are numerous testimonies such as the "domus de janas" (hypogeic tombs), the Giants' grave, the megalithic circles, the menhirs, the dolmens and the well temples; however, the element that more than any other characterizes the Sardinian prehistoric landscape are the nuraghe; the remains of thousands of these Bronze Age buildings of various types (simple and complex) are still visible today. There are also numerous traces left by the Phoenicians and Punics who introduced new urban forms on the coasts.
The Romans gave a new administrative structure to the whole island through the restructuring of several cities, the creation of new centers and the construction of many infrastructures of which the ruins remain, such as the palace of Re Barbaro in Porto Torres or the Roman Amphitheatre of Cagliari. Even from the early Christian and Byzantine epoch there are several testimonies throughout the territory both on the coasts and inside, especially linked to buildings of worship.

A particular development had Romanesque architecture during the Giudicati period. Starting from 1063 the Sardinian judges ("judikes"), through substantial donations, had favored the arrival to the island of monks of different orders from various regions of Italy and France. These circumstances favored in turn the arrival to the island of workers from Pisa, Lombardy, Provence and Muslim Spain, giving rise to unprecedented artistic manifestations, marked by the fusion of these experiences.

The cornerstone in the evolution of Romanesque architectural forms was the basilica of San Gavino in Porto Torres. Among the most relevant examples there are the cathedrals of Sant'Antioco di Bisarcio (Ozieri), San Pietro di Sorres in Borutta, San Nicola di Ottana, the palatine chapel of Santa Maria del Regno of Ardara, the Santa Giusta Cathedral, Nostra Signora di Tergu, the Basilica di Saccargia in Codrongianos and Santa Maria di Uta and, of the 13th century, the cathedrals of Santa Maria di Monserrato (Tratalias) and San Pantaleo (Dolianova). As for military architecture, numerous castles to defend the territory were built during this period. At the beginning of the 14th century date the fortifications and towers of Cagliari, designed by Giovanni Capula.

After their arrival in 1324, the Aragonese concentrated the first realizations in Cagliari; the oldest Catalan Gothic church in Sardinia is the shrine of Our Lady of Bonaria. Also in Cagliari in the same years the Aragonese chapel was built inside the cathedral. In the first half of the fifteenth century a real Gothic jewel was built, the complex of San Domenico, which included the church and the convent, almost completely destroyed during the air raids of 1943, and of which only the cloister remains. Other works were the churches of San Francesco of Stampace (of which only a part of the cloister remains), Sant'Eulalia and San Giacomo. In Alghero in the second half of the fifteenth century the construction of the church of San Francesco and in the sixteenth century of the cathedral began.

Renaissance architecture, although poorly represented, includes notable examples such as the installation of the cathedral of San Nicola di Sassari (late Gothic but with a strong Renaissance influence), the church of Sant'Agostino di Cagliari (designed by Palearo Fratino), the church of Santa Caterina in Sassari (designed by Bernardoni, a pupil of Vignola).

On the contrary, the Baroque architecture has found wide prominence, interesting examples are the Collegiata di Sant'Anna in Cagliari, the facade of the Cathedral of San Nicola in Sassari, the church of San Michele in Cagliari, as well as the cathedral of Cagliari, Ales and Oristano, rebuilt or modified between the seventeenth and eighteenth centuries.

Starting from the nineteenth century, new architectural forms of neoclassical inspiration spread throughout the island. Among the most important figures of this architectural and urban phase is that of the architect from Cagliari Gaetano Cima, whose works are scattered throughout the Sardinian territory. Alongside the works of Cima, it is worth mentioning those of Giuseppe Cominotti (Palazzo and Civic Theater of Sassari) and Antonio Cano (dome of S. Maria di Betlem in Sassari and the cathedral of Santa Maria della Neve in Nuoro). In the second half of the nineteenth century in Sassari was built the neo Gothic palace Giordano (1878) which is one of the earliest examples of revivalism in the island.

An interesting realization of eclectic style, derived from the union between revivalist and Art Nouveau models, appears to be the City hall of Cagliari, completed in the early twentieth century. The advent of fascism has strongly influenced architecture in Sardinia in the twenties and thirties: interesting achievements of that period are the new centers of Fertilia, Arborea and the city of Carbonia, one of the greatest examples of rationalist architecture.

Numerous findings of the typical statues of the Mother Goddess and pottery engraved with geometric designs testify the artistic expressions of the Pre-Nuragic peoples. Subsequently, the Nuragic civilization produced hundreds of bronze statuettes and the enigmatic stone statuary of the Giants of Mont'e Prama.

The union between the nuragic populations and the merchants coming from every part of the Mediterranean led to a refined production of gold artifacts, rings, earrings and jewelery of all kinds, but also votive steles and wall decorations. In addition to architecture linked to public works, the Romans introduced the mosaics and decorated the rich villas of the patricians with sculptures and paintings.

In the Middle Ages, during the Giudicati period, the architecture of the churches were enriched with capitals, sarcophagi, frescoes, marble altars and later embellished with retables, paintings by important artists such as the Master of Castelsardo, Pietro Cavaro, Andrea Lusso, and the school of the so-called Master of Ozieri who was headed by Giovanni del Giglio and Pietro Giovanni Calvano, of Senese origin.

In the nineteenth century and in early twentieth century originated the myths of an uncontaminated and timeless island. Recounted by the many travelers who visited Sardinia in that period, like D. H. Lawrence, such myths were celebrated mainly by Sardinian artists such as Giuseppe Biasi, Francesco Ciusa, Filippo Figari, Mario Delitala and Stanis Dessy. In their works they highlighted the autochthonous values of the agro-pastoral world, not yet homologated to the modernity that was pressing from the outside. Other important Sardinian artists of the second half of the twentieth century were Costantino Nivola, Maria Lai, Albino Manca and Pinuccio Sciola.

Megalithic building structures called nuraghes are scattered in great numbers throughout Sardinia. Su Nuraxi di Barumini is a UNESCO World Heritage Site.

Italian, which is the official language throughout Italy, is the most widely spoken language today, followed by the island's historical language, Sardinian ("su sardu").

Sardinian is a distinct branch of the Romance language family: it is therefore a separate language rather than an Italian dialect, and it is also closer to its Latin roots than Italian itself. Sardinian has been formally recognized as one of the twelve historical language minorities of Italy since 1997, by regional and Italian law. The language has been influenced by Catalan, Spanish and recently Italian, while the once spoken Nuragic contributes many features to it in many ancient remnants. In 2006 the regional administration has approved the use of a standardised writing system, the so-called "Limba Sarda Comuna", in official acts. As a literary language, Sardinian is gaining importance, despite heated debate about the lack of a commonly acknowledged standard orthography and controversial proposed solutions to this problem.

The two most widely spoken forms of the language are the Southern dialects, known as Campidanese ("su sardu campidanesu"), and the Northern dialects known as Logudorese ("su sardu logudoresu"), extending almost to the suburbs of Sassari. The Sardinian language is quite different from the other Romance languages and is homogeneous in terms of morphology, syntax and lexicon, but it also shows a spectrum of variation in terms of phonetics between the Northern and the Southern dialects.

Sassarese ("lu sassaresu") and Gallurese ("lu gadduresu") are classified as Corso-Sardinian languages, therefore more akin to the Italo-Dalmatian branch than to the Sardinian one, and are spoken in the north.
In Sardinia there are examples of language islands: Algherese ("l'alguerés") is a dialect of Catalan spoken in the city of Alghero; on the islands of San Pietro and Sant'Antioco, located in the extreme south west of Sardinia, the local population speaks a variant of Ligurian called Tabarchino ("el tabarchin"); fewer and fewer people speak Venetian, Friulian and Istriot in Arborea and Fertilia, since these villages have been populated in the 1920s and 1930s by colonists who mainly came from north-eastern Italy, and families from Istria and Dalmatia immediately after World War II.

Due to the Italian assimilation policies carried out since 1760 and the ongoing absorption into the Italian culture, over the course of time the once prevalent indigenous languages have been increasingly losing ground to Italian and the process of ongoing language shift has led to their extreme endangerment. In fact, according to the data published by ISTAT in 2006, 52.5% of the Sardinian population speaks just Italian in the family environment, while 29.3% alternates Italian and Sardinian and only 16.6% uses Sardinian or other non-Italian languages; outside the circle of family and friends, the last option drops to 5,2%.

Following the recent growth of the foreign-born population, the presence of other languages, principally Romanian, Arabic, Wolof and Chinese, is expanding in some urban areas.

Colourful and of various and original forms, the Sardinian traditional clothes are a clear symbol of belonging to specific collective identities. Although the basic model is homogeneous and common throughout the island, each town or village has its own traditional clothing which differentiates it from the others.

Sardinia is home to one of the oldest forms of vocal polyphony, generally known as cantu a tenore. In 2005, Unesco classed the "cantu a tenore" among intangible world heritage. Several famous musicians have found it irresistible, including Frank Zappa, Ornette Coleman, and Peter Gabriel. The latter travelled to the town of Bitti in the central mountainous region and recorded the now world-famous Tenores di Bitti CD on his Real World label. The guttural sounds produced in this form make a remarkable sound, similar to Tuvan throat singing. Another polyphonic style of singing, more like the Corsican "paghjella" and liturgic in nature, is found in Sardinia and is known as "cantu a cuncordu".

Another unique instrument is the launeddas. Three reed-canes (two of them glued together with beeswax) produce distinctive harmonies, which have their roots many thousands of years ago, as demonstrated by the bronze statuettes from Ittiri, of a man playing the three reed canes, dated to 2000 BC.

Beyond this, the tradition of "cantu a chiterra" (guitar songs) has its origins in town squares, when artists would compete against one another. The most famous singer of this genre are Maria Carta and Elena Ledda.

Sardinian culture is alive and well, and young people are actively involved in their own music and dancing. In 2004, BBC presenter Andy Kershaw travelled to the island with Sardinian music specialist Pablo Farba and interviewed many artists. His programme can be heard on BBC Radio 3. Sardinia has produced a number of notable jazz musicians such as Antonello Salis, Marcello Melis, and Paolo Fresu.

The main opera houses of the island are the Teatro Lirico in Cagliari and the Teatro Verdi in Sassari (soon to be replaced by the new Teatro Auditorium Comunale).

Meat, dairy products, grains and vegetables constitute the most basic elements of the traditional diet,
to a lesser extent rock lobster ("aligusta"), scampi, bottarga ("butàriga"), squid, tuna.

Suckling pig ("porcheddu") and wild boar ("sirbone") are roasted on the spit or boiled in stews of beans and vegetables, thickened with bread.
Herbs such as mint and myrtle are used. Much Sardinian bread is made dry, which keeps longer than high-moisture breads.
Those are baked as well, including "civraxiu", "coccoi pintau", a highly decorative bread and "pistoccu" made with flour and water only, originally meant for herders, but often served at home with tomatoes, basil, oregano, garlic and a strong cheese. Traditional cheeses include pecorino sardo, pecorino romano, casizolu, ricotta and the casu marzu (notable for containing live insect larvae).

One of the most famous of foods is pane carasau, the flat bread of Sardinia, famous for its thin crunchiness. Originally the making of this bread was a hard process which needed three women to do the job. This flat bread is always made by hand as it gives a different flavor the more you work the dough. After working the dough it will be rolled out in very thin circles and placed in an extremely hot stone oven where the dough will blow up into a ball shape. Once the dough achieves that state it is then removed from the oven where it is then cut into two thin sheets and stacked to go back into the oven.

Alcoholic beverages include many peculiar wines such as Cannonau, Malvasia, Vernaccia, Vermentino, various liquors like Abbardente, Filu Ferru and Mirto. Beer is the most drunk alcoholic beverage, Sardinia boasts the highest consumption per capita of beer in Italy (twice higher than national average). Birra Ichnusa is the most commercialized beer produced in Sardinia.

Cagliari is home to Cagliari Calcio, which was founded in 1920 and play in the Serie A, the Italian first division. It won the Italian Championship in the 1969–70 Serie A season, becoming the first club in Southern Italy to achieve such a result. Home matches are played at the Sardegna Arena.

Sassari is home to Dinamo Basket Sassari, the only Sardinian professional basketball club playing in the Lega Basket Serie A, the highest level club competition in Italian professional basketball.
It was founded in 1960, and is also known as Dinamo Banco di Sardegna thanks to a long sponsorship deal with the Sardinian bank. Since its promotion in Lega A in 2010, it has been enjoying the support of fans from Sassari and all over Sardinia with full-house matches on every game played at home. Dinamo Sassari achieved the highest titles in the Italian basketball in 2015, winning the Coppa Italia, the Supercoppa and the Italian basketball championship.

In the Province of Sassari is the Mores motor racing circuit, the only FIA Circuit homologated by CSAI (Cars) and the IMF (Motorcycles), in Sardinia.

Cagliari hosted a Formula 3000 race in 2002 and 2003 on a 2.414-km street circuit around Sant'Elia stadium. In 2003, Renault F1's Jarno Trulli and former Ferrari driver Jean Alesi did a spectacular exhibition. At the Grand Prix BMW-F1 driver Robert Kubica took part in a F3 car, as did BMW WTCC Augusto Farfus, GP2's Fairuz Fauzy and Vitaly Petrov. Since 2004 Sardinia has hosted the Rally d'Italia Sardegna, a rally competition in the FIA World Rally Championship schedule. The rally is held on narrow, twisty, sandy and bumpy mountainous roads in the north of the island.

On the island of Caprera is located the "Centro Velico Caprera", that is considered one of the largest school of sailing in the Mediterranean Sea, founded in 1967.

The Yacht Club Costa Smeralda located in Porto Cervo and founded in 1967 is the main yachting club in the island.

Annually the island hosts the Loro Piana Super Yacht Regatta and the Maxy Yacht Rolex Cup.
Part of the Louis Vuitton Trophy was held in the Maddalena archipelago in 2010.

"Vento di Sardegna" (en: Wind of Sardinia) was a sailboat sponsored by the Autonomous Region of Sardinia. Its skipper, Andrea Mura, won the Single-Handed Trans-Atlantic Race in 2013 and in 2017, the Two Handed Transatlantic Race (Twostar) regatta in 2012 and the Route du Rhum.

Porto Pollo, north of Palau, is a bay well known by windsurfers and Kitesurfers. The bay is divided by a thin tongue of land that separates it in an area for advanced and beginner/intermediate windsurfers. There is also a restricted area for kitesurf. Many freestyle windsurfers gwent to Porto Pollo for training and 2007 saw the finale of the freestyle pro kids Europe 2007 contest. Because of the Venturi effect between Sardinia and Corsica, western wind accelerates between the islands and creates the wind that makes Porto Pollo popular among windsurfing enthusiasts.

Cagliari hosts regularly international regattas, such RC44 championship, Farr 40 World championship, Audi MedCup and Kite Championships. In view of the 36th America's Cup, scheduled to take place in New Zealand in 2021, Luna Rossa Challenge has chose Cagliari as place for its preparation.

Four ski resorts are located on the Gennargentu Range at Separadorgiu, Monte Spada, S'Arena and Bruncu Spina, they are equipped with ski schools, skilifts and ski equipment hire.

"S'Istrumpa", also known as Sardinian Wrestling, is a traditional Sardinian sport, officially recognized by the Italian National Olympic Committee (C.O.N.I.) and the International Federation of Celtic Wrestling (I.F.C.W.). It shows similarities with the Scottish Backhold and the gouren. Istrumpa's wrestlers participate annually at the championships for Celtic wrestling stiles.

Sardinia boasts ancient equestrian traditions and is the Italian region with the highest number of horse riders (29% of population) and boasts also fine darts tradition, which many believe originated in the Sassari region of the country towards the end of the 15th century. In those days, the darts were carved from beech ("fagus") wood and the flights were feathers drawn from the indigenous "pollo sultano" ('sultana bird'), famed for its spectacular violet-blue plumage.

Following an enormous reforestation plan Sardinia has become the Italian region with the largest forest extension. 1,213,250 hectares (12,132 km) or 50% of the island is covered by forested areas. The "Corpo forestale e di vigilanza ambientale della Regione Sarda" is the Sardinian Forestry Corps. Sardinia is the Italian region most affected by forest fires during the summer.

The Regional Landscape Plan prohibits new building activities on the coast (except in urban centers), next to forests, lakes or other environmental or cultural sites and the Coastal conservation agency ensures the protection of natural areas on the Sardinian coast.

Renewable energies have increased noticeably in recent years, mainly wind power, favoured by the windy climate, but also solar power (Carlo Rubbia, Nobelist in physics, is creating an experimental solar thermal energy central) and biofuel, based on jatropha oil and colza oil. 586.8 megawatts of wind power capacity were installed on the island at the end of 2009.

Sardinia is home to a wide variety of rare or uncommon animals, such as several species of mammals, many of them belonging to an endemic subspecies: the Mediterranean monk seal, Sarcidano horse, Giara horse, albino donkey, Sardinian feral cat, mouflon, Sardinian long-eared bat, Sardinian deer, fallow deer, Sardinian fox ("Vulpes vulpes ichnusae"), Sardinian hare ("Lepus capensis mediterraneus"), wild boar ("Sus scrofa meridionalis"), edible dormouse and European pine marten.

Rare amphibians, found only on the island, are the Sardinian brook salamander, brown cave salamander, imperial cave salamander, Monte Albo cave salamander, Supramonte cave salamander and Sarrabus cave salamander ("Speleomantes sarrabusensis"); the Sardinian tree frog is also found in Corsica and in the Tuscan Archipelago. Among reptiles worthy of note are Bedriaga's rock lizard, the Tyrrhenian wall lizard and Fitzinger's algyroides, endemic species of Sardinia and Corsica. The island is inhabited by terrestrial tortoises and sea turtles like Hermann's tortoise, the spur-thighed tortoise, marginated tortoise ("Testudo marginata sarda"), Nabeul tortoise, loggerhead sea turtle and green sea turtle. A new arachnid species, endemic to the island, has been recently found: the Nuragic spider.

Sardinia has four endemic subspecies of birds found nowhere else in the world: its great spotted woodpecker (ssp "harterti"), great tit (ssp "ecki"), common chaffinch (ssp "sarda"), and Eurasian jay (ssp "ichnusae"). It also shares a further 10 endemic subspecies of bird with Corsica. In some cases Sardinia is a delimited part of the species range. For example, the subspecies of hooded crow, "Corvus cornix" ssp "cornix" occurs in Sardinia and Corsica, but no further south.

Some birds of prey found here are the griffon vulture, common buzzard, golden eagle, long-eared owl, western marsh harrier, peregrine falcon, European honey buzzard, Sardinian goshawk ("Accipiter gentilis arrigonii"), Bonelli's eagle and Eleonora's falcon, whose name comes from Eleonor of Arborea, national heroine of Sardinia, expert in falconry.
The hundreds of lagoons and coastal lakes that dot the island are home for many species of wading birds, such as the greater flamingo.

Conversely, Sardinia lacks many species common on the European continent, such as the viper, wolf, bear and marmot.

The island has also long been used for grazing flocks of indigenous Sardinian sheep. The Sardinian Anglo-Arab is a horse breed that was established in Sardinia, where it has been selectively bred for more than one hundred years.

Three different breeds of dogs are peculiar to Sardinia: the Pastore Fonnese, Dogo Sardo and Levriero Sardo.

In Sardinia there are more than 100 beaches. The geology of the island provides a variety of beaches, for example beaches made of fine sand or of quartz grain. Along the west coast there are steep cliffs and gentle sandy beaches. The northern east coast (near Olbia) has a lot of large sandy beaches. The middle of the east coast (near Cala Gonone) consists of cliffs and caves. And in the south-east coast (Villasimius, Arbatax and other villages) there are rocky beaches as well as sandy beaches.

Over 600,000 hectares of Sardinian territory is environmentally preserved (about 25% of the island's territory).
The island has three national parks:

Ten regional parks:

There are 60 wildlife reserves, 5 W.W.F oases, 25 natural monuments and one Geomineral Park, preserved by UNESCO.

Northern Sardinian Coasts are included in the Pelagos Sanctuary for Mediterranean Marine Mammals, a Marine Protected Area, that covers a surface of about , aimed at the protection of marine mammals.





</doc>
<doc id="29378" url="https://en.wikipedia.org/wiki?curid=29378" title="Scrooge McDuck">
Scrooge McDuck

Scrooge McDuck is a fictional character created in 1947 by Carl Barks during his time as a work-for-hire for The Walt Disney Company. Scrooge is an elderly Scottish anthropomorphic Pekin duck with a yellow-orange bill, legs, and feet. He typically wears a red or blue frock coat, top hat, pince-nez glasses, and spats. He is portrayed in animations as speaking with a Scottish accent.

Named after Ebenezer Scrooge from the 1843 novel "A Christmas Carol", Scrooge is an incredibly wealthy business magnate and self-proclaimed "adventure-capitalist" whose dominant character trait is his thrift. He is brother to Matilda McDuck and Hortense McDuck, the maternal uncle of Della and Donald Duck, the grand-uncle of Huey, Dewey, and Louie, and a usual financial backer of Gyro Gearloose. Within the context of the fictional Duck universe, he is the world's richest person. He is an oil tycoon, businessman, owner of the largest mining concerns, many factories to operate different activities. His "Money Bin"—and indeed Scrooge himself—are often used as a humorous metonyms for great wealth in popular culture around the world.

McDuck was initially characterized as a greedy miser and antihero (as Charles Dickens' original Scrooge was), but in later appearances he was often portrayed as a charitable and thrifty hero, adventurer, explorer, and philanthropist. He was originally created by Barks as an antagonist for Donald Duck, first appearing in the 1947 "Four Color" story "Christmas on Bear Mountain" (#178). However, McDuck's popularity grew so large that he became a major figure of the Duck universe. In 1952 he was given his own comic book series, called "Uncle Scrooge", which still runs today.

Scrooge was most famously drawn by his creator Carl Barks, and later by Don Rosa. Like other Disney franchise characters, Scrooge McDuck's international popularity has resulted in literature that is often translated into other languages. Comics have remained Scrooge's primary medium, although he has also appeared in animated cartoons, most extensively in the television series "DuckTales" (1987–1990) and its reboot as the main protagonist of both series.

Scrooge McDuck, maternal uncle of previously established character Donald Duck, made his first named appearance in the story "Christmas on Bear Mountain" which was published in Dell's Four Color Comics #178, December 1947, written and drawn by artist Carl Barks. His appearance may have been based on a similar-looking, Scottish "thrifty saver" Donald Duck character from the 1943 propaganda short "The Spirit of '43".

In "Christmas on Bear Mountain", Scrooge was a bearded, bespectacled, reasonably wealthy old duck, visibly leaning on his cane, and living in isolation in a "huge mansion". Scrooge's misanthropic thoughts in this first story are quite pronounced: ""Here I sit in this big lonely dump, waiting for Christmas to pass! Bah! That silly season when everybody loves everybody else! A curse on it! Me—I'm different! Everybody hates me, and I hate everybody!""

Barks later reflected, "Scrooge in 'Christmas on Bear Mountain' was only my first idea of a rich, old uncle. I had made him too old and too weak. I discovered later on that I had to make him more active. I could not make an old guy like that do the things I wanted him to do."

Barks would later claim that he originally only intended to use Scrooge as a one-shot character, but then decided Scrooge (and his fortune) could prove useful for motivating further stories. Barks continued to experiment with Scrooge's appearance and personality over the next four years.

Scrooge's second appearance, in "The Old Castle's Secret" (first published in June 1948), had Scrooge recruiting his nephews to search for a family treasure hidden in Dismal Downs, the McDuck family's ancestral castle, built in the middle of Rannoch Moor in Scotland. "Foxy Relations" (first published in November 1948) was the first story where Scrooge is called by his title and catchphrase "The Richest Duck in the World".

The story, "Voodoo Hoodoo", first published in Dell's Four Color Comics #238, August 1949, was the first story to hint at Scrooge's past with the introduction of two figures from it. The first was Foola Zoola, an old African sorcerer and chief of the Voodoo tribe who had cursed Scrooge, seeking revenge for the destruction of his village and the taking of his tribe's lands by Scrooge decades ago.

Scrooge privately admitted to his nephews that he had used an army of "cutthroats" to get the tribe to abandon their lands, in order to establish a rubber plantation. The event was placed by Carl Barks in 1879 during the story, but it would later be retconned by Don Rosa to 1909 to fit with Scrooge's later-established personal history. 

The second figure was Bombie the Zombie, the organ of the sorcerer's curse and revenge. He had reportedly sought Scrooge for decades before reaching Duckburg, mistaking Donald for Scrooge.

Barks, with a note of skepticism often found in his stories, explained the zombie as a living person who has never died, but has somehow gotten under the influence of a sorcerer. Although some scenes of the story were intended as a parody of Bela Lugosi's "White Zombie", the story is the first to not only focus on Scrooge's past but also touch on the darkest aspects of his personality.

"Trail of the Unicorn", first published in February 1950, introduced Scrooge's private zoo. One of his pilots had managed to photograph the last living unicorn, which lived in the Indian part of the Himalayas. Scrooge offered a reward to competing cousins Donald Duck and Gladstone Gander, which would go to the one who captured the unicorn for Scrooge's collection of animals.

This was also the story that introduced Scrooge's private airplane. Barks would later establish Scrooge as an experienced aviator. Donald had previously been shown as a skilled aviator, as was Flintheart Glomgold in later stories. In comparison, Huey, Dewey, and Louie were depicted as only having taken flying lessons in the story "Frozen Gold" (published in January 1945).

"The Pixilated Parrot", first published in July 1950, introduced the precursor to Scrooge's money bin; in this story, Scrooge's central office building is said to contain "three cubic acres of money". Two nameless burglars who briefly appear during the story are considered to be the precursors of the Beagle Boys.

"The Magic Hourglass", first published in September 1950, was arguably the first story to change the focus of the Duck stories from Donald to Scrooge. During the story, several themes were introduced for Scrooge.

Donald first mentions in this story that his uncle practically owns Duckburg, a statement that Scrooge's rival John D. Rockerduck would later put in dispute. Scrooge first hints that he was not born into wealth, as he remembers buying the Hourglass in Morocco when he was a member of a ship's crew as a cabin boy. It is also the first story in which Scrooge mentions speaking another language besides his native English and reading other alphabets besides the Latin alphabet, as during the story, he speaks Arabic and reads the Arabic alphabet.

The latter theme would be developed further in later stories. Barks and current Scrooge writer Don Rosa have depicted Scrooge as being fluent in Arabic, Dutch, German, Mongolian, Spanish, Mayan, Bengali, Finnish, and a number of Chinese dialects. Scrooge acquired this knowledge from years of living or traveling to the various regions of the world where those languages are spoken. Later writers would depict Scrooge having at least working knowledge of several other languages.

Scrooge was shown in "The Magic Hourglass" in a more positive light than in previous stories, but his more villainous side is present too. Scrooge is seen in this story attempting to reacquire a magic hourglass that he gave to Donald, before finding out that it acted as a protective charm for him. Scrooge starts losing one billion dollars each minute, and comments that he will go bankrupt within 600 years. This line is a parody of Orson Welles's line in "Citizen Kane" "You know, Mr. Thatcher, at the rate of a million dollars a year, I'll have to close this place in... 60 years". To convince his nephews to return it, he pursues them throughout Morocco, where they had headed to earlier in the story. Memorably during the story, Scrooge interrogates Donald by having him tied up and tickled with a feather in an attempt to get Donald to reveal the hourglass's location. Scrooge finally manages to retrieve it, exchanging it for a flask of water, as he had found his nephews exhausted and left in the desert with no supplies. As Scrooge explains, he intended to give them a higher offer, but he just could not resist having somebody at his mercy without taking advantage of it.

"A Financial Fable", first published in March 1951, had Scrooge teaching Donald some lessons in productivity as the source of wealth, along with the laws of supply and demand. Perhaps more importantly, it was also the first story where Scrooge observes how diligent and industrious Huey, Louie and Dewey are, making them more similar to himself rather than to Donald. Donald in Barks's stories is depicted as working hard on occasion, but given the choice often proves to be a shirker. The three younger nephews first side with Scrooge rather than Donald in this story, with the bond between granduncle and grandnephews strengthening in later stories. However, there have been rare instances where Donald proved invaluable to Scrooge, such as when the group traveled back in time to Ancient Egypt to retrieve a pharaoh's papyrus. Donald cautions against taking it with him, as no one would believe the story unless it was unearthed. Donald then buries it and makes a marking point from the Nile River, making Scrooge think to himself admiringly, "Donald must have swallowed the !"

"Terror of the Beagle Boys", first published in November 1951, introduced the readers to the Beagle Boys, although Scrooge in this story seems to be already familiar with them. "The Big Bin on Killmotor Hill" introduced Scrooge's money bin, built on Killmotor Hill in the center of Duckburg.

By this point, Scrooge had become familiar to readers in the United States and Europe. Other Disney writers and artists besides Barks began using Scrooge in their own stories, including Italian writer Romano Scarpa. Western Publishing, the then-publisher of the Disney crafty comics, started thinking about using Scrooge as a protagonist rather than a supporting character, and then decided to launch Scrooge in his own self-titled comic. "Uncle Scrooge" #1, featuring the story "Only a Poor Old Man", was published in March 1952. This story along with "Back to the Klondike", first published a year later in March 1953, became the biggest influences in how Scrooge's character, past, and beliefs would become defined.

After this point, Barks produced most of his longer stories in "Uncle Scrooge", with a focus mainly on adventure, while his ten-page stories for Walt Disney's Comics and Stories continued to feature Donald as the star and focused on comedy. In Scrooge's stories, Donald and his nephews were cast as Scrooge's assistants, who accompanied Scrooge in his adventures around the world. This change of focus from Donald to Scrooge was also reflected in stories by other contemporary writers. Since then, Scrooge remains a central figure of the Duck comics' universe, thus the coining of the term "Scrooge McDuck Universe".

After Barks's retirement, the character continued under other artists. In 1972, Barks was persuaded to write more stories for Disney. He wrote Junior Woodchuck stories where Scrooge often plays the part of the villain, closer to the role he had before he acquired his own series. Under Barks, Scrooge always was a malleable character who would take on whatever persona was convenient to the plot.

The Italian writer and artist Romano Scarpa made several additions to Scrooge McDuck's universe, including characters such as Brigitta McBridge, Scrooge's self-styled fiancée, and Gideon McDuck, a newspaper editor who is Scrooge's brother. Those characters have appeared mostly in European comics. So is also the case for Scrooge's rival John D. Rockerduck (created by Barks for just one story) and Donald's cousin Fethry Duck, who sometimes works as a reporter for Scrooge's newspaper.

Another major development was the arrival of writer and artist Don Rosa in 1986 with his story "The Son of the Sun", released by Gladstone Publishing and nominated for a Harvey Award, one of the comics industry's highest honors. Rosa has said in interviews that he considers Scrooge to be his favorite Disney character. Unlike most other Disney writers, Don Rosa considered Scrooge as a historical character whose Disney adventures had occurred in the fifties and sixties and ended (in his undepicted death) in 1967 when Barks retired. He considered only Barks' stories canonical, and fleshed out a timeline as well as a family tree based on Barks' stories. Eventually he wrote and drew "The Life and Times of Scrooge McDuck", a full history in twelve chapters which received an Eisner Award in 1995. Later editions included additional chapters. Under Rosa, Scrooge became more ethical; while he never cheats, he ruthlessly exploits any loopholes. He owes his fortune to his hard work and his money bin is "full of souvenirs" since every coin reminds him of a specific circumstance. Rosa remains the foremost contemporary duck artist and has been nominated for five 2007 Eisner Awards. His work is regularly reprinted by itself as well as along with Barks stories for which he created a sequel.

Daan Jippes, who can mimic Barks's art to a close extent, repenciled all of Barks's 1970s Junior Woodchucks stories, as well as Barks' final Uncle Scrooge stories, from the 1990s to the early 2000s. Other notable Disney artists who have worked with the Scrooge character include Marco Rota, William Van Horn, and Tony Strobl.

In an interview with the Norwegian "Aftenposten" from 1992 Don Rosa says that "in the beginning Scrooge [owed] his existence to his nephew Donald, but that has changed and today it's Donald that [owes] his existence to Scrooge" and he also says that this is one of the reasons why he is so interested in Scrooge.

The character is almost exclusively portrayed as having worked his way up the financial ladder from humble immigrant roots.

The comic book series The Life and Times of Scrooge McDuck, (written by Don Rosa) shows Scrooge as a young boy, he took up a job polishing and shining boots in his native Glasgow. A pivotal moment comes when a ditchdigger pays him with a 1875 US dime, which was useless as currency in 19th century Glasgow. Enraged, Scrooge vowed to never be taken advantage of again, to be "sharper than the sharpies and smarter than the smarties." He takes a position as cabin boy on a Clyde cattle ship to the United States to make his fortune at the age of 13. In 1898, after many adventures he finally ends up in Klondike, where he finds a golden rock the size of a goose's egg. By the following year he had made his first $1,000,000 and bought the deed for Killmule Hill from Casey Coot, the son of Clinton Coot and grandson of Cornelius Coot. He finally ends up in Duckburg in 1902. After some dramatic events where he faces both the Beagle Boys and president Roosevelt and his "Rough Riders" at the same time, he tears down the rest of the old fort Duckburg and builds his famous Money Bin at the site.

In the years to follow, Uncle Scrooge travels all around the world in order to increase his fortune, while his family remained behind to manage the Money Bin. When Scrooge finally returns to Duckburg, he is the richest duck in the world, rivaled only by Flintheart Glomgold, John D. Rockerduck, and less prominently, the maharaja of the fictional country Howdoyoustan (play on Hindustan). His experiences, however, had changed him into a hostile miser, and he made his own family leave. Some 12 years later, he closed his empire down, but eventually returned to a public life 5 years later and started his business.

He keeps the majority of his wealth in a massive Money Bin overlooking the city of Duckburg. In the short "Scrooge McDuck and Money", he remarks to his nephews that this money is "just petty cash". In the Dutch and Italian version he regularly forces Donald and his nephews to polish the coins one by one in order to pay off Donald's debts; Scrooge will not pay them much for this lengthy, tedious, hand-breaking work. As far as he is concerned, even 5 cents an hour is too much expenditure.

A shrewd businessman and noted tightwad, he is fond of diving into and swimming in his money, without injury. He is also the richest member of The Billionaires Club of Duckburg, a society which includes the most successful businessmen of the world and allows them to keep connections with each other. Glomgold and Rockerduck are also influential members of the Club. His most famous prized possession is his Number One Dime.
The sum of Scrooge's wealth is unclear. According to Barks' "The Second Richest Duck" as noted by a "Time" article, Scrooge is worth "one multiplujillion, nine obsquatumatillion, six hundred twenty-three dollars and sixty-two cents". In the "DuckTales" episode "Liquid Assets", Fenton Crackshell (Scrooge's accountant) notes that McDuck's money bin contains "607 tillion 386 zillion 947 trillion 522 billion dollars and 36 cents". Don Rosa's "Life and Times of Scrooge McDuck" notes that Scrooge amounts to "five multiplujillion, nine impossibidillion, seven fantastica trillion dollars and sixteen cents". A thought bubble from Scrooge McDuck sitting in his car with his chauffeur in "Walt Disney's Christmas Parade" No.1 (published in 1949) that takes place in the story "Letter to Santa" clearly states "What's the use of having 'eleven octillion dollars' if I don't make a big noise about it?" In the DuckTales "", Scrooge mentions "We quadzillionaires have our own ideas of fun." In the first episode of the 2017 Ducktales series, McDuck states that he runs "a multi-trillion dollar business."

"Forbes" magazine has occasionally tried to estimate McDuck's wealth in real terms; in 2007, the magazine estimated his wealth at $28.8 billion; in 2011, it rose to $44.1 billion due to the rise in gold prices. The YouTube Channel Film Theory used the size of Scrooge's Money Bin as a basis and calculated that it could contain over $300 trillion. Whatever the amount, Scrooge never considers it to be enough; he believes that he has to continue to earn money by any means possible. A running gag is Scrooge always making profit on any business deal.

Scrooge never completed a formal education, as he left school at an early age. However, he has a sharp mind and is always ready to learn new skills. Because of his secondary occupation as a treasure hunter, Scrooge has become something of a scholar and an amateur archaeologist. Starting with Barks, several writers have explained how Scrooge becomes aware of the treasures he decides to pursue. This often involves periods of research consulting various written sources in search of passages that might lead him to a treasure. Often Scrooge decides to search for the possible truth behind old legends, or discovers obscure references to the activities of ancient conquerors, explorers and military leaders that he considers interesting enough to begin a new expedition.

As a result of his research, Scrooge has built up an extensive personal library, which includes many rare tomes. In Barks's and Rosa's stories, among the prized pieces of this library is an almost complete collection of Spanish and Dutch naval logs of the 16th and 17th centuries. Their references to the fates of other ships have often allowed Scrooge to locate sunken ships and recover their treasures from their watery graves. Mostly self-taught as he is, Scrooge is a firm believer in the saying "knowledge is power". Scrooge is also an accomplished linguist and entrepreneur, having learned to speak several different languages during his business trips around the world, selling refrigerators to Eskimos, wind to windmill manufacturers in the Netherlands, etc.

Both as a businessman and as a treasure hunter, Scrooge is noted for his drive to set new goals and face new challenges. As Carl Barks described his character, for Scrooge there is "always another rainbow". The phrase later provided the title for one of Barks's better-known paintings depicting Scrooge. Periods of inactivity between adventures and lack of serious challenges tend to be depressing for Scrooge after a while; some stories see these phases take a toll on his health. Scrooge's other motto is "Work smarter, not harder."

As a businessman, Scrooge often resorts to aggressive tactics and deception. He seems to have gained significant experience in manipulating people and events towards his own ends. As often seen in stories by writer Guido Martina and occasionally by others, Scrooge is noted for his cynicism, especially towards ideals of morality when it comes to business and the pursuit of set goals. This has been noted by some as not being part of Barks's original profile of the character, but has since come to be accepted as one valid interpretation of Scrooge's way of thinking.

Scrooge seems to have a personal code of honesty that offers him an amount of self-control. He can often be seen contemplating the next course of action, divided between adopting a ruthless pursuit of his current goal against those tactics he considers more honest. At times, he can sacrifice his goal in order to remain within the limits of this sense of honesty. Several fans of the character have come to consider these depictions as adding to the depth of his personality, because based on the decisions he takes Scrooge can be both the hero and the villain of his stories. This is one thing he has in common with his nephew Donald. Scrooge's sense of honesty also distinguishes him from his rival Flintheart Glomgold, who places no such self-limitations. During the cartoon series "DuckTales", at times he would be heard saying to Glomgold, "You're a cheater, and cheaters "never" prosper!"

Scrooge has a nasty temper and rarely hesitates to use cartoon violence against those who provoke his ire (often his nephew Donald, but also bill and tax collectors as well as door-to-door salesmen); however, he seems to be against the use of lethal force. On occasion, he has even saved the lives of enemies who had threatened his own life but were in danger of losing their own. According to Scrooge's own explanation, this is to save himself from feelings of guilt over their deaths; he generally awaits no gratitude from them. Scrooge has also opined that only in fairy tales do bad people turn good, and that he is old enough to not believe in fairy tales. Scrooge believes in keeping his word—never breaking a promise once given. In Italian-produced stories of the 1950s to 1970s, however, particularly those written by Guido Martina, Scrooge often acts differently from in American or Danish comics productions.

Carl Barks gave Scrooge a definite set of ethics which were in tone with the time he was supposed to have made his fortune. The robber barons and industrialists of the 1890–1920s era were McDuck's competition as he earned his fortune. Scrooge proudly asserts "I made it by being tougher than the toughies and smarter than the smarties! And I made it square!" It is obvious that Barks's creation is averse to dishonesty in the pursuit of wealth. When Disney filmmakers first contemplated a Scrooge feature cartoon in the fifties, the animators had no understanding of the Scrooge McDuck character and merely envisioned Scrooge as a duck version of Ebenezer Scrooge—a very unsympathetic character. In the end they shelved the idea because a duck who gets all excited about money just was not funny enough.

In an interview, Barks summed up his beliefs about Scrooge and capitalism:

Scrooge is very misunderstood. In his early years, he was very kind and polite. But the 'slaps' of society from cruel people, as well as the ungratefulness of those who he had helped to overcome their problems, made Scrooge cruel, selfish, and powerful. Feeling that he had been taken advantage of, he didn't want to believe that others had real problems or difficulties in their lives. This made him seem obnoxious at best, and heartless at worst. As a result, no one could understand his problems, including his nephew, and his great-nephews. This isolation paved the path to acquiring untold wealth and power. But despite it all, he still has a good heart, and will help those he sees as distressed or troubled.

In the "DuckTales" series, Scrooge has adopted the nephews (as Donald has joined the Navy and is away on his tour of duty), and as a result his rougher edges are smoothed out somewhat. While most of his traits remain from the comics, he is notably more jovial and less irritable in the animated cartoon. In an early episode, Scrooge credits his improved temperament to the nephews and Webby (his housekeeper's granddaughter, who comes to live in Scrooge's mansion), saying that "for the first time since I left Scotland, I have a family". Though Scrooge is far from heartless in the comics, he is rarely so openly sentimental. While he still hunts for treasure in "Ducktales", many episodes focus on his attempts to thwart villains. However, he remains just as tightfisted with money as he has always been. But he's also a goodhearted and generous with his family and friends.

Scrooge displays a strict code of honor, insisting that the only valid way to acquire wealth is to "earn it square," and he goes to great lengths to thwart those (sometimes even his own nephews) who gain money dishonestly. This code also prevents him from ever being dishonest himself, and he avows that "Scrooge McDuck's word is as good as gold." He also expresses great disgust at being viewed by others as a greedy liar and cheater.

The series fleshes out Scrooge's upbringing by depicting his life as an individual who worked hard his entire life to earn his keep and to fiercely defend it against those who were truly dishonest but also, he defend his family and friends from any dangers, including villains. His value teaches his nephews not to be dishonest with him or anybody else. It is shown that money is no longer the most important thing in his life. For one episode, he was under a love spell, which caused him to lavish his time on a goddess over everything else. The nephews find out that the only way to break the spell is make the person realize that the object of their love will cost them something they truly love. The boys make it appear that Scrooge's love is allergic to money; however, he simply decides to give up his wealth so he can be with her. Later, when he realizes he will have to give up his nephews to be with her, the spell is immediately broken, showing that family is the most important thing to him.

On occasion, he demonstrates considerable physical strength by single-handedly beating bigger foes. He credits his robustness to "lifting money bags."

Many of the European comics based on the Disney Universe have created their own version of Scrooge McDuck, usually involving him in slapstick adventures. This is particularly true of the Italian comics which were very popular in the 1960s-1980s in most parts of Western continental Europe. In these, Scrooge is mainly an anti-hero dragging his long-suffering nephews into treasure hunts and shady business deals. Donald is a reluctant participant in these travels, only agreeing to go along when his uncle reminds him of the debts and back-rent Donald owes him, threatens him with a sword or blunderbuss, or offers a share of the loot. When he promises Donald a share of the treasure, Scrooge will add a little loophole in the terms which may seem obscure at first but which he brings up at the end of the adventure to deny Donald his share, keeping the whole for himself. After Donald risks life and limb – something which Scrooge shows little concern for – he tends to end up with nothing.

Another running joke is Scrooge reminiscing about his adventures while gold prospecting in the Klondike much to Donald and the nephews' chagrin at hearing the never-ending and tiresome stories.

According to Carl Barks' 1955 one-pager "Watt an Occasion" ("Uncle Scrooge" #12), Scrooge is 75 years of age. According to Don Rosa, Scrooge was born in Scotland in 1867, and earned his Number One Dime (or First Coin) exactly ten years later. The "DuckTales" episodes (and many European comics) show a Scrooge who hailed from Scotland in the 19th century, yet was clearly familiar with all the technology and amenities of the 1980s. Despite this extremely advanced age, Scrooge does not appear to be on the verge of dotage, and is vigorous enough to keep up with his nephews in adventures; with rare exception there appears to be no sign of him slowing down.

Barks responded to some fan letters asking about Scrooge's Adamic age, that in the story "That's No Fable!", when Scrooge drank water from a Fountain of Youth for several days, rather than making him young again (bodily contact with the water was required for that), ingesting the water rejuvenated his body and cured him of his rheumatia, which arguably allowed Scrooge to live beyond his expected years with no sign of slowdown or senility. Don Rosa's solution to the issue of Scrooge's age is that he set all of his stories in the 1950s or earlier, which was when he himself discovered and reveled in Barks's stories as a kid, and in his unofficial timelines, he had Scrooge die in 1967, at the age of 100 years.

In the 15th Episode of the new Ducktales reboot it is revealed that Scrooge was also stuck (by unmentioned time) in the Demovogana timeless demon dimension which is used to explain his young look.

"Forbes" magazine routinely lists Scrooge McDuck on its annual "Fictional 15" list of the richest fictional characters by net worth:
Grupo Ronda S.A. Has the license to use the character, as well as other Disney characters in the board game "Tío Rico Mc. Pato " from 1972 to the present, being one of the most popular table games in Colombia and being the Direct competition of the Monopoly (game) in the region.

In tribute to its famous native, Glasgow City Council added Scrooge to its list of "Famous Glaswegians" in 2007, alongside the likes of Billy Connolly and Charles Rennie Mackintosh.

In 2008 "The Weekly Standard" parodied the bailout of the financial markets by publishing a memo where Scrooge applies to the TARP program.

An extortionist named Arno Funke targeted German department store chain Karstadt from 1992 until his capture in 1994, under the alias "Dagobert", the German (first) name for Scrooge McDuck.

In the "Family Guy" episode "Lottery Fever", Peter injures himself trying to dive into a pile of coins like Scrooge McDuck.

In the 2013 episode of "Breaking Bad", "Buried", Saul Goodman associate Patrick Kuby remarks to fellow associate Huell Babineaux "we are here to do a job, not channel Scrooge McDuck" when Huell lays down on Walter White's pile of cash stored in a storage facility locker.

"Dagobertducktaks" ("Dagobert Duck" is the Dutch name for Scrooge McDuck), a tax for the wealthy, was elected Dutch word of the year 2014 in a poll by Van Dale.

In August 2017, the YouTube channel "The Film Theorists", hosted by Matthew "MatPat" Patrick, estimated the worth of the gold coins in the money bin of Scrooge McDuck based on four sources, with the lowest source equaling $52,348,493,767.50 and the highest source ("three cubic acres") equaling $333,927,633,863,527.10 of gold value.

The popularity of Scrooge McDuck comics spawned an entire mythology around the character, including new supporting characters, adventures, and life experiences as told by numerous authors. The popularity of the Duck universe – the fandom term for the associated intellectual properties that have developed from Scrooge's stories over the years, including the city of Duckburg – has led Don Rosa to claim that "in the beginning Scrooge [owed] his existence to his nephew Donald, but that has changed and today it's Donald that [owes] his existence to Scrooge."

In addition to the many original and existing characters in stories about Scrooge McDuck, authors have frequently led historical figures to meet Scrooge over the course of his life. Most notably, Scrooge has met US president Theodore Roosevelt. Roosevelt and Scrooge would meet each other at least three times: in the Dakotas in 1883, in Duckburg in 1902, and in Panama in 1906. "See Historical Figures in Scrooge McDuck stories".

Based on writer Don Rosa's "The Life and Times of Scrooge McDuck", a popular timeline chronicling Scrooge's adventures was created consisting of the most important "facts" about Scrooge's life. "See Scrooge McDuck timeline according to Don Rosa".

In 2014, composer Tuomas Holopainen of Nightwish released a conceptual album based on the book, "The Life and Times of Scrooge McDuck". The album is titled "Music Inspired by the Life and Times of Scrooge". Don Rosa illustrated the cover artwork for the album .

The character of Scrooge has appeared in various mediums aside from comic books. Scrooge's voice was first heard on the 1960 record album "Donald Duck and His Friends;" Dal McKennon voiced the character for this appearance. Scrooge's first appearance in animated form (save for a brief "Mickey Mouse Club" television series cameo) was in Disney's 1967 theatrical short "Scrooge McDuck and Money" (voiced by Bill Thompson), in which he teaches his nephews basic financial tips.

In 1974, Disneyland Records released an adaptation of the Charles Dickens classic "A Christmas Carol," for which Alan Young was hired to voice Scrooge McDuck playing the character who inspired his name, Ebenezer Scrooge (Thompson had died in 1971). Young, who himself was born in Great Britain, was best known for playing Wilbur Post on the hit television series "Mister Ed" from 1961 to 1965. Eight years later, the Walt Disney Animation Studios decided to make a featurette of this same story, this time dubbed "Mickey's Christmas Carol" (1983), and once again hired Young to voice the role. He also appeared as himself in the television special "Sport Goofy in Soccermania" (1987) (the only time when he was voiced by Will Ryan).

Scrooge's biggest role outside comics would come in the 1987 animated series "DuckTales", a series loosely based on Carl Barks's comics, and where Alan Young returned to voice his character. In this series, premiered over two-hours on September 18, 1987, while the regular episodes began three days later, Scrooge becomes caretaker of Huey, Dewey and Louie when Donald joins the United States Navy. Scrooge's "DuckTales" persona is considerably softer than in most previous appearances; his ruthlessness is played down considerably and his often abrasive personality is reduced in many episodes to that of a crotchety but lovable old uncle. Still, there are flashes of Barks' Scrooge to be seen, particularly in early episodes of the first season. After the series Scrooge also appeared in "". He was mentioned in the "Darkwing Duck" episode "Tiff of the Titans", but never really seen.

He has appeared in some episodes of "Raw Toonage", two shorts of "Mickey Mouse Works" and some episodes (specially "House of Scrooge") of "Disney's House of Mouse", as well as the direct-to-video films "Mickey's Once Upon a Christmas" and "Mickey's Twice Upon a Christmas". His video game appearances include the three "DuckTales" releases ("DuckTales", "DuckTales 2", and ""), and in "Toontown Online" as the accidental creator of the Cogs. Additionally, he is a secret playable character in 2008 quiz game, "Disney TH!NK Fast". In the 2012 Nintendo 3DS game "", he is one of the first characters Mickey rescues, running a shop in the fortress selling upgrades and serving as a Sketch summon in which he uses his cane pogostick from the Ducktales NES games.

In 1961 a 45rpm single record was released entitled "Donald Duck and Uncle Scrooge's Money Rocket" (aka "Uncle Scrooge's Rocket to the Moon"), a story of how Scrooge builds a rocket to send all his money to the moon to protect it from the "Beagle Boys".
Scrooge also makes an appearance in Disney's and Square Enix's "Kingdom Hearts" series, in a role where he helps Mickey Mouse set up a world transit system. He first appears in "Kingdom Hearts II" as a minor non-playable character in Hollow Bastion, where he is trying to recreate his favorite ice cream flavor – sea-salt. Scrooge later appears in the prequel, "", this time with a speaking role. He works on establishing an ice-cream business in Radiant Garden and gives Ventus three passes to the Dream Festival in Disney Town. Young reprises the role in the English version of "Birth by Sleep".

Scrooge has appeared in the Boom! Studios "Darkwing Duck" comic, playing a key role at the end of its initial story, "The Duck Knight Returns". Later he would also play a key role on the final story arc "Dangerous Currency", where he teams up with Darkwing Duck in order to stop the Phantom Blot and Magica De Spell from taking over St. Canard and Duckburg.

In 2015, Scrooge was seen in the "Mickey Mouse" short "Goofy's First Love", where Mickey and Donald are trying to help Goofy find his love. Donald suggests money, and they head over to Scrooge's mansion where Donald tells his uncle that Goofy needs a million dollars. Scrooge then has his butler kick them out. When Goofy is inadvertently launched from the treadmill and catapulted off another building, he lands in Scrooge's mansion. The butler kicks Goofy out and the process repeats itself but this time Mickey and Donald are catapulted as well and kicked out by the butler. Scrooge is seen at the end attending Goofy's wedding with a sandwich. In the 2016 "Mickey Mouse" Christmas special, "Duck the Halls", after Young's death, John Kassir took over voicing Scrooge McDuck, however he later tweeted that he won't be reprising his role in the reboot. Kassir continues to voice the character in subsequent appearances in this series. Scrooge makes a cameo appearance in the "Legend of the Three Caballeros" episode "Shangri-La-Di-Da", voiced by Eric Bauza.

In the new "DuckTales," Scrooge is played by Scottish actor David Tennant, who brings both the nephews and Donald into his home at the end of the series premier. This series indicates that Scrooge previously adventured with his nephew Donald and his niece Della Duck, but an event ten years prior to the start of the series resulted in Scrooge and Donald severing ties. He seems to have a rather negative attitude about family as a result, and is initially uninterested in spending time with the boys until they assist him in a couple of adventures.





</doc>
<doc id="29379" url="https://en.wikipedia.org/wiki?curid=29379" title="Shiva (Judaism)">
Shiva (Judaism)

Shiva (, literally "seven") is the week-long mourning period in Judaism for first-degree relatives. The ritual is referred to as "sitting shiva". Traditionally, there are five stages of mourning in Judaism. Shiva is considered the third stage of mourning, and constitutes of seven days. Following the prior two stages, shiva embraces a time when individuals discuss their loss and accept the comfort of others. It is required to observe shiva for parents, spouses, children, and/or siblings who have died. It is not required to observe shiva for individuals who are less than thirty days old at the time of death. At the funeral, mourners wear an outer garment or ribbon that is torn during the procession in a ritual known as "keriah". This garment is worn throughout the entirety of shiva. Typically, the seven days begin immediately after the deceased have been buried. Following burial, mourners assume the "halakhic" status of "avel" (). It is necessary for the burial spot to be entirely covered with earth in order for shiva to commence. This state lasts for the entire duration of shiva. During the period of shiva, individuals remain at home. Friends and family visit those in mourning in order to give their condolences and provide comfort. The process, though dating back to biblical times, mimics the natural way an individual confronts and overcomes grief. Shiva allows for the individual to express their sorrow, discuss the loss of a loved one, and slowly re-enter society.

The word "shiva" comes from the Hebrew word "shiv'ah" (). Historical and biblical accounts depict multi-day periods of mourning, time set aside strictly for observing and expressing grief. There are many instances mentioned which describe the traditional Judaic process of mourning known as "shiva".

Amos stated that he would "turn your feasts into mourning, and all your songs into lamentations". In this instance, Amos described a period of time set aside specifically for grieving and accepting the loss of a loved one. During this time, he states that throughout the process of mourning one should renounce from feasting and songs, thus signifying a rejection of celebrations.

During the loss of his two sons, Aaron, a High Priest, describes a similar process of mourning to that of which we see today. Aaron explained to Moses that the time allotted for mourning is not meant to feast, however it is meant to express grief for the loss of loved ones.

Within the story of Joseph and his father, Jacob, the Genesis 50:1-14 describes the time in which Joseph grieves over the loss of his father. The seven day period of mourning that Joseph endured was depicted by the sages before the revelation at Mount Sinai.

In the Book of Job, it was stated that Job mourned his misfortune for seven days. During this time, he sat on the ground with his friends surrounding him. This account bears similarities to the maintained tradition of "sitting shiva" for precisely seven days.

The process of mourning begins with the first stage, otherwise known as Aninut. During this time, individuals experience the initial shock of their loss. Often times emotions associated with the period of Aninut include anger, denial, and disbelief. This is the most extreme period of mourning, and it is at this time in which the keriah, or the rending of the garments, is performed. The stage commences from the moment the individual dies until the end of the funeral. Following Aninut is shiva, in which the mourners delve into seven days dedicated towards remembrance of the deceased individual. Throughout shiva, individuals are instructed to take a break from their routines in order to focus on their loss as well. Following shiva is the stage of mourning known as sheloshim. During this period, mourning proceeds for thirty days following the burial. The first seven days of sheloshim is the period of shiva, however sheloshim continues on after shiva has ended. After the intense period of shiva, which is mainly contained to the home, sheloshim allows individuals to leave their residences and begin to interact with others again. Sheloshim encourages individuals to begin to partake in social relations in order to slowly ease back into normal daily activities. Through the final stage, yahrzeit or yizkor, the twelve month period of mourning ceases and yearly remembrance ceremonies are held for the individual who had died.

The period of shiva commences immediately after an individual has been buried, and ends after seven days in the morning following public services, or shacharit. However, if there are no public services held on the morning of the seventh day, services are done in the home of the mourner. In order to compute the time of shiva, Jewish traditions state that an entire day can be counted as a fraction as well. Therefore, the first day of shiva occurs following burial and the last day of shiva occurs directly after services conclude. Both the first and last days, though partial, are considered to be full days. For sheloshim, the thirty days are counted following the date of burial and continue on until the end of morning services. As with shiva, partial days during the period of sheloshim are considered entire days as well. 

Religious holidays during shiva and sheloshim require the traditional rules to be bent slightly. Because Judaism embraces the holidays with joy, the sadness and grief associated with mourning are meant to be set aside until the holiday concludes. Typically, if an individual dies before the beginning of a holiday, the holiday removes the observance of shiva or sheloshim. The days of the holiday are counted towards the days of mourning, and the rules enforced during mourning are revoked in order to encourage the celebration of a holiday. If a death occurred during the holiday or unknowingly, mourning commences after the holiday ends. In other situations, if the entirety of shiva has been observed prior to the start of a holiday, the holiday will cancel the observance of sheloshim, signifying the fulfillment of this period of mourning.

During Shabbat, private mourning continues, while public mourning is suspended. Individuals are permitted to wear shoes and leave their home to partake in public prayer services. In order to prepare for Shabbat, individuals are allowed to interrupt shiva for up to one hour and fifteen minutes in order to cook, dress, and perform other tasks. If this is not enough time to do so, in certain situations there may be two and a half hours allotted for such.

During Passover, any days in observance of shiva before the start will equate to seven when the holiday begins. Since Passover is celebrated for eight days, any mourning prior will total to fifteen days when holiday ends, leaving only fifteen days of observance of sheloshim.

During Shavuot, any days in observance of shiva before the start will equate to seven when the holiday begins. The first day of Shavuot equates to seven days. The second day of Shavuot is considered the fifteenth day, leaving only fifteen days left of observance of sheloshim.

During Succot, any days in observance of shiva before the start will equate to seven when the holiday begins. Since Succot is observed for seven days, any mourning prior will total to fourteen days when the holiday ends. Shemini Atzeret is considered the eighth day of Succot, and equates to seven days of mourning. Simchat Torah is considered the twenty-second day of mourning, leaving only eight days of observance of sheloshim.

During Rosh Hashanah, any days in observance of shiva before the start will equate to seven days when the holiday begins. Yom Kippur following Rosh Hashanah, will symbolize the end of mourning, and the end of both shiva and sheloshim.

During Yom Kippur, any days in observance of shiva before the start will equate to seven days when the holiday begins. Succot, following Yom Kippur, will symbolize the end of mourning, and the end of both shiva and sheloshim.

If the death occurs during Yom Tov, shiva does not begin until the burial is completed. Burial may not take place on Yom Tov, but can during the intermediate days of Succot or Passover, otherwise known as Chol HaMoed.

If a burial occurs on Chol HaMoed of Passover, shiva does not begin until after the Yom Tov is completed. In the Diaspora, where most Yom Tovim are observed for two days, mourning does not take place on the second day, but the day is still counted as one of the days of shiva.

There are many traditions that are upheld in order to observe shiva. Throughout this time, mourners are required to stay at home and refrain from engaging with the social world.

After hearing of the death of a close relative, Jewish beliefs and traditions instruct individuals to tear their clothing as the primary expression of grief. The process of tearing the garment is known as "keriah". Upon tearing the clothing, the mourner recites a blessing which describes "the true Judge" as God. This blessing reminds mourners to acknowledge that God has taken the life of a close relative, and is seen as the first step in the acceptance of grief. The garment is torn over the heart if the individual who died was a parent, or over the chest on the right side if the individual who died was another relative. The torn article of clothing is worn throughout the period of shiva, the only exception being on Shabbat.

After being near or around the deceased, it is ancient custom to wash yourself, or at minimum wash hands, as a means of purification. After a funeral, or visitation to a cemetery, individuals are required to wash hands as a mark of spiritual transition through water. During shiva, it is especially mandatory to do so before entering the home. There are many different origins of this tradition, however typically the act is associated with symbolic cleansing, the idea being that death is impure in a spiritual sense. Within Judaism, the living is thought to emphasize value of life rather than focus on death. When washing hands after visiting the deceased, it is custom to not pass the cup of water used from person to person. The reason behind this stems from the beliefs and hopes of stopping the tragedy where it began, rather than allowing it to continue from person to person as symbolized by the passing of the cup.

The first meal which should be eaten after the funeral is known as the "seudat havra'ah" (). Traditionally, mourners should be served the meal of condolences by neighbors. The act of preparing such meal is considered to be a mitzvah. Though being the tradition, if the meal of condolences is unable to be prepared by a neighbor, extended family may do so, and in the last case the mourner themselves may prepare the meal. It was seen that many times following the death of a loved one, individuals who were in mourning possessed a death wish and often times attempted to undergo starvation. The meal given to them upon returning home provided warmth in order to lessen such wishes. In order to be deemed the meal of condolences, the food selections must contain several specific dishes. An example of this is bread, which is symbolic for the staff of life. Aside from this, the meal must contain hardboiled eggs, cooked vegetables, and coffee or tea. Often times wine is allowed to be served as well. The only time the meal of condolences is not served occurs when there is no public observance of mourning or if the individual who died did so due to suicide.

Within Judaism, candles are symbolic of special events throughout life. They are lit during major holidays, during shabbat, and during the process of mourning candles are required to burn for the entirety of shiva. Prior to the death of Rabbi Judah Hanasi in the thirteenth century, he instructed that a light should be kept burning. During shiva, the candle represents the deceased. The light is symbolic of the human being, the wick and flame are representative of the body and soul respectively, as well as their connection with one another. Traditionally, candles are required to be made of either oil or paraffin and are not allowed to be electric. The candle is ideally burned in the home of the deceased, however exceptions can be made. Regardless, however, candles should be in the presence of those observing shiva. During major holidays, the candle may be moved in order to lessen the feeling of mourning and focus on the joyous occasion at hand.

Individuals who are in mourning, or in a shiva home, are required to cover mirrors from the time an individual dies until the end of shiva. There are several reasons as to why Judaism requires this. The first reason may stem from the idea that man was created in the image of God. In doing so, man acquires the same dignity and value as God. When a creation of God dies, this lessens His image. The death of human beings disrupts the connection between the living man and living God. Since the purpose of mirrors is to reflect such image, they are covered during mourning. A second reason as to why mirrors are covered in Judaism branches from contemplation of ones relationship with God during the death of a loved one. At this time, individuals are instructed to focus on grief and mourning rather than themselves. In order to prevent selfish thoughts, all mirrors are covered within the homes of mourners. A third reason which depicts why mirrors should be covered comes from the law which states that an individual may not stand directly in front of an image or worship one. Therefore, mirrors and pictures are hidden during mourning.

Leather shoes are not permitted to be worn during the observance of shiva. The reasoning behind this involves a lack of luxury. Without leather shoes, an individual is able to concentrate on mourning and the deeper meaning of life. However, exceptions to this rule include pregnant women. and those with ailments of the feet. Aside from those observing shiva or sheloshim, guests and individuals who are not should refrain from wearing leather shoes in the home of mourners as well.

Similar to the idea of wearing leather shoes, hygiene and personal grooming fall under the idea of the task being done for pleasure. Such acts are prohibited during the observation of shiva or sheloshim as they are seen as actions done for physical comfort. However, there is a fine line which separates grooming for hygienic reasons and for comfort. Therefore, in order to prevent grooming for comfort individuals who are mourning are instructed to only bathe separate parts of the body, head, and face. On top of this, cold or cool water is recommended. The use of cosmetics are not allowed as this constitutes as an act done for comfort and pleasure. However, the exception to this rule being a woman who is a bride, engaged to be married, dating to be married, or feels as though the use of makeup is necessary.

"Sitting" shiva refers to the act of sitting on low stools during times of mourning. As mentioned in the Book of Job, upon mourning, Job's friends "sat down with him upon the ground seven days and seven nights". Therefore originally, individuals who were observing a period of mourning were required to turn couches or beds over and sit on the ground. After time, modifications towards this rule were made. The "Halakhah" states that an individual is required to sit on low stools, or on the floor. The individual partakes in sitting on a low stool in order to signify their lack of concern for personal comfort during their time of mourning.

The best place for the observance of shiva to take place is within the home of the deceased individual. However, if observance in the home of the deceased is not permitted or is unable to be done, the second best place is in the home of a relative close to the deceased. During the observance of shiva, individuals are not permitted to leave the premises, however, there are certain exceptions to this rule. Exceptions include, not having enough room to house every individual observing, the loss of another loved one, and the inability to conduct services in the home. If an individual mourning is allowed to leave the home, they must do so without disturbing others and never alone.

Praying in the home of a mourner is done in order to demonstrate respect for the grieving individual as well as the deceased. Even as early as 1790, the Hebra Maarib beZemanah Oheb Shalom was founded in order to provide mourners observing shiva with a minyan. During 1853 in London, the Hebrath Menachem Abelim Hesed Ve Emeth was constructed to accomplish a similar goal. Throughout history, prayers during mourning have been important. However during shiva, the prayers change slightly.

During the process of mourning, Kaddish is typically recited. Rather than losing faith in the religion, Jewish traditions require those who have experienced the loss of a loved one to publicly assert their faith in God. This is typically done in front of a minyan. The recitation of Kaddish is done in order to protect the dignity and merit of the individual who died within God's eyes. Judaism believes that prior to a soul's entry into heaven, a maximum of twelve months is required in order for even the worst soul to be purified. Though the entirety of mourning lasts for twelve months, Kaddish is only recited for eleven months so as to not imply the soul required an entire twelve months of purification.

Traditionally the true mourner's prayer is known as "El Molai Rachamim" in Ashkenazi literature and "Hashkavah" in Sephardic literature. Often times the mourner's prayer is mistaken for Kaddish. The recitation of the mourner's prayer is done for the soul of an individual who has died. The prayer itself is an appeal for the soul of the deceased to be given proper rest. Typically recitation of this prayer is done at the graveside during burial, during the unveiling of the tombstone, as well as at remembrances services during Yom Kippur, Shmini Atzeret, the final day of Passover, and the final day of Shavuot. If the recitation is done as an individual commemoration, the prayer contains the name of the individual who died. However, if the recitation is done in the presence of a group, the prayer will contain a description of the individual who died.

The recitation of the mourner's prayer is done differently depending on the gender of the one for whom is said.

If the mourner's prayer is recited on behalf of a woman, the following text is recited:

If the mourner's prayer is recited on behalf of a man, the following text is recited:

A minyan is traditionally a quorum of ten or more adult males. Often times in Conservative or Reform communities, a minyan is composed of a mix of ten or more adult males and females. During shiva, a minyan will gather at the home of those in mourning for services. The services are similar to those held at a synagogue. During shiva, however, certain prayers or verses are either added or omitted. During the days that the Torah is read in a synagogue, it is likewise read at the shiva home. An effort is made by the community to lend a Torah scroll to the mourner for this purpose.

Addition of "Psalm XLIX" - Redemption of the Soul 

Omission of "Pitum Haketoret" 

Omission of "Tachanun" and/or "Nephilat Appayim" 

Substitution of "Psalm XVI" for "Psalm XLIX" during the omission of "Tachanun" 

Omission of "Psalm XX" 

Omission of "The Priestly Benediction (Number VI: 24-26)" 

Omission of the six Psalms before Friday night services

Omission of "Psalm XC: 17" verse: "And let the graciousness of the Lord our God be upon us: establish thou also upon us the work of our hands, yea, the work of our hands establish thou it" 

Spices are omitted from use in the home of a mourner during Havdalah (the end of shabbat) 




</doc>
<doc id="29381" url="https://en.wikipedia.org/wiki?curid=29381" title="Semi-trailer truck">
Semi-trailer truck

A semi-trailer truck (more commonly semi truck or simply "semi") is the combination of a tractor unit and one or more semi-trailers to carry freight. A semi-trailer attaches to the tractor with a fifth wheel coupling (hitch), with much of its weight borne by the tractor. The result is that both the tractor and semi-trailer will have a distinctly different design than a rigid truck and trailer.

It is variously known as a "transport" in Canada; "semi" or "single" in Australia and New Zealand; "semi", "tractor-trailer", "big rig", or "eighteen-wheeler" in the United States; and "articulated lorry", abbreviated "artic", in the United Kingdom, Ireland and New Zealand.

In North America, the combination vehicles made up of a powered truck and one or more semitrailers are known as "semis", "semitrailers", "tractor-trailers", "big rigs", "semi trucks", "eighteen-wheelers", or "semi-tractor trailers".

The tractor unit typically has two or three axles; those built for hauling heavy-duty commercial-construction machinery may have as many as five, some often being lift axles.

The most common tractor-cab layout has a forward engine, one steering axle, and two drive axles. The fifth-wheel trailer coupling on most tractor trucks is movable fore and aft, to allow adjustment in the weight distribution over its rear axle(s).

Ubiquitous in Europe, but less common in North America since the 1990s, is the cabover engine configuration, where the driver sits next to, or over the engine. With changes in the US to the maximum length of the combined vehicle, the cabover was largely phased out of North American over-the-road (long-haul) service by 2007. Cabovers were difficult to service; for a long time the cab could not be lifted on its hinges to a full 90-degree forward tilt, severely limiting access to the front part of the engine.

, a truck can cost $100,000, while the diesel fuel cost can be $70,000 per year. Trucks average from , with fuel economy standards requiring better than efficiency by 2014. Power requirements in standard conditions are 170 hp at or 280 hp at , and somewhat different power usage in other conditions.

The cargo trailer usually has tandem axles at the rear, each of which has dual wheels, or eight tires on the trailer, four per axle. In the US it is common to refer to the number of wheel hubs, rather than the number of tires; an axle can have either single or dual tires with no legal difference. The combination of eight tires on the trailer and ten tires on the tractor is what led to the moniker "eighteen wheeler", although this term is considered by some truckers to be a misnomer (the term "eighteen-wheeler" is a nickname for a five-axle over-the-road combination). Many trailers are equipped with movable tandem axles to allow adjusting the weight distribution.

To connect the second of a set of doubles to the first trailer, and to support the front half of the second trailer, a converter gear known as a "dolly" is used. This has one or two axles, a fifth-wheel coupling for the rear trailer, and a tongue with a ring-hitch coupling for the forward trailer. Individual states may further allow longer vehicles, known as "longer combination vehicles" (or LCVs), and may allow them to operate on roads other than Interstates.

Long combination vehicle types include:

Future long combination vehicles under consideration and study for the U.S. MAP-21 transportation bill are container doubles. These combinations are under study for potential recommendation in November 2014:

The US federal government, which only regulates the Interstate Highway System, does not set maximum length requirements (except on auto and boat transporters), only minimums. Tractors can pull two or three trailers if the combination is legal in that state. Weight maximums are on a single axle, on a tandem, and total for any vehicle or combination. There is a maximum width of and no maximum height.

Roads other than the Interstates are regulated by the individual states, and laws vary widely. Maximum weight varies between to , depending on the combination. Most states restrict operation of larger tandem trailer setups such as triple units, turnpike doubles and Rocky-Mountain doubles. Reasons for limiting the legal trailer configurations include both safety concerns and the impracticality of designing and constructing roads that can accommodate the larger wheelbase of these vehicles and the larger minimum turning radii associated with them. In general, these configurations are restricted to the Interstates. Except for these units, double setups are not restricted to certain roads any more than a single setup. They are also not restricted by weather conditions or "difficulty of operation". The Canadian province of Ontario, however, does have weather-related operating restrictions for larger tandem trailer setups.

The noticeable difference between tractor units in Europe and North America is that almost all European models are cab over engine (called "forward control" in England), while the majority of North American trucks are "conventional" (called "normal control" or "bonneted" in England). European trucks, whether straight trucks or fully articulated, have a sheer face on the front. This allows shorter trucks with longer trailers (with larger freight capacity) within the legal maximum total length. Furthermore, it offers greater maneuverability in confined areas, a more balanced weight-distribution and better overall view for the driver. The major disadvantage is that for repairs on COE trucks, the entire cab has to hinge forward to allow maintenance access. Conversely, "conventional" cab tractors offer the driver a more comfortable driving environment, easier access getting in or out and better protection in a collision.

In Europe usually both rear tractor axles have dual wheels, while single or dual wheels are used for the cargo trailer. The most common combination used in Europe is a semi tractor with three axles, one a lift axle, and a cargo trailer with three axles, one also a lift axle, giving six axles and 22 wheels in total. This format is now common across Europe, due to reduced road-loading requirements, especially for bridges. The lift axles used on both tractors and trailers allow the trucks to remain legal, but increases maneuverability while at the same time reducing fuel consumption and tyre wear when carrying lighter loads, by raising one or multiple axle set(s) off the roadway. Although lift axles usually operate automatically, they can be lowered manually even while carrying light loads, in order to remain within legal (safe) limits when, for example, navigating back-road bridges with severely restricted axle loads. For greater detail, see the United Kingdom section, below.

When using a dolly, which generally has to be equipped with lights and a license plate, rigid trucks can be used to pull semi-trailers. The dolly is equipped with a fifth wheel to which the trailer is coupled. Because the dolly attaches to a pintle hitch on the truck, maneuvering a trailer hooked to a dolly is different from maneuvering a fifth wheel trailer. Backing the vehicle requires same technique as backing an ordinary truck/full trailer combination, though the dolly/semi setup is probably longer, thus requiring more space for maneuvering. The tractor/semi-trailer configuration is rarely used on timber trucks, since these will use the two big advantages of having the weight of the load on the drive wheels, and the loader crane used to lift the logs from the ground can be mounted on the rear of the truck behind the load, allowing a short (lightweight) crane to reach both ends of the vehicle without uncoupling. Also construction trucks are more often seen in a rigid + midaxle trailer configuration instead of the tractor/semi-trailer setup.

In the United Kingdom, the maximum permitted gross weight of a semi-trailer truck, without the use of a Special Type General Order (STGO), is . In order for a semi-trailer truck to be permitted on UK roads the tractor and semi-trailer must have three or more axles each. Lower weight semi-trailer trucks can mean some tractors and trailer having fewer axles. In practice, like double decker buses and coaches in the UK, there is no legal height limit for semi-trailer trucks; however, bridges over do not have the height marked on them. Semi-trailer trucks on Continental Europe have a height limit of .
Vehicles heavier than are permitted on UK roads but are indivisible loads, which would be classed as abnormal (or oversize). Such vehicles are required to display an STGO (Special Types General Order) plate on the front of the tractor unit and, under certain circumstances, are required to travel by an authorized route and have an escort.

Most UK trailers are long and, dependent on the position of the fifth wheel and kingpin, a coupled tractor unit and trailer will have a combined length of between . Although the Construction and Use Regulations allow a maximum rigid length of , this, combined with a shallow kingpin and fifth wheel set close to the rear of the tractor unit, can give an overall length of around .

Starting in January 2012 the Department for Transport is conducting a trial of longer semi-trailers. The trial involves 900 semi-trailers of in length (i.e. longer than the current maximum), and a further 900 semi-trailers of in length (i.e. longer). This will result in the total maximum length of the semi-trailer truck being for trailers in length, and for trailers long. The increase in length will not result in the weight limit being exceeded and will allow some operators to approach the weight limit which may not have been previously possible due to the previous length of trailers. The trial will run for a maximum of 10 years. Providing certain requirements are fulfilled, a Special Types General Order (STGO) allows for vehicles of any size or weight to travel on UK roads. However, in practice any such vehicle has to travel by a route authorized by the Department of Transport and move under escort. The escort of abnormal loads in the UK is now predominantly carried out by private companies, but extremely large or heavy loads that require road closures must still be escorted by the police.

In the UK, some semi-trailer trucks have eight tires on three axles on the tractor; these are known as six-wheelers or "six leggers", with either the center or rear axle having single wheels which normally steer as well as the front axle and can be raised when not needed (i.e. when unloaded or only a light load is being carried; an arrangement known as a TAG axle when it is the rear axle, or mid-lift when it is the center axle). Some trailers have two axles which have twin tires on each axle; other trailers have three axles, of which one axle can be a lift axle which has super-single wheels. In the UK, two wheels bolted to the same hub are classed as a single wheel, therefore a standard six-axle articulated truck is considered to have twelve wheels, even though it has twenty tires. The UK also allows semi-trailer truck which have six tires on two axles; these are known as four-wheelers.
In 2009, the operator Denby Transport designed and built a B-Train (or B-Double) semi-trailer truck called the Denby Eco-Link to show the benefits of such a vehicle, which were a reduction in road accidents and result in less road deaths, a reduction in emissions due to the one tractor unit still being used and no further highway investment being required. Furthermore, Denby Transport asserted that two Eco-Links would replace three standard semi-trailer trucks while, if limited to the current UK weight limit of , it was claimed the Eco-Link would reduce carbon emissions by 16% and could still halve the number of trips needed for the same amount of cargo carried in conventional semi-trailer trucks. This is based on the fact that for light but bulky goods such as toilet paper, plastic bottles, cereals and aluminum cans, conventional semi-trailer trucks run out of cargo space before they reach the weight limit. At , as opposed to usually associated with B-Trains, the Eco-Link also exerts less weight per axle on the road compared to the standard six-axle semi-trailer truck.

The vehicle was built after Denby Transport believed they had found a legal-loophole in the present UK law to allow the Eco-Link to be used on the public roads. The relevant legislation concerned the 1986 Road Vehicles Construction and Use Regulations. The 1986 regulations state that "certain vehicles" may be permitted to draw more than one trailer and can be up to . The point of law reportedly hinged on the definition of a "towing implement", with Denby prepared to argue that the second trailer on the Eco-Link was one. The Department for Transport were of the opinion that this refers to recovering a vehicle after an accident or breakdown, but the regulation does not explicitly state this.

During BTAC performance testing the Eco-Link was given an "excellent" rating for its performance in maneuverability, productivity, safety and emissions tests, superseding ordinary semi-trailer trucks in many respects. Private trials had also reportedly shown the Denby vehicle had a 20% shorter stopping distance than conventional semi-trailer trucks of the same weight, due to having extra axles. The active steer system meant that the Eco-Link had a turning circle of , the same as a conventional semi-trailer truck.

Although the Department for Transport advised that the Eco-Link was not permissible on public roads, Denby Transport gave the Police prior warning of the timing and route of the test drive on the public highway, as well as outlining their position in writing to the Eastern Traffic Area Office. On 1 December 2009 Denby Transport were preparing to drive the Eco-Link on public roads, but this was cut short because the Police pulled the semi-trailer truck over as it left the gates in order to test it for its legality "to investigate any... offenses which may be found". The Police said the vehicle was unlawful due to its length and Denby Transport was served with a notice by the Vehicle and Operator Services Agency (VOSA) inspector to remove the vehicle from the road for inspection. Having returned to the yard, Denby Transport was formally notified by Police and VOSA that the semi-trailer truck could not be used. Neither the Eco-Link, nor any other B-Train, have since been permitted on UK roads. However, this prompted the Department for Transport to undertake a desk study into semi-trailer trucks, which has resulted in the longer semi-trailer trial which commenced in 2012.

The maximum overall length in the EU and EEA member states was with a maximum weight of 40 tonnes, or 44 tonnes if carrying an ISO container. However, rules limiting the semi-trailers to and 18.75 m are met with trucks carrying a standardized body with one additional 7.82 m body on tow as a trailer. 25.25 metre truck combinations were developed under the branding of "EcoCombi" which influenced the name of "EuroCombi" for an ongoing standardization effort where such truck combinations shall be legal to operate in all jurisdictions of the European Economic Area. With the 50% increase in cargo weight, the fuel efficiency increases with an average of 20% with a corresponding relative decrease in carbon emissions and with the added benefit of one third fewer trucks on the road. The 1996 EU regulation defines a Europe Module System (EMS) as it was implemented in Sweden. The wording of EMS combinations and EuroCombi are now used interchangeably to point to truck combinations as specified in the EU document; however, apart from Sweden and Finland, the EuroCombi is only allowed to operate on specific tracks in other EU member states. Since 1996, when Sweden and Finland formally won a final exemption from the European Economic Area rules with 60 tonne and combinations, all other From 2006, 25.25 m truck trailer combinations are to be allowed on restricted routes within Germany, following a similar (on-going) trial in The Netherlands. Similarly, Denmark has allowed 25.25 m combinations on select routes. These vehicles will run a 60 tonne weight limit. Two types are to be used: 1) a 26-tonne truck pulling a dolly and semi-trailer, or 2) an articulated tractor unit pulling a B-double, member states gained the ability to adopt the same rules. In Italy the maximum permitted weight (unless exceptional transport is authorized) is 44 tonnes for any kind of combination with five axles or more.

The tractor/semi-trailer configuration is rarely used on timber trucks, since these will use the two big advantages of having the weight of the load on the drive wheels, and the loader crane used to lift the logs from the ground can be mounted on the rear of the truck behind the load, allowing a short (lightweight) crane to reach both ends of the vehicle without uncoupling. Also construction trucks are more often seen in a rigid + midaxle trailer configuration instead of the tractor/semi-trailer setup.

Denmark and Norway allow trucks (Denmark from 2008, and Norway from 2008 on selected routes). In Sweden the allowed length has been since 1967. Before that, the maximum length was unlimited; the only limitations were on axle load. What stopped Sweden from adopting the same rules as the rest of Europe, when securing road safety, was the national importance of a competitive forestry industry. Finland, with the same road safety issues and equally important forestry industry, followed suit. The change made trucks able to carry three stacks of cut-to-length logs instead of two, as it would be in a short combination. They have one on stack together with a crane on the 6×4 truck, and two additional stacks on a four axle trailer. The allowed gross weight in both countries is up to 60 tonnes depending on the distance between the first and last axle.

In the negotiations starting in the late 1980s preceding Sweden and Finland's entries to the European Economic Area and later the European Union, they insisted on exemptions from the EU rules citing environmental concerns and the transportation needs of the logging industry. In 1995, after their entry to the union, the rules changed again, this time to allow trucks carrying a standard CEN unit of to draw a standard semi-trailer on a dolly, a total overall length of 25.25 m. Later, B-double combinations came into use, often with one container on the B-link and a container (or two containers) on a semi-trailer bed. In allowing the longer truck combinations, what would take two semi-trailer trucks and one truck and trailer to haul on the continent now could be handled by just two 25.25 m trucks – greatly reducing overall costs and emissions. Prepared since late 2012 and effective on January 2013, Finland has changed its regulations to allow total maximum legal weight of a combination to be 76 tonnes. At the same time the maximum allowed height would be increased by ; from current maximum of to . The effect this major maximum weight increase would cause to the roads and bridges in Finland over time is strongly debated.

However, longer and heavier combinations are regularly seen on public roads; special permits are issued for special cargo. The mining company Boliden AB have a standing special permit for 80 tonne combinations on select routes between mines in the inland and the processing plant in Boliden, taking a 50 tonne load of ore. Volvo has a special permit for a , steering B-trailer-trailer combination carrying two containers to and from Gothenburg harbour and the Volvo Trucks factory, all on the island of Hisingen. Another example is the ongoing project "En Trave Till" (lit. "One more pile/stack") started in December 2008. It will allow even longer vehicles to further rationalize the logging transports. As the name of the project points out, it will be able to carry four stacks of timber, instead of the usual three. The test is limited to Norrbotten county and the European route E4 between the timber terminal in Överkalix and the sawmill in Munksund (outside Piteå). The vehicle is a long truck trailer combination with a gross weight exceeding 90 tonnes. It is estimated that this will give a 20% lower cost and 20-25% CO2 emissions reduction compared to the regular 60 tonne truck combinations. As the combinations spreads its weight over more axles, braking distance, road wear and traffic safety is believed to be either the same or improved with the 90 tonne truck-trailer. In the same program two types of 74 tonne combinations will be tested in Dalsland and Bohuslän counties in western Sweden: an enhanced truck and trailer combination for use in the forest and a b-double for plain highway transportation to the mill in Skoghall. In 2012, the Northland Mining company received permission for 90 tonne combinations with normal axle load (an extra dolly) for use on the Kaunisvaara-Svappavaara route, carrying iron ore.

Australian road transport has a reputation for using very large trucks and road trains. This is reflected in the most popular configurations of trucks generally having dual drive axles and three axles on the trailers, with four tyres on each axle. This means that Australian single semi-trailer trucks will usually have 22 tyres, which is generally more than their counterparts in other countries. Super single tyres are sometimes used on tri-axle trailers. The suspension is designed with travel limiting, which will hold the rim off the road for one blown or deflated tyre for each side of the trailer, so a trailer can be driven at reduced speed to a safe place for repair. Super singles are also often used on the steer axle in Australia to allow greater loading over the steer axle. The increase in loading of steer tyres requires a permit.

Long haul transport usually operates as B-doubles with two trailers (each with three axles), for a total of nine axles (including steering). In some lighter duty applications only one of the rear axles of the truck is driven, and the trailer may have only two axles. From July 2007, the Australian Federal and State Governments allowed the introduction of B-triple trucks on a specified network of roads. B-Triples are set up differently from conventional road trains. The front of their first trailer is supported by the turntable on the prime mover. The second and third trailers are supported by turntables on the trailers in front of them. As a result, B-Triples are much more stable than road trains and handle exceptionally well. True road trains only operate in remote areas, regulated by each state or territory government.

In total, the maximum length that any articulated vehicle may be (without a special permit and escort) is , its maximum load may be up to 164 tonnes gross, and may have up to four trailers. However, heavy restrictions apply to the areas where such a vehicle may travel in most states. In remote areas such as the Northern Territory great care must be taken when sharing the road with longer articulated vehicles that often travel during the daytime, especially four-trailer road trains.

Articulated trucks towing a single trailer or two trailers (commonly known as "short doubles") with a maximum overall length of are referred to as "General access heavy vehicles" and are permitted in all areas, including metropolitan. B-doubles are limited to a maximum total weight of 62.5 tonnes and overall length of , or if they are fitted with approved FUPS (Front Underrun Protection System) devices. B-doubles may only operate on designated roads, which includes most highways and some major metropolitan roads. B-doubles are very common in all parts of Australia including state capitals and on major routes they outnumber single trailer configurations.

Maximum width of any vehicle is and a height of . In the past few years, allowance has been made by several states to allow certain designs of heavy vehicles up to high but they are also restricted to designated routes. In effect, a 4.6 meter high B-double will have to follow two sets of rules: they may access only those roads that are permitted for B-doubles "and" for 4.6 meter high vehicles.

In Australia, both conventional prime movers and cabovers are common, however, cabovers are most often seen on B-doubles on the eastern seaboard where the reduction in total length allows the vehicle to pull longer trailers and thus more cargo than it would otherwise.

New Zealand's legislation governing truck dimensions falls under the Vehicle Dimensions and Mass Rules published by NZ Transport Agency. New rules were introduced effective 1 February 2017. which increased the maximum height, width and weight of loads and vehicles to simplify regulations, increase the amount of freight carried and to improve the range of vehicles and trailers available to transport operators. 

Common combinations in New Zealand consist of a standard semi-trailer, a B-double or a rigid towing vehicle pulling a trailer with a drawbar. Standard maximum vehicle lengths for trailers with one axle set are:
Trailers with two axle sets can be , including heavy rigid vehicles towing two trailers.
Oversized loads require, at minimum, a permit, and may require a pilot vehicle.

There are many types of semi-trailers in use, designed to haul a wide range of products.

The cargo trailer is, by means of a king pin, hooked to a horseshoe-shaped quick-release coupling device called a fifth wheel or a turntable hitch at the rear of the towing engine that allows easy hook up and release. The truck trailer cannot move by itself because it only has wheels at the rear end: it requires a forward axle, provided by the towing engine, to carry half the load weight. When braking hard at high speeds, the vehicle has a tendency to fold at the pivot point between the towing vehicle and the trailer. Such a truck accident is called a "trailer swing", although it is also commonly described as a "jackknife". Jackknifing is a condition where the tractive unit swings round against the trailer, and not vice versa.

Semi trucks use air pressure, rather than hydraulic fluid, to actuate the brake. The use of air hoses allows for ease of coupling and uncoupling of trailers from the tractor unit. The most common failure is "brake fade", usually caused when the drums or discs and the linings of the brakes overheat from excessive use.

The parking brake of the tractor unit and the emergency brake of the trailer are spring brakes that require air pressure in order to be released. They are applied when air pressure is released from the system, and disengaged when air pressure is supplied. This is a fail-safe design feature which ensures that if air pressure to either unit is lost, the vehicle will stop to a grinding halt, instead of continuing without brakes and becoming uncontrollable. The trailer controls are coupled to the tractor through two "gladhand connectors", which provide air pressure, and an electrical cable, which provides power to the lights and any specialized features of the trailer.

"Glad-hand connectors" (also known as "palm couplings") are air hose connectors, each of which has a flat engaging face and retaining tabs. The faces are placed together, and the units are rotated so that the tabs engage each other to hold the connectors together. This arrangement provides a secure connection but allows the couplers to break away without damaging the equipment if they are pulled, as may happen when the tractor and trailer are separated without first uncoupling the air lines. These connectors are similar in design to the ones used for a similar purpose between railroad cars. Two air lines typically connect to the trailer unit. An "emergency" or "main" air supply line pressurizes the trailer's air tank and disengages the emergency brake, and a second "service" line controls the brake application during normal operation.

In the UK, male/female quick release connectors ("red line" or emergency), have a female on the truck and male on the trailer, but a "yellow line" or service has a male on the truck and female on the trailer. This avoids coupling errors (causing no brakes) plus the connections will not come apart if pulled by accident. The three electrical lines will fit one way around a primary black, a secondary green, and an ABS lead, all of which are collectively known as "suzies" or "suzie coils".

Another braking feature of semi-trucks is engine braking, which could be either a compression brake (usually shortened to "Jake brake") or exhaust brake or combination of both. However, the use of compression brake alone produces a loud and distinctive noise, and to control noise pollution, some local municipalities have prohibited or restricted the use of engine brake systems inside their jurisdictions, particularly in residential areas. The advantage to using engine braking instead of conventional brakes is that a truck can descend a long grade without overheating its wheel brakes. Some vehicles can also be equipped with hydraulic or electric retarders which have an advantage of near silent operation.

Because of the wide variety of loads the semi may carry, they usually have a manual transmission to allow the driver to have as much control as possible. However, all truck manufacturers now offer semi-automatic transmissions (manual gearboxes with automated gear change), as well as automatic transmissions.

Semi-truck transmissions can have as few as three forward speeds or as many as 18 forward speeds (plus 2 reverse speeds). A large number of transmission ratios means the driver can operate the engine more efficiently. Modern on-highway diesel engines are designed to provide maximum torque in a narrow RPM range (usually 1200-1500 RPM); having more gear ratios means the driver can hold the engine in its optimum range regardless of road speed (drive axle ratio must also be considered).

A ten-speed manual transmission, for example is controlled via a six-slot H-box pattern, similar to that in five-speed cars — five forward and one reverse gear. Gears six to ten (and high speed reverse) are accessed by a Lo/High range splitter; gears one to five are Lo range; gears six to ten are High range using the same shift pattern. A Super-10 transmission, by contrast, has no range splitter; it uses alternating "stick and button" shifting (stick shifts 1-3-5-7-9, button shifts 2-4-6-8-10). The 13-, 15-, and 18-speed transmissions have the same basic shift pattern, but include a splitter button to enable additional ratios found in each range. Some transmissions may have 12 speeds.

Another difference between semi-trucks and cars is the way the clutch is set up. On an automobile, the clutch pedal is depressed full stroke to the floor for every gear shift, to ensure the gearbox is disengaged from the engine. On a semi-truck with constant mesh transmission (non synchronized), such as by the Eaton Roadranger series, not only is double clutching required, but a clutch brake is required as well. The clutch brake stops the rotation of the gears, and allows the truck to be put into gear without grinding when stationary. The clutch is pressed to the floor only to allow smooth engagement of low gears when starting from a full stop; when the truck is moving, the clutch pedal is pressed only far enough to break torque for gear changes.

An electrical connection is made between the tractor and the trailer through a cable often referred to as a "pigtail". This cable is a bundle of wires in a single casing. Each wire controls one of the electrical circuits on the trailer, such as running lights, brake lights, turn signals, etc. A straight cable would break when the rig went around corners, so a coiled cable is used which retracts these coils when not under tension. It is these coils that cause the cable to look like a pigtail.

In most countries, a trailer or semi-trailer must have minimum

Although dual wheels are the most common, use of two single, wider tires, known as "super singles", on each axle is becoming popular among bulk cargo carriers and other weight-sensitive operators. With increased efforts to reduce greenhouse gas emissions, the use of the super-single tire is gaining popularity. There are several advantages to this configuration. The first of these is that super singles reduce fuel consumption. In 1999, tests on an oval track showed a 10% fuel savings when super singles were used. These savings are realized because less energy is wasted flexing fewer tire sidewalls. Second, the lighter overall tire weight allows a truck to be loaded with more freight. The third advantage is that the single wheel encloses less of the brake unit, which allows faster cooling and reduces brake fade.

One of the major disadvantages of the super singles is that they are currently not as widely available as a standard tire. In addition, if a tire should become deflated or be destroyed, there is not another tire attached to the same hub to maintain the dynamic stability of the vehicle, as would be the case with dual wheels. With dual wheels, the remaining tire may be overloaded, but it will typically allow the vehicle to be safely stopped or driven to a repair facility.

In Europe, super singles became popular when the allowed weight of semitrailer rigs was increased from 38 to 40 tonnes. In this reform the trailer industry replaced two axles with dual wheels, with three axles on wide-base single wheels. The significantly lower axle weight on super singles must be considered when comparing road wear from single versus dual wheels. The majority of super singles sold in Europe have a width of . The standard 385 tires have a legal load limit of . (Note that expensive, specially reinforced 385 tires approved for do exist. Their market share is tiny, except for mounting on the steer axle.)

An innovation rapidly growing in popularity is the skirted trailer. The space between the road and the bottom of the trailer frame was traditionally left open until it was realized that the turbulent air swirling under the trailer is a major source of aerodynamic drag. Three split skirt concepts were verified by the United States Environmental Protection Agency (EPA) to provide fuel savings greater than 5%, and four split skirt concepts had EPA-verified fuel savings between 4% and 5%.

Skirted trailers are often combined with Underrun Protection Systems ("underride guards"), greatly improving safety for passenger vehicles sharing the road.

Technically called a Rear Underrun Protection System (RUPS), this is a rigid assembly hanging down from the bottom rear of the trailer, which is intended to provide some protection for passenger cars which collide with the rear of the trailer. Public awareness of this safeguard was increased in the aftermath of the accident that killed actress Jayne Mansfield on 29 June 1967, when the car she was in hit the rear of a tractor-trailer, causing fatal head trauma. After her death, the NHTSA recommended requiring a rear underride guard, also known as a "Mansfield bar", an "ICC bar", or a "DOT bumper". In Europe, side and rear underrun protection are mandated on all lorries and trailers with a gross weight of 3,500 kg or more.

The bottom rear of the trailer is near head level for an adult seated in a car, and without the underride guard, the only protection for such an adult's head in a rear-end collision would be the car's windshield. The front of the car goes under the platform of the trailer rather than making contact via the passenger car bumper, so the car's protective crush zone becomes irrelevant and air bags are ineffective in protecting the car passengers. The underride guard provides a rigid area for the car to contact at the height of its bumper.

In addition to rear underride guards, truck tractor cabs may be equipped with a Front Underrun Protection System (FUPS) at the front bumper of the truck. The safest tractor-trailers are also equipped with side underride guards, also called Side Underrun Protection System (SUPS). These additional barriers prevent passenger cars from skidding underneath the trailer from the side, such as in an oblique or side collision, or if the trailer jackknifes across the road. In addition to safety benefits, these underride guards may improve fuel mileage by reducing air turbulence under the trailer at highway speeds.

Another benefit of having a sturdy underride guard is that it may be secured to a loading dock with a hook to prevent "trailer creep", a movement of the trailer away from the dock, which opens up a dangerous gap during loading or unloading operations.

Current semi-truck manufacturers include:

A special driver's license is required to operate various commercial vehicles.

Regulations vary by province. A license to operate a vehicle with air brakes is required (i.e., normally a Class I, II, or III commercial license with an "A" or "S" endorsement in provinces other than Ontario). In Ontario, a "Z" endorsement is required to drive any vehicle using air brakes; in provinces other than Ontario, the "A" endorsement is for air brake operation only, and an "S" endorsement is for both operation and adjustment of air brakes. Anyone holding a valid Ontario driver's license (i.e., excluding a motorcycle license) with a "Z" endorsement can legally drive any air-brake-equipped truck-trailer combination with a registered- or actual-gross-vehicle-weight (i.e., including towing- and towed-vehicle) up to 11 tonnes, that includes one trailer weighing no more than 4.6 tonnes if the license falls under the following three classes: Class E (school bus—maximum 24-passenger capacity or ambulance), F (regular bus—maximum 24-passenger capacity or ambulance) or G (car, van, or small-truck).

A Class B (any school bus), C (any urban-transit-vehicle or highway-coach), or D (heavy trucks other than tractor-trailers) license enables its holder to drive any truck-trailer combination with a registered- or actual-gross-vehicle-weight (i.e., including towing- and towed-vehicle) greater than 11 tonnes, that includes one trailer weighing no more than 4.6 tonnes. Anyone holding an Ontario Class A license (or its equivalent) can drive any truck-trailer combination with a registered- or actual-gross-vehicle-weight (i.e., including towing- and towed-vehicles) greater than 11 tonnes, that includes one or more trailers weighing more than 4.6 tonnes.

Drivers of semi-trailer trucks generally require a Class A commercial driver's license (CDL) to operate any combination vehicles with a gross combination weight rating (or GCWR) in excess of if the gross vehicle weight rating (GVWR) of the towed vehicle(s) is in excess of . Some states (such as North Dakota) provide exemptions for farmers, allowing non-commercial license holders to operate semis within a certain air-mile radius of their reporting location. State exemptions, however, are only applicable in intrastate commerce; stipulations of the Code of Federal Regulations (CFR) may be applied in interstate commerce. Also a person under the age of 21 cannot operate a commercial vehicle outside the state where the commercial license was issued. This restriction may also be mirrored by certain states in their intrastate regulations. A person must be at least 18 in order to be issued a commercial license.

In addition, "endorsements" are necessary for certain cargo and vehicle arrangements and types;

The Road Traffic Security Rules () require a combination vehicle driver license () to drive a combination vehicle (). These rules define a combination vehicle as a motor vehicle towing a heavy trailer, i.e., a trailer with a gross weight of more than .

A category CE driving licence is required to drive a tractor-trailer in Europe. Category C (Γ in Greece) is required for vehicles over , while category E is for heavy trailers, which in the case of trucks and buses means any trailer over . Vehicles over —which is the maximum limit of B license—but under can be driven with a C1 license. Buses require a D (Δ in Greece) license. A bus that is registered for no more than 16 passengers, excluding the driver, can be driven with a D1 license.

Truck drivers in Australia require an endorsed license. These endorsements are gained through training and experience. The minimum age to hold an endorsed license is 18 years, and/or must have held open (full) driver's license for minimum 12 months.
The following are the heavy vehicle license classes in Australia:

In order to obtain an HC License the driver must have held an MR or HR license for at least 12 months. To upgrade to an MC License the driver must have held a HR or HC license for at least 12 months. From licenses MR and upward there is also a B Condition which may apply to the license if testing in a synchromesh or automatic transmission vehicle. The B Condition may be removed upon the driver proving the ability to drive a constant mesh transmission using the clutch. "Constant mesh transmission refers to "crash box" transmissions, predominantly Road Ranger eighteen-speed transmissions in Australia."

In New Zealand, drivers of heavy vehicles require specific licenses, termed as classes. A Class 1 license ("car license") will allow the driving of any vehicle with Gross Laden Weight (GLW) or Gross Combination Weight (GCW) of or less. For other types of vehicles the classes are separately licensed as follows:
Further information on the New Zealand licensing system for heavy vehicles can be found at the New Zealand Transport Agency.

Modern day semi-trailer trucks often operate as a part of a domestic or international transport infrastructure to support containerized cargo shipment.

Various types of rail flat bed train cars are modified to hold the cargo trailer or container with wheels or without. This is called "Intermodal" or "piggyback". The system allows the cargo to switch from highway to railway or vice versa with relative ease by using gantry cranes.

The large trailers pulled by a tractor unit come in many styles, lengths, and shapes. Some common types are: vans, reefers, flatbeds, sidelifts and tankers. These trailers may be refrigerated, heated, ventilated, or pressurized, depending on climate and cargo. Some trailers have movable wheel axles that can be adjusted by moving them on a track underneath the trailer body and securing them in place with large pins. The purpose of this is to help adjust weight distribution over the various axles, to comply with local laws.






</doc>
<doc id="29383" url="https://en.wikipedia.org/wiki?curid=29383" title="Stonewall riots">
Stonewall riots

The Stonewall riots (also referred to as the Stonewall uprising or the Stonewall rebellion) were a series of spontaneous, violent demonstrations by members of the gay (LGBT) community against a police raid that took place in the early morning hours of June 28, 1969, at the Stonewall Inn in the Greenwich Village neighborhood of Manhattan, New York City. They are widely considered to constitute the most important event leading to the gay liberation movement and the modern fight for LGBT rights in the United States.

Gay Americans in the 1950s and 1960s faced an anti-gay legal system. Early homophile groups in the U.S. sought to prove that gay people could be assimilated into society, and they favored non-confrontational education for homosexuals and heterosexuals alike. The last years of the 1960s, however, were very contentious, as many social/political movements were active, including the civil rights movement, the counterculture of the 1960s, and the anti–Vietnam War movement. These influences, along with the liberal environment of Greenwich Village, served as catalysts for the Stonewall riots.

Very few establishments welcomed openly gay people in the 1950s and 1960s. Those that did were often bars, although bar owners and managers were rarely gay. At the time, the Stonewall Inn was owned by the Mafia. It catered to an assortment of patrons and was known to be popular among the poorest and most marginalized people in the gay community: drag queens, transgender people, effeminate young men, butch lesbians, male prostitutes, and homeless youth. Police raids on gay bars were routine in the 1960s, but officers quickly lost control of the situation at the Stonewall Inn. Tensions between New York City police and gay residents of Greenwich Village erupted into more protests the next evening, and again several nights later. Within weeks, Village residents quickly organized into activist groups to concentrate efforts on establishing places for gays and lesbians to be open about their sexual orientation without fear of being arrested.

After the Stonewall riots, gays and lesbians in New York City faced gender, race, class, and generational obstacles to becoming a cohesive community. Within six months, two gay activist organizations were formed in New York, concentrating on confrontational tactics, and three newspapers were established to promote rights for gays and lesbians. Within a few years, gay rights organizations were founded across the U.S. and the world. On June 28, 1970, the first gay pride marches took place in New York, Los Angeles, San Francisco and Chicago commemorating the anniversary of the riots. Similar marches were organized in other cities. Today, Gay Pride events are held annually throughout the world toward the end of June to mark the Stonewall riots. The Stonewall National Monument was established at the site in 2016. As of 2017, plans were advancing by the State of New York to host the largest international LGBT pride celebration in 2019, known as Stonewall 50 / WorldPride, to commemorate the 50th anniversary of the Stonewall Riots.

Following the social upheaval of World War II, many people in the United States felt a fervent desire to "restore the prewar social order and hold off the forces of change", according to historian Barry Adam. Spurred by the national emphasis on anti-communism, Senator Joseph McCarthy conducted hearings searching for communists in the U.S. government, the U.S. Army, and other government-funded agencies and institutions, leading to a national paranoia. Anarchists, communists, and other people deemed un-American and subversive were considered security risks. Homosexuals were included in this list by the U.S. State Department on the theory that they were susceptible to blackmail. In 1950, a Senate investigation chaired by Clyde R. Hoey noted in a report, "It is generally believed that those who engage in overt acts of perversion lack the emotional stability of normal persons", and said all of the government's intelligence agencies "are in complete agreement that sex perverts in Government constitute security risks". Between 1947 and 1950, 1,700 federal job applications were denied, 4,380 people were discharged from the military, and 420 were fired from their government jobs for being suspected homosexuals.

Throughout the 1950s and 1960s, the Federal Bureau of Investigation (FBI) and police departments kept lists of known homosexuals, their favored establishments, and friends; the U.S. Post Office kept track of addresses where material pertaining to homosexuality was mailed. State and local governments followed suit: bars catering to homosexuals were shut down, and their customers were arrested and exposed in newspapers. Cities performed "sweeps" to rid neighborhoods, parks, bars, and beaches of gay people. They outlawed the wearing of opposite gender clothes, and universities expelled instructors suspected of being homosexual. Thousands of gay men and women were publicly humiliated, physically harassed, fired, jailed, or institutionalized in mental hospitals. Many lived double lives, keeping their private lives secret from their professional ones.

In 1952, the American Psychiatric Association listed homosexuality in the "Diagnostic and Statistical Manual" (DSM) as a mental disorder. A large-scale study of homosexuality in 1962 was used to justify inclusion of the disorder as a supposed pathological hidden fear of the opposite sex caused by traumatic parent–child relationships. This view was widely influential in the medical profession. In 1956, however, the psychologist Evelyn Hooker performed a study that compared the happiness and well-adjusted nature of self-identified homosexual men with heterosexual men and found no difference. Her study stunned the medical community and made her a hero to many gay men and lesbians, but homosexuality remained in the "DSM" until 1973.

In response to this trend, two organizations formed independently of each other to advance the cause of homosexuals and provide social opportunities where gays and lesbians could socialize without fear of being arrested. Los Angeles area homosexuals created the Mattachine Society in 1950, in the home of communist activist Harry Hay. Their objectives were to unify homosexuals, educate them, provide leadership, and assist "sexual deviants" with legal troubles. Facing enormous opposition to their radical approach, in 1953 the Mattachine shifted their focus to assimilation and respectability. They reasoned that they would change more minds about homosexuality by proving that gays and lesbians were normal people, no different from heterosexuals. Soon after, several women in San Francisco met in their living rooms to form the Daughters of Bilitis (DOB) for lesbians. Although the eight women who created the DOB initially came together to be able to have a safe place to dance, as the DOB grew they developed similar goals to the Mattachine, and urged their members to assimilate into general society.

One of the first challenges to government repression came in 1953. An organization named ONE, Inc. published a magazine called "ONE". The U.S. Postal Service refused to mail its August issue, which concerned homosexuals in heterosexual marriages, on the grounds that the material was obscene despite it being covered in brown paper wrapping. The case eventually went to the Supreme Court, which in 1958 ruled that ONE, Inc. could mail its materials through the Postal Service.

Homophile organizations—as homosexual groups were called—grew in number and spread to the East Coast. Gradually, members of these organizations grew bolder. Frank Kameny founded the Mattachine of Washington, D.C. He had been fired from the U.S. Army Map Service for being a homosexual, and sued unsuccessfully to be reinstated. Kameny wrote that homosexuals were no different from heterosexuals, often aiming his efforts at mental health professionals, some of whom attended Mattachine and DOB meetings telling members they were abnormal. 

In 1965, news on Cuban prison work camps for homosexuals inspired Mattachine New York and D.C. to organize protests at the United Nations and the White House. Similar demonstrations were then held also at other government buildings. The purpose was to protest the treatment of gays in Cuba and U.S. employment discrimination. These pickets shocked many gay people, and upset some of the leadership of Mattachine and the DOB. At the same time, demonstrations in the civil rights movement and opposition to the Vietnam War all grew in prominence, frequency, and severity throughout the 1960s, as did their confrontations with police forces.

On the outer fringes of the few small gay communities were people who challenged gender expectations. They were effeminate men and masculine women, or people assigned male at birth who dressed and lived as women and people assigned female at birth who dressed and lived as men, respectively, either part or full-time. Contemporary nomenclature classified them as transvestites, and they were the most visible representatives of sexual minorities. They belied the carefully crafted image portrayed by the Mattachine Society and DOB that asserted homosexuals were respectable, normal people. The Mattachine and DOB considered the trials of being arrested for wearing clothing of the opposite gender as a parallel to the struggles of homophile organizations: similar but distinctly separate. Gay and transgender people staged a small riot at the Cooper Do-nuts cafe in Los Angeles in 1959 in response to police harassment.

In a larger event in 1966 in San Francisco, drag queens, hustlers, and transvestites were sitting in Compton's Cafeteria when the police arrived to arrest men dressed as women. A riot ensued, with the patrons of the cafeteria slinging cups, plates, and saucers, and breaking the plexiglass windows in the front of the restaurant, and returning several days later to smash the windows again after they were replaced. Professor Susan Stryker classifies the Compton's Cafeteria riot as an "act of anti-transgender discrimination, rather than an act of discrimination against sexual orientation" and connects the uprising to the issues of gender, race, and class that were being downplayed by homophile organizations. It marked the beginning of transgender activism in San Francisco.

The Manhattan neighborhoods of Greenwich Village and Harlem were home to a sizable homosexual population after World War I, when men and women who had served in the military took advantage of the opportunity to settle in larger cities. The enclaves of gays and lesbians, described by a newspaper story as "short-haired women and long-haired men", developed a distinct subculture through the following two decades. Prohibition inadvertently benefited gay establishments, as drinking alcohol was pushed underground along with other behaviors considered immoral. New York City passed laws against homosexuality in public and private businesses, but because alcohol was in high demand, speakeasies and impromptu drinking establishments were so numerous and temporary that authorities were unable to police them all.

The social repression of the 1950s resulted in a cultural revolution in Greenwich Village. A cohort of poets, later named the Beat poets, wrote about the evils of the social organization at the time, glorifying anarchy, drugs, and hedonistic pleasures over unquestioning social compliance, consumerism, and closed mindedness. Of them, Allen Ginsberg and William S. Burroughs—both Greenwich Village residents—also wrote bluntly and honestly about homosexuality. Their writings attracted sympathetic liberal-minded people, as well as homosexuals looking for a community.

By the early 1960s, a campaign to rid New York City of gay bars was in full effect by order of Mayor Robert F. Wagner, Jr., who was concerned about the image of the city in preparation for the 1964 World's Fair. The city revoked the liquor licenses of the bars, and undercover police officers worked to entrap as many homosexual men as possible. Entrapment usually consisted of an undercover officer who found a man in a bar or public park, engaged him in conversation; if the conversation headed toward the possibility that they might leave together—or the officer bought the man a drink—he was arrested for solicitation. One story in the "New York Post" described an arrest in a gym locker room, where the officer grabbed his crotch, moaning, and a man who asked him if he was all right was arrested. Few lawyers would defend cases as undesirable as these, and some of those lawyers kicked back their fees to the arresting officer.

The Mattachine Society succeeded in getting newly elected Mayor John Lindsay to end the campaign of police entrapment in New York City. They had a more difficult time with the New York State Liquor Authority (SLA). While no laws prohibited serving homosexuals, courts allowed the SLA discretion in approving and revoking liquor licenses for businesses that might become "disorderly". Despite the high population of gays and lesbians who called Greenwich Village home, very few places existed, other than bars, where they were able to congregate openly without being harassed or arrested. In 1966 the New York Mattachine held a "sip-in" at a Greenwich Village bar named Julius, which was frequented by gay men, to illustrate the discrimination homosexuals faced.

None of the bars frequented by gays and lesbians were owned by gay people. Almost all of them were owned and controlled by organized crime, who treated the regulars poorly, watered down the liquor, and overcharged for drinks. However, they also paid off police to prevent frequent raids.

The Stonewall Inn, located at 51 and 53 Christopher Street, along with several other establishments in the city, was owned by the Genovese crime family. In 1966, three members of the Mafia invested $3,500 to turn the Stonewall Inn into a gay bar, after it had been a restaurant and a nightclub for heterosexuals. Once a week a police officer would collect envelopes of cash as a payoff, as the Stonewall Inn had no liquor license. It had no running water behind the bar—used glasses were run through tubs of water and immediately reused. There were no fire exits, and the toilets overran consistently. Though the bar was not used for prostitution, drug sales and other "cash transactions" took place. It was the only bar for gay men in New York City where dancing was allowed; dancing was its main draw since its re-opening as a gay club.

Visitors to the Stonewall Inn in 1969 were greeted by a bouncer who inspected them through a peephole in the door. The legal drinking age was 18, and to avoid unwittingly letting in undercover police (who were called "Lily Law", "Alice Blue Gown", or "Betty Badge"), visitors would have to be known by the doorman, or look gay. The entrance fee on weekends was $3, for which the customer received two tickets that could be exchanged for two drinks. Patrons were required to sign their names in a book to prove that the bar was a private "bottle club", but rarely signed their real names. There were two dance floors in the Stonewall; the interior was painted black, making it very dark inside, with pulsing gel lights or black lights. If police were spotted, regular white lights were turned on, signaling that everyone should stop dancing or touching. In the rear of the bar was a smaller room frequented by "queens"; it was one of two bars where effeminate men who wore makeup and teased their hair (though dressed in men's clothing) could go. Only a few transvestites, or men in full drag, were allowed in by the bouncers. The customers were "98 percent male" but a few lesbians sometimes came to the bar. Younger homeless adolescent males, who slept in nearby Christopher Park, would often try to get in so customers would buy them drinks. The age of the clientele ranged between the upper teens and early thirties, and the racial mix was evenly distributed among white, black, and Hispanic patrons. Because of its even mix of people, its location, and the attraction of dancing, the Stonewall Inn was known by many as ""the" gay bar in the city".

Police raids on gay bars were frequent—occurring on average once a month for each bar. Many bars kept extra liquor in a secret panel behind the bar, or in a car down the block, to facilitate resuming business as quickly as possible if alcohol was seized. Bar management usually knew about raids beforehand due to police tip-offs, and raids occurred early enough in the evening that business could commence after the police had finished. During a typical raid, the lights were turned on, and customers were lined up and their identification cards checked. Those without identification or dressed in full drag were arrested; others were allowed to leave. Some of the men, including those in drag, used their draft cards as identification. Women were required to wear three pieces of feminine clothing, and would be arrested if found not wearing them. Employees and management of the bars were also typically arrested. The period immediately before June 28, 1969, was marked by frequent raids of local bars—including a raid at the Stonewall Inn on the Tuesday before the riots—and the closing of the Checkerboard, the Tele-Star, and two other clubs in Greenwich Village.

At 1:20 a.m. on Saturday, June 28, 1969, four plainclothes policemen in dark suits, two patrol officers in uniform, and Detective Charles Smythe and Deputy Inspector Seymour Pine arrived at the Stonewall Inn's double doors and announced "Police! We're taking the place!" Stonewall employees do not recall being tipped off that a raid was to occur that night, as was the custom. According to Duberman (p. 194), there was a rumor that one might happen, but since it was much later than raids generally took place, Stonewall management thought the tip was inaccurate. Days after the raid, one of the bar owners complained that the tipoff had never come, and that the raid was ordered by the Bureau of Alcohol, Tobacco, and Firearms, who objected that there were no stamps on the liquor bottles, indicating the alcohol was bootlegged.

Historian David Carter presents information indicating that the Mafia owners of the Stonewall and the manager were blackmailing wealthier customers, particularly those who worked in the Financial District. They appeared to be making more money from extortion than they were from liquor sales in the bar. Carter deduces that when the police were unable to receive kickbacks from blackmail and the theft of negotiable bonds (facilitated by pressuring gay Wall Street customers), they decided to close the Stonewall Inn permanently. Two undercover policewomen and two undercover policemen had entered the bar earlier that evening to gather visual evidence, as the Public Morals Squad waited outside for the signal. Once inside, they called for backup from the Sixth Precinct using the bar's pay telephone. The music was turned off and the main lights were turned on. Approximately 205 people were in the bar that night. Patrons who had never experienced a police raid were confused. A few who realized what was happening began to run for doors and windows in the bathrooms, but police barred the doors. Michael Fader remembered,

Things happened so fast you kind of got caught not knowing. All of a sudden there were police there and we were told to all get in lines and to have our identification ready to be led out of the bar.

The raid did not go as planned. Standard procedure was to line up the patrons, check their identification, and have female police officers take customers dressed as women to the bathroom to verify their sex, upon which any men dressed as women would be arrested. Those dressed as women that night refused to go with the officers. Men in line began to refuse to produce their identification. The police decided to take everyone present to the police station, after separating those cross-dressing in a room in the back of the bar. Maria Ritter, then known as male to her family, recalled, "My biggest fear was that I would get arrested. My second biggest fear was that my picture would be in a newspaper or on a television report in my mother's dress!" Both patrons and police recalled that a sense of discomfort spread very quickly, spurred by police who began to assault some of the lesbians by "feeling some of them up inappropriately" while frisking them.

The police were to transport the bar's alcohol in patrol wagons. Twenty-eight cases of beer and nineteen bottles of hard liquor were seized, but the patrol wagons had not yet arrived, so patrons were required to wait in line for about 15 minutes. Those who were not arrested were released from the front door, but they did not leave quickly as usual. Instead, they stopped outside and a crowd began to grow and watch. Within minutes, between 100 and 150 people had congregated outside, some after they were released from inside the Stonewall, and some after noticing the police cars and the crowd. Although the police forcefully pushed or kicked some patrons out of the bar, some customers released by the police performed for the crowd by posing and saluting the police in an exaggerated fashion. The crowd's applause encouraged them further: "Wrists were limp, hair was primped, and reactions to the applause were classic."

When the first patrol wagon arrived, Inspector Pine recalled that the crowd—most of whom were homosexual—had grown to at least ten times the number of people who were arrested, and they all became very quiet. Confusion over radio communication delayed the arrival of a second wagon. The police began escorting Mafia members into the first wagon, to the cheers of the bystanders. Next, regular employees were loaded into the wagon. A bystander shouted, "Gay power!", someone began singing "We Shall Overcome", and the crowd reacted with amusement and general good humor mixed with "growing and intensive hostility". An officer shoved a transvestite, who responded by hitting him on the head with her purse as the crowd began to boo. Author Edmund White, who had been passing by, recalled, "Everyone's restless, angry, and high-spirited. No one has a slogan, no one even has an attitude, but something's brewing." Pennies, then beer bottles, were thrown at the wagon as a rumor spread through the crowd that patrons still inside the bar were being beaten.

A scuffle broke out when a woman in handcuffs was escorted from the door of the bar to the waiting police wagon several times. She escaped repeatedly and fought with four of the police, swearing and shouting, for about ten minutes. Described as "a typical New York butch" and "a dyke–stone butch", she had been hit on the head by an officer with a baton for, as one witness claimed, complaining that her handcuffs were too tight. Bystanders recalled that the woman, whose identity remains unknown (Stormé DeLarverie has been identified by some, including herself, as the woman, but accounts vary), sparked the crowd to fight when she looked at bystanders and shouted, "Why don't you guys do something?" After an officer picked her up and heaved her into the back of the wagon, the crowd became a mob and went "berserk": "It was at that moment that the scene became explosive."

The police tried to restrain some of the crowd, and knocked a few people down, which incited bystanders even more. Some of those handcuffed in the wagon escaped when police left them unattended (deliberately, according to some witnesses). As the crowd tried to overturn the police wagon, two police cars and the wagon—with a few slashed tires—left immediately, with Inspector Pine urging them to return as soon as possible. The commotion attracted more people who learned what was happening. Someone in the crowd declared that the bar had been raided because "they didn't pay off the cops", to which someone else yelled "Let's pay them off!" Coins sailed through the air towards the police as the crowd shouted "Pigs!" and "Faggot cops!" Beer cans were thrown and the police lashed out, dispersing some of the crowd who found a construction site nearby with stacks of bricks. The police, outnumbered by between 500 and 600 people, grabbed several people, including folk singer Dave Van Ronk—who had been attracted to the revolt from a bar two doors away from the Stonewall. Though Van Ronk was not gay, he had experienced police violence when he participated in antiwar demonstrations: "As far as I was concerned, anybody who'd stand against the cops was all right with me, and that's why I stayed in... Every time you turned around the cops were pulling some outrage or another." Ten police officers—including two policewomen—barricaded themselves, Van Ronk, Howard Smith (a writer for "The Village Voice"), and several handcuffed detainees inside the Stonewall Inn for their own safety.

Multiple accounts of the riot assert that there was no pre-existing organization or apparent cause for the demonstration; what ensued was spontaneous. Michael Fader explained,

We all had a collective feeling like we'd had enough of this kind of shit. It wasn't anything tangible anybody said to anyone else, it was just kind of like everything over the years had come to a head on that one particular night in the one particular place, and it was not an organized demonstration... Everyone in the crowd felt that we were never going to go back. It was like the last straw. It was time to reclaim something that had always been taken from us... All kinds of people, all different reasons, but mostly it was total outrage, anger, sorrow, everything combined, and everything just kind of ran its course. It was the police who were doing most of the destruction. We were really trying to get back in and break free. And we felt that we had freedom at last, or freedom to at least show that we demanded freedom. We weren't going to be walking meekly in the night and letting them shove us around—it's like standing your ground for the first time and in a really strong way, and that's what caught the police by surprise. There was something in the air, freedom a long time overdue, and we're going to fight for it. It took different forms, but the bottom line was, we weren't going to go away. And we didn't.

The only photograph taken during the first night of the riots shows the homeless youth who slept in nearby Christopher Park, scuffling with police. The Mattachine Society newsletter a month later offered its explanation of why the riots occurred: "It catered largely to a group of people who are not welcome in, or cannot afford, other places of homosexual social gathering... The Stonewall became home to these kids. When it was raided, they fought for it. That, and the fact that they had nothing to lose other than the most tolerant and broadminded gay place in town, explains why."

Garbage cans, garbage, bottles, rocks, and bricks were hurled at the building, breaking the windows. Witnesses attest that "flame queens", hustlers, and gay "street kids"—the most outcast people in the gay community—were responsible for the first volley of projectiles, as well as the uprooting of a parking meter used as a battering ram on the doors of the Stonewall Inn. Sylvia Rivera, a self-identified street queen who had been in the Stonewall during the raid, remembered:

You've been treating us like shit all these years? Uh-uh. Now it's our turn!... It was one of the greatest moments in my life.

The mob lit garbage on fire and stuffed it through the broken windows as the police grabbed a fire hose. Because it had no water pressure, the hose was ineffective in dispersing the crowd, and seemed only to encourage them. When demonstrators broke through the windows—which had been covered by plywood by the bar owners to deter the police from raiding the bar—the police inside unholstered their pistols. The doors flew open and officers pointed their weapons at the angry crowd, threatening to shoot. "The Village Voice" writer Howard Smith, in the bar with the police, took a wrench from the bar and stuffed it in his pants, unsure if he might have to use it against the mob or the police. He watched someone squirt lighter fluid into the bar; as it was lit and the police took aim, sirens were heard and fire trucks arrived. The onslaught had lasted 45 minutes.

The Tactical Patrol Force (TPF) of the New York City Police Department arrived to free the police trapped inside the Stonewall. One officer's eye was cut, and a few others were bruised from being struck by flying debris. Bob Kohler, who was walking his dog by the Stonewall that night, saw the TPF arrive: "I had been in enough riots to know the fun was over... The cops were totally humiliated. This never, ever happened. They were angrier than I guess they had ever been, because everybody else had rioted... but the fairies were not supposed to riot... no group had ever forced cops to retreat before, so the anger was just enormous. I mean, they wanted to kill." With larger numbers, police detained anyone they could and put them in patrol wagons to go to jail, though Inspector Pine recalled, "Fights erupted with the transvestites, who wouldn't go into the patrol wagon." His recollection was corroborated by another witness across the street who said, "All I could see about who was fighting was that it was transvestites and they were fighting furiously."

The TPF formed a phalanx and attempted to clear the streets by marching slowly and pushing the crowd back. The mob openly mocked the police. The crowd cheered, started impromptu kick lines, and sang to the tune of Ta-ra-ra Boom-de-ay: "We are the Stonewall girls/ We wear our hair in curls/ We don't wear underwear/ We show our pubic hair." Lucian Truscott reported in "The Village Voice": "A stagnant situation there brought on some gay tomfoolery in the form of a chorus line facing the line of helmeted and club-carrying cops. Just as the line got into a full kick routine, the TPF advanced again and cleared the crowd of screaming gay power[-]ites down Christopher to Seventh Avenue." One participant who had been in the Stonewall during the raid recalled, "The police rushed us, and that's when I realized this is not a good thing to do, because they got me in the back with a nightstick." Another account stated, "I just can't ever get that one sight out of my mind. The cops with the [nightsticks] and the kick line on the other side. It was the most amazing thing... And all the sudden that kick line, which I guess was a spoof on the machismo... I think that's when I felt rage. Because people were getting smashed with bats. And for what? A kick line."

Craig Rodwell, owner of the Oscar Wilde Memorial Bookshop, reported watching police chase participants through the crooked streets, only to see them appear around the next corner behind the police. Members of the mob stopped cars, overturning one of them to block Christopher Street. Jack Nichols and Lige Clarke, in their column printed in "Screw", declared that "massive crowds of angry protesters chased [the police] for blocks screaming, 'Catch them!' "

By 4:00 in the morning the streets had nearly been cleared. Many people sat on stoops or gathered nearby in Christopher Park throughout the morning, dazed in disbelief at what had transpired. Many witnesses remembered the surreal and eerie quiet that descended upon Christopher Street, though there continued to be "electricity in the air". One commented: "There was a certain beauty in the aftermath of the riot... It was obvious, at least to me, that a lot of people really were gay and, you know, this was our street." Thirteen people had been arrested. Some in the crowd were hospitalized, and four police officers were injured. Almost everything in the Stonewall Inn was broken. Inspector Pine had intended to close and dismantle the Stonewall Inn that night. Pay phones, toilets, mirrors, jukeboxes, and cigarette machines were all smashed, possibly in the riot and possibly by the police.

During the siege of the Stonewall, Craig Rodwell called "The New York Times", the "New York Post", and the "Daily News" to inform them what was happening. All three papers covered the riots; the "Daily News" placed coverage on the front page. News of the riot spread quickly throughout Greenwich Village, fueled by rumors that it had been organized by the Students for a Democratic Society, the Black Panthers, or triggered by "a homosexual police officer whose roommate went dancing at the Stonewall against the officer's wishes". All day Saturday, June 28, people came to stare at the burned and blackened Stonewall Inn. Graffiti appeared on the walls of the bar, declaring "Drag power", "They invaded our rights", "Support gay power", and "Legalize gay bars", along with accusations of police looting, and—regarding the status of the bar—"We are open."

The next night, rioting again surrounded Christopher Street; participants remember differently which night was more frantic or violent. Many of the same people returned from the previous evening—hustlers, street youths, and "queens"—but they were joined by "police provocateurs", curious bystanders, and even tourists. Remarkable to many was the sudden exhibition of homosexual affection in public, as described by one witness: "From going to places where you had to knock on a door and speak to someone through a peephole in order to get in. We were just out. We were in the streets."

Thousands of people had gathered in front of the Stonewall, which had opened again, choking Christopher Street until the crowd spilled into adjoining blocks. The throng surrounded buses and cars, harassing the occupants unless they either admitted they were gay or indicated their support for the demonstrators. Sylvia Rivera saw a friend of hers jump on a nearby car trying to drive through; the crowd rocked the car back and forth, terrifying its occupants. Another of Rivera's friends, Marsha P. Johnson, an African-American street queen, climbed a lamppost and dropped a heavy bag onto the hood of a police car, shattering the windshield. As on the previous evening, fires were started in garbage cans throughout the neighborhood. More than a hundred police were present from the Fourth, Fifth, Sixth, and Ninth Precincts, but after 2:00 a.m. the TPF arrived again. Kick lines and police chases waxed and waned; when police captured demonstrators, whom the majority of witnesses described as "sissies" or "swishes", the crowd surged to recapture them. Street battling ensued again until 4:00 a.m.

Beat poet and longtime Greenwich Village resident Allen Ginsberg lived on Christopher Street, and happened upon the jubilant chaos. After he learned of the riot that had occurred the previous evening, he stated, "Gay power! Isn't that great!... It's about time we did something to assert ourselves", and visited the open Stonewall Inn for the first time. While walking home, he declared to Lucian Truscott, "You know, the guys there were so beautiful—they've lost that wounded look that fags all had 10 years ago."

Activity in Greenwich Village was sporadic on Monday and Tuesday, partly due to rain. Police and Village residents had a few altercations, as both groups antagonized each other. Craig Rodwell and his partner Fred Sargeant took the opportunity the morning after the first riot to print and distribute 5,000 leaflets, one of them reading: "Get the Mafia and the Cops out of Gay Bars." The leaflets called for gays to own their own establishments, for a boycott of the Stonewall and other Mafia-owned bars, and for public pressure on the mayor's office to investigate the "intolerable situation".

Not everyone in the gay community considered the revolt a positive development. To many older homosexuals and many members of the Mattachine Society who had worked throughout the 1960s to promote homosexuals as no different from heterosexuals, the display of violence and effeminate behavior was embarrassing. Randy Wicker, who had marched in the first gay picket lines before the White House in 1965, said the "screaming queens forming chorus lines and kicking went against everything that I wanted people to think about homosexuals... that we were a bunch of drag queens in the Village acting disorderly and tacky and cheap." Others found the closing of the Stonewall Inn, termed a "sleaze joint", as advantageous to the Village.

On Wednesday, however, "The Village Voice" ran reports of the riots, written by Howard Smith and Lucian Truscott, that included unflattering descriptions of the events and its participants: "forces of faggotry", "limp wrists", and "Sunday fag follies". A mob descended upon Christopher Street once again and threatened to burn down the offices of "The Village Voice". Also in the mob of between 500 and 1,000 were other groups that had had unsuccessful confrontations with the police, and were curious how the police were defeated in this situation. Another explosive street battle took place, with injuries to demonstrators and police alike, looting in local shops, and arrests of five people. The incidents on Wednesday night lasted about an hour, and were summarized by one witness: "The word is out. Christopher Street shall be liberated. The fags have had it with oppression."

The feeling of urgency spread throughout Greenwich Village, even to people who had not witnessed the riots. Many who were moved by the rebellion attended organizational meetings, sensing an opportunity to take action. On July 4, 1969, the Mattachine Society performed its annual picketing in front of Independence Hall in Philadelphia, called the Annual Reminder. Organizers Craig Rodwell, Frank Kameny, Randy Wicker, Barbara Gittings, and Kay Lahusen, who had all participated for several years, took a bus along with other picketers from New York City to Philadelphia. Since 1965, the pickets had been very controlled: women wore skirts and men wore suits and ties, and all marched quietly in organized lines. This year Rodwell remembered feeling restricted by the rules Kameny had set. When two women spontaneously held hands, Kameny broke them apart, saying, "None of that! None of that!" Rodwell, however, convinced about ten couples to hold hands. The hand-holding couples made Kameny furious, but they earned more press attention than all of the previous marches. Participant Lilli Vincenz remembered, "It was clear that things were changing. People who had felt oppressed now felt empowered." Rodwell returned to New York City determined to change the established quiet, meek ways of trying to get attention. One of his first priorities was planning Christopher Street Liberation Day.

Although the Mattachine Society had existed since the 1950s, many of their methods now seemed too mild for people who had witnessed or been inspired by the riots. Mattachine recognized the shift in attitudes in a story from their newsletter entitled, "The Hairpin Drop Heard Around the World." When a Mattachine officer suggested an "amicable and sweet" candlelight vigil demonstration, a man in the audience fumed and shouted, "Sweet! "Bullshit!" That's the role society has been forcing these queens to play." With a flyer announcing: "Do You Think Homosexuals Are Revolting? You Bet Your Sweet Ass We Are!", the Gay Liberation Front (GLF) was soon formed, the first gay organization to use "gay" in its name. Previous organizations such as the Mattachine Society, the Daughters of Bilitis, and various homophile groups had masked their purpose by deliberately choosing obscure names.

The rise of militancy became apparent to Frank Kameny and Barbara Gittings — who had worked in homophile organizations for years and were both very public about their roles — when they attended a GLF meeting to see the new group. A young GLF member demanded to know who they were and what their credentials were. Gittings, nonplussed, stammered, "I'm gay. That's why I'm here." The GLF borrowed tactics from and aligned themselves with black and antiwar demonstrators with the ideal that they "could work to restructure American society". They took on causes of the Black Panthers, marching to the Women's House of Detention in support of Afeni Shakur, and other radical New Left causes. Four months after they formed, however, the group disbanded when members were unable to agree on operating procedure.

Within six months of the Stonewall riots, activists started a citywide newspaper called "Gay"; they considered it necessary because the most liberal publication in the city—"The Village Voice"—refused to print the word "gay" in GLF advertisements seeking new members and volunteers. Two other newspapers were initiated within a six-week period: "Come Out!" and "Gay Power"; the readership of these three periodicals quickly climbed to between 20,000 and 25,000.

GLF members organized several same-sex dances, but GLF meetings were chaotic. When Bob Kohler asked for clothes and money to help the homeless youth who had participated in the riots, many of whom slept in Christopher Park or Sheridan Square, the response was a discussion on the downfall of capitalism. In late December 1969, several people who had visited GLF meetings and left out of frustration formed the Gay Activists Alliance (GAA). The GAA was to be entirely focused on gay issues, and more orderly. Their constitution started, "We as liberated homosexual activists demand the freedom for expression of our dignity and value as human beings." The GAA developed and perfected a confrontational tactic called a zap, where they would catch a politician off guard during a public relations opportunity, and force him or her to acknowledge gay and lesbian rights. City councilmen were zapped, and Mayor John Lindsay was zapped several times—once on television when GAA members made up the majority of the audience.

Raids on gay bars did not stop after the Stonewall riots. In March 1970, Deputy Inspector Seymour Pine raided the Zodiac and 17 Barrow Street. An after-hours gay club with no liquor or occupancy licenses called The Snake Pit was soon raided, and 167 people were arrested. One of them was Diego Viñales, an Argentinian national so frightened that he might be deported as a homosexual that he tried to escape the police precinct by jumping out of a two-story window, impaling himself on a spike fence. "The New York Daily News" printed a graphic photo of the young man's impalement on the front page. GAA members organized a march from Christopher Park to the Sixth Precinct in which hundreds of gays, lesbians, and liberal sympathizers peacefully confronted the TPF. They also sponsored a letter-writing campaign to Mayor Lindsay in which the Greenwich Village Democratic Party and Congressman Ed Koch sent pleas to end raids on gay bars in the city.

The Stonewall Inn lasted only a few weeks after the riot. By October 1969 it was up for rent. Village residents surmised it was too notorious a location, and Rodwell's boycott discouraged business.

Christopher Street Liberation Day on June 28, 1970 marked the first anniversary of the Stonewall riots with an assembly on Christopher Street; with simultaneous Gay Pride marches in Los Angeles and Chicago, these were the first Gay Pride marches in U.S. history. The next year, Gay Pride marches took place in Boston, Dallas, Milwaukee, London, Paris, West Berlin, and Stockholm. The march in New York covered 51 blocks, from Christopher Street to Central Park. It took less than half the scheduled time not due to the excitement of the marchers, but because they were wary about walking through the city with gay banners and signs. Although the parade permit was delivered only two hours before the start of the march, the marchers encountered little resistance from onlookers. "The New York Times" reported (on the front page) that the marchers took up the entire street for about 15 city blocks. Reporting by "The Village Voice" was positive, describing "the out-front resistance that grew out of the police raid on the Stonewall Inn one year ago".

By 1972, the participating cities included Atlanta, Buffalo, Detroit, Washington, D.C., Miami, Minneapolis, and Philadelphia, as well as San Francisco.

Frank Kameny soon realized the pivotal change brought by the Stonewall riots. An organizer of gay activism in the 1950s, he was used to persuasion, trying to convince heterosexuals that gay people were no different than they were. When he and other people marched in front of the White House, the State Department, and Independence Hall only five years earlier, their objective was to look as if they could work for the U.S. government. Ten people marched with Kameny then, and they alerted no press to their intentions. Although he was stunned by the upheaval by participants in the Annual Reminder in 1969, he later observed, "By the time of Stonewall, we had fifty to sixty gay groups in the country. A year later there was at least fifteen hundred. By two years later, to the extent that a count could be made, it was twenty-five hundred."

Similar to Kameny's regret at his own reaction to the shift in attitudes after the riots, Randy Wicker came to describe his embarrassment as "one of the greatest mistakes of his life". The image of gays retaliating against police, after so many years of allowing such treatment to go unchallenged, "stirred an unexpected spirit among many homosexuals". Kay Lahusen, who photographed the marches in 1965, stated, "Up to 1969, this movement was generally called the homosexual or homophile movement... Many new activists consider the Stonewall uprising the birth of the gay liberation movement. Certainly it was the birth of gay pride on a massive scale." David Carter, in his article "What made Stonewall different", explained that even though there were several uprisings before Stonewall, the reason Stonewall was so historical was that thousands of people were involved, the riot lasted a long time (six days), it was the first to get major media coverage, and it sparked the formation of many gay rights groups.

Within two years of the Stonewall riots there were gay rights groups in every major American city, as well as Canada, Australia, and Western Europe. People who joined activist organizations after the riots had very little in common other than their same-sex attraction. Many who arrived at GLF or GAA meetings were taken aback by the number of gay people in one place. Race, class, ideology, and gender became frequent obstacles in the years after the riots. This was illustrated during the 1973 Stonewall rally when, moments after Barbara Gittings exuberantly praised the diversity of the crowd, feminist activist Jean O'Leary protested what she perceived as the mocking of women by cross-dressers and drag queens in attendance. During a speech by O'Leary, in which she claimed that drag queens made fun of women for entertainment value and profit, Sylvia Rivera and Lee Brewster jumped on the stage and shouted "You go to bars because of what drag queens did for you, and "these bitches" tell us to quit being ourselves!" Both the drag queens and lesbian feminists in attendance left in disgust.

O'Leary also worked in the early 1970s to exclude trans people from gay rights issues because she felt that rights for trans people would be too difficult to attain. Sylvia Rivera left New York City in the mid-1970s, relocating to upstate New York, but later returned to the city in the mid-1990s to advocate for homeless members of the gay community. The initial disagreements between participants in the movements, however, often evolved after further reflection. O'Leary later regretted her stance against the drag queens attending in 1973: "Looking back, I find this so embarrassing because my views have changed so much since then. I would never pick on a transvestite now." "It was horrible. How could I work to exclude transvestites and at the same time criticize the feminists who were doing their best back in those days to exclude lesbians?"

O'Leary was referring to the Lavender Menace, a description by second wave feminist Betty Friedan for attempts by members of the National Organization for Women (NOW) to distance themselves from the perception of NOW as a haven for lesbians. As part of this process, Rita Mae Brown and other lesbians who had been active in NOW were forced out. They staged a protest in 1970 at the Second Congress to Unite Women, and earned the support of many NOW members, finally gaining full acceptance in 1971.

The growth of lesbian feminism in the 1970s at times so conflicted with the gay liberation movement that some lesbians refused to work with gay men. Many lesbians found men's attitudes patriarchal and chauvinistic, and saw in gay men the same misguided notions about women as they saw in heterosexual men. The issues most important to gay men—entrapment and public solicitation—were not shared by lesbians. In 1977 a Lesbian Pride Rally was organized as an alternative to sharing gay men's issues, especially what Adrienne Rich termed "the violent, self-destructive world of the gay bars". Veteran gay activist Barbara Gittings chose to work in the gay rights movement, rationalizing "It's a matter of where does it hurt the most? For me it hurts the most not in the female arena, but the gay arena."

Throughout the 1970s gay activism had significant successes. One of the first and most important was the "zap" in May 1970 by the Los Angeles GLF at a convention of the American Psychiatric Association (APA). At a conference on behavior modification, during a film demonstrating the use of electroshock therapy to decrease same-sex attraction, Morris Kight and GLF members in the audience interrupted the film with shouts of "Torture!" and "Barbarism!" They took over the microphone to announce that medical professionals who prescribed such therapy for their homosexual patients were complicit in torturing them. Although 20 psychiatrists in attendance left, the GLF spent the hour following the zap with those remaining, trying to convince them that homosexuals were not mentally ill. When the APA invited gay activists to speak to the group in 1972, activists brought John E. Fryer, a gay psychiatrist who wore a mask, because he felt his practice was in danger. In December 1973—in large part due to the efforts of gay activists—the APA voted unanimously to remove homosexuality from the "Diagnostic and Statistical Manual".

Gay men and lesbians came together to work in grassroots political organizations responding to organized resistance in 1977. A coalition of conservatives named Save Our Children staged a campaign to repeal a civil rights ordinance in Dade County, Florida. Save Our Children was successful enough to influence similar repeals in several American cities in 1978. However, the same year a campaign in California called the Briggs Initiative, designed to force the dismissal of homosexual public school employees, was defeated. Reaction to the influence of Save Our Children and the Briggs Initiative in the gay community was so significant that it has been called the second Stonewall for many activists, marking their initiation into political participation.

The Stonewall riots marked such a significant turning point that many aspects of prior gay and lesbian culture, such as bar culture formed from decades of shame and secrecy, were forcefully ignored and denied. Historian Martin Duberman writes, "The decades preceding Stonewall... continue to be regarded by most gays and lesbians as some vast neolithic wasteland." Sociologist Barry Adam notes, "Every social movement must choose at some point what to retain and what to reject out of its past. What traits are the results of oppression and what are healthy and authentic?" In conjunction with the growing feminist movement of the early 1970s, roles of butch and femme that developed in lesbian bars in the 1950s and 1960s were rejected, because as one writer put it: "all role playing is sick." Lesbian feminists considered the butch roles as archaic imitations of masculine behavior. Some women, according to Lillian Faderman, were eager to shed the roles they felt forced into playing. The roles returned for some women in the 1980s, although they allowed for more flexibility than before Stonewall.

Author Michael Bronski highlights the "attack on pre-Stonewall culture", particularly gay pulp fiction for men, where the themes often reflected self-hatred or ambivalence about being gay. Many books ended unsatisfactorily and drastically, often with suicide, and writers portrayed their gay characters as alcoholics or deeply unhappy. These books, which he describes as "an enormous and cohesive literature by and for gay men", have not been reissued and are lost to later generations. Dismissing the reason simply as political correctness, Bronski writes, "gay liberation was a youth movement whose sense of history was defined to a large degree by rejection of the past."

The riots spawned from a bar raid became a literal example of gays and lesbians fighting back, and a symbolic call to arms for many people. Historian David Carter remarks in his book about the Stonewall riots that the bar itself was a complex business that represented a community center, an opportunity for the Mafia to blackmail its own customers, a home, and a place of "exploitation and degradation". The true legacy of the Stonewall riots, Carter insists, is the "ongoing struggle for lesbian, gay, bisexual, and transgender equality". Historian Nicholas Edsall writes, Stonewall has been compared to any number of acts of radical protest and defiance in American history from the Boston Tea Party on. But the best and certainly a more nearly contemporary analogy is with Rosa Parks' refusal to move to the back of the bus in Montgomery, Alabama, in December 1955, which sparked the modern civil rights movement. Within months after Stonewall radical gay liberation groups and newsletters sprang up in cities and on college campuses across America and then across all of northern Europe as well.

Before the rebellion at the Stonewall Inn, homosexuals were, as historians Dudley Clendinen and Adam Nagourney write, a secret legion of people, known of but discounted, ignored, laughed at or despised. And like the holders of a secret, they had an advantage which was a disadvantage, too, and which was true of no other minority group in the United States. They were invisible. Unlike African Americans, women, Native Americans, Jews, the Irish, Italians, Asians, Hispanics, or any other cultural group which struggled for respect and equal rights, homosexuals had no physical or cultural markings, no language or dialect which could identify them to each other, or to anyone else... But that night, for the first time, the usual acquiescence turned into violent resistance... From that night the lives of millions of gay men and lesbians, and the attitude toward them of the larger culture in which they lived, began to change rapidly. People began to appear in public as homosexuals, demanding respect.

Historian Lillian Faderman calls the riots the "shot heard round the world", explaining, "The Stonewall Rebellion was crucial because it sounded the rally for that movement. It became an emblem of gay and lesbian power. By calling on the dramatic tactic of violent protest that was being used by other oppressed groups, the events at the Stonewall implied that homosexuals had as much reason to be disaffected as they."
Joan Nestle co-founded the Lesbian Herstory Archives in 1974, and credits "its creation to that night and the courage that found its voice in the streets." Cautious, however, not to attribute the start of gay activism to the Stonewall riots, Nestle writes,

I certainly don't see gay and lesbian history starting with Stonewall... and I don't see resistance starting with Stonewall. What I do see is a historical coming together of forces, and the sixties changed how human beings endured things in this society and what they refused to endure... Certainly something special happened on that night in 1969, and we've made it more special in our need to have what I call a point of origin... it's more complex than saying that it all started with Stonewall.

The events of the early morning of June 28, 1969 were not the first instances of homosexuals fighting back against police in New York City and elsewhere. Not only had the Mattachine Society been active in major cities such as Los Angeles and Chicago, but similarly marginalized people started the riot at Compton's Cafeteria in 1966, and another riot responded to a raid on Los Angeles' Black Cat Tavern in 1967. However, several circumstances were in place that made the Stonewall riots memorable. The location of the raid was a factor: it was across the street from "The Village Voice" offices, and the narrow crooked streets gave the rioters advantage over the police. Many of the participants and residents of Greenwich Village were involved in political organizations that were effectively able to mobilize a large and cohesive gay community in the weeks and months after the rebellion. The most significant facet of the Stonewall riots, however, was the commemoration of them in Christopher Street Liberation Day, which grew into the annual Gay Pride events around the world.

Stonewall (officially Stonewall Equality Limited) is an LGBT rights charity in the United Kingdom, founded in 1989, and named after the Stonewall Inn because of the Stonewall riots. The Stonewall Awards is an annual event by Stonewall held since 2006 to recognize people who have affected the lives of British lesbian, gay, and bisexual people.

The middle of the 1990s was marked by the inclusion of bisexuals as a represented group within the gay community, when they successfully sought to be included on the platform of the 1993 March on Washington for Lesbian, Gay and Bi Equal Rights and Liberation. Transgender people also asked to be included, but were not, though trans-inclusive language was added to the march's list of demands. The transgender community continued to find itself simultaneously welcome and at odds with the gay community as attitudes about binary and fluid sexual orientation and gender developed and came increasingly into conflict. In 1994, New York City celebrated "Stonewall 25" with a march that went past the United Nations Headquarters and into Central Park. Estimates put the attendance at 1.1 million people. Sylvia Rivera led an alternate march in New York City in 1994 to protest the exclusion of transgender people from the events. Attendance at LGBT Pride events has grown substantially over the decades. Most large cities around the world now have some kind of Pride demonstration. Pride events in some cities mark the largest annual celebration of any kind. The growing trend towards commercializing marches into parades—with events receiving corporate sponsorship—has caused concern about taking away the autonomy of the original grassroots demonstrations that put inexpensive activism in the hands of individuals.

A "Stonewall Shabbat Seder" was first held at B’nai Jeshurun, a synagogue on New York’s Upper West Side, in 1995.

In June 1999, the U.S. Department of the Interior designated 51 and 53 Christopher Street and the surrounding streets as a National Historic Landmark, the first of significance to the lesbian, gay, bisexual and transgender community. In a dedication ceremony, Assistant Secretary of the Department of the Interior John Berry stated, "Let it forever be remembered that here—on this spot—men and women stood proud, they stood fast, so that we may be who we are, we may work where we will, live where we choose and love whom our hearts desire." The Stonewall Inn itself was named a National Historic Landmark in 2000, and it is located in the Greenwich Village Historic District, a preserved area.

On June 1, 2009, President Barack Obama declared June 2009 Lesbian, Gay, Bisexual, and Transgender Pride Month, citing the riots as a reason to "commit to achieving equal justice under law for LGBT Americans". The year marked the 40th anniversary of the riots, giving journalists and activists cause to reflect on progress made since 1969. Frank Rich in "The New York Times" noted that no federal legislation exists to protect the rights of gay Americans. An editorial in the "Washington Blade" compared the scruffy, violent activism during and following the Stonewall riots to the lackluster response to failed promises given by President Obama; for being ignored, wealthy LGBT activists reacted by promising to give less money to Democratic causes. Two years later, the Stonewall Inn served as a rallying point for celebrations after the New York Senate voted to pass same-sex marriage. The act was signed into law by Governor Andrew Cuomo on June 24, 2011. Individual states continue to battle with homophobia. The Missouri Senate passed a measure its supporters characterize as a religious freedom bill that could change the state's constitution despite Democrats' objections, and their 39-hour filibuster. This bill allows the "protection of certain religious organizations and individuals from being penalized by the state because of their sincere religious beliefs or practices concerning marriage between two persons of the same sex" discriminating against homosexual patronage.

Obama also referenced the Stonewall riots in a call for full equality during his second inaugural address on January 21, 2013:

We, the people, declare today that the most evident of truths—that all of us are created equal—is the star that guides us still; just as it guided our forebears through Seneca Falls, and Selma, and Stonewall... Our journey is not complete until our gay brothers and sisters are treated like anyone else under the law—for if we are truly created equal, then surely the love we commit to one another must be equal as well.
This was a historic moment, being the first time that a president mentioned gay rights or the word "gay" in an inaugural address.

In 2014, a marker dedicated to the Stonewall riots was included in the Legacy Walk, an outdoor public display in Chicago celebrating LGBT history and people.

On May 29, 2015, the New York City Landmarks Preservation Commission announced it would officially consider designating the Stonewall Inn as a landmark, making it the first city location to be considered based on its LGBT cultural significance alone. On June 23, 2015, the New York City Landmarks Preservation Commission unanimously approved the designation of the Stonewall Inn as a city landmark, making it the first landmark honored for its role in the fight for gay rights.

The Stonewall Book Award is a set of three literary awards that annually recognize "exceptional merit relating to the gay/lesbian/bisexual/transgender experience" in English-language books published in the U.S.

On June 24, 2016, President Obama announced the establishment of the Stonewall National Monument, a 7.7-acre site to be administered by the National Park Service. The designation, which followed transfer of city parkland to the federal government, protects Christopher Park and adjacent areas totaling more than seven acres; the Stonewall Inn is within the boundaries of the monument but remains privately owned. The National Park Foundation formed a new nonprofit organization to raise funds for a ranger station and interpretive exhibits for the monument.








</doc>
<doc id="29388" url="https://en.wikipedia.org/wiki?curid=29388" title="Sheffer stroke">
Sheffer stroke

In Boolean functions and propositional calculus, the Sheffer stroke, named after Henry M. Sheffer, written ↑, also written | (not to be confused with "||", which is often used to represent disjunction), or D"pq" (in Bocheński notation), denotes a logical operation that is equivalent to the negation of the conjunction operation, expressed in ordinary language as "not both". It is also called nand ("not and") or the alternative denial, since it says in effect that at least one of its operands is false. In digital electronics, it corresponds to the NAND gate.

Like its dual, the NOR operator (also known as the Peirce arrow or Quine dagger), NAND can be used by itself, without any other logical operator, to constitute a logical formal system (making NAND functionally complete). This property makes the NAND gate crucial to modern digital electronics, including its use in computer processor design.

The NAND operation is a logical operation on two logical values. It produces a value of true, if — and only if — at least one of the propositions is false.

The truth table of formula_1 (also written as formula_2 "W"
are inference rules.

Since the only connective of this logic is |, the symbol | could be discarded altogether, leaving only the parentheses to group the letters. A pair of parentheses must always enclose a pair of "wff"s. Examples of theorems in this simplified notation are

The notation can be simplified further, by letting
for any "U". This simplification causes the need to change some rules:
The result is a parenthetical version of the Peirce existential graphs.

Another way to simplify the notation is to eliminate parenthesis by using Polish Notation. For example, the earlier examples with only parenthesis could be rewritten using only strokes as follows

This follows the same rules as the parenthesis version, with opening parenthesis replaced with a Sheffer stroke and the (redundant) closing parenthesis removed.





</doc>
<doc id="29390" url="https://en.wikipedia.org/wiki?curid=29390" title="Stalactite">
Stalactite

A stalactite (, ; from the Greek "stalasso", (σταλάσσω), "to drip", and meaning "that which drips") is a type of formation that hangs from the ceiling of caves, hot springs, or manmade structures such as bridges and mines. Any material which is soluble, can be deposited as a colloid, or is in suspension, or is capable of being melted, may form a stalactite. Stalactites may be composed of lava, minerals, mud, peat, pitch, sand, sinter, and amberat (crystallized urine of pack rats). A stalactite is not necessarily a speleothem, though speleothems are the most common form of stalactite because of the abundance of limestone caves.

The corresponding formation on the floor of the cave is known as a stalagmite.

The most common stalactites are speleothems, which occur in limestone caves. They form through deposition of calcium carbonate and other minerals, which is precipitated from mineralized water solutions. Limestone is the chief form of calcium carbonate rock which is dissolved by water that contains carbon dioxide, forming a calcium bicarbonate solution in underground caverns. The chemical formula for this reaction is:

This solution travels through the rock until it reaches an edge and if this is on the roof of a cave it will drip down. When the solution comes into contact with air the chemical reaction that created it is reversed and particles of calcium carbonate are deposited. The reversed reaction is:

An average growth rate is a year. The quickest growing stalactites are those formed by a constant supply of slow dripping water rich in calcium carbonate (CaCO) and carbon dioxide (CO), which can grow at per year. The drip rate must be slow enough to allow the CO to degas from the solution into the cave atmosphere, resulting in deposition of CaCO on the stalactite. Too fast a drip rate and the solution, still carrying most of the CaCO, falls to the cave floor where degassing occurs and CaCO is deposited as a stalagmite. 

All limestone stalactites begin with a single mineral-laden drop of water. When the drop falls, it deposits the thinnest ring of calcite. Each subsequent drop that forms and falls deposits another calcite ring. Eventually, these rings form a very narrow (≈4 to 5 mm diameter), hollow tube commonly known as a "soda straw" stalactite. Soda straws can grow quite long, but are very fragile. If they become plugged by debris, water begins flowing over the outside, depositing more calcite and creating the more familiar cone-shaped stalactite. The same water drops that fall from the tip of a stalactite deposit more calcite on the floor below, eventually resulting in a rounded or cone-shaped stalagmite. Unlike stalactites, stalagmites never start out as hollow "soda straws". Given enough time, these formations can meet and fuse to create pillars of calcium carbonate known as a "column".

Stalactite formation generally begins over a large area, with multiple paths for the mineral rich water to flow. As minerals are dissolved in one channel slightly more than other competing channels, the dominant channel begins to draw more and more of the available water, which speeds its growth, ultimately resulting in all other channels being choked off. This is one reason why formations tend to have minimum distances from one another. The larger the formation, the greater the interformation distance.

Another type of stalactite is formed in lava tubes while lava is still active inside. The mechanism of formation is the deposition of material on the ceilings of caves, however with lava stalactites formation happens very quickly in only a matter of hours, days, or weeks, whereas limestone stalactites may take up to thousands of years. A key difference with lava stalactites is that once the lava has ceased flowing, so too will the stalactites cease to grow. This means that if the stalactite were to be broken it would never grow back.

The generic term "lavacicle" has been applied to lava stalactites and stalagmites indiscriminately and evolved from the word icicle.

Like limestone stalactites, they can leave lava drips on the floor that turn into lava stalagmites and may eventually fuse with the corresponding stalactite to form a column.

Shark tooth stalactites The shark tooth stalactite is broad and tapering in appearance. It may begin as a small driblet of lava from a semi-solid ceiling, but then grows by accreting layers as successive flows of lava rise and fall in the lava tube, coating and recoating the stalactite with more material. They can vary from a few millimeters to over a meter in length. 

Splash stalactites
As lava flows through a tube, material will be splashed up on the ceiling and ooze back down, hardening into a stalactite. This type of formation results in a very irregularly shaped stalactite, looking somewhat like stretched taffy. Often they may be of a different color than the original lava that formed the cave.

Tubular lava stalactites
When the roof of a lava tube is cooling, a skin will form that traps semi-molten material inside. Trapped gases force lava to extrude out through small openings that result in hollow, tubular stalactites analogous to the soda straws formed as depositional speleothems in solution caves, The longest known is almost 2 meters in length. These are common in Hawaiian lava tubes and are often associated with a drip stalagmite that forms below as material is carried through the tubular stalactite and piles up on the floor beneath. Sometimes the tubular form collapses near the distal end, most likely when the pressure of escaping gases decreased and still-molten portions of the stalactites deflated and cooled.
Often these tubular stalactites will acquire a twisted, vermiform appearance as bits of lava crystallize and force the flow in different directions. These tubular lava helictites may also be influenced by air currents through a tube and point downwind.

A common stalactite found seasonally or year round in many caves is the ice stalactite, commonly referred to as icicles, especially on the surface. Water seepage from the surface will penetrate into a cave and if temperatures are below freezing the water will form stalactites. Creation may also be done by the freezing of water vapor. Similar to lava stalactites, ice stalactites form very quickly within hours or days. Unlike lava stalactites however, they may grow back as long as water and temperatures are suitable.

Ice stalactites can also form under sea ice when saline water is introduced to ocean water. These specific stalactites are referred to as brinicles.

Ice stalactites may also form corresponding stalagmites below them and given time may grow together to form an ice column.

Stalactites can also form on concrete, and on plumbing where there is a slow leak and calcium, magnesium or other ions in the water supply, although they form much more rapidly there than in the natural cave environment. These secondary deposits, such as stalactites, stalagmites, flowstone and others, which are derived from the lime, mortar or other calcareous material in concrete, outside of the "cave" environment, can not be classified as "speleothems" due to the definition of the term. The term "calthemite" is used to encompass the secondary deposits which mimic the shapes and forms of speleothems outside the cave environment.

The way stalactites form on concrete is due to different chemistry than those that form naturally in limestone caves and is due of the presence of calcium oxide in cement. Concrete is made from aggregate, sand and cement. When water is added to the mix, the calcium oxide in the cement reacts with water to form calcium hydroxide (Ca(OH)). The chemical formula for this is:

Over time, any rainwater that penetrates cracks in set (hard) concrete will carry any free calcium hydroxide in solution to the edge of the concrete. Stalactites can form when the solution emerges on the underside of the concrete structure where it is suspended in the air, for example, on a ceiling or a beam. When the solution comes into contact with air on the underside of the concrete structure, another chemical reaction takes place. The solution reacts with carbon dioxide in the air and precipitates calcium carbonate.

When this solution drops down it leaves behind particles of calcium carbonate and over time these form into a stalactite. They are normally a few centimeters long and with a diameter of approximately . The growth rate of stalactites is significantly influenced by supply continuity of saturated solution and the drip rate. A straw shaped stalactite which has formed under a concrete structure can grow as much as 2 mm per day in length, when the drip rate is approximately 11 minutes between drops. Changes in leachate solution pH can facilitate additional chemical reactions, which may also influence calthemite stalactite growth rates.

The White Chamber in the Jeita Grotto's upper cavern in Lebanon contains an limestone stalactite which is accessible to visitors and is claimed to be the longest stalactite in the world. Another such claim is made for a limestone stalactite that hangs in the Chamber of Rarities in the Gruta Rei do Mato (Sete Lagoas, Minas Gerais, Brazil). However, vertical cavers have often encountered longer stalactites while exploring. One of the longest stalactites viewable by the general public is in Pol an Ionain (Doolin Cave), County Clare, Ireland, in a karst region known as The Burren; what makes it more impressive is the fact that the stalactite is held on by a section of calcite less than .

Stalactites are first mentioned (though not by name) by the Roman natural historian Pliny in a text which also mentions stalagmites and columns and refers to their creation by the dripping of water. The term "stalactite" was coined in the 17th century by the Danish Physician Ole Worm, who coined the Latin word from the Greek word σταλακτός (stalaktos, "dripping") and the Greek suffix -ίτης (-ites, connected with or belonging to).





</doc>
<doc id="29391" url="https://en.wikipedia.org/wiki?curid=29391" title="Strangers in Paradise">
Strangers in Paradise

Strangers in Paradise is a long-running, mostly self-published black-and-white comic book that was written and drawn by Terry Moore. Essentially the story of a love triangle between two women and one man, "Strangers in Paradise" is a slice-of-life dramady that veered into the crime genre.

The first issue was published January 1, 1993. The series reached its planned conclusion in 2007 with issue #90 of volume 3. A follow up novel was announced at Comic-Con International 2012.

Terry Moore stated that "I started out wanting to do a newspaper strip, and tried one idea after another before I realised I hated the gag-a-day life and really wanted to try a story instead." The story he chose to tell turned out to be "Strangers in Paradise", or "this story about 2 girls and a guy who gets to know them" (from Moore's introduction to "The Collected Strangers in Paradise, Volume One"), which used characters he had developed during his time on the gag-a-day circuit. For example, Katchoo appears as a "happy-go-lucky wood nymph" in an early strip by Moore about an enchanted forest. These strips were collected into two trade paperbacks, but they did not include three issues. Because of this, the entire run was later published in one large paperback edition entitled "The Complete Paradise Too". This volume can be considered the true origin of Katchoo, Francine and the "Strangers in Paradise" universe.

"SiP", as it is commonly known, began as a three-issue mini-series published by Antarctic Press in 1993, which focused entirely on the relationship between the three main characters and Francine's unfaithful boyfriend. This is now known as "Volume 1". Thirteen issues were published under Moore's own "Abstract Studio" imprint, and these make up "Volume 2". This is where the "thriller" plot was introduced. The series moved to Image Comics' Homage imprint for the start of "Volume 3", but after eight issues moved back to Abstract Studio, where it continued with the same numbering. Volume 3 concluded at issue #90, released June 6, 2007.

Moore revived the series as "Strangers in Paradise XXV" in 2018 for the 25th anniversary. The new miniseries included characters and elements from Moore's other works, "Echo (comic book)", "Rachel Rising", and "Motor Girl".

The story primarily concerns the difficult relationship between two women, Helen Francine Peters (referred to as Francine throughout the series) and Katina Marie ("Katchoo") Choovanski, and their friend David Qin. Francine considers Katchoo her best friend; Katchoo is in love with Francine. David is in love with Katchoo (a relationship which Katchoo herself is deeply conflicted over). 

The love triangle (which later expands into a love rectangle with the introduction of Casey Bullock, who marries Francine's ex-boyfriend Freddie Femmur and later divorces him, in order to pursue both David and Katchoo) alternates with the mystery and intrigue regarding Katchoo's past as an underage lesbian hooker and the Parker Crime Syndicate. Run by David's lesbian sister Darcy, the "Parker Girls" work for the shadowy 'Big Six' organization, an international crime syndicate with influence over the world of politics. "Parker Girls" are highly trained women used by organized crime to control, manipulate, spy upon, and ultimately kill men and women in positions of power and authority, for the Big Six.


The series received the Eisner Award for Best Serialized Story in 1996 for "I Dream of You" as well as the National Cartoonists Society Reuben Award for Best Comic Book in 2003. It also won the GLAAD Award for Best Comic Book in 2001.

"Strangers in Paradise" has been collected into a series of full-size trade paperbacks, hardback collections, and smaller format paperback collections. These reprints collect the issues into different sets.

The full-size paperback collections to date are:

The hardback collections to date are:

The "pocket book" collections to date are:

Other books to date are:


Two limited edition statuettes of Katchoo were produced by Clayburn Moore as the first in a planned series of three statues based around the series. In the first she is standing in a skimpy black dress, and in the second she is reclining in a bath wearing her leather jacket and holding a drink and a gun.

In 2009 Shocker Toys released a Katchoo figure as part of the first series of its "Indie Spotlight" line.

In 1996 a series of trading cards was released by Comic Images, consisting of a 90-card base set plus extra collector's cards, such as the 500 'autograph cards' that featured Terry Moore's signature and information on the creation of "SiP". These extra cards were inserted randomly into packs. Also produced was a matching "SiP" binder, which came with 12 9-pocket sleeves to hold the cards.

Advertised on the official "SiP" website are character pin badges representing Francine, Katchoo and David. There is also a black tote bag featuring the "Strangers in Paradise" logo and a tumbler decorated with colour panels from the series, in addition to a postcard set and two T shirts, although several of these items are listed as 'sold out', and are hard to come by elsewhere.

On September 13, 2017, Angela Robinson and Moore announced they were developing the film adaptation.



</doc>
<doc id="29392" url="https://en.wikipedia.org/wiki?curid=29392" title="Summer">
Summer

Summer is the hottest of the four temperate seasons, falling after spring and before autumn. At the summer solstice, the days are longest and the nights are shortest, with day-length decreasing as the season progresses after the solstice. The date of the beginning of summer varies according to climate, tradition and culture. When it is summer in the Northern Hemisphere, it is winter in the Southern Hemisphere, and vice versa.

From an astronomical view, the equinoxes and solstices would be the middle of the respective seasons, but sometimes astronomical summer is defined as starting at the solstice, the time of maximal insolation, or on the traditional date of June 21. A variable seasonal lag means that the meteorological center of the season, which is based on average temperature patterns, occurs several weeks after the time of maximal insolation. The meteorological convention is to define summer as comprising the months of June, July, and August in the northern hemisphere and the months of December, January, and February in the southern hemisphere. Under meteorological definitions, all seasons are arbitrarily set to start at the beginning of a calendar month and end at the end of a month. This meteorological definition of summer also aligns with the commonly viewed notion of summer as the season with the longest (and warmest) days of the year, in which daylight predominates. The meteorological reckoning of seasons is used in Australia, Austria, Denmark, Russia and Japan. It is also used by many in the United Kingdom. In Ireland, the summer months according to the national meteorological service, Met Éireann, are June, July and August. However, according to the Irish Calendar, summer begins on 1 May and ends on 1 August. School textbooks in Ireland follow the cultural norm of summer commencing on 1 May rather than the meteorological definition of 1 June.

Days continue to lengthen from equinox to solstice and summer days progressively shorten after the solstice, so meteorological summer encompasses the build-up to the longest day and a diminishing thereafter, with summer having many more hours of daylight than spring. Reckoning by hours of daylight alone, summer solstice marks the midpoint, not the beginning, of the seasons. Midsummer takes place over the shortest night of the year, which is the summer solstice, or on a nearby date that varies with tradition.

Where a seasonal lag of half a season or more is common, reckoning based on astronomical markers is shifted half a season. By this method, in North America, summer is the period from the summer solstice (usually 20 or 21 June in the Northern Hemisphere) to the autumn equinox.

Reckoning by cultural festivals, the summer season in the United States is traditionally regarded as beginning on Memorial Day (the last Monday in May) and ending on Labor Day (the first Monday in September), more closely in line with the meteorological definition for the parts of the country that have four-season weather. The similar Canadian tradition starts summer on Victoria Day one week prior (although summer conditions vary widely across Canada's expansive territory) and ends, as in the United States, on Labour Day.

In Chinese astronomy, summer starts on or around 5 May, with the "jiéqì" (solar term) known as lìxià (立夏), i.e. "establishment of summer", and it ends on or around 6 August.

In southern and southeast Asia, where the monsoon occurs, summer is more generally defined as lasting from March, April, May and June, the warmest time of the year, ending with the onset of the monsoon rains.

Because the temperature lag is shorter in the oceanic temperate southern hemisphere, most countries in this region use the meteorological definition with summer starting on 1 December and ending on the last day of February.

Summer is traditionally associated with hot or warm weather. In the Mediterranean regions, it is also associated with dry weather, while in other places (particularly in Eastern Asia because of the Monsoon) it is associated with rainy weather. The wet season is the main period of vegetation growth within the savanna climate regime. Where the wet season is associated with a seasonal shift in the prevailing winds, it is known as a monsoon.

In the northern Atlantic Ocean, a distinct tropical cyclone season occurs from 1 June to 30 November. The statistical peak of the Atlantic hurricane season is 10 September. The Northeast Pacific Ocean has a broader period of activity, but in a similar time frame to the Atlantic. The Northwest Pacific sees tropical cyclones year-round, with a minimum in February and March and a peak in early September. In the North Indian basin, storms are most common from April to December, with peaks in May and November. In the Southern Hemisphere, the tropical cyclone season runs from 1 November until the end of April with peaks in mid-February to early March.

Thunderstorm season in the United States and Canada runs in the spring through summer. These storms can produce hail, strong winds and tornadoes, usually during the afternoon and evening.

Schools and universities typically have a summer break to take advantage of the warmer weather and longer days. In almost all countries, children are out of school during this time of year for summer break, although dates vary. In the United States, public schools usually end in early June while colleges finish in early May, although some schools get out on the last or second last Thursday in May. In England and Wales, school ends in mid-July and resumes again in early September; in Scotland, the summer holiday begins in late June and ends in mid- to late-August. Similarly, in Canada the summer holiday starts on the last or second-last Friday in June and ends in late August or on the first Monday of September, with the exception of when that date falls before Labour Day, in which case, ends on the second Monday of the month. In Russia the summer holiday begins at the end of May and ends on August 31. In the Southern Hemisphere, school summer holiday dates include the major holidays of Christmas and New Year's Day. School summer holidays in Australia, New Zealand and South Africa begin in early December and end in early February, with dates varying between states. In India, school ends in late April and resumes in early or mid June. In Cameroon and Nigeria, schools usually finish for summer vacation in mid-July, and resume in the later weeks of September or the first week of October.

A wide range of public holidays fall during summer, including:

People generally take advantage of the high temperatures by spending more time outdoors during summer. Activities such as travelling to the beach and picnics occur during the summer months. Sports such as association football, basketball, American football, volleyball, skateboarding, baseball, softball, cricket, tennis and golf are played. Water sports also occur. These include water skiing, wake boarding, swimming, surfing, tubing and water polo. The modern Olympics have been held during the summer months every four years since 1896. The 2000 Summer Olympics, in Sydney, however, were held during the Australian Spring.

Summer is normally a low point in television viewing, and television schedules generally reflect this by not scheduling new episodes of their most popular shows between the end of May sweeps and the beginning of the television season in September, instead scheduling low-cost reality television shows and burning off commitments to already-cancelled series. There is an exception to this with children's television. Many television shows made for children and are popular with children are released during the summer months, especially on children's cable channels such as the Disney Channel in the United States, as children are off school. Disney Channel, for example, ends its preschool programming earlier in the day for older school age children in the summer months while it reverts to the original scheduling as the new school year begins. Conversely, the music and film industries generally experience higher returns during the summer than other times of the year and market their summer hits accordingly. Summer is most popular for animated movies to be released theatrically in movie theaters.

With most school-age children and college students (except those attending summer school) on summer vacation during the summer months, especially in the United States, travel and vacationing traditionally peaks during the summer, with the volume of travel in a typical summer weekend rivaled only by Thanksgiving. Teenagers and college students often take summer jobs in industries that cater to recreation. Business activity for the recreation, tourism, restaurant, and retail industries peak during the summer months as well as the holiday season.



</doc>
<doc id="29393" url="https://en.wikipedia.org/wiki?curid=29393" title="Spring">
Spring

Spring(s) may refer to:




















</doc>
<doc id="29394" url="https://en.wikipedia.org/wiki?curid=29394" title="Shrike">
Shrike

Shrikes () are carnivorous passerine birds of the family Laniidae. The family is composed of thirty-one species in four genera. They are fairly closely related to the bush-shrike family Malaconotidae.

The family name, and that of the largest genus, "Lanius", is derived from the Latin word for "butcher", and some shrikes are also known as butcherbirds because of their feeding habits. The common English name "shrike" is from Old English , alluding to the shrike's shriek-like call.

Most shrike species have a Eurasian and African distribution, with just two breeding in North America (the loggerhead and great grey shrikes). There are no members of this family in South America or Australia, although one species reaches New Guinea. The shrikes vary in the extent of their ranges, with some species like the great grey shrike ranging across the Northern Hemisphere to the Newton's fiscal which is restricted to the island of São Tomé.

They inhabit open habitats, especially steppe and savannah. A few species of shrike are forest dwellers, seldom occurring in open habitats. Some species breed in northern latitudes during the summer, then migrate to warmer climes for the winter.

Shrikes are medium-sized birds, up to in length, with grey, brown, or black and white plumage. Their beaks are hooked, like that of a bird of prey, reflecting their predatory nature, and their calls are strident.

Shrikes are known for their habit of catching insects and small vertebrates and impaling their bodies on thorns, the spikes on barbed-wire fences, or any available sharp point. This helps them to tear the flesh into smaller, more conveniently-sized fragments, and serves as a cache so that the shrike can return to the uneaten portions at a later time. This same behaviour of impaling insects serves as an adaptation to eating the toxic lubber grasshopper, "Romalea microptera". The bird waits for 1–2 days for the toxins within the grasshopper to degrade, then they can eat it.

Shrikes are territorial, and these territories are defended from other pairs. In migratory species a breeding territory is defended in the breeding grounds and a smaller feeding territory is established during migration and in the wintering grounds. Where several species of shrike exist together, competition for territories can be intense.

Shrikes make regular use of exposed perch sites, where they adopt a conspicuous upright stance. These sites are used in order to watch for prey items and to advertise their presence to rivals.

The shrikes are generally monogamous breeders, although polygyny has been recorded in some species. Co-operative breeding, where younger birds help their parents raise the next generation of young, has been recorded in both species in the genera "Eurocephalus" and "Corvinella" as well as one species of "Lanius". Males attract females to their territory with well stocked caches, which may include inedible but brightly coloured items. During courtship the male will perform a ritualised dance which includes actions that mimic the skewering of prey on thorns and will feed the female. Shrikes make simple, cup-shaped nests from twigs and grasses, in bushes and the lower branches of trees.

The family Laniidae was introduced (as Lanidia) by the French polymath Constantine Samuel Rafinesque in 1815.

FAMILY: LANIIDAE 

Other species, popularly called shrikes, are in the families:

The Prionopidae and Malaconotidae are quite closely related to the Laniidae, and were formerly included in the shrike family. The cuckoo-shrikes are not closely related to the true shrikes.

The Australasian butcherbirds are not shrikes, although they occupy a similar ecological niche.



</doc>
<doc id="29396" url="https://en.wikipedia.org/wiki?curid=29396" title="Screwdriver (cocktail)">
Screwdriver (cocktail)

A screwdriver is a popular alcoholic highball drink made with orange juice and vodka. While the basic drink is simply the two ingredients, there are many variations; the most common one is made with one part vodka, one part of any kind of orange soda, and one part of orange juice. Many of the variations have different names in different parts of the world. The International Bartenders Association has designated this cocktail as an IBA Official Cocktail.

This drink appears in literature as early as 1938.

The screwdriver is mentioned in 1944: "A Screwdriver—a drink compounded of vodka and orange juice and supposedly invented by interned American fliers"; and in 1949: "the latest Yankee concoction of vodka and orange juice, called a 'screwdriver'".

A screwdriver with two parts of Sloe gin, and filled with orange juice is a "Slow (Sloe) Screw".

A screwdriver with two parts of Sloe gin, one part of Southern Comfort and filled with orange juice is a "Slow Comfortable Screw".

A screwdriver with one part of Sloe gin, one part of Southern Comfort, one part Galliano and filled with orange juice is a "Slow Comfortable Screw Up Against The Wall".

A screwdriver with one part of Sloe gin, one part of Southern Comfort, one part Galliano, one part tequila and filled with orange juice is a "Slow Comfortable Screw Up Against The Wall Mexican Style".

A screwdriver with one part of Sloe gin, one part of Southern Comfort, one part Galliano, one part peach schnapps, and filled with orange juice is a "Slow Comfortable Screw Up Against a Fuzzy Wall".

A screwdriver with one part of Sloe gin, one part of Southern Comfort, one part Galliano, one part peach schnapps, one part sparkling rosé, and filled with orange juice is a "Slow Comfortable Screw Up Against a Fuzzy Pink Wall".

A screwdriver with two parts vodka, four parts orange juice, and one part Galliano is a Harvey Wallbanger.

A screwdriver with equal parts vanilla vodka and Blue Curaçao topped with lemon-lime soda is a "Sonic Screwdriver".

A shot of vodka with a slice of orange is a Cordless Screwdriver.



</doc>
<doc id="29398" url="https://en.wikipedia.org/wiki?curid=29398" title="Single-stage-to-orbit">
Single-stage-to-orbit

A single-stage-to-orbit (or SSTO) vehicle reaches orbit from the surface of a body without jettisoning hardware, expending only propellants and fluids. The term usually, but not exclusively, refers to reusable vehicles. No Earth-launched SSTO launch vehicles have ever been constructed. To date, orbital launches have been performed either by multi-stage fully or partially expendable rockets, the Space Shuttle having both attributes.

Launch costs for low Earth orbit (LEO) range from $10,000 to $19,000 per kg of payload ($4,500–$8,500 / pound). Reusable SSTO vehicles offer the promise of reduced launch expenses by eliminating recurring costs associated with hardware replacement inherent in expendable launch systems. However, the nonrecurring costs associated with design, development, research and engineering (DDR&E) of reusable SSTO systems are much higher than expendable systems due to the substantial technical challenges of SSTO.

It is considered to be marginally possible to launch a single-stage-to-orbit chemically-fueled spacecraft from Earth. The principal complicating factors for SSTO from Earth are: high orbital velocity of over ; the need to overcome Earth's gravity, especially in the early stages of flight; and flight within Earth's atmosphere, which limits speed in the early stages of flight and influences engine performance.

Notable single stage to orbit research spacecraft include Skylon, the DC-X, the Lockheed Martin X-33, and the Roton SSTO. However, despite showing some promise, none of them has come close to achieving orbit yet due to problems with finding the most efficient propulsion system.

Single-stage-to-orbit is much easier to achieve on extraterrestrial bodies which have weaker gravitational fields and lower atmospheric pressure than Earth, such as the Moon and Mars, and has been achieved from the Moon by both the Apollo program's Lunar Module and several robotic spacecraft of the Soviet Luna program.

For a long time before the second half of the twentieth century, the concept of a single stage to orbit vehicle was rarely considered, and was generally considered to be impractical; therefore very little research was conducted into the concept. However, advancements in flight technology led to the idea becoming more plausible, and during the 1960s some of the first concept designs for this kind of craft began to emerge.

Phil Bono (Douglas Space), Daniel Koelle of MBB and Edward Gomersall(NASA Ames)were among the earlier proponents of SSTO designs.
One of the earliest was the One stage Orbital Space Truck (OOST) designed by Philip Bono, an engineer for Douglas Aircraft Company, which was a concept for an expendable booster stage which could deliver a payload to orbit in one stage. A reusable version named ROOST was also proposed. Another early SSTO design was a reusable launch vehicle named NEXUS which was designed by Krafft Arnold Ehricke in the early 1960s. It was one of the largest space craft ever conceptualized with a diameter of over fifty metres and the capability to lift up to two thousand short tons into Earth orbit, intended for missions to further out locations in the solar system such as Mars. The North American Air Augmented VTOVL from 1963 was a similarly large craft which would have used external burning ramjets to decrease the liftoff mass of the vehicle by removing the need for large amounts of liquid oxygen while travelling through the atmosphere.

From 1965, Robert Salked investigated various single stage to orbit spaceplane concepts, which would include wings. He proposed a vehicle which would burn hydrocarbon fuel while in the atmosphere and then switch to hydrogen fuel for increasing efficiency once in space. This was around the same time as the development of the space shuttle.

Further examples of Bono's early concepts (prior to the 1990s) which were never constructed include:

Around 1985 the NASP project was intended to create a scramjet vehicle to reach orbit, but this had its funding stopped and was cancelled. At around the same time, the HOTOL tried to use precooled jet engine technology, but failed to show significant advantages over rocket technology.

The DC-X, short for Delta Clipper Experimental, was an unmanned one third scale SSTO vehicle which was too small to actually achieve orbit but was instead built to demonstrate vertical takeoff and landing. It is one of only a few prototype SSTO vehicles ever built. Several other prototypes of this design were proposed, including the DC-X2 (a half-scale prototype) and the DC-Y, a full scale vehicle which would be capable of single stage insertion into orbit. Neither of these were built, but the project was taken over by NASA in 1995, and they built the DC-XA, an upgraded one third scale prototype. This vehicle was lost when it landed with only three of its four landing pads deployed, which caused it to tip over on its side and explode. The project has not been continued since.

Despite this failure, several future proposals have been based on the DC-X design.

From 1999 to 2001 Rotary Rocket attempted to build a SSTO vehicle called the Roton. It received a large amount of media attention and a working sub-scale prototype was completed, but the design was largely impractical.

There have been various approaches to SSTO, including pure rockets that are launched and land vertically, air-breathing scramjet-powered vehicles that are launched and land horizontally, nuclear-powered vehicles, and even jet-engine-powered vehicles that can fly into orbit and return landing like an airliner, completely intact.

For rocket-powered SSTO, the main challenge is achieving a high enough mass-ratio to carry sufficient propellant to achieve orbit, plus a meaningful payload weight. One possibility is to give the rocket an initial speed with a space gun, as planned in the Quicklaunch project.

For air-breathing SSTO, the main challenge is system complexity and associated research and development costs, material science, and construction techniques necessary for surviving sustained high-speed flight within the atmosphere, "and" achieving a high enough mass-ratio to carry sufficient propellant to achieve orbit, plus a meaningful payload weight. Air-breathing designs typically fly at supersonic or hypersonic speeds, and usually include a rocket engine for the final burn for orbit.

Whether rocket-powered or air-breathing, a reusable vehicle must be rugged enough to survive multiple round trips into space without adding excessive weight or maintenance. In addition a reusable vehicle must be able to reenter without damage, and land safely.

While single-stage rockets were once thought to be beyond reach, advances in materials technology and construction techniques have shown them to be possible. For example, calculations show that the Titan II first stage, launched on its own, would have a 25-to-1 ratio of fuel to vehicle hardware.
It has a sufficiently efficient engine to achieve orbit, but without carrying much payload.

The design space constraints of SSTO vehicles were described by rocket design engineer Robert Truax:

The Tsiolkovsky rocket equation expresses the maximum change in velocity any single rocket stage can achieve:

where:

The mass ratio of a vehicle is defined as a ratio the initial vehicle mass when fully loaded with propellants formula_7 to the final vehicle mass formula_8 after the burn:

where:

The propellant mass fraction (formula_16) of a vehicle can be expressed solely as a function of the mass ratio:

The structural coefficient (formula_18) is a critical parameter in SSTO vehicle design. Structural efficiency of a vehicle is maximized as the structural coefficient approaches zero. The structural coefficient is defined as:

The overall structural mass fraction formula_20 can be expressed in terms of the structural coefficient:

An additional expression for the overall structural mass fraction can be found by noting that the payload mass fraction formula_22, propellant mass fraction and structural mass fraction sum to one:

Equating the expressions for structural mass fraction and solving for the initial vehicle mass yields:

This expression shows how the size of a SSTO vehicle is dependent on its structural efficiency. Given a mission profile formula_26 and propellant type formula_27,the size of a vehicle increases with an increasing structural coefficient. This growth factor sensitivity is shown parametrically for both SSTO and two-stage-to-orbit (TSTO) vehicles for a standard LEO mission. The curves vertically asymptote at the maximum structural coefficient limit where mission criteria can no longer be met:

In comparison to a non-optimized TSTO vehicle using restricted staging, a SSTO rocket launching an identical payload mass and using the same propellants will always require a substantially smaller structural coefficient to achieve the same delta-v. Given that current materials technology places a lower limit of approximately 0.1 on the smallest structural coefficients attainable, reusable SSTO vehicles are typically an impractical choice even when using the highest performance propellants available.

Hydrogen might seem the obvious fuel for SSTO vehicles. When burned with oxygen, hydrogen gives the highest specific impulse of any commonly used fuel: around 450 seconds, compared with up to 350 seconds for kerosene.

Hydrogen has the following advantages:


However, hydrogen also has these disadvantages:


These issues can be dealt with, but at extra cost.

While kerosene tanks can be 1% of the weight of their contents, hydrogen tanks often must weigh 10% of their contents. This is because of both the low density and the additional insulation required to minimize boiloff (a problem which does not occur with kerosene and many other fuels). The low density of hydrogen further affects the design of the rest of the vehicle — pumps and pipework need to be much larger in order to pump the fuel to the engine. The end result is the thrust/weight ratio of hydrogen-fueled engines is 30–50% lower than comparable engines using denser fuels.

This inefficiency indirectly affects gravity losses as well; the vehicle has to hold itself up on rocket power until it reaches orbit. The lower excess thrust of the hydrogen engines due to the lower thrust/weight ratio means that the vehicle must ascend more steeply, and so less thrust acts horizontally. Less horizontal thrust results in taking longer to reach orbit, and gravity losses are increased by at least . While not appearing large, the mass ratio to delta-v curve is very steep to reach orbit in a single stage, and this makes a 10% difference to the mass ratio on top of the tankage and pump savings.

The overall effect is that there is surprisingly little difference in overall performance between SSTOs that use hydrogen and those that use denser fuels, except that hydrogen vehicles may be rather more expensive to develop and buy. Careful studies have shown that some dense fuels (for example liquid propane) exceed the performance of hydrogen fuel when used in an SSTO launch vehicle by 10% for the same dry weight.

In the 1960s Philip Bono investigated single-stage, VTVL tripropellant rockets, and showed that it could improve payload size by around 30%.

Operational experience with the DC-X experimental rocket has caused a number of SSTO advocates to reconsider hydrogen as a satisfactory fuel. The late Max Hunter, while employing hydrogen fuel in the DC-X, often said that he thought the first successful orbital SSTO would more likely be fueled by propane.

Some SSTO vehicles use the same engine for all altitudes, which is a problem for traditional engines with a bell-shaped nozzle. Depending on the atmospheric pressure, different bell shapes are optimal. Engines operating in the lower atmosphere have shorter bells than those designed to work in vacuum. Having a bell that is only optimal at a single altitude lowers the overall engine efficiency.

One possible solution would be to use an aerospike engine, which can be effective in a wide range of ambient pressures. In fact, a linear aerospike engine was used in the X-33 design.

Other solutions involve using multiple engines and other altitude adapting designs such as double-mu bells or extensible bell sections.

Still, at very high altitudes, the extremely large engine bells tend to expand the exhaust gases down to near vacuum pressures. As a result, these engine bells are counterproductive due to their excess weight. Some SSTO vehicles simply use very high pressure engines which permit high ratios to be used from ground level. This gives good performance, negating the need for more complex solutions.

Some designs for SSTO attempt to use airbreathing jet engines that collect oxidizer and reaction mass from the atmosphere to reduce the take-off weight of the vehicle.

Some of the issues with this approach are:


Thus with for example scramjet designs (e.g. X-43) the mass budgets do not seem to close for orbital launch.

Similar issues occur with single-stage vehicles attempting to carry conventional jet engines to orbit—the weight of the jet engines is not compensated sufficiently by the reduction in propellant.

On the other hand, LACE-like precooled airbreathing designs such as the Skylon spaceplane (and ATREX) which transition to rocket thrust at rather lower speeds (Mach 5.5) do seem to give, on paper at least, an improved orbital mass fraction over pure rockets (even multistage rockets) sufficiently to hold out the possibility of full reusability with better payload fraction.

It is important to note that mass fraction is an important concept in the engineering of a rocket. However, mass fraction may have little to do with the costs of a rocket, as the costs of fuel are very small when compared to the costs of the engineering program as a whole. As a result, a cheap rocket with a poor mass fraction may be able to deliver more payload to orbit with a given amount of money than a more complicated, more efficient rocket.

Many vehicles are only narrowly suborbital, so practically anything that gives a relatively small delta-v increase can be helpful, and outside assistance for a vehicle is therefore desirable.

Proposed launch assists include:

And on-orbit resources such as:

Due to weight issues such as shielding, many nuclear propulsion systems are unable to lift their own weight, and hence are unsuitable for launching to orbit. However some designs such as the Orion project and some nuclear thermal designs do have a thrust to weight ratio in excess of 1, enabling them to lift off. Clearly one of the main issues with nuclear propulsion would be safety, both during a launch for the passengers, but also in case of a failure during launch. No current program is attempting nuclear propulsion from Earth's surface.

Because they can be more energetic than the potential energy that chemical fuel allows for, some laser or microwave powered rocket concepts have the potential to launch vehicles into orbit, single stage. In practice, this area is relatively undeveloped, and current technology falls far short of this.

The high cost per launch of the Space Shuttle sparked interest throughout the 1980s in designing a cheaper successor vehicle. Several official design studies were done, but most were basically smaller versions of the existing Shuttle concept.

Most cost analysis studies of the Space Shuttle have shown that workforce is by far the single greatest expense. Early shuttle discussions speculated airliner-type operation, with a two-week turnaround. However, senior NASA planners envisioned no more than 10 to 12 flights per year for the entire shuttle fleet. The absolute maximum flights per year for the entire fleet was limited by external tank manufacturing capacity to 24 per year.

Very efficient (hence complex and sophisticated) main engines were required to fit within the available vehicle space. Likewise the only known suitable lightweight thermal protection was delicate, maintenance-intensive silica tiles. These and other design decisions resulted in a vehicle that requires great maintenance after every mission. The engines are removed and inspected, and prior to the new "block II" main engines, the turbopumps were removed, disassembled and rebuilt. While Space Shuttle Atlantis was refurbished and relaunched in 53 days between missions STS-51-J and STS-61-B, generally months were required to repair an orbiter for a new mission.

It is easier to achieve SSTO from a body with lower gravitational pull than Earth, such as the Moon or Mars. The Apollo Lunar Module ascended from the lunar surface to lunar orbit in a single stage.

A detailed study into SSTO vehicles was prepared by Chrysler Corporation's Space Division in 1970–1971 under NASA contract NAS8-26341. Their proposal (Shuttle SERV) was an enormous vehicle with more than of payload, utilizing jet engines for (vertical) landing. While the technical problems seemed to be solvable, the USAF required a winged design that led to the Shuttle as we know it today.

The unmanned DC-X technology demonstrator, originally developed by McDonnell Douglas for the Strategic Defense Initiative (SDI) program office, was an attempt to build a vehicle that could lead to an SSTO vehicle. The one-third-size test craft was operated and maintained by a small team of three people based out of a trailer, and the craft was once relaunched less than 24 hours after landing. Although the test program was not without mishap (including a minor explosion), the DC-X demonstrated that the maintenance aspects of the concept were sound. That project was cancelled when it landed with three of four legs deployed, tipped over, and exploded on the fourth flight after transferring management from the Strategic Defense Initiative Organization to NASA.

The Aquarius Launch Vehicle was designed to bring bulk materials to orbit as cheaply as possible.

Current and previous SSTO projects include the Japanese Kankoh-maru project, the Skylon, ARCA Haas 2C, and the Indian Avatar spaceplane.

The British Government partnered with the ESA in 2010 to promote a single-stage to orbit spaceplane concept called Skylon. This design was pioneered by Reaction Engines Limited (REL), a company founded by Alan Bond after HOTOL was canceled. The Skylon spaceplane has been positively received by the British government, and the British Interplanetary Society. Following a successful propulsion system test that was audited by ESA's propulsion division in mid-2012, REL announced that it would begin a three-and-a-half-year project to develop and build a test jig of the Sabre engine to prove the engines performance across its air-breathing and rocket modes. In November 2012, it was announced that a key test of the engine precooler had been successfully completed, and that ESA had verified the precooler's design. The project's development is now allowed to advance to its next phase, which involves the construction and testing of a full-scale prototype engine.

Many studies have shown that regardless of selected technology, the most effective cost reduction technique is economies of scale. Merely launching a large total number reduces the manufacturing costs per vehicle, similar to how the mass production of automobiles brought about great increases in affordability.

Using this concept, some aerospace analysts believe the way to lower launch costs is the exact opposite of SSTO. Whereas reusable SSTOs would reduce per launch costs by making a reusable high-tech vehicle that launches frequently with low maintenance, the "mass production" approach views the technical advances as a source of the cost problem in the first place. By simply building and launching large quantities of rockets, and hence launching a large volume of payload, costs can be brought down. This approach was attempted in the late 1970s, early 1980s in West Germany with the Democratic Republic of the Congo-based OTRAG rocket.

A related idea is to obtain economies of scale from building simple, massive, multi-stage rockets using cheap, off-the-shelf parts. The vehicles would be dumped into the ocean after use. This strategy is known as the "big dumb booster" approach.

This is somewhat similar to the approach some previous systems have taken, using simple engine systems with "low-tech" fuels, as the Russian and Chinese space programs still do. These nations' launches are significantly cheaper than their Western counterparts.

An alternative to scale is to make the discarded stages practically reusable: this is the goal of the SpaceX reusable launch system development program and its Falcon 9, Falcon Heavy and BFR vehicles. A similar approach is being pursued by Blue Origin.



</doc>
<doc id="29400" url="https://en.wikipedia.org/wiki?curid=29400" title="Structural biology">
Structural biology

Structural biology is a branch of molecular biology, biochemistry, and biophysics concerned with the molecular structure of biological macromolecules (especially proteins, made up of amino acids, and RNA or DNA, made up of nucleic acids), how they acquire the structures they have, and how alterations in their structures affect their function. This subject is of great interest to biologists because macromolecules carry out most of the functions of cells, and it is only by coiling into specific three-dimensional shapes that they are able to perform these functions. This architecture, the "tertiary structure" of molecules, depends in a complicated way on each molecule's basic composition, or "primary structure." 

Biomolecules are too small to see in detail even with the most advanced light microscopes. The methods that structural biologists use to determine their structures generally involve measurements on vast numbers of identical molecules at the same time. These methods include:
Most often researchers use them to study the "native states" of macromolecules. But variations on these methods are also used to watch nascent or denatured molecules assume or reassume their native states. See protein folding. 

A third approach that structural biologists take to understanding structure is bioinformatics to look for patterns among the diverse sequences that give rise to particular shapes. Researchers often can deduce aspects of the structure of integral membrane proteins based on the membrane topology predicted by hydrophobicity analysis. See protein structure prediction. 
In the past few years it has become possible for highly accurate physical molecular models to complement the "in silico" study of biological structures. Examples of these models can be found in the Protein Data Bank.




</doc>
<doc id="29402" url="https://en.wikipedia.org/wiki?curid=29402" title="Sunni Islam">
Sunni Islam

Sunni Islam () is the largest denomination of Islam. Its name comes from the word Sunnah, referring to the exemplary behaviour of the Islamic prophet Muhammad. The differences between Sunni and Shia Muslims arose from a disagreement over the choice of Muhammad's successor and subsequently acquired broader political significance, as well as theological and juridical dimensions.

According to Sunni traditions, Muhammad did not clearly designate a successor and the Muslim community acted according to his sunnah in electing his father-in-law Abu Bakr as the first caliph. This contrasts with the Shi'a view, which holds that Muhammad announced at the event of Ghadir Khumm his son-in-law and cousin Ali ibn Abi Talib as his successor. Unlike the first three caliphs, Ali was from the same clan as Muhammad, Banu Hashim, and Shia Muslims consider him legitimate, inter alia, by favour of his blood ties to Muhammad, too. Political tensions between Sunnis and Shias continued with varying intensity throughout Islamic history and they have been exacerbated in recent times by ethnic conflicts and the rise of Wahhabism.

, Sunni Muslims constituted 87–90% of the world's Muslim population. Sunni Islam is the world's largest religious denomination, followed by Catholicism. Its adherents are referred to in Arabic as ' ("the people of the sunnah and the community") or ' for short. In English, its doctrines and practices are sometimes called "Sunnism", while adherents are known as Sunni Muslims, Sunnis, Sunnites and Ahlus Sunnah. Sunni Islam is sometimes referred to as "orthodox Islam". However, other scholars of Islam, such as John Burton believe that there's no such thing as "orthodox Islam".

The Quran, together with hadith (especially those collected in Kutub al-Sittah) and binding juristic consensus form the basis of all traditional jurisprudence within Sunni Islam. Sharia rulings are derived from these basic sources, in conjunction with analogical reasoning, consideration of public welfare and juristic discretion, using the principles of jurisprudence developed by the traditional legal schools.

In matters of creed, the Sunni tradition upholds the six pillars of "iman" (faith) and comprises the Ash'ari and Maturidi schools of rationalistic theology as well as the textualist school known as traditionalist theology.

 (Classical Arabic: ), also commonly referred to as , is a term derived from "" ( , plural ' ) meaning "habit", "usual practice", "custom", "tradition". The Muslim use of this term refers to the sayings and living habits of the prophet Muhammad. In Arabic, this branch of Islam is referred to as ' (), "the people of the sunnah and the community", which is commonly shortened to "" (Arabic أهل السنة).

One common mistake is to assume that Sunni Islam represents a normative Islam that emerged during the period after Muhammad's death, and that Sufism and Shi'ism developed out of Sunni Islam. This perception is partly due to the reliance on highly ideological sources that have been accepted as reliable historical works, and also because the vast majority of the population is Sunni. Both Sunnism and Shiaism are the end products of several centuries of competition between ideologies. Both sects used each other to further cement their own identities and doctrines.

The first four caliphs are known among Sunnis as the Rashidun or "Rightly-Guided Ones". Sunni recognition includes the aforementioned Abu Bakr as the first, Umar as the second, Uthman as the third, and Ali as the fourth.

Sunnis recognised different rulers as the caliph, though they did not include anyone in the list of the rightly guided ones or "Rashidun" after the murder of Ali, until the caliphate was constitutionally abolished in Turkey on 3 March 1924.

The seeds of metamorphosis of caliphate into kingship were sown, as the second caliph Umar had feared, as early as the regime of the third caliph Uthman, who appointed many of his kinsmen from his clan Banu Umayya, including Marwan and Walid bin Uqba on important government positions, becoming the main cause of turmoil resulting in his murder and the ensuing infighting during Ali's time and rebellion by Muawiya, another of Uthman's kinsman. This ultimately resulted in the establishment of firm dynastic rule of Banu Umayya after Husain, the younger son of Ali from Fatima, was killed at the battle of karbala. The rise to power of Banu Umayya, the Meccan tribe of elites who had vehemently opposed Muhammad under the leadership of Abu Sufyan, Muawiya's father, right up to the conquest of Mecca by Muhammad, as his successors with the accession of Uthman to caliphate, replaced the egalitarian society formed as a result of Muhammad's revolution to a society stratified between haves and have-nots as a result of nepotism, and in the words of El-Hibri through "the use of religious charity revenues ("zakat") to subsidise family interests, which Uthman justified as ""al-sila"" (pious filial support)." Ali, during his rather brief regime after Uthman maintained austere life style and tried hard to bring back the egalitarian system and supremacy of law over the ruler idealised in Muhammad's message, but faced continued opposition, and wars one after another by Aisha-Talhah-Zubair, by Muawiya and finally by the Kharjites. After he was murdered his followers immediately elected Hasan ibn Ali his elder son from Fatima to succeed him. Hasan, however, shortly afterwards signed a treaty with Muawiaya relinquishing power in favour of the latter, with a condition inter alia, that one of the two who will outlive the other will be the caliph, and that this caliph will not appoint a successor but will leave the matter of selection of the caliph to the public. Subsequently, Hasan was poisoned to death and Muawiya enjoyed unchallenged power. Not honouring his treaty with Hasan he however nominated his son Yazid to succeed him. Upon Muawiya's death, Yazid asked Husain the younger brother of Hasan, Ali's son and Muhammad's grandson, to give his allegiance to Yazid, which he plainly refused. His caravan was cordoned by Yazid's army at Karbala and he was killed with all his male companions – total 72 people, in a day long battle after which Yazid established himself as a sovereign, though strong public uprising erupted after his death against his dynasty to avenge the massacre of Karbala, but Banu Umayya were able to quickly suppress them all and ruled the Muslim world, till they were finally overthrown by Banu Abbas.

The rule of and "caliphate" of Banu Umayya came to an end at the hands of Banu Abbas a branch of Banu Hashim, the tribe of Muhammad, only to usher another dynastic monarchy styled as caliphate from 750 CE. This period is seen formative in Sunni Islam as the founders of the four schools viz, Abu Hanifa, Malik bin Anas, Shafi'i and Ahmad bin Hambal all practised during this time, so also did Jafar al Sadiq who elaborated the doctrine of imamate, the basis for the Shi'a religious thought. There was no clearly accepted formula for determining succession in the Abbasid caliphate. Two or three sons or other relatives of the dying caliph emerged as candidates to the throne, each supported by his own party of supporters. A trial of strength ensued and the most powerful party won and expected favours of the caliph they supported once he ascended the throne. The caliphate of this dynasty ended with the death of the Caliph al-Ma'mun in 833 CE, when the period of Turkish domination began.

The fall of the Ottoman, the biggest Sunni empire in the world for six centuries, the mightiest power in the Mediterranean world and one of the important participants in World War I which joined the war on the side of the Central Powers, bringing caliphate to an end was an epochal event. This resulted in Sunni protests in far off places including the Khilafat Movement in India, which was later on upon gaining independence from Britain divided into Sunni dominated Pakistan and secular India. Pakistan, the most populous Sunni state at its birth, however later got partitioned into Pakistan and Bangladesh. The demise of Ottoman caliphate also resulted in the emergence of Saudi Arabia, a dynastic absolute monarchy with the support of the British and Muhammad ibn Abd al-Wahhab, the founder of Wahhabism. This was followed by a considerable rise in Wahhabism, Salafism and Jihadism under the influence of the preaching of Ibn Taymiyyah a follower of Ahmad bin Hanbal.The expediencies of cold war resulted in encouragement of Afghan refugees in Pakistan to be radicalised, trained and armed to fight the communist regime backed by USSR forces in Afghanistan giving birth to Taliban. The Taliban wrestled power from the communists in Afghanistan and formed a government under the leadership of Mohammed Omar, who was addressed as the Emir of the faithful, an honorific way of addressing the caliph. The Taliban regime was recognised by Pakistan and Saudi Arabia till after 9/11 perpetrated by Osama bin Laden a Saudi national by birth harboured by the Taliban took place, resulting in a war on terror launched against the Taliban.

The sequence of events of the 20th century has led to resentment in some quarters of the Sunni community due to the loss of pre-eminence in several previously Sunni-dominated regions such as the Levant, Mesopotamia, the Balkans, the Caucasus and the Indian sub continent. The latest attempt by a section of Salafis to re establish a Sunni caliphate can be seen in the appearance of ISIS whose leader Abu Bakr al Baghdadi is known among his followers as caliph and "Amir-al-maumineen" "The Commander of the Faithful". Jihadism is however being strongly opposed from within the Muslim community, "Ummah" as it is called in Arabic in all quarters of the world.

Sunnis believe that the companions of Muhammad were the best of Muslims. This belief is based upon prophetic traditions such as one narrated by Abdullah, son of Masud, in which Muhammad said: "The best of the people are my generation, then those who come after them, then those who come after them." Support for this view is also found in the Quran, according to Sunnis. Sunnis also believe that the companions were true believers since it was the companions who were given the task of compiling the Quran. Furthermore, narrations that were narrated by the companions (ahadith) are considered by Sunnis to be a second source of knowledge of the Muslim faith. A study conducted by the "Pew Research Center" in 2010 and released January 2011 found that there are 1.62 billion Muslims around the world, and it is estimated over 85–90% are Sunni.

Sunni Islam does not have a formal hierarchy. Leaders are informal, and gain influence through study to become a scholar of Islamic law, called "sharia".
According to the Islamic Center of Columbia, South Carolina, anyone with the intelligence and the will can become an Islamic scholar. During Midday Mosque services on Fridays, the congregation will choose a well-educated person to lead the service, known as a Khateeb (one who speaks).

There are many intellectual traditions within the field of Islamic law, often referred to as legal schools. These varied traditions reflect differing viewpoints on some laws and obligations within Islamic law. While one school may see a certain act as a religious obligation, another may see the same act as optional. These schools aren't regarded as sects; rather, they represent differing viewpoints on issues that are not considered the core of Islamic belief. Historians have differed regarding the exact delineation of the schools based on the underlying principles they follow.

Many traditional scholars saw Sunni Islam in two groups: Ahl al-Ra'i, or "people of reason," due to their emphasis on scholarly judgment and discourse; and Ahl al-Hadith, or "people of traditions," due to their emphasis on restricting juristic thought to only what is found in scripture. Ibn Khaldun defined the Sunni schools as three: the Hanafi school representing reason, the Ẓāhirīte school representing tradition, and a broader, middle school encompassing the Shafi'ite, Malikite and Hanbalite schools.

During the Middle Ages, the Mamluk Sultanate in Egypt delineated the acceptable Sunni schools as only Hanafi, Maliki, Shafi'i and Hanbali, excluding the Ẓāhirī school. The Ottoman Empire later reaffirmed the official status of four schools as a reaction to the Shiite character of their ideological and political archrival, the Persian Safavids, though former Prime Minister of Sudan Al-Sadiq al-Mahdi, as well as the Amman Message issued by King Abdullah II of Jordan, recognize the Ẓāhirī and keep the number of Sunni schools at five.

Interpreting Islamic law by deriving specific rulings – such as how to pray – is commonly known as Islamic jurisprudence. The schools of law all have their own particular tradition of interpreting this jurisprudence. As these schools represent clearly spelled out methodologies for interpreting Islamic law, there has been little change in the methodology with regard to each school. While conflict between the schools was often violent in the past, today the schools recognize one another as viable legal methods rather than sources of error or heresy in contrast to one another. Each school has its evidences, and differences of opinion are generally respected. Conflict between the schools was often violent in the past.

All the branches of Sunni Islam testify to six principal articles of faith known as the six pillars of "iman" (Arabic for "faith"), which are believed to be essential. These are 


These six articles are what "all" present-day Sunnis agree on, from those who adhere to traditional Sunnism to those who adhere to latter-day movements. Additionally, classical Sunni Islam also outlined numerous other cardinal doctrines from the eighth-century onwards in the form of organized creeds such as the Creed of Tahawi, in order to codify what constituted "Sunni orthodoxy." While none of these creeds gained the importance attributed to the Nicene Creed in Christianity, primarily because ecumenical councils never happened in Islam, the beliefs outlined in these creeds became the "orthodox" doctrine by ijma, or binding consensus. But while most of the tenets outlined in the classical creeds are accepted by all Sunnis, some of these doctrines have been rejected by the aforementioned movements as lacking strictly scriptural precedent. Traditionally, these other important Sunni articles of faith have included the following (those that are controversial today because of their rejection by such groups shall be denoted by an asterisk):


Some Islamic scholars faced questions that they felt were not explicitly answered in the "Quran" and the "Sunnah", especially questions with regard to philosophical conundra such as the nature of God, the existence of human free will, or the eternal existence of the "Quran." Various schools of theology and philosophy developed to answer these questions, each claiming to be true to the "Quran" and the Muslim tradition ("sunnah"). Among Sunni Muslims, various schools of thought in theology began to be born out of the sciences of kalam in opposition to the textualists who stood by affirming texts without delving into philosophical speculation as they saw it as an innovation in Islam. The following were the three dominant schools of theology that grew. All three of these are accepted by Muslims around the globe, and are considered within "Islamic orthodoxy". The key beliefs of classical Sunni Islam are all agreed upon (being the six pillars of Iman) and codified in the treatise on Aqeedah by Imam Ahmad ibn Muhammad al-Tahawi in his Aqeedat Tahawiyyah.

Founded by Abu al-Hasan al-Ash'ari (873–935). This theological school of Aqeedah was embraced by many Muslim scholars and developed in parts of the Islamic world throughout history; al-Ghazali wrote on the creed discussing it and agreeing upon some of its principles.
Ash'ari theology stresses divine revelation over human reason. Contrary to the Mu'tazilites, they say that ethics cannot be derived from human reason, but that God's commands, as revealed in the "Quran" and the "Sunnah" (the practices of Muhammad and his companions as recorded in the traditions, or hadith), are the sole source of all morality and ethics.

Regarding the nature of God and the divine attributes, the Ash'ari rejected the Mu'tazili position that all Quranic references to God as having real attributes were metaphorical. The Ash'aris insisted that these attributes were as they "best befit His Majesty". The Arabic language is a wide language in which one word can have 15 different meanings, so the Ash'aris endeavor to find the meaning that best befits God and is not contradicted by the Quran. Therefore, when God states in the Quran, "He who does not resemble any of His creation," this clearly means that God cannot be attributed with body parts because He created body parts. Ash'aris tend to stress divine omnipotence over human free will and they believe that the Quran is eternal and uncreated.

Founded by Abu Mansur al-Maturidi (died 944). Maturidiyyah was a minority tradition until it was accepted by the Turkish tribes of Central Asia (previously they had been Ash'ari and followers of the Shafi'i school, it was only later on migration into Anatolia that they became Hanafi and followers of the Maturidi creed.) One of the tribes, the Seljuk Turks, migrated to Turkey, where later the Ottoman Empire was established. Their preferred school of law achieved a new prominence throughout their whole empire although it continued to be followed almost exclusively by followers of the Hanafi school while followers of the Shafi and Maliki schools within the empire followed the Ash'ari and Athari schools of thought. Thus, wherever can be found Hanafi followers, there can be found the Maturidi creed.

Traditionalist theology is a movement of Islamic scholars who reject rationalistic Islamic theology (kalam) in favor of strict textualism in interpreting the Quran and sunnah. The name derives from "tradition" in its technical sense as translation of the Arabic word "hadith". It is also sometimes referred to by several other names.

Adherents of traditionalist theology believe that the "zahir" (literal, apparent) meaning of the Qur'an and the hadith have sole authority in matters of belief and law; and that the use of rational disputation is forbidden even if it verifies the truth. They engage in a literal reading of the Qur'an, as opposed to one engaged in "ta'wil" (metaphorical interpretation). They do not attempt to conceptualize the meanings of the Qur'an rationally, and believe that their realities should be consigned to God alone ("tafwid"). In essence, the text of the Qur'an and Hadith is accepted without asking "how" or "Bi-la kaifa".

Traditionalist theology emerged among scholars of hadith who eventually coalesced into a movement called "ahl al-hadith" under the leadership of Ahmad ibn Hanbal. In matters of faith, they were pitted against Mu'tazilites and other theological currents, condemning many points of their doctrine as well as the rationalistic methods they used in defending them. In the tenth century al-Ash'ari and al-Maturidi found a middle ground between Mu'tazilite rationalism and Hanbalite literalism, using the rationalistic methods championed by Mu'tazilites to defend most tenets of the traditionalist doctrine. Although the mainly Hanbali scholars who rejected this synthesis were in the minority, their emotive, narrative-based approach to faith remained influential among the urban masses in some areas, particularly in Abbasid Baghdad.

While Ash'arism and Maturidism are often called the Sunni "orthodoxy", traditionalist theology has thrived alongside it, laying rival claims to be the orthodox Sunni faith. In the modern era it has had a disproportionate impact on Islamic theology, having been appropriated by Wahhabi and other traditionalist Salafi currents and spread well beyond the confines of the Hanbali school of law.

There has also been a rich tradition of mysticism within Sunni Islam, which has most prominently manifested itself in the principal orders of Sunni Sufism. Historically, Sufism became "an incredibly important part of Islam" and "one of the most widespread and omnipresent aspects of Muslim life" in Islamic civilization from the early medieval period onwards, when it began to permeate nearly all major aspects of Sunni Islamic life in regions stretching from India and Iraq to Senegal. Sufism continued to remain a crucial part of daily Islamic life until the twentieth century, when its historical influence upon Islamic civilization began to be combated by the rise of Salafism and Wahhabism. Islamic scholar Timothy Winter has remarked: "[In] classical, mainstream, medieval Sunni Islam ... [the idea of] 'orthodox Islam' would not ... [have been possible] without Sufism," and that the classical belief in Sufism being an essential component of Islam has only weakened in some quarters of the Islamic world "a generation or two ago" with the rise of Salafism. In the modern world, the classical interpretation of Sunni orthodoxy, which sees in Sufism an essential dimension of Islam alongside the disciplines of jurisprudence and theology, is represented by institutions such as Al-Azhar University and Zaytuna College, with Al-Azhar's current Grand Imam Ahmed el-Tayeb defining "Sunni orthodoxy" as being a follower "of any of the four schools of [legal] thought (Hanafi, Shafi'i, Maliki or Hanbali) and ... [also] of the Sufism of Imam Junayd of Baghdad in doctrines, manners and [spiritual] purification."

In the eleventh-century, Sufism, which had previously been a less "codified" trend in Islamic piety, began to be "ordered and crystallized" into orders which have continued until the present day. All these orders were founded by a major Sunni Islamic saint, and some of the largest and most widespread included the Qadiriyya (after Abdul-Qadir Gilani [d. 1166]), the Rifa'iyya (after Ahmed al-Rifa'i [d. 1182]), the Chishtiyya (after Moinuddin Chishti [d. 1236]), the Shadiliyya (after Abul Hasan ash-Shadhili [d. 1258]), and the Naqshbandiyya (after Baha-ud-Din Naqshband Bukhari [d. 1389]). Contrary to popular perception in the West, however, neither the founders of these orders nor their followers ever considered themselves to be anything other than orthodox Sunni Muslims, and in fact all of these orders were attached to one of the four orthodox legal schools of Sunni Islam. Thus, the Qadiriyya order was Hanbali, with its founder, Abdul-Qadir Gilani, being a renowned Hanbali jurist; the Chishtiyya was Hanafi; the Shadiliyya order was Maliki; and the Naqshbandiyya order was Hanafi. Thus, "many of the most eminent defenders of Islamic orthodoxy, such as Abdul-Qadir Gilani, Ghazali, and the Sultan Ṣalāḥ ad-Dīn (Saladin) were connected with Sufism."

The contemporary Salafi and Wahhabi strands of Sunnis, however, do not accept the traditional stance on mystical practices.

The Quran as it exists today in book form was compiled by Muhammad's companions ("Sahabah") within a handful of months of his death, and is accepted by all sects of Islam. However, there were many matters of belief and daily life that were not directly prescribed in the Quran, but were actions that were observed by Muhammad and the early Muslim community. Later generations sought out oral traditions regarding the early history of Islam, and the practices of Muhammad and his first followers, and wrote them down so that they might be preserved. These recorded oral traditions are called hadith. Muslim scholars have through the ages sifted through the hadith and evaluated the chain of narrations of each tradition, scrutinizing the trustworthiness of the narrators and judging the strength of each hadith accordingly.

"Kutub al-Sittah" are six books containing collections of hadiths. Sunni Muslims accept the hadith collections of Bukhari and Muslim as the most authentic ("sahih", or correct), and while accepting all hadiths verified as authentic, grant a slightly lesser status to the collections of other recorders. There are, however, four other collections of hadith that are also held in particular reverence by Sunni Muslims, making a total of six:

There are also other collections of hadith which also contain many authentic hadith and are frequently used by scholars and specialists. Examples of these collections include:





</doc>
<doc id="29403" url="https://en.wikipedia.org/wiki?curid=29403" title="Sour mix">
Sour mix

Sour mix (also known as sweet and sour mix) is a mixer that is yellow-green in color and is used in many cocktails. It is made from approximately equal parts lemon and/or lime juice and simple syrup and shaken vigorously with ice. This produces a pearly-white liquid with a pronounced flavor.

Optionally, egg-whites may be added to make the liquid slightly foamy.

Sour mix can be mixed with liquor(s) to make a sour drink; most common are vodka sour (vodka) and whiskey sour (whiskey).

Pre-mixed versions are available, and are in use in many bars. These typically consist of a powder which must be rehydrated by adding water prior to use.



</doc>
<doc id="29405" url="https://en.wikipedia.org/wiki?curid=29405" title="Sikh">
Sikh

A Sikh (; ' ) is a person associated with Sikhism, a monotheistic religion that originated in the 15th century based on the revelation of Guru Nanak. The term "Sikh" has its origin in the Sanskrit words शिष्य ('), meaning a disciple, or a student. A Sikh, according to Article I of the "Sikh Rehat Maryada" (the Sikh code of conduct), is "any human being who faithfully believes in One Immortal Being; ten Gurus, from Guru Nanak to Guru Gobind Singh; Guru Granth Sahib; the teachings of the ten Gurus and the baptism bequeathed by the tenth Guru".

The greater Punjab region of the Indian subcontinent has been the historic homeland of the Sikhs, and was ruled by the Sikhs for significant parts of the 18th and 19th centuries. Today, the Punjab state in northwest India has a majority Sikh population, and sizable communities of Sikhs exist around the world. Many countries, such as the United Kingdom, recognize Sikhs as a designated ethnicity on their censuses. The American non-profit organization United Sikhs has sought to have Sikh included on the U.S. census as well, arguing that Sikhs "self-identify as an 'ethnic minority and believe "that they are more than just a religion".

Male Sikhs generally have "Singh" (Lion) as their middle or last name (not all Singhs are Sikhs), and female Sikhs have "Kaur" (Princess) as their middle or last name. Sikhs who have undergone the "Khanḍe-kī-Pahul" (the Sikh initiation ceremony) may also be recognized by the five Ks: Kesh, uncut hair which is kept covered, usually by a turban; "Kara", an iron or steel bracelet; "Kirpan", a sword tucked into a "gatra" strap or a "kamal kasar" belt; "Kachehra", a cotton undergarment; and "Kanga", a small wooden comb. Initiated male and female Sikhs must cover their hair with a turban.

Guru Nanak (1469–1539), founder of Sikhism, was born to Mehta Kalu and Mata Tripta, in the village of Talwandi, now called Nankana Sahib, near Lahore. Guru Nanak was a religious leader and social reformer. However, Sikh political history may be said to begin with the death of the fifth Sikh guru, Guru Arjan Dev, in 1606. Religious practices were formalised by Guru Gobind Singh on 30 March 1699. Gobind Singh initiated five people from a variety of social backgrounds, known as the "Panj Piare" (the five beloved ones) to form the Khalsa, or collective body of initiated Sikhs. During the period of Mughal rule in India (1556–1707) several Sikh gurus were killed by the Mughals for opposing their persecution of minority religious communities including Sikhs. Sikhs subsequently militarized to oppose Mughal rule.

After defeating the Afghan and Mughal, sovereign states called Misls were formed, under Jassa Singh Ahluwalia. The Confederacy was unified and transformed into the Sikh Empire under Maharaja Ranjit Singh Bahadur, which was characterised by religious tolerance and pluralism, with Christians, Muslims and Hindus in positions of power. The empire is considered the zenith of political Sikhism, encompassing Kashmir, Ladakh and Peshawar. Hari Singh Nalwa, the commander-in-chief of the Sikh Khalsa Army in the North West Frontier, expanded the confederacy to the Khyber Pass. Its secular administration implemented military, economic and governmental reforms.After the annexation of the Sikh kingdom by the British, the latter recognized the martial qualities of the Sikhs and Punjabis in general and started recruiting from that area. During the 1857 Indian mutiny, the Sikhs stayed loyal to the British. This resulted in heavy recruiting from Punjab to the colonial army for the next 90 years of the British Raj. The distinct turban that differentiates a Sikh from other turban wearers is a relic of the rules of the British Indian Army. According to Mahmud, the British did not discover the Martial race of the Sikh, it was rather created by the British
The British colonial rule saw the emergence of many reform movements in India including Punjab. This included formation in 1873 and 1879 of the First and Second Singh Sabha respectively. The Sikh leaders of the Singh Sabha worked to offer a clear definition of Sikh identity and tried to purify Sikh belief and practice.

The later part of British colonial rule saw the emergence of the Akali movement to bring reform in the gurdwaras during the early 1920s. The movement led to the introduction of Sikh Gurdwara Bill in 1925, which placed all the historical Sikh shrines in India under the control of Shiromani Gurdwara Parbandhak Committee (SGPC).

The months leading up to the partition of India in 1947 were marked by conflict in the Punjab between Sikhs and Muslims. This caused the religious migration of Punjabi Sikhs and Hindus from West Punjab, mirroring a similar religious migration of Punjabi Muslims from East Punjab.
The 1960s saw growing animosity between Sikhs and Hindus in India, with the Sikhs demanding the creation of a Punjab state on a linguistic basis similar to other states in India. This was promised to Sikh leader Master Tara Singh by Jawaharlal Nehru, in return for Sikh political support during negotiations for Indian independence. Although the Sikhs obtained the Punjab, they lost Hindi-speaking areas to Himachal Pradesh, Haryana and Rajasthan. Chandigarh was made a union territory and the capital of Haryana and Punjab on 1 November 1966.

Tensions arose again during the late 1970s, fueled by Sikh claims of discrimination and marginalisation by the Hindu-dominated Indian National Congress party and tactics adopted by the Prime Minister Indira Gandhi. According to Katherine Frank, Indira Gandhi's assumption of emergency powers in 1975 resulted in the weakening of the "legitimate and impartial machinery of government," and her increasing "paranoia" about opposing political groups led her to institute a "despotic policy of playing castes, religions and political groups against each other for political advantage." Sikh leader Jarnail Singh Bhindranwale articulated Sikh demands for justice, and this triggered violence in the Punjab. The prime minister's 1984 defeat of Bhindranwale led to an attack on the Golden Temple in Operation Blue Star and to her assassination by her Sikh bodyguards. Gandhi's assassination resulted in an explosion of violence against Sikh communities and the killing of thousands of Sikhs throughout India. Since 1984, relations between Sikhs and Hindus have moved toward a rapprochement aided by economic prosperity. However, a 2002 claim by the Hindu right-wing Rashtriya Swayamsevak Sangh (RSS) that "Sikhs are Hindus" disturbed Sikh sensibilities. The Khalistan movement campaigns for justice for the victims of the violence, and for the political and economic needs of the Punjab.

During the 1999 Vaisakhi, Sikhs worldwide celebrated the 300th anniversary of the creation of the Khalsa. Canada Post honoured Sikh Canadians with a commemorative stamp in conjunction with the 300th anniversary of Vaisakhi. On April 9, 1999, Indian president K.R. Narayanan issued a stamp commemorating the 300th anniversary of the Khalsa.

From the Guru Granth Sahib,

The five Ks ("panj kakaar") are five articles of faith which all baptized Sikhs (Amritdhari Sikhs) are obliged to wear. The symbols represent the ideals of Sikhism: honesty, equality, fidelity, meditating on Waheguru and never bowing to tyranny.
The five symbols are:

The Sikhs have a number of musical instruments: the rebab, dilruba, taus, jori and sarinda. Playing the sarangi was encouraged by Guru Hargobind. The rebab was played by Bhai Mardana as he accompanied Guru Nanak on his journeys. The jori and sarinda were introduced to Sikh devotional music by Guru Arjan. The taus was designed by Guru Hargobind, who supposedly heard a peacock singing and wanted to create an instrument mimicking its sounds ("taus" is the Persian word for peacock). The dilruba was designed by Guru Gobind Singh at the request of his followers, who wanted a smaller instrument than the taus. After Japji Sahib, all of the shabad in the Guru Granth Sahib were composed as raags. This type of singing is known as Gurmat Sangeet.

When they marched into battle, the Sikhs would play a "Ranjit Nagara" (victory drum) to boost morale. Nagaras (usually two to three feet in diameter, although some were up to five feet in diameter) are played with two sticks. The beat of the large drums, and the raising of the Nishan Sahib, meant that the "singhs" were on their way.

Numbering about 27 million worldwide, Sikhs make up 0.39 percent of the world population; approximately 83 percent live in India. About 76 percent of all Sikhs live in the north Indian State of Punjab, where they form a majority (about two-thirds) of the population. Substantial communities of Sikhs live in the Indian states or union territories of Chandigarh where they form 13.11% of the population, Haryana (more than 1.2 million), Rajasthan, West Bengal, Uttar Pradesh, Delhi, Maharashtra, Uttarakhand, Madhya Pradesh, Assam and Jammu and Kashmir.

Sikh migration from British India began in earnest during the second half of the 19th century, when the British completed their annexation of the Punjab. The British Raj recruited Sikhs for the Indian Civil Service (particularly the British Indian Army), which led to Sikh migration throughout India and the British Empire. During the Raj, semiskilled Sikh artisans were transported from the Punjab to British East Africa to help build railroads. Sikhs emigrated from India after World War II, most going to the United Kingdom but many to North America. Some Sikhs who had settled in eastern Africa were expelled by Ugandan dictator Idi Amin in 1972. Economics is a major factor in Sikh migration, and significant communities exist in the United Kingdom, the United States, Malaysia, East Africa, Australia, Singapore and Thailand. Due to this, Canada is the country that has the highest number of Sikhs in proportion to the population in the world at 1.4% of Canada's total population.

Although the rate of Sikh migration from the Punjab has remained high, traditional patterns of Sikh migration favouring English-speaking countries (particularly the United Kingdom) have changed during the past decade due to stricter immigration laws. Moliner (2006) wrote that as a consequence of Sikh migration to the UK "becom[ing] virtually impossible since the late 1970s", migration patterns evolved to continental Europe. Italy is a rapidly growing destination for Sikh migration, with Reggio Emilia and Vicenza having significant Sikh population clusters. Italian Sikhs are generally involved in agriculture, agricultural processing, the manufacture of machine tools and horticulture.

Johnson and Barrett (2004) estimate that the global Sikh population increases annually by 392,633 (1.7 percent per year, based on 2004 figures); this percentage includes births, deaths and conversions. Primarily for socio-economic reasons, Indian Sikhs have the lowest adjusted growth rate of any major religious group in India, at 16.9 percent per decade (estimated from 1991 to 2001). The Sikh population has the lowest gender balance in India, with only 903 women per 1,000 men according to the 2011 Indian census.

Guru Nanak in Sri Granth Sahib calls for treating everyone equally. Other Sikh Gurus also denounced the hierarchy of the caste system. However they all came from just one caste, the Khatris. Despite that social stratification exists in the Sikh community.

Over 60% of Sikhs belong to the Jat caste, Tohar caste is sub caste of jutt, which is an agrarian caste. Despite being very small in numbers, the mercantile Khatri and Arora castes wield considerable influence within the Sikh community. Other common Sikh castes include Sainis(kshatriyas), Rajputs, Ramgarhias (artisans), Ahluwalias (formerly brewers), Kambojs (rural caste), Labanas (merchants), Kumhars and the two Dalit castes, known in Sikh terminology as the Mazhabis (the Chuhras) and the Ravidasias (the Chamars).

According to Surinder Singh Jodhka, the Sikh religion does not advocate discrimination against any caste or creed, however, in practice, Sikhs belonging to the landowning dominant castes have not shed all their prejudices against the Dalit castes. While Dalits would be allowed entry into the village gurdwaras they would not be permitted to cook or serve langar (Communal meal). Therefore, wherever they could mobilise resources, the Sikh Dalits of Punjab have tried to construct their own gurdwara and other local level institutions in order to attain a certain degree of cultural autonomy. In 1953, Sikh leader, Master Tara Singh, succeeded in persuading the Indian Government to include Sikh castes of the converted untouchables in the list of scheduled castes. In the Shiromani Gurdwara Prabandhak Committee, 20 of the 140 seats are reserved for low-caste Sikhs.,

According to a 1994 estimate, Punjabis (Sikhs and non-Sikhs) comprised 10 to 15 percent of all ranks in the Indian Army, although the state contained less than 3% of the country's population. The Indian government does not release religious or ethnic origins of the military personnel, but a 1991 report by Tim McGirk estimated that 20 percent of Indian Army officers were Sikhs. Together with the Gurkhas recruited from Nepal, the Maratha Light Infantry from Maharashtra and the Jat Regiment, the Sikhs are one of the few communities to have exclusive regiments in the Indian Army. The Sikh Regiment is one of the most-decorated regiments in the army, with 73 Battle Honours, 14 Victoria Crosses, 21 first-class Indian Orders of Merit (equivalent to the Victoria Cross), 15 Theatre Honours, five COAS Unit Citations, two Param Vir Chakras, 14 Maha Vir Chakras, five Kirti Chakras, 67 Vir Chakras and 1,596 other awards. The highest-ranking general in the history of the Indian Air Force is a Punjabi Sikh, Marshal of the Air Force Arjan Singh. Plans by the United Kingdom Ministry of Defence for a Sikh infantry regiment were scrapped in June 2007.

Historically, most Indians have been farmers and 66 percent of the Indian population are engaged in agriculture. Indian Sikhs are employed in agriculture to a lesser extent; India's 2001 census found 39 percent of the working population of the Punjab employed in this sector. The success of the 1960s Green Revolution, in which India went from "famine to plenty, from humiliation to dignity", was based in the Punjab (which became known as "the breadbasket of India"). The Punjab is the wealthiest Indian state per capita, with the average Punjabi income three times the national average. The Green Revolution centred on Indian farmers adopting more intensive and mechanised agricultural methods, aided by the electrification of the Punjab, cooperative credit, consolidation of small holdings and the existing, British Raj-developed canal system. According to Swedish political scientist Ishtiaq Ahmad, a factor in the success of the Indian green revolution was the "Sikh cultivator, often the Jat, whose courage, perseverance, spirit of enterprise and muscle prowess proved crucial". However, not all aspects of the green revolution were beneficial. Indian physicist Vandana Shiva wrote that the green revolution made the "negative and destructive impacts of science [i.e. the green revolution] on nature and society" invisible, and was a catalyst for Punjabi Sikh and Hindu tensions despite a growth in material wealth.
Punjabi Sikhs are engaged in a number of professions which include science, engineering and medicine. Notable examples are nuclear scientist Piara Singh Gill (who worked on the Manhattan Project), fibre-optics pioneer Narinder Singh Kapany and physicist, science writer and broadcaster Simon Singh.

In business, the UK-based clothing retailers New Look and the Thai-based Jaspal were founded by Sikhs. India's largest pharmaceutical company, Ranbaxy Laboratories, is headed by Sikhs. UK Sikhs have the highest percentage of home ownership (82 percent) of any religious community. UK Sikhs are the second-wealthiest (after the Jewish community) religious group in the UK, with a median total household wealth of £229,000.
In Singapore Kartar Singh Thakral expanded his family's trading business, Thakral Holdings, into total assets of almost $1.4 billion and is Singapore's 25th-richest person. Sikh Bob Singh Dhillon is the first Indo-Canadian billionaire. The Sikh diaspora has been most successful in North America.
Sikh intellectuals, sportsmen and artists include poet and Bollywood lyricist Rajkavi Inderjeet Singh Tulsi, writer Khushwant Singh, England cricketer Monty Panesar, former 400m runner Milkha Singh, Indian wrestler and actor Dara Singh, former Indian hockey team captains Ajitpal Singh and Balbir Singh Sr., former Indian cricket captain Bishen Singh Bedi, Harbhajan Singh (India's most successful off spin cricket bowler), Navjot Singh Sidhu (former Indian cricketer turned politician). Bollywood actresses include Neetu Singh, Simran Judge, Poonam Dhillon, Mahi Gill, Esha Deol, Parminder Nagra, Gul Panag, Mona Singh, Namrata Singh Gujral and director Gurinder Chadha, Parminder Gill .

Sikhs have migrated worldwide, with a variety of occupations. The Sikh Gurus preached ethnic and social harmony, and Sikhs comprise a number of ethnic groups. Those with over 1,000 members include the Ahluwalia, Arain, Arora, Bhatra, Bairagi, Bania, Basith, Bawaria, Bazigar, Bhabra, Chamar, Chhimba, Darzi, Dhobi, Gujar, Jatt, Jhinwar, Kahar, Kalal, Kamboj, Khatri, Kumhar, Labana, Lohar, Mahtam, Mazhabi, Megh, Mirasi, Mochi, Mohyal, Nai, Rajput, Ramgarhia, Saini, Sansi, Sudh, Tarkhan, Kashyap Rajput .

An order of Punjabi Sikhs, the Nihang or the Akalis, was formed during Ranjit Singh's time. Under their leader, Akali Phula Singh, they won many battles for the Sikh Confederacy during the early 19th century.

Sikhs supported the British during the Indian Rebellion of 1857. By the beginning of World War I, Sikhs in the British Indian Army totaled over 100,000 (20 percent of the force). Until 1945 fourteen Victoria Crosses were awarded to Sikhs, a per-capita regimental record. In 2002 the names of all Sikh VC and George Cross recipients were inscribed on the monument of the Memorial Gates on Constitution Hill, next to Buckingham Palace. Chanan Singh Dhillon was instrumental in campaigning for the memorial.
During World War I, Sikh battalions fought in Egypt, Palestine, Mesopotamia, Gallipoli and France. Six battalions of the Sikh Regiment were raised during World War II, serving in the Second Battle of El Alamein, the Burma and Italian campaigns and in Iraq and receiving 27 battle honours. Around the world, Sikhs are commemorated in Commonwealth cemeteries.

During the late 19th and early 20th centuries, Sikhs began to emigrate to East Africa, the Far East, Canada, the United States and the United Kingdom. In 1907 the Khalsa Diwan Society was established in Vancouver, and four years later the first gurdwara was established in London. In 1912 the first gurdwara in the United States was founded in Stockton, California.

Since Sikhs (like many Middle Eastern men) wear turbans and keep beards, some people in Western countries have mistaken Sikh men for Muslim or Arabic and Afghan men since the September 11 attacks and the Iraq War. Several days after the 9/11 attacks Sikh Balbir Singh Sodhi was murdered by Frank Roque, who thought Sodhi was connected with al-Qaeda. CNN suggested an increase in hate crimes against Sikh men in the United States and the UK after the 9/11 attacks.

Since Sikhism has never actively sought converts, the Sikhs have remained a relatively homogeneous ethnic group. The 3HO organisation claim to have inspired a moderate growth in non-Indian adherents of Sikhism. In 1998 an estimated 7,800 3HO Sikhs, known colloquially as ‘gora’ (ਗੋਰਾ) or ‘white’ Sikhs, were mainly centred around Española, New Mexico and Los Angeles, California. Sikhs and the Sikh American Legal Defense and Education Fund overturned a 1925 Oregon law banning the wearing of turbans by teachers and government officials.

In an attempt to foster Sikh leaders in the Western world, youth initiatives by a number of organisations exist. The Sikh Youth Alliance of North America sponsors an annual Sikh Youth Symposium, a public-speaking and debate competition held in gurdwaras throughout the U.S. and Canada. There are a number of Sikh office holders in Canada. In the United States, the current U.S. Ambassador to the United Nations and former governor of South Carolina, Nikki Haley, was born and raised as a Sikh, but converted to Christianity after her marriage. She still actively attends both Sikh and Christian services.

The Khalistan movement is a Sikh nationalist movement, which seeks to create a separate country called Khalistān (, "The Land of the Pure") in the Punjab region of South Asia. The territorial definition of the proposed country ranges from the Punjab state of India to the greater Punjab region, including the neighbouring Indian states.

The Punjab region has been the traditional homeland for the Sikhs. Before its conquest by the British it had been ruled by the Sikhs for 82 years; the Sikh Misls ruled over the entire Punjab sporadically from early 1700s and from 1767 to 1799, until their confederacy was unified into the Sikh Empire by Maharajah Ranjit Singh. However, the region also has a substantial number of Hindus and Muslims who were always well represented in the governance and military under Sikh rule. Before 1947, the Sikhs formed the largest religious group only in the Ludhiana district of the British province. When the Muslim League demanded a separate country for Muslims via the Lahore Resolution of 1940, a section of Sikh leaders grew concerned that their community would be left without any homeland following the partition of India between the Hindus and the Muslims. They put forward the idea of Khalistan, envisaging it as a theocratic state covering a small part of the greater Punjab region.

After the partition was announced, the majority of the Sikhs migrated from the Pakistani province of Punjab to the Indian province of Punjab, which then included the parts of the present-day Haryana and Himachal Pradesh. Following India's independence in 1947, the Punjabi Suba Movement led by the Akali Dal aimed at creation of a Punjabi linguistic state (Suba) in the Punjab region of India in the 1950s, consistent with the policy of linguistic states (States Reorganisation Act, 1956) applied elsewhere in India. Concerned that creating a Punjabi-majority state would effectively mean creating a Sikh-majority state, the Indian government initially rejected the demand. After a series of protests, violent clampdowns on the Sikhs, and the Indo-Pakistani War of 1965, the Government finally agreed to partition the state, creating a new Punjabi speaking Punjab state and splitting the rest of the region to the states of Himachal Pradesh, the new state Haryana. Subsequently, the Sikh leaders started demanding more autonomy for the states, alleging that the Central government was discriminating against Punjab (such as denying the state control over its waters, disproportionate extraction of its water resources to benefit other Indian regions, leaving significant Punjabi speaking regions outside of Punjab etc). Although the Akali Dal explicitly opposed the demand for an independent Sikh country, the issues raised by it and the Indian state's discrimination against the Sikhs (and Punjab) were used as a premise for the creation of a separate country by the proponents of Khalistan. The allegation of "secessionism" could have been overplayed by the Indian state to legitimise its brutal crushing of Sikh campaigns for rights or their dissent (such as their disproportionate role in opposing the Indira dictatorship (The Emergency (India))

In 1971, the Khalistan proponent Jagjit Singh Chauhan travelled to the United States. He placed an advertisement in "The New York Times" proclaiming the formation of Khalistan and was able to collect millions of dollars from the Sikh diaspora. On 12 April 1980, he held a meeting with the Indian prime minister Indira Gandhi before declaring the formation of "National Council of Khalistan", at Anandpur Sahib. He declared himself as the President of the Council and Balbir Singh Sandhu as its Secretary General. In May 1980, Jagjit Singh Chauhan travelled to London and announced the formation of Khalistan. A similar announcement was made by Balbir Singh Sandhu, in Amritsar, who released stamps and currency of Khalistan. The inaction of the authorities in Amritsar and elsewhere was decried by Akali Dal headed by the Sikh leader Harchand Singh Longowal as a political stunt by the Congress(I) party of Indira Gandhi.

The Khalistan movement reached its zenith in the 1970s and 1980s, flourishing in the Indian state of Punjab, which has a Sikh-majority population and has been the traditional homeland of the Sikh religion. Various pro-Khalistan outfits have been involved in a separatist movement against the government of India ever since. There are claims of funding from Sikhs outside India to attract young people into these pro-Khalistan militant groups.

In the 1980s, some of the Khalistan proponents turned to militancy as a consequence of state violence and heavy handedness, resulting in counter-militancy operations by the Indian security forces. In one such operation, Operation Blue Star (June 1984), the Indian Army led by the Indian army General Kuldip Singh Brar and supervised by General Arun Shridhar Vaidya forcibly entered the Harimandir Sahib (the Golden Temple) on the pretext of overpowering armed militants and the alleged militant leader Jarnail Singh Bhindranwale. The handling of the operation, damage to the Akal Takht (which is one of the five seats of temporal physical religious authority of the Sikhs), destruction of the Sikh Reference Library and loss of life on both sides, led to widespread criticism of the Indian Government. Many Sikhs strongly maintain that the attack resulted in the desecration of the holiest Sikh shrine. The Sikh Reference Library was also destroyed by the Indian forces during this attack. The Indian Prime Minister Indira Gandhi was assassinated by her two Sikh bodyguards in retaliation. Although the accused of the assassination were tried, found guilty and punished by the legal system, following her death, thousands of Sikhs were massacred in the 1984 anti-Sikh riots in Delhi and multiple places in India in an orchestrated attempt at genocide.

In January 1986, the Golden Temple was occupied by militants belonging to All India Sikh Students Federation and Damdami Taksal. On 26 January 1986, a gathering known as the Sarbat Khalsa (a de-facto parliament) passed a resolution ("gurmattā") favouring the creation of Khalistan. Subsequently, a number of rebel militant groups in favour of Khalistan waged a major insurgency against the government of India. Indian security forces suppressed the insurgency in the early 1990s, but Sikh political groups such as the Khalsa Raj Party and SAD (A) continued to pursue an independent Khalistan through non-violent means. Pro-Khalistan organisations such as Dal Khalsa (International) are also active outside India, supported by a section of the Sikh diaspora.

In November 2015, a Sarbat Khalsa, or congregation of the Sikh community was called in response to recent unrest in the Punjab region. The Sarbat Khalsa adopted 13 resolutions to strengthen Sikh institutions and traditions. The 12th resolution reaffirmed the resolutions adopted by the Sarbat Khalsa in 1986, including the declaration of the sovereign state of Khalistan.

Sikh art and culture are nearly synonymous with that of the Punjab, and Sikhs are easily recognised by their distinctive turban (Dastar). The Punjab has been called India's melting pot, due to the confluence of invading cultures from the rivers from which the region gets its name. Sikh culture is therefore a synthesis of cultures. Sikhism has forged a unique architecture, which S. S. Bhatti described as "inspired by Guru Nanak's creative mysticism" and "is a mute harbinger of holistic humanism based on pragmatic spirituality".

During the Mughal and Afghan persecution of the Sikhs during the 17th and 18th centuries, the latter were concerned with preserving their religion and gave little thought to art and culture. With the rise of Ranjit Singh and the Sikh Raj in Lahore and Delhi, there was a change in the landscape of art and culture in the Punjab; Hindus and Sikhs could build decorated shrines without the fear of destruction or looting.

The Sikh Confederacy was the catalyst for a uniquely Sikh form of expression, with Ranjit Singh commissioning forts, palaces, bungas (residential places) and colleges in a Sikh style. Sikh architecture is characterised by gilded fluted domes, cupolas, kiosks, stone lanterns, ornate balusters and square roofs. A pinnacle of Sikh style is Harmandir Sahib (also known as the Golden Temple) in Amritsar.

Sikh culture is influenced by militaristic motifs (with the Khanda the most obvious), and most Sikh artifacts—except for the relics of the Gurus—have a military theme. This theme is evident in the Sikh festivals of Hola Mohalla and Vaisakhi, which feature marching and displays of valor.

Although the art and culture of the Sikh diaspora have merged with that of other Indo-immigrant groups into categories like "British Asian", "Indo-Canadian" and "Desi-Culture", a minor cultural phenomenon which can be described as "political Sikh" has arisen. The art of diaspora Sikhs like Amarjeet Kaur Nandhra and Amrit and Rabindra Kaur Singh (the "Singh Twins") is influenced by their Sikhism and current affairs in the Punjab.

Bhangra and Giddha are two forms of Punjabi folk dancing which have been adapted and pioneered by Sikhs. Punjabi Sikhs have championed these forms of expression worldwide, resulting in Sikh culture becoming linked to Bhangra (although "Bhangra is not a Sikh institution but a Punjabi one").

Sikh painting is a direct offshoot of the Kangra school of painting. In 1810, Ranjeet Singh (1780–1839) occupied Kangra Fort and appointed Sardar Desa Singh Majithia his governor of the Punjab hills. In 1813 the Sikh army occupied Guler State, and Raja Bhup Singh became a vassal of the Sikhs. With the Sikh kingdom of Lahore becoming the paramount power, some of the Pahari painters from Guler migrated to Lahore for the patronage of Maharaja Ranjeet Singh and his Sardars.

The Sikh school adapted Kangra painting to Sikh needs and ideals. Its main subjects are the ten Sikh gurus and stories from Guru Nanak's Janamsakhis. The tenth Guru, Gobind Singh, left a deep impression on the followers of the new faith because of his courage and sacrifices. Hunting scenes and portraits are also common in Sikh painting.


"Architectural Heritage of a Sikh State: Faridkot" by Subhash Parihar, Delhi: Aryan Books International, 2009, 



</doc>
<doc id="29407" url="https://en.wikipedia.org/wiki?curid=29407" title="Superworld">
Superworld

Superworld is a superhero-themed role-playing game published by Chaosium in 1983. Written by "Basic Role-Playing" and "RuneQuest" author Steve Perrin, "Superworld" began as one third of the "Worlds of Wonder" product, which also included a generic fantasy setting, "Magic World", and a generic science fiction setting, "Future World", all using the same core "Basic Role-Playing" rules. Only "Superworld" became a game in its own right.

"Superworld" is based on the traditional Chaosium "Basic Role-Playing" system augmented by super-powers.

Seven characteristics (Strength, Constitution, Size, Intelligence, Power, Dexterity, Appearance) are rolled with dice (2D6+6, rather the 3d6 used for many other "Basic Role-Playing" games.) The sum of these characteristics gives a total of Hero Points used to buy super powers.

The super powers system follows the "Champions" model of powers that are described by their effects. For example, one does not buy "Laser Vision", but the effect "Energy Blast" and specifies that it is a laser emitted by the hero's eyes. Each effect can be modified by Advantages (less energy expenditure, for example) or Disadvantages (reduced number of uses, for example) which increase or reduce the cost of a power.

Hero Points can also be used to buy skills or increase characteristics. It is possible to get more Hero Points for character creation by choosing Disabilities for the character, such as Public Identity, Vulnerability to a Substance, Psychological Problems, etc. More Hero Points would be awarded for experience at the end of a game session.

The system functions in the same way as the other "Basic Role-Playing" games, by rolling percentile dice against skills. Lower rolls than needed can cause increased effect from Specials (equivalent to Impales in "RuneQuest"), or Criticals, and high rolls can cause critical failures (Fumbles). Combat rules have many options and take into account three types of energy for damage: Kinetic, Electric, and Radiation.

The game box contains three rules booklets, a booklet of character sheets, one of tables for the Gamemaster, a page of cardboard figure silhouettes to be cut out, and a set 6, 8, and 20-sided dice. 1984 printings also contains a 4-page errata booklet.


(1984) Scenario. Author: Ken Rolston. Set in a high school, and designed for teenage characters. It comes with six young pregenerated heroes, or lets players use their own. Beginning with the funeral of one of their friends, it sets the heroes on the track of a drug distribution network in their school, directed by the aforementioned Dr. Drugs.

It also includes rules for the creation and management of adolescent characters that have just discovered their powers, and a plan of Warren G. Harding High School, though the scenario recommends substituting the school in which the GM and the players studied.

(1985) Rules supplement. 
Many authors: Stephen R. Marsh, Stephen Perrin, Ian Lee Starcher, Anthony Affronti, Jimmy Akin II, William A Barton, Norman Doege, Bruce Dresselhaus, Ray Greer, Zoran Kovacich, George MacDonald, Steve Maurer, Sandy Petersen, Wayne Shaw, John Sullivan—most are listed because they provided one or more optional rules.

Includes:

(1984) Scenario / Campaign. 
Authors: Stephen Perrin, Yurek Chodak, Donald Harrington, Charles Huber.
A linked collection of three scenarios based on the members of the criminal organization HAVOC. All the characters are presented with characteristics for with three different systems, "Superworld", "Champions" and "Villains & Vigilantes". Each may be played separately, or as part of a campaign.


The "Wild Cards" series of science fiction books came from an Albuquerque, New Mexico campaign gamemastered by George R. R. Martin, and played in by other science fiction writers.




</doc>
<doc id="29408" url="https://en.wikipedia.org/wiki?curid=29408" title="Samuel Taylor Coleridge">
Samuel Taylor Coleridge

Samuel Taylor Coleridge (; 21 October 177225 July 1834) was an English poet, literary critic, philosopher and theologian who, with his friend William Wordsworth, was a founder of the Romantic Movement in England and a member of the Lake Poets. He wrote the poems "The Rime of the Ancient Mariner" and "Kubla Khan", as well as the major prose work "Biographia Literaria". His critical work, especially on William Shakespeare, was highly influential, and he helped introduce German idealist philosophy to English-speaking culture. Coleridge coined many familiar words and phrases, including suspension of disbelief. He had a major influence on Ralph Waldo Emerson and on American transcendentalism.

Throughout his adult life Coleridge had crippling bouts of anxiety and depression; it has been speculated that he had bipolar disorder, which had not been defined during his lifetime. He was physically unhealthy, which may have stemmed from a bout of rheumatic fever and other childhood illnesses. He was treated for these conditions with laudanum, which fostered a lifelong opium addiction.

Coleridge was born on 21 October 1772 in the town of Ottery St Mary in Devon, England. Samuel's father was the Reverend John Coleridge (1718–1781), the well-respected vicar of St Mary's Church, Ottery St Mary and was headmaster of the King's School, a free grammar school established by King Henry VIII (1509–1547) in the town. He had previously been Master of Hugh Squier's School in South Molton, Devon, and Lecturer of nearby Molland.
John Coleridge had three children by his first wife. Samuel was the youngest of ten by the Reverend Mr. Coleridge's second wife, Anne Bowden (1726–1809), probably the daughter of John Bowden, Mayor of South Molton, Devon, in 1726. Coleridge suggests that he "took no pleasure in boyish sports" but instead read "incessantly" and played by himself. After John Coleridge died in 1781, 8-year-old Samuel was sent to Christ's Hospital, a charity school which was founded in the 16th century in Greyfriars, London, where he remained throughout his childhood, studying and writing poetry. At that school Coleridge became friends with Charles Lamb, a schoolmate, and studied the works of Virgil and William Lisle Bowles.
In one of a series of autobiographical letters written to Thomas Poole, Coleridge wrote: "At six years old I remember to have read "Belisarius", "Robinson Crusoe", and "Philip Quarll" – and then I found the "Arabian Nights' Entertainments" – one tale of which (the tale of a man who was compelled to seek for a pure virgin) made so deep an impression on me (I had read it in the evening while my mother was mending stockings) that I was haunted by spectres whenever I was in the dark – and I distinctly remember the anxious and fearful eagerness with which I used to watch the window in which the books lay – and whenever the sun lay upon them, I would seize it, carry it by the wall, and bask, and read."

However, Coleridge seems to have appreciated his teacher, as he wrote in recollections of his school days in "Biographia Literaria": 
I enjoyed the inestimable advantage of a very sensible, though at the same time, a very severe master [...] At the same time that we were studying the Greek Tragic Poets, he made us read Shakespeare and Milton as lessons: and they were the lessons too, which required most time and trouble to bring up, so as to escape his censure. I learnt from him, that Poetry, even that of the loftiest, and, seemingly, that of the wildest odes, had a logic of its own, as severe as that of science; and more difficult, because more subtle, more complex, and dependent on more, and more fugitive causes. [...] In our own English compositions (at least for the last three years of our school education) he showed no mercy to phrase, metaphor, or image, unsupported by a sound sense, or where the same sense might have been conveyed with equal force and dignity in plainer words... In fancy I can almost hear him now, exclaiming "Harp? Harp? Lyre? Pen and ink, boy, you mean! Muse, boy, Muse? your Nurse's daughter, you mean! Pierian spring? Oh aye! the cloister-pump, I suppose!" [...] Be this as it may, there was one custom of our master's, which I cannot pass over in silence, because I think it ... worthy of imitation. He would often permit our theme exercises, ... to accumulate, till each lad had four or five to be looked over. Then placing the whole number abreast on his desk, he would ask the writer, why this or that sentence might not have found as appropriate a place under this or that other thesis: and if no satisfying answer could be returned, and two faults of the same kind were found in one exercise, the irrevocable verdict followed, the exercise was torn up, and another on the same subject to be produced, in addition to the tasks of the day. 

Throughout his life, Coleridge idealised his father as pious and innocent, while his relationship with his mother was more problematic. His childhood was characterised by attention seeking, which has been linked to his dependent personality as an adult. He was rarely allowed to return home during the school term, and this distance from his family at such a turbulent time proved emotionally damaging. He later wrote of his loneliness at school in the poem "Frost at Midnight":
"With unclosed lids, already had I dreamt/Of my sweet birthplace."

From 1791 until 1794, Coleridge attended Jesus College, Cambridge. In 1792, he won the Browne Gold Medal for an ode that he wrote on the slave trade. In December 1793, he left the college and enlisted in the 15th (The King's) Light Dragoons using the false name "Silas Tomkyn Comberbache", perhaps because of debt or because the girl that he loved, Mary Evans, had rejected him. Afterwards, he was rumoured to have had a bout of severe depression. His brothers arranged for his discharge a few months later under the reason of "insanity" and he was readmitted to Jesus College, though he would never receive a degree from the University.

At Jesus College, Coleridge was introduced to political and theological ideas then considered radical, including those of the poet Robert Southey. Coleridge joined Southey in a plan, soon abandoned, to found a utopian commune-like society, called Pantisocracy, in the wilderness of Pennsylvania. In 1795, the two friends married sisters Sara and Edith Fricker, in St Mary Redcliffe, Bristol, but Coleridge's marriage with Sara proved unhappy. He grew to detest his wife, whom he only married because of social constraints. He eventually separated from her. Coleridge made plans to establish a journal, "The Watchman", to be printed every eight days to avoid a weekly newspaper tax. The first issue of the short-lived journal was published in March 1796. It had ceased publication by May of that year.

The years 1797 and 1798, during which he lived in what is now known as Coleridge Cottage, in Nether Stowey, Somerset, were among the most fruitful of Coleridge's life. In 1795, Coleridge met poet William Wordsworth and his sister Dorothy. (Wordsworth, having visited him and being enchanted by the surroundings, rented Alfoxton Park, a little over three miles [5 km] away.) Besides the "Rime of The Ancient Mariner", Coleridge composed the symbolic poem "Kubla Khan", written—Coleridge himself claimed—as a result of an opium dream, in "a kind of a reverie"; and the first part of the narrative poem "Christabel". The writing of "Kubla Khan", written about the Mongol emperor Kublai Khan and his legendary palace at Xanadu, was said to have been interrupted by the arrival of a "Person from Porlock" – an event that has been embellished upon in such varied contexts as science fiction and Nabokov's "Lolita". During this period, he also produced his much-praised "conversation poems" "This Lime-Tree Bower My Prison", "Frost at Midnight", and "".

In 1798, Coleridge and Wordsworth published a joint volume of poetry, "Lyrical Ballads", which proved to be the starting point for the English romantic age. Wordsworth may have contributed more poems, but the real star of the collection was Coleridge's first version of "The Rime of the Ancient Mariner". It was the longest work and drew more praise and attention than anything else in the volume. In the spring Coleridge temporarily took over for Rev. Joshua Toulmin at Taunton's Mary Street Unitarian Chapel while Rev. Toulmin grieved over the drowning death of his daughter Jane. Poetically commenting on Toulmin's strength, Coleridge wrote in a 1798 letter to John Prior Estlin, "I walked into Taunton (eleven miles) and back again, and performed the divine services for Dr. Toulmin. I suppose you must have heard that his daughter, (Jane, on 15 April 1798) in a melancholy derangement, suffered herself to be swallowed up by the tide on the sea-coast between Sidmouth and Bere (Beer). These events cut cruelly into the hearts of old men: but the good Dr. Toulmin bears it like the true practical Christian, – there is indeed a tear in his eye, but that eye is lifted up to the Heavenly Father."

Coleridge also worked briefly in Shropshire, where he came in December 1797 as locum to its local Unitarian minister, Dr Rowe, in their church in the High Street at Shrewsbury. He is said to have read his "Rime of the Ancient Mariner" at a literary evening in Mardol. He was then contemplating a career in the ministry, and gave a probationary sermon in High Street church on Sunday, 14 January 1798. William Hazlitt, a Unitarian minister's son, was in the congregation, having walked from Wem to hear him. Coleridge later visited Hazlitt and his father at Wem but within a day or two of preaching he received a letter from Josiah Wedgwood II, who had offered to help him out of financial difficulties with an annuity of £150 (approximately £13,000 in today's money) per year on condition he give up his ministerial career. Coleridge accepted this, to the disappointment of Hazlitt who hoped to have him as a neighbour in Shropshire.

From 16 September 1798, Coleridge and the Wordsworths left for a stay in Germany; Coleridge soon went his own way and spent much of his time in university towns. In February 1799 he enrolled at the University of Göttingen where he attended lectures by Johann Friedrich Blumenbach and Johann Gottfried Eichhorn. During this period, he became interested in German philosophy, especially the transcendental idealism and critical philosophy of Immanuel Kant, and in the literary criticism of the 18th century dramatist Gotthold Lessing. Coleridge studied German and, after his return to England, translated the dramatic trilogy "Wallenstein" by the German Classical poet Friedrich Schiller into English. He continued to pioneer these ideas through his own critical writings for the rest of his life (sometimes without attribution), although they were unfamiliar and difficult for a culture dominated by empiricism.

In 1799, Coleridge and Wordsworths stayed at Thomas Hutchinson's farm on the River Tees at Sockburn, near Darlington.
It was at Sockburn that Coleridge wrote his ballad-poem "Love", addressed to Sara Hutchinson. The knight mentioned is the mailed figure on the Conyers tomb in ruined Sockburn church. The figure has a wyvern at his feet, a reference to the Sockburn Worm slain by Sir John Conyers (and a possible source for Lewis Carroll's "Jabberwocky"). The worm was supposedly buried under the rock in the nearby pasture; this was the 'greystone' of Coleridge's first draft, later transformed into a 'mount'. The poem was a direct inspiration for John Keats' famous poem "La Belle Dame Sans Merci".

Coleridge's early intellectual debts, besides German idealists like Kant and critics like Lessing, were first to William Godwin's "Political Justice", especially during his Pantisocratic period, and to David Hartley's "Observations on Man", which is the source of the psychology which is found in "Frost at Midnight". Hartley argued that one becomes aware of sensory events as impressions, and that "ideas" are derived by noticing similarities and differences between impressions and then by naming them. Connections resulting from the coincidence of impressions create linkages, so that the occurrence of one impression triggers those links and calls up the memory of those ideas with which it is associated (See Dorothy Emmet, "Coleridge and Philosophy").

Coleridge was critical of the literary taste of his contemporaries, and a literary conservative insofar as he was afraid that the lack of taste in the ever growing masses of literate people would mean a continued desecration of literature itself.

In 1800, he returned to England and shortly thereafter settled with his family and friends at Keswick in the Lake District of Cumberland to be near Grasmere, where Wordsworth had moved. He was a houseguest of the Wordsworth's for eighteen months, but was a difficult houseguest, as his dependency on laudanum grew and his frequent nightmares would wake the children. He was also a fussy eater, to Dorothy Wordsworth's frustration, who had to cook. For example, not content with salt, Coleridge sprinkled cayenne pepper on his eggs, which he ate from a teacup. His marital problems, nightmares, illnesses, increased opium dependency, tensions with Wordsworth, and a lack of confidence in his poetic powers fuelled the composition of "Dejection: An Ode" and an intensification of his philosophical studies.

In 1802, Coleridge took a nine-day walking holiday in the fells of the Lake District. Coleridge is credited with the first recorded descent of Scafell to Mickledore via Broad Stand, although this was more due to his getting lost than a keenness for mountaineering.

In 1804, he travelled to Sicily and Malta, working for a time as Acting Public Secretary of Malta under the Civil Commissioner, Alexander Ball, a task he performed quite successfully. He lived in San Anton Palace in the village of Attard. However, he gave this up and returned to England in 1806. Dorothy Wordsworth was shocked at his condition upon his return. From 1807 to 1808, Coleridge returned to Malta and then travelled in Sicily and Italy, in the hope that leaving Britain's damp climate would improve his health and thus enable him to reduce his consumption of opium. Thomas De Quincey alleges in his "Recollections of the Lakes and the Lake Poets" that it was during this period that Coleridge became a full-blown opium addict, using the drug as a substitute for the lost vigour and creativity of his youth. It has been suggested, however, that this reflects de Quincey's own experiences more than Coleridge's.

His opium addiction (he was using as much as two quarts of laudanum a week) now began to take over his life: he separated from his wife Sara in 1808, quarrelled with Wordsworth in 1810, lost part of his annuity in 1811, and put himself under the care of Dr. Daniel in 1814. His addiction caused severe constipation, which required regular and humiliating enemas.

In 1809, Coleridge made his second attempt to become a newspaper publisher with the publication of the journal entitled "The Friend". It was a weekly publication that, in Coleridge’s typically ambitious style, was written, edited, and published almost entirely single-handedly. Given that Coleridge tended to be highly disorganised and had no head for business, the publication was probably doomed from the start. Coleridge financed the journal by selling over five hundred subscriptions, over two dozen of which were sold to members of Parliament, but in late 1809, publication was crippled by a financial crisis and Coleridge was obliged to approach "Conversation Sharp", Tom Poole and one or two other wealthy friends for an emergency loan to continue. "The Friend" was an eclectic publication that drew upon every corner of Coleridge's remarkably diverse knowledge of law, philosophy, morals, politics, history, and literary criticism. Although it was often turgid, rambling, and inaccessible to most readers, it ran for 25 issues and was republished in book form a number of times. Years after its initial publication, "The Friend" became a highly influential work and its effect was felt on writers and philosophers from John Stuart Mill to Ralph Waldo Emerson.

Between 1810 and 1820, Coleridge gave a series of lectures in London and Bristol – those on Shakespeare renewed interest in the playwright as a model for contemporary writers. Much of Coleridge's reputation as a literary critic is founded on the lectures that he undertook in the winter of 1810–11, which were sponsored by the Philosophical Institution and given at Scot's Corporation Hall off Fetter Lane, Fleet Street. These lectures were heralded in the prospectus as "A Course of Lectures on Shakespeare and Milton, in Illustration of the Principles of Poetry." Coleridge's ill-health, opium-addiction problems, and somewhat unstable personality meant that all his lectures were plagued with problems of delays and a general irregularity of quality from one lecture to the next. As a result of these factors, Coleridge often failed to prepare anything but the loosest set of notes for his lectures and regularly entered into extremely long digressions which his audiences found difficult to follow. However, it was the lecture on "Hamlet" given on 2 January 1812 that was considered the best and has influenced "Hamlet" studies ever since. Before Coleridge, "Hamlet" was often denigrated and belittled by critics from Voltaire to Dr. Johnson. Coleridge rescued the play's reputation, and his thoughts on it are often still published as supplements to the text.

In August 1814, Coleridge was approached by Lord Byron's publisher, John Murray, about the possibility of translating Goethe's classic "Faust" (1808). Coleridge was regarded by many as the greatest living writer on the demonic and he accepted the commission, only to abandon work on it after six weeks. Until recently, scholars were in agreement that Coleridge never returned to the project, despite Goethe's own belief in the 1820s that he had in fact completed a long translation of the work. In September 2007, Oxford University Press sparked a heated scholarly controversy by publishing an English translation of Goethe's work that purported to be Coleridge's long-lost masterpiece (the text in question first appeared anonymously in 1821).

Between 1814 and 1816, Coleridge lived in Calne, Wiltshire and seemed able to focus on his work and manage his addiction, drafting "Biographia Literaria". He rented rooms from a local surgeon, Mr Page, on Church Street, just opposite the entrance to the churchyard. A blue plaque marks the property today.

In April 1816, Coleridge, with his addiction worsening, his spirits depressed, and his family alienated, took residence in the Highgate homes, then just north of London, of the physician James Gillman, first at South Grove and later at the nearby 3 The Grove. It is unclear whether his growing use of opium (and the brandy in which it was dissolved) was a symptom or a cause of his growing depression. Gillman was partially successful in controlling the poet's addiction. Coleridge remained in Highgate for the rest of his life, and the house became a place of literary pilgrimage for writers including Carlyle and Emerson.

In Gillman's home, Coleridge finished his major prose work, the "Biographia Literaria" (mostly drafted in 1815, and finished in 1817), a volume composed of 23 chapters of autobiographical notes and dissertations on various subjects, including some incisive literary theory and criticism. He composed a considerable amount of poetry, of variable quality. He published other writings while he was living at the Gillman homes, notably the "Lay Sermons" of 1816 and 1817, "Sibylline Leaves" (1817), "Hush" (1820), "Aids to Reflection" (1825), and "On the Constitution of the Church and State" (1830). He also produced essays published shortly after his death, such as "Essay on Faith" (1838) and "Confessions Of An Inquiring Spirit" (1840). A number of his followers were central to the Oxford Movement, and his religious writings profoundly shaped Anglicanism in the mid-nineteenth century.

Coleridge also worked extensively on the various manuscripts which form his "Opus Maximum", a work which was in part intended as a post-Kantian work of philosophical synthesis. The work was never published in his lifetime, and has frequently been seen as evidence for his tendency to conceive grand projects which he then had difficulty in carrying through to completion. But while he frequently berated himself for his "indolence", the long list of his published works calls this myth into some question. Critics are divided on whether the "Opus Maximum", first published in 2002, successfully resolved the philosophical issues he had been exploring for most of his adult life.

Coleridge died in Highgate, London on 25 July 1834 as a result of heart failure compounded by an unknown lung disorder, possibly linked to his use of opium. Coleridge had spent 18 years under the roof of the Gillman family, who built an addition onto their home to accommodate the poet.Faith may be defined as fidelity to our own being, so far as such being is not and cannot become an object of the senses; and hence, by clear inference or implication to being generally, as far as the same is not the object of the senses; and again to whatever is affirmed or understood as the condition, or concomitant, or consequence of the same. This will be best explained by an instance or example. That I am conscious of something within me peremptorily commanding me to do unto others as I would they should do unto me; in other words a categorical (that is, primary and unconditional) imperative; that the maxim ("regula maxima", or supreme rule) of my actions, both inward and outward, should be such as I could, without any contradiction arising therefrom, will to be the law of all moral and rational beings. "Essay On Faith"

Carlyle described him at Highgate: "Coleridge sat on the brow of Highgate Hill, in those years, looking down on London and its smoke-tumult, like a sage escaped from the inanity of life's battle ... The practical intellects of the world did not much heed him, or carelessly reckoned him a metaphysical dreamer: but to the rising spirits of the young generation he had this dusky sublime character; and sat there as a kind of "Magus", girt in mystery and enigma; his Dodona oak-grove (Mr. Gilman's house at Highgate) whispering strange things, uncertain whether oracles or jargon." 

Coleridge is buried in the aisle of St. Michael's Parish Church in Highgate, London. He was originally buried at Old Highgate Chapel but was re-interred in St. Michael's in 1961. Coleridge could see the red door of the then new church from his last residence across the green, where he lived with a doctor he'd hoped might cure him (in a house owned today by Kate Moss). When it was discovered Coleridge’s vault had become derelict, the coffins – Coleridge's and those of his wife, daughter, son-in-law, and grandson – were moved to St Michael’s after an international fundraising appeal.

Drew Clode, a member of St. Michael’s stewardship committee states, “they put the coffins in a convenient space which was dry and secure, and quite suitable, bricked them up and forgot about them”. A recent excavation revealed the coffins were not in the location most believed, the far corner of the crypt, but actually below a memorial slab in the nave inscribed with: “Beneath this stone lies the body of Samuel Taylor Coleridge”. 

St. Michael's plans to restore the crypt and allow public access. Says vicar Kunle Ayodeji of the plans: “. . we hope that the whole crypt can be cleared as a space for meetings and other uses, which would also allow access to Coleridge’s cellar.”

Coleridge is one of the most important figures in English poetry. His poems directly and deeply influenced all the major poets of the age. He was known by his contemporaries as a meticulous craftsman who was more rigorous in his careful reworking of his poems than any other poet, and Southey and Wordsworth were dependent on his professional advice. His influence on Wordsworth is particularly important because many critics have credited Coleridge with the very idea of "Conversational Poetry". The idea of utilising common, everyday language to express profound poetic images and ideas for which Wordsworth became so famous may have originated almost entirely in Coleridge’s mind. It is difficult to imagine Wordsworth’s great poems, "The Excursion" or "The Prelude", ever having been written without the direct influence of Coleridge’s originality.

As important as Coleridge was to poetry as a poet, he was equally important to poetry as a critic. His philosophy of poetry, which he developed over many years, has been deeply influential in the field of literary criticism. This influence can be seen in such critics as A. O. Lovejoy and I. A. Richards.

Coleridge is probably best known for his long poems, "The Rime of the Ancient Mariner" and "Christabel". Even those who have never read the "Rime" have come under its influence: its words have given the English language the metaphor of an albatross around one's neck, the quotation of "water, water everywhere, nor any drop to drink" (almost always rendered as "but not a drop to drink"), and the phrase "a sadder and a wiser man" (again, usually rendered as "a sadder but wiser man"). The phrase "All creatures great and small" may have been inspired by "The Rime": "He prayeth best, who loveth best;/ All things both great and small;/ For the dear God who loveth us;/ He made and loveth all." "Christabel" is known for its musical rhythm, language, and its Gothic tale.

"Kubla Khan", or, "A Vision in a Dream, A Fragment", although shorter, is also widely known. Both "Kubla Khan" and "Christabel" have an additional "Romantic" aura because they were never finished. Stopford Brooke characterised both poems as having no rival due to their "exquisite metrical movement" and "imaginative phrasing."

The eight of Coleridge's poems listed above are now often discussed as a group entitled "Conversation poems". The term itself was coined in 1928 by George McLean Harper, who borrowed the subtitle of "The Nightingale: A Conversation Poem" (1798) to describe the seven other poems as well. The poems are considered by many critics to be among Coleridge's finest verses; thus Harold Bloom has written, "With "Dejection", "The Ancient Mariner", and "Kubla Khan", "Frost at Midnight" shows Coleridge at his most impressive." They are also among his most influential poems, as discussed further below.

Harper himself considered that the eight poems represented a form of blank verse that is "...more fluent and easy than Milton's, or any that had been written since Milton". In 2006 Robert Koelzer wrote about another aspect of this apparent "easiness", noting that Conversation poems such as "... Coleridge's "The Eolian Harp" and "The Nightingale" maintain a middle register of speech, employing an idiomatic language that is capable of being construed as un-symbolic and un-musical: language that lets itself be taken as 'merely talk' rather than rapturous 'song'."

The last ten lines of "Frost at Midnight" were chosen by Harper as the "best example of the peculiar kind of blank verse Coleridge had evolved, as natural-seeming as prose, but as exquisitely artistic as the most complicated sonnet." The speaker of the poem is addressing his infant son, asleep by his side:
<poem>
Therefore all seasons shall be sweet to thee,
Whether the summer clothe the general earth
With greenness, or the redbreast sit and sing
Betwixt the tufts of snow on the bare branch
Of mossy apple-tree, while the nigh thatch
Smokes in the sun-thaw; whether the eave-drops fall
Heard only in the trances of the blast,
Or if the secret ministry of frost
Shall hang them up in silent icicles,
Quietly shining to the quiet Moon.
</poem>

In 1965, M. H. Abrams wrote a broad description that applies to the Conversation poems: "The speaker begins with a description of the landscape; an aspect or change of aspect in the landscape evokes a varied by integral process of memory, thought, anticipation, and feeling which remains closely intervolved with the outer scene. In the course of this meditation the lyric speaker achieves an insight, faces up to a tragic loss, comes to a moral decision, or resolves an emotional problem. Often the poem rounds itself to end where it began, at the outer scene, but with an altered mood and deepened understanding which is the result of the intervening meditation." In fact, Abrams was describing both the Conversation poems and later poems influenced by them. Abrams' essay has been called a "touchstone of literary criticism". As Paul Magnuson described it in 2002, "Abrams credited Coleridge with originating what Abrams called the 'greater Romantic lyric', a genre that began with Coleridge's 'Conversation' poems, and included Wordsworth's "Tintern Abbey", Shelley's "Stanzas Written in Dejection" and Keats's "Ode to a Nightingale", and was a major influence on more modern lyrics by Matthew Arnold, Walt Whitman, Wallace Stevens, and W. H. Auden."

In addition to his poetry, Coleridge also wrote influential pieces of literary criticism including "Biographia Literaria", a collection of his thoughts and opinions on literature which he published in 1817. The work delivered both biographical explanations of the author's life as well as his impressions on literature. The collection also contained an analysis of a broad range of philosophical principles of literature ranging from Aristotle to Immanuel Kant and Schelling and applied them to the poetry of peers such as William Wordsworth. Coleridge's explanation of metaphysical principles were popular topics of discourse in academic communities throughout the 19th and 20th centuries, and T.S. Eliot stated that he believed that Coleridge was "perhaps the greatest of English critics, and in a sense the last." Eliot suggests that Coleridge displayed "natural abilities" far greater than his contemporaries, dissecting literature and applying philosophical principles of metaphysics in a way that brought the subject of his criticisms away from the text and into a world of logical analysis that mixed logical analysis and emotion. However, Eliot also criticises Coleridge for allowing his emotion to play a role in the metaphysical process, believing that critics should not have emotions that are not provoked by the work being studied. Hugh Kenner in "Historical Fictions", discusses Norman Fruman's "Coleridge, the Damaged Archangel" and suggests that the term "criticism" is too often applied to "Biographia Literaria", which both he and Fruman describe as having failed to explain or help the reader understand works of art. To Kenner, Coleridge's attempt to discuss complex philosophical concepts without describing the rational process behind them displays a lack of critical thinking that makes the volume more of a biography than a work of criticism.

In "Biographia Literaria" and his poetry, symbols are not merely "objective correlatives" to Coleridge, but instruments for making the universe and personal experience intelligible and spiritually covalent. To Coleridge, the "cinque spotted spider," making its way upstream "by fits and starts," [Biographia Literaria] is not merely a comment on the intermittent nature of creativity, imagination, or spiritual progress, but the journey and destination of his life. The spider's five legs represent the central problem that Coleridge lived to resolve, the conflict between Aristotelian logic and Christian philosophy. Two legs of the spider represent the "me-not me" of thesis and antithesis, the idea that a thing cannot be itself and its opposite simultaneously, the basis of the clockwork Newtonian world view that Coleridge rejected. The remaining three legs—exothesis, mesothesis and synthesis or the Holy trinity—represent the idea that things can diverge without being contradictory. Taken together, the five legs—with synthesis in the center, form the Holy Cross of Ramist logic. The cinque-spotted spider is Coleridge's emblem of holism, the quest and substance of Coleridge's thought and spiritual life.

Coleridge wrote reviews of Ann Radcliffe's books and "The Mad Monk", among others. He comments in his reviews: "Situations of torment, and images of naked horror, are easily conceived; and a writer in whose works they abound, deserves our gratitude almost equally with him who should drag us by way of sport through a military hospital, or force us to sit at the dissecting-table of a natural philosopher. To trace the nice boundaries, beyond which terror and sympathy are deserted by the pleasurable emotions, – to reach those limits, yet never to pass them, hic labor, hic opus est." and "The horrible and the preternatural have usually seized on the popular taste, at the rise and decline of literature. Most powerful stimulants, they can never be required except by the torpor of an unawakened, or the languor of an exhausted, appetite... We trust, however, that satiety will banish what good sense should have prevented; and that, wearied with fiends, incomprehensible characters, with shrieks, murders, and subterraneous dungeons, the public will learn, by the multitude of the manufacturers, with how little expense of thought or imagination this species of composition is manufactured."

However, Coleridge used these elements in poems such as "The Rime of the Ancient Mariner" (1798), "Christabel" and "Kubla Khan" (published in 1816, but known in manuscript form before then) and certainly influenced other poets and writers of the time. Poems like these both drew inspiration from and helped to inflame the craze for Gothic romance. Coleridge also made considerable use of Gothic elements in his commercially successful play "Remorse".

Mary Shelley, who knew Coleridge well, mentions "The Rime of the Ancient Mariner" twice directly in "Frankenstein", and some of the descriptions in the novel echo it indirectly. Although William Godwin, her father, disagreed with Coleridge on some important issues, he respected his opinions and Coleridge often visited the Godwins. Mary Shelley later recalled hiding behind the sofa and hearing his voice chanting "The Rime of the Ancient Mariner".

Despite being mostly remembered today for his poetry and literary criticism, Coleridge was also (perhaps in his own eyes primarily) a theologian. His writings include discussions of the status of Scripture, the doctrines of the Fall, justification and sanctification, and the personality and infinity of God. A key figure in the Anglican theology of his day, his writings are still regularly referred to by contemporary Anglican theologians, and F. D. Maurice, F. J. A. Hort, F. W. Robertson, B. F. Westcott, John Oman, and Thomas Erskine (once called the "Scottish Coleridge") were all influenced by him.

Coleridge was also a profound political thinker. While he began his life as a political radical, and an enthusiast for the French Revolution; over the years Coleridge developed a more conservative view of society, somewhat in the manner of Burke. Although seen as cowardly treachery by the next generation of Romantic poets, Coleridge’s later thought became a fruitful source for the evolving radicalism of J. S. Mill. Mill found three aspects of Coleridge’s thought especially illuminating:


A current standard edition is "The Collected Works of Samuel Taylor Coleridge," edited by Kathleen Coburn and many others from 1969 to 2002. This collection appeared across 16 volumes as Bollingen Series 75, published variously by Princeton University Press and Routledge & Kegan Paul. The set is broken down as follows into further parts, resulting in a total of 34 separate printed volumes :





</doc>
<doc id="29409" url="https://en.wikipedia.org/wiki?curid=29409" title="Spica">
Spica

Spica, also designated Alpha Virginis (α Virginis, abbreviated Alpha Vir, α Vir), is the brightest star in the constellation of Virgo and the 16th brightest star in the night sky. Analysis of its parallax shows that it is located 250 ± 10 light years from the Sun. It is a spectroscopic binary and rotating ellipsoidal variable; a system whose two main stars are so close together they are egg-shaped rather than spherical, and can only be separated by their spectra. The primary is a blue giant and a variable star of the Beta Cephei type.

Spica, along with Denebola or Regulus depending on the source and Arcturus, is part of the Spring Triangle asterism, and by extension, also of the Great Diamond together with the star Cor Caroli.

As one of the nearest massive binary star systems to the Sun, Spica has been the subject of many observational studies.

Spica is believed to be the star that gave Hipparchus the data that led him to discover the precession of the equinoxes. A temple to Menat (an early Hathor) at Thebes was oriented with reference to Spica when it was built in 3200 BC, and, over time, precession slowly but noticeably changed Spica's location relative to the temple. Nicolaus Copernicus made many observations of Spica with his home-made triquetrum for his researches on precession.

Spica is 2.06 degrees from the ecliptic and can be occulted by the Moon and sometimes by the planets. The last planetary occultation of Spica occurred when Venus passed in front of the star (as seen from Earth) on November 10, 1783. The next occultation will occur on September 2, 2197, when Venus again passes in front of Spica. The Sun passes a little more than 2° north of Spica around October 16 every year, and the star's heliacal rising occurs about two weeks later. Every 8 years, Venus passes Spica around the time of the star's heliacal rising, as in 2009 when it passed 3.5° north of the star on November 3.

A method of finding Spica is to follow the arc of the handle of the Big Dipper (Plough) to Arcturus, and then continue on the same angular distance to Spica. This can be recalled by the mnemonic phrase, "arc to Arcturus and spike to Spica."

Spica is a close binary star whose components orbit about each other every four days. They stay close enough together that they cannot be resolved as two stars through a telescope. The changes in the orbital motion of this pair results in a Doppler shift in the absorption lines of their respective spectra, making them a double-lined spectroscopic binary. Initially, the orbital parameters for this system were inferred using spectroscopic measurements. Between 1966 and 1970, the Narrabri Stellar Intensity Interferometer was used to observe the pair and to directly measure the orbital characteristics and the angular diameter of the primary, which was found to be , and the angular size of the semi-major axis of the orbit was found to be only slightly larger at .

The primary star has a stellar classification of B1 III–IV. The luminosity class matches the spectrum of a star that is midway between a subgiant and a giant star, and it is no longer a B-type main-sequence star. This is a massive star with more than 10 times the mass of the Sun and seven times the Sun's radius. The total luminosity of this star is about 12,100 times that of the Sun, and eight times the luminosity of its companion. The primary is one of the nearest stars to the Sun that has enough mass to end its life in a Type II supernova explosion.

The primary is classified as a Beta Cephei-type variable star that varies in brightness over a 0.1738-day period. The spectrum shows a radial velocity variation with the same period, indicating that the surface of the star is regularly pulsating outward and then contracting. This star is rotating rapidly, with a rotational velocity of 199 km/s along the equator.

The secondary member of this system is one of the few stars whose spectrum is affected by the Struve–Sahade effect. This is an anomalous change in the strength of the spectral lines over the course of an orbit, where the lines become weaker as the star is moving away from the observer. It may be caused by a strong stellar wind from the primary scattering the light from secondary when it is receding. This star is smaller than the primary, with about 7 times the mass of the Sun and 3.6 times the Sun's radius. Its stellar classification is B2 V, making this a main-sequence star.

Spica is a rotating ellipsoidal variable, which is a non-eclipsing close binary star system where the stars are mutually distorted through their gravitational interaction. This effect causes the apparent magnitude of the star system to vary by 0.03 over an interval that matches the orbital period. This slight dip in magnitude is barely noticeable visually. Both stars rotate faster than their mutual orbital period. This lack of synchronization and the high ellipticity of their orbit may indicate that this is a young star system. Over time, the mutual tidal interaction of the pair may lead to rotational synchronization and orbit circularization.

Spica is an polarimetric variable, which suggests that protostellar material is entrained between the two stars.

"α Virginis" (Latinised to "Alpha Virginis") is the system's Bayer designation.

The traditional name "Spica" derives from Latin "spīca virginis" "the virgin's ear of [wheat] grain". It was also anglicized as "Virgin's Spike".
Johann Bayer cited the name "Arista".

Other traditional names are "Azimech", from Arabic السماك الأعزل "al-simāk al-a‘zal", 'the Undefended'; "Alarph", Arabic for 'the Grape Gatherer', and "Sumbalet" ("Sombalet", "Sembalet" and variants), from Arabic "sunbulah" "corn ear". 

In 2016, the International Astronomical Union organized a Working Group on Star Names (WGSN) to catalog and standardize proper names for stars. The WGSN's first bulletin of July 2016 included a table of the first two batches of names approved by the WGSN; which included "Spica" for this star. It is now so entered in the IAU Catalog of Star Names.

In Chinese, (), meaning "Horn (asterism)", refers to an asterism consisting of Spica and ζ Virginis. Consequently, Spica are known as (, ).

In Hindu astronomy, Spica corresponds to the Nakshatra "Chitrā".

Both American ships USS "Spica" (AK-16) and USNS "Spica" (T-AFS-9) were named after this star while USS "Azimech" (AK-124), a "Crater"-class cargo ship, was given one of the star's medieval names.

A blue star represents Spica on the of the Brazilian state of Pará. Spica is also the star representing Pará on the Brazilian flag.

A South Korean Girl Group was named after the star.

Spica is a Vocaloid song sung by Hatsune Miku

In a non-canonical chapter in , Subaru had a daughter with Rem named Spica.

Spica is the pseudonym of Lili in the children's manga series, "Zodiac P.I."

In his "Three Books of Occult Philosophy", Cornelius Agrippa attributes Spica's kabbalistic symbol to Hermes Trismegistus.


</doc>
<doc id="29414" url="https://en.wikipedia.org/wiki?curid=29414" title="Stuart Little">
Stuart Little

Stuart Little is a 1945 American children's novel by E. B. White, his first book for children, and is widely recognized as a classic in children's literature. "Stuart Little" was illustrated by the subsequently award-winning artist Garth Williams, also his first work for children. It is a realistic fantasy about Stuart Little who, though born to human parents in New York City, ″looked very much like a rat/mouse in every way″ (chapter I).

In a letter White wrote in response to inquiries from readers, he described how he came to conceive of Stuart Little: "many years ago I went to bed one night in a railway sleeping car, and during the night I dreamed about a tiny boy who acted rather like a rat. That's how the story of Stuart Little got started". He had the dream in the spring of 1926, while sleeping on a train on his way back to New York from a visit to the Shenandoah Valley. Biographer Michael Sims wrote that Stuart "arrived in [White's] mind in a direct shipment from the subconscious." White typed up a few stories about Stuart, which he told to his 18 nieces and nephews when they asked him to tell them a story. In 1935, White's wife Katharine showed these stories to Clarence Day, then a regular contributor to "The New Yorker". Day liked the stories and encouraged White not to neglect them, but neither Oxford University Press nor Viking Press was interested in the stories, and White did not immediately develop them further.

In the fall of 1938, as his wife wrote her annual collection of children's book reviews for "The New Yorker", White wrote a few paragraphs in his "One Man's Meat" column in "Harper's Magazine" about writing children's books. Anne Carroll Moore, the head children's librarian at the New York Public Library, read this column and responded by encouraging him to write a children's book that would "make the library lions roar". White's editor at Harper, who had heard about the Stuart stories from Katherine, asked to see them, and by March 1939 was intent on publishing them. Around that time, White wrote to James Thurber that he was "about half done" with the book; however, he made little progress with it until the winter of 1944-1945.

When Stuart is born to an ordinary family in New York City, he is normal in every way except that he is only four inches high and looks exactly like a mouse. At first the family is concerned with how Stuart will survive in a human-sized world, but by the age of seven, he speaks, thinks, and behaves on the level of a human of sixteen and shows surprising ingenuity in adapting, performing such helpful family tasks as fishing his mother's wedding ring from a sink drain. The family's cat, Snowball, dislikes Stuart because while he feels a natural instinct to chase him, he is aware that Stuart is a member of the human family and thus off-limits. 

On a cold winter's day, the family discovers a songbird named Margalo half-frozen on their doorstep. Margalo is taken in and spends the winter in the family, where she befriends Stuart; Stuart in turn protects her from Snowball. The bird repays his kindness by saving Stuart when he is trapped in a garbage can and shipped out to sea for disposal. In the spring, when she is set free from the house, she continues to visit Stuart, infuriating Snowball, who now finds himself with two small animals he is not allowed to eat.

Snowball makes a deal with the neighbor's cat to eat Margalo to get rid of one of his temptations (reasoning that it's only wrong if "he" eats her). Margalo is warned in advance and flees in the middle of the night. Stuart is heartbroken but becomes determined to find her. He first goes to the local dentist, who is a friend of the Little family. The dentist suggests that Margalo may have flown to Connecticut, and loans Stuart his motorized, gas-powered toy car for the long journey. 

Stuart travels from adventure to adventure and finds himself in the town of Ames Crossing, where he takes work as a substitute teacher. There he learns that living in Ames Crossing is a fifteen-year-old girl named Harriet Ames who is the same size as Stuart but looks like a human being. Stuart purchases a miniature souvenir canoe, prepping it to make it comfortable and waterproof, and invites Harriet out on a boating date. However, when the two arrive for the date, the canoe has been discovered and played with by local children, who have ruined it. Harriet tries to be polite but is put-off by Stuart's sulking over his broken boat. Stuart decides to leave Ames Crossing and continue on his quest to find Margalo. He sets off once more in his toy car, positive that he will see her again.

Lucien Agosta, in his overview of the critical reception of the book, notes that "Critical reactions to "Stuart Little" have varied from disapprobation to unqualified admiration since the book was published in 1945, though generally it has been well received." Anne Carroll Moore, who had initially encouraged White to write the book, was critical of it when she read a proof of it. She wrote letters to White; his wife, Katharine; and Ursula Nordstrom, the children's editors at Harper's, advising that the book not be published.

Malcolm Cowley, who reviewed the book for "The New York Times", wrote, "Mr. White has a tendency to write amusing scenes instead of telling a story. To say that "Stuart Little" is one of the best children's books published this year is very modest praise for a writer of his talent." The book has become a children's classic, and is widely read by children and used by teachers. White received the Laura Ingalls Wilder Medal in 1970 for "Stuart Little" and "Charlotte's Web".

The book was very loosely adapted into a 1999 film of the same name, which combined live-action with computer animation. A 2002 sequel to the first film, "Stuart Little 2", was truer to the book. A third film, "" was released direct-to-video in 2006. This film was entirely computer-animated, and its plot was not derived from the book. All three films feature Hugh Laurie as Mr. Little, Geena Davis as Mrs. Little, and Michael J. Fox as the voice of Stuart Little.

In 2015, it was announced that a remake of "Stuart Little" film is in the works at Sony Pictures Entertainment and Red Wagon Entertainment. The movie will remain as a hybrid of live-action/computer animation. Douglas Wick, the producer of the original films will produce the remake.

"The World of Stuart Little," a 1966 episode of NBC's "Children's Theater", narrated by Johnny Carson, won a Peabody Award and was nominated for an Emmy. An animated television series, "", (based on the film adaptations) was produced for HBO Family and aired for 13 episodes in 2003.

Three video games based on Stuart Little were produced, but were mostly based off the film adaptations of the same name. "Stuart Little: the Journey Home" based on the 1999 film, was released only for the Game Boy Color in 2001. A game based on "Stuart Little 2" was released for the PlayStation, Game Boy Advance and Microsoft Windows in 2002. And a third game entitled "Stuart Little 3: Big Photo Adventure" was released exclusively for the PlayStation 2 in 2005. Although it bears no resemblance to the film "".




</doc>
<doc id="29417" url="https://en.wikipedia.org/wiki?curid=29417" title="Statite">
Statite

A statite (a portmanteau of "static" and "satellite") is a hypothetical type of artificial satellite that employs a solar sail to continuously modify its orbit in ways that gravity alone would not allow. Typically, a statite would use the solar sail to "hover" in a location that would not otherwise be available as a stable geosynchronous orbit. Statites have been proposed that would remain in fixed locations high over Earth's poles, using reflected sunlight to counteract the gravity pulling them down. Statites might also employ their sails to change the shape or velocity of more conventional orbits, depending upon the purpose of the particular statite.

The concept of the statite was invented independently (and approximately simultaneously) by Robert L. Forward (who coined the term "statite"), and by Colin McInnes, who used the term "halo orbit" (not to be confused with the type of halo orbit invented by Robert Farquhar). Subsequently, the terms "non-Keplerian orbit" and "artificial Lagrange point" have been used as a generalization of the above terms.

No statites have been deployed to date, as solar sail technology is still in its infancy. NASA's cancelled Sunjammer solar sail mission had the stated objective of flying to an artificial Lagrange point near the Earth/Sun L1 point, to demonstrate the feasibility of the Geostorm geomagnetic storm warning mission concept proposed by NOAA's Patricia Mulligan.



</doc>
<doc id="29419" url="https://en.wikipedia.org/wiki?curid=29419" title="Stanford (disambiguation)">
Stanford (disambiguation)

Stanford may refer to: 








</doc>
<doc id="29420" url="https://en.wikipedia.org/wiki?curid=29420" title="Solar sail">
Solar sail

Solar sails (also called light sails or photon sails) are a proposed method of spacecraft propulsion using radiation pressure exerted by sunlight on large mirrors. A useful analogy may be a sailing boat; the light exerting a force on the mirrors is akin to a sail being blown by the wind. High-energy laser beams could be used as an alternative light source to exert much greater force than would be possible using sunlight, a concept known as beam sailing.

Solar sail craft offer the possibility of low-cost operations combined with long operating lifetimes. Since they have few moving parts and use no propellant, they can potentially be used numerous times for delivery of payloads.

Solar sails use a phenomenon that has a proven, measured effect on spacecraft. Solar pressure affects all spacecraft, whether in interplanetary space or in orbit around a planet or small body. A typical spacecraft going to Mars, for example, will be displaced thousands of kilometers by solar pressure, so the effects must be accounted for in trajectory planning, which has been done since the time of the earliest interplanetary spacecraft of the 1960s. Solar pressure also affects the orientation of a craft, a factor that must be included in spacecraft design.

The total force exerted on an 800 by 800 meter solar sail, for example, is about at Earth's distance from the Sun, making it a low-thrust propulsion system, similar to spacecraft propelled by electric engines, but as it uses no propellant, that force is exerted almost constantly and the collective effect over time is great enough to be considered a potential manner of propelling spacecraft.

Johannes Kepler observed that comet tails point away from the Sun and suggested that the Sun caused the effect. In a letter to Galileo in 1610, he wrote, "Provide ships or sails adapted to the heavenly breezes, and there will be some who will brave even that void." He might have had the comet tail phenomenon in mind when he wrote those words, although his publications on comet tails came several years later

James Clerk Maxwell, in 1861–64, published his theory of electromagnetic fields and radiation, which shows that light has momentum and thus can exert pressure on objects. Maxwell's equations provide the theoretical foundation for sailing with light pressure. So by 1864, the physics community and beyond knew sunlight carried momentum that would exert a pressure on objects.

Jules Verne, in "From the Earth to the Moon", published in 1865, wrote "there will some day appear velocities far greater than these [of the planets and the projectile], of which light or electricity will probably be the mechanical agent ... we shall one day travel to the moon, the planets, and the stars." This is possibly the first published recognition that light could move ships through space.

Pyotr Lebedev was first to successfully demonstrate light pressure, which he did in 1899 with a torsional balance; Ernest Nichols and Gordon Hull conducted a similar independent experiment in 1901 using a Nichols radiometer.

Svante Arrhenius predicted in 1908 the possibility of solar radiation pressure distributing life spores across interstellar distances, providing one means to explain the concept of panspermia. He apparently was the first scientist to state that light could move objects between stars.

Konstantin Tsiolkovsky first proposed using the pressure of sunlight to propel spacecraft through space and suggested, "using tremendous mirrors of very thin sheets to utilize the pressure of sunlight to attain cosmic velocities".

Friedrich Zander (Tsander) published a technical paper in 1925 that included technical analysis of solar sailing. Zander wrote of "applying small forces" using "light pressure or transmission of light energy to distances by means of very thin mirrors".

JBS Haldane speculated in 1927 about the invention of tubular spaceships that would take humanity to space and how "wings of metallic foil of a square kilometre or more in area are spread out to catch the Sun's radiation pressure".

J.D. Bernal wrote in 1929, "A form of space sailing might be developed which used the repulsive effect of the Sun's rays instead of wind. A space vessel spreading its large, metallic wings, acres in extent, to the full, might be blown to the limit of Neptune's orbit. Then, to increase its speed, it would tack, close-hauled, down the gravitational field, spreading full sail again as it rushed past the Sun."

The first formal technology and design effort for a solar sail began in 1976 at Jet Propulsion Laboratory for a proposed mission to rendezvous with Halley's Comet.

Many people believe that spacecraft using solar sails are pushed by the Solar winds just as sailboats and sailing ships are pushed by the winds across the waters on Earth. But Solar radiation exerts a pressure on the sail due to reflection and a small fraction that is absorbed.

The momentum of a photon or an entire flux is given by Einstein's relation:

where p is the momentum, E is the energy (of the photon or flux), and c is the speed of light. Solar radiation pressure can be related to the irradiance (solar constant) value of 1361 W/m at 1 AU (Earth-Sun distance), as revised in 2011:


An ideal sail is flat and has 100% specular reflection. An actual sail will have an overall efficiency of about 90%, about 8.17 μN/m, due to curvature (billow), wrinkles, absorbance, re-radiation from front and back, non-specular effects, and other factors.

The force on a sail and the actual acceleration of the craft vary by the inverse square of distance from the Sun (unless extremely close to the Sun), and by the square of the cosine of the angle between the sail force vector and the radial from the Sun, so

where R is distance from the Sun in AU. An actual square sail can be modeled as:

Note that the force and acceleration approach zero generally around θ = 60° rather than 90° as one might expect with an ideal sail.

If some of the energy is absorbed, the absorbed energy will heat the sail, which re-radiates that energy from the front and rear surfaces, depending on the emissivity of those two surfaces.

Solar wind, the flux of charged particles blown out from the Sun, exerts a nominal dynamic pressure of about 3 to 4 nPa, three orders of magnitude less than solar radiation pressure on a reflective sail.

Sail loading (areal density) is an important parameter, which is the total mass divided by the sail area, expressed in g/m. It is represented by the Greek letter σ.

A sail craft has a characteristic acceleration, a, which it would experience at 1 AU when facing the Sun. Using the value from above of 9.08 μN per square metre of radiation pressure at 1 AU, a is related to areal density by:

Assuming 90% efficiency, a = 8.17 / σ mm/s

The lightness number, λ, is the dimensionless ratio of maximum vehicle acceleration divided by the Sun's local gravity. Using the values at 1 AU:

The lightness number is also independent of distance from the Sun because both gravity and light pressure fall off as the inverse square of the distance from the Sun. Therefore, this number defines the types of orbit maneuvers that are possible for a given vessel.

The table presents some example values. Payloads are not included. The first two are from the detailed design effort at JPL in the 1970s. The third, the lattice sailer, might represent about the best possible performance level. The dimensions for square and lattice sails are edges. The dimension for heliogyro is blade tip to blade tip.

An active attitude control system (ACS) is essential for a sail craft to achieve and maintain a desired orientation. The required sail orientation changes slowly (often less than 1 degree per day) in interplanetary space, but much more rapidly in a planetary orbit. The ACS must be capable of meeting these orientation requirements. Attitude control is achieved by a relative shift between the craft's center of pressure and its center of mass. This can be achieved with control vanes, movement of individual sails, movement of a control mass, or altering reflectivity.

Holding a constant attitude requires that the ACS maintain a net torque of zero on the craft. The total force and torque on a sail, or set of sails, is not constant along a trajectory. The force changes with solar distance and sail angle, which changes the billow in the sail and deflects some elements of the supporting structure, resulting in changes in the sail force and torque.

Sail temperature also changes with solar distance and sail angle, which changes sail dimensions. The radiant heat from the sail changes the temperature of the supporting structure. Both factors affect total force and torque.

To hold the desired attitude the ACS must compensate for all of these changes.

In Earth orbit, solar pressure and drag pressure are typically equal at an altitude of about 800 km, which means that a sail craft would have to operate above that altitude. Sail craft must operate in orbits where their turn rates are compatible with the orbits, which is generally a concern only for spinning disk configurations.

Sail operating temperatures are a function of solar distance, sail angle, reflectivity, and front and back emissivities. A sail can be used only where its temperature is kept within its material limits. Generally, a sail can be used rather close to the Sun, around 0.25 AU, or even closer if carefully designed for those conditions.

Potential applications for sail craft range throughout the Solar System, from near the Sun to the comet clouds beyond Neptune. The craft can make outbound voyages to deliver loads or to take up station keeping at the destination. They can be used to haul cargo and possibly also used for human travel.

For trips within the inner Solar System, they can deliver loads and then return to Earth for subsequent voyages, operating as an interplanetary shuttle. For Mars in particular, the craft could provide economical means of routinely supplying operations on the planet according to Jerome Wright, "The cost of launching the necessary conventional propellants from Earth are enormous for manned missions. Use of sailing ships could potentially save more than $10 billion in mission costs."

Solar sail craft can approach the Sun to deliver observation payloads or to take up station keeping orbits. They can operate at 0.25 AU or closer. They can reach high orbital inclinations, including polar.

Solar sails can travel to and from all of the inner planets. Trips to Mercury and Venus are for rendezvous and orbit entry for the payload. Trips to Mars could be either for rendezvous or swing-by with release of the payload for aerodynamic braking.

Minimum transfer times to the outer planets benefit from using an indirect transfer (solar swing-by). However, this method results in high arrival speeds. Slower transfers have lower arrival speeds.

The minimum transfer time to Jupiter for "a" of 1 mm/s with no departure velocity relative to Earth is 2 years when using an indirect transfer (solar swing-by). The arrival speed ("V") is close to 17 km/s. For Saturn, the minimum trip time is 3.3 years, with an arrival speed of nearly 19 km/s.

The Sun's inner gravitational focus point lies at minimum distance of 550 AU from the Sun, and is the point to which light from distant objects is focused by gravity as a result of it passing by the Sun. This is thus the distant point to which solar gravity will cause the region of deep space on the other side of the Sun to be focused, thus serving effectively as a very large telescope objective lens.

It has been proposed that an inflated sail, made of beryllium, that starts at 0.05 AU from the Sun would gain an initial acceleration of 36.4 m/s, and reach a speed of 0.00264c (about 950 km/s) in less than a day. Such proximity to the Sun could prove to be impractical in the near term due to the structural degradation of beryllium at high temperatures, diffusion of hydrogen at high temperatures as well as an electrostatic gradient, generated by the ionization of beryllium from the solar wind, posing a burst risk. A revised perihelion of 0.1 AU would reduce the aforementioned temperature and solar flux exposure.
Such a sail would take "Two and a half years to reach the heliopause, six and a half years to reach the Sun’s inner gravitational focus, with arrival at the inner Oort Cloud in no more than thirty years." "Such a mission could perform useful astrophysical observations en route, explore gravitational focusing techniques, and image Oort Cloud objects while exploring particles and fields in that region that are of galactic rather than solar origin."

Robert L. Forward has commented that a solar sail could be used to modify the orbit of a satellite about the Earth. In the limit, a sail could be used to "hover" a satellite above one pole of the Earth. Spacecraft fitted with solar sails could also be placed in close orbits such that they are stationary with respect to either the Sun or the Earth, a type of satellite named by Forward a "statite". This is possible because the propulsion provided by the sail offsets the gravitational attraction of the Sun. Such an orbit could be useful for studying the properties of the Sun for long durations. Likewise a solar sail-equipped spacecraft could also remain on station nearly above the polar solar terminator of a planet such as the Earth by tilting the sail at the appropriate angle needed to counteract the planet's gravity.

In his book "The Case for Mars", Robert Zubrin points out that the reflected sunlight from a large statite, placed near the polar terminator of the planet Mars, could be focused on one of the Martian polar ice caps to significantly warm the planet's atmosphere. Such a statite could be made from asteroid material.

The MESSENGER probe orbiting Mercury used light pressure on its solar panels to perform fine trajectory corrections on the way to Mercury. By changing the angle of the solar panels relative to the Sun, the amount of solar radiation pressure was varied to adjust the spacecraft trajectory more delicately than possible with thrusters. Minor errors are greatly amplified by gravity assist maneuvers, so using radiation pressure to make very small corrections saved large amounts of propellant.

In the 1970s, Robert Forward proposed two beam-powered propulsion schemes using either lasers or masers to push giant sails to a significant fraction of the speed of light.

In the science fiction novel "Rocheworld", Forward described a light sail propelled by super lasers. As the starship neared its destination, the outer portion of the sail would detach. The outer sail would then refocus and reflect the lasers back onto a smaller, inner sail. This would provide braking thrust to stop the ship in the destination star system.

Both methods pose monumental engineering challenges. The lasers would have to operate for years continuously at gigawatt strength. Forward's solution to this requires enormous solar panel arrays to be built at or near the planet Mercury. A planet-sized mirror or fresnel lens would need to be located at several dozen astronomical units from the Sun to keep the lasers focused on the sail. The giant braking sail would have to act as a precision mirror to focus the braking beam onto the inner "deceleration" sail.

A potentially easier approach would be to use a maser to drive a "solar sail" composed of a mesh of wires with the same spacing as the wavelength of the microwaves directed at the sail, since the manipulation of microwave radiation is somewhat easier than the manipulation of visible light. The hypothetical "Starwisp" interstellar probe design would use microwaves, rather than visible light, to push it. Masers spread out more rapidly than optical lasers owing to their longer wavelength, and so would not have as great an effective range.

Masers could also be used to power a painted solar sail, a conventional sail coated with a layer of chemicals designed to evaporate when struck by microwave radiation. The momentum generated by this evaporation could significantly increase the thrust generated by solar sails, as a form of lightweight ablative laser propulsion.

To further focus the energy on a distant solar sail, Forward proposed a lens designed as a large zone plate. This would be placed at a location between the laser or maser and the spacecraft.

Another more physically realistic approach would be to use the light from the Sun to accelerate. The ship would first drop into an orbit making a close pass to the Sun, to maximize the solar energy input on the sail, then it would begin to accelerate away from the system using the light from the Sun. Acceleration will drop approximately as the inverse square of the distance from the Sun, and beyond some distance, the ship would no longer receive enough light to accelerate it significantly, but would maintain the final velocity attained. When nearing the target star, the ship could turn its sails toward it and begin to use the outward pressure of the destination star to decelerate. Rockets could augment the solar thrust.

Similar solar sailing launch and capture were suggested for directed panspermia to expand life in other solar system. Velocities of 0.05% the speed of light could be obtained by solar sails carrying 10 kg payloads, using thin solar sail vehicles with effective areal densities of 0.1 g/m with thin sails of 0.1 µm thickness and sizes on the order of one square kilometer. Alternatively, swarms of 1 mm capsules could be launched on solar sails with radii of 42 cm, each carrying 10,000 capsules of a hundred million extremophile microorganisms to seed life in diverse target environments.

Small solar sails have been proposed to accelerate the deorbiting of small artificial satellites from Earth orbits. Satellites in low Earth orbit can use a combination of solar pressure on the sail and increased atmospheric drag to accelerate satellite reentry. A de-orbit sail developed at Cranfield University is part of the UK satellite TechDemoSat-1, launched in 2014, and is expected to be deployed at the end of the satellite's five-year useful life. The sail's purpose is to bring the satellite out of orbit over a period of about 25 years. In July 2015 British 3U CubeSat called DeorbitSail was launched into space with the purpose of testing 16 m deorbit structure, but eventually it failed to deploy it. There is also a student 2U CubeSat mission called PW-Sat2 planned to launch in 2017 that will test 4 m deorbit sail. In June 2017 a second British 3U CubeSat called InflateSail deployed a 10 m deorbit sail at an altitude of .
In June 2017 the 3U Cubesat URSAMAIOR has been launched in Low Earth Orbit to test the deorbiting system ARTICA developed by Spacemind . The device, which occupies only 0.4 U of the cubesat, shall deploy a sail of 2.1 m to deorbit the satellite at the end of the operational life 

IKAROS, launched in 2010, was the first practical solar sail vehicle. As of 2015, it was still under thrust, proving the practicality of a solar sail for long-duration missions. It is spin-deployed, with tip-masses in the corners of its square sail. The sail is made of thin polyimide film, coated with evaporated aluminium. It steers with electrically-controlled liquid crystal panels. The sail slowly spins, and these panels turn on and off to control the attitude of the vehicle. When on, they diffuse light, reducing the momentum transfer to that part of the sail. When off, the sail reflects more light, transferring more momentum. In that way, they turn the sail. Thin-film solar cells are also integrated into the sail, powering the spacecraft. The design is very reliable, because spin deployment, which is preferable for large sails, simplified the mechanisms to unfold the sail and the LCD panels have no moving parts.

Parachutes have very low mass, but a parachute is not a workable configuration for a solar sail. Analysis shows that a parachute configuration would collapse from the forces exerted by shroud lines, since radiation pressure does not behave like aerodynamic pressure, and would not act to keep the parachute open.

The highest thrust-to-mass designs for ground-assembled deploy-able structures are square sails with the masts and guy lines on the dark side of the sail. Usually there are four masts that spread the corners of the sail, and a mast in the center to hold guy-wires. One of the largest advantages is that there are no hot spots in the rigging from wrinkling or bagging, and the sail protects the structure from the Sun. This form can, therefore, go close to the Sun for maximum thrust. Most designs steer with small moving sails on the ends of the spars.

In the 1970s JPL studied many rotating blade and ring sails for a mission to rendezvous with Halley's Comet. The intention was to stiffen the structures using angular momentum, eliminating the need for struts, and saving mass. In all cases, surprisingly large amounts of tensile strength were needed to cope with dynamic loads. Weaker sails would ripple or oscillate when the sail's attitude changed, and the oscillations would add and cause structural failure. The difference in the thrust-to-mass ratio between practical designs was almost nil, and the static designs were easier to control.

JPL's reference design was called the "heliogyro". It had plastic-film blades deployed from rollers and held out by centrifugal forces as it rotated. The spacecraft's attitude and direction were to be completely controlled by changing the angle of the blades in various ways, similar to the cyclic and collective pitch of a helicopter. Although the design had no mass advantage over a square sail, it remained attractive because the method of deploying the sail was simpler than a strut-based design.

Heliogyro design is similar to the blades on a helicopter. The design is faster to manufacture due to lightweight centrifugal stiffening of sails. Also, they are highly efficient in cost and velocity because the blades are lightweight and long. Unlike the square and spinning disk designs, heliogyro is easier to deploy because the blades are compacted on a reel. The blades roll out when they are deploying after the ejection from the spacecraft. As the heliogyro travels through space the system spins around because of the centrifugal acceleration. Finally, payloads for the space flights are placed in the center of gravity to even out the distribution of weight to ensure stable flight.

JPL also investigated "ring sails" (Spinning Disk Sail in the above diagram), panels attached to the edge of a rotating spacecraft. The panels would have slight gaps, about one to five percent of the total area. Lines would connect the edge of one sail to the other. Masses in the middles of these lines would pull the sails taut against the coning caused by the radiation pressure. JPL researchers said that this might be an attractive sail design for large manned structures. The inner ring, in particular, might be made to have artificial gravity roughly equal to the gravity on the surface of Mars.

A solar sail can serve a dual function as a high-gain antenna. Designs differ, but most modify the metalization pattern to create a holographic monochromatic lens or mirror in the radio frequencies of interest, including visible light.

Pekka Janhunen from FMI has invented a type of solar sail called the electric solar wind sail. Mechanically it has little in common with the traditional solar sail design. The sails are replaced with straightened conducting tethers (wires) placed radially around the host ship. The wires are electrically charged to create an electric field around the wires. The electric field extends a few tens of metres into the plasma of the surrounding solar wind. The solar electrons are reflected by the electric field (like the photons on a traditional solar sail). The radius of the sail is from the electric field rather than the actual wire itself, making the sail lighter. The craft can also be steered by regulating the electric charge of the wires. A practical electric sail would have 50–100 straightened wires with a length of about 20 km each.

Electric solar wind sails can adjust their electrostatic fields and sail attitudes.

A magnetic sail would also employ the solar wind. However, the magnetic field deflects the electrically charged particles in the wind. It uses wire loops, and runs a static current through them instead of applying a static voltage.

All these designs maneuver, though the mechanisms are different.

Magnetic sails bend the path of the charged protons that are in the solar wind. By changing the sails' attitudes, and the size of the magnetic fields, they can change the amount and direction of the thrust.

The most common material in current designs is a thin layer of aluminum coating on a polymer (plastic) sheet, such as aluminized 2 µm Kapton film. The polymer provides mechanical support as well as flexibility, while the thin metal layer provides the reflectivity. Such material resists the heat of a pass close to the Sun and still remains reasonably strong. The aluminum reflecting film is on the Sun side. The sails of "Cosmos 1" were made of aluminized PET film (Mylar).

Eric Drexler developed a concept for a sail in which the polymer was removed. He proposed very high thrust-to-mass solar sails, and made prototypes of the sail material. His sail would use panels of thin aluminium film (30 to 100 nanometres thick) supported by a tensile structure. The sail would rotate and would have to be continually under thrust. He made and handled samples of the film in the laboratory, but the material was too delicate to survive folding, launch, and deployment. The design planned to rely on space-based production of the film panels, joining them to a deploy-able tension structure. Sails in this class would offer high area per unit mass and hence accelerations up to "fifty times higher" than designs based on deploy-able plastic films.
The material developed for the Drexler solar sail was a thin aluminium film with a baseline thickness of 0.1 µm, to be fabricated by vapor deposition in a space-based system. Drexler used a similar process to prepare films on the ground. As anticipated, these films demonstrated adequate strength and robustness for handling in the laboratory and for use in space, but not for folding, launch, and deployment.

Research by Geoffrey Landis in 1998–1999, funded by the NASA Institute for Advanced Concepts, showed that various materials such as alumina for laser lightsails and carbon fiber for microwave pushed lightsails were superior sail materials to the previously standard aluminium or Kapton films.

In 2000, Energy Science Laboratories developed a new carbon fiber material that might be useful for solar sails. The material is over 200 times thicker than conventional solar sail designs, but it is so porous that it has the same mass. The rigidity and durability of this material could make solar sails that are significantly sturdier than plastic films. The material could self-deploy and should withstand higher temperatures.

There has been some theoretical speculation about using molecular manufacturing techniques to create advanced, strong, hyper-light sail material, based on nanotube mesh weaves, where the weave "spaces" are less than half the wavelength of light impinging on the sail. While such materials have so far only been produced in laboratory conditions, and the means for manufacturing such material on an industrial scale are not yet available, such materials could mass less than 0.1 g/m, making them lighter than any current sail material by a factor of at least 30. For comparison, 5 micrometre thick Mylar sail material mass 7 g/m, aluminized Kapton films have a mass as much as 12 g/m, and Energy Science Laboratories' new carbon fiber material masses 3 g/m.

The least dense metal is lithium, about 5 times less dense than aluminium. Fresh, unoxidized surfaces are reflective. At a thickness of 20 nm, lithium has an area density of 0.011 g/m. A high-performance sail could be made of lithium alone at 20 nm (no emission layer). It would have to be fabricated in space and not used to approach the Sun. In the limit, a sail craft might be constructed with a total areal density of around 0.02 g/m, giving it a lightness number of 67 and a of about 400 mm/s. Magnesium and beryllium are also potential materials for high-performance sails. These 3 metals can be alloyed with each other and with aluminium.

Aluminium is the common choice for the reflection layer. It typically has a thickness of at least 20 nm, with a reflectivity of 0.88 to 0.90. Chromium is a good choice for the emission layer on the face away from the Sun. It can readily provide emissivity values of 0.63 to 0.73 for thicknesses from 5 to 20 nm on plastic film. Usable emissivity values are empirical because thin-film effects dominate; bulk emissivity values do not hold up in these cases because material thickness is much thinner than the emitted wavelengths.

Sails are fabricated on Earth on long tables where ribbons are unrolled and joined to create the sails. Sail material needed to have as little weight as possible because it would require the use of the shuttle to carry the craft into orbit. Thus, these sails are packed, launched, and unfurled in space.

In the future, fabrication could take place in orbit inside large frames that support the sail. This would result in lower mass sails and elimination of the risk of deployment failure.

Sailing operations are simplest in interplanetary orbits, where attitude changes are done at low rates. For outward bound trajectories, the sail force vector is oriented forward of the Sun line, which increases orbital energy and angular momentum, resulting in the craft moving farther from the Sun. For inward trajectories, the sail force vector is oriented behind the Sun line, which decreases orbital energy and angular momentum, resulting in the craft moving in toward the Sun. It is worth noting that only the Sun's gravity pulls the craft toward the Sun—there is no analog to a sailboat's tacking to windward. To change orbital inclination, the force vector is turned out of the plane of the velocity vector.

In orbits around planets or other bodies, the sail is oriented so that its force vector has a component along the velocity vector, either in the direction of motion for an outward spiral, or against the direction of motion for an inward spiral.

Trajectory optimizations can often require intervals of reduced or zero thrust. This can be achieved by rolling the craft around the Sun line with the sail set at an appropriate angle to reduce or remove the thrust.

A close solar passage can be used to increase a craft's energy. The increased radiation pressure combines with the efficacy of being deep in the Sun's gravity well to substantially increase the energy for runs to the outer Solar System. The optimal approach to the Sun is done by increasing the orbital eccentricity while keeping the energy level as high as practical. The minimum approach distance is a function of sail angle, thermal properties of the sail and other structure, load effects on structure, and sail optical characteristics (reflectivity and emissivity). A close passage can result in substantial optical degradation. Required turn rates can increase substantially for a close passage. A sail craft arriving at a star can use a close passage to reduce energy, which also applies to a sail craft on a return trip from the outer Solar System.

A lunar swing-by can have important benefits for trajectories leaving from or arriving at Earth. This can reduce trip times, especially in cases where the sail is heavily loaded. A swing-by can also be used to obtain favorable departure or arrival directions relative to Earth.

A planetary swing-by could also be employed similar to what is done with coasting spacecraft, but good alignments might not exist due to the requirements for overall optimization of the trajectory.

The following table lists some example concepts using beamed laser propulsion as proposed by the physicist Robert L. Forward:

Ref:

Both the Mariner 10 mission, which flew by the planets Mercury and Venus, and the MESSENGER mission to Mercury demonstrated the use of solar pressure as a method of attitude control in order to conserve attitude-control propellant.

Hayabusa also used solar pressure on its solar paddles as a method of attitude control to compensate for broken reaction wheels and chemical thruster.

MTSAT-1R (Multi-Functional Transport Satellite)'s solar sail counteracts the torque produced by sunlight pressure on the solar array. The trim tab on the solar array makes small adjustments to the torque balance.

NASA has successfully tested deployment technologies on small scale sails in vacuum chambers.

On February 4, 1993, the Znamya 2, a 20-meter wide aluminized-mylar reflector, was successfully deployed from the Russian Mir space station. Although the deployment succeeded, propulsion was not demonstrated. A second test, Znamya 2.5, failed to deploy properly.

In 1999, a full-scale deployment of a solar sail was tested on the ground at DLR/ESA in Cologne.

A joint private project between Planetary Society, Cosmos Studios and Russian Academy of Science in 2001 made a suborbital prototype test, which failed because of rocket failure.

A 15-meter-diameter solar sail (SSP, solar sail sub payload, "soraseiru sabupeiro-do") was launched together with ASTRO-F on a M-V rocket on February 21, 2006, and made it to orbit. It deployed from the stage, but opened incompletely.

On August 9, 2004, the Japanese ISAS successfully deployed two prototype solar sails from a sounding rocket. A clover-shaped sail was deployed at 122 km altitude and a fan-shaped sail was deployed at 169 km altitude. Both sails used 7.5-micrometer film. The experiment purely tested the deployment mechanisms, not propulsion.

On 21 May 2010, Japan Aerospace Exploration Agency (JAXA) launched the world's first interplanetary solar sail spacecraft "IKAROS" ("Interplanetary Kite-craft Accelerated by Radiation Of the Sun") to Venus. Using a new solar-photon propulsion method, it was the first true solar sail spacecraft fully propelled by sunlight, and was the first spacecraft to succeed in solar sail flight.

JAXA successfully tested IKAROS in 2010. The goal was to deploy and control the sail and, for the first time, to determine the minute orbit perturbations caused by light pressure. Orbit determination was done by the nearby AKATSUKI probe from which IKAROS detached after both had been brought into a transfer orbit to Venus. The total effect over the six month flight was 100 m/s.

Until 2010, no solar sails had been successfully used in space as primary propulsion systems. On 21 May 2010, the Japan Aerospace Exploration Agency (JAXA) launched the IKAROS (Interplanetary Kite-craft Accelerated by Radiation Of the Sun) spacecraft, which deployed a 200 m polyimide experimental solar sail on June 10.<ref name="jaxa.jp/press/2010/06/20100611_ikaros_e"></ref> In July, the next phase for the demonstration of acceleration by radiation began. On 9 July 2010, it was verified that IKAROS collected radiation from the Sun and began photon acceleration by the orbit determination of IKAROS by range-and-range-rate (RARR) that is newly calculated in addition to the data of the relativization accelerating speed of IKAROS between IKAROS and the Earth that has been taken since before the Doppler effect was utilized.<ref name="http://www.jaxa.jp/press/2010/07/20100709_ikaros_j.html"></ref> The data showed that IKAROS appears to have been solar-sailing since 3 June when it deployed the sail.

IKAROS has a diagonal spinning square sail 14×14 m (196 m) made of a thick sheet of polyimide. The polyimide sheet had a mass of about 10 grams per square metre. A thin-film solar array is embedded in the sail. Eight LCD panels are embedded in the sail, whose reflectance can be adjusted for attitude control. IKAROS spent six months traveling to Venus, and then began a three-year journey to the far side of the Sun.

A team from the NASA Marshall Space Flight Center (Marshall), along with a team from the NASA Ames Research Center, developed a solar sail mission called NanoSail-D, which was lost in a launch failure aboard a Falcon 1 rocket on 3 August 2008. The second backup version, NanoSail-D2, also sometimes called simply NanoSail-D, was launched with FASTSAT on a Minotaur IV on November 19, 2010, becoming NASA's first solar sail deployed in low earth orbit. The objectives of the mission were to test sail deployment technologies, and to gather data about the use of solar sails as a simple, "passive" means of de-orbiting dead satellites and space debris. The NanoSail-D structure was made of aluminium and plastic, with the spacecraft massing less than . The sail has about of light-catching surface. After some initial problems with deployment, the solar sail was deployed and over the course of its 240-day mission reportedly produced a "wealth of data" concerning the use of solar sails as passive deorbit devices.

NASA launched the second NanoSail-D unit stowed inside the FASTSAT satellite on the Minotaur IV on November 19, 2010. The ejection date from the FASTSAT microsatellite was planned for December 6, 2010, but deployment only occurred on January 20, 2011.

In June 21, 2005, a joint private project between Planetary Society, Cosmos Studios and Russian Academy of Science launched a prototype sail "Cosmos 1" from a submarine in the Barents Sea, but the Volna rocket failed, and the spacecraft failed to reach orbit. They intended to use the sail to gradually raise the spacecraft to a higher Earth orbit over a mission duration of one month. The launch attempt sparked public interest according to Louis Friedman. Despite the failed launch attempt of Cosmos 1, The Planetary Society received applause for their efforts from the space community and sparked a rekindled interest in solar sail technology.

On Carl Sagan's 75th birthday (November 9, 2009) the Planetary Society announced plans to make three further attempts, dubbed LightSail-1, -2, and -3. The new design will use a 32 m Mylar sail, deployed in four triangular segments like NanoSail-D. The launch configuration is a 3U CubeSat format, and as of 2015, it is scheduled as a secondary payload for a 2016 launch on the first SpaceX Falcon Heavy launch.
"LightSail-1" was launched on 20 May 2015. The purpose of the test was to allow a full checkout of the satellite's systems in advance of the main 2018 mission, LightSail-2.

Despite the losses of "Cosmos 1" and NanoSail-D (which were due to failure of their launchers), scientists and engineers around the world remain encouraged and continue to work on solar sails. While most direct applications created so far intend to use the sails as inexpensive modes of cargo transport, some scientists are investigating the possibility of using solar sails as a means of transporting humans. This goal is strongly related to the management of very large (i.e. well above 1 km) surfaces in space and the sail making advancements. Manned space flight utilizing solar sails is still in the development state of infancy.

A technology demonstration sail craft, dubbed "Sunjammer", was in development with the intent to prove the viability and value of sailing technology. "Sunjammer" had a square sail, 124 feet (38 meters) wide on each side (total area 13,000 sq ft or 1,208 sq m). It would have traveled from the Sun-Earth Lagrangian point 900,000 miles from Earth (1.5 million km) to a distance of 1,864,114 miles (3 million kilometers). The demonstration was expected to launch on a Falcon 9 in January 2015. It would have been a secondary payload, released after the placement of the DSCOVR climate satellite at the L1 point. Citing a lack of confidence in its contractor's ability to deliver, the mission was cancelled in October 2014.

, the European Space Agency (ESA) has a proposed deorbit sail, named ""Gossamer"", that would be intended to be used to accelerate the deorbiting of small (less than ) artificial satellites from low-Earth orbits. The launch mass is with a launch volume of only . Once deployed, the sail would expand to and would use a combination of solar pressure on the sail and increased atmospheric drag to accelerate satellite reentry.

The Near-Earth Asteroid Scout (NEA Scout) is a mission being jointly developed by NASA's Marshall Space Flight Center (MSFC) and the Jet Propulsion Laboratory (JPL), consisting of a controllable low-cost CubeSat solar sail spacecraft capable of encountering near-Earth asteroids (NEA). Four 7 m booms would deploy, unfurling the 83 m aluminized polyimide solar sail. In 2015, NASA announced it had selected NEA Scout to launch as one of several secondary payloads aboard EM-1, the first flight of the agency's heavy-lift SLS launch vehicle.

OKEANOS (Outsized Kite-craft for Exploration and Astronautics in the Outer Solar System) is a proposed mission concept by Japan's JAXA to Jupiter's Trojan asteroids using a hybrid solar sail for propulsion; the sail is covered with thin solar panels to power an ion engine. "In-situ" analysis of the collected samples would be performed by either direct contact or using a lander carrying a high-resolution mass spectrometer. A lander and a sample-return to Earth are options under study. The OKEANOS Jupiter Trojan Asteroid Explorer is a finalist for Japan's ISAS 2nd Large-class mission to be launched in the late 2020s.

The well-funded Breakthrough Starshot project announced in April 12, 2016, aims to develop a fleet of 1000 light sail nanocraft carrying miniature cameras, propelled by ground-based lasers and send them to Alpha Centauri at 20% the speed of light. The trip would take 20 years.

A similar technology appeared in the episode, . In the episode, Lightships are described as an ancient technology used by Bajorans to travel beyond their solar system by using light from the Bajoran sun and specially constructed sails to propel them through space.





</doc>
<doc id="29425" url="https://en.wikipedia.org/wiki?curid=29425" title="Sabellianism">
Sabellianism

In Christianity, Sabellianism in the Eastern church or Patripassianism in the Western church is the belief that the Father, Son, and Holy Spirit are three different "modes" or "aspects" of God, as opposed to a Trinitarian view of three distinct persons within the Godhead. The term "Sabellianism" comes from Sabellius, who was a theologian and priest from the 3rd century. None of his writings have survived and so all that is known about him comes from his opponents. All evidence shows that Sabellius held Jesus to be deity while denying the plurality of persons in God and holding a belief similar to modalistic monarchianism. Modalistic monarchianism has been generally understood to have arisen during the second and third centuries, and to have been regarded as heresy after the fourth, although this is disputed by some. 

Sabellianism has been rejected by the majority of Christian churches in favour of Trinitarianism, which was eventually defined as three distinct, co-equal, co-eternal Persons of One Substance by the Athanasian Creed, probably dating from the late 5th or early 6th century. The Greek term "homoousian" or "consubstantial" () had been used before its adoption by the First Council of Nicaea. The Gnostics were the first to use the word ', while before the Gnostics there is no trace at all of its existence. The early church theologians were probably made aware of this concept, and thus of the doctrine of emanation, taught by the Gnostics. In Gnostic texts the word ' is used with the following meanings:
It has been noted that this Greek term "homoousian" or "consubstantial", which Athanasius of Alexandria favoured, was also a term reportedly used by Sabellius—a term that many who held with Athanasius were uneasy about. Their objection to the term "homoousian" was that it was considered to be un-Scriptural, suspicious, and "of a Sabellian tendency." This was because Sabellius also considered the Father and the Son to be "one substance," meaning that, to Sabellius, the Father and Son were one essential person, though operating as different manifestations or modes. Athanasius' use of the word is intended to affirm that while the Father and Son are eternally distinct in a truly personal manner (i.e. with mutual love John 3:35, 14:31), both are nevertheless One Being, Essence, Nature, or Substance, having One personal Spirit.

Modalism has been mainly associated with Sabellius, who taught a form of it in Rome in the 3rd century. This had come to him via the teachings of Noetus and Praxeas. Noetus was excommunicated from the Church after being examined by council, and Praxeas is said to have recanted his modalistic views in writing, teaching again his former faith. Sabellius likewise was excommunicated by council in Alexandria, and after complaint of this was made to Rome, a second council then assembled in Rome and also ruled against not only Sabellianism, but against Arianism, and against Tritheism, while affirming a "Divine Triad" as the catholic understanding of the "Divine Monarchy". 
Hippolytus of Rome knew Sabellius personally, writing how he and others had admonished Sabellius in "Refutation of All Heresies". He knew Sabellius opposed Trinitarian theology, yet he called Modal Monarchism the heresy of Noetus, not that of Sabellius. Sabellianism was embraced by Christians in Cyrenaica, to whom Dionysius, Patriarch of Alexandria (who was instrumental in the excommunication of Sabellius in Alexandria), wrote letters arguing against this belief. Hippolytus himself perceived modalism as a new and peculiar idea which was covertly gaining a following:

Some others are secretly introducing another doctrine, who have become disciples of one Noetus, who was a native of Smyrna, (and) lived not very long ago. This person was greatly puffed up and inflated with pride, being inspired by the conceit of a strange spirit. | There has appeared one, Noetus by name, and by birth a native of Smyrna. This person introduced a heresy from the tenets of Heraclitus. Now a certain man called Epigonus becomes his minister and pupil, and this person during his sojourn at Rome disseminated his godless opinion. But Cleomenes, who had become his disciple, an alien both in way of life and habits from the Church, was wont to corroborate the (Noetian) doctrine. | But in like manner, also, Noetus, being by birth a native of Smyrna, and a fellow addicted to reckless babbling, as well as crafty withal, introduced (among us) this heresy which originated from one Epigonus. It reached Rome, and was adopted by Cleomenes, and so has continued to this day among his successors. 

Tertullian also perceived modalism as entering into the Church from without as a new idea, and opposing the doctrine which had been received through succession. After setting forth his understanding of the manner of faith which had been received by the Church, he then describes how the "simple" who always constitute the majority of believers are often startled at the idea that the One God exists in three and were opposed to his understanding of "the rule of faith." Proponents of Tertullian argue that he described the "simple" as the majority, rather than those who opposed him as the majority. This is contended from Tertullian's argument that they were putting forth ideas of their own which had not been taught to them by their elders:

We, however, as we indeed always have done (and more especially since we have been better instructed by the Paraclete, who leads men indeed into all truth), believe that there is one only God, but under the following dispensation, or οἰκονομία , as it is called, that this one only God has also a Son, His Word, who proceeded from Himself, by whom all things were made, and without whom nothing was made. Him we believe to have been sent by the Father into the Virgin, and to have been born of her—being both Man and God, the Son of Man and the Son of God, and to have been called by the name of Jesus Christ; we believe Him to have suffered, died, and been buried, according to the Scriptures, and, after He had been raised again by the Father and taken back to heaven, to be sitting at the right hand of the Father, and that He will come to judge the quick and the dead; who sent also from heaven from the Father, according to His own promise, the Holy Ghost, the Paraclete, the sanctifier of the faith of those who believe in the Father, and in the Son, and in the Holy Ghost. That this rule of faith has come down to us from the beginning of the gospel, even before any of the older heretics, much more before Praxeas, a pretender of yesterday, will be apparent both from the lateness of date which marks all heresies, and also from the absolutely novel character of our new-fangled Praxeas. In this principle also we must henceforth find a presumption of equal force against all heresies whatsoever—that whatever is first is true, whereas that is spurious which is later in date.

The simple, indeed, (I will not call them unwise and unlearned,) who always constitute the majority of believers, are startled at the dispensation (of the Three in One), on the ground that their very rule of faith withdraws them from the world’s plurality of gods to the one only true God; not understanding that, although He is the one only God, He must yet be believed in with His own οἰκονομία . The numerical order and distribution of the Trinity they assume to be a division of the Unity; whereas the Unity which derives the Trinity out of its own self is so far from being destroyed, that it is actually supported by it. They are constantly throwing out against us that we are preachers of two gods and three gods, while they take to themselves pre-eminently the credit of being worshippers of the One God; just as if the Unity itself with irrational deductions did not produce heresy, and the Trinity rationally considered constitute the truth.

According to modalism and Sabellianism, God is said to be only one person who reveals himself in different ways called "modes", "faces", "aspects", "roles" or "masks" (Greek πρόσωπα "prosopa" ; Latin "personae") of the One God, as perceived by "the believer", rather than "three co-eternal persons" within "the Godhead", or a "co-equal Trinity". Modalists note that the only number expressly and repeatedly ascribed to God in the Old Testament is "One," do not accept interpreting this number as denoting union (i.e. Gen 2:24) when it is applied to God, and dispute the meaning or validity of related New Testament passages cited by Trinitarians. The Comma Johanneum, which is generally regarded as a spurious text in First John (1 John 5:7) known primarily from the King James Version and some versions of the Textus Receptus, but not included in modern critical texts, is an instance (the only one expressly stated) of the word "Three" describing God. Many modalists point out the lack of the word "Trinity" in any canonical scripture.

Passages such as Deut 6:4-5; Deut 32:12; 2Kings 19:15-19; Job 6:10; Job 31:13-15; Psalm 71:22; Psalm 83:16,18; Is 42:8; Is 45:5-7; Is 48:2,9,11-13; Mal 2:8,10; Matt 19:17; Romans 3:30; 2Cor 11:2-3; Gal 3:20; and Jude 1:25 are referenced by modalists as affirming that the Being of the One God is solidly single, and although known in several modes, precludes any concept of divine co-existence. Hippolytus described similar reasoning by Noetus and his followers saying: Now they seek to exhibit the foundation for their dogma by citing the word in the law, “I am the God of your fathers: ye shall have no other gods beside me;” and again in another passage, “I am the first,” He saith, “and the last; and beside me there is none other.” Thus they say they prove that God is one... And we cannot express ourselves otherwise, he says; for the apostle also acknowledges one God, when he says, “Whose are the fathers, (and) of whom as concerning the flesh Christ came, who is over all, God blessed for ever.”

Oneness Pentecostals, an identifier used by some modern modalists, claim that Colossians 1:12-20 refers to Christ's relationship with the Father in the sense of different roles of God:
giving thanks to the Father, who has qualified you to share in the inheritance of the saints in light. He has delivered us from the domain of darkness and transferred us to the kingdom of his beloved Son, in whom we have redemption, the forgiveness of sins. He is the image of the invisible God, the firstborn of all creation. For by him all things were created, in heaven and on earth, visible and invisible, whether thrones or dominions or rulers or authorities; all things were created through him and for him. And he is before all things, and in him all things hold together. And he is the head of the body, the church. He is the beginning, the firstborn from the dead, that in everything he might be preeminent. For in him all the fullness of God was pleased to dwell, and through him to reconcile to himself all things, whether on earth or in heaven, making peace by the blood of his cross.

Oneness Pentecostals also cite Christ's response to Philip's query on who the Father was in John 14:10 to support this assertion:

Trinitarian Christians hold that verses such as Colossians 1:12-20 remove all reasonable doubt that scripture teaches the Son, Who IS the Word of God (i.e. John 1:1-3), is literally "living," and literally Creator of everything together with God the Father and the Spirit of God. In the Trinitarian view, the above usage not only takes John 14:10 out of its immediate context, but is also resolutely contrary to the congruence of the Gospel of John as a whole, and strongly suspected of begging the question in interpretation. Trinitarians understand John 14:10 as informed by parallel verses such as John 1:14 and John 1:18, and as affirming the eternal union of the Son with His Father:

Many doctrinal exchanges between modalists and Trinitarians are similar to the above. Passages such as Gen 1:26-27; Gen 16:11-13; Gen 32:24,30; Judg 6:11-16; Is 48:16; Zech 2:8-9; Matt 3:16-17; Mark 13:32; Luke 12:10; John 5:18-27; John 14:26-28; John 15:26; John 16:13-16; John 17:5,20-24; Acts 1:6-9; and Heb 1:1-3,8-10 are referenced by Trinitarians as affirming that the Being of the One God is an eternal, personal, and mutually indwelling communion of Father [God], Son [the Word of God], and Holy Spirit [the Spirit of God]. Addressing the fact that the word "Trinity" does not occur in scripture, Trinitarians attest that extra-biblical doctrinal language often summarizes our understanding scripture in a clear and concise manner—other examples being even the words "modalism", "mode", and "role"—and that use of such language does not of itself demonstrate accuracy or inaccuracy. Further, the accusative implication that the word "Trinity" gained common use apart from careful and pious fidelity to scripture may be associated with ad hominem argumentation. Hippolytus described his own response to Noetus' doctrine, claiming the truth to be more evident than either of the two mutually opposed views of Arianism and Sabellianism : In this way, then, they choose to set forth these things, and they make use only of one class of passages; just in the same one-sided manner that Theodotus employed when he sought to prove that Christ was a mere man. But neither has the one party nor the other understood the matter rightly, as the Scriptures themselves confute their senselessness, and attest the truth. See, brethren, what a rash and audacious dogma they have introduced... For who will not say that there is one God? Yet he will not on that account deny the economy [i.e., the number and disposition of persons in the Trinity]. The proper way, therefore, to deal with the question is first of all to refute the interpretation put upon these passages by these men, and then to explain their real meaning.

Tertullian said of Praxeas' followers:For, confuted on all sides on the distinction between the Father and the Son, which we maintain without destroying their inseparable union... they endeavour to interpret this distinction in a way which shall nevertheless tally with their own opinions: so that, all in one Person, they distinguish two, Father and Son, understanding the Son to be flesh, that is man, that is Jesus; and the Father to be spirit, that is God, that is Christ. Thus they, while contending that the Father and the Son are one and the same, do in fact begin by dividing them rather than uniting them.”
A comparison of the above statement by Tertullian with the following example statement made by Oneness Pentecostals today is striking: "Jesus is the Son of God according to the flesh... and the very God Himself according to the Spirit..."

The form of the LORD's Name appearing in verse nineteen of the Great Commission, Matthew 28:16-20, has also historically been spoken during Christian baptism, Trinitarian Christians believing the three distinct, albeit co-inherent, persons of the Holy Trinity received witness by Jesus' baptism. Many modalists do not use this form as the LORD's Name. It is also suggested by some modern Oneness Pentecostal critics, that Matthew 28:19 is not part of the original text, because Eusebius of Caesarea quoted it by saying "In my name", and in that source there was no mention of baptism in the verse. Eusebius did, however, quote the "trinitarian" formula in his later writings. (Conybeare ("Hibbert Journal" i (1902-3), page 102). Matthew 28:19 is quoted also in the Didache (Didache 7:1), which dates to the late 1st Century or early 2nd Century) and in the Diatesseron (Diatesseron 55:5-7), which dates to the mid 2nd Century harmony of the Synoptic Gospels. The "Shem-Tob's Hebrew Gospel of Matthew" (George Howard), written during the 14th century, also has no reference of baptism or a "trinitarian" formula in Matthew 28:19. However, it is also true that no Greek manuscript of the Gospel of Matthew has ever been found which does not contain Matthew 28:19. The earliest extant copies of Matthew's Gospel date to the 3rd Century, and they contain Matthew 28:19. Therefore, scholars generally agree that Matthew 28:19 is likely part of the original Gospel of Matthew, though a minority disputes this.

In passages of scripture such as Matthew 3:16-17 where the Father, Son, and Holy Spirit are separated in the text and witness, modalists view this phenomenon as confirming God's omnipresence, and His ability to manifest himself as he pleases. Oneness Pentecostals and Modalists attempt to dispute the traditional doctrine of eternal co-existent union, while affirming the Christian doctrine of God taking on flesh as Jesus Christ. Like Trinitarians, Oneness adherents attest that Jesus Christ is fully God and fully man. However, Trinitarians believe that the "Word of God," the eternal second Person of the Trinity, was manifest as the Son of God by taking humanity to Himself and by glorifying that Humanity to equality with God through His resurrection, in eternal union with His own Divinity. In contrast, Oneness adherents hold that the One and Only true God—Who manifests Himself in any way He chooses, including as Father, Son and Holy Spirit (though not choosing to do so in an eternally simultaneous manner)—became man in the temporary role of Son. Many Oneness Pentecostals have also placed a strongly Nestorian distinction between Jesus' humanity and Divinity as in the example compared with Tertullian's statement above.

Oneness Pentecostals and other modalists are regarded by Roman Catholic, Greek Orthodox, and most other mainstream Christians as heretical for denying the literal existence of God's Beloved Son from Heaven, including His eternal Being and personal communion with the Father as High Priest, Mediator, Intercessor and Advocate; rejecting the direct succession of apostolic gifts and authority through the ordination of the Christian bishops; rejecting the identity of mainstream Christians as the God-begotten Body and Church which Christ founded; and rejecting the affirmations of the ecumenical councils such as the Councils of Nicaea and Constantinople, including the Holy Trinity. These rejections are for mainstream Christendom similar to Unitarianism, in that they primarily result from Christological heresy. While many Unitarians are Arians, modalists differentiate themselves from Arian or Semi-Arian Unitarians by affirming Christ's full Godhead, whereas both the Arian and Semi-Arian views assert Christ as not of one substance (Greek: οὐσία) with, and therefore also not equal with, God the Father. Dionysius, bishop of Rome, set forth the understanding of traditional Christianity concerning both Arianism and Sabellianism in "Against the Sabellians", ca. AD 262. He, in similarity to Hippolytus, explained that the two errors are at opposite extremes in seeking to understand the Son of God, Arianism misusing that the Son is distinct respecting the Father, and Sabellianism misusing that the Son is equal respecting the Father. In fact, he also repudiated the idea of three Gods as error as well. While Arianism and Sabellianism may appear to be diametrically opposed, the former claiming Christ to be created and the latter claiming Christ is God, both in common deny the Trinitarian belief that Christ is God Eternal in His Humanity, and that this is the very basis of man's hope of salvation. "One, not by conversion of the Godhead into flesh, but by taking of the manhood into God."

Hippolytus' account of the excommunication of Noetus is as follows: When the blessed presbyters heard this, they summoned him before the Church, and examined him. But he denied at first that he held such opinions. Afterwards, however, taking shelter among some, and having gathered round him some others who had embraced the same error, he wished thereafter to uphold his dogma openly as correct. And the blessed presbyters called him again before them, and examined him. But he stood out against them, saying, “What evil, then, am I doing in glorifying Christ?” And the presbyters replied to him, “We too know in truth one God; we know Christ; we know that the Son suffered even as He suffered, and died even as He died, and rose again on the third day, and is at the right hand of the Father, and cometh to judge the living and the dead. And these things which we have learned we allege.” Then, after examining him, they expelled him from the Church. And he was carried to such a pitch of pride, that he established a school.

Today's Oneness Pentecostal organisations left their original organization when a council of Pentecostal leaders officially adopted Trinitarianism, and have since established schools.

Epiphanius (Haeres 62) about 375 notes that the adherents of Sabellius were still to be found in great numbers, both in Mesopotamia and at Rome. The First Council of Constantinople in 381 in canon VII and the Third Council of Constantinople in 680 in canon XCV declared the baptism of Sabellius to be invalid, which indicates that Sabellianism was still extant.

The chief critics of Sabellianism were Tertullian and Hippolytus. In his work "Adversus Praxeas", Chapter I, Tertullian wrote "By this Praxeas did a twofold service for the devil at Rome: he drove away prophecy, and he brought in heresy; he put to flight the Paraclete, and he crucified the Father." Likewise Hippolytus wrote, "Do you see, he says, how the Scriptures proclaim one God? And as this is clearly exhibited, and these passages are testimonies to it, I am under necessity, he says, since one is acknowledged, to make this One the subject of suffering. For Christ was God, and suffered on account of us, being Himself the Father, that He might be able also to save us... See, brethren, what a rash and audacious dogma they have introduced, when they say without shame, the Father is Himself Christ, Himself the Son, Himself was born, Himself suffered, Himself raised Himself. But it is not so." From these notions came the term "Patripassianism" for the movement, from the Latin words "pater" for "father", and "passus" from the verb "to suffer" because it implied that the Father suffered on the Cross.

It is important to note that our only sources extant for our understanding of Sabellianism are from their detractors. Scholars today are not in agreement as to what exactly Sabellius or Praxeas taught. It is easy to suppose that Tertullian and Hippolytus at least at times misrepresented the opinions of their opponents.

The Greek Orthodox teach that God is not of a substance that is comprehensible since God the Father has no origin and is eternal and infinite. Thus it is improper to speak of things as "physical" and "metaphysical"; rather it is correct to speak of things as "created" and "uncreated." God the Father is the origin and source of the Trinity of Whom the Son is begotten and the Spirit proceeding, all Three being Uncreated. Therefore, the consciousness of God is not obtainable to created beings either in this life or the next (see apophatism). Through co-operation with the Holy Spirit (called theosis), Mankind can become good (God-like), not becoming uncreated, but partaker of His divine energies (2 Peter 1:4). From such a perspective Mankind can be reconciled from the Knowledge of Good and the Knowledge of Evil he obtained in the Garden of Eden (see the Fall of Man), his created substance thus partaking of Uncreated God through the indwelling Presence of the eternally incarnate (Phil 3:21) Son of God and His Father by the Spirit (John 17:22-24, Rom 8:11,16-17).

At the Arroyo Seco World Wide Camp Meeting, near Los Angeles, in 1913, Canadian evangelist R.E. McAlister stated at a baptismal service that the apostles had baptized in the name of Jesus only and not in the triune Name of Father, Son, and Holy Spirit. Later that night, John G. Schaeppe, a German immigrant, had a vision of Jesus and woke up the camp shouting that the name of Jesus needed to be glorified. From that point, Frank J. Ewart began requiring that anyone baptized using the Trinitarian formula needed to be rebaptized in the name of Jesus “only.” Support for this position began to spread, along with a belief in one Person in the Godhead, acting in different modes or offices.
The General Council of the Assemblies of God convened in St. Louis, Missouri in October 1916, to confirm their belief in Trinitarian orthodoxy. The Oneness camp was faced by a majority who required acceptance of the Trinitarian baptismal formula and the orthodox doctrine of the Trinity or remove themselves from the denomination. In the end, about a quarter of the ministers withdrew.

Oneness Pentecostalism teaches that God is one Person, and that the Father (a spirit) is united with Jesus (a man) as the Son of God. However, Oneness Pentecostalism differs somewhat by rejecting sequential modalism, and by the full acceptance of the begotten humanity of the Son, not eternally begotten, who was the man Jesus and was born, crucified, and risen, and not the deity. This directly opposes the pre-existence of the Son as a pre-existent mode, which Sabellianism generally does not oppose.

Oneness Pentecostals believe that Jesus was "Son" only when he became flesh on earth, but was the Father before being made man. They refer to the Father as the "Spirit" and the Son as the "Flesh", but they believe that Jesus and the Father are one essential Person, though operating as different "manifestations" or "modes". Oneness Pentecostals reject the Trinity doctrine, viewing it as pagan and nonscriptural, and hold to the Jesus' Name doctrine with respect to baptisms. They are often referred to as "Modalists" or "Jesus Only". Oneness Pentecostalism can be compared to Sabellianism, or can be described as holding to a form of Sabellianism, as both are nontrinitarian, and as both believe that Jesus was "Almighty God in the Flesh", but they do not totally identify each other.

It cannot be certain whether Sabellius taught Modalism completely as it is taught today as Oneness doctrine, since only a few fragments of his writings are extant and, therefore, all we have of his teachings comes through the writing of his detractors.

The following excerpts which demonstrate some of the known doctrinal characteristics of ancient Sabellians may be seen to compare with the doctrines in the modern Oneness movement:


While Oneness Pentecostals seek to differentiate themselves from ancient Sabellianism, modern theologians such as James R. White and Robert Morey see no significant difference between the ancient heresy of Sabellianism and current Oneness doctrine. This is based on the denial by Oneness Pentecostals of the Trinity, especially of the Divinity and Eternality of the SON of God, based upon a denial of the distinction between the Father, Son, and Holy Spirit. Sabellianism, Patripassianism, Modalistic Monarchianism, functionalism, Jesus Only, Father Only, and Oneness Pentecostalism are viewed by these theologians as being derived from a Platonic doctrine that God was an indivisible Monad and could not be differentiated as distinct Persons.




</doc>
<doc id="29426" url="https://en.wikipedia.org/wiki?curid=29426" title="Sino-Indian War">
Sino-Indian War

The Sino-Indian War (), also known as the Sino-Indian Border Conflict (), was a war between China and India that occurred in 1962. A disputed Himalayan border was the main pretext for war, but other issues played a role. There had been a series of violent border incidents after the 1959 Tibetan uprising, when India had granted asylum to the Dalai Lama. India initiated a Forward Policy in which it placed outposts along the border, including several north of the McMahon Line, the eastern portion of a Line of Actual Control proclaimed by Chinese Premier Zhou Enlai in 1959.

Unable to reach political accommodation on disputed territory along the 3,225-kilometre-long Himalayan border, the Chinese launched simultaneous offensives in Ladakh and across the McMahon Line on 20 October 1962, coinciding with the Cuban Missile Crisis. Chinese troops advanced over Indian forces in both theatres, capturing Rezang La in Chushul in the western theatre, as well as Tawang in the eastern theatre. The war ended when China declared a ceasefire on 20 November 1962, and simultaneously announced its withdrawal to its claimed 'line of actual control'.

Much of the battle took place in harsh mountain conditions, entailing large-scale combat at altitudes of over 4,000 metres (14,000 feet). The Sino-Indian War was also noted for the non-deployment of the navy or air force by either the Chinese or Indian side.

The buildup and offensive from China occurred concurrently with the 13-day Cuban Missile Crisis (16–28 October 1962) that saw both the United States and the Soviet Union confronting each other, and India did not receive assistance from either of these world powers until the Cuban Missile Crisis was resolved.

China and India shared a long border, sectioned into three stretches by Nepal, Sikkim (then an Indian protectorate), and Bhutan, which follows the Himalayas between Burma and what was then West Pakistan. A number of disputed regions lie along this border. At its western end is the Aksai Chin region, an area the size of Switzerland, that sits between the Chinese autonomous region of Xinjiang and Tibet (which China declared as an autonomous region in 1965). The eastern border, between Burma and Bhutan, comprises the present Indian state of Arunachal Pradesh (formerly the North East Frontier Agency). Both of these regions were overrun by China in the 1962 conflict.

Most combat took place at high altitudes. The Aksai Chin region is a desert of salt flats around 5,000 metres above sea level, and Arunachal Pradesh is mountainous with a number of peaks exceeding 7,000 metres. The Chinese Army had possession of one of the highest ridges in the regions. The high altitude and freezing conditions also cause logistical and welfare difficulties; in past similar conflicts (such as the Italian Campaign of World War I) harsh conditions have caused more casualties than have enemy action. The Sino-Indian War was no different, with many troops on both sides dying in the freezing cold.

The cause of the war was a dispute over the sovereignty of the widely separated Aksai Chin and Arunachal Pradesh border regions. Aksai Chin, claimed by India to belong to Kashmir and by China to be part of Xinjiang, contains an important road link that connects the Chinese regions of Tibet and Xinjiang. China's construction of this road was one of the triggers of the conflict.

The western portion of the Sino-Indian boundary originated in 1834, with the conquest of Ladakh by the armies of Raja Gulab Singh (Dogra) under the suzerainty of the Sikh Empire. Following an unsuccessful campaign into Tibet, Gulab Singh and the Tibetans signed a treaty in 1842 agreeing to stick to the "old, established frontiers", which were left unspecified. The British defeat of the Sikhs in 1846 resulted in the transfer of the Jammu and Kashmir region including Ladakh to the British, who then installed Gulab Singh as the Maharaja under their suzerainty. British commissioners contacted Chinese officials to negotiate the border, who did not show any interest. The British boundary commissioners fixed the southern end of the boundary at Pangong Lake, but regarded the area north of it till the Karakoram Pass as "terra incognita".

The Maharaja of Kashmir and his officials were keenly aware of the trade routes from Ladakh. Starting from Leh, there were two main routes into Central Asia: one passed through the Karakoram Pass to Shahidulla at the foot of the Kunlun Mountains and went on to Yarkand through the Kilian and Sanju passes; the other went east via the Chang Chenmo Valley, passed the Lingzi Tang Plains in the Aksai Chin region, and followed the course of the Karakash River to join the first route at Shahidulla. The Maharaja regarded Shahidulla as his northern outpost, in effect treating the Kunlun mountains as the boundary of his domains. His British suzerains were sceptical of such an extended boundary because Shahidulla was 79 miles away from the Karakoram pass and the intervening area was uninhabited. Nevertheless, the Maharaja was allowed to treat Shahidulla as his outpost for more than 20 years.

Chinese Turkestan regarded the high mountains of the Kunlun range with the Kilian and Sanju passes as its southern boundary. Thus the Maharaja's claim was uncontested. After the 1862 Dungan Revolt, which saw the expulsion of the Chinese from Turkestan, the Maharaja of Kashmir constructed a small fort at Shahidulla in 1864. The fort was most likely supplied from Khotan, whose ruler was now independent and on friendly terms with Kashmir. When the Khotanese ruler was deposed by the Kashgaria strongman Yakub Beg, the Maharaja was forced to abandon his post in 1867. It was then occupied by Yakub Beg's forces until the end of the Dungan Revolt.
In the intervening period, W. H. Johnson of Survey of India was commissioned to survey the Aksai Chin region. While in the course of his work, he was "invited" by the Khotanese ruler to visit his capital. After returning, Johnson declared that Khotan's border was at Brinjga, in the Kunlun mountains, and the entire the Karakash Valley was within the territory of Kashmir. The boundary of Kashmir that he drew, stretching from Shahidullah to the eastern edge of Chang Chenmo Valley along the Kunlun mountains, is referred to as the "Johnson Line" (or "Ardagh-Johnson Line").

After the Chinese reconquered Turkestan in 1878, renaming it Xinjiang, they again reverted to their traditional boundary. By now, the Russian Empire was entrenched in Central Asia, and the British were anxious to avoid a common border with the Russians. After creating the Wakhan corridor as the buffer in the northwest of Kashmir, they wanted the Chinese to fill out the "no man's land" between the Karakoram and Kunlun ranges. Under British (and possibly Russian) encouragement, the Chinese occupied the area up to the Yarkand River valley (called Raskam), including Shahidulla, by 1890. They also erected a boundary pillar at the Karakoram pass by about 1892. These efforts appear half-hearted. A map provided by Hung Ta-chen, a senior Chinese official at St. Petersburgh, in 1893 showed the boundary of Xinjiang up to Raskam. In the east, it was similar to the Johnson line, placing Aksai Chin in Kashmir territory.

By 1892, the British settled on the policy that their preferred boundary for Kashmir was the "Indus watershed", i.e., the water-parting from which waters flow into the Indus river system on one side and into the Tarim basin on the other. In the north, this water-parting was along the Karakoram range. In the east, it was more complicated because the Chip Chap River, Galwan River and the Chang Chenmo River flow into the Indus whereas the Karakash River flows into the Tarim basin. A boundary alignment along this water-parting was defined by the Viceroy Lord Elgin and communicated to London. The British government in due course proposed it to China via its envoy Sir Claude MacDonald in 1899. This boundary, which came to be called the Macartney–MacDonald Line, ceded to China the Aksai Chin plains in the northeast, and the Trans-Karakoram Tract in the north. In return, the British wanted China to cede its 'shadowy suzerainty' on Hunza.

In 1911 the Xinhai Revolution resulted in power shifts in China, and by the end of World War I, the British officially used the Johnson Line. They took no steps to establish outposts or assert control on the ground. According to Neville Maxwell, the British had used as many as 11 different boundary lines in the region, as their claims shifted with the political situation. From 1917 to 1933, the "Postal Atlas of China", published by the Government of China in Peking had shown the boundary in Aksai Chin as per the Johnson line, which runs along the Kunlun mountains. The "Peking University Atlas", published in 1925, also put the Aksai Chin in India. Upon independence in 1947, the government of India used the Johnson Line as the basis for its official boundary in the west, which included the Aksai Chin. On 1 July 1954, India's first Prime Minister Jawaharlal Nehru definitively stated the Indian position, claiming that Aksai Chin had been part of the Indian Ladakh region for centuries, and that the border (as defined by the Johnson Line) was non-negotiable. According to George N. Patterson, when the Indian government finally produced a report detailing the alleged proof of India's claims to the disputed area, "the quality of the Indian evidence was very poor, including some very dubious sources indeed".

In 1956–57, China constructed a road through Aksai Chin, connecting Xinjiang and Tibet, which ran south of the Johnson Line in many places. Aksai Chin was easily accessible to the Chinese, but access from India, which meant negotiating the Karakoram mountains, was much more difficult. The road came on Chinese maps published in 1958.

In 1826, British India gained a common border with China after the British wrested control of Manipur and Assam from the Burmese, following the First Anglo-Burmese War of 1824–1826. In 1847, Major J. Jenkins, agent for the North East Frontier, reported that the Tawang was part of Tibet. In 1872, four monastic officials from Tibet arrived in Tawang and supervised a boundary settlement with Major R. Graham, NEFA official, which included the Tawang Tract as part of Tibet. Thus, in the last half of the 19th century, it was clear that the British treated the Tawang Tract as part of Tibet. This boundary was confirmed in a 1 June 1912 note from the British General Staff in India, stating that the "present boundary (demarcated) is south of Tawang, running westwards along the foothills from near Ugalguri to the southern Bhutanese border." A 1908 map of The Province of Eastern Bengal and Assam prepared for the Foreign Department of the Government of India, showed the international boundary from Bhutan continuing to the Baroi River, following the Himalayas foothill alignment. In 1913, representatives of Great Britain, China and Tibet attended a conference in Simla regarding the borders between Tibet, China and British India. Whilst all three representatives initialed the agreement, Beijing later objected to the proposed boundary between the regions of Outer Tibet and Inner Tibet, and did not ratify it. The details of the Indo-Tibetan boundary was not revealed to China at the time. The foreign secretary of the British Indian government, Henry McMahon, who had drawn up the proposal, decided to bypass the Chinese (although instructed not to by his superiors) and settle the border bilaterally by negotiating directly with Tibet. According to later Indian claims, this border was intended to run through the highest ridges of the Himalayas, as the areas south of the Himalayas were traditionally Indian. The McMahon Line lay south of the boundary India claims. India's government held the view that the Himalayas were the ancient boundaries of the Indian subcontinent, and thus should be the modern boundaries of India, while it is the position of the Chinese government that the disputed area in the Himalayas have been geographically and culturally part of Tibet since ancient times.

Months after the Simla agreement, China set up boundary markers south of the McMahon Line. T. O'Callaghan, an official in the Eastern Sector of the North East Frontier, relocated all these markers to a location slightly south of the McMahon Line, and then visited Rima to confirm with Tibetan officials that there was no Chinese influence in the area. The British-run Government of India initially rejected the Simla Agreement as incompatible with the Anglo-Russian Convention of 1907, which stipulated that neither party was to negotiate with Tibet "except through the intermediary of the Chinese government". The British and Russians cancelled the 1907 agreement by joint consent in 1921. It was not until the late 1930s that the British started to use the McMahon Line on official maps of the region.

China took the position that the Tibetan government should not have been allowed to make such a treaty, rejecting Tibet's claims of independent rule. For its part, Tibet did not object to any section of the McMahon Line excepting the demarcation of the trading town of Tawang, which the Line placed under British-Indian jurisdiction. Up until World War II, Tibetan officials were allowed to administer Tawang with complete authority. Due to the increased threat of Japanese and Chinese expansion during this period, British Indian troops secured the town as part of the defence of India's eastern border.

In the 1950s, India began patrolling the region. It found that, at multiple locations, the highest ridges actually fell north of the McMahon Line. Given India's historic position that the original intent of the line was to separate the two nations by the highest mountains in the world, in these locations India extended its forward posts northward to the ridges, regarding this move as compliant with the original border proposal, although the Simla Convention did not explicitly state this intention.

The 1940s saw huge change in South Asia with the Partition of India in 1947 (resulting in the establishment of the two new states of India and Pakistan), and the establishment of the People's Republic of China (PRC) in 1949. One of the most basic policies for the new Indian government was that of maintaining cordial relations with China, reviving its ancient friendly ties. India was among the first nations to grant diplomatic recognition to the newly created PRC.

At the time, Chinese officials issued no condemnation of Nehru's claims or made any opposition to Nehru's open declarations of control over Aksai Chin. In 1956, Chinese Premier Zhou Enlai stated that he had no claims over Indian-controlled territory. He later argued that Aksai Chin was already under Chinese jurisdiction and that the McCartney-MacDonald Line was the line China could accept. Zhou later argued that as the boundary was undemarcated and had never been defined by treaty between any Chinese or Indian government, the Indian government could not unilaterally define Aksai Chin's borders.

In 1950, the Chinese People's Liberation Army took control of Tibet, which all Chinese governments regarded as still part of China. Later the Chinese extended their influence by building a road in 1956–67 and placing border posts in Aksai Chin. India found out after the road was completed, protested against these moves and decided to look for a diplomatic solution to ensure a stable Sino-Indian border. To resolve any doubts about the Indian position, Prime Minister Jawaharlal Nehru declared in parliament that India regarded the McMahon Line as its official border. The Chinese expressed no concern at this statement, and in 1951 and 1952, the government of China asserted that there were no frontier issues to be taken up with India.

In 1954, Prime Minister Nehru wrote a memo calling for India's borders to be clearly defined and demarcated; in line with previous Indian philosophy, Indian maps showed a border that, in some places, lay north of the McMahon Line. Chinese Premier Zhou Enlai, in November 1956, again repeated Chinese assurances that the People's Republic had no claims on Indian territory, although official Chinese maps showed of territory claimed by India as Chinese. CIA documents created at the time revealed that Nehru had ignored Burmese premier Ba Swe when he warned Nehru to be cautious when dealing with Zhou. They also allege that Zhou purposefully told Nehru that there were no border issues with India.

In 1954, China and India negotiated the Five Principles of Peaceful Coexistence, by which the two nations agreed to abide in settling their disputes. India presented a frontier map which was accepted by China, and the slogan "Hindi-Chini bhai-bhai" (Indians and Chinese are brothers) was popular then. Nehru in 1958 had privately told G. Parthasarathi, the Indian envoy to China not to trust the Chinese at all and send all communications directly to him, bypassing the Defence Minister VK Krishna Menon since his communist background clouded his thinking about China. According to Georgia Tech scholar , Nehru's policy on Tibet was to create a strong Sino-Indian partnership which would be catalysed through agreement and compromise on Tibet. Garver believes that Nehru's previous actions had given him confidence that China would be ready to form an "Asian Axis" with India.

This apparent progress in relations suffered a major setback when, in 1959, Nehru accommodated the Tibetan religious leader at the time, the 14th Dalai Lama, who fled Lhasa after a failed Tibetan uprising against Chinese rule. The Chairman of the Chinese Communist Party, Mao Zedong, was enraged and asked the Xinhua News Agency to produce reports on Indian expansionists operating in Tibet.

Border incidents continued through this period. In August 1959, the People's Liberation Army took an Indian prisoner at Longju, which had an ambiguous position in the McMahon Line, and two months later in Aksai Chin, a clash led to the death of nine Indian frontier policemen.

On 2 October, Soviet Premier Nikita Khrushchev defended Nehru in a meeting with Mao. This action reinforced China's impression that the Soviet Union, the United States and India all had expansionist designs on China. The People's Liberation Army went so far as to prepare a self-defence counterattack plan. Negotiations were restarted between the nations, but no progress was made.

As a consequence of their non-recognition of the McMahon Line, China's maps showed both the North East Frontier Area (NEFA) and Aksai Chin to be Chinese territory. In 1960, Zhou Enlai unofficially suggested that India drop its claims to Aksai Chin in return for a Chinese withdrawal of claims over NEFA. Adhering to his stated position, Nehru believed that China did not have a legitimate claim over either of these territories, and thus was not ready to concede them. This adamant stance was perceived in China as Indian opposition to Chinese rule in Tibet. Nehru declined to conduct any negotiations on the boundary until Chinese troops withdrew from Aksai Chin, a position supported by the international community. India produced numerous reports on the negotiations, and translated Chinese reports into English to help inform the international debate. China believed that India was simply securing its claim lines in order to continue its "grand plans in Tibet". India's stance that China withdraw from Aksai Chin caused continual deterioration of the diplomatic situation to the point that internal forces were pressuring Nehru to take a military stance against China.

In 1960, based on an agreement between Nehru and Zhou Enlai, officials from India and China held discussions in order to settle the boundary dispute. China and India disagreed on the major watershed that defined the boundary in the western sector. The Chinese statements with respect to their border claims often misrepresented the cited sources.

At the beginning of 1961, Nehru appointed General as army Chief of General Staff, but he refused to increase military spending and prepare for a possible war. According to James Barnard Calvin of the U.S. Navy, in 1959, India started sending Indian troops and border patrols into disputed areas. This program created both skirmishes and deteriorating relations between India and China. The aim of this policy was to create outposts behind advancing Chinese troops to interdict their supplies, forcing them north of the disputed line. There were eventually 60 such outposts, including 43 north of the McMahon Line, to which India claimed sovereignty. China viewed this as further confirmation of Indian expansionist plans directed towards Tibet. According to the Indian official history, implementation of the Forward Policy was intended to provide evidence of Indian occupation in the previously unoccupied region through which Chinese troops had been advancing. Kaul was confident, through contact with Indian Intelligence and CIA information, that China would not react with force. Indeed, at first the PLA simply withdrew, but eventually Chinese forces began to counter-encircle the Indian positions which clearly encroached into the north of McMahon Line. This led to a tit-for-tat Indian reaction, with each force attempting to outmanoeuver the other. Despite the escalating nature of the dispute, the two forces withheld from engaging each other directly.

Chinese attention was diverted for a time by the military activity of the Nationalists on Taiwan, but on 23 June the U.S. assured China that a Nationalist invasion would not be permitted. China's heavy artillery facing Taiwan could then be moved to Tibet. It took China six to eight months to gather the resources needed for the war, according to Anil Athale, author of the official Indian history. The Chinese sent a large quantity of non-military supplies to Tibet through the Indian port of Calcutta.

Various border conflicts and "military incidents" between India and China flared up throughout the summer and autumn of 1962. In May, the Indian Air Force was told not to plan for close air support, although it was assessed as being a feasible way to counter the unfavourable ratio of Chinese to Indian troops. In June, a skirmish caused the deaths of dozens of Chinese troops. The Indian Intelligence Bureau received information about a Chinese buildup along the border which could be a precursor to war.

During June–July 1962, Indian military planners began advocating "probing actions" against the Chinese, and accordingly, moved mountain troops forward to cut off Chinese supply lines. According to Patterson, the Indian motives were threefold:

On 10 July 1962, 350 Chinese troops surrounded an Indian post in Chushul (north of the McMahon Line) but withdrew after a heated argument via loudspeaker. On 22 July, the Forward Policy was extended to allow Indian troops to push back Chinese troops already established in disputed territory. Whereas Indian troops were previously ordered to fire only in self-defence, all post commanders were now given discretion to open fire upon Chinese forces if threatened. In August, the Chinese military improved its combat readiness along the McMahon Line and began stockpiling ammunition, weapons and fuel.

Given his foreknowledge of the coming Cuban Missile Crisis, Mao Zedong was able to persuade Nikita Khrushchev to reverse the Russian policy of backing India, at least temporarily. In mid-October, the Communist organ "Pravda" encouraged peace between India and China. When the Cuban Missile Crisis ended and Mao's rhetoric changed, Russia reversed course.

In June 1962, Indian forces established an outpost at Dhola, on the southern slopes of the Thag La Ridge. Dhola lay north of the McMahon Line but south of the ridges along which India interpreted the McMahon Line to run. In August, China issued diplomatic protests and began occupying positions at the top of Thag La. On 8 September, a 60-strong PLA unit descended to the south side of the ridge and occupied positions that dominated one of the Indian posts at Dhola. Fire was not exchanged, but Nehru said to the media that the Indian Army had instructions to "free our territory" and the troops had been given discretion to use force. On 11 September, it was decided that "all forward posts and patrols were given permission to fire on any armed Chinese who entered Indian territory".

The operation to occupy Thag La was flawed in that Nehru's directives were unclear and it got underway very slowly because of this. In addition to this, each man had to carry over the long trek and this severely slowed down the reaction. By the time the Indian battalion reached the point of conflict, Chinese units controlled both banks of the Namka Chu River. On 20 September, Chinese troops threw grenades at Indian troops and a firefight developed, triggering a long series of skirmishes for the rest of September.

Some Indian troops, including Brigadier Dalvi who commanded the forces at Thag La, were also concerned that the territory they were fighting for was not strictly territory that "we should have been convinced was ours". According to Neville Maxwell, even members of the Indian defence ministry were categorically concerned with the validity of the fighting in Thag La.

On 4 October, Kaul assigned some troops to secure regions south of the Thag La Ridge. Kaul decided to first secure Yumtso La, a strategically important position, before re-entering the lost Dhola post. Kaul had then realised that the attack would be desperate and the Indian government tried to stop an escalation into all-out war. Indian troops marching to Thag La had suffered in the previously unexperienced conditions; two Gurkha soldiers died of pulmonary edema.

On 10 October, an Indian Punjabi patrol of 50 troops to Yumtso La were met by an emplaced Chinese position of some 1,000 soldiers. Indian troops were in no position for battle, as Yumtso La was 16,000 feet (4,900 m) above sea level and Kaul did not plan on having artillery support for the troops. The Chinese troops opened fire on the Indians under their belief that they were north of the McMahon Line. The Indians were surrounded by Chinese positions which used mortar fire. They managed to hold off the first Chinese assault, inflicting heavy casualties.

At this point, the Indian troops were in a position to push the Chinese back with mortar and machine gun fire. Brigadier Dalvi opted not to fire, as it would mean decimating the Rajput who were still in the area of the Chinese regrouping. They helplessly watched the Chinese ready themselves for a second assault. In the second Chinese assault, the Indians began their retreat, realising the situation was hopeless. The Indian patrol suffered 25 casualties, and the Chinese 33. The Chinese troops held their fire as the Indians retreated, and then buried the Indian dead with military honours, as witnessed by the retreating soldiers. This was the first occurrence of heavy fighting in the war.

This attack had grave implications for India and Nehru tried to solve the issue, but by 18 October, it was clear that the Chinese were preparing for an attack on India, with massive troop buildups on the border. A long line of mules and porters had also been observed supporting the buildup and reinforcement of positions south of the Thag La Ridge.

Two of the major factors leading up to China's eventual conflicts with Indian troops were India's stance on the disputed borders and perceived Indian subversion in Tibet. There was "a perceived need to punish and end perceived Indian efforts to undermine Chinese control of Tibet, Indian efforts which were perceived as having the objective of restoring the pre-1949 status quo ante of Tibet". The other was "a perceived need to punish and end perceived Indian aggression against Chinese territory along the border". John W. Garver argues that the first perception was incorrect based on the state of the Indian military and polity in the 1960s. It was, nevertheless a major reason for China's going to war. He argues the Chinese perception of Indian aggression to be "substantially accurate".

The CIA's declassified POLO documents reveal contemporary American analysis of Chinese motives during the war. According to this document, "Chinese apparently were motivated to attack by one primary consideration — their determination to retain the ground on which PLA forces stood in 1962 and to punish the Indians for trying to take that ground". In general terms, they tried to show the Indians once and for all that China would not acquiesce in a military "reoccupation" policy. Secondary reasons for the attack were to damage Nehru's prestige by exposing Indian weakness and to expose as traitorous Khrushchev's policy of supporting Nehru against a Communist country.

Another factor which might have affected China's decision for war with India was a perceived need to stop a Soviet-U.S.-India encirclement and isolation of China. India's relations with the Soviet Union and United States were both strong at this time, but the Soviets (and Americans) were preoccupied by the Cuban Missile Crisis and would not interfere with the Sino-Indian War. P. B. Sinha suggests that China waited until October to attack because the timing of the war was exactly in parallel with American actions so as to avoid any chance of American or Soviet involvement. Although American buildup of forces around Cuba occurred on the same day as the first major clash at Dhola, and China's buildup between 10 and 20 October appeared to coincide exactly with the United States establishment of a blockade against Cuba which began 20 October, the Chinese probably prepared for this before they could anticipate what would happen in Cuba. Another explanation is that the confrontation in the Taiwan Strait had eased by then.

Garver argues that the Chinese correctly assessed Indian border policies, particularly the Forward Policy, as attempts for incremental seizure of Chinese-controlled territory. On Tibet, Garver argues that one of the major factors leading to China's decision for war with India was a common tendency of humans "to attribute others behavior to interior motivations, while attributing their own behavior to situational factors". Studies from China published in the 1990s confirmed that the root cause for China going to war with India was the perceived Indian aggression in Tibet, with the forward policy simply catalysing the Chinese reaction.

Neville Maxwell and Allen Whiting argue that the Chinese leadership believed they were defending territory that was legitimately Chinese, and which was already under de facto Chinese occupation prior to Indian advances, and regarded the Forward Policy as an Indian attempt at creeping annexation. Mao Zedong himself compared the Forward Policy to a strategic advance in Chinese chess:

India claims that the motive for the Forward Policy was to cut off the supply routes for Chinese troops posted in NEFA and Aksai Chin. According to the official Indian history, the forward policy was continued because of its initial success, as it claimed that Chinese troops withdrew when they encountered areas already occupied by Indian troops. It also claimed that the Forward Policy was having success in cutting out supply lines of Chinese troops who had advanced South of the McMahon Line, though there was no evidence of such advance before the 1962 war. The Forward Policy rested on the assumption that Chinese forces "were not likely to use force against any of our posts, even if they were in a position to do so". No serious re-appraisal of this policy took place even when Chinese forces ceased withdrawing. Nehru's confidence was probably justified given the difficulty for China to supply the area over the high altitude terrain over 5000 km from the more populated areas of China.

The Chinese leadership initially held a sympathetic view towards India as the latter had been ruled by British colonial masters for centuries. Nehru's forward policy convinced PRC leadership that the independent Indian leadership was a reincarnation of British imperialism. Mao Zedong stated: "Rather than being constantly accused of aggression, it's better to show the world what really happens when China indeed moves its muscles."

Chinese policy toward India, therefore, operated on two contradictory assumptions in the first half of 1961. On the one hand, the Chinese leaders continued to entertain a hope, although a shrinking one, that some opening for talks would appear. On the other hand, they read Indian statements and actions as clear signs that Nehru wanted to talk only about a Chinese withdrawal. Regarding the hope, they were willing to negotiate and tried to prod Nehru into a similar attitude. Regarding Indian intentions, they began to act politically and to build a rationale based on the assumption that Nehru already had become a lackey of imperialism; for this reason he opposed border talks.

Krishna Menon is reported to have said that when he arrived in Geneva on 6 June 1961 for an international conference in Laos, Chinese officials in Chen Yi's delegation indicated that Chen might be interested in discussing the border dispute with him. At several private
meetings with Menon, Chen avoided any discussion of the dispute and Menon surmised that the Chinese wanted him to broach the matter first. He did not, as he was under instructions from Nehru to avoid taking the initiative, leaving the Chinese with the impression
that Nehru was unwilling to show any flexibility.

In September, the Chinese took a step toward criticising Nehru openly in their commentary. After citing Indonesian and Burmese press criticism of Nehru by name, the Chinese critiqued his moderate remarks on colonialism (People's Daily Editorial, 9 September): "Somebody at the Non-Aligned Nations Conference advanced the argument that the era of classical colonialism is gone and dead...contrary to facts." This was a distortion of Nehru's remarks but appeared close enough to be credible. On the same day, Chen Yi referred to Nehru by implication at the Bulgarian embassy reception: "Those who attempted to deny history, ignore reality, and distort the truth and who attempted to divert the Conference from its important object have failed to gain support and were isolated." On 10 September, they dropped all circumlocutions and criticised him by name in a China Youth article and NCNA report—the first time in almost two years that they had commented extensively on the Prime Minister.

By early 1962, the Chinese leadership began to believe that India's intentions were to launch a massive attack against Chinese troops, and that the Indian leadership wanted a war. In 1961, the Indian army had been sent into Goa, a small region without any other international borders apart from the Indian one, after Portugal refused to surrender the exclave colony to the Indian Union. Although this action met little to no international protest or opposition, China saw it as an example of India's expansionist nature, especially in light of heated rhetoric from Indian politicians. India's Home Minister declared, "If the Chinese will not vacate the areas occupied by it, India will have to repeat what it did in Goa. India will certainly drive out the Chinese forces", while another member of the Indian Congress Party pronounced, "India will take steps to end [Chinese] aggression on Indian soil just as it ended Portuguese aggression in Goa". By mid-1962, it was apparent to the Chinese leadership that negotiations had failed to make any progress, and the Forward Policy was increasingly perceived as a grave threat as Delhi increasingly sent probes deeper into border areas and cut off Chinese supply lines. Foreign Minister Marshal Chen Yi commented at one high-level meeting, "Nehru's forward policy is a knife. He wants to put it in our heart. We cannot close our eyes and await death." The Chinese leadership believed that their restraint on the issue was being perceived by India as weakness, leading to continued provocations, and that a major counterblow was needed to stop perceived Indian aggression.

Xu Yan, prominent Chinese military historian and professor at the PLA's National Defense University, gives an account of the Chinese leadership's decision to go to war. By late September 1962, the Chinese leadership had begun to reconsider their policy of "armed coexistence", which had failed to address their concerns with the forward policy and Tibet, and consider a large, decisive strike. On 22 September 1962, the "People's Daily" published an article which claimed that "the Chinese people were burning with 'great indignation' over the Indian actions on the border and that New Delhi could not 'now say that warning was not served in advance'."

The Indian side was confident war would not be triggered and made little preparations. India had only two divisions of troops in the region of the conflict. In August 1962, Brigadier D. K. Palit claimed that a war with China in the near future could be ruled out. Even in September 1962, when Indian troops were ordered to "expel the Chinese" from Thag La, Maj. General J. S. Dhillon expressed the opinion that "experience in Ladakh had shown that a few rounds fired at the Chinese would cause them to run away." Because of this, the Indian army was completely unprepared when the attack at Yumtso La occurred.

Recently declassified CIA documents which were compiled at the time reveal that India's estimates of Chinese capabilities made them neglect their military in favour of economic growth. It is claimed that if a more military-minded man had been in place instead of Nehru, India would have been more likely to have been ready for the threat of a counter-attack from China.

On 6 October 1962, the Chinese leadership convened. Lin Biao reported that PLA intelligence units had determined that Indian units might assault Chinese positions at Thag La on 10 October (Operation Leghorn). The Chinese leadership and the Central Military Council decided upon war to launch a large-scale attack to punish perceived military aggression from India. In Beijing, a larger meeting of Chinese military was convened in order to plan for the coming conflict.

Mao and the Chinese leadership issued a directive laying out the objectives for the war. A main assault would be launched in the eastern sector, which would be coordinated with a smaller assault in the western sector. All Indian troops within China's claimed territories in the eastern sector would be expelled, and the war would be ended with a unilateral Chinese ceasefire and withdrawal, followed by a return to the negotiating table. India led the Non-Aligned Movement, Nehru enjoyed international prestige, and China, with a larger military, would be portrayed as an aggressor. He said that a well-fought war "will guarantee at least thirty years of peace" with India, and determined the benefits to offset the costs.

China also reportedly bought significant amount of Indian rupee currency notes from Hong Kong, supposedly to distribute amongst its soldiers in preparation for the war.

On 8 October, additional veteran and elite divisions were ordered to prepare to move into Tibet from the Chengdu and Lanzhou military regions.

On 12 October, Nehru declared that he had ordered the Indian army to "clear Indian territory in the NEFA of Chinese invaders" and personally met with Kaul, issuing instructions to him.

On 14 October, an editorial on "People's Daily" issued China's final warning to India: "So it seems that Mr. Nehru has made up his mind to attack the Chinese frontier guards on an even bigger scale.  ... It is high time to shout to Mr. Nehru that the heroic Chinese troops, with the glorious tradition of resisting foreign aggression, can never be cleared by anyone from their own territory ... If there are still some maniacs who are reckless enough to ignore our well-intentioned advice and insist on having another try, well, let them do so. History will pronounce its inexorable verdict ... At this critical moment ... we still want to appeal once more to Mr. Nehru: better rein in at the edge of the precipice and do not use the lives of Indian troops as stakes in your gamble."

Marshal Liu Bocheng headed a group to determine the strategy for the war. He concluded that the opposing Indian troops were among India's best, and to achieve victory would require deploying crack troops and relying on force concentration to achieve decisive victory. On 16 October, this war plan was approved, and on the 18th, the final approval was given by the Politburo for a "self-defensive counter-attack", scheduled for 20 October.

On 20 October 1962, the Chinese People's Liberation Army launched two attacks, 1000 kilometres apart. In the western theatre, the PLA sought to expel Indian forces from the Chip Chap valley in Aksai Chin while in the eastern theatre, the PLA sought to capture both banks of the Namka Chu river. Some skirmishes also took place at the Nathula Pass, which is in the Indian state of Sikkim (an Indian protectorate at that time). Gurkha rifles travelling north were targeted by Chinese artillery fire. After four days of fierce fighting, the three regiments of Chinese troops succeeded in securing a substantial portion of the disputed territory.

Chinese troops launched an attack on the southern banks of the Namka Chu River on 20 October. The Indian forces were undermanned, with only an understrength battalion to support them, while the Chinese troops had three regiments positioned on the north side of the river. The Indians expected Chinese forces to cross via one of five bridges over the river and defended those crossings. The PLA bypassed the defenders by crossing the shallow October river instead. They formed up into battalions on the Indian-held south side of the river under cover of darkness, with each battalion assigned against a separate group of Rajputs.

At 5:14 am, Chinese mortar fire began attacking the Indian positions. Simultaneously, the Chinese cut the Indian telephone lines, preventing the defenders from making contact with their headquarters. At about 6:30 am, the Chinese infantry launched a surprise attack from the rear and forced the Indians to leave their trenches.

The Chinese overwhelmed the Indian troops in a series of flanking manoeuvres south of the McMahon Line and prompted their withdrawal from Namka Chu. Fearful of continued losses, Indian troops retreated into Bhutan. Chinese forces respected the border and did not pursue. Chinese forces now held all of the territory that was under dispute at the time of the Thag La confrontation, but they continued to advance into the rest of NEFA.

On 22 October, at 12:15 am, PLA mortars fired on Walong, on the McMahon line. Flares launched by Indian troops the next day revealed numerous Chinese milling around the valley. The Indians tried to use their mortars against the Chinese but the PLA responded by lighting a bush fire, causing confusion among the Indians. Some 400 Chinese troops attacked the Indian position. The initial Chinese assault was halted by accurate Indian mortar fire. The Chinese were then reinforced and launched a second assault. The Indians managed to hold them back for four hours, but the Chinese used weight of numbers to break through. Most Indian forces were withdrawn to established positions in Walong, while a company supported by mortars and medium machine guns remained to cover the retreat.

On the morning 23 October, the Indian Army discovered a Chinese force gathered in a cramped pass and opened fire with mortars and machine guns, leading to heavy fighting. About 200 Chinese soldiers were killed and wounded in this action. Nine Indian soldiers were also killed. The fighting continued well into the afternoon, until the company was ordered to withdraw. Meanwhile, the 4th Sikhs made contact with the Chinese and subjected them to withering mortar and machine gun fire as the Chinese set off a brush fire and attempted to sneak forward.

Elsewhere, Chinese troops launched a three-pronged attack on Tawang, which the Indians evacuated without any resistance.

Over the following days, there were clashes between Indian and Chinese patrols at Walong as the Chinese rushed in reinforcements. On 25 October, the Chinese made a probe, which was met with resistance from the 4th Sikhs. The following day, a patrol from the 4th Sikhs was encircled, and after being unable to break the encirclement, an Indian unit was able to flank the Chinese, allowing the Sikhs to break free.

On the Aksai Chin front, China already controlled most of the disputed territory. Chinese forces quickly swept the region of any remaining Indian troops. Late on 19 October, Chinese troops launched a number of attacks throughout the western theatre. By 22 October, all posts north of Chushul had been cleared.

On 20 October, the Chinese easily took the Chip Chap Valley, Galwan Valley, and Pangong Lake. Many outposts and garrisons along the Western front were unable to defend against the surrounding Chinese troops. Most Indian troops positioned in these posts offered resistance but were either killed or taken prisoner. Indian support for these outposts was not forthcoming, as evidenced by the Galwan post, which had been surrounded by enemy forces in August, but no attempt made to relieve the besieged garrison. Following the 20 October attack, nothing was heard from Galwan.

On 24 October, Indian forces fought hard to hold the Rezang La Ridge, in order to prevent a nearby airstrip from falling to the Chinese.

After realising the magnitude of the attack, the Indian Western Command withdrew many of the isolated outposts to the south-east. Daulet Beg Oldi was also evacuated, but it was south of the Chinese claim line and was not approached by Chinese forces. Indian troops were withdrawn in order to consolidate and regroup in the event that China probed south of their claim line.

By 24 October, the PLA had entered territory previously administered by India to give the PRC a diplomatically strong position over India. The majority of Chinese forces had advanced sixteen kilometres south of the control line prior to the conflict. Four days of fighting were followed by a three-week lull. Zhou ordered the troops to stop advancing as he attempted to negotiate with Nehru. The Indian forces had retreated into more heavily fortified positions around Se La and Bomdi La which would be difficult to assault. Zhou sent Nehru a letter, proposing

Nehru's 27 October reply expressed interest in the restoration of peace and friendly relations and suggested a return to the "boundary prior to 8 September 1962". He was categorically concerned about a mutual twenty kilometre withdrawal after "40 or 60 kilometres of blatant military aggression". He wanted the creation of a larger immediate buffer zone and thus resist the possibility of a repeat offensive. Zhou's 4 November reply repeated his 1959 offer to return to the McMahon Line in NEFA and the Chinese traditionally claimed MacDonald Line in Aksai Chin. Facing Chinese forces maintaining themselves on Indian soil and trying to avoid political pressure, the Indian parliament announced a national emergency and passed a resolution which stated their intent to "drive out the aggressors from the sacred soil of India". The United States and the United Kingdom supported India's response. The Soviet Union was preoccupied with the Cuban Missile Crisis and did not offer the support it had provided in previous years. With the backing of other great powers, a 14 November letter by Nehru to Zhou once again rejected his proposal.

Neither side declared war, used their air force, or fully broke off diplomatic relations, but the conflict is commonly referred to as a war. This war coincided with the Cuban Missile Crisis and was viewed by the western nations at the time as another act of aggression by the Communist bloc.
According to Calvin, the Chinese side evidently wanted a diplomatic resolution and discontinuation of the conflict.

After Zhou received Nehru's letter (rejecting Zhou's proposal), the fighting resumed on the eastern theatre on 14 November (Nehru's birthday), with an Indian attack on Walong, claimed by China, launched from the defensive position of Se La and inflicting heavy casualties on the Chinese. The Chinese resumed military activity on Aksai Chin and NEFA hours after the Walong battle.

In the eastern theatre, the PLA attacked Indian forces near Se La and Bomdi La on 17 November. These positions were defended by the Indian 4th Infantry Division. Instead of attacking by road as expected, PLA forces approached via a mountain trail, and their attack cut off a main road and isolated 10,000 Indian troops.

Se La occupied high ground, and rather than assault this commanding position, the Chinese captured Thembang, which was a supply route to Se La.

On the western theatre, PLA forces launched a heavy infantry attack on 18 November near Chushul. Their attack started at 4:35 am, despite a mist surrounding most of the areas in the region. At 5:45 the Chinese troops advanced to attack two platoons of Indian troops at Gurung Hill.

The Indians did not know what was happening, as communications were dead. As a patrol was sent, China attacked with greater numbers. Indian artillery could not hold off the superior Chinese forces. By 9:00 am, Chinese forces attacked Gurung Hill directly and Indian commanders withdrew from the area and also from the connecting Spangur Gap.

The Chinese had been simultaneously attacking Rezang La which was held by 123 Indian troops. At 5:05 am, Chinese troops launched their attack audaciously. Chinese medium machine gun fire pierced through the Indian tactical defences.

At 6:55 am the sun rose and the Chinese attack on the 8th platoon began in waves. Fighting continued for the next hour, until the Chinese signaled that they had destroyed the 7th platoon. Indians tried to use light machine guns on the medium machine guns from the Chinese but after 10 minutes the battle was over. Logistical inadequacy once again hurt the Indian troops. The Chinese gave the Indian troops a respectful military funeral. The battles also saw the death of Major Shaitan Singh of the Kumaon Regiment, who had been instrumental in the first battle of Rezang La. The Indian troops were forced to withdraw to high mountain positions. Indian sources believed that their troops were just coming to grips with the mountain combat and finally called for more troops. The Chinese declared a ceasefire, ending the bloodshed.

Indian forces suffered heavy casualties, with dead Indian troops' bodies being found in the ice, frozen with weapons in hand. The Chinese forces also suffered heavy casualties, especially at Rezang La. This signalled the end of the war in Aksai Chin as China had reached their claim line – many Indian troops were ordered to withdraw from the area. China claimed that the Indian troops wanted to fight on until the bitter end. The war ended with their withdrawal, so as to limit the amount of casualties.

The PLA penetrated close to the outskirts of Tezpur, Assam, a major frontier town nearly fifty kilometres from the Assam-North-East Frontier Agency border. The local government ordered the evacuation of the civilians in Tezpur to the south of the Brahmaputra River, all prisons were thrown open, and government officials who stayed behind destroyed Tezpur's currency reserves in anticipation of a Chinese advance.

China had reached its claim lines so the PLA did not advance farther, and on 19 November, it declared a unilateral cease-fire. Zhou Enlai declared a unilateral ceasefire to start on midnight, 21 November. Zhou's ceasefire declaration stated,

Zhou had first given the ceasefire announcement to Indian chargé d'affaires on 19 November (before India's request for United States air support), but New Delhi did not receive it until 24 hours later. The aircraft carrier was ordered back after the ceasefire, and thus, American intervention on India's side in the war was avoided. Retreating Indian troops, who hadn't come into contact with anyone knowing of the ceasefire, and Chinese troops in NEFA and Aksai Chin, were involved in some minor battles, but for the most part, the ceasefire signalled an end to the fighting. The United States Air Force flew in supplies to India in November 1962, but neither side wished to continue hostilities.

Toward the end of the war India increased its support for Tibetan refugees and revolutionaries, some of them having settled in India, as they were fighting the same common enemy in the region. The Nehru administration ordered the raising of an elite Indian-trained "Tibetan Armed Force" composed of Tibetan refugees.
The Chinese military action has been viewed by the United States as part of the PRC's policy of making use of aggressive wars to settle its border disputes and to distract both its own population and international opinion from its internal issues. According to James Calvin from the United States Marine Corps, western nations at the time viewed China as an aggressor during the China–India border war, and the war was part of a monolithic communist objective for a world dictatorship of the proletariat. This was further triggered by Mao Zedong's views that: "The way to world conquest lies through Havana, Accra, and Calcutta". Calvin believes that Chinese actions show a "pattern of conservative aims and limited objectives, rather than expansionism" and blames this particular conflict on India's provocations towards China. Calvin also expresses that China, in the past, has been adamant to gain control over regions to which it has a "traditional claim", which triggered the dispute over NEFA and Aksai Chin and indeed Tibet. Calvin's assumption, based on the history of the Cold War and the Domino Effect, assumed that China might ultimately try to regain control of everything that it considers as "traditionally Chinese" which in its view includes the entirety of South East Asia.

The Kennedy administration was disturbed by what they considered "blatant Chinese communist aggression against India". In a May 1963 National Security Council meeting, contingency planning on the part of the United States in the event of another Chinese attack on India was discussed. Defense Secretary Robert McNamara and General Maxwell Taylor advised the president to use nuclear weapons should the Americans intervene in such a situation. McNamara stated "Before any substantial commitment to defend India against China is given, we should recognise that in order to carry out that commitment against any substantial Chinese attack, we would have to use nuclear weapons. Any large Chinese Communist attack on any part of that area would require the use of nuclear weapons by the U.S., and this is to be preferred over the introduction of large numbers of U.S. soldiers." After hearing this and listening to two other advisers, Kennedy stated "We should defend India, and therefore we will defend India." It remains unclear if his aides were trying to dissuade the President of considering any measure with regard to India by immediately raising the stakes to an unacceptable level, nor is it clear if Kennedy was thinking of conventional or nuclear means when he gave his reply. By 1964 China had developed its own nuclear weapon which would have likely caused any American nuclear policy in defense of India to be reviewed. The Johnson Administration considered and then rejected giving nuclear weapons technology to the Indians. India developed its own nuclear weapon by 1974, within 10 years of the Chinese.

The United States was unequivocal in its recognition of the Indian boundary claims in the eastern sector, while not supporting the claims of either side in the western sector. Britain, on the other hand, agreed with the Indian position completely, with the foreign secretary stating, 'we have taken the view of the government of India on the present frontiers and the disputed territories belong to India.'

The non-aligned nations remained mostly uninvolved, and only the United Arab Republic openly supported India. Of the non-aligned nations, six, Egypt, Burma, Cambodia, Sri Lanka, Ghana and Indonesia, met in Colombo on 10 December 1962. The proposals stipulated a Chinese withdrawal of 20 km from the customary lines without any reciprocal withdrawal on India's behalf. The failure of these six nations to unequivocally condemn China deeply disappointed India.

In 1972, Chinese Premier Zhou explained the Chinese point of view to President Nixon of the US. As for the causes of the war, Zhou asserted that China did not try to expel Indian troops from south of the McMahon line and that three open warning telegrams were sent to Nehru before the war. Indian patrols south of the McMahon line were expelled and suffered casualties in the Chinese attack. Zhou also told Nixon that Chairman Mao ordered the troops to return to show good faith. The Indian government maintains that the Chinese military could not advance further south due to logistical problems and the cut-off of resource supplies.

While Western nations did not view Chinese actions favourably because of fear of the Chinese and competitiveness, Pakistan, which had had a turbulent relationship with India ever since the Indian partition, improved its relations with China after the war. Prior to the war, Pakistan also shared a disputed boundary with China, and had proposed to India that the two countries adopt a common defence against "northern" enemies (i.e. China), which was rejected by India. China and Pakistan took steps to peacefully negotiate their shared boundaries, beginning on 13 October 1962, and concluding in December of that year. Pakistan also expressed fear that the huge amounts of western military aid directed to India would allow it to threaten Pakistan's security in future conflicts. Mohammed Ali, External Affairs Minister of Pakistan, declared that massive Western aid to India in the Sino-Indian dispute would be considered an unfriendly act towards Pakistan. As a result, Pakistan made efforts to improve its relations with China. The following year, China and Pakistan peacefully settled disputes on their shared border, and negotiated the China-Pakistan Border Treaty in 1963, as well as trade, commercial, and barter treaties. On 2 March 1963, Pakistan conceded its northern claim line in Pakistani-controlled Kashmir to China in favor of a more southerly boundary along the Karakoram Range. The border treaty largely set the border along the MacCartney-Macdonald Line. India's military failure against China would embolden Pakistan to initiate the Second Kashmir War with India. It effectively ended in a stalemate as Calvin states that the Sino-Indian War had caused the previously passive government to take a stand on actively modernising India's military. China offered diplomatic support to Pakistan in this war but did not offer military support. In January 1966, China condemned the Tashkent Agreement between India and Pakistan as a Soviet-US plot in the region. In the Indo-Pakistani War of 1971, Pakistan expected China to provide military support, but it was left alone as India successfully helped the rebels in East Pakistan to found the new nation-state of Bangladesh.

During the conflict, Nehru wrote two desperate letters to U.S. President John F. Kennedy, requesting 12 squadrons of fighter jets and a modern radar system. These jets were seen as necessary to beef up Indian air strength so that air-to-air combat could be initiated safely from the Indian perspective (bombing troops was seen as unwise for fear of Chinese retaliatory action). Nehru also asked that these aircraft be manned by American pilots until Indian airmen were trained to replace them. These requests were rejected by the Kennedy Administration (which was involved in the Cuban Missile Crisis during most of the Sino-Indian War). The U.S. nonetheless provided non-combat assistance to Indian forces and planned to send the carrier USS "Kitty Hawk" to the Bay of Bengal to support India in case of an air war.

Some reports suggest a contradictory response from the U.S. According to former Indian diplomat G. Parthasarathy, "only after we got nothing from the US did arms supplies from the Soviet Union to India commence." In 1962, President of Pakistan Ayub Khan made clear to India that Indian troops could safely be transferred from the Pakistan frontier to the Himalayas.

According to the China's official military history, the war achieved China's policy objectives of securing borders in its western sector, as China retained de facto control of the Aksai Chin. After the war, India abandoned the Forward Policy, and the de facto borders stabilised along the Line of Actual Control.

According to James Calvin of Marine Corps Command and Staff College, even though China won a military victory it lost in terms of its international image. China's first nuclear weapon test in October 1964 and its support of Pakistan in the 1965 India Pakistan War tended to confirm the American view of communist world objectives, including Chinese influence over Pakistan.

Lora Saalman opined in a study of Chinese military publications, that while the war led to much blame, debates and ultimately acted as causation of military modernisation of India but the war is now treated as basic reportage of facts with relatively diminished interest by Chinese analysts.

The aftermath of the war saw sweeping changes in the Indian military to prepare it for similar conflicts in the future, and placed pressure on Indian prime minister Jawaharlal Nehru, who was seen as responsible for failing to anticipate the Chinese attack on India. Indians reacted with a surge in patriotism and memorials were erected for many of the Indian troops who died in the war. Arguably, the main lesson India learned from the war was the need to strengthen its own defences and a shift from Nehru's foreign policy with China based on his stated concept of "brotherhood". Because of India's inability to anticipate Chinese aggression, Prime Minister Nehru faced harsh criticism from government officials, for having promoted pacifist relations with China. Indian President Radhakrishnan said that Nehru's government was naive and negligent about preparations, and Nehru admitted his failings. According to Inder Malhotra, a former editor of "The Times of India" and a commentator for "The Indian Express", Indian politicians invested more effort in removing Defence Minister Krishna Menon than in actually waging war. Krishna Menon's favoritism weakened the Indian Army, and national morale dimmed. The public saw the war as a political and military debacle. Under American advice (by American envoy John Kenneth Galbraith who made and ran American policy on the war as all other top policy makers in the US were absorbed in coincident Cuban Missile Crisis) Indians refrained, not according to the best choices available, from using the Indian air force to beat back the Chinese advances. The CIA later revealed that at that time the Chinese had neither the fuel nor runways long enough for using their air force effectively in Tibet. Indians in general became highly sceptical of China and its military. Many Indians view the war as a betrayal of India's attempts at establishing a long-standing peace with China and started to question the once popular "Hindi-Chini bhai-bhai" (meaning "Indians and Chinese are brothers"). The war also put an end to Nehru's earlier hopes that India and China would form a strong Asian Axis to counteract the increasing influence of the Cold War bloc superpowers.

The unpreparedness of the army was blamed on Defence Minister Menon, who resigned his government post to allow for someone who might modernise India's military further. India's policy of weaponisation via indigenous sources and self-sufficiency was thus cemented. Sensing a weakened army, Pakistan, a close ally of China, began a policy of provocation against India by infiltrating Jammu and Kashmir and ultimately triggering the Second Kashmir War with India in 1965 and Indo-Pakistani war of 1971. The Attack of 1965 was successfully stopped and ceasefire was negotiated under international pressure. In the Indo-Pakistani war of 1971 India won a clear victory, resulting in liberation of Bangladesh (formerly East-Pakistan).

As a result of the war, the Indian government commissioned an investigation, resulting in the classified Henderson Brooks–Bhagat Report on the causes of the war and the reasons for failure. India's performance in high-altitude combat in 1962 led to an overhaul of the Indian Army in terms of doctrine, training, organisation and equipment. Neville Maxwell claimed that the Indian role in international affairs after the border war was also greatly reduced after the war and India's standing in the non-aligned movement suffered. The Indian government has attempted to keep the Hendersen-Brooks-Bhagat Report secret for decades, although portions of it have recently been leaked by Neville Maxwell.

According to James Calvin, an analyst from the U.S. Navy, India gained many benefits from the 1962 conflict. This war united the country as never before. India got 32,000 square miles (8.3 million hectares, 83,000 km) of disputed territory even if it felt that NEFA was hers all along. The new Indian republic had avoided international alignments; by asking for help during the war, India demonstrated its willingness to accept military aid from several sectors. And, finally, India recognised the serious weaknesses in its army. It would more than double its military manpower in the next two years and it would work hard to resolve the military's training and logistic problems to later become the second-largest army in the world. India's efforts to improve its military posture significantly enhanced its army's capabilities and preparedness. This played a role in subsequent wars against Pakistan.

Soon after the end of the war, the Indian government passed the Defence of India Act in December 1962, permitting the "apprehension and detention in custody of any person [suspected] of being of hostile origin." The broad language of the act allowed for the arrest of any person simply for having a Chinese surname, Chinese ancestry or a Chinese spouse. The Indian government incarcerated thousands of Chinese-Indians in an internment camp in Deoli, Rajasthan, where they were held for years without trial. The last internees were not released until 1967. Thousands more Chinese-Indians were forcibly deported or coerced to leave India. Nearly all internees had their properties sold off or looted. Even after their release, the Chinese Indians faced many restrictions in their freedom. They could not travel freely until the mid-1990s.

India also reported some military conflicts with China after the 1962 war. In late 1967, there were two incidents in which both countries exchanged fire in Sikkim. The first one was dubbed the "Nathu La incident", and the other being "Chola incident" in which advancing Chinese forces were forced to withdraw from Sikkim, then a protectorate of India and later a state of India after annexation in 1975. In the 1987 Sino-Indian skirmish, both sides showed military restraint and it was a bloodless conflict.

In 1993 and 1996, the two sides signed the Sino-Indian Bilateral Peace and Tranquility Accords, agreements to maintain peace and tranquility along the Line of Actual Control (LoAC). Ten meetings of a Sino-Indian Joint Working Group (SIJWG) and five of an expert group have taken place to determine where the LoAC lies, but little progress has occurred.

On 20 November 2006 Indian politicians from Arunachal Pradesh expressed their concern over Chinese military modernization and appealed to parliament to take a harder stance on the PRC following a military buildup on the border similar to that in 1962. Additionally, China's military aid to Pakistan as well is a matter of concern to the Indian public, as the two sides have engaged in various wars.

On 6 July 2006, the historic Silk Road passing through this territory via the Nathu La pass was reopened. Both sides have agreed to resolve the issues by peaceful means.

In October 2011, it was stated that India and China will formulate a border mechanism to handle different perceptions as to the LAC and resume the bilateral army exercises between Indian and Chinese army from early 2012.







</doc>
<doc id="29430" url="https://en.wikipedia.org/wiki?curid=29430" title="Simple module">
Simple module

In mathematics, specifically in ring theory, the simple modules over a ring "R" are the (left or right) modules over "R" that have no non-zero proper submodules. Equivalently, a module "M" is simple if and only if every cyclic submodule generated by a non-zero element of "M" equals "M". Simple modules form building blocks for the modules of finite length, and they are analogous to the simple groups in group theory.

In this article, all modules will be assumed to be right unital modules over a ring "R".

Z-modules are the same as abelian groups, so a simple Z-module is an abelian group which has no non-zero proper subgroups. These are the cyclic groups of prime order.

If "I" is a right ideal of "R", then "I" is simple as a right module if and only if "I" is a minimal non-zero right ideal: If "M" is a non-zero proper submodule of "I", then it is also a right ideal, so "I" is not minimal. Conversely, if "I" is not minimal, then there is a non-zero right ideal "J" properly contained in "I". "J" is a right submodule of "I", so "I" is not simple.

If "I" is a right ideal of "R", then "R"/"I" is simple if and only if "I" is a maximal right ideal: If "M" is a non-zero proper submodule of "R"/"I", then the preimage of "M" under the quotient map is a right ideal which is not equal to "R" and which properly contains "I". Therefore, "I" is not maximal. Conversely, if "I" is not maximal, then there is a right ideal "J" properly containing "I". The quotient map has a non-zero kernel which is not equal to , and therefore is not simple.

Every simple "R"-module is isomorphic to a quotient "R"/"m" where "m" is a maximal right ideal of "R". By the above paragraph, any quotient "R"/"m" is a simple module. Conversely, suppose that "M" is a simple "R"-module. Then, for any non-zero element "x" of "M", the cyclic submodule "xR" must equal "M". Fix such an "x". The statement that "xR" = "M" is equivalent to the surjectivity of the homomorphism that sends "r" to "xr". The kernel of this homomorphism is a right ideal "I" of "R", and a standard theorem states that "M" is isomorphic to "R"/"I". By the above paragraph, we find that "I" is a maximal right ideal. Therefore, "M" is isomorphic to a quotient of "R" by a maximal right ideal.

If "k" is a field and "G" is a group, then a group representation of "G" is a left module over the group ring "k[G]" (for details, see the main page on this relationship). The simple "k[G]" modules are also known as irreducible representations. A major aim of representation theory is to understand the irreducible representations of groups.

The simple modules are precisely the modules of length 1; this is a reformulation of the definition.

Every simple module is indecomposable, but the converse is in general not true.

Every simple module is cyclic, that is it is generated by one element.

Not every module has a simple submodule; consider for instance the Z-module Z in light of the first example above.

Let "M" and "N" be (left or right) modules over the same ring, and let "f" : "M" → "N" be a module homomorphism. If "M" is simple, then "f" is either the zero homomorphism or injective because the kernel of "f" is a submodule of "M". If "N" is simple, then "f" is either the zero homomorphism or surjective because the image of "f" is a submodule of "N". If "M" = "N", then "f" is an endomorphism of "M", and if "M" is simple, then the prior two statements imply that "f" is either the zero homomorphism or an isomorphism. Consequently, the endomorphism ring of any simple module is a division ring. This result is known as Schur's lemma.

The converse of Schur's lemma is not true in general. For example, the Z-module Q is not simple, but its endomorphism ring is isomorphic to the field Q.

If "M" is a module which has a non-zero proper submodule "N", then there is a short exact sequence
A common approach to proving a fact about "M" is to show that the fact is true for the center term of a short exact sequence when it is true for the left and right terms, then to prove the fact for "N" and "M"/"N". If "N" has a non-zero proper submodule, then this process can be repeated. This produces a chain of submodules
In order to prove the fact this way, one needs conditions on this sequence and on the modules "M"/"M". One particularly useful condition is that the length of the sequence is finite and each quotient module "M"/"M" is simple. In this case the sequence is called a composition series for "M". In order to prove a statement inductively using composition series, the statement is first proved for simple modules, which form the base case of the induction, and then the statement is proved to remain true under an extension of a module by a simple module. For example, the Fitting lemma shows that the endomorphism ring of a finite length indecomposable module is a local ring, so that the strong Krull-Schmidt theorem holds and the category of finite length modules is a Krull-Schmidt category.

The Jordan–Hölder theorem and the Schreier refinement theorem describe the relationships amongst all composition series of a single module. The Grothendieck group ignores the order in a composition series and views every finite length module as a formal sum of simple modules. Over semisimple rings, this is no loss as every module is a semisimple module and so a direct sum of simple modules. Ordinary character theory provides better arithmetic control, and uses simple C"G" modules to understand the structure of finite groups "G". Modular representation theory uses Brauer characters to view modules as formal sums of simple modules, but is also interested in how those simple modules are joined together within composition series. This is formalized by studying the Ext functor and describing the module category in various ways including quivers (whose nodes are the simple modules and whose edges are composition series of non-semisimple modules of length 2) and Auslander–Reiten theory where the associated graph has a vertex for every indecomposable module.

An important advance in the theory of simple modules was the Jacobson density theorem. The Jacobson density theorem states:
In particular, any primitive ring may be viewed as (that is, isomorphic to) a ring of "D"-linear operators on some "D"-space.

A consequence of the Jacobson density theorem is Wedderburn's theorem; namely that any right artinian simple ring is isomorphic to a full matrix ring of "n" by "n" matrices over a division ring for some "n". This can also be established as a corollary of the Artin–Wedderburn theorem.



</doc>
<doc id="29438" url="https://en.wikipedia.org/wiki?curid=29438" title="Sonar">
Sonar

Sonar (originally an acronym for SOund Navigation And Ranging) is a technique that uses sound propagation (usually underwater, as in submarine navigation) to navigate, communicate with or detect objects on or under the surface of the water, such as other vessels. Two types of technology share the name "sonar": "passive" sonar is essentially listening for the sound made by vessels; "active" sonar is emitting pulses of sounds and listening for echoes. Sonar may be used as a means of acoustic location and of measurement of the echo characteristics of "targets" in the water. Acoustic location in air was used before the introduction of radar. Sonar may also be used in air for robot navigation, and SODAR (an upward looking in-air sonar) is used for atmospheric investigations. The term "sonar" is also used for the equipment used to generate and receive the sound. The acoustic frequencies used in sonar systems vary from very low (infrasonic) to extremely high (ultrasonic). The study of underwater sound is known as underwater acoustics or hydroacoustics.

Although some animals (dolphins and bats) have used sound for communication and object detection for millions of years, use by humans in the water is initially recorded by Leonardo da Vinci in 1490: a tube inserted into the water was said to be used to detect vessels by placing an ear to the tube.

In the 19th century an underwater bell was used as an ancillary to lighthouses to provide warning of hazards.

The use of sound to "echo-locate" underwater in the same way as bats use sound for aerial navigation seems to have been prompted by the "Titanic" disaster of 1912. The world's first patent for an underwater echo-ranging device was filed at the British Patent Office by English meteorologist Lewis Fry Richardson a month after the sinking of the Titanic, and a German physicist Alexander Behm obtained a patent for an echo sounder in 1913.

The Canadian engineer Reginald Fessenden, while working for the Submarine Signal Company in Boston, built an experimental system beginning in 1912, a system later tested in Boston Harbor, and finally in 1914 from the U.S. Revenue (now Coast Guard) Cutter Miami on the Grand Banks off Newfoundland Canada. In that test, Fessenden demonstrated depth sounding, underwater communications (Morse code) and echo ranging (detecting an iceberg at 2 miles (3 km) range). The so-called Fessenden oscillator, at about 500 Hz frequency, was unable to determine the bearing of the berg due to the 3-metre wavelength and the small dimension of the transducer's radiating face (less than 1 metre in diameter). The ten Montreal-built British H-class submarines launched in 1915 were equipped with a Fessenden oscillator.

During World War I the need to detect submarines prompted more research into the use of sound. The British made early use of underwater listening devices called hydrophones, while the French physicist Paul Langevin, working with a Russian immigrant electrical engineer Constantin Chilowsky, worked on the development of active sound devices for detecting submarines in 1915. Although piezoelectric and magnetostrictive transducers later superseded the electrostatic transducers they used, this work influenced future designs. Lightweight sound-sensitive plastic film and fibre optics have been used for hydrophones (acousto-electric transducers for in-water use), while Terfenol-D and PMN (lead magnesium niobate) have been developed for projectors.

In 1916, under the British Board of Invention and Research, Canadian physicist Robert William Boyle took on the active sound detection project with A. B. Wood, producing a prototype for testing in mid-1917. This work, for the Anti-Submarine Division of the British Naval Staff, was undertaken in utmost secrecy, and used quartz piezoelectric crystals to produce the world's first practical underwater active sound detection apparatus. To maintain secrecy, no mention of sound experimentation or quartz was made – the word used to describe the early work ("supersonics") was changed to "ASD"ics, and the quartz material to "ASD"ivite: "ASD" for "Anti-Submarine Division", hence the British acronym "ASDIC". In 1939, in response to a question from the Oxford English Dictionary, the Admiralty made up the story that it stood for "Allied Submarine Detection Investigation Committee", and this is still widely believed, though no committee bearing this name has been found in the Admiralty archives.

By 1918, both France and Britain had built prototype active systems. The British tested their ASDIC on in 1920 and started production in 1922. The 6th Destroyer Flotilla had ASDIC-equipped vessels in 1923. An anti-submarine school HMS "Osprey" and a training flotilla of four vessels were established on Portland in 1924. The U.S. Sonar QB set arrived in 1931.

By the outbreak of World War II, the Royal Navy had five sets for different surface ship classes, and others for submarines, incorporated into a complete anti-submarine attack system. The effectiveness of early ASDIC was hamstrung by the use of the depth charge as an anti-submarine weapon. This required an attacking vessel to pass over a submerged contact before dropping charges over the stern, resulting in a loss of ASDIC contact in the moments leading up to attack. The hunter was effectively firing blind, during which time a submarine commander could take evasive action. This situation was remedied by using several ships cooperating and by the adoption of "ahead-throwing weapons", such as Hedgehog and later Squid, which projected warheads at a target ahead of the attacker and thus still in ASDIC contact. Developments during the war resulted in British ASDIC sets that used several different shapes of beam, continuously covering blind spots. Later, acoustic torpedoes were used.

At the start of World War II, British ASDIC technology was transferred for free to the United States. Research on ASDIC and underwater sound was expanded in the UK and in the US. Many new types of military sound detection were developed. These included sonobuoys, first developed by the British in 1944 under the codename "High Tea", dipping/dunking sonar and mine detection sonar. This work formed the basis for post-war developments related to countering the nuclear submarine.

Work on sonar had also been carried out in the Axis countries, notably in Germany, which included countermeasures. At the end of World War II, this German work was assimilated by Britain and the U.S. Sonars have continued to be developed by many countries, including USSR, for both military and civil uses. In recent years the major military development has been the increasing interest in low-frequency active sonar.

During the 1930s American engineers developed their own underwater sound-detection technology, and important discoveries were made, such as thermoclines, that would help future development. After technical information was exchanged between the two countries during the Second World War, Americans began to use the term "SONAR" for their systems, coined as the equivalent of RADAR.

In 1917, the US Navy acquired Dr. J. Warren Horton's services for the first time. On leave from Bell Labs, he served the Government as a technical expert, first at the experimental station at Nahant, Massachusetts, and later at US Naval Headquarters, in London, England. At Nahant he applied the newly developed vacuum tube, then associated with the formative stages of the field of applied science now known as electronics, to the detection of underwater signals. As a result, the carbon button microphone, which had been used in earlier detection equipment, was replaced by the precursor of the modern hydrophone. Also during this period, he experimented with methods for towing detection. This was due to the increased sensitivity of his device. The principles are still used in modern towed sonar systems.

To meet the defense needs of Great Britain, he was sent to England to install in the Irish Sea bottom-mounted hydrophones connected to a shore listening post by submarine cable. While this equipment was being loaded on the cable-laying vessel, the war ended and Dr. Horton returned home.

During World War II, he continued his participation in the development of sonar systems for the detection of submarins, mines, and torpedoes.
He published "Fundamentals of Sonar" in 1957 as Chief Research Consultant at the US Navy Underwater Sound Laboratory. He held this position until 1959 when he became Technical Director, a position he held until mandatory retirement in 1963.
There was little progress in development from 1915 to 1940. In 1940, the US sonars typically consisted of a magnetostrictive transducer and an array of nickel tubes connected to a 1-foot-diameter steel plate attached back-to-back to a Rochelle salt crystal in a spherical housing. This assembly penetrated the ship hull and was manually rotated to the desired angle. The piezoelectric Rochelle salt crystal had better parameters, but the magnetostrictive unit was much more reliable. Early World War II losses prompted rapid research in the field, pursuing both improvements in magnetostrictive transducer parameters and Rochelle salt reliability. Ammonium dihydrogen phosphate (ADP), a superior alternative, was found as a replacement for Rochelle salt; the first application was a replacement of the 24 kHz Rochelle-salt transducers. Within nine months, Rochelle salt was obsolete. The ADP manufacturing facility grew from few dozen personnel in early 1940 to several thousands in 1942.

One of the earliest application of ADP crystals were hydrophones for acoustic mines; the crystals were specified for low-frequency cutoff at 5 Hz, withstanding mechanical shock for deployment from aircraft from , and ability to survive neighbouring mine explosions. One of key features of ADP reliability is its zero aging characteristics; the crystal keeps its parameters even over prolonged storage.

Another application was for acoustic homing torpedoes. Two pairs of directional hydrophones were mounted on the torpedo nose, in the horizontal and vertical plane; the difference signals from the pairs were used to steer the torpedo left-right and up-down. A countermeasure was developed: the targeted submarine discharged an effervescent chemical, and the torpedo went after the noisier fizzy decoy. The counter-countermeasure was a torpedo with active sonar – a transducer was added to the torpedo nose, and the microphones were listening for its reflected periodic tone bursts. The transducers comprised identical rectangular crystal plates arranged to diamond-shaped areas in staggered rows.

Passive sonar arrays for submarines were developed from ADP crystals. Several crystal assemblies were arranged in a steel tube, vacuum-filled with castor oil, and sealed. The tubes then were mounted in parallel arrays.

The standard US Navy scanning sonar at the end of World War II operated at 18 kHz, using an array of ADP crystals. Desired longer range, however, required use of lower frequencies. The required dimensions were too big for ADP crystals, so in the early 1950s magnetostrictive and barium titanate piezoelectric systems were developed, but these had problems achieving uniform impedance characteristics, and the beam pattern suffered. Barium titanate was then replaced with more stable lead zirconate titanate (PZT), and the frequency was lowered to 5 kHz. The US fleet used this material in the AN/SQS-23 sonar for several decades. The SQS-23 sonar first used magnetostrictive nickel transducers, but these weighed several tons, and nickel was expensive and considered a critical material; piezoelectric transducers were therefore substituted. The sonar was a large array of 432 individual transducers. At first, the transducers were unreliable, showing mechanical and electrical failures and deteriorating soon after installation; they were also produced by several vendors, had different designs, and their characteristics were different enough to impair the array's performance. The policy to allow repair of individual transducers was then sacrificed, and "expendable modular design", sealed non-repairable modules, was chosen instead, eliminating the problem with seals and other extraneous mechanical parts.

The Imperial Japanese Navy at the onset of World War II used projectors based on quartz. These were big and heavy, especially if designed for lower frequencies; the one for Type 91 set, operating at 9 kHz, had a diameter of and was driven by an oscillator with 5 kW power and 7 kV of output amplitude. The Type 93 projectors consisted of solid sandwiches of quartz, assembled into spherical cast iron bodies. The Type 93 sonars were later replaced with Type 3, which followed German design and used magnetostrictive projectors; the projectors consisted of two rectangular identical independent units in a cast iron rectangular body about . The exposed area was half the wavelength wide and three wavelengths high. The magnetostrictive cores were made from 4 mm stampings of nickel, and later of an iron-aluminium alloy with aluminium content between 12.7% and 12.9%. The power was provided from a 2 kW at 3.8 kV, with polarization from a 20 V, 8 A DC source.

The passive hydrophones of the Imperial Japanese Navy were based on moving-coil design, Rochelle salt piezo transducers, and carbon microphones.

Magnetostrictive transducers were pursued after World War II as an alternative to piezoelectric ones. Nickel scroll-wound ring transducers were used for high-power low-frequency operations, with size up to in diameter, probably the largest individual sonar transducers ever. The advantage of metals is their high tensile strength and low input electrical impedance, but they have electrical losses and lower coupling coefficient than PZT, whose tensile strength can be increased by prestressing. Other materials were also tried; nonmetallic ferrites were promising for their low electrical conductivity resulting in low eddy current losses, Metglas offered high coupling coefficient, but they were inferior to PZT overall. In the 1970s, compounds of rare earths and iron were discovered with superior magnetomechanic properties, namely the Terfenol-D alloy. This made possible new designs, e.g. a hybrid magnetostrictive-piezoelectric transducer. The most recent sch material is Galfenol.

Other types of transducers include variable-reluctance (or moving-armature, or electromagnetic) transducers, where magnetic force acts on the surfaces of gaps, and moving coil (or electrodynamic) transducers, similar to conventional speakers; the latter are used in underwater sound calibration, due to their very low resonance frequencies and flat broadband characteristics above them.

Active sonar uses a sound transmitter and a receiver. When the two are in the same place it is monostatic operation. When the transmitter and receiver are separated it is bistatic operation. When more transmitters (or more receivers) are used, again spatially separated, it is multistatic operation. Most sonars are used monostatically with the same array often being used for transmission and reception. Active sonobuoy fields may be operated multistatically.

Active sonar creates a pulse of sound, often called a "ping", and then listens for reflections (echo) of the pulse. This pulse of sound is generally created electronically using a sonar projector consisting of a signal generator, power amplifier and electro-acoustic transducer/array. A beamformer is usually employed to concentrate the acoustic power into a beam, which may be swept to cover the required search angles. Generally, the electro-acoustic transducers are of the Tonpilz type and their design may be optimised to achieve maximum efficiency over the widest bandwidth, in order to optimise performance of the overall system. Occasionally, the acoustic pulse may be created by other means, e.g. (1) chemically using explosives, or (2) airguns or (3) plasma sound sources.

To measure the distance to an object, the time from transmission of a pulse to reception is measured and converted into a range by knowing the speed of sound. To measure the bearing, several hydrophones are used, and the set measures the relative arrival time to each, or with an array of hydrophones, by measuring the relative amplitude in beams formed through a process called beamforming. Use of an array reduces the spatial response so that to provide wide cover multibeam systems are used. The target signal (if present) together with noise is then passed through various forms of signal processing, which for simple sonars may be just energy measurement. It is then presented to some form of decision device that calls the output either the required signal or noise. This decision device may be an operator with headphones or a display, or in more sophisticated sonars this function may be carried out by software. Further processes may be carried out to classify the target and localise it, as well as measuring its velocity.

The pulse may be at constant frequency or a chirp of changing frequency (to allow pulse compression on reception). Simple sonars generally use the former with a filter wide enough to cover possible Doppler changes due to target movement, while more complex ones generally include the latter technique. Since digital processing became available pulse compression has usually been implemented using digital correlation techniques. Military sonars often have multiple beams to provide all-round cover while simple ones only cover a narrow arc, although the beam may be rotated, relatively slowly, by mechanical scanning.

Particularly when single frequency transmissions are used, the Doppler effect can be used to measure the radial speed of a target. The difference in frequency between the transmitted and received signal is measured and converted into a velocity. Since Doppler shifts can be introduced by either receiver or target motion, allowance has to be made for the radial speed of the searching platform.

One useful small sonar is similar in appearance to a waterproof flashlight. The head is pointed into the water, a button is pressed, and the device displays the distance to the target. Another variant is a "fishfinder" that shows a small display with shoals of fish. Some civilian sonars (which are not designed for stealth) approach active military sonars in capability, with quite exotic three-dimensional displays of the area near the boat.

When active sonar is used to measure the distance from the transducer to the bottom, it is known as echo sounding. Similar methods may be used looking upward for wave measurement.

Active sonar is also used to measure distance through water between two sonar transducers or a combination of a hydrophone (underwater acoustic microphone) and projector (underwater acoustic speaker). A transducer is a device that can transmit and receive acoustic signals ("pings"). When a hydrophone/transducer receives a specific interrogation signal it responds by transmitting a specific reply signal. To measure distance, one transducer/projector transmits an interrogation signal and measures the time between this transmission and the receipt of the other transducer/hydrophone reply. The time difference, scaled by the speed of sound through water and divided by two, is the distance between the two platforms. This technique, when used with multiple transducers/hydrophones/projectors, can calculate the relative positions of static and moving objects in water.

In combat situations, an active pulse can be detected by an opponent and will reveal a submarine's position.

A very directional, but low-efficiency, type of sonar (used by fisheries, military, and for port security) makes use of a complex nonlinear feature of water known as non-linear sonar, the virtual transducer being known as a "parametric array".

Project Artemis was a one-of-a-kind low-frequency sonar for surveillance that was deployed off Bermuda for several years in the early 1960s. The active portion was deployed from a World War II tanker, and the receiving array was built into a fixed position on an offshore bank.

This is an active sonar device that receives a stimulus and immediately (or with a delay) retransmits the received signal or a predetermined one.

A sonar target is small relative to the sphere, centred around the emitter, on which it is located. Therefore, the power of the reflected signal is very low, several orders of magnitude less than the original signal. Even if the reflected signal was of the same power, the following example (using hypothetical values) shows the problem: Suppose a sonar system is capable of emitting a 10,000 W/m signal at 1 m, and detecting a 0.001 W/m signal. At 100 m the signal will be 1 W/m (due to the inverse-square law). If the entire signal is reflected from a 10 m target, it will be at 0.001 W/m when it reaches the emitter, i.e. just detectable. However, the original signal will remain above 0.001 W/m until 300 m. Any 10 m target between 100 and 300 m using a similar or better system would be able to detect the pulse, but would not be detected by the emitter. The detectors must be very sensitive to pick up the echoes. Since the original signal is much more powerful, it can be detected many times further than twice the range of the sonar (as in the example).

Active sonar have two performance limitations: due to noise and reverberation. In general, one or other of these will dominate, so that the two effects can be initially considered separately.

In noise-limited conditions at initial detection:
where SL is the source level, PL is the propagation loss (sometimes referred to as transmission loss), TS is the target strength, NL is the noise level, AG is the array gain of the receiving array (sometimes approximated by its directivity index) and DT is the detection threshold.

In reverberation-limited conditions at initial detection (neglecting array gain):
where RL is the reverberation level, and the other factors are as before.


Passive sonar listens without transmitting. It is often employed in military settings, although it is also used in science applications, "e.g.", detecting fish for presence/absence studies in various aquatic environments - see also passive acoustics and passive radar. In the very broadest usage, this term can encompass virtually any analytical technique involving remotely generated sound, though it is usually restricted to techniques applied in an aquatic environment.

Passive sonar has a wide variety of techniques for identifying the source of a detected sound. For example, U.S. vessels usually operate 60 Hz alternating current power systems. If transformers or generators are mounted without proper vibration insulation from the hull or become flooded, the 60 Hz sound from the windings can be emitted from the submarine or ship. This can help to identify its nationality, as all European submarines and nearly every other nation's submarine have 50 Hz power systems. Intermittent sound sources (such as a wrench being dropped) may also be detectable to passive sonar. Until fairly recently, an experienced, trained operator identified signals, but now computers may do this.

Passive sonar systems may have large sonic databases, but the sonar operator usually finally classifies the signals manually. A computer system frequently uses these databases to identify classes of ships, actions (i.e. the speed of a ship, or the type of weapon released), and even particular ships. Publications for classification of sounds are provided by and continually updated by the US Office of Naval Intelligence.

Passive sonar on vehicles is usually severely limited because of noise generated by the vehicle. For this reason, many submarines operate nuclear reactors that can be cooled without pumps, using silent convection, or fuel cells or batteries, which can also run silently. Vehicles' propellers are also designed and precisely machined to emit minimal noise. High-speed propellers often create tiny bubbles in the water, and this cavitation has a distinct sound.

The sonar hydrophones may be towed behind the ship or submarine in order to reduce the effect of noise generated by the watercraft itself. Towed units also combat the thermocline, as the unit may be towed above or below the thermocline.

The display of most passive sonars used to be a two-dimensional waterfall display. The horizontal direction of the display is bearing. The vertical is frequency, or sometimes time. Another display technique is to color-code frequency-time information for bearing. More recent displays are generated by the computers, and mimic radar-type plan position indicator displays.

Unlike active sonar, only one-way propagation is involved. Because of the different signal processing used, the minimal detectable signal-to-noise ratio will be different. The equation for determining the performance of a passive sonar is
where SL is the source level, PL is the propagation loss, NL is the noise level, AG is the array gain and DT is the detection threshold. The figure of merit of a passive sonar is

The detection, classification and localisation performance of a sonar depends on the environment and the receiving equipment, as well as the transmitting equipment in an active sonar or the target radiated noise in a passive sonar.

Sonar operation is affected by variations in sound speed, particularly in the vertical plane. Sound travels more slowly in fresh water than in sea water, though the difference is small. The speed is determined by the water's bulk modulus and mass density. The bulk modulus is affected by temperature, dissolved impurities (usually salinity), and pressure. The density effect is small. The speed of sound (in feet per second) is approximately:

This empirically derived approximation equation is reasonably accurate for normal temperatures, concentrations of salinity and the range of most ocean depths. Ocean temperature varies with depth, but at between 30 and 100 meters there is often a marked change, called the thermocline, dividing the warmer surface water from the cold, still waters that make up the rest of the ocean. This can frustrate sonar, because a sound originating on one side of the thermocline tends to be bent, or refracted, through the thermocline. The thermocline may be present in shallower coastal waters. However, wave action will often mix the water column and eliminate the thermocline. Water pressure also affects sound propagation: higher pressure increases the sound speed, which causes the sound waves to refract away from the area of higher sound speed. The mathematical model of refraction is called Snell's law.

If the sound source is deep and the conditions are right, propagation may occur in the 'deep sound channel'. This provides extremely low propagation loss to a receiver in the channel. This is because of sound trapping in the channel with no losses at the boundaries. Similar propagation can occur in the 'surface duct' under suitable conditions. However, in this case there are reflection losses at the surface.

In shallow water propagation is generally by repeated reflection at the surface and bottom, where considerable losses can occur.

Sound propagation is affected by absorption in the water itself as well as at the surface and bottom. This absorption depends upon frequency, with several different mechanisms in sea water. Long-range sonar uses low frequencies to minimise absorption effects.

The sea contains many sources of noise that interfere with the desired target echo or signature. The main noise sources are waves and shipping. The motion of the receiver through the water can also cause speed-dependent low frequency noise.

When active sonar is used, scattering occurs from small objects in the sea as well as from the bottom and surface. This can be a major source of interference. This acoustic scattering is analogous to the scattering of the light from a car's headlights in fog: a high-intensity pencil beam will penetrate the fog to some extent, but broader-beam headlights emit much light in unwanted directions, much of which is scattered back to the observer, overwhelming that reflected from the target ("white-out"). For analogous reasons active sonar needs to transmit in a narrow beam to minimise scattering.

The sound "reflection" characteristics of the target of an active sonar, such as a submarine, are known as its target strength. A complication is that echoes are also obtained from other objects in the sea such as whales, wakes, schools of fish and rocks.

Passive sonar detects the target's "radiated" noise characteristics. The radiated spectrum comprises a continuous spectrum of noise with peaks at certain frequencies which can be used for classification.

"Active" (powered) countermeasures may be launched by a submarine under attack to raise the noise level, provide a large false target, and obscure the signature of the submarine itself.

"Passive" (i.e., non-powered) countermeasures include:

Modern naval warfare makes extensive use of both passive and active sonar from water-borne vessels, aircraft and fixed installations. Although active sonar was used by surface craft in World War II, submarines avoided the use of active sonar due to the potential for revealing their presence and position to enemy forces. However, the advent of modern signal-processing enabled the use of passive sonar as a primary means for search and detection operations. In 1987 a division of Japanese company Toshiba reportedly sold machinery to the Soviet Union that allowed their submarine propeller blades to be milled so that they became radically quieter, making the newer generation of submarines more difficult to detect.

The use of active sonar by a submarine to determine bearing is extremely rare and will not necessarily give high quality bearing or range information to the submarines fire control team. However, use of active sonar on surface ships is very common and is used by submarines when the tactical situation dictates it is more important to determine the position of a hostile submarine than conceal their own position. With surface ships, it might be assumed that the threat is already tracking the ship with satellite data as any vessel around the emitting sonar will detect the emission. Having heard the signal, it is easy to identify the sonar equipment used (usually with its frequency) and its position (with the sound wave's energy). Active sonar is similar to radar in that, while it allows detection of targets at a certain range, it also enables the emitter to be detected at a far greater range, which is undesirable.

Since active sonar reveals the presence and position of the operator, and does not allow exact classification of targets, it is used by fast (planes, helicopters) and by noisy platforms (most surface ships) but rarely by submarines. When active sonar is used by surface ships or submarines, it is typically activated very briefly at intermittent periods to minimize the risk of detection. Consequently, active sonar is normally considered a backup to passive sonar. In aircraft, active sonar is used in the form of disposable sonobuoys that are dropped in the aircraft's patrol area or in the vicinity of possible enemy sonar contacts.

Passive sonar has several advantages, most importantly that it is silent. If the target radiated noise level is high enough, it can have a greater range than active sonar, and allows the target to be identified. Since any motorized object makes some noise, it may in principle be detected, depending on the level of noise emitted and the ambient noise level in the area, as well as the technology used. To simplify, passive sonar "sees" around the ship using it. On a submarine, nose-mounted passive sonar detects in directions of about 270°, centered on the ship's alignment, the hull-mounted array of about 160° on each side, and the towed array of a full 360°. The invisible areas are due to the ship's own interference. Once a signal is detected in a certain direction (which means that something makes sound in that direction, this is called broadband detection) it is possible to zoom in and analyze the signal received (narrowband analysis). This is generally done using a Fourier transform to show the different frequencies making up the sound. Since every engine makes a specific sound, it is straightforward to identify the object. Databases of unique engine sounds are part of what is known as "acoustic intelligence" or ACINT.

Another use of passive sonar is to determine the target's trajectory. This process is called target motion analysis (TMA), and the resultant "solution" is the target's range, course, and speed. TMA is done by marking from which direction the sound comes at different times, and comparing the motion with that of the operator's own ship. Changes in relative motion are analyzed using standard geometrical techniques along with some assumptions about limiting cases.

Passive sonar is stealthy and very useful. However, it requires high-tech electronic components and is costly. It is generally deployed on expensive ships in the form of arrays to enhance detection. Surface ships use it to good effect; it is even better used by submarines, and it is also used by airplanes and helicopters, mostly to a "surprise effect", since submarines can hide under thermal layers. If a submarine's commander believes he is alone, he may bring his boat closer to the surface and be easier to detect, or go deeper and faster, and thus make more sound.

Examples of sonar applications in military use are given below. Many of the civil uses given in the following section may also be applicable to naval use.

Until recently, ship sonars were usually with hull mounted arrays, either amidships or at the bow. It was soon found after their initial use that a means of reducing flow noise was required. The first were made of canvas on a framework, then steel ones were used. Now domes are usually made of reinforced plastic or pressurized rubber. Such sonars are primarily active in operation. An example of a conventional hull mounted sonar is the SQS-56.

Because of the problems of ship noise, towed sonars are also used. These also have the advantage of being able to be placed deeper in the water. However, there are limitations on their use in shallow water. These are called towed arrays (linear) or variable depth sonars (VDS) with 2/3D arrays. A problem is that the winches required to deploy/recover these are large and expensive. VDS sets are primarily active in operation while towed arrays are passive.

An example of a modern active/passive ship towed sonar is Sonar 2087 made by Thales Underwater Systems.

Modern torpedoes are generally fitted with an active/passive sonar. This may be used to home directly on the target, but wake homing torpedoes are also used. An early example of an acoustic homer was the Mark 37 torpedo.

Torpedo countermeasures can be towed or free. An early example was the German "Sieglinde" device while the "Bold" was a chemical device. A widely used US device was the towed AN/SLQ-25 Nixie while Mobile submarine simulator (MOSS) was a free device. A modern alternative to the Nixie system is the UK Royal Navy S2170 Surface Ship Torpedo Defence system.

Mines may be fitted with a sonar to detect, localize and recognize the required target. Further information is given in acoustic mine and an example is the CAPTOR mine.

Mine Countermeasure (MCM) Sonar, sometimes called "Mine and Obstacle Avoidance Sonar (MOAS)", is a specialized type of sonar used for detecting small objects. Most MCM sonars are hull mounted but a few types are VDS design. An example of a hull mounted MCM sonar is the Type 2193 while the SQQ-32 Mine-hunting sonar and Type 2093 systems are VDS designs. See also Minesweeper (ship)

Submarines rely on sonar to a greater extent than surface ships as they cannot use radar at depth. The sonar arrays may be hull mounted or towed. Information fitted on typical fits is given in Oyashio class submarine and Swiftsure class submarine.

Helicopters can be used for antisubmarine warfare by deploying fields of active/passive sonobuoys or can operate dipping sonar, such as the AQS-13. Fixed wing aircraft can also deploy sonobuoys and have greater endurance and capacity to deploy them. Processing from the sonobuoys or Dipping Sonar can be on the aircraft or on ship. Dipping sonar has the advantage of being deployable to depths appropriate to daily conditions Helicopters have also been used for mine countermeasure missions using towed sonars such as the AQS-20A.

Dedicated sonars can be fitted to ships and submarines for underwater communication. See also the section on the underwater acoustics page.

For many years, the United States operated a large set of passive sonar arrays at various points in the world's oceans, collectively called Sound Surveillance System (SOSUS) and later Integrated Undersea Surveillance System (IUSS). A similar system is believed to have been operated by the Soviet Union. As permanently mounted arrays in the deep ocean were utilised, they were in very quiet conditions so long ranges could be achieved. Signal processing was carried out using powerful computers ashore. With the ending of the Cold War a SOSUS array has been turned over to scientific use.

In the United States Navy, a special badge known as the Integrated Undersea Surveillance System Badge is awarded to those who have been trained and qualified in its operation.

Sonar can be used to detect frogmen and other scuba divers. This can be applicable around ships or at entrances to ports. Active sonar can also be used as a deterrent and/or disablement mechanism. One such device is the Cerberus system.

See Underwater Port Security System and Anti-frogman techniques#Ultrasound detection.

Limpet Mine Imaging Sonar (LIMIS) is a hand-held or ROV-mounted imaging sonar designed for patrol divers (combat frogmen or clearance divers) to look for limpet mines in low visibility water.

The LUIS is another imaging sonar for use by a diver.

Integrated Navigation Sonar System (INSS) is a small flashlight-shaped handheld sonar for divers that displays range.

This is a sonar designed to detect and locate the transmissions from hostile active sonars. An example of this is the Type 2082 fitted on the British Vanguard class submarines.

Fishing is an important industry that is seeing growing demand, but world catch tonnage is falling as a result of serious resource problems. The industry faces a future of continuing worldwide consolidation until a point of sustainability can be reached. However, the consolidation of the fishing fleets are driving increased demands for sophisticated fish finding electronics such as sensors, sounders and sonars. Historically, fishermen have used many different techniques to find and harvest fish. However, acoustic technology has been one of the most important driving forces behind the development of the modern commercial fisheries.

Sound waves travel differently through fish than through water because a fish's air-filled swim bladder has a different density than seawater. This density difference allows the detection of schools of fish by using reflected sound. Acoustic technology is especially well suited for underwater applications since sound travels farther and faster underwater than in air. Today, commercial fishing vessels rely almost completely on acoustic sonar and sounders to detect fish. Fishermen also use active sonar and echo sounder technology to determine water depth, bottom contour, and bottom composition.
Companies such as eSonar, Raymarine UK, Marport Canada, Wesmar, Furuno, Krupp, and Simrad make a variety of sonar and acoustic instruments for the deep sea commercial fishing industry. For example, net sensors take various underwater measurements and transmit the information back to a receiver on board a vessel. Each sensor is equipped with one or more acoustic transducers depending on its specific function. Data is transmitted from the sensors using wireless acoustic telemetry and is received by a hull mounted hydrophone. The analog signals are decoded and converted by a digital acoustic receiver into data which is transmitted to a bridge computer for graphical display on a high resolution monitor.

Echo sounding is a process used to determine the depth of water beneath ships and boats. A type of active sonar, echo sounding is the transmission of an acoustic pulse directly downwards to the seabed, measuring the time between transmission and echo return, after having hit the bottom and bouncing back to its ship of origin. The acoustic pulse is emitted by a transducer which receives the return echo as well. The depth measurement is calculated by multiplying the speed of sound in water(averaging 1,500 meters per second) by the time between emission and echo return.

The value of underwater acoustics to the fishing industry has led to the development of other acoustic instruments that operate in a similar fashion to echo-sounders but, because their function is slightly different from the initial model of the echo-sounder, have been given different terms.

The net sounder is an echo sounder with a transducer mounted on the headline of the net rather than on the bottom of the vessel. Nevertheless, to accommodate the distance from the transducer to the display unit, which is much greater than in a normal echo-sounder, several refinements have to be made. Two main types are available. The first is the cable type in which the signals are sent along a cable. In this case there has to be the provision of a cable drum on which to haul, shoot and stow the cable during the different phases of the operation. The second type is the cable less net-sounder – such as Marport’s Trawl Explorer - in which the signals are sent acoustically between the net and hull mounted receiver/hydrophone on the vessel. In this case no cable drum is required but sophisticated electronics are needed at the transducer and receiver.

The display on a net sounder shows the distance of the net from the bottom (or the surface), rather than the depth of water as with the echo-sounder's hull-mounted transducer. Fixed to the headline of the net, the footrope can usually be seen which gives an indication of the net performance. Any fish passing into the net can also be seen, allowing fine adjustments to be made to catch the most fish possible. In other fisheries, where the amount of fish in the net is important, catch sensor transducers are mounted at various positions on the cod-end of the net. As the cod-end fills up these catch sensor transducers are triggered one by one and this information is transmitted acoustically to display monitors on the bridge of the vessel. The skipper can then decide when to haul the net.

Modern versions of the net sounder, using multiple element transducers, function more like a sonar than an echo sounder and show slices of the area in front of the net and not merely the vertical view that the initial net sounders used.

The sonar is an echo-sounder with a directional capability that can show fish or other objects around the vessel.

Small sonars have been fitted to Remotely Operated Vehicles (ROV) and Unmanned Underwater Vehicles (UUV) to allow their operation in murky conditions. These sonars are used for looking ahead of the vehicle. The Long-Term Mine Reconnaissance System is an UUV for MCM purposes.

Sonars which act as beacons are fitted to aircraft to allow their location in the event of a crash in the sea. Short and Long Baseline sonars may be used for caring out the location, such as LBL.

In 2013 an inventor in the United States unveiled a "spider-sense" bodysuit, equipped with ultrasonic sensors and haptic feedback systems, which alerts the wearer of incoming threats; allowing them to respond to attackers even when blindfolded.

Detection of fish, and other marine and aquatic life, and estimation their individual sizes or total biomass using active sonar techniques. As the sound pulse travels through water it encounters objects that are of different density or acoustic characteristics than the surrounding medium, such as fish, that reflect sound back toward the sound source. These echoes provide information on fish size, location, abundance and behavior. Data is usually processed and analysed using a variety of software such as Echoview.
See Also: Hydroacoustics and Fisheries Acoustics.

An upward looking echo sounder mounted on the bottom or on a platform may be used to make measurements of wave height and period. From this statistics of the surface conditions at a location can be derived.

Special short range sonars have been developed to allow measurements of water velocity.

Sonars have been developed that can be used to characterise the sea bottom into, for example, mud, sand, and gravel. Relatively simple sonars such as echo sounders can be promoted to seafloor classification systems via add-on modules, converting echo parameters into sediment type. Different algorithms exist, but they are all based on changes in the energy or shape of the reflected sounder pings. Advanced substrate classification analysis can be achieved using calibrated (scientific) echosounders and parametric or fuzzy-logic analysis of the acoustic data (See: Acoustic Seabed Classification)

Side-scan sonars can be used to derive maps of seafloor topography (bathymetry) by moving the sonar across it just above the bottom. Low frequency sonars such as GLORIA have been used for continental shelf wide surveys while high frequency sonars are used for more detailed surveys of smaller areas.

Powerful low frequency echo-sounders have been developed for providing profiles of the upper layers of the ocean bottom.

Various synthetic aperture sonars have been built in the laboratory and some have entered use in mine-hunting and search systems. An explanation of their operation is given in synthetic aperture sonar.

Parametric sources use the non-linearity of water to generate the difference frequency between two high frequencies. A virtual end-fire array is formed. Such a projector has advantages of broad bandwidth, narrow beamwidth, and when fully developed and carefully measured it has no obvious sidelobes: see Parametric array. Its major disadvantage is very low efficiency of only a few percent. P.J. Westervelt summarizes the trends involved.

Use of sonar has been proposed for determining the depth of hydrocarbon seas on Titan.

Research has shown that use of active sonar can lead to mass strandings of marine mammals. Beaked whales, the most common casualty of the strandings, have been shown to be highly sensitive to mid-frequency active sonar. Other marine mammals such as the blue whale also flee away from the source of the sonar, while naval activity was suggested to be the most probable cause of a mass stranding of dolphins. The US Navy, which part-funded some of studies, said that the findings only showed behavioural responses to sonar, not actual harm, but they "will evaluate the effectiveness of [their] marine mammal protective measures in light of new research findings". A 2008 US Supreme Court ruling on the use of sonar by the US Navy noted that there had been no cases where sonar had been conclusively shown to have harmed or killed a marine mammal.
Some marine animals, such as whales and dolphins, use echolocation systems, sometimes called "biosonar" to locate predators and prey. Research on the effects of sonar on blue whales in the Southern California Bight shows that mid-frequency sonar use disrupts the whales' feeding behavior. This indicates that sonar-induced disruption of feeding and displacement from high-quality prey patches could have significant and previously undocumented impacts on baleen whale foraging ecology, individual fitness and population health.

High-intensity sonar sounds can create a small temporary shift in the hearing threshold of some fish.

The frequencies of sonars range from infrasonic to above a megahertz. Generally, the lower frequencies have longer range, while the higher frequencies offer better resolution, and smaller size for a given directionality.

To achieve reasonable directionality, frequencies below 1 kHz generally require large size, usually achieved as towed arrays.

Low frequency sonars are loosely defined as 1–5 kHz, albeit some navies regard 5–7 kHz also as low frequency. Medium frequency is defined as 5–15 kHz. Another style of division considers low frequency to be under 1 kHz, and medium frequency at between 1–10 kHz.

American World War II era sonars operated at a relatively high frequency of 20–30 kHz, to achieve directionality with reasonably small transducers, with typical maximum operational range of 2500 yd. Postwar sonars used lower frequencies to achieve longer range; e.g. SQS-4 operated at 10 kHz with range up to 5000 yd. SQS-26 and SQS-53 operated at 3 kHz with range up to 20,000 yd; their domes had size of approx. a 60-ft personnel boat, an upper size limit for conventional hull sonars. Achieving larger sizes by conformal sonar array spread over the hull has not been effective so far, for lower frequencies linear or towed arrays are therefore used.

Japanese WW2 sonars operated at a range of frequencies. The Type 91, with 30 inch quartz projector, worked at 9 kHz. The Type 93, with smaller quartz projectors, operated at 17.5 kHz (model 5 at 16 or 19 kHz magnetostrictive) at powers between 1.7 and 2.5 kilowatts, with range of up to 6 km. The later Type 3, with German-design magnetostrictive transducers, operated at 13, 14.5, 16, or 20 kHz (by model), using twin transducers (except model 1 which had three single ones), at 0.2 to 2.5 kilowatts. The Simple type used 14.5 kHz magnetostrictive transducers at 0.25 kW, driven by capacitive discharge instead of oscillators, with range up to 2.5 km.

The sonar's resolution is angular; objects further apart will be imaged with lower resolutions than nearby ones.

Another source lists ranges and resolutions vs frequencies for sidescan sonars. 30 kHz provides low resolution with range of 1000–6000 m, 100 kHz gives medium resolution at 500–1000 m, 300 kHz gives high resolution at 150–500 m, and 600 kHz gives high resolution at 75–150 m. Longer range sonars are more adversely affected by nonhomogenities of water. Some environments, typically shallow waters near the coasts, have complicated terrain with many features; higher frequencies become necessary there.

As a specific example, the Sonar 2094 Digital, a towed fish capable of reaching depth of 1000 or 2000 meters, performs side-scanning at 114 kHz (600m range at each side, 50 by 1 degree beamwidth) and 410 kHz (150m range, 40 by 0.3 degree beamwidth), with 3 kW pulse power.

A JW Fishers system offers side-scanning at 1200 kHz with very high spatial resolution, optionally coupled with longer-range 600 kHz (range 200 ft at each side) or 100 kHz (up to 2000 ft per side, suitable for scanning large areas for big targets).


Fisheries Acoustics References




</doc>
<doc id="29440" url="https://en.wikipedia.org/wiki?curid=29440" title="Slavs">
Slavs

Slavs are an Indo-European ethno-linguistic group who speak the various Slavic languages of the larger Balto-Slavic linguistic group. They are native to Eurasia, stretching from Central, Eastern, and Southeastern Europe all the way north and eastwards to Northeast Europe, Northern Asia (Siberia), the Caucasus, and Central Asia (especially Kazakhstan and Turkmenistan) as well as historically in Western Europe (particularly in East Germany) and Western Asia (including Anatolia). From the early 6th century they spread to inhabit the majority of Central, Eastern and Southeastern Europe. Also, today there is a large Slavic diaspora throughout North America, particularly in the United States and Canada as a result of immigration.

Slavs are the largest ethno-linguistic group in Europe. Present-day Slavic people are classified into East Slavs (chiefly Belarusians, Russians, Rusyns, and Ukrainians), West Slavs (chiefly Czechs, Kashubs, Moravians, Poles, Silesians, Slovaks and Sorbs), and South Slavs (chiefly Bosniaks, Bulgarians, Croats,
Macedonians, Montenegrins, Serbs and Slovenes) .

Slavs can be further grouped by religion. Orthodox Christianity is practiced by the majority of Slavs. The Orthodox Slavs include the Belarusians, Bulgarians, Macedonians, Montenegrins, Russians, Serbs, and Ukrainians and are defined by Orthodox customs and Cyrillic script as well as their cultural connection to the Byzantine Empire (Serbs also use Serbian Latin script on equal terms). Their second most common religion is Roman Catholicism. The Catholic Slavs include Croats, Czechs, Kashubs, Moravians, Poles, Silesians, Slovaks, Slovenes, and Sorbs and are defined by their Latinate influence and heritage and connection to Western Europe. There are also substantial Protestant and Lutheran minorities especially amongst the West Slavs, such as the historical Bohemian (Czech) Hussites.

The third largest religion amongst the Slavs is Islam. Muslim Slavs include the Bosniaks, Pomaks, Gorani, Torbeši, and other Muslims of the former Yugoslavia as well as certain East Slavs who settled in the Crimean Peninsula and converted to the Islamic faith via influence from the Crimean Tatars. Modern Slavic nations and ethnic groups are considerably diverse both genetically and culturally, and relations between them – even within the individual groups – range from ethnic solidarity to mutual hostility.

The oldest mention of the Slavic ethnonym is the 6th century AD Procopius, writing in Byzantine Greek, using various forms such as "Sklaboi" (), "Sklabēnoi" (), "Sklauenoi" (), "Sthlabenoi" (), or "Sklabinoi" (), while his contemporary Jordanes refers to the "" in Latin. The oldest documents written in Old Church Slavonic, dating from the 9th century, attest the autonym as "Slověne" (). These forms point back to a Slavic autonym which can be reconstructed in Proto-Slavic as , plural "Slověne". 

The reconstructed autonym "" is usually considered a derivation from "slovo" ("word"), originally denoting "people who speak (the same language)," i. e. people who understand each other, in contrast to the Slavic word denoting German people, namely , meaning "silent, mute people" (from Slavic "mute, mumbling"). The word "slovo" ("word") and the related "slava" ("glory, fame") and "slukh" ("hearing") originate from the Proto-Indo-European root ("be spoken of, glory"), cognate with Ancient Greek ( "fame"), as in the name Pericles, Latin ("be called"), and English .

Ancient Roman sources refer to the Early Slavic peoples as Veneti, who dwelled in a region of central-Europe east of the Germanic tribe of Suebi, and west of the Iranian Sarmatians in the 1st and 2nd centuries AD. The name "Veneti" derives from Latin "venus, -eris" ('love, lovable, friendly'). The Slavs under name of the "Antes" and the "Sclaveni" make their first appearance in Byzantine records in the early 6th century. Byzantine historiographers under emperor Justinian I (527–565), such as Procopius of Caesarea, Jordanes and Theophylact Simocatta describe tribes of these names emerging from the area of the Carpathian Mountains, the lower Danube and the Black Sea, invading the Danubian provinces of the Eastern Empire. 

Jordanes in his work "Getica" (written in 550 or 551 AD). describes the Veneti as a "populous nation" whose dwellings begin at the sources of the Vistula and occupy "a great expanse of land." He describes the Veneti as the ancestors of Antes and Slaveni, two early Slavic tribes, who appeared on the Byzantine frontier in the early 6th century. Procopius wrote in 545 that "the Sclaveni and the Antae actually had a single name in the remote past; for they were both called "Sporoi" in olden times." The name "Sporoi" derives from Greek σπείρω ("I scatter grain"). He described them as barbarians, who lived under democracy, and that they believe in one god, "the maker of lightning" (Perun), to whom they made sacrifice. They lived in scattered housing, and constantly changed settlement. Regarding warfare, they were mainly foot soldiers with small shields and battleaxes, lightly clothed, some entering battle naked with only their genitals covered. Their language is "barbarous" (that is, not Greek-speaking), and the two tribes do not differ in appearance, being tall and robust, "while their bodies and hair are neither very fair or blond, nor indeed do they incline entirely to the dark type, but they are all slightly ruddy in color. And they live a hard life, giving no heed to bodily comforts..." Jordanes described the Sclaveni having swamps and forests for their cities. Another 6th-century source refers to them living among nearly impenetrable forests, rivers, lakes, and marshes.

Menander Protector mentions a Daurentius (circa 577–579) who slew an Avar envoy of Khagan Bayan I. The Avars asked the Slavs to accept the suzerainty of the Avars; he however declined and is reported as saying: "Others do not conquer our land, we conquer theirs – so it shall always be for us".

According to eastern homeland theory, prior to becoming known to the Roman world, Slavic-speaking tribes were part of the many multi-ethnic confederacies of Eurasia – such as the Sarmatian, Hun and Gothic empires. The Slavs emerged from obscurity when the westward movement of Germans in the 5th and 6th centuries CE (thought to be in conjunction with the movement of peoples from Siberia and Eastern Europe: Huns, and later Avars and Bulgars) started the great migration of the Slavs, who settled the lands abandoned by Germanic tribes fleeing the Huns and their allies: westward into the country between the Oder and the Elbe-Saale line; southward into Bohemia, Moravia, much of present-day Austria, the Pannonian plain and the Balkans; and northward along the upper Dnieper river. It has also been suggested that some Slavs may have migrated with the movement of the Vandals to the Iberian Peninsula and even as far as North Africa.

Around the 6th century, Slavs appeared on Byzantine borders in great numbers. The Byzantine records note that grass would not regrow in places where the Slavs had marched through, so great were their numbers. After a military movement even the Peloponnese and Asia Minor were reported to have Slavic settlements. This southern movement has traditionally been seen as an invasive expansion. By the end of the 6th century, Slavs had settled the Eastern Alps regions.

When their migratory movements ended, there appeared among the Slavs the first rudiments of state organizations, each headed by a prince with a treasury and a defense force. In the 7th century, the Frankish merchant Samo, who supported the Slavs fighting their Avar rulers, became the ruler of the first known Slav state in Central Europe, Samo's Empire. However, most probably this early Slavic polity did not outlive its founder and ruler. Though, this provided the foundation for subsequent Slavic states to arise on the former territory of this realm with Carantania being the oldest of them. Very old also are the Principality of Nitra and the Moravian principality (see under Great Moravia). In this period, there existed West Slavic tribes and states such as the Balaton Principality, but the subsequent expansion of the Magyars into the Carpathian Basin, as well as the Germanization of Austria gradually separated the South Slavs from the West and East Slavs. The First Bulgarian Empire was founded in 681, and the Slavic language Old Church Slavonic became the main and official of the empire in 864. Bulgaria was instrumental in the spread of Slavic literacy and Christianity to the rest of the Slavic world. Later Slavic states, which formed in the following centuries included the Kievan Rus', the Second Bulgarian Empire, the Kingdom of Poland, Duchy of Bohemia, the Kingdom of Croatia, Banate of Bosnia and the Grand Principality of Serbia.

As of 1878, there were only three free Slavic states in the world: the Russian Empire, Serbia and Montenegro. In the entire Austro-Hungarian Empire of approximately 50 million people, about 23 million were Slavs. The Slavic peoples who were, for the most part, denied a voice in the affairs of the Austria-Hungary, were calling for national self-determination. Because of the vastness and diversity of the territory occupied by Slavic people, there were several centers of Slavic consolidation. In the 19th century, Pan-Slavism developed as a movement among intellectuals, scholars, and poets, but it rarely influenced practical politics and did not find support in some Slavic nations. Pan-Slavism became compromised when the Russian Empire started to use it as an ideology justifying its territorial conquests in Central Europe as well as subjugation of other Slavic ethnic groups such as Poles and Ukrainians, and the ideology became associated with Russian imperialism.

During World War I, representatives of the Czechs, Slovaks, Poles, Serbs, Croats, and Slovenes set up organizations in the Allied countries to gain sympathy and recognition. In 1918, after World War I ended, the Slavs established such independent states as Czechoslovakia, the Second Polish Republic, and the State of Slovenes, Croats and Serbs (which merged into Yugoslavia).

During World War II, Nazi Germany planned to kill, deport, or enslave the Slavic and Jewish population of occupied Central and Eastern Europe to create "Living space" for German settlers, and also planned the starvation of 80 million people in the Soviet Union. The partial fulfilment of these plans resulted in the deaths of an estimated 19.3 million civilians and prisoners of war.

The first half of the 20th century in Russia and the Soviet Union was marked by a succession of wars, famines and other disasters, each accompanied by large-scale population losses. Stephen J. Lee estimates that, by the end of World War II in 1945, the Russian population was about 90 million fewer than it could have been otherwise.

The common Slavic experience of communism combined with the repeated usage of the ideology by Soviet propaganda after World War II within the Eastern bloc (Warsaw Pact) was a forced high-level political and economic hegemony of the USSR dominated by Russians. A notable political union of the 20th century that covered most South Slavs was Yugoslavia, but it ultimately broke apart in the 1990s along with the Soviet Union.

The word "Slavs" was used in the national anthem of Yugoslavia (1943–1992) and the Federal Republic of Yugoslavia (1992–2003), later Serbia and Montenegro (2003–2006).

Former Soviet states, as well as countries that used to be satellite states or territories of the Warsaw Pact, have numerous minority Slavic populations, many of whom are originally from the Russian SFSR, Ukrainian SSR and Byelorussian SSR. As of now, Kazakhstan has the largest Slavic minority population with most being Russians (Ukrainians, Belarusians and Poles are present as well but in much smaller numbers).

Pan-Slavism, a movement which came into prominence in the mid-19th century, emphasized the common heritage and unity of all the Slavic peoples. The main focus was in the Balkans where the South Slavs had been ruled for centuries by other empires: the Byzantine Empire, Austria-Hungary, the Ottoman Empire, and Venice. The Russian Empire used Pan-Slavism as a political tool; as did the Soviet Union, which gained political-military influence and control over most Slavic-majority nations between 1939 and 1948 and retained a hegemonic role until the period 1989–1991.

Proto-Slavic, the supposed ancestor language of all Slavic languages, is a descendant of common Proto-Indo-European, via a Balto-Slavic stage in which it developed numerous lexical and morphophonological isoglosses with the Baltic languages. In the framework of the Kurgan hypothesis, "the Indo-Europeans who remained after the migrations [from the steppe] became speakers of Balto-Slavic". Proto-Slavic is defined as the last stage of the language preceding the geographical split of the historical Slavic languages. That language was uniform, and on the basis of borrowings from foreign languages and Slavic borrowings into other languages, cannot be said to have any recognizable dialects – this suggests that there was, at one time, a relatively small Proto-Slavic homeland.

Slavic linguistic unity was to some extent visible as late as Old Church Slavonic manuscripts which, though based on local Slavic speech of Thessaloniki, could still serve the purpose of the first common Slavic literary language. Slavic studies began as an almost exclusively linguistic and philological enterprise. As early as 1833, Slavic languages were recognized as Indo-European.

Standardised Slavic languages that have official status in at least one country are: Belarusian, Bosnian, Bulgarian, Croatian, Czech, Macedonian, Montenegrin, Polish, Russian, Serbian, Slovak, Slovene, and Ukrainian.

The alphabets used for Slavic languages are frequently connected to the dominant religion among the respective ethnic groups. Orthodox Christians use the Cyrillic alphabet while Roman Catholics use the Latin alphabet; the Bosniaks, who are Muslim, also use the Latin alphabet. Additionally, some Eastern Catholics and Roman Catholics use the Cyrillic alphabet. Serbian and Montenegrin use both the Cyrillic and Latin alphabets. There is also a Latin script to write in Belarusian, called the Lacinka alphabet.

Slavs are customarily divided along geographical lines into three major subgroups: West Slavs, East Slavs, and South Slavs, each with a different and a diverse background based on unique history, religion and culture of particular Slavic groups within them. Apart from prehistorical archaeological cultures, the subgroups have had notable cultural contact with non-Slavic Bronze- and Iron Age civilisations. Modern Slavic nations and ethnic groups are considerably diverse both genetically and culturally, and relations between them – even within the individual ethnic groups themselves – are varied, ranging from a sense of connection to mutual feelings of hostility.

West Slavs have origin in early Slavic tribes which settled in Central Europe after the East Germanic tribes had left this area during the migration period. They are noted as having mixed with Germanics, Hungarians, Celts (particularly the Boii), Old Prussians, and the Pannonian Avars. The West Slavs came under the influence of the Western Roman Empire (Latin) and of the Roman Catholic Church.

East Slavs have origins in early Slavic tribes who mixed and contacted with Finno-Ugrics, Balts, and Caucasians. Their early Slavic component, Antes, mixed or absorbed Iranians, and later received influence from the Khazars and Vikings. The East Slavs trace their national origins to the tribal unions of Kievan Rus' and Khaganate, beginning in the 10th century. They came particularly under the influence of the Byzantine Empire and of the Eastern Orthodox and Eastern Catholic Churchs. They later became established in the 16th century in the area north surrounding the Sarmatic Plain and in the steppes of modern-day eastern Ukraine.

South Slavs from most of the region have origins in early Slavic tribes who mixed with the local Proto-Balkanic tribes (Illyrian, Dacian, Thracian, Paeonian, Hellenic tribes), and Celtic tribes (particularly the Scordisci), as well as with Romans (and the Romanized remnants of the former groups), and also with remnants of temporarily settled invading East Germanic, Asiatic or Caucasian tribes such as Gepids, Huns, Avars and Bulgars. The original inhabitants of present-day Slovenia and continental Croatia have origins in early Slavic tribes who mixed with Romans and romanized Celtic and Illyrian people as well as with Avars and Germanic peoples (Lombards and East Goths). The South Slavs (except the Slovenes and Croats) came under the cultural sphere of the Eastern Roman Empire (Byzantine Empire), of the Ottoman Empire and of the Eastern Orthodox Church and Islam, while the Slovenes and the Croats were influenced by the Western Roman Empire (Latin) and thus by the Roman Catholic Church in a similar fashion to that of the West Slavs.

The pagan Slavic populations were Christianized between the 7th and 12th centuries. Orthodox Christianity is predominant in the East and South Slavs, while Roman Catholicism is predominant in West Slavs and the western South Slavs. The religious borders are largely comparable to the East–West Schism which began in the 11th century.

The majority of contemporary Slavic populations who profess a religion are Orthodox, followed by Catholic, while a small minority are Protestant. There are minor Slavic Muslim groups. Religious delineations by nationality can be very sharp; usually in the Slavic ethnic groups the vast majority of religious people share the same religion. Some Slavs are atheist or agnostic: in the Czech Republic 20% were atheists according to a 2012 poll.

Mainly Eastern Orthodoxy:

Mainly Roman Catholicism:

Mainly Islam:

Throughout their history, Slavs came into contact with non-Slavic groups. In the postulated homeland region (present-day Ukraine), they had contacts with the Iranic Sarmatians and the Germanic Goths. After their subsequent spread, the Slavs began assimilating non-Slavic peoples. For example, in the Balkans, there were Paleo-Balkan peoples, such as Romanized and Hellenized (Jireček Line) Illyrians, Thracians and Dacians, as well as Greeks and Celtic Scordisci. Over time, due to the larger number of Slavs, most descendants of the indigenous populations of the Balkans were Slavicized. The Thracians and Illyrians vanished as defined ethnic groups from the population during this period – although the modern Albanian nation claims descent from the Illyrians. Exceptions are Greece, where because Slavs were fewer than Greeks, they came to be Hellenized (aided in time by more Greeks returning to Greece in the 9th century and the role of the church and administration); and Romania, where Slavic people settled en route for present-day Greece, Republic of Macedonia, Bulgaria and East Thrace, where the Slavic population gradually assimilated. 

Ruling status of Bulgars and subsequent control of land cast the nominal legacy of "Bulgarian country and people" onto future generations, but Bulgars were gradually also Slavicized into the present day South Slavic ethnic group Bulgarians. The Romance speakers within the fortified Dalmatian cities managed to retain their culture and language for a long time. Dalmatian Romance was spoken until the high Middle Ages. But, they too were eventually assimilated into the body of Slavs.

In the Western Balkans, South Slavs and Germanic Gepids intermarried with invaders, eventually producing a Slavicized population. In Central Europe, the West Slavs intermixed with Germanic, Hungarian, and Celtic peoples, while in Eastern Europe the East Slavs had encountered Uralic and Scandinavian peoples. Scandinavians (Varangians) and Finnic peoples were involved in the early formation of the Rus' state but were completely Slavicized after a century. Some Finno-Ugric tribes in the north were also absorbed into the expanding Rus population.. In the 11th and 12th centuries, constant incursions by nomadic Turkic tribes, such as the Kipchak and the Pecheneg, caused a massive migration of East Slavic populations to the safer, heavily forested regions of the north. In the Middle Ages, groups of Saxon ore miners settled in medieval Bosnia, Serbia and Bulgaria, where they were Slavicized.

Polabian Slavs (Wends) settled in eastern parts of England (the Danelaw), apparently as Danish allies. Polabian-Pomeranian Slavs are also known to have even settled on Norse age Iceland. "Saqaliba" refers to the Slavic mercenaries and slaves in the medieval Arab world in North Africa, Sicily and Al-Andalus. Saqaliba served as caliph's guards. In the 12th century, Slavic piracy in the Baltics increased. The Wendish Crusade was started against the Polabian Slavs in 1147, as a part of the Northern Crusades. Niklot, pagan chief of the Slavic Obodrites, began his open resistance when Lothar III, Holy Roman Emperor, invaded Slavic lands. In August 1160 Niklot was killed, and German colonization ("Ostsiedlung") of the Elbe-Oder region began. In Hanoverian Wendland, Mecklenburg-Vorpommern and Lusatia, invaders started germanization. Early forms of germanization were described by German monks: Helmold in the manuscript "Chronicon Slavorum" and Adam of Bremen in "Gesta Hammaburgensis ecclesiae pontificum." The Polabian language survived until the beginning of the 19th century in what is now the German state of Lower Saxony. In Eastern Germany, around 20% of Germans have historic Slavic paternal ancestry, as revealed in Y-DNA testing. Similarly, in Germany, around 20% of the foreign surnames are of Slavic origin.

Cossacks, although Slavic-speaking and practicing as Orthodox Christians, came from a mix of ethnic backgrounds, including Tatars and other Turks. Many early members of the Terek Cossacks were Ossetians.

The Gorals of southern Poland and northern Slovakia are partially descended from Romance-speaking Vlachs, who migrated into the region from the 14th to 17th centuries and were absorbed into the local population. The population of Moravian Wallachia also descend of this population.

Conversely, some Slavs were assimilated into other populations. Although the majority continued towards Southeast Europe, attracted by the riches of the territory which would become Bulgaria, a few remained in the Carpathian Basin in Central Europe. There they were ultimately assimilated into the Magyar people. Numerous river and other placenames in Romania are of Slavic origin.

There are an estimated 360 million Slavs worldwide.





</doc>
<doc id="29441" url="https://en.wikipedia.org/wiki?curid=29441" title="Skylab">
Skylab

Skylab was the United States' space station that orbited the Earth from 1973 to 1979, when it fell back to Earth amid huge worldwide media attention. Launched and operated by NASA, Skylab included a workshop, a solar observatory, and other systems necessary for crew survival and scientific experiments. It was launched unmanned by a modified Saturn V rocket, with a weight of . Lifting Skylab into low earth orbit was the final mission and launch of a Saturn V rocket (famous for carrying the manned Moon landing missions). There were a total of three manned expeditions to the station, conducted between May 1973 and February 1974. Each mission delivered a three-astronaut crew in the Apollo Command/Service Module (Apollo CSM) launched by the smaller Saturn IB rocket. For the final two manned missions to Skylab, a backup Apollo CSM/Saturn IB was assembled and made ready in case an in-orbit rescue mission was needed, but this backup vehicle was never flown.

The station was damaged during launch when the micrometeoroid shield tore away from the workshop, taking one of the main solar panel arrays with it and jamming the other main array. This deprived Skylab of most of its electrical power, and also removed protection from intense solar heating, threatening to make it unusable. The first crew was able to save Skylab by deploying a replacement heat shade and freeing the jammed solar panels. This was the first time a repair of this magnitude had been performed in space.

Skylab included the Apollo Telescope Mount (a multi-spectral solar observatory), Multiple Docking Adapter (with two docking ports), Airlock Module with extravehicular activity (EVA) hatches, and the Orbital Workshop (the main habitable space inside Skylab). Electrical power came from solar arrays, as well as fuel cells in the docked Apollo CSM. The rear of the station included a large waste tank, propellant tanks for maneuvering jets, and a heat radiator. Numerous experiments were conducted aboard Skylab during its operational life. Solar science was significantly advanced by the telescope, and observation of the Sun was unprecedented. Thousands of photographs of Earth were taken, and the Earth Resources Experiment Package (EREP) viewed Earth with sensors that recorded data in the visible, infrared, and microwave spectral regions. The record for human time spent in orbit was extended beyond the 23 days set by the Soyuz 11 crew aboard Salyut 1, to 84 days by the Skylab 4 crew.

Later plans to reuse Skylab were stymied by delays in development of the Space Shuttle and Skylab's decaying orbit could not be stopped. Skylab's atmospheric reentry began on July 11, 1979. Before re-entry, NASA ground controllers tried to adjust Skylab's orbit to minimize the risk of debris landing in populated areas, with their target—the south Indian Ocean—partially successful. Debris showered Western Australia; recovered pieces indicated that the station had disintegrated lower than expected. As the Skylab program drew to a close, NASA's focus had shifted to the development of the Space Shuttle. After Skylab, NASA space station/laboratory projects included Spacelab, Shuttle-"Mir", and Space Station "Freedom" (which was later merged into the International Space Station).

Rocket engineer Wernher von Braun, science fiction writer Arthur C. Clarke, and other early advocates of manned space travel, expected until the 1960s that a space station would be an important early step in space exploration. Von Braun participated in the publishing of a series of influential articles in "Collier's" magazine from 1952 to 1954, titled "Man Will Conquer Space Soon!". He envisioned a large, circular station 250 feet (75m) in diameter that would rotate to generate artificial gravity and require a fleet of 7,000-ton (6,500-metric ton) space shuttles for construction in orbit. The 80 men aboard the station would include astronomers operating a telescope, meteorologists to forecast the weather, and soldiers to conduct surveillance. Von Braun expected that future expeditions to the Moon and Mars would leave from the station.

The development of the transistor, the solar cell, and telemetry, led in the 1950s and early 1960s to unmanned satellites that could take photographs of weather patterns or enemy nuclear weapons and send them to Earth. A large station was no longer necessary for such purposes, and the United States Apollo program to send men to the Moon chose a mission mode that would not need in-orbit assembly. A smaller station that a single rocket could launch, retained value however for scientific purposes.

In 1959, von Braun, head of the Development Operations Division at the Army Ballistic Missile Agency, submitted his final Project Horizon plans to the U.S. Army. The overall goal of Horizon was to place men on the Moon, a mission that would soon be taken over by the rapidly forming NASA. Although concentrating on the Moon missions, von Braun also detailed an orbiting laboratory built out of a Horizon upper stage, an idea used for Skylab. A number of NASA centers studied various space station designs in the early 1960s. Studies generally looked at platforms launched by the Saturn V, followed up by crews launched on Saturn IB using an Apollo Command/Service Module, or a Gemini capsule on a Titan II-C, the latter being much less expensive in the case where cargo was not needed. Proposals ranged from an Apollo-based station with two to three men, or a small "canister" for four men with Gemini capsules resupplying it, to a large, rotating station with 24 men and an operating lifetime of about five years. A proposal to study the use of a Saturn S-IVB as a manned space laboratory was documented in 1962 by the Douglas Aircraft Company.

The Department of Defense (DoD) and NASA cooperated closely in many areas of space. In September 1963, NASA and the DoD agreed to cooperate in building a space station. The DoD wanted its own manned facility, however, and in December it announced Manned Orbital Laboratory (MOL), a small space station primarily intended for photo reconnaissance using large telescopes directed by a two-man crew. The station was the same diameter as a Titan II upper stage, and would be launched with the crew riding atop in a modified Gemini capsule with a hatch cut into the heat shield on the bottom of the capsule. MOL competed for funding with a NASA station for the next five years and politicians and other officials often suggested that NASA participate in MOL or use the DoD design. The military project led to changes to the NASA plans so that they would resemble MOL less.

NASA management was concerned about losing the 400,000 workers involved in Apollo after landing on the Moon in 1969. A reason von Braun, head of NASA's Marshall Space Flight Center during the 1960s, advocated for a smaller station after his large one was not built was that he wished to provide his employees with work beyond developing the Saturn rockets, which would be completed relatively early during Project Apollo. NASA set up the "Apollo Logistic Support System Office", originally intended to study various ways to modify the Apollo hardware for scientific missions. The office initially proposed a number of projects for direct scientific study, including an extended-stay lunar mission which required two Saturn V launchers, a "lunar truck" based on the Lunar Module (LEM), a large manned solar telescope using a LEM as its crew quarters, and small space stations using a variety of LEM or CSM-based hardware. Although it did not look at the space station specifically, over the next two years the office would become increasingly dedicated to this role. In August 1965, the office was renamed, becoming the "Apollo Applications Program" (AAP).

As part of their general work, in August 1964 the Manned Spacecraft Center (MSC) presented studies on an expendable lab known as "Apollo "X"", short for "Apollo Extension System". "Apollo X" would have replaced the LEM carried on the top of the S-IVB stage with a small space station slightly larger than the CSM's service area, containing supplies and experiments for missions between 15 and 45 days' duration. Using this study as a baseline, a number of different mission profiles were looked at over the next six months.

In November 1964, von Braun proposed a more ambitious plan to build a much larger station built from the S-II second stage of a Saturn V. His design replaced the S-IVB third stage with an aeroshell, primarily as an adapter for the CSM on top. Inside the shell was a cylindrical equipment section. On reaching orbit, the S-II second stage would be vented to remove any remaining hydrogen fuel, then the equipment section would be slid into it via a large inspection hatch. This became known as a "wet workshop" concept, because of the conversion of an active fuel tank. The station filled the entire interior of the S-II stage's hydrogen tank, with the equipment section forming a "spine" and living quarters located between it and the walls of the booster. This would have resulted in a very large living area. Power was to be provided by solar cells lining the outside of the S-II stage.

One problem with this proposal was that it required a dedicated Saturn V launch to fly the station. At the time the design was being proposed, it was not known how many of the then-contracted Saturn Vs would be required to achieve a successful Moon landing. However, several planned Earth-orbit test missions for the LEM and CSM had been canceled, leaving a number of Saturn IBs free for use. Further work led to the idea of building a smaller "wet workshop" based on the S-IVB, launched as the second stage of a Saturn IB.

A number of S-IVB-based stations were studied at MSC from mid-1965, which had much in common with the Skylab design that eventually flew. An airlock would be attached to the hydrogen tank, in the area designed to hold the LEM, and a minimum amount of equipment would be installed in the tank itself in order to avoid taking up too much fuel volume. Floors of the station would be made from an open metal framework that allowed the fuel to flow through it. After launch, a follow-up mission launched by a Saturn IB would launch additional equipment, including solar panels, an equipment section and docking adapter, and various experiments. Douglas Aircraft, builder of the S-IVB stage, was asked to prepare proposals along these lines. The company had for several years been proposing stations based on the S-IV stage, before it was replaced by the S-IVB.
On April 1, 1966, MSC sent out contracts to Douglas, Grumman, and McDonnell for the conversion of a S-IVB spent stage, under the name "Saturn S-IVB spent-stage experiment support module" (SSESM). In May, astronauts voiced concerns over the purging of the stage's hydrogen tank in space. Nevertheless, in late July it was announced that the Orbital Workshop would be launched as a part of Apollo mission AS-209, originally one of the Earth-orbit CSM test launches, followed by two Saturn I/CSM crew launches, AAP-1 and AAP-2.

MOL remained AAP's chief competitor for funds, although the two programs cooperated on technology. NASA considered flying experiments on MOL, or using its Titan IIIC booster instead of the much more expensive Saturn IB. The agency decided that the Air Force station was not large enough, and that converting Apollo hardware for use with Titan would be too slow and too expensive. The DoD later canceled MOL in June 1969.

Design work continued over the next two years, in an era of shrinking budgets. (NASA sought $450 million for Apollo Applications in fiscal year 1967, for example, but received $42 million.) In August 1967, the agency announced that the lunar mapping and base construction missions examined by the AAP were being canceled. Only the Earth-orbiting missions remained, namely the Orbital Workshop and Apollo Telescope Mount solar observatory.

The success of Apollo 8 in December 1968, launched on the third flight of a Saturn V, made it likely that one would be available to launch a dry workshop. Later, several Moon missions were canceled as well, originally to be Apollo missions 18 through 20. The cancellation of these missions freed up three Saturn V boosters for the AAP program. Although this would have allowed them to develop von Braun's original S-II based mission, by this time so much work had been done on the S-IV based design that work continued on this baseline. With the extra power available, the wet workshop was no longer needed; the S-IC and S-II lower stages could launch a "dry workshop", with its interior already prepared, directly into orbit.

A dry workshop simplified plans for the interior of the station. Industrial design firm Raymond Loewy/William Snaith recommended emphasizing habitability and comfort for the astronauts by providing a wardroom for meals and relaxation and a window to view Earth and space, although astronauts were dubious about the designers' focus on details such as color schemes. Habitability had not previously been an area of concern when building spacecraft due to their small size and brief mission durations, but the Skylab missions would last for months. NASA sent a scientist on Jacques Piccard's "Ben Franklin" submarine in the Gulf Stream in July and August 1969 to learn how six people would live in an enclosed space for four weeks.

Astronauts were uninterested in watching movies on a proposed entertainment center or in playing games, but they did want books and individual music choices. Food was also important; early Apollo crews complained about its quality, and a NASA volunteer found it intolerable to live on the Apollo food for four days on Earth. Its taste and composition were unpleasant, in the form of cubes and squeeze tubes. Skylab food significantly improved on its predecessors by prioritizing edibility over scientific needs.

Each astronaut had a private sleeping area the size of a small walk-in closet, with a curtain, sleeping bag, and locker. Designers also added a shower and a toilet for comfort and to obtain precise urine and feces samples for examination on Earth.

Skylab did not have recycling systems such as conversion of urine to drinking water; it also did not dispose of waste by dumping it into space. The S-IVB's liquid oxygen tank below the OWS was used to store trash and waste water, passed through an airlock.

Rescuing astronauts from Skylab was possible in the most likely emergency circumstances. The crew could use the CSM to quickly return to Earth if the station suffered serious damage. If the CSM failed, the spacecraft and Saturn IB for the next Skylab mission would have been launched with two astronauts to retrieve the crew; given Skylab's ample supplies, its residents would have been able to wait up to several weeks for the rescue mission.

On August 8, 1969, the McDonnell Douglas Corporation received a contract for the conversion of two existing S-IVB stages to the Orbital Workshop configuration. One of the S-IV test stages was shipped to McDonnell Douglas for the construction of a mock-up in January 1970. The Orbital Workshop was renamed "Skylab" in February 1970 as a result of a NASA contest. The actual stage that flew was the upper stage of the AS-212 rocket (the S-IVB stage, S-IVB 212). The mission computer used aboard Skylab was the IBM System/4Pi TC-1, a relative of the AP-101 Space Shuttle computers. A Saturn V originally produced for the Apollo program—before the cancellation of Apollo 18, 19, and 20—was repurposed and redesigned to launch Skylab. The Saturn V's upper stage was removed, but with the controlling Instrument Unit remaining in its standard position.

Skylab was launched on May 14, 1973 by the modified Saturn V. The launch is sometimes referred to as Skylab 1, or SL-1. Severe damage was sustained during launch and deployment, including the loss of the station's micrometeoroid shield/sun shade and one of its main solar panels. Debris from the lost micrometeoroid shield further complicated matters by pinning the remaining solar panel to the side of the station, preventing its deployment and thus leaving the station with a huge power deficit.

Immediately following Skylab's launch, Pad A at Kennedy Space Center Launch Complex 39 was deactivated, and construction proceeded to modify it for the Space Shuttle program, originally targeting a maiden launch in March 1979. The manned missions to Skylab would occur using a Saturn IB rocket from Launch Pad 39B.

SL-1 was the last unmanned launch from LC-39A until February 19, 2017, when SpaceX CRS-10 was launched from there.

Three manned missions, designated SL-2, SL-3 and SL-4, were made to Skylab. The first manned mission, SL-2, launched on May 25, 1973 atop a Saturn IB and involved extensive repairs to the station. The crew deployed a parasol-like sunshade through a small instrument port from the inside of the station, bringing station temperatures down to acceptable levels and preventing overheating that would have melted the plastic insulation inside the station and released poisonous gases. This solution was designed by NASA's "Mr. Fix It" Jack Kinzler, who won the NASA Distinguished Service Medal for his efforts. The crew conducted further repairs via two spacewalks (extra-vehicular activity, or EVA). The crew stayed in orbit with Skylab for 28 days. Two additional missions followed, with the launch dates of July 28, 1973 (SL-3) and November 16, 1973 (SL-4), and mission durations of 59 and 84 days, respectively. The last Skylab crew returned to Earth on February 8, 1974.

In addition to the three manned missions, there was a rescue mission on standby that had a crew of two, but could take five back down.


Also of note was the three-man crew of Skylab Medical Experiment Altitude Test, who spent 56 days at low-pressure in 1972 on Earth. This was a spaceflight analog test in full gravity, but various Skylab hardware and medical knowledge was gained

Skylab orbited Earth 2,476 times during the 171 days and 13 hours of its occupation during the three manned Skylab expeditions. Each of these extended the human record of 23 days for amount of time spent in space set by the Soviet Soyuz 11 crew aboard the space station Salyut 1 on June 30, 1971. Skylab 2 lasted 28 days, Skylab 3 56 days, and Skylab 4 84 days. Astronauts performed ten spacewalks, totaling 42 hours and 16 minutes. Skylab logged about 2,000 hours of scientific and medical experiments, 127,000 frames of film of the Sun and 46,000 of Earth. Solar experiments included photographs of eight solar flares, and produced valuable results that scientists stated would have been impossible to obtain with unmanned spacecraft. The existence of the Sun's coronal holes were confirmed because of these efforts. Many of the experiments conducted investigated the astronauts' adaptation to extended periods of microgravity.

A typical day began at 6 a.m. Central Time Zone. Although the toilet was small and noisy, both veteran astronauts—who had endured earlier missions' rudimentary waste-collection systems—and rookies complimented it. The first crew enjoyed taking a shower once a week, but found drying themselves in weightlessness and vacuuming excess water difficult; later crews usually cleaned themselves daily with wet washcloths instead of using the shower. Astronauts also found that bending over in weightlessness to put on socks or tie shoelaces strained their stomach muscles.

Breakfast began at 7 a.m. Astronauts usually stood to eat, as sitting in microgravity also strained their stomach muscles. They reported that their food—although greatly improved from Apollo—was bland and repetitive, and weightlessness caused utensils, food containers, and bits of food to float away; also, gas in their drinking water contributed to flatulence. After breakfast and preparation for lunch, experiments, tests and repairs of spacecraft systems and, if possible, 90 minutes of physical exercise followed; the station had a bicycle and other equipment, and astronauts could jog around the water tank. After dinner, which was scheduled for 6 p.m., crews performed household chores and prepared for the next day's experiments. Following lengthy daily instructions (some of which were up to 15 meters long) sent via teleprinter, the crews were often busy enough to postpone sleep.

The station offered what a later study called "a highly satisfactory living and working environment for crews", with enough room for personal privacy. Although it had a dart set, playing cards, and other recreational equipment in addition to books and music players, the window with its view of Earth became the most popular way to relax in orbit.

Prior to departure about 80 experiments were named, although they are also described as "almost 300 separate investigations". Experiments were divided into six broad categories:

Because the solar scientific airlock—one of two research airlocks—was unexpectedly occupied by the "Parasol" that replaced the missing meteorite shield, a few experiments were instead installed outside with the telescopes during space walks, or shifted to the Earth-facing scientific airlock.

Skylab 2 spent less time than planned on most experiments due to station repairs. On the other hand, Skylab 3 and Skylab 4 far exceeded the initial experiment plans, once the crews adjusted to the environment and established comfortable working relationships with ground control.

The figure (below) lists an overview of most major experiments. Skylab 4 carried out several more experiments, such as to observe Comet Kohoutek.

Skylab had certain features to protect vulnerable technology from radiation. The window was vulnerable to darkening, and this darkening could affect experiment S190. As a result, a light shield that could be open or shut was designed and installed on Skylab. To protect a wide variety of films, used for a variety of experiments and for astronaut photography, there were 5 film vaults. There were four smaller film vaults in the Multiple Docking Adapter, which had to have four not one, because each spar could not carry enough weight for single larger film vault. The orbital workshop could handle a single larger safe, which is also more efficient for shielding. The large vault in the orbital workshop had an empty mass of 2398 lb (1088 kg, 171.3 stones). The four smaller vaults had combined mass of 1545 lb. The primary construction material of all five safes was aluminum. When Skylab re-entered there was one 180 lb chunk of aluminum found that was thought to be a door to one of the film vaults. The big film vault was one of the heaviest single pieces of Skylab to re-enter Earth's atmosphere.

A later example of a radiation vault is the Juno Radiation Vault for the Juno orbiter for Jupiter; in that case it was designed to protect much of the unmanned spacecraft's electronics and it used 1 cm thick walls of titanium.

The film vault was used for storing film from various sources including the Apollo Telescope Mount solar instruments. Six ATM experiments used film to record data, and over the course of the missions over 150,000 successful exposures were recorded. The film canister had to be manually retrieved on manned spacewalks to the instruments during the missions. The film canisters were returned to Earth aboard the Apollo capsules when each mission ended, and were among the heaviest items that had to be returned at the end of each mission. The heaviest canisters weighed 40 kg and could hold up to 16,000 frames of film.

There were two types of gyroscopes on Skylab. Control-moment gyroscopes could physically move the station, and rate gyroscopes measured the rate of rotation to find its orientation. (see Control moment gyroscope (CMG)) The CMG helped provide the fine pointing needed by the Apollo Telescope Mount, and to resist various forces that can change the station's orientation.

Some of the forces acting on Skylab that the pointing system needed to resist:

Skylab was one of the first large spacecraft to use big gyroscopes, that could control its attitude. The control could also be used to help point the instruments. The gyroscopes took about ten hours to get spun up if they were turned off. There was also a thruster system to control Skylab's attitude. There were 9 rate-gyroscope sensors, 3 for each axis. These were sensors that fed their output to the Skylab digital computer. Two of three were active and their input was averaged, while the third was a backup. From NASA SP-400 "Skylab, Our First Space Station", "each Skylab control-moment gyroscope consisted of a motor-driven rotor, electronics assembly, and power inverter assembly. The 21-inch diameter rotor weighed 155 pounds and rotated at approximately 8950 revolutions per minute".

There were three control movement gyroscopes on Skylab, but only two were required to maintain pointing. The control and sensor gyroscopes were part of a system that help detect and control the orientation of the station in space. Other sensors that helped with this were a Sun tracker and a star tracker. The sensors fed data to the main computer, which could then use the control gyroscopes and or the thruster system to keep Skylab pointed as desired.

Skylab had a zero-gravity shower system in the work and experiment section of the Orbital Workshop designed and built at the Manned Spaceflight Center. It had a cylindrical curtain that went from floor to ceiling and a vacuum system to suck away water. The floor of the shower had foot restraints.

To bathe, the user coupled a pressurized bottle of warmed water to the shower’s plumbing, then stepped inside and secured the curtain. A push-button shower nozzle was connected by a stiff hose to the top of the shower. The system was designed for about 6 pints (2.8 liters) of water per shower, the water drawn from the personal hygiene water tank. The use of both the liquid soap and water was carefully planned out, with enough soap and warm water for one shower per week per person.

The first astronaut to use the space shower was Paul J. Weitz on Skylab 2, the first manned mission. A Skylab shower took about two and a half hours, including the time to set up the shower and dissipate used water. The procedure for operating the shower was as follows:

One of the big concerns with bathing in space was control of droplets of water so that they did not cause an electrical short by floating into the wrong area. The vacuum water system was thus integral to the shower. The vacuum fed to a centrifugal separator, filter, and collection bag to allow the system to vacuum up the fluids. Waste water was injected into a disposal bag which was in turn put in the waste tank. The material for the shower enclosure was fire-proof beta cloth wrapped around hoops of diameter; the top hoop was connected to the ceiling. The shower could be collapsed to the floor when not in use. Skylab also supplied astronauts with rayon terrycloth towels which had a color-coded stitching for each crew-member. There were 420 towels on board Skylab initially.

A simulated Skylab shower was also used during the 56 day Earth-bound Skylab analog mission SMEAT; the crew used the shower after exercise and found it a positive experience. .

There was a variety of hand-held and fixed experiments that used various types of film. In addition to the instruments in the ATM solar observatory, 35 and 70 mm film cameras were carried on board. A TV camera was carried that recorded video electronically.

It was determined that film would fog up to due to radiation over the course of the mission. To prevent this film was stored in vaults.

Personal (hand-held) camera equipment:

Film for the DAC was contained in DAC Film Magazines, which contained up to 140 feet (42.7 m) of film. At 24 frames per second this was enough for 4 minutes of filming, with progressively longer film times with lower frame rates such as 16 minutes at 6 frames per second. The film had to be loaded or unloaded from the DAC in a photographic dark room.


Experiment S190B was the Actron Earth Terrain Camera

The S190A was the "Multispectral Photographic Camera "

There was also a pair of Leitz Trinovid 10 x 40 binoculars modified for use in space to aid in Earth observations.

Skylab was controlled in part by a digital computer system, and one of its main jobs was to control the pointing of the station; pointing was especially important for its solar power collection and observatory functions. The computer consisted of two actual computers, a primary and a secondary. The system ran several thousand words of code, which was also backed up on the Memory Load Unit (MLU). The two computers were linked to each other and various input and output items by the workshop computer interface. Operations could be switched from the primary to the backup, which were the same design, either automatically if errors were detected, by the Skylab crew, or from the ground.

The Skylab computer was a space-hardened and customized version of the TC-1 computer, a version of the IBM System/4 Pi, itself based on the System 360 computer. The TC-1 had a 16,000 word memory based on ferrite memory cores, while the MLU was a read-only tape drive that contained a backup of the main computer programs. The tape drive would take 11 seconds to upload the backup of the software program to a main computer. The TC-1 used 16-bit words and the central processor came from the 4Pi computer. There was a 16k and an 8k version of the software program.

The computer weighed 100 pounds (45.4 kg, 7.14 stones) and consumed about ten percent of the station's electrical power.

After launch the computer is what the controllers on the ground communicated with to control the station's orientation. When the sun-shield was torn off the ground staff had to balance solar heating with electrical production. On March 6, 1978 the computer system was re-activated by NASA to control the re-entry.

The system had a user interface which consisted of a display, ten buttons, and a three position switch. Because the numbers were in octal (base-8), it only had numbers zero to seven (8 keys), and the other two keys were enter and clear. The display could show minutes and seconds which would count-down to orbital benchmarks, or it could display keystrokes when using the interface. The interface could be used to change the software program. The user interface was called the Digital Address System (DAS) and could send commands to the computer's command system. The command system could also get commands from the ground.

For personal computing needs Skylab crews were equipped with models of the then new hand-held electronic scientific calculator, which was used in place of slide-rules used on prior space missions as the primary personal computer. The model used was the Hewlett Packard HP 35. Some slide rules continued in use aboard Skylab, and a circular slide rule was at the workstation.

The three crewed Skylab missions used only about 16.8 of the 24 man-months of oxygen, food, water, and other supplies stored aboard Skylab. A fourth manned mission was under consideration, which would have used the launch vehicle kept on standby for the Skylab Rescue mission. This would have been a 20-day mission to boost Skylab to a higher altitude and do more scientific experiments. Another plan was to use a Teleoperator Retrieval System (TRS) launched aboard the Space Shuttle (then under development), to robotically re-boost the orbit. When Skylab 5 was cancelled, it was expected Skylab would stay in orbit until the 1980s, which was enough time to overlap with the beginning of Shuttle launches. Other options for launching TRS included the Titan III and Atlas Agena. None of these options received the level of effort and funding needed for execution before Skylab's sooner-than-expected re-entry. Skylab's internal systems were evaluated and tested from the ground, and effort was put into plans for re-using it, as late as 1978.

Though no one returned after the end of the SL-4 mission in February 1974, the crew left a bag filled with supplies to welcome visitors, and left the hatch unlocked. NASA discouraged any discussion of additional visits due to the station's age, but in 1977 and 1978, when the agency still believed the Space Shuttle would be ready by 1979, it completed two studies on reusing the station. By September 1978, the agency believed Skylab was safe for crews, with all major systems intact and operational. It still had 180 man-days of water and 420 man-days of oxygen, and astronauts could refill both; the station could hold up to about 600 to 700 man-days of drinkable water and 420 man-days of food. Before SL-4 left they did one more boost, running the Skylab thrusters for 3 minutes which added 11 km in height to its orbit. Skylab was left in a 433 by 455 km orbit on departure. At this time, the NASA-accepted estimate for its re-entry was nine years.

The studies cited several benefits from reusing Skylab, which one called a resource worth "hundreds of millions of dollars" with "unique habitability provisions for long duration space flight". Because no more operational Saturn V rockets were available after the Apollo program, four to five shuttle flights and extensive space architecture would have been needed to build another station as large as Skylab's volume. Its ample size—much greater than that of the shuttle alone, or even the shuttle plus Spacelab—was enough, with some modifications, for up to seven astronauts of both sexes, and experiments needing a long duration in space; even a movie projector for recreation was possible.

Proponents of Skylab's reuse also said repairing and upgrading Skylab would provide information on the results of long-duration exposure to space for future stations. The most serious issue for reactivation was stationkeeping, as one of the station's gyroscopes had failed and the attitude control system needed refueling; these issues would need EVA to fix or replace. The station had not been designed for extensive resupply. However, although it was originally planned that Skylab crews would only perform limited maintenance they successfully made major repairs during EVA, such as the SL-2 crew's deployment of the solar panel and the SL-4 crew's repair of the primary coolant loop. The SL-2 crew fixed one item during EVA by, reportedly, "hit[ting] it with [a] hammer".

Some studies also said, beyond the opportunity for space construction and maintenance experience, reactivating the station would free up shuttle flights for other uses, and reduce the need to modify the shuttle for long-duration missions. Even if the station were not manned again, went one argument, it would serve as a useful experimental platform.

The reactivation would likely have occurred in four phases:

The first three phases would have required about $60 million in 1980s dollars, not including launch costs.

Other options for launching TRS were Titan III or Atlas Agena.

After a boost of by SL-4's Apollo CSM before its departure in 1974, Skylab was left in a parking orbit of by that was expected to last until at least the early 1980s, based on estimates of the 11-year sunspot cycle that began in 1976. NASA first considered as early as 1962 the potential risks of a space station reentry, but decided not to incorporate a retrorocket system in Skylab due to cost and acceptable risk.

The spent 49-ton Saturn V S-II stage which had launched Skylab in 1973 remained in orbit for almost two years, and made an uncontrolled reentry on January 11, 1975. Some debris, most prominently the five heavy J-2 engines, likely survived to impact in the North Atlantic Ocean. Although this event did not receive heavy media or public attention, it was followed closely by NASA and the Air Force, and helped emphasize the need for improved planning and public awareness for Skylab's eventual reentry.

British mathematician Desmond King-Hele of the Royal Aircraft Establishment predicted in 1973 that Skylab would de-orbit and crash to earth in 1979, sooner than NASA's forecast, because of increased solar activity. Greater-than-expected solar activity heated the outer layers of Earth's atmosphere and increased drag on Skylab. By late 1977, NORAD also forecast a reentry in mid-1979; a National Oceanic and Atmospheric Administration (NOAA) scientist criticized NASA for using an inaccurate model for the second most-intense sunspot cycle in a century, and for ignoring NOAA predictions published in 1976.

The reentry of the USSR's nuclear powered Cosmos 954 in January 1978, and the resulting radioactive debris fall in northern Canada, drew more attention to Skylab's orbit. Although Skylab did not contain radioactive materials, the State Department warned NASA about the potential diplomatic repercussions of station debris. Battelle Memorial Institute forecast that up to 25 tons of metal debris could land in 500 pieces over an area 4,000 miles long and 1,000 miles wide. The lead-lined film vault, for example, might land intact at 400 feet per second.

Ground controllers re-established contact with Skylab in March 1978 and recharged its batteries. Although NASA worked on plans to reboost Skylab with the Space Shuttle through 1978 and the TRS was almost complete, the agency gave up in December when it became clear that the shuttle would not be ready in time; its first flight, STS-1, did not occur until April 1981. Also rejected were proposals to launch the TRS using one or two unmanned rockets or to attempt to destroy the station with missiles.

Skylab's demise in 1979 was an international media event, with T-shirts and hats with bullseyes and "Skylab Repellent" with a money-back guarantee, wagering on the time and place of re-entry, and nightly news reports. The "San Francisco Examiner" offered a $10,000 prize for the first piece of Skylab delivered to its offices; the competing "San Francisco Chronicle" offered $200,000 if a subscriber suffered personal or property damage. A Nebraska neighborhood painted a target so that the station would have "something to aim for", a resident said.

NASA calculated that the odds were 1 to 152 of debris hitting any human. The odds were 1 to 7 of debris hitting a city of 100,000 people or more, and special teams were readied to head to any country hit by debris. The event caused so much panic in the Philippines that President Ferdinand Marcos appeared on national television to reassure the public.

A week before re-entry, NASA forecast that it would occur between July 10 and 14, with the 12th the most likely date, and the Royal Aircraft Establishment predicted the 14th. In the hours before the event, ground controllers adjusted Skylab's orientation to minimize the risk of re-entry on a populated area. They aimed the station at a spot south-southeast of Cape Town, South Africa, and re-entry began at approximately 16:37 UTC, July 11, 1979. The Air Force provided data from a secret tracking system. The station did not burn up as fast as NASA expected. Debris landed about east of Perth, Western Australia due to a 4% calculation error, and was found between Esperance, Western Australia and Rawlinna, from 31° to 34°S and 122° to 126°E, about 130–150 km (81–93 miles) radius around Balladonia, Western Australia. Residents and an airline pilot saw dozens of colorful flares as large pieces broke up in the atmosphere. The Shire of Esperance facetiously fined NASA A$400 for littering; the fine was paid in April 2009, when radio show host Scott Barley of Highway Radio raised the funds from his morning show listeners on behalf of NASA.

Stan Thornton found 24 pieces of Skylab at his home in Esperance, and a Philadelphia businessman flew him, his parents, and his girlfriend to San Francisco where he collected the "Examiner" prize. In a coincidence for the organizers, the annual Miss Universe pageant was scheduled a few days later on July 20, 1979 in Perth, and a large piece of Skylab debris was displayed on the stage. Analysis of the debris showed that the station had disintegrated above the Earth, much lower than expected.

After the demise of Skylab, NASA focused on the reusable Spacelab module, an orbital workshop that could be deployed with the Space Shuttle and returned to Earth. The next American major space station project was Space Station Freedom, which was merged into the International Space Station in 1993 and launched starting in 1998. Shuttle-Mir was another project and led to the US funding Spektr, Priroda, and the Mir Docking Module in the 1990s.

There was a Skylab Rescue mission assembled for the second manned mission to Skylab, but it was not needed. Another rescue mission was assembled for the last Skylab and was also on standby for ASTP. That launch stack might have been used for Skylab 5, what would be the fourth manned Skylab mission but this was cancelled and the SA-209 Saturn IB rocket was put on display at NASA Kennedy Space Center.

Launch vehicles:

Skylab 5 would have been a short 20-day mission to conduct more scientific experiments and use the Apollo's Service Propulsion System engine to boost Skylab into a higher orbit. Vance Brand (commander), William B. Lenoir (science pilot), and Don Lind (pilot) would have been the crew for this mission, with Brand and Lind being the prime crew for the Skylab Rescue flights. Brand and Lind also trained for a mission that would have aimed Skylab for a controlled deorbit.

In addition to the flown Skylab space station, a second flight-quality backup Skylab space station had been built during the program. NASA considered using it for a second station in May 1973 or later, to be called Skylab B (S-IVB 515), but decided against it. Launching another Skylab with another Saturn V rocket would have been very costly, and it was decided to spend this money on the development of the Space Shuttle instead. The backup is on display at the National Air and Space Museum in Washington, D.C.

A full-size training mock-up once used for astronaut training is located at the Lyndon B. Johnson Space Center visitor's center in Houston, Texas. Another full-size training mock-up is at the U.S. Space & Rocket Center in Huntsville, Alabama. Originally displayed indoors, it was subsequently stored outdoors for several years to make room for other exhibits. To mark the 40th anniversary of the Skylab program, the Orbital Workshop portion of the trainer was restored and moved into the Davidson Center in 2013. NASA transferred the backup Skylab to the National Air and Space Museum in 1975. On display in the Museum's Space Hall since 1976, the orbital workshop has been slightly modified to permit viewers to walk through the living quarters.

The numerical identification of the manned Skylab missions was the cause of some confusion. Originally, the unmanned launch of Skylab and the three manned missions to the station were numbered "SL-1" through "SL-4". During the preparations for the manned missions, some documentation was created with a different scheme—"SLM-1" through "SLM-3"—for those missions only. William Pogue credits Pete Conrad with asking the Skylab program director which scheme should be used for the mission patches, and the astronauts were told to use 1-2-3, not 2-3-4. By the time NASA administrators tried to reverse this decision, it was too late, as all the in-flight clothing had already been manufactured and shipped with the 1-2-3 mission patches.

The "Skylab Medical Experiment Altitude Test" or SMEAT was a 56-day (8 week) Earth analog Skylab test. The test had a low-pressure high oxygen-percentage atmosphere but had full gravity due to the effect of being on Earth's surface. The test had a three-man crew with Commander (Crippen), Science Pilot (Bobko), and Pilot (Thornton); there was a focus on medical studies and Thornton was an M.D. They entered a pressure chamber converted to be more a like Skylab on July 26, 1972, and it ended on September 20, 1972.

From 1966 to 1974, the Skylab program cost a total of $2.2 billion, equivalent to $10 billion in 2010 dollars. As its three three-man crews spent 510 total man-days in space, each man-day cost approximately $20 million, compared to $7.5 million for the International Space Station.








</doc>
<doc id="29442" url="https://en.wikipedia.org/wiki?curid=29442" title="Sacramento (disambiguation)">
Sacramento (disambiguation)

Sacramento is the capital of the U.S. state of California.

Sacramento may also refer to:









</doc>
<doc id="29445" url="https://en.wikipedia.org/wiki?curid=29445" title="StrongARM">
StrongARM

The StrongARM is a family of computer microprocessors developed by Digital Equipment Corporation and manufactured in the late 1990s which implemented the ARM v4 instruction set architecture. It was later sold to Intel in 1997, who continued to manufacture it before replacing it with the XScale in the early 2000s.

The StrongARM was a collaborative project between DEC and Advanced RISC Machines to create a faster ARM microprocessor. The StrongARM was designed to address the upper-end of the low-power embedded market, where users needed more performance than the ARM could deliver while being able to accept more external support. Targets were devices such as newer personal digital assistants and set-top boxes.

Traditionally, the semiconductor division of DEC was located in Massachusetts. In order to gain access to the design talent in Silicon Valley, DEC opened a design center in Palo Alto, California. This design center was led by Dan Dobberpuhl and was the main design site for the StrongARM project. Another design site which worked on the project was in Austin, Texas that was created by some ex-DEC designers returning from Apple Computer and Motorola. The project was set up in 1995, and quickly delivered their first design, the SA-110.

DEC agreed to sell StrongARM to Intel as part of a lawsuit settlement in 1997. Intel used the StrongARM to replace their ailing line of RISC processors, the i860 and i960.

When the semiconductor division of DEC was sold to Intel, many engineers from the Palo Alto design group moved to SiByte, a start-up company designing MIPS system-on-a-chip (SoC) products for the networking market. The Austin design group spun off to become Alchemy Semiconductor, another start-up company designing MIPS SoCs for the hand-held market. A new StrongARM core was developed by Intel and introduced in 2000 as the XScale.

The SA-110 was the first microprocessor in the StrongARM family. The first versions, operating at 100, 160, and 200 MHz, were announced on 5 February 1996. When announced, samples of these versions were available, with volume production slated for mid-1996. Faster 166 and 233 MHz versions were announced on 12 September 1996. Samples of these versions were available at announcement, with volume production slated for December 1996. Throughout 1996, the SA-110 was the highest performing microprocessor for portable devices. Towards the end of 1996 it was a leading CPU for internet/intranet appliances and thin client systems. The SA-110's first design win was the Apple MessagePad 2000. It was also used in a number of products including the Acorn Computers Risc PC and Eidos Optima video editing system. The SA-110's lead designers were Daniel W. Dobberpuhl, Gregory W. Hoeppner, Liam Madden, and Richard T. Witek.

The SA-110 had a simple microarchitecture. It was a scalar design that executed instructions in-order with a five-stage classic RISC pipeline. The microprocessor was partitioned into several blocks, the IBOX, EBOX, IMMU, DMMU, BIU, WB and PLL. The IBOX contained hardware that operated in the first two stages of the pipeline such as the program counter. It fetched, decoded and issued instructions. Instruction fetch occurs during the first stage, decode and issue during the second. The IBOX decodes the more complex instructions in the ARM instruction set by translating them into sequences of simpler instructions. The IBOX also handled branch instructions. The SA-110 did not have branch prediction hardware, but had mechanisms for their speedy processing.

Execution starts at stage three. The hardware that operates during this stage is contained in the EBOX, which comprises the register file, arithmetic logic unit (ALU), barrel shifter, multiplier and condition code logic. The register file had three read ports and two write ports. The ALU and barrel shifter executed instructions in a single cycle. The multiplier is not pipelined and has a latency of multiple cycles.

The IMMU and DMMU are memory management units for instructions and data, respectively. Each MMU contained a 32-entry fully associative translation lookaside buffer (TLB) that can map 4 KB, 64 KB or 1 MB pages. The write buffer (WB) has eight 16-byte entries. It enables the pipelining of stores. The bus interface unit (BIU) provided the SA-110 with an external interface.

The PLL generates the internal clock signal from an external 3.68 MHz clock signal. It was not designed by DEC, but was contracted to the Centre Suisse d'Electronique et de Microtechnique (CSEM) located in Neuchâtel, Switzerland.

The instruction cache and data cache each have a capacity of 16 KB and are 32-way set-associative and virtually addressed. The SA-110 was designed to be used with slow (and therefore low-cost) memory and therefore the high set associativity allows a higher hit rate than competing designs, and the use of virtual addresses allows memory to be simultaneously cached and uncached. The caches are responsible for most of the transistor count and they take up half the die area.

The SA-110 contained 2.5 million transistors and is 7.8 mm by 6.4 mm large (49.92 mm). It was fabricated by DEC in its proprietary CMOS-6 process at its Fab 6 fab in Hudson, Massachusetts. CMOS-6 was DEC's sixth-generation complementary metal–oxide–semiconductor (CMOS) process. CMOS-6 has a 0.35 µm feature size, a 0.25 µm effective channel length but for use with the SA-110, only three levels of aluminium interconnect. It used a power supply with a variable voltage of 1.2 to 2.2 volts (V) to enable designs to find a balance between power consumption and performance (higher voltages enable higher clock rates). The SA-110 was packaged in a 144-pin thin quad flat pack (TQFP).

The SA-1100 was a derivative of the SA-110 developed by DEC. Announced in 1997, the SA-1100 was targeted for portable applications such as PDAs and differs from the SA-110 by providing a number of features that are desirable for such applications. To accommodate these features, the data cache was reduced in size to 8 KB.

The extra features are integrated memory, PCMCIA, and color LCD controllers connected to an on-die system bus, and five serial I/O channels that are connected to a peripheral bus attached to the system bus. The memory controller supported FPM and EDO DRAM, SRAM, flash, and ROM. The PCMCIA controller supports two slots. The memory address and data bus is shared with the PCMCIA interface. Glue logic is required. The serial I/O channels implement a slave USB interface, a SDLC, two UARTs, an IrDA interface, a MCP, and a synchronous serial port.

The SA-1100 had a companion chip, the SA-1101. It was introduced by Intel on 7 October 1998. The SA-1101 provided additional peripherals to complement those integrated on the SA-1100 such as a video output port, two PS/2 ports, a USB controller and a PCMCIA controller that replaces that on the SA-1100. Design of the device started by DEC, but was only partially complete when acquired by Intel, who had to finish the design. It was fabricated at DEC's former Hudson, Massachusetts fabrication plant, which was also sold to Intel.

The SA-1100 contained 2.5 million transistors and measured 8.24 mm by 9.12 mm (75.15 mm). It was fabricated in a 0.35 μm CMOS process with three levels of aluminium interconnect and was packaged in a 208-pin TQFP.

One of the early recipients of this processor was-ill-fated Psion netBook and its more consumer oriented sibling Psion Series 7.

The SA-1110 was a derivative of the SA-110 developed by Intel. It was announced on 31 March 1999, positioned as an alternative to the SA-1100. At announcement, samples were set for June 1999 and volume later that year. Intel discontinued the SA-1110 in early 2003. The SA-1110 was available in 133 or 206 MHz versions. It differed from the SA-1100 by featuring support for 66 MHz (133 MHz version only) or 103 MHz (206 MHz version only) SDRAM. Its companion chip, which provided additional support for peripherals, was the SA-1111. The SA-1110 was packaged in a 256-pin micro ball grid array. It was used in mobile phones, personal data assistants (PDAs) such as the Compaq (later HP) iPAQ and HP Jornada, the Sharp SL-5x00 Linux Based Platforms and the Simputer. It was also used to run the Intel Web Tablet, a tablet device that is considered potentially the first to introduce large screen, portable web browsing. Intel dropped the product just prior to launch in 2001.

The SA-1500 was a derivative of the SA-110 developed by DEC initially targeted for set-top boxes. It was designed and manufactured in low volumes by DEC but was never put into production by Intel. The SA-1500 was available at 200 to 300 MHz. The SA-1500 featured an enhanced SA-110 core, an on-chip coprocessor called the "Attached Media Processor" (AMP), and an on-chip SDRAM and I/O bus controller. The SDRAM controller supported 100 MHz SDRAM, and the I/O controller implemented a 32-bit I/O bus that may run at frequencies up to 50 MHz for connecting to peripherals and the SA-1501 companion chip.

The AMP implemented a long instruction word instruction set containing instructions designed for multimedia, such as integer and floating-point multiply–accumulate and SIMD arithmetic. Each long instruction word is 64 bits wide and specifies an arithmetic operation and a branch or a load/store. Instructions operate on operands from a 64-entry 36-bit register file, and on a set of control registers. The AMP communicates with the SA-110 core via an on-chip bus and it shares the data cache with the SA-110. The AMP contained an ALU with a shifter, a branch unit, a load/store unit, a multiply–accumulate unit, and a single-precision floating-point unit. The AMP supported user-defined instructions via a 512-entry writable control store.

The SA-1501 companion chip provided additional video and audio processing capabilities and various I/O functions such as PS/2 ports, a parallel port, and interfaces for various peripherals.

The SA-1500 contains 3.3 million transistors and measures 60 mm. It was fabricated in a 0.28 µm CMOS process. It used a 1.5 to 2.0 V internal power supply and 3.3 V I/O, consuming less than 0.5 W at 100 MHz and 2.5 W at 300 MHz. It was packaged in a 240-pin metal quad flat package or a 256-ball plastic ball grid array.



</doc>
<doc id="29450" url="https://en.wikipedia.org/wiki?curid=29450" title="Shaul Mofaz">
Shaul Mofaz

Lieutenant General Shaul Mofaz (; born Shahrām Mofazzazkār ; 4 November 1948) is an Iranian-born Israeli former soldier and politician. He joined the Israel Defense Forces in 1966 and served in the Paratroopers Brigade. He fought in the Six-Day War, Yom Kippur War, 1982 Lebanon War, and Operation Entebbe with the paratroopers and Sayeret Matkal, an elite special forces unit. In 1998 he became the sixteenth IDF's Chief of the General Staff, serving until 2002. He is of Iranian Jewish ancestry.

After leaving the army, he entered politics. He was appointed Minister of Defense in 2002, holding the position until 2006 when he was elected to the Knesset on the Kadima list. He then served as Deputy Prime Minister and Minister of Transportation and Road Safety until 2009. After becoming Kadima leader in March 2012 he became Leader of the Opposition, before returning to the cabinet during a 70-day spell in which he served as Acting Prime Minister, Vice Prime Minister and Minister without Portfolio. Kadima was reduced to just two seats in the 2013 elections, and Mofaz retired from politics shortly before the 2015 elections.

Shaul Mofaz was born Shahrām Mofazzazkār () on 4 November 1948 in Tehran, to Persian Jewish parents from Isfahan. Mofaz immigrated to Israel with his parents in 1957. Upon graduating from high school in 1966, he joined the Israel Defense Forces and served in the Paratroopers Brigade. He served in the Six-Day War, Yom Kippur War, 1982 Lebanon War, and Operation Entebbe with the paratroopers and Sayeret Matkal, an elite special forces unit.
Mofaz was then appointed an infantry brigade commander for the 1982 Lebanon War. Afterwards he attended the US Marine Corps Command and Staff College in Quantico, Virginia, United States. On his return he was briefly appointed commander of the Officers School, before returning to active service as commander of the Paratroop Brigade in 1986.

Mofaz served in a series of senior military posts, having been promoted to the rank of Brigadier General (1988). In 1993 he was made commander of the IDF forces in the West Bank. In 1994, he was promoted to Major General, commanding the Southern Corps. His rapid rise continued; in 1997 Mofaz was appointed Deputy Chief of the General Staff and in 1998 he was appointed Chief of the General Staff.

His term of Chief of Staff was noted for financial and structural reforms of the Israeli Army. But the most significant event in his tenure was the eruption of the Second Intifada in September, 2000. The tough tactics undertaken by Mofaz drew widespread concern from the international community but were broadly supported by the Israeli public. Controversy erupted over the offensive in Jenin, intermittent raids in the Gaza Strip, and the continued isolation of Yasser Arafat.

Mofaz foresaw the wave of violence coming early as 1999 and prepared the IDF for intense guerrilla warfare in the territories. He fortified posts at the Gaza Strip and kept Israel Defense Forces casualties low. While he was known for claiming, "Israel has the most moral army in the world," he drew criticism from both Israeli and international human rights monitoring groups because of the methods he had undertaken, including using armored bulldozers to demolish 2,500 Palestinian civilian homes, displacing thousands, in order to create a security "buffer zone" along the Rafah border.

Following a government crisis in 2002, Shaul Mofaz was appointed Defense Minister by Ariel Sharon. Although he supported an agreement with the Palestinians, he was willing to make no compromise in the war against militant groups such as Hamas, Islamic Jihad, Tanzim, and Al-Aqsa Martyrs Brigades.

The fact that he had only recently left his position as IDF Chief of Staff prevented him from participating in the 2003 election (by which time Mofaz had joined Sharon's Likud). Nevertheless, Sharon reappointed him as Defense Minister in the new government.

On 21 November 2005, Mofaz rejected Sharon's invitation to join his new party, Kadima, and instead announced his candidacy for the leadership of Likud. But, on 11 December 2005, one day after he promised he would never leave the Likud, he withdrew from both the leadership race and the Likud to join Kadima.

Following the elections in late March 2006, Mofaz was moved from the position of Defense Minister and received the Transport ministry in the new Cabinet installed on 4 May 2006.

In 2008, with Israel's then prime minister, Ehud Olmert, being pressured to resign due to corruption charges, Mofaz announced that he would run for the leadership of the Kadima party.

On 5 August 2008, Mofaz officially entered the race to be leader of Kadima. That same day he received a blessing by Shas spiritual leader Rabbi Ovadia Yosef. On 17 September 2008, he lost the Kadima party election, losing to Tzipi Livni for the spot of the Prime Minister and leader of Kadima. Livni's narrow margin of 431 votes was 43.1% to Shaul Mofaz's 42.0%, a huge difference from the 10 to 12-point exit polls margins. She said the "national responsibility (bestowed) by the public brings me to approach this job with great reverence". Mofaz accepted the Kadima primary's result, despite his lawyer, Yehuda Weinstein's appeal advice, and telephoned Livni congratulating her. Livni got 16,936 votes, with 16,505 votes, for Mofaz. Public Security Minister Avi Dichter and Interior Minister Meir Sheetrit had 6.5% and 8.5% respectively.

Placed second on the Kadima list, Mofaz retained his seat in the 2009 elections, but lost his cabinet position after Likud formed the government.

On 27 March 2012, Shaul Mofaz won the Kadima party leadership primaries by a landslide, defeating party chairwoman Tzipi Livni. Mofaz became Vice Prime Minister as part of a deal reached for a government of national unity with Binyamin Netanyahu. Mofaz said during the Kadima primaries that he would not join a government led by Netanyahu.

Mofaz left over Netanyahu's indecision over a draft reform law and warned that the prime minister was trying to patch together a majority for a vote to plunge the region into war.

In 2013 Kadima, just 4 years prior the ruling party, received 2% of the votes, barely passing to the Knesset.

In the buildup to the 2015 elections Kadima was not expect to pass the threshold, as it was raised to 3.25%. Mofaz negotiated with the Zionist Union alliance to bring Kadima onto their slate, but ended negotiations when it became clear he would not be their candidate for Defense Minister. Immediately after Mofaz announced he was not joining the Zionist Union slate, it was announced the former Military Intelligence Directorate (Israel) head Amos Yadlin was appointed to the Zionist Union slate and would be their candidate for Defense Minister. Within a week of his announcement that he was not running with the Zionist Union, Mofaz announced his retirement from politics.

A fictionalized version of Mofaz appeared in the 2008 drama film "Lemon Tree".



</doc>
<doc id="29452" url="https://en.wikipedia.org/wiki?curid=29452" title="Stasi">
Stasi

The Ministry for State Security (, MfS) or State Security Service ("Staatssicherheitsdienst", SSD), commonly known as the Stasi (), was the official state security service of the German Democratic Republic (East Germany). It has been described as one of the most effective and repressive intelligence and secret police agencies to have ever existed. The Stasi was headquartered in East Berlin, with an extensive complex in Berlin-Lichtenberg and several smaller facilities throughout the city. The Stasi motto was ""Schild und Schwert der Partei"" (Shield and Sword of the Party), referring to the ruling Socialist Unity Party of Germany (German: "Sozialistische Einheitspartei Deutschlands", SED) and also echoing a theme of the KGB, the Soviet counterpart and close partner, with respect to its own ruling party, the CPSU. Erich Mielke was the Stasi's longest-serving chief, in power for thirty-two of the GDR's forty years of existence.

One of its main tasks was spying on the population, mainly through a vast network of citizens turned informants, and fighting any opposition by overt and covert measures, including hidden psychological destruction of dissidents ("Zersetzung", literally meaning decomposition). Its "Main Directorate for Reconnaissance" (German: "Hauptverwaltung Aufklärung") was responsible for both espionage and for conducting covert operations in foreign countries. Under its long-time head Markus Wolf, this directorate gained a reputation as one of the most effective intelligence agencies of the Cold War.

Numerous Stasi officials were prosecuted for their crimes after 1990. After German reunification, the surveillance files that the Stasi had maintained on millions of East Germans were laid open, so that any citizen could inspect their personal file on request; these files are now maintained by the Federal Commissioner for the Stasi Records.

The Stasi was founded on 8 February 1950. Wilhelm Zaisser was the first Minister of State Security of the GDR, and Erich Mielke was his deputy. Zaisser tried to depose SED General Secretary Walter Ulbricht after the June 1953 uprising, but was instead removed by Ulbricht and replaced with Ernst Wollweber thereafter. Wollweber resigned in 1957 after clashes with Ulbricht and Erich Honecker, and was succeeded by his deputy, Erich Mielke.

In 1957, Markus Wolf became head of the Hauptverwaltung Aufklärung (HVA) (Main Reconnaissance Administration), the foreign intelligence section of the Stasi. As intelligence chief, Wolf achieved great success in penetrating the government, political and business circles of West Germany with spies. The most influential case was that of Günter Guillaume, which led to the downfall of West German Chancellor Willy Brandt in May 1974. In 1986, Wolf retired and was succeeded by Werner Grossmann.

Although Mielke's Stasi was superficially granted independence in 1957, until 1990 the KGB continued to maintain liaison officers in all eight main Stasi directorates, each with his own office inside the Stasi's Berlin compound, and in each of the fifteen Stasi district headquarters around East Germany. Collaboration was so close that the KGB invited the Stasi to establish operational bases in Moscow and Leningrad to monitor visiting East German tourists and Mielke referred to the Stasi officers as "Chekists of the Soviet Union". In 1978, Mielke formally granted KGB officers in East Germany the same rights and powers that they enjoyed in the Soviet Union.

The Ministry for State Security also included the following entities:

Between 1950 and 1989, the Stasi employed a total of 274,000 people in an effort to root out the class enemy. In 1989, the Stasi employed 91,015 people full-time, including 2,000 fully employed unofficial collaborators, 13,073 soldiers and 2,232 officers of GDR army, along with 173,081 unofficial informants inside GDR and 1,553 informants in West Germany.

Regular commissioned Stasi officers were recruited from conscripts who had been honourably discharged from their 18 months' compulsory military service, had been members of the SED, had had a high level of participation in the Party's youth wing's activities and had been Stasi informers during their service in the Military. The candidates would then have to be recommended by their military unit political officers and Stasi agents, the local chiefs of the District (Bezirk) Stasi and Volkspolizei office, of the district in which they were permanently resident, and the District Secretary of the SED. These candidates were then made to sit through several tests and exams, which identified their intellectual capacity to be an officer, and their political reliability. University graduates who had completed their military service did not need to take these tests and exams. They then attended a two-year officer training programme at the Stasi college ("Hochschule") in Potsdam. Less mentally and academically endowed candidates were made ordinary technicians and attended a one-year technology-intensive course for non-commissioned officers.

By 1995, some 174,000 "inoffizielle Mitarbeiter" (IMs) Stasi informants had been identified, almost 2.5% of East Germany's population between the ages of 18 and 60. 10,000 IMs were under 18 years of age. From the volume of material destroyed in the final days of the regime, the office of the Federal Commissioner for the Stasi Records (BStU) believes that there could have been as many as 500,000 informers. A former Stasi colonel who served in the counterintelligence directorate estimated that the figure could be as high as 2 million if occasional informants were included. There is significant debate about how many IMs were actually employed.

Full-time officers were posted to all major industrial plants (the extensiveness of any surveillance largely depended on how valuable a product was to the economy) and one tenant in every apartment building was designated as a watchdog reporting to an area representative of the Volkspolizei (Vopo). Spies reported every relative or friend who stayed the night at another's apartment. Tiny holes were drilled in apartment and hotel room walls through which Stasi agents filmed citizens with special video cameras. Schools, universities, and hospitals were extensively infiltrated.

The Stasi had formal categorizations of each type of informant, and had official guidelines on how to extract information from, and control, those with whom they came into contact. The roles of informants ranged from those already in some way involved in state security (such as the police and the armed services) to those in the dissident movements (such as in the arts and the Protestant Church). Information gathered about the latter groups was frequently used to divide or discredit members. Informants were made to feel important, given material or social incentives, and were imbued with a sense of adventure, and only around 7.7%, according to official figures, were coerced into cooperating. A significant proportion of those informing were members of the SED; to employ some form of blackmail, however, was not uncommon. A large number of Stasi informants were tram conductors, janitors, doctors, nurses and teachers; Mielke believed that the best informants were those whose jobs entailed frequent contact with the public.

The Stasi's ranks swelled considerably after Eastern Bloc countries signed the 1975 Helsinki accords, which GDR leader Erich Honecker viewed as a grave threat to his regime because they contained language binding signatories to respect "human and basic rights, including freedom of thought, conscience, religion, and conviction". The number of IMs peaked at around 180,000 in that year, having slowly risen from 20,000–30,000 in the early 1950s, and reaching 100,000 for the first time in 1968, in response to "Ostpolitik" and protests worldwide. The Stasi also acted as a proxy for KGB to conduct activities in other Eastern Bloc countries, such as Poland, where the Soviets were despised.

The Stasi infiltrated almost every aspect of GDR life. In the mid-1980s, a network of IMs began growing in both German states; by the time that East Germany collapsed in 1989, the Stasi employed 91,015 employees and 173,081 informants. About one out of every 63 East Germans collaborated with the Stasi. By at least one estimate, the Stasi maintained greater surveillance over its own people than any secret police force in history. The Stasi employed one full-time agent for every 166 East Germans. The ratios swelled when informers were factored in: counting part-time informers, the Stasi had one informer per 6.5 people. By comparison, the Gestapo employed one secret policeman per 2,000 people. This comparison led Nazi hunter Simon Wiesenthal to call the Stasi even more oppressive than the Gestapo. Stasi agents infiltrated and undermined West Germany's government and spy agencies.

In some cases, spouses even spied on each other. A high-profile example of this was peace activist Vera Lengsfeld, whose husband, Knud Wollenberger, was a Stasi informant.

The Stasi perfected the technique of psychological harassment of perceived enemies known as "Zersetzung" () – a term borrowed from chemistry which literally means "decomposition". 
By the 1970s, the Stasi had decided that the methods of overt persecution that had been employed up to that time, such as arrest and torture, were too crude and obvious. It was realised that psychological harassment was far less likely to be recognised for what it was, so its victims, and their supporters, were less likely to be provoked into active resistance, given that they would often not be aware of the source of their problems, or even its exact nature. "Zersetzung" was designed to side-track and "switch off" perceived enemies so that they would lose the will to continue any "inappropriate" activities.

Tactics employed under "Zersetzung" generally involved the disruption of the victim's private or family life. This often included psychological attacks, such as breaking into homes and subtly manipulating the contents, in a form of gaslighting – moving furniture, altering the timing of an alarm, removing pictures from walls or replacing one variety of tea with another. Other practices included property damage, sabotage of cars, purposely incorrect medical treatment, smear campaigns including sending falsified compromising photos or documents to the victim's family, denunciation, provocation, psychological warfare, psychological subversion, wiretapping, bugging, mysterious phone calls or unnecessary deliveries, even including sending a vibrator to a target's wife. Usually, victims had no idea that the Stasi were responsible. Many thought that they were losing their minds, and mental breakdowns and suicide could result.

One great advantage of the harassment perpetrated under "Zersetzung" was that its subtle nature meant that it was able to be plausibly denied. This was important given that the GDR was trying to improve its international standing during the 1970s and 80s, especially in conjunction with the "Ostpolitik" of West German Chancellor Willy Brandt massively improving relations between the two German states.

Other files (the Rosenholz Files), which contained the names of East German spies abroad, led American spy agencies to capture them. After German reunification, revelations of Stasi's international activities were publicized, such as its military training of the West German Red Army Faction.

Directorate X was responsible for disinformation. Rolf Wagenbreth, director of disinformation operations, stated "Our friends in Moscow call it 'dezinformatsiya'. Our enemies in America call it 'active measures', and I, dear friends, call it ‘my favorite pastime'".

Recruitment of informants became increasingly difficult towards the end of the GDR's existence, and, after 1986, there was a negative turnover rate of IMs. This had a significant impact on the Stasi's ability to survey the population, in a period of growing unrest, and knowledge of the Stasi's activities became more widespread. Stasi had been tasked during this period with preventing the country's economic difficulties becoming a political problem, through suppression of the very worst problems the state faced, but it failed to do so.

Stasi officers reportedly had discussed re-branding East Germany as a democratic capitalist country to the West, but which in practice would have been taken over by Stasi officers. The plan specified 2,587 OibE officers ("Offiziere im besonderen Einsatz", "officers on special assignment") who would have assumed power as detailed in the Top Secret Document 0008-6/86 of 17 March 1986. According to Ion Mihai Pacepa, the chief intelligence officer in communist Romania, other communist intelligence services had similar plans. On 12 March 1990, "Der Spiegel" reported that the Stasi was indeed attempting to implement 0008-6/86. Pacepa has noted that what happened in Russia and how KGB Colonel Vladimir Putin took over Russia resembles these plans. See Putinism.

On 7 November 1989, in response to the rapidly changing political and social situation in the GDR in late 1989, Erich Mielke resigned. On 17 November 1989, the Council of Ministers "(Ministerrat der DDR)" renamed the Stasi as the "Office for National Security" "(Amt für Nationale Sicherheit" – AfNS), which was headed by "Generalleutnant" Wolfgang Schwanitz. On 8 December 1989, GDR Prime Minister Hans Modrow directed the dissolution of the AfNS, which was confirmed by a decision of the "Ministerrat" on 14 December 1989.

As part of this decision, the "Ministerrat" originally called for the evolution of the AfNS into two separate organizations: a new foreign intelligence service "(Nachrichtendienst der DDR)" and an "Office for the Protection of the Constitution of the GDR" "(Verfassungsschutz der DDR)", along the lines of the West German "Bundesamt für Verfassungsschutz", however, the public reaction was extremely negative, and under pressure from the "Round Table" "(Runder Tisch)", the government dropped the creation of the "Verfassungsschutz der DDR" and directed the immediate dissolution of the AfNS on 13 January 1990. Certain functions of the AfNS reasonably related to law enforcement were handed over to the GDR Ministry of Internal Affairs. The same ministry also took guardianship of remaining AfNS facilities.

When the parliament of Germany investigated public funds that disappeared after the Fall of the Berlin Wall, it found out that East Germany had transferred large amounts of money to Martin Schlaff through accounts in Vaduz, the capital of Liechtenstein, in return for goods "under Western embargo".

Moreover, high-ranking Stasi officers continued their post-GDR careers in management positions in Schlaff's group of companies. For example, in 1990, Herbert Kohler, Stasi commander in Dresden, transferred 170 million marks to Schlaff for "harddisks" and months later went to work for him.
The investigations concluded that "Schlaff's empire of companies played a crucial role" in the Stasi attempts to secure the financial future of Stasi agents and keep the intelligence network alive.
The "Stern" magazine noted that KGB officer Vladimir Putin worked with his Stasi colleagues in Dresden in 1989.

During the Peaceful Revolution of 1989, Stasi offices were overrun by angry citizens, but not before the Stasi destroyed a number of documents (approximately 5%) consisting of, by one calculation, 1 billion sheets of paper.

With the fall of the German Democratic Republic the Stasi was dissolved. Stasi employees began to destroy the extensive files and documents they held, by hand, fire and with the use of shredders. When these activities became known, a protest began in front of the Stasi headquarters, The evening of 15 January 1990 saw a large crowd form outside the gates calling for a stop to the destruction of sensitive files. The building contained vast records of personal files, many of which would form important evidence in convicting those who had committed crimes for the Stasi. The protesters continued to grow in number until they were able to overcome the police and gain entry into the complex. Once inside, specific targets of the protesters' anger were portraits of Erich Honecker which were trampled on or burnt. Among the protesters were former Stasi collaborators seeking to destroy incriminating documents. While there were no fatalities, protesters roughed up and threw some Stasi employees out of office windows or into rivers, resulting in some injuries.

With the German Reunification on 3 October 1990, a new government agency was founded called the "Federal Commissioner for the Records of the State Security Service of the former German Democratic Republic" (), officially abbreviated "BStU". There was a debate about what should happen to the files, whether they should be opened to the people or kept closed.

Those who opposed opening the files cited privacy as a reason. They felt that the information in the files would lead to negative feelings about former Stasi members, and, in turn, cause violence. Pastor Rainer Eppelmann, who became Minister of Defense and Disarmament after March 1990, felt that new political freedoms for former Stasi members would be jeopardized by acts of revenge. Prime Minister Lothar de Maizière even went so far as to predict murder. They also argued against the use of the files to capture former Stasi members and prosecute them, arguing that not all former members were criminals and should not be punished solely for being a member. There were also some who believed that everyone was guilty of something. Peter-Michael Diestel, the Minister of Interior, opined that these files could not be used to determine innocence and guilt, claiming that "there were only two types of individuals who were truly innocent in this system, the newborn and the alcoholic". Other opinions, such as the one of West German Interior Minister Wolfgang Schäuble, believed in putting the Stasi behind them and working on German reunification.

Others argued that everyone should have the right to see their own file, and that the files should be opened to investigate former Stasi members and prosecute them, as well as not allow them to hold office. Opening the files would also help clear up some of the rumors that were currently circulating. Some also believed that politicians involved with the Stasi should be investigated.

The fate of the files was finally decided under the Unification Treaty between the GDR and Federal Republic of Germany (FRG). This treaty took the Volkskammer law further and allowed more access and use of the files. Along with the decision to keep the files in a central location in the East, they also decided who could see and use the files, allowing people to see their own files.

In 1992, following a declassification ruling by the German government, the Stasi files were opened, leading people to look for their files. Timothy Garton Ash, an English historian, after reading his file, wrote "The File: A Personal History".

Between 1991 and 2011, around 2.75 million individuals, mostly GDR citizens, requested to see their own files. The ruling also gave people the ability to make duplicates of their documents. Another big issue was how the media could use and benefit from the documents. It was decided that the media could obtain files as long as they were depersonalized and not regarding an individual under the age of 18 or a former Stasi member. This ruling not only gave the media access to the files, but also gave schools access.

Even though groups of this sort were active in the community, those who were tracking down ex-members were, as well. Many of these hunters succeeded in catching ex-Stasi; however, charges could not be made for merely being a member. The person in question would have to have participated in an illegal act, not just be a registered Stasi member. Among the high-profile individuals who were arrested and tried were Erich Mielke, Third Minister of State Security of the GDR, and Erich Honecker, head of state for the GDR. Mielke was sentenced to six years prison for the murder of two policemen in 1931. Honecker was charged with authorizing the killing of would-be escapees on the East-West frontier and the Berlin Wall. During his trial, he went through cancer treatment. Because he was nearing death, Honecker was allowed to spend his final time in freedom. He died in Chile in May 1994.

Some of it is very easy due to the number of archives and the failure of shredding machines (in some cases "shredding" meant tearing paper in two by hand and documents could be recovered easily). In 1995, the BStU began reassembling the shredded documents; 13 years later, the three dozen archivists commissioned to the projects had only reassembled 327 bags; they are now using computer-assisted data recovery to reassemble the remaining 16,000 bagsestimated at 45 million pages. It is estimated that this task may be completed at a cost of 30 million dollars.

The CIA acquired some Stasi records during the looting of the Stasi's archives. The Federal Republic of Germany has asked for their return and received some in April 2000. See also Rosenholz files.

The Anti-Stalinist Action Normannenstraße (ASTAK), an association founded by former GDR Citizens' Committees, has transformed the former headquarters of the Stasi into a museum. It is divided into three floors:
The ground floor has been kept as it used to be. The decor is original, with many statues and flags.

Photo gallery:

Former Stasi agent Matthias Warnig (codename "Arthur") is currently the CEO of Nord Stream.
German investigations have revealed that some of the key Gazprom Germania managers are former Stasi agents.

Former Stasi officers continue to be politically active via the "Gesellschaft zur Rechtlichen und Humanitären Unterstützung e. V." (GRH, Society for Legal and Humanitarian Support). Former high-ranking officers and employees of the Stasi, including the last Stasi director, Wolfgang Schwanitz, make up the majority of the organization's members, and it receives support from the German Communist Party, among others.

Impetus for the establishment of the GRH was provided by the criminal charges filed against the Stasi in the early 1990s. The GRH, decrying the charges as "victor's justice", called for them to be dropped. Today the group provides an alternative if somewhat utopian voice in the public debate on the GDR legacy. It calls for the closure of the museum in Hohenschönhausen and can be a vocal presence at memorial services and public events. In March 2006 in Berlin, GRH members disrupted a museum event; a political scandal ensued when the Berlin Senator (Minister) of Culture refused to confront them.

Behind the scenes, the GRH also lobbies people and institutions promoting opposing viewpoints. For example, in March 2006, the Berlin Senator for Education received a letter from a GRH member and former Stasi officer attacking the Museum for promoting "falsehoods, anticommunist agitation and psychological terror against minors". Similar letters have also been received by schools organizing field trips to the museum.







</doc>
<doc id="29455" url="https://en.wikipedia.org/wiki?curid=29455" title="Sandra Bullock">
Sandra Bullock

Sandra Annette Bullock (; born July 26, 1964) is an American actress, producer, and philanthropist. She made her acting debut with a minor role in the 1987 thriller "Hangmen", and made her television debut in the film "" (1989), and played the lead role in the short-lived NBC sitcom "Working Girl." Her breakthrough role was in the film "Demolition Man" (1993). She subsequently starred in several successful films including "Speed" (1994), "While You Were Sleeping" (1995), "The Net" (1995), "A Time to Kill" (1996), "Hope Floats" (1998), and "Practical Magic" (1998).

Bullock achieved further success in the following decades in "Miss Congeniality" (2000), "Two Weeks Notice" (2002), "Crash" (2004), "The Proposal" (2009), "The Heat" (2013), and "Ocean's 8" (2018). She was awarded the Academy Award for Best Actress and the Golden Globe Award for Best Actress in a Drama for playing Leigh Anne Tuohy in "The Blind Side" (2009), and was nominated in the same categories for her performance in "Gravity" (2013). Bullock's greatest commercial success is the animated comedy film "Minions" (2015), which grossed over US$1 billion at the box office. In 2007, she was one of Hollywood's highest-paid actresses. She was also named "Most Beautiful Woman" by "People" magazine in 2015.

In addition to her acting career, Bullock is the founder of the production company Fortis Films. She has produced some of the films in which she starred, including "Two Weeks Notice", "", and "All About Steve". She was an executive producer of the ABC sitcom "George Lopez", and made several appearances during its run.

Bullock was born in Arlington, Virginia. Her father, John W. Bullock (born 1925), was a United States Army employee and part-time voice coach; her mother, Helga Mathilde Meyer (1942–2000), was an opera singer and voice teacher. Helga was German, while John is from Birmingham, Alabama. Bullock's maternal grandfather was a German rocket scientist from Nuremberg. John, who was in charge of the U.S. Army's Military Postal Service in Europe, was stationed in Nuremberg when he met Helga. They married in Germany and moved to Arlington, where John worked with the Army Materiel Command before becoming a contractor for The Pentagon. Bullock has a younger sister, Gesine Bullock-Prado, who went on to serve as vice president of Bullock's production company Fortis Films. Until the age of 18, Bullock held American-German dual citizenship. She then held only American citizenship until 2009, when she reapplied for German citizenship.

Bullock was raised in Germany (Nuremberg) and Austria (Vienna and Salzburg) for 12 years, and grew up speaking German. She attended the humanistic Waldorf School in Nuremberg. As a child, while her mother went on European opera tours, Bullock usually stayed with her aunt Christl and cousin Susanne, the latter of whom would later marry German politician Peter Ramsauer. Bullock studied ballet and vocal arts as a child and frequently accompanied her mother, taking small parts in her opera productions. She sang in the opera's children's choir at the Staatstheater Nürnberg. Bullock has a scar above her left eye, caused by falling into a creek when she was a child. She attended Washington-Lee High School, where she was a cheerleader and performed in high school theater productions. After graduating in 1982, she attended East Carolina University in Greenville, North Carolina, where she received a BFA in Drama in 1987. While at ECU, she performed in multiple theater productions, including "Peter Pan" and "Three Sisters". She then moved to Manhattan, New York, where she supported herself as a bartender, cocktail waitress, and coat checker while auditioning for roles.

While in New York, Bullock took acting classes with Sanford Meisner. She appeared in several student films, and later landed a role in an Off-Broadway play "No Time Flat". Director Alan J. Levi was impressed by Bullock's performance and offered her a part in the TV movie "" (1989). This led to her being cast in a series of small roles in several independent films as well as in the lead role of the short-lived NBC television version of the film "Working Girl" (1990). She went on to appear in several films, such as "Love Potion No. 9" (1992), "The Thing Called Love" (1993) and "Fire on the Amazon" (1993).

Bullock had a prominent supporting role in the science-fiction/action film "Demolition Man" (1993), followed by a leading role in "Speed" the following year. "Speed" took in $350 million at the box office worldwide.

A string of successes during the mid-1990s included "While You Were Sleeping" (1995), for which she received her first Golden Globe Award nomination for Best Actress – Motion Picture Musical or Comedy, "The Net" (1995) and "A Time to Kill" (1996). Bullock received $11 million for "" (1997), which she agreed to star in for financial backing for her own project, "Hope Floats" (1998). She has stated that she regrets making the sequel.

She was selected as one of "People" magazine's 50 Most Beautiful People in the World in 1996 and 1999, and was also ranked No. 58 in "Empire" magazine's Top 100 Movie Stars of All Time list.

In 2000, Bullock starred in "Miss Congeniality", a financial success that took in $212 million at the box office worldwide, and received another Golden Globe Award nomination for Best Actress – Motion Picture Musical or Comedy. She was presented with the 2002 Raúl Juliá Award for Excellence for her efforts, as the executive producer of the sitcom "George Lopez," in helping expand career openings for Hispanic talent in the media and entertainment industry. She also made several appearances on the show as Accident Amy, an accident-prone employee at the factory Lopez's character manages. The same year, she starred opposite Hugh Grant in "Two Weeks Notice" (2002).

In 2004, Bullock had a supporting role in the film "Crash", which won the Academy Award for Best Picture. She received positive reviews for her performance, with some critics suggesting that it was the best performance of her career. She later received a $17.5 million salary for "" (2005). The same year, she was a co-recipient of the Women in Film Crystal Award.

Although Bullock was reunited with her "Speed" co-star Keanu Reeves in the romantic drama "The Lake House", their film characters are separated throughout the film, so Bullock and Reeves were only on set together for two weeks during filming. The same year, Bullock appeared in "Infamous", playing author Harper Lee. Bullock also starred in "Premonition" with Julian McMahon, which was released in March 2007. In 2008, Bullock was announced as "the face" of the cosmetic brand Artistry.

The year 2009 proved to be especially good for Bullock, giving the actress two record highs in her career, as earlier in the year she released "The Proposal", with co-star Ryan Reynolds, grossed $317 million at the box office worldwide, making it her fourth-most successful picture to date. She received her third Golden Globe Award nomination for Best Actress role for Motion Picture Musical or Comedy.

In November 2009, Bullock starred in "The Blind Side", which opened at No. 2 behind "" with $34.2 million, making it her second-highest opening weekend ever. "The Blind Side" was unusual in that it had a 17.6% increase at the box office its second weekend, and it took the top spot of the box office in its third weekend. The film cost $29 million to make according to the Box Office Mojo. It grossed over $309 million, making it her highest-grossing domestic film, her fourth-highest-grossing film worldwide, and the first one in history to pass the $200 million mark with only one top-billed female star.
Bullock had initially turned down the role of Leigh Anne Tuohy three times due to a discomfort with portraying a devout Christian. She was awarded the Academy Award for Best Actress, Golden Globe Award for Best Actress in a Motion Picture – Drama, Screen Actors Guild Award for Outstanding Performance by a Female Actor in a Leading Role and Critics' Choice Movie Award for Best Actress. "The Blind Side" also received an Academy Award for Best Picture nomination.

Winning the Oscar also gave Bullock another unique distinction—since she won two "Razzies" the day before, for her performance in "All About Steve" (2009), she is the only performer ever to have been named both "Best" and "Worst" for the same year.

In 2011, Bullock starred in the drama "Extremely Loud & Incredibly Close" alongside Tom Hanks, a film adaptation based on the novel of the same name. Despite mixed reviews, the film was nominated for numerous awards, including an Academy Award for Best Picture nomination. Bullock was nominated for Best Actress Drama by Teen Choice Awards.

In 2013, Bullock starred in the comedy film "The Heat", alongside Melissa McCarthy. It received positive reviews from critics, and took in $230 million at the box office worldwide. Bullock also starred in the space thriller film "Gravity", opposite George Clooney. The film premiered at the 70th Venice Film Festival, and was released on October 4, 2013 to coincide with the beginning of World Space Week. "Gravity" received universal acclaim among critics and a standing ovation in Venice. The film was called "the most realistic and beautifully choreographed film ever set in space". Bullock's performance was praised, with some critics calling "Gravity" the best work of her career. "Variety" wrote, 

"Gravity" took in $716 million at the box office worldwide, making it Bullock's second-most successful picture. For her role as Dr. Ryan Stone, Bullock was nominated for the Academy Award, Golden Globe Award, BAFTA Award, Screen Actors Guild Award, and Critics' Choice Movie Award for Best Actress. By August 2014, Bullock was the highest-earning actress in Hollywood. In 2015, she voiced the character of Scarlet Overkill in the animated film "Minions", which became her highest-grossing film to date, with over $1.1 billion worldwide. That same year, Bullock also starred in and co-executive produced the comedy-drama film "Our Brand Is Crisis".

, Bullock's films have grossed over $5.1 billion worldwide, which makes her the 29th-most profitable movie star according to "The Numbers"; her total domestic gross stands at roughly $2.5 billion, placing her as the 21st-top U.S. star at the box office.

Bullock starred in an all-female spin-off of the "Ocean's Eleven" franchise, titled "Ocean's 8" (2018), which was directed by Gary Ross.

Since her acting debut, Bullock has been dubbed "America's sweetheart" in the media due to her "friendly and direct and so unpretentious" nature.

On March 24, 2005, Bullock received a motion pictures star on the Hollywood Walk of Fame at 6801 Hollywood Boulevard in Hollywood.

While critics have praised her screen persona, some have been less receptive towards her films. At the 2009 release of "The Proposal", Mark Kermode said Bullock had made only three good films: "Speed", "While You Were Sleeping", and "Crash", and added that "she's funny, she's gorgeous, it's impossible not to love her, and yet she makes rotten film after rotten film".

Bullock was selected by "People" magazine as its 2010 Woman of the Year and ranked No. 12 on "People"s Most Beautiful 2011 list.
In 2010, "Time" magazine included Bullock in its annual "TIME" 100 as one of the "Most Influential People in the World."

In 2013, "The Hollywood Reporter" named Bullock among the most powerful women in entertainment. In September 2013, Bullock joined other Hollywood legends at the TCL Chinese Theatre on Hollywood Boulevard by making imprints of her hand- and footprints in cement in the theater's forecourt.

In November 2013 it was announced that Bullock was named "Entertainment Weekly"s Entertainer of the Year due to her success with "The Heat" and "Gravity", which "Entertainment Weekly" believed would earn her an Oscar nomination. Bullock shared the title with other distinguished people in the industry such as the creators of the television show "Breaking Bad", Matthew McConaughey, Jennifer Lawrence, "Grumpy Cat" and others.

In 2014, Bullock ranked No. 2 on "Forbes" list of most powerful actresses and was honored with the Decade of Hotness Award by Spike Guys' Choice Awards.
In 2015, she was named "The Most Beautiful Woman" by "People."

Bullock owns a production company, Fortis Films. She was an executive producer of the "George Lopez" sitcom (co-produced with Robert Borden), which garnered a syndication deal that banked her some $10 million. Bullock tried to produce a film based on F.X. Toole's short story "Million-Dollar Baby", but could not interest the studios in a female boxing drama. The story was eventually adapted and directed by Clint Eastwood as the Oscar-winning film, "Million Dollar Baby" (2004). Fortis Films also produced "All About Steve" which was released in September 2009. Her father, John Bullock, is the company's CEO, and her sister, Gesine Bullock-Prado, is the former president.

In November 2006, Bullock founded an Austin, Texas restaurant, Bess Bistro, located on West 6th Street. She later opened another business, Walton's Fancy and Staple, across the street in a building she extensively renovated. Walton's is a bakery, upscale restaurant and floral shop that also offers services including event planning. After almost nine years in business, Bess Bistro closed on September 20, 2015.

Bullock has been a public supporter of the American Red Cross, having donated $1 million to the organization at least five times. Her first public donation of that amount was to the Red Cross's Liberty Disaster Relief Fund. Three years later, she sent money in response to the 2004 Indian Ocean earthquake and tsunamis. In 2010, she donated $1 million to relief efforts in Haiti following the Haiti earthquake, and again donated the same amount following the 2011 Tōhoku earthquake and tsunami. In 2017, she donated $1 million to support Red Cross relief efforts for Hurricane Harvey in Texas.

Along with other stars, Bullock did a public service announcement urging people to sign a petition for clean-up efforts of the oil spill in the Gulf of Mexico. Bullock backs the Texas non-profit organization The Kindred Life Foundation, Inc., and in late 2008 joined other top celebrities in supporting the work of KLF's founder and CEO Amos Ramirez. At a fundraising gala for the organization, Bullock said, "Amos has led many efforts across our nation that have helped families that are in need. Our country needs more organizations that are committed to the service that Kindred Life is."

In 2012, Bullock was inducted into the Warren Easton Hall of Fame for her donations to charities, and in 2013 was honored with the Favorite Humanitarian Award at the 39th People's Choice Awards in 2013 for her contributions to New Orleans' Warren Easton High School, which was severely damaged by 2005's Hurricane Katrina.

Bullock was once engaged to actor Tate Donovan, whom she met while filming "Love Potion No. 9". Their relationship lasted three years. She previously dated football player Troy Aikman, and actors Matthew McConaughey and Ryan Gosling.

Bullock married motorcycle builder and "Monster Garage" host Jesse James on July 16, 2005. They first met when Bullock arranged for her ten-year-old godson to meet James as a Christmas present. In November 2009, Bullock and James entered into a custody battle with James' second ex-wife, former adult film actress Janine Lindemulder, with whom James had a child. Bullock and James subsequently won full legal custody of James' five-year-old daughter.

In March 2010, a scandal arose when several women claimed to have had affairs with James during his marriage to Bullock. Bullock canceled European promotional appearances for "The Blind Side" citing "unforeseen personal reasons." On March 18, 2010, James responded to the rumors of infidelity by issuing a public apology to Bullock. He stated, "The vast majority of the allegations reported are untrue and unfounded … beyond that, I will not dignify these private matters with any further public comment." James declared, "There is only one person to blame for this whole situation, and that is me." He asked that Bullock and their children one day "find it in their hearts to forgive me" for their "pain and embarrassment." James' publicist subsequently announced on March 30, 2010 that James had checked into a rehabilitation facility to "deal with personal issues" and save his marriage to Bullock. However, on April 28, 2010, it was reported that Bullock had filed for divorce on April 23 in Austin, Texas. Their divorce was finalized on June 28, 2010, with "conflict of personalities" cited as the reason.

Bullock announced on April 28, 2010 that she had proceeded with plans to adopt a son born in January 2010 in New Orleans, Louisiana. Bullock and James had begun an initial adoption process four months earlier. Bullock's son began living with them in January 2010, but they chose to keep the news private until after the Oscars in March 2010. However, given the couple's separation and then divorce, Bullock continued the adoption of her son as a single parent.

In December 2015, Bullock announced that she had adopted a second child, and appeared on the cover of "People" magazine with her then -year-old new daughter.

On December 20, 2000, Bullock, who was the only passenger, and the two crew all escaped uninjured in the crash of a chartered business jet. The pilots were unable to activate the runway lights during a night landing at Jackson Hole Airport due to not using up-to-date approach plates, but continued the landing anyway. The aircraft landed in the airport's graded safety area between the runway and parallel taxiway and hit a snowbank. The accident caused a separation of the nose cone and landing gear, partial separation of the right wing, and a bend in the left wing.

On April 18, 2008, while Bullock was in Massachusetts shooting the film "The Proposal", she and her then-husband Jesse James were in an SUV that was hit head-on (driver's side offset) at moderate speed by a drunken driver. Vehicle damage was minor and there were no injuries.

In October 2004, Bullock won a multimillion-dollar judgment against Benny Daneshjou, the builder of her Lake Austin, Texas home; the jury ruled that the house was uninhabitable. It has since been torn down and rebuilt. Daneshjou and his insurer later settled with Bullock for roughly half the awarded verdict.

On April 22, 2007, a woman named Marcia Diana Valentine was found lying outside James and Bullock's home in Orange County, California. When James confronted the woman, she ran to her car, got behind the wheel, and tried to run over him. She was said to be an obsessed fan of Bullock. Valentine was charged with one felony count each of aggravated assault and stalking. Bullock obtained a restraining order to bar Valentine from "contacting or coming near her home, family or work for three years". Valentine pleaded not guilty to charges of aggravated assault and stalking. She was subsequently convicted of stalking and sentenced to three years' probation.

Beginning in 2002, Bullock was also stalked across several states by a man named Thomas James Weldon. In 2003, Bullock obtained a restraining order against him, which was renewed in 2006. After the restraining order expired and Weldon was released from a mental institution, he again traveled across several states to find Bullock; she then obtained another restraining order.

In June 2014, Joshua James Corbett broke into Bullock's Los Angeles home. Bullock locked herself in a room and dialed 911. Corbett pleaded no contest in 2017 and was sentenced to five years' probation for stalking Bullock and breaking into her residence. He was then subject to a 10-year protective order requiring him to stay away from Bullock. On May 2, 2018, after Corbett missed a court date the previous month, police officers went to his parents' residence where he lived in a guest house to arrest him. When he refused to leave and threatened to shoot officers, a SWAT team was called. After a five-hour standoff, the SWAT team deployed gas canisters and entered the house, where they found Corbett had committed suicide. Corbett's cause of death was "multiple incised wounds" according to the Los Angeles County coroner.




</doc>
<doc id="29458" url="https://en.wikipedia.org/wiki?curid=29458" title="Smallfilms">
Smallfilms

Smallfilms is a British television producution company that made animated TV programmes for children from 1959 until the 1980s. In 2014 the company began operating again, producing a new series of its most famous show, "The Clangers". It was originally a partnership between Oliver Postgate (writer, animator and narrator) and Peter Firmin (modelmaker and illustrator). Several very popular series of short films were made using stop-motion animation, including "Clangers", "Noggin the Nog" and "Ivor the Engine". Another Smallfilms production, "Bagpuss", came top of a BBC poll to find the favourite British children's programme of the 20th century.

In 1957, Postgate was appointed a stage manager with Associated-Rediffusion, the company that then held the commercial weekday television franchise for London. Attached to the children's programming section, he thought he could do better with the relatively low budgets of the then black and white television productions.

He wrote "Alexander the Mouse", a story about a mouse born to be king. Using an Irish-produced magnetic system—on which animated characters were magnetically attached to a painted background, then filmed using a 45 degree mirror—he persuaded Peter Firmin, who was then teaching at the Central School of Art, to create the painted backgrounds. Postgate later recalled that they broadcast around 26 of these programmes live-to-air, a task made harder by the production problems encountered by the use and restrictions of using magnets.

After the relative success of "Alexander the Mouse", Postgate agreed a deal to make his next series on film, for a budget of £175 per programme (a minuscule amount even at that time). Making a stop motion animation table in his bedroom, he wrote the Chinese serial "The Journey of Master Ho": a formal Chinese epic, about a small boy and a water-buffalo. This was intended for deaf children, a distinct advantage in that the production required no soundtrack, which reduced production costs. He engaged a painter to produce the backgrounds, but as the painter was classically Chinese-trained he produced them in three-quarter view, rather than in the conventional Egyptian full-view manner needed for flat animation under a camera. This resulted in the Firmin-produced characters looking as if they were short in one leg, but the success of the production provided the foundation for Postgate and Firmin to start up their own company, solely producing animated children's television programmes, initially for ITV, but soon afterward with the BBC.

Postgate's initial BBC career was not solely concerned with Smallfilms. To gain experience, he accepted a contract as a television director in the BBC Children's Department in 1960, on a show entitled "Little Laura", another animated series made on film, written and drawn by V. H. Drummond. The series continued in production until 1962, with Postgate credited also as animator on the 1962 series. He also wrote serials for long-running BBC children's programmes "Blue Peter" and stories for "Vision On".

Setting up the business in a disused cowshed at Firmin's home in Blean near Canterbury, Kent, Postgate and Firmin made children's animation programmes, based on concepts that mostly originated from Postgate. Firmin did the artwork and built the models, whilst Postgate wrote the scripts, did the stop motion filming, and voiced many of the characters. "Smallfilms" was able to produce two minutes of film per day, ten times as much as a conventional animation studio, with Postgate moving the (originally cardboard) characters himself, and working his 16mm camera frame-by-frame with a home-made clicker. As Postgate voiced so many of the productions, including the WereBear story tapes, his distinctive voice became familiar to generations of children.

They began in 1959 with "Ivor the Engine", a series for ITV about a Welsh steam locomotive who wanted to sing in a choir, based on Postgate's wartime encounter with Welshman Denzyl Ellis, who was once a fireman on the Royal Scot. It was remade in colour for the BBC in the 1970s. This was followed, also in 1959, by "Noggin the Nog", their first production for the BBC, which established Smallfilms as a safe and reliable pair of hands to produce children's entertainment, in the days when the number of UK television channels was restricted to two.

In 2000, Postgate and his friend Loaf set up a small publishing company called The Dragons Friendly Society, to look after "Noggin the Nog", "Pogles' Wood" and "Pingwings".

After Postgate's death in December 2008, Smallfilms was inherited by his son Daniel. Universal took the distribution rights to the works of Smallfilms. Any such agreement does not include the materials published through The Dragons Friendly Society.

In 2014, Postgate's son, Daniel Postgate, collaborated with Peter Firmin on the production of a new series of "Clangers", with Daniel writing many of the episodes.

Postgate and Firmin recognised that their product was not sold to children, but to commissioning television executives. Postgate described in a later interview the then "gentlemanly and rather innocent" business of programme commissioning thus: "We would go to the BBC once a year, show them the films we'd made, and they would say: 'Yes, lovely, now what are you going to do next?' We would tell them, and they would say: 'That sounds fine, we'll mark it in for eighteen months from now,' and we would be given praise and encouragement and some money in advance, and we'd just go away and do it." The only occasion that this informal arrangement caused any real difficulty emerged in the 1965 series "The Pogles", which BBC management felt was too frightening for the intended audience, and led to them asking for a change of direction: resulting in a revised show, and a change of name to "Pogles' Wood".

Postgate had strict views regarding storylines, which perhaps limited the possibilities for series development. When asked if the "Clangers" adventures were quite surreal sometimes, Postgate replied: "They're surreal but logical. I have a strong prejudice against fantasy for its own sake. Once one gets to a point beyond where cause-and-effect mean anything at all, then science fiction becomes science nonsense. Everything that happened was strictly logical, according to the laws of physics which happened to apply in that part of the world."

In June 2015, the BBC's Mark Savage reported: "Firmin said the "Clangers" surrealism had led to accusations that Postgate was taking hallucinogenic drugs". Firmin told Savage: "People used to say, 'Ooh, what's Oliver on, with all of these weird ideas?' And we used to say, 'He's on cups of tea and biscuits.'"

The Smallfilms system was reliant on the company's two key employees, Postgate and Firmin, and was devoid of modern considerations and essentials, as Postgate pointed out: "[We were] excused the interference of educationalists, sociologists and other pseudo-scientists, which produces eventually a confection of formulae which have no integrity. No, the mainspring of what we did was because it was fun."

Recognising their commissioning audience, Smallfilms purposefully developed storylines that would engage both adults and children. While the storylines and production were remembered by children, the adult jokes, like those about the Welsh in "Ivor the Engine", or the fact that the Clangers swore occasionally, gave the films an instant parental engagement, and a later nostalgic revival amongst former children re-watching their favourite programmes.

From October 2008 until 2013, production company Coolabi held the merchandising and distribution rights to a number of the Smallfilms productions. Coolabi hoped to introduce "Bagpuss" to a new generation, saying that there was "significant potential to build on the affection in which this classic brand is held".

However, in the event it was Smallfilms itself that returned the classic shows to production, agreeing a deal with the BBC in 2014 to produce a further 52 episodes of "Clangers", as a third series of that show for broadcasting in 2015, which the company also pre-sold in the United States.




</doc>
<doc id="29460" url="https://en.wikipedia.org/wiki?curid=29460" title="List of mayors of Sacramento, California">
List of mayors of Sacramento, California

This is a list of mayors of Sacramento, California. The Sacramento City Council met for the first time on August 1, 1849 and the citizens approved the city charter on October 13, 1849. The City Charter was recognized by the State of California on February 27, 1850 and Sacramento was incorporated on March 18, 1850.

"See also :" Lists of incumbents


http://ohp.parks.ca.gov/?page_id=21454

</doc>
<doc id="29462" url="https://en.wikipedia.org/wiki?curid=29462" title="Sabotage">
Sabotage

Sabotage is a deliberate action aimed at weakening a polity, effort or organization through subversion, obstruction, disruption or destruction. One who engages in sabotage is a "saboteur". Saboteurs typically try to conceal their identities because of the consequences of their actions.

Any unexplained adverse condition might be sabotage. Sabotage is sometimes called tampering, meddling, tinkering, malicious pranks, malicious hacking, a practical joke or the like to avoid needing to invoke legal and organizational requirements for addressing sabotage.

It is said that less wealthy workers in France, who wore not leather but wooden shoes, used to throw these "sabots" in the machines to subvert production, but this is not supported by the etymology. Rather, the French source word literally means to "walk noisily," and wearing wooden shoes is an example of walking noisily. Originally this was used metaphorically to refer to labor disputes, not damage.

One of its first appearances in French literature is in the "Dictionnaire du Bas-Langage ou manières de parler usitées parmi le peuple" of D'Hautel, edited in 1808.

The verb "saboter" is also found in 1873–1874 in the "Dictionnaire de la langue française" of Émile Littré. But it is at the end of the 19th century that it really began to be used with the meaning of "deliberately and maliciously destroying property" or "working slower". In 1897, Émile Pouget, a famous syndicalist and anarchist wrote "action de saboter un travail" (action of sabotaging a work) in "Le Père Peinard" and in 1911 he also wrote a book entitled "Le Sabotage".

At the inception of the Industrial Revolution, skilled workers such as the Luddites (1811–1812) used sabotage as a means of negotiation in labor disputes.

Labor unions such as the Industrial Workers of the World (IWW) have advocated sabotage as a means of self-defense and direct action against unfair working conditions.

The IWW was shaped in part by the industrial unionism philosophy of Big Bill Haywood, and in 1910 Haywood was exposed to sabotage while touring Europe:

The experience that had the most lasting impact on Haywood was witnessing a general strike on the French railroads. Tired of waiting for parliament to act on their demands, railroad workers walked off their jobs all across the country. The French government responded by drafting the strikers into the army and then ordering them back to work. Undaunted, the workers carried their strike to the job. Suddenly, they could not seem to do anything right. Perishables sat for weeks, sidetracked and forgotten. Freight bound for Paris was misdirected to Lyon or Marseille instead. This tactic — the French called it "sabotage" — won the strikers their demands and impressed Bill Haywood.

For the IWW, sabotage came to mean any withdrawal of efficiency, including the slowdown, the strike, working to rule, or creative bungling of job assignments.

One of the most severe examples was at the construction site of the Robert-Bourassa Generating Station in 1974, in Québec, Canada, when workers used bulldozers to topple electric generators, damaged fuel tanks, and set buildings on fire. The project was delayed a year, and the direct cost of the damage estimated at $2 million CAD. The causes were not clear, but three possible factors have been cited: inter-union rivalry, poor working conditions, and the perceived arrogance of American executives of the contractor, Bechtel Corporation.

Certain groups turn to destruction of property to stop environmental destruction or to make visible arguments against forms of modern technology they consider detrimental to the environment. The U.S. Federal Bureau of Investigation (FBI) and other law enforcement agencies use the term eco-terrorist when applied to damage of property. Proponents argue that since property cannot feel terror, damage to property is more accurately described as sabotage. Opponents, by contrast, point out that property owners and operators can indeed feel terror. The image of the monkey wrench thrown into the moving parts of a machine to stop it from working was popularized by Edward Abbey in the novel "The Monkey Wrench Gang" and has been adopted by eco-activists to describe destruction of earth damaging machinery.

From 1992 to late 2007 a radical environmental activist movement known as ELF or Earth Liberation Front engaged in a near constant campaign of decentralized sabotage of any construction projects near wild lands and extractive industries such as logging and even the burning down of a ski resort of Vail Colorado. ELF used sabotage tactics often in loose coordination with other environmental activist movements to physically delay or destroy threats to wild lands as the political will developed to protect the targeted wild areas that ELF engaged.

In war, the word is used to describe the activity of an individual or group not associated with the military of the parties at war, such as a foreign agent or an indigenous supporter, in particular when actions result in the destruction or damaging of a productive or vital facility, such as equipment, factories, dams, public services, storage plants or logistic routes. Prime examples of such sabotage are the events of Black Tom and the Kingsland Explosion. Like spies, saboteurs who conduct a military operation in civilian clothes or enemy uniforms behind enemy lines are subject to prosecution and criminal penalties instead of detention as prisoners of war. It is common for a government in power during war or supporters of the war policy to use the term loosely against opponents of the war. Similarly, German nationalists spoke of a stab in the back having cost them the loss of World War I.

A modern form of sabotage is the distribution of software intended to damage specific industrial systems. For example, the U.S. Central Intelligence Agency (CIA) is alleged to have sabotaged a Siberian pipeline during the Cold War, using information from the Farewell Dossier. A more recent case may be the Stuxnet computer worm, which was designed to subtly infect and damage specific types of industrial equipment. Based on the equipment targeted and the location of infected machines, security experts believe it was an attack on the Iranian nuclear program by the United States, Israel or, according to the latest news, even Russia.

Sabotage, done well, is inherently difficult to detect and difficult to trace to its origin. During World War II, the U.S. Federal Bureau of Investigation (FBI) investigated 19,649 cases of sabotage and concluded the enemy had not caused any of them.

Sabotage in warfare, according to the Office of Strategic Services (OSS) manual, varies from highly technical "coup de main" acts that require detailed planning and specially trained operatives, to innumerable simple acts that ordinary citizen-saboteurs can perform. Simple sabotage is carried out in such a way as to involve a minimum danger of injury, detection, and reprisal. There are two main methods of sabotage; physical destruction and the "human element". While physical destruction as a method is self-explanatory, its targets are nuanced, reflecting objects to which the saboteur has normal and inconspicuous access in everyday life. The "human element" is based on universal opportunities to make faulty decisions, to adopt a non-cooperative attitude, and to induce others to follow suit.

There are many examples of physical sabotage in wartime. However, one of the most effective uses of sabotage is against organizations. The OSS manual provides numerous techniques under the title "General Interference with Organizations and Production":


From the section entitled, "General Devices for Lowering Morale and Creating Confusion" comes the following quintessential simple sabotage advice: "Act stupid."

The United States Office of Strategic Services, later renamed the CIA, noted specific value in committing simple sabotage against the enemy during wartime: "... slashing tires, draining fuel tanks, starting fires, starting arguments, acting stupidly, short-circuiting electric systems, abrading machine parts will waste materials, manpower, and time." To underline the importance of simple sabotage on a widespread scale, they wrote, "Widespread practice of simple sabotage will harass and demoralize enemy administrators and police." The OSS was also focused on the battle for hearts and minds during wartime; "the very practice of simple sabotage by natives in enemy or occupied territory may make these individuals identify themselves actively with the United Nations War effort, and encourage them to assist openly in periods of Allied invasion and occupation."

On 30 July 1916, the Black Tom explosion occurred when German agents set fire to a complex of warehouses and ships in Jersey City, New Jersey that held munitions, fuel, and explosives bound to aid the Allies in their fight.

On 11 January 1917, Fiodore Wozniak, using a rag saturated with phosphorus or an incendiary pencil supplied by German sabotage agents, set fire to his workbench at an ammunition assembly plant near Lyndhurst, New Jersey, causing a four-hour fire that destroyed half a million 3-inch explosive shells and destroyed the plant for an estimated at $17 million in damages. Wozniak's involvement was not discovered until 1927.

On 12 February 1917, Bedouins allied with the British destroyed a Turkish railroad near the port of Wajh, derailing a Turkish locomotive. The Bedouins traveled by camel and used explosives to demolish a portion of track.

In Ireland, the Irish Republican Army (IRA) used sabotage against the British following the Easter 1916 uprising. The IRA compromised communication lines and lines of transportation and fuel supplies. The IRA also employed passive sabotage, refusing dock and train workers to work on ships and rail cars used by the government. In 1920, agents of the IRA committed arson against at least fifteen British warehouses in Liverpool. The following year, the IRA set fire to numerous British targets again, including the Dublin Customs House, this time sabotaging most of Liverpool's firetrucks in the firehouses before lighting the matches.

Sabotage training for the Allies consisted of teaching would-be saboteurs key components of working machinery to destroy.
"Saboteurs learned hundreds of small tricks to cause the Germans big trouble. The cables in a telephone junction box ... could be jumbled to make the wrong connections when numbers were dialed. A few ounces of plastique, properly placed, could bring down a bridge, cave in a mine shaft, or collapse the roof of a railroad tunnel."

The Polish Home Army Armia Krajowa, who commanded the majority of resistance organizations in Poland (even the National Forces, except the Military Organization Lizard Union; The Home Army also included the Polish Socialist Party – Freedom, Equality, Independence) and coordinating and aiding the Jewish Military Union as well as more reluctantly helping the Jewish Combat Organization, was responsible for the greatest number of acts of sabotage in German—occupied Europe. The Home Army's sabotage operations Operation Garland and Operation Ribbon are just two examples. In all, the Home Army damaged 6,930 locomotives, set 443 rail transports on fire, damaged over 19,000 rail cars "wagony," and blew up 38 rail bridges, not to mention the attacks against the rail roads. The Home Army was also responsible for 4,710 built-in flaws in parts for aircraft engines and 92,000 built-in flaws in artillery projectiles, among other examples of significant sabotage. In addition, over 25,000 acts of more minor sabotage were committed. It continued to fight against both the Germans and the Soviets; however, it did aid the Western Allies by collecting constant and detailed information on the German rail, wheeled, and horse transports. As for Stalin's proxies, their actions lead to a great number of the Polish and Jewish hostages, mostly civilians, murdered in reprisal by the Germans. The Gwardia Ludowa destroyed around 200 German trains during the war, and indiscriminately threw hand grenades into places frequented by Germans.

The French Resistance ran an extremely effective sabotage campaign against the Germans during World War II. Receiving their sabotage orders through messages over the BBC radio or by aircraft, the French used both passive and active forms of sabotage. Passive forms included losing German shipments and allowing poor quality material to pass factory inspections. Many active sabotage attempts were against critical rail lines of transportation. German records count 1,429 instances of sabotage from French Resistance forces between January 1942 and February 1943. From January through March 1944, sabotage accounted for three times the number of locomotives damaged by Allied air power. See also Normandy Landings for more information about sabotage on D-Day.

During World War II, the Allies committed sabotage against the Peugeot truck factory. After repeated failures in Allied bombing attempts to hit the factory, a team of French Resistance fighters and Special Operations Executive (SOE) agents distracted the German guards with a game of soccer while part of their team entered the plant and destroyed machinery.

In December 1944, the Germans ran a false flag sabotage infiltration, Operation Greif, which was commanded by Waffen-SS commando Otto Skorzeny during the Battle of the Bulge. German commandos, wearing US Army uniforms, carrying US Army weapons, and using US Army vehicles, penetrated US lines to spread panic and confusion among US troops and to blow up bridges, ammunition dumps, and fuel stores and to disrupt the lines of communication. Many of the commandos were captured by the Americans. Because they were wearing US uniforms, a number of the Germans were executed as spies, either summarily or after military commissions.

From 1948 to 1960, the Malayan Communists committed numerous effective acts of sabotage against the Malaysian Government, first targeting railway bridges, then hitting larger targets such as military camps. Most of their efforts were centered around crippling Malaysia's economy and involved sabotage against trains, rubber trees, water pipes, and electric lines. The Communist's sabotage efforts were so successful that they caused backlash amongst the Malaysian population, who gradually withdrew support for the Communist movement as their livelihoods became threatened.

In Mandatory Palestine from 1945 to 1948, Jewish groups opposed British control. Though that control was to end according to the United Nations Partition Plan for Palestine in 1948, the groups used sabotage as an opposition tactic. The Haganah focused their efforts on camps used by the British to hold refugees and radar installations that could be used to detect illegal immigrant ships. The Stern Gang and the Irgun used terrorism and sabotage against the British government and against lines of communications. In November 1946, the Irgun and Stern Gang attacked a railroad twenty-one times in a three-week period, eventually causing shell-shocked Arab railway workers to strike. The 6th Airborne Division was called in to provide security as a means of ending the strike.

The Viet Cong used swimmer saboteurs often and effectively during the Vietnam War. Between 1969 and 1970, swimmer saboteurs sunk, destroyed, or damaged 77 assets of the U.S. and its allies. Viet Cong swimmers were poorly equipped but well-trained and resourceful. The swimmers provided a low-cost/low-risk option with high payoff; possible loss to the country for failure compared to the possible gains from a successful mission led to the obvious conclusion the swimmer saboteurs were a good idea.

On 1 January 1984, the Cuscatlan bridge over Lempa river in El Salvador, critical to flow of commercial and military traffic, was destroyed by guerrilla forces using explosives after using mortar fire to "scatter" the bridge's guards, causing an estimated $3.7 million in required repairs, and considerably impacting on El Salvadoran business and security.

In 1982 in Honduras, a group of nine Salvadorans and Nicaraguans destroyed a main electrical power station, leaving the capital city Tegucigalpa without power for three days.

Some criminals have engaged in acts of sabotage for reasons of extortion. For example, Klaus-Peter Sabotta sabotaged German railway lines in the late 1990s in an attempt to extort DM10 million from the German railway operator Deutsche Bahn. He is now serving a sentence of life imprisonment. In 1989, ex-Scotland Yard detective Rodney Whitchelo was sentenced to 17 years in prison for spiking Heinz baby food products in supermarkets, in an extortion attempt on the food manufacturer.

The term political sabotage is sometimes used to define the acts of one political camp to disrupt, harass or damage the reputation of a political opponent, usually during an electoral campaign, such as during Watergate. Smear campaigns are a commonly used tactic. The term could also describe the actions and expenditures of private entities, corporations and organizations against democratically approved or enacted laws, policies and programs.

After the Cold War ended, the Mitrokhin Archives were declassified, which included detailed KGB plans of active measures to subvert politics in opposing nations.

Sabotage is a crucial tool of the successful coup d'etat, which requires control of communications before, during, and after the coup is staged. Simple sabotage against physical communications platforms using semi-skilled technicians, or even those trained only for this task, could effectively silence the target government of the coup, leaving the information battle space open to the dominance of the coup's leaders. To underscore the effectiveness of sabotage, "A single cooperative technician will be able temporarily to put out of action a radio station which would otherwise require a full-scale assault."

Railroads, where strategically important to the regime the coup is against, are prime targets for sabotage—if a section of the track is damaged entire portions of the transportation network can be stopped until it is fixed.

A sabotage radio was a small two-way radio designed for use by resistance movements in World War II, and after the war often used by expeditions and similar parties.

Arquilla and Rondfeldt, in their work entitled "Networks and Netwars", differentiate their definition of "netwar" from a list of "trendy synonyms," including "cybotage," a portmanteau from the words "sabotage" and "cyber". They dub the practitioners of cybotage "cyboteurs" and note while all cybotage is not netwar, some netwar is cybotage.

Counter-sabotage, defined by "Webster's Dictionary", is "counterintelligence designed to detect and counteract sabotage". The United States Department of Defense definition, found in the "Dictionary of Military and Associated Terms", is "action designed to detect and counteract sabotage. See also counterintelligence".

During World War II, British subject Eddie Chapman, trained by the Germans in sabotage, became a double agent for the British. The German Abwehr entrusted Chapman to destroy the British de Havilland Company's main plant which manufactured the outstanding Mosquito light bomber, but required photographic proof from their agent to verify the mission's completion. A special unit of the Royal Engineers known as the Magic Gang covered the de Havilland plant with canvas panels and scattered papier-mâché furniture and chunks of masonry around three broken and burnt giant generators. Photos of the plant taken from the air reflected devastation for the factory and a successful sabotage mission, and Chapman, as a British sabotage double-agent, fooled the Germans for the duration of the war.

In Japanese, the verb saboru (サボる) means to skip school or loaf on the job.




</doc>
<doc id="29463" url="https://en.wikipedia.org/wiki?curid=29463" title="Scabbard">
Scabbard

A scabbard is a sheath for holding a sword, knife, or other large blade. Scabbards have been made of many materials over the millennia, including leather, wood, and metals such as brass or steel.

Most commonly, scabbards were worn suspended from a sword belt or shoulder belt called a baldric.

Wooden scabbards were usually covered in fabric or leather; the leather versions also usually bore metal fittings for added protection and carrying ease. Japanese blades typically have their sharp cutting edge protected by a wooden scabbard called a "saya". Many scabbards, such as ones the Greeks and Romans used, were small and light. They were designed for holding the sword rather than protecting it. All-metal scabbards were popular items for a display of wealth among elites in the European Iron Age, and often intricately decorated. Little is known about the scabbards of the early Iron Age, due to their wooden construction. However, during the Middle and late Iron Ages, the scabbard became important especially as a vehicle for decorative elaboration. After 200 BC fully decorated scabbards became rare. A number of ancient scabbards have been recovered from weapons sacrifices, a few of which had a lining of fur on the inside. The fur was probably kept oily, keeping the blade free from rust. The fur would also allow a smoother, quicker draw.

Entirely metal scabbards became popular in Europe early in the 19th century and eventually superseded most other types. Metal was more durable than leather and could better withstand the rigours of field use, particularly among troops mounted on horseback. In addition, metal offered the ability to present a more military appearance, as well as the opportunity to display increased ornamentation. Nevertheless, leather scabbards never entirely lost favour among military users and were widely used as late as the American Civil War (1861–65).

Some military police forces, naval shore patrols, law enforcement and other groups used leather scabbards as a kind of truncheon.

Scabbards were historically, albeit rarely, worn across the back with the intention of being quickly unsheathed, but only by a handful of Celtic tribes, and only with very short lengths of sword. This is because it is almost impossible to draw any true two-handed sword and extraordinarily difficult to draw the majority of one-handed swords from a scabbard on the back. Common depictions of long swords being drawn from the back are a modern invention, and have enjoyed such great popularity in fiction and fantasy, that they are widely and incorrectly believed to have been common in Medieval times. Some more well-known examples of this include the back scabbard depicted in the film "Braveheart" and the back scabbard seen in the video game series "The Legend of Zelda". There is some limited data from woodcuts and textual fragments that Mongol light horse archers, Chinese soldiers, Japanese Samurai and European Knights wore a slung baldric over the shoulder, allowing longer blades such as greatswords/zweihanders and nodachi/ōdachi to be strapped across the back, though these would have to be removed from the back before the sword could be unsheathed.

In "The Ancient Celts" by Barry Cunliffe, page 94, Cunliffe writes, "All these pieces of equipment [shields, spears, swords, mail] mentioned in the texts, are reflected in the archaeological record and in the surviving iconography, though it is sometimes possible to detect regional variations. Among the Parisii of Yorkshire, for example, "the sword was sometimes worn across the back and therefore had to be drawn over the shoulder from behind the head".""

The metal fitting where the blade enters the leather or metal scabbard is called the throat, which is often part of a larger scabbard mount, or locket, that bears a carrying ring or stud to facilitate wearing the sword. The blade's point in leather scabbards is usually protected by a metal tip, or chape, which, on both leather and metal scabbards, is often given further protection from wear by an extension called a drag, or shoe.



</doc>
<doc id="29467" url="https://en.wikipedia.org/wiki?curid=29467" title="Spinel">
Spinel

Spinel () is the magnesium aluminium member of the larger spinel group of minerals. It has the formula MgAlO in the cubic crystal system. Its name comes from Latin "spina" (arrow). Balas ruby is an old name for a rose-tinted variety of spinel.

Spinel crystallizes in the isometric system; common crystal forms are octahedra, usually twinned. It has an imperfect octahedral cleavage and a conchoidal fracture. Its hardness is 8, its specific gravity is 3.5–4.1, and it is transparent to opaque with a vitreous to dull luster. It may be colorless, but is usually various shades of pink, rose, red, blue, green, yellow, brown, black, or (uncommon) violet. There is a unique natural white spinel, now lost, that surfaced briefly in what is now Sri Lanka. Some spinels are among the most famous gemstones; among them are the Black Prince's Ruby and the "Timur ruby" in the British Crown Jewels, and the "Côte de Bretagne", formerly from the French Crown jewels. The Samarian Spinel is the largest known spinel in the world, weighing .

The transparent red spinels were called spinel-rubies or balas rubies. In the past, before the arrival of modern science, spinels and rubies were equally known as rubies. After the 18th century the word ruby was only used for the red gem variety of the mineral corundum and the word spinel came to be used. "Balas" is derived from Balascia, the ancient name for Badakhshan, a region in central Asia situated in the upper valley of the Panj River, one of the principal tributaries of the Oxus River. Mines in the Gorno Badakhshan region of Tajikistan constituted for centuries the main source for red and pink spinels.

Spinel has long been found in the gemstone-bearing gravel of Sri Lanka and in limestones of the Badakshan Province in modern-day Afghanistan and Tajikistan; and of Mogok in Burma. Recently gem quality spinels also found in the marbles of Luc Yen (Vietnam), Mahenge and Matombo (Tanzania), Tsavo (Kenya) and in the gravels of Tunduru (Tanzania) and Ilakaka (Madagascar). Spinel is found as a metamorphic mineral, and also as a primary mineral in rare mafic igneous rocks; in these igneous rocks, the magmas are relatively deficient in alkalis relative to aluminium, and aluminium oxide may form as the mineral corundum or may combine with magnesia to form spinel. This is why spinel and ruby are often found together. The spinel petrogenesis in mafic magmatic rocks is strongly debated, but certainly results from mafic magma interaction with more evolved magma or rock (e.g. gabbro, troctolite). 

Spinel, (Mg,Fe)(Al,Cr)O, is common in peridotite in the uppermost Earth's mantle, between approximately 20 km to approximately 120 km, possibly to lower depths depending on the chromium content. At significantly shallower depths, above the Moho, calcic plagioclase is the more stable aluminous mineral in peridotite while garnet is the stable phase deeper in the mantle below the spinel stability region.

Spinel, (Mg,Fe)AlO, is a common mineral in the Ca-Al-rich inclusions (CAIs) in some chondritic meteorites.

Synthetic spinel, accidentally produced in the middle of the 18th century, has been described more recently in scientific publications in 2000 and 2004. By 2015, transparent spinel was being made in sheets and other shapes through sintering. Synthetic spinel which looks like glass but has notably higher strength against pressure, can also have applications in military and commercial use.





</doc>
<doc id="29468" url="https://en.wikipedia.org/wiki?curid=29468" title="Speech recognition">
Speech recognition

Speech recognition is the inter-disciplinary sub-field of computational linguistics that develops methodologies and technologies that enables the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the linguistics, computer science, and electrical engineering fields.

Some speech recognition systems require "training" (also called "enrollment") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called "speaker independent" systems. Systems that use training are called "speaker dependent".

Speech recognition applications include voice user interfaces such as voice dialing (e.g. "Call home"), call routing (e.g. "I would like to make a collect call"), domotic appliance control, search (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input).

The term "voice recognition" or "speaker identification" refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.

From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems. These speech industry players include Google, Microsoft, IBM, Baidu, Apple, Amazon, Nuance, SoundHound, iFLYTEK many of which have publicized the core technology in their speech recognition systems as being based on deep learning.

In 1952 three Bell Labs researchers built a system for single-speaker digit recognition. Their system worked by locating the formants in the power spectrum of each utterance. The 1950s era technology was limited to single-speaker systems with vocabularies of around ten words.

Gunnar Fant developed the source-filter model of speech production and published it in 1960, which proved to be a useful model of speech production.

Unfortunately, funding at Bell Labs dried up for several years when, in 1969, the influential John Pierce wrote an open letter that was critical of speech recognition research. Pierce defunded speech recognition research at Bell Labs where no research on speech recognition was done until Pierce retired and James L. Flanagan took over.

Raj Reddy was the first person to take on continuous speech recognition as a graduate student at Stanford University in the late 1960s. Previous systems required the users to make a pause after each word. Reddy's system was designed to issue spoken commands for the game of chess.

Also around this time Soviet researchers invented the dynamic time warping (DTW) algorithm and used it to create a recognizer capable of operating on a 200-word vocabulary. The DTW algorithm processed the speech signal by dividing it into short frames, e.g. 10ms segments, and processing each frame as a single unit. Although DTW would be superseded by later algorithms, the technique of dividing the signal into frames would carry on. Achieving speaker independence was a major unsolved goal of researchers during this time period.

In 1971, DARPA funded five years of speech recognition research through its Speech Understanding Research program with ambitious end goals including a minimum vocabulary size of 1,000 words. It was thought that speech "understanding" would be key to making progress in speech "recognition", although that later proved to not be true. BBN, IBM, Carnegie Mellon and Stanford Research Institute all participated in the program. The government funding revived speech recognition research that had been largely abandoned in the United States after John Pierce's letter.

Despite the fact that CMU's Harpy system met the original goals of the program, many predictions turned out to be nothing more than hype, disappointing DARPA administrators. This disappointment led to DARPA not continuing the funding. Several innovations happened during this time, such as the invention of beam search for use in CMU's Harpy system. The field also benefited from the discovery of several algorithms in other fields such as linear predictive coding and cepstral analysis.

In 1972, the IEEE Acoustics, Speech, and Signal Processing group held a conference in Newton, Massachusetts. Four years later, the first ICASSP was held in Philadelphia, which since then has been a major venue for the publication of research on speech recognition.

During the late 1960s Leonard Baum developed the mathematics of Markov chains at the Institute for Defense Analysis. A decade later, at CMU, Raj Reddy's students James Baker and Janet M. Baker began using the Hidden Markov Model (HMM) for speech recognition. James Baker had learned about HMMs from a summer job at the Institute of Defense Analysis during his undergraduate education. The use of HMMs allowed researchers to combine different sources of knowledge, such as acoustics, language, and syntax, in a unified probabilistic model.

Under Fred Jelinek's lead, IBM created a voice activated typewriter called Tangora, which could handle a 20,000 word vocabulary by the mid 1980s. Jelinek's statistical approach put less emphasis on emulating the way the human brain processes and understands speech in favor of using statistical modeling techniques like HMMs. (Jelinek's group independently discovered the application of HMMs to speech.) This was controversial with linguists since HMMs are too simplistic to account for many common features of human languages. However, the HMM proved to be a highly useful way for modeling speech and replaced dynamic time warping to become the dominant speech recognition algorithm in the 1980s. IBM had a few competitors including Dragon Systems founded by James and Janet M. Baker in 1982. The 1980s also saw the introduction of the n-gram language model. Katz introduced the back-off model in 1987, which allowed language models to use multiple length n-grams. During the same time, also CSELT was using HMM (the diphonies were studied since 1980) to recognize language like Italian. At the same time, CSELT led a series of European projects (Esprit I, II), and summarized the state-of-the-art in a book, later (2013) reprinted.

Much of the progress in the field is owed to the rapidly increasing capabilities of computers. At the end of the DARPA program in 1976, the best computer available to researchers was the PDP-10 with 4 MB ram. Using these computers it could take up to 100 minutes to decode just 30 seconds of speech. A few decades later, researchers had access to tens of thousands of times as much computing power. As the technology advanced and computers got faster, researchers began tackling harder problems such as larger vocabularies, speaker independence, noisy environments and conversational speech. In particular, this shifting to more difficult tasks has characterized DARPA funding of speech recognition since the 1980s. For example, progress was made on speaker independence first by training on a larger variety of speakers and then later by doing explicit speaker adaptation during decoding. Further reductions in word error rate came as researchers shifted acoustic models to be discriminative instead of using maximum likelihood estimation.

In the mid-Eighties new speech recognition microprocessors were released: for example RIPAC, an independent-speaker recognition (for continuous speech) chip tailored for telephone services, was presented in the Netherlands in 1986. It was designed by CSELT/Elsag and manufactured by SGS.

The 1990s saw the first introduction of commercially successful speech recognition technologies. Two of the earliest products were Dragon Dictate, a consumer product released in 1990 and originally priced at $9,000, and a recognizer from Kurzweil Applied Intelligence released in 1987. AT&T deployed the Voice Recognition Call Processing service in 1992 to route telephone calls without the use of a human operator. The technology was developed by Lawrence Rabiner and others at Bell Labs.
By this point, the vocabulary of the typical commercial speech recognition system was larger than the average human vocabulary. Raj Reddy's former student, Xuedong Huang, developed the Sphinx-II system at CMU. The Sphinx-II system was the first to do speaker-independent, large vocabulary, continuous speech recognition and it had the best performance in DARPA's 1992 evaluation. Handling continuous speech with a large vocabulary was a major milestone in the history of speech recognition. Huang went on to found the speech recognition group at Microsoft in 1993. Raj Reddy's student Kai-Fu Lee joined Apple where, in 1992, he helped develop a speech interface prototype for the Apple computer known as Casper.

Lernout & Hauspie, a Belgium-based speech recognition company, acquired several other companies, including Kurzweil Applied Intelligence in 1997 and Dragon Systems in 2000. The L&H speech technology was used in the Windows XP operating system. L&H was an industry leader until an accounting scandal brought an end to the company in 2001. The speech technology from L&H was bought by ScanSoft which became Nuance in 2005. Apple originally licensed software from Nuance to provide speech recognition capability to its digital assistant Siri.

In the 2000s DARPA sponsored two speech recognition programs: Effective Affordable Reusable Speech-to-Text (EARS) in 2002 and Global Autonomous Language Exploitation (GALE). Four teams participated in the EARS program: IBM, a team led by BBN with LIMSI and Univ. of Pittsburgh, 
Cambridge University, and a team composed of ICSI, SRI and University of Washington. EARS funded the collection of the Switchboard telephone speech corpus containing 260 hours of recorded conversations from over 500 speakers. The GALE program focused on Arabic and Mandarin broadcast news speech. Google's first effort at speech recognition came in 2007 after hiring some researchers from Nuance. The first product was GOOG-411, a telephone based directory service. The recordings from GOOG-411 produced valuable data that helped Google improve their recognition systems. Google voice search is now supported in over 30 languages.

In the United States, the National Security Agency has made use of a type of speech recognition for keyword spotting since at least 2006. This technology allows analysts to search through large volumes of recorded conversations and isolate mentions of keywords. Recordings can be indexed and analysts can run queries over the database to find conversations of interest. Some government research programs focused on intelligence applications of speech recognition, e.g. DARPA's EARS's program and IARPA's Babel program.

In the early 2000s, speech recognition was still dominated by traditional approaches such as Hidden Markov Models combined with feedforward artificial neural networks.
Today, however, many aspects of speech recognition have been taken over by a deep learning method called Long short-term memory (LSTM), 
a recurrent neural network published by Sepp Hochreiter & Jürgen Schmidhuber in 1997. LSTM RNNs avoid the vanishing gradient problem and can learn "Very Deep Learning" tasks that require memories of events that happened thousands of discrete time steps ago, which is important for speech.
Around 2007, LSTM trained by Connectionist Temporal Classification (CTC) started to outperform traditional speech recognition in certain applications. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to all smartphone users.

The use of deep feedforward (non-recurrent) networks for acoustic modeling was introduced during later part of 2009 by Geoffrey Hinton and his students at University of Toronto and by Li Deng and colleagues at Microsoft Research, initially in the collaborative work between Microsoft and University of Toronto which was subsequently expanded to include IBM and Google (hence "The shared views of four research groups" subtitle in their 2012 review paper). A Microsoft research executive called this innovation "the most dramatic change in accuracy since 1979". In contrast to the steady incremental improvements of the past few decades, the application of deep learning decreased word error rate by 30%. This innovation was quickly adopted across the field. Researchers have begun to use deep learning techniques for language modeling as well.

In the long history of speech recognition, both shallow form and deep form (e.g. recurrent nets) of artificial neural networks had been explored for many years during 1980s, 1990s and a few years into the 2000s.
But these methods never won over the non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.
A number of key difficulties had been methodologically analyzed in the 1990s, including gradient diminishing and weak temporal correlation structure in the neural predictive models.
All these difficulties were in addition to the lack of big training data and big computing power in these early days. Most speech recognition researchers who understood such barriers hence subsequently moved away from neural nets to pursue generative modeling approaches until the recent resurgence of deep learning starting around 2009–2010 that had overcome all these difficulties. Hinton et al. and Deng et al. reviewed part of this recent history about how their collaboration with each other and then with colleagues across four groups (University of Toronto, Microsoft, Google, and IBM) ignited a renaissance of applications of deep feedforward neural networks to speech recognition.

Both acoustic modeling and language modeling are important parts of modern statistically-based speech recognition algorithms. Hidden Markov models (HMMs) are widely used in many systems. Language modeling is also used in many other natural language processing applications such as document classification or statistical machine translation.

Modern general-purpose speech recognition systems are based on Hidden Markov Models. These are statistical models that output a sequence of symbols or quantities. HMMs are used in speech recognition because a speech signal can be viewed as a piecewise stationary signal or a short-time stationary signal. In a short time-scale (e.g., 10 milliseconds), speech can be approximated as a stationary process. Speech can be thought of as a Markov model for many stochastic purposes.

Another reason why HMMs are popular is because they can be trained automatically and are simple and computationally feasible to use. In speech recognition, the hidden Markov model would output a sequence of "n"-dimensional real-valued vectors (with "n" being a small integer, such as 10), outputting one of these every 10 milliseconds. The vectors would consist of cepstral coefficients, which are obtained by taking a Fourier transform of a short time window of speech and decorrelating the spectrum using a cosine transform, then taking the first (most significant) coefficients. The hidden Markov model will tend to have in each state a statistical distribution that is a mixture of diagonal covariance Gaussians, which will give a likelihood for each observed vector. Each word, or (for more general speech recognition systems), each phoneme, will have a different output distribution; a hidden Markov model for a sequence of words or phonemes is made by concatenating the individual trained hidden Markov models for the separate words and phonemes.

Described above are the core elements of the most common, HMM-based approach to speech recognition. Modern speech recognition systems use various combinations of a number of standard techniques in order to improve results over the basic approach described above. A typical large-vocabulary system would need context dependency for the phonemes (so phonemes with different left and right context have different realizations as HMM states); it would use cepstral normalization to normalize for different speaker and recording conditions; for further speaker normalization it might use vocal tract length normalization (VTLN) for male-female normalization and maximum likelihood linear regression (MLLR) for more general speaker adaptation. The features would have so-called delta and delta-delta coefficients to capture speech dynamics and in addition might use heteroscedastic linear discriminant analysis (HLDA); or might skip the delta and delta-delta coefficients and use splicing and an LDA-based projection followed perhaps by heteroscedastic linear discriminant analysis or a global semi-tied co variance transform (also known as maximum likelihood linear transform, or MLLT). Many systems use so-called discriminative training techniques that dispense with a purely statistical approach to HMM parameter estimation and instead optimize some classification-related measure of the training data. Examples are maximum mutual information (MMI), minimum classification error (MCE) and minimum phone error (MPE).

Decoding of the speech (the term for what happens when the system is presented with a new utterance and must compute the most likely source sentence) would probably use the Viterbi algorithm to find the best path, and here there is a choice between dynamically creating a combination hidden Markov model, which includes both the acoustic and language model information, and combining it statically beforehand (the finite state transducer, or FST, approach).

A possible improvement to decoding is to keep a set of good candidates instead of just keeping the best candidate, and to use a better scoring function (re scoring) to rate these good candidates so that we may pick the best one according to this refined score. The set of candidates can be kept either as a list (the N-best list approach) or as a subset of the models (a lattice). Re scoring is usually done by trying to minimize the Bayes risk (or an approximation thereof): Instead of taking the source sentence with maximal probability, we try to take the sentence that minimizes the expectancy of a given loss function with regards to all possible transcriptions (i.e., we take the sentence that minimizes the average distance to other possible sentences weighted by their estimated probability). The loss function is usually the Levenshtein distance, though it can be different distances for specific tasks; the set of possible transcriptions is, of course, pruned to maintain tractability. Efficient algorithms have been devised to re score lattices represented as weighted finite state transducers with edit distances represented themselves as a finite state transducer verifying certain assumptions.

Dynamic time warping is an approach that was historically used for speech recognition but has now largely been displaced by the more successful HMM-based approach.

Dynamic time warping is an algorithm for measuring similarity between two sequences that may vary in time or speed. For instance, similarities in walking patterns would be detected, even if in one video the person was walking slowly and if in another he or she were walking more quickly, or even if there were accelerations and deceleration during the course of one observation. DTW has been applied to video, audio, and graphics – indeed, any data that can be turned into a linear representation can be analyzed with DTW.

A well-known application has been automatic speech recognition, to cope with different speaking speeds. In general, it is a method that allows a computer to find an optimal match between two given sequences (e.g., time series) with certain restrictions. That is, the sequences are "warped" non-linearly to match each other. This sequence alignment method is often used in the context of hidden Markov models.

Neural networks emerged as an attractive acoustic modeling approach in ASR in the late 1980s. Since then, neural networks have been used in many aspects of speech recognition such as phoneme classification, isolated word recognition, audiovisual speech recognition, audiovisual speaker recognition and speaker adaptation.

In contrast to HMMs, neural networks make no assumptions about feature statistical properties and have several qualities making them attractive recognition models for speech recognition. When used to estimate the probabilities of a speech feature segment, neural networks allow discriminative training in a natural and efficient manner. Few assumptions on the statistics of input features are made with neural networks. However, in spite of their effectiveness in classifying short-time units such as individual phonemes and isolated words, neural networks are rarely successful for continuous recognition tasks, largely because of their lack of ability to model temporal dependencies.

However, recently LSTM Recurrent Neural Networks (RNNs) and Time Delay Neural Networks(TDNN's) have been used which have been shown to be able to identify latent temporal dependencies and use this information to perform the task of speech recognition.

Deep Neural Networks and Denoising Autoencoders were also being experimented with to tackle this problem in an effective manner.

Due to the inability of feedforward Neural Networks to model temporal dependencies, an alternative approach is to use neural networks as a pre-processing e.g. feature transformation, dimensionality reduction, for the HMM based recognition.

A deep feedforward neural network (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers. Similar to shallow neural networks, DNNs can model complex non-linear relationships. DNN architectures generate compositional models, where extra layers enable composition of features from lower layers, giving a huge learning capacity and thus the potential of modeling complex patterns of speech data.

A success of DNNs in large vocabulary speech recognition occurred in 2010 by industrial researchers, in collaboration with academic researchers, where large output layers of the DNN based on context dependent HMM states constructed by decision trees were adopted.
recent overview articles.

One fundamental principle of deep learning is to do away with hand-crafted feature engineering and to use raw features. This principle was first explored successfully in the architecture of deep autoencoder on the "raw" spectrogram or linear filter-bank features, showing its superiority over the Mel-Cepstral features which contain a few stages of fixed transformation from spectrograms.
The true "raw" features of speech, waveforms, have more recently been shown to produce excellent larger-scale speech recognition results.

Since 2014, there has been much research interest in "end-to-end" ASR. Traditional phonetic-based (i.e., all HMM-based model) approaches required separate components and training for the pronunciation, acoustic and language model. End-to-end models jointly learn all the components of the speech recognizer. This is valuable since it simplifies the training process and deployment process. For example, a n-gram language model is required for all HMM-based systems, and a typical n-gram language model often takes several gigabytes in memory making them impractical to deploy on mobile devices. Consequently, modern commercial ASR systems from Google and Apple (as of 2017) are deployed on the cloud and require a network connection as opposed to the device locally.

The first attempt of end-to-end ASR was with Connectionist Temporal Classification (CTC) based systems introduced by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014. The model consisted of recurrent neural networks and a CTC layer. Jointly, the RNN-CTC model learns the pronunciation and acoustic model together, however it is incapable of learning the language due to conditional independence assumptions similar to a HMM. Consequently, CTC models can directly learn to map speech acoustics to English characters, but the models make many common spelling mistakes and must rely on a separate language model to clean up the transcripts. Later, Baidu expanded on the work with extremely large datasets and demonstrated some commercial success in Chinese Mandarin and English. In 2016, University of Oxford presented LipNet, the first end-to-end sentence-level lip reading model, using spatiotemporal convolutions coupled with an RNN-CTC architecture, surpassing human-level performance in a restricted grammar dataset.

An alternative approach to CTC-based models are attention-based models. Attention-based ASR models were introduced simultaneously by Chan et al. of Carnegie Mellon University and Google Brain and Bahdanaua et al. of the University of Montreal in 2016. The model named "Listen, Attend and Spell" (LAS), literally "listens" to the acoustic signal, pays "attention" to different parts of the signal and "spells" out the transcript one character at a time. Unlike CTC-based models, attention-based models do not have conditional-independence assumptions and can learn all the components of a speech recognizer including the pronunciation, acoustic and language model directly. This means, during deployment, there is no need to carry around a language model making it very practical for deployment onto applications with limited memory. By the end of 2016, the attention-based models have seen considerable success including outperforming the CTC models (with or without an external language model). Various extensions have been proposed since the original LAS model. Latent Sequence Decompositions (LSD) was proposed by Carnegie Mellon University, MIT and Google Brain to directly emit sub-word units which are more natural than English characters; University of Oxford and Google DeepMind extended LAS to "Watch, Listen, Attend and Spell" (WLAS) to handle lip reading surpassing human-level performance.

Typically a manual control input, for example by means of a finger control on the steering-wheel, enables the speech recognition system and this is signalled to the driver by an audio prompt. Following the audio prompt, the system has a "listening window" during which it may accept a speech input for recognition.
Simple voice commands may be used to initiate phone calls, select radio stations or play music from a compatible smartphone, MP3 player or music-loaded flash drive. Voice recognition capabilities vary between car make and model. Some of the most recent car models offer natural-language speech recognition in place of a fixed set of commands, allowing the driver to use full sentences and common phrases. With such systems there is, therefore, no need for the user to memorize a set of fixed command words.

In the health care sector, speech recognition can be implemented in front-end or back-end of the medical documentation process. Front-end speech recognition is where the provider dictates into a speech-recognition engine, the recognized words are displayed as they are spoken, and the dictator is responsible for editing and signing off on the document. Back-end or deferred speech recognition is where the provider dictates into a digital dictation system, the voice is routed through a speech-recognition machine and the recognized draft document is routed along with the original voice file to the editor, where the draft is edited and report finalized. Deferred speech recognition is widely used in the industry currently.

One of the major issues relating to the use of speech recognition in healthcare is that the American Recovery and Reinvestment Act of 2009 (ARRA) provides for substantial financial benefits to physicians who utilize an EMR according to "Meaningful Use" standards. These standards require that a substantial amount of data be maintained by the EMR (now more commonly referred to as an Electronic Health Record or EHR). The use of speech recognition is more naturally suited to the generation of narrative text, as part of a radiology/pathology interpretation, progress note or discharge summary: the ergonomic gains of using speech recognition to enter structured discrete data (e.g., numeric values or codes from a list or a controlled vocabulary) are relatively minimal for people who are sighted and who can operate a keyboard and mouse.

A more significant issue is that most EHRs have not been expressly tailored to take advantage of voice-recognition capabilities. A large part of the clinician's interaction with the EHR involves navigation through the user interface using menus, and tab/button clicks, and is heavily dependent on keyboard and mouse: voice-based navigation provides only modest ergonomic benefits. By contrast, many highly customized systems for radiology or pathology dictation implement voice "macros", where the use of certain phrases – e.g., "normal report", will automatically fill in a large number of default values and/or generate boilerplate, which will vary with the type of the exam – e.g., a chest X-ray vs. a gastrointestinal contrast series for a radiology system.

As an alternative to this navigation by hand, cascaded use of speech recognition and information extraction has been studied as a way to fill out a handover form for clinical proofing and sign-off. The results are encouraging, and the paper also opens data, together with the related performance benchmarks and some processing software, to the research and development community for studying clinical documentation and language-processing.

Prolonged use of speech recognition software in conjunction with word processors has shown benefits to short-term-memory restrengthening in brain AVM patients who have been treated with resection. Further research needs to be conducted to determine cognitive benefits for individuals whose AVMs have been treated using radiologic techniques.

Substantial efforts have been devoted in the last decade to the test and evaluation of speech recognition in fighter aircraft. Of particular note have been the US program in speech recognition for the Advanced Fighter Technology Integration (AFTI)/F-16 aircraft (F-16 VISTA), the program in France for Mirage aircraft, and other programs in the UK dealing with a variety of aircraft platforms. In these programs, speech recognizers have been operated successfully in fighter aircraft, with applications including: setting radio frequencies, commanding an autopilot system, setting steer-point coordinates and weapons release parameters, and controlling flight display.

Working with Swedish pilots flying in the JAS-39 Gripen cockpit, Englund (2004) found recognition deteriorated with increasing g-loads. The report also concluded that adaptation greatly improved the results in all cases and that the introduction of models for breathing was shown to improve recognition scores significantly. Contrary to what might have been expected, no effects of the broken English of the speakers were found. It was evident that spontaneous speech caused problems for the recognizer, as might have been expected. A restricted vocabulary, and above all, a proper syntax, could thus be expected to improve recognition accuracy substantially.

The Eurofighter Typhoon, currently in service with the UK RAF, employs a speaker-dependent system, requiring each pilot to create a template. The system is not used for any safety-critical or weapon-critical tasks, such as weapon release or lowering of the undercarriage, but is used for a wide range of other cockpit functions. Voice commands are confirmed by visual and/or aural feedback. The system is seen as a major design feature in the reduction of pilot workload, and even allows the pilot to assign targets to his aircraft with two simple voice commands or to any of his wingmen with only five commands.

Speaker-independent systems are also being developed and are under test for the F35 Lightning II (JSF) and the Alenia Aermacchi M-346 Master lead-in fighter trainer. These systems have produced word accuracy scores in excess of 98%.

The problems of achieving high recognition accuracy under stress and noise pertain strongly to the helicopter environment as well as to the jet fighter environment. The acoustic noise problem is actually more severe in the helicopter environment, not only because of the high noise levels but also because the helicopter pilot, in general, does not wear a facemask, which would reduce acoustic noise in the microphone. Substantial test and evaluation programs have been carried out in the past decade in speech recognition systems applications in helicopters, notably by the U.S. Army Avionics Research and Development Activity (AVRADA) and by the Royal Aerospace Establishment (RAE) in the UK. Work in France has included speech recognition in the Puma helicopter. There has also been much useful work in Canada. Results have been encouraging, and voice applications have included: control of communication radios, setting of navigation systems, and control of an automated target handover system.

As in fighter applications, the overriding issue for voice in helicopters is the impact on pilot effectiveness. Encouraging results are reported for the AVRADA tests, although these represent only a feasibility demonstration in a test environment. Much remains to be done both in speech recognition and in overall speech technology in order to consistently achieve performance improvements in operational settings.

Training for air traffic controllers (ATC) represents an excellent application for speech recognition systems. Many ATC training systems currently require a person to act as a "pseudo-pilot", engaging in a voice dialog with the trainee controller, which simulates the dialog that the controller would have to conduct with pilots in a real ATC situation.
Speech recognition and synthesis techniques offer the potential to eliminate the need for a person to act as pseudo-pilot, thus reducing training and support personnel. In theory, Air controller tasks are also characterized by highly structured speech as the primary output of the controller, hence reducing the difficulty of the speech recognition task should be possible. In practice, this is rarely the case. The FAA document 7110.65 details the phrases that should be used by air traffic controllers. While this document gives less than 150 examples of such phrases, the number of phrases supported by one of the simulation vendors speech recognition systems is in excess of 500,000.

The USAF, USMC, US Army, US Navy, and FAA as well as a number of international ATC training organizations such as the Royal Australian Air Force and Civil Aviation Authorities in Italy, Brazil, and Canada are currently using ATC simulators with speech recognition from a number of different vendors.

ASR is now commonplace In the field of telephony, and is becoming more widespread in the field of computer gaming and simulation. Despite the high level of integration with word processing in general personal computing. However, ASR in the field of document production has not seen the expected increases in use.

The improvement of mobile processor speeds has made speech recognition practical in smartphones. Speech is used mostly as a part of a user interface, for creating predefined or custom speech commands. Leading software vendors in this field are: Google, Microsoft Corporation (Microsoft Voice Command), Digital Syphon (Sonic Extractor), LumenVox, Nuance Communications (Nuance Voice Control), Voci Technologies, VoiceBox Technology, Speech Technology Center, Vito Technologies (VITO Voice2Go), Speereo Software (Speereo Voice Translator), Verbyx VRX and SVOX.

For language learning, speech recognition can be useful for learning a second language. It can teach proper pronunciation, in addition to helping a person develop fluency with their speaking skills.

Students who are blind (see Blindness and education) or have very low vision can benefit from using the technology to convey words and then hear the computer recite them, as well as use a computer by commanding with their voice, instead of having to look at the screen and keyboard.

Students who are physically disabled or suffer from Repetitive strain injury/other injuries to the upper extremities can be relieved from having to worry about handwriting, typing, or working with scribe on school assignments by using speech-to-text programs. They can also utilize speech recognition technology to freely enjoy searching the Internet or using a computer at home without having to physically operate a mouse and keyboard.

Speech recognition can allow students with learning disabilities to become better writers. By saying the words aloud, they can increase the fluidity of their writing, and be alleviated of concerns regarding spelling, punctuation, and other mechanics of writing. Also, see Learning disability.

Use of voice recognition software, in conjunction with a digital audio recorder and a personal computer running word-processing software has proven to be positive for restoring damaged short-term-memory capacity, in stroke and craniotomy individuals.

People with disabilities can benefit from speech recognition programs. For individuals that are Deaf or Hard of Hearing, speech recognition software is used to automatically generate a closed-captioning of conversations such as discussions in conference rooms, classroom lectures, and/or religious services.

Speech recognition is also very useful for people who have difficulty using their hands, ranging from mild repetitive stress injuries to involve disabilities that preclude using conventional computer input devices. In fact, people who used the keyboard a lot and developed RSI became an urgent early market for speech recognition. Speech recognition is used in deaf telephony, such as voicemail to text, relay services, and captioned telephone. Individuals with learning disabilities who have problems with thought-to-paper communication (essentially they think of an idea but it is processed incorrectly causing it to end up differently on paper) can possibly benefit from the software but the technology is not bug proof. Also the whole idea of speak to text can be hard for intellectually disabled person's due to the fact that it is rare that anyone tries to learn the technology to teach the person with the disability.

This type of technology can help those with dyslexia but other disabilities are still in question. The effectiveness of the product is the problem that is hindering it being effective. Although a kid may be able to say a word depending on how clear they say it the technology may think they are saying another word and input the wrong one. Giving them more work to fix, causing them to have to take more time with fixing the wrong word.


The performance of speech recognition systems is usually evaluated in terms of accuracy and speed. Accuracy is usually rated with word error rate (WER), whereas speed is measured with the real time factor. Other measures of accuracy include Single Word Error Rate (SWER) and Command Success Rate (CSR).

Speech recognition by machine is a very complex problem, however. Vocalizations vary in terms of accent, pronunciation, articulation, roughness, nasality, pitch, volume, and speed. Speech is distorted by a background noise and echoes, electrical characteristics. Accuracy of speech recognition may vary with the following:

As mentioned earlier in this article, accuracy of speech recognition may vary depending on the following factors:
e.g. the 10 digits "zero" to "nine" can be recognized essentially perfectly, but vocabulary sizes of 200, 5000 or 100000 may have error rates of 3%, 7% or 45% respectively.
e.g. the 26 letters of the English alphabet are difficult to discriminate because they are confusable words (most notoriously, the E-set: "B, C, D, E, G, P, T, V, Z");
an 8% error rate is considered good for this vocabulary.
A speaker-dependent system is intended for use by a single speaker.
A speaker-independent system is intended for use by any speaker (more difficult).
With isolated speech, single words are used, therefore it becomes easier to recognize the speech.
With discontinuous speech full sentences separated by silence are used, therefore it becomes easier to recognize the speech as well as with isolated speech. 
With continuous speech naturally spoken sentences are used, therefore it becomes harder to recognize the speech, different from both isolated and discontinuous speech.
e.g. Querying application may dismiss the hypothesis "The apple is red." 
e.g. Constraints may be semantic; rejecting "The apple is angry." 
e.g. Syntactic; rejecting "Red is apple the." 
Constraints are often represented by a grammar. 
When a person reads it's usually in a context that has been previously prepared, but when a person uses spontaneous speech, it is difficult to recognize the speech because of the disfluencies (like "uh" and "um", false starts, incomplete sentences, stuttering, coughing, and laughter) and limited vocabulary. 
Environmental noise (e.g. Noise in a car or a factory) 
Acoustical distortions (e.g. echoes, room acoustics)
Speech recognition is a multi-levelled pattern recognition task.
e.g. Phonemes, Words, Phrases, and Sentences;
e.g. Known word pronunciations or legal word sequences, which can compensate for errors or uncertainties at lower level;
By combining decisions probabilistically at all lower levels, and making more deterministic decisions only at the highest level, speech recognition by a machine is a process broken into several phases. Computationally, it is a problem in which a sound pattern has to be recognized or classified into a category that represents a meaning to a human. Every acoustic signal can be broken in smaller more basic sub-signals. As the more complex sound signal is broken into the smaller sub-sounds, different levels are created, where at the top level we have complex sounds, which are made of simpler sounds on lower level, and going to lower levels even more, we create more basic and shorter and simpler sounds. The lowest level, where the sounds are the most fundamental, a machine would check for simple and more probabilistic rules of what sound should represent. Once these sounds are put together into more complex sound on upper level, a new set of more deterministic rules should predict what new complex sound should represent. The most upper level of a deterministic rule should figure out the meaning of complex expressions. In order to expand our knowledge about speech recognition we need to take into a consideration neural networks. There are four steps of neural network approaches: 
For telephone speech the sampling rate is 8000 samples per second; 
computed every 10 ms, with one 10 ms section called a frame;

Analysis of four-step neural network approaches can be explained by further information. Sound is produced by air (or some other medium) vibration, which we register by ears, but machines by receivers. Basic sound creates a wave which has 2 descriptions; Amplitude (how strong is it), and frequency (how often it vibrates per second).

The sound waves can be digitized: Sample a strength at short intervals like in picture above to get bunch of numbers that approximate at each time step the strength of a wave. Collection of these numbers represent analog wave. This new wave is digital. Sound waves are complicated because they superimpose one on top of each other. Like the waves would. This way they create odd-looking waves. For example, if there are two waves that interact with each other we can add them which creates new odd-looking wave.


Given basic sound blocks that a machine digitized, one has a bunch of numbers which describe a wave and waves describe words. Each frame has a unit block of sound, which are broken into basic sound waves and represented by numbers which, after Fourier Transform, can be statistically evaluated to set to which class of sounds it belongs. The nodes in the figure on a slide represent a feature of a sound in which a feature of a wave from the first layer of nodes to the second layer of nodes based on statistical analysis. This analysis depends on programmer's instructions. At this point, a second layer of nodes represents higher level features of a sound input which is again statistically evaluated to see what class they belong to. Last level of nodes should be output nodes that tell us with high probability what original sound really was.

Speech recognition can become a means of attack, theft, or accidental operation. For example, activation words like "Alexa" spoken in an audio or video broadcast can cause devices in homes and offices to start listening for input inappropriately, or possibly take an unwanted action. Voice-controlled devices are also accessible to visitors to the building, or even those outside the building if they can be heard inside. Attackers may be able to gain access to personal information, like calendar, address book contents, private messages, and documents. They may also be able to impersonate the user to send messages or make online purchases.

Two attacks have been demonstrated that use artificial sounds. One transmits ultrasound and attempt to send commands without nearby people noticing. The other adds small, inaudible distortions to other speech or music that are specially crafted to confuse the specific speech recognition system into recognizing music as speech, or to make what sounds like one command to a human sound like a different command to the system.

Popular speech recognition conferences held each year or two include SpeechTEK and SpeechTEK Europe, ICASSP, Interspeech/Eurospeech, and the IEEE ASRU. Conferences in the field of natural language processing, such as ACL, NAACL, EMNLP, and HLT, are beginning to include papers on speech processing. Important journals include the IEEE Transactions on Speech and Audio Processing (later renamed IEEE Transactions on Audio, Speech and Language Processing and since Sept 2014 renamed IEEE/ACM Transactions on Audio, Speech and Language Processing—after merging with an ACM publication), Computer Speech and Language, and Speech Communication.

Books like "Fundamentals of Speech Recognition" by Lawrence Rabiner can be useful to acquire basic knowledge but may not be fully up to date (1993). Another good source can be "Statistical Methods for Speech Recognition" by Frederick Jelinek and "Spoken Language Processing (2001)" by Xuedong Huang etc. More up to date are "Computer Speech", by Manfred R. Schroeder, second edition published in 2004, and "Speech Processing: A Dynamic and Optimization-Oriented Approach" published in 2003 by Li Deng and Doug O'Shaughnessey. The recently updated textbook of "Speech and Language Processing (2008)" by Jurafsky and Martin presents the basics and the state of the art for ASR. Speaker recognition also uses the same features, most of the same front-end processing, and classification techniques as is done in speech recognition. A most recent comprehensive textbook, "Fundamentals of Speaker Recognition" is an in depth source for up to date details on the theory and practice. A good insight into the techniques used in the best modern systems can be gained by paying attention to government sponsored evaluations such as those organised by DARPA (the largest speech recognition-related project ongoing as of 2007 is the GALE project, which involves both speech recognition and translation components).

A good and accessible introduction to speech recognition technology and its history is provided by the general audience book "The Voice in the Machine. Building Computers That Understand Speech" by Roberto Pieraccini (2012).

The most recent book on speech recognition is "Automatic Speech Recognition: A Deep Learning Approach" (Publisher: Springer) written by D. Yu and L. Deng published near the end of 2014, with highly mathematically-oriented technical detail on how deep learning methods are derived and implemented in modern speech recognition systems based on DNNs and related deep learning methods. A related book, published earlier in 2014, "Deep Learning: Methods and Applications" by L. Deng and D. Yu provides a less technical but more methodology-focused overview of DNN-based speech recognition during 2009–2014, placed within the more general context of deep learning applications including not only speech recognition but also image recognition, natural language processing, information retrieval, multimodal processing, and multitask learning.

In terms of freely available resources, Carnegie Mellon University's Sphinx toolkit is one place to start to both learn about speech recognition and to start experimenting. Another resource (free but copyrighted) is the HTK book (and the accompanying HTK toolkit). For more recent and state-of-the-art techniques, Kaldi toolkit can be used.

A Demo of an on-line speech recognizer is available on Cobalt's webpage.

For more software resources, see List of speech recognition software.




</doc>
<doc id="29469" url="https://en.wikipedia.org/wiki?curid=29469" title="Sapphire">
Sapphire

Sapphire is a precious gemstone, a variety of the mineral corundum, an aluminium oxide (). It is typically blue, but natural "fancy" sapphires also occur in yellow, purple, orange, and green colors; "parti sapphires" show two or more colors. The only color which sapphire cannot be is red – as red colored corundum is called ruby, another corundum variety. Pink colored corundum may be either classified as ruby or sapphire depending on locale. This variety in color is due to trace amounts of elements such as iron, titanium, chromium, copper, or magnesium.

Commonly, natural sapphires are cut and polished into gemstones and worn in jewelry. They also may be created synthetically in laboratories for industrial or decorative purposes in large crystal boules. Because of the remarkable hardness of sapphires – 9 on the Mohs scale (the third hardest mineral, after diamond at 10 and moissanite at 9.5) – sapphires are also used in some non-ornamental applications, such as infrared optical components, high-durability windows, wristwatch crystals and movement bearings, and very thin electronic wafers, which are used as the insulating substrates of very special-purpose solid-state electronics (especially integrated circuits and GaN-based LEDs).

Sapphire is the birthstone for September and the gem of the 45th anniversary. A sapphire jubilee occurs after 65 years.

Sapphire is one of the two gem-varieties of corundum, the other being ruby (defined as corundum in a shade of red). Although blue is the best-known sapphire color, they occur in other colors, including gray and black, and they can be colorless. A pinkish orange variety of sapphire is called padparadscha.

Significant sapphire deposits are found in Eastern Australia, Thailand, Sri Lanka, China (Shandong), Madagascar, East Africa, and in North America in a few locations, mostly in Montana. Sapphire and rubies are often found in the same geological setting.

Every sapphire mine produces a wide range of quality – and origin is not a guarantee of quality. For sapphire, Kashmir receives the highest premium although Burma, Sri Lanka, and Madagascar also produce large quantities of fine quality gems.

The cost of natural sapphires varies depending on their color, clarity, size, cut, and overall quality. For gems of exceptional quality, an independent determination from a respected laboratory such as the GIA, AGL or Gubelin of origin often adds to value.

Gemstone color can be described in terms of hue, saturation, and tone. Hue is commonly understood as the "color" of the gemstone. Saturation refers to the vividness or brightness of the hue, and tone is the lightness to darkness of the hue. Blue sapphire exists in various mixtures of its primary (blue) and secondary hues, various tonal levels (shades) and at various levels of saturation (vividness).

Blue sapphires are evaluated based upon the purity of their primary hue. Purple, violet, and green are the most common secondary hues found in blue sapphires. Violet and purple can contribute to the overall beauty of the color, while green is considered to be distinctly negative. Blue sapphires with up to 15% violet or purple are generally said to be of fine quality. Gray is the normal saturation modifier or mask found in blue sapphires. Gray reduces the saturation or brightness of the hue, and therefore has a distinctly negative effect.

The color of fine blue sapphires may be described as a vivid medium dark violet to purplish blue where the primary blue hue is at least 85% and the secondary hue no more than 15%, without the least admixture of a green secondary hue or a gray mask.

The Logan sapphire in the National Museum of Natural History, in Washington, D.C., is one of the largest faceted gem-quality blue sapphires in existence.

Sapphires in colors other than blue are called "fancy" or "parti colored" sapphires.

Fancy sapphires are often found in yellow, orange, green, brown, purple and violet hues.

Particolored sapphires are those stones which exhibit two or more colors within a single stone. Australia is the largest source of particolored sapphires; they are not commonly used in mainstream jewelry and remain relatively unknown. Particolored sapphires cannot be created synthetically and only occur naturally.

Colorless sapphires have historically been used as diamond substitutes in jewelry.

Pink sapphires occur in shades from light to dark pink, and deepen in color as the quantity of chromium increases. The deeper the pink color, the higher their monetary value. In the United States, a minimum color saturation must be met to be called a ruby, otherwise the stone is referred to as a "pink sapphire".

"Padparadscha" is a delicate, light to medium toned, pink-orange to orange-pink hued corundum, originally found in Sri Lanka, but also found in deposits in Vietnam and parts of East Africa. Padparadscha sapphires are rare; the rarest of all is the totally natural variety, with no sign of artificial treatment.

The name is derived from the Sanskrit "padma ranga" (padma = lotus; ranga = color), a color akin to the lotus flower ("Nelumbo nucifera").

Natural padparadscha sapphires often draw higher prices than many of even the finest blue sapphires. Recently, more sapphires of this color have appeared on the market as a result of a new artificial treatment method called "lattice diffusion".

A "star sapphire" is a type of sapphire that exhibits a star-like phenomenon known as asterism; red stones are known as "star rubies". Star sapphires contain intersecting needle-like inclusions following the underlying crystal structure that causes the appearance of a six-rayed "star"-shaped pattern when viewed with a single overhead light source. The inclusion is often the mineral rutile, a mineral composed primarily of titanium dioxide. The stones are cut "en cabochon", typically with the center of the star near the top of the dome. Occasionally, twelve-rayed stars are found, typically because two different sets of inclusions are found within the same stone, such as a combination of fine needles of rutile with small platelets of hematite; the first results in a whitish star and the second results in a golden-colored star. During crystallisation, the two types of inclusions become preferentially oriented in different directions within the crystal, thereby forming two six-rayed stars that are superimposed upon each other to form a twelve-rayed star. Misshapen stars or 12-rayed stars may also form as a result of twinning.
The inclusions can alternatively produce a "cat's eye" effect if the 'face-up' direction of the cabochon's dome is oriented perpendicular to the crystal's c-axis rather than parallel to it. If the dome is oriented in between these two directions, an 'off-center' star will be visible, offset away from the high point of the dome.

The Star of Adam is the largest blue star sapphire which weighs 1404.49 carats. The gem was mined in the city of Ratnapura, southern Sri Lanka. The Black Star of Queensland, the second largest gem-quality star sapphire in the world, weighs 733 carats. The Star of India mined in Sri Lanka and weighing 563.4 carats is thought to be the third-largest star sapphire, and is currently on display at the American Museum of Natural History in New York City. The 182-carat Star of Bombay, mined in Sri Lanka and located in the National Museum of Natural History in Washington, D.C., is another example of a large blue star sapphire. The value of a star sapphire depends not only on the weight of the stone, but also the body color, visibility, and intensity of the asterism.

A rare variety of natural sapphire, known as color-change sapphire, exhibits different colors in different light. Color change sapphires are blue in outdoor light and purple under incandescent indoor light, or green to gray-green in daylight and pink to reddish-violet in incandescent light. Color change sapphires come from a variety of locations, including Thailand and Tanzania. The color-change effect is caused by the interaction of the sapphire, which absorbs specific wavelengths of light, and the light-source, whose spectral output varies depending upon the illuminant. Transition-metal impurities in the sapphire, such as chromium and vanadium, are responsible for the color change.

Certain synthetic color-change sapphires have a similar color change to the natural gemstone alexandrite and they are sometimes marketed as "alexandrium" or "synthetic alexandrite". However, the latter term is a misnomer: synthetic color-change sapphires are, technically, not synthetic alexandrites but rather alexandrite "simulants". This is because genuine alexandrite is a variety of chrysoberyl: not sapphire, but an entirely different mineral.

Rubies are corundum which contain chromium impurities that absorb yellow-green light and result in deeper ruby red color with increasing content. Purple sapphires contain trace amounts of vanadium and come in a variety of shades. Corundum that contains ~0.01% of titanium is colorless. If trace amounts of iron are present, a very pale yellow to green color may be seen. However, if both titanium and iron impurities are present together, and in the correct valence states, the result is a deep-blue color.

Unlike localized ("intra-atomic") absorption of light which causes color for chromium and vanadium impurities, blue color in sapphires comes from intervalence charge transfer, which is the transfer of an electron from one transition-metal ion to another via the conduction or valence band. The iron can take the form Fe or Fe, while titanium generally takes the form Ti. If Fe and Ti ions are substituted for Al, localized areas of charge imbalance are created. An electron transfer from Fe and Ti can cause a change in the valence state of both. Because of the valence change there is a specific change in energy for the electron, and electromagnetic energy is absorbed. The wavelength of the energy absorbed corresponds to yellow light. When this light is subtracted from incident white light, the complementary color blue results. Sometimes when atomic spacing is different in different directions there is resulting blue-green dichroism.

Intervalence charge transfer is a process that produces a strong colored appearance at a low percentage of impurity. While at least 1% chromium must be present in corundum before the deep red ruby color is seen, sapphire blue is apparent with the presence of only 0.01% of titanium and iron.

Sapphires can be treated by several methods to enhance and improve their clarity and color. It is common practice to heat natural sapphires to improve or enhance color. This is done by heating the sapphires in furnaces to temperatures between for several hours, or by heating in a nitrogen-deficient atmosphere oven for seven days or more. Upon heating, the stone becomes more blue in color, but loses some of the rutile inclusions (silk). When high temperatures are used, the stone loses all silk (inclusions) and it becomes clear under magnification. The inclusions in natural stones are easily seen with a jeweler's loupe. Evidence of sapphire and other gemstones being subjected to heating goes back at least to Roman times. Un-heated natural stones are somewhat rare and will often be sold accompanied by a certificate from an independent gemological laboratory attesting to "no evidence of heat treatment".
Yogo sapphires sometimes do not need heat treating because their cornflower blue coloring is uniform and deep, they are generally free of the characteristic inclusions, and they have high uniform clarity. When Intergem Limited began marketing the Yogo in the 1980s as the world's only guaranteed untreated sapphire, heat treatment was not commonly disclosed; by 1982 the heat treatment became a major issue. At that time, 95% of all the world's sapphires were being heated to enhance their natural color. Intergem's marketing of guaranteed untreated Yogos set them against many in the gem industry. This issue appeared as a front-page story in the "Wall Street Journal" on 29 August 1984 in an article by Bill Richards, "Carats and Schticks: Sapphire Marketer Upsets The Gem Industry".

Diffusion treatments are used to add impurities to the sapphire to enhance color. Typically beryllium is diffused into a sapphire under very high heat, just below the melting point of the sapphire. Initially ("c." 2000) orange sapphires were created, although now the process has been advanced and many colors of sapphire are often treated with beryllium. The colored layer can be removed when stones chip or are repolished or refaceted, depending on the depth of the impurity layer. Treated padparadschas may be very difficult to detect, and many stones are certified by gemological labs ("e.g.", Gubelin, SSEF, AGTA).

According to United States Federal Trade Commission guidelines, disclosure is required of any mode of enhancement that has a significant effect on the gem's value.

There are several ways of treating sapphire. Heat-treatment in a reducing or oxidising atmosphere (but without the use of any other added impurities) is commonly used to improve the color of sapphires, and this process is sometimes known as "heating only" in the gem trade. In contrast, however, heat treatment combined with the deliberate addition of certain specific impurities (e.g. beryllium, titanium, iron, chromium or nickel, which are absorbed into the crystal structure of the sapphire) is also commonly performed, and this process can be known as "diffusion" in the gem trade. However, despite what the terms "heating only" and "diffusion" might suggest, both of these categories of treatment actually involve diffusion processes.

Sapphires are mined from alluvial deposits or from primary underground workings. Commercial mining locations for sapphire and ruby include (but are not limited to) the following countries: Afghanistan, Australia, Myanmar/Burma, Cambodia, China, Colombia, India, Kenya, Laos, Madagascar, Malawi, Nepal, Nigeria, Pakistan, Sri Lanka, Tajikistan, Tanzania, Thailand, United States, and Vietnam. Sapphires from different geographic locations may have different appearances or chemical-impurity concentrations, and tend to contain different types of microscopic inclusions. Because of this, sapphires can be divided into three broad categories: classic metamorphic, non-classic metamorphic or magmatic, and classic magmatic.

Sapphires from certain locations, or of certain categories, may be more commercially appealing than others, particularly classic metamorphic sapphires from Kashmir, Burma, or Sri Lanka that have not been subjected to heat-treatment.

The Logan sapphire, the Star of India, and the Star of Bombay originate from Sri Lankan mines. Madagascar is the world leader in sapphire production (as of 2007) specifically its deposits in and around the town of Ilakaka. Prior to the opening of the Ilakaka mines, Australia was the largest producer of sapphires (such as in 1987). In 1991 a new source of sapphires was discovered in Andranondambo, southern Madagascar. That area has been exploited for its sapphires started in 1993, but it was practically abandoned just a few years later—because of the difficulties in recovering sapphires in their bedrock.

In North America, sapphires have been mined mostly from deposits in Montana: fancies along the Missouri River near Helena, Montana, Dry Cottonwood Creek near Deer Lodge, Montana, and Rock Creek near Philipsburg, Montana. Fine blue Yogo sapphires are found at Yogo Gulch west of Lewistown, Montana. A few gem-grade sapphires and rubies have also been found in the area of Franklin, North Carolina.

The sapphire deposits of Kashmir are well known in the gem industry, despite the fact their peak production took place in a relatively short period at the end of the nineteenth and early twentieth centuries. They have a superior cornflower blue hue to them with a mysterious and almost sleepy quality, described by some gem enthusiasts as ‘blue velvet”. Kashmir-origin contributes meaningfully to the value of a sapphire, and most corundum of Kashmir origin can be readily identified by its characteristic silky appearance and exceptional hue. The unique blue appears lustrous under any kind of light, unlike non-Kashmir sapphires which may appear purplish or grayish in comparison. Sotheby's has been in the forefront overseeing record-breaking sales of Kashmir sapphires worldwide. In October 2014, Sotheby’s Hong Kong achieved consecutive per-carat price records for Kashmir sapphires - first with the 12.00 carat Cartier sapphire ring at US$193,975 per carat, then with a 17.16 carat sapphire at US$236,404, and again in June 2015 when the per-carat auction record was set at US$240,205. At present, the world record price-per-carat for sapphire at auction is held by a sapphire from Kashmir in a ring, which sold in October 2015 for approximately US$242,000 per carat (HK$52,280,000 in total, including buyer's premium, or more than US$6.74 million).

In 1902, the French chemist Auguste Verneuil developed a process for producing synthetic sapphire crystals. In the Verneuil process, named after him, fine alumina powder is added to an oxyhydrogen flame, and this is directed downward against a mantle. The alumina in the flame is slowly deposited, creating a teardrop shaped "boule" of sapphire material. Chemical dopants can be added to create artificial versions of the ruby, and all the other natural colors of sapphire, and in addition, other colors never seen in geological samples. Artificial sapphire material is identical to natural sapphire, except it can be made without the flaws that are found in natural stones. The disadvantage of Verneuil process is that the grown crystals have high internal strains. Many methods of manufacturing sapphire today are variations of the Czochralski process, which was invented in 1916 by Polish chemist Jan Czochralski. In this process, a tiny sapphire seed crystal is dipped into a crucible made of the precious metal iridium or molybdenum, containing molten alumina, and then slowly withdrawn upward at a rate of 1 to 100 mm per hour. The alumina crystallizes on the end, creating long carrot-shaped boules of large size up to 200 kg in mass.

Synthetic sapphire is also produced industrially from agglomerated aluminium oxide, sintered and fused (such as by hot isostatic pressing) in an inert atmosphere, yielding a transparent but slightly porous polycrystalline product.

In 2003, the world's production of synthetic sapphire was 250 tons (1.25 × 10 carats), mostly by the United States and Russia. The availability of cheap synthetic sapphire unlocked many industrial uses for this unique material.

The first laser was made with a rod of synthetic ruby. Titanium-sapphire lasers are popular due to their relatively rare capacity to be tuned to various wavelengths in the red and near-infrared region of the electromagnetic spectrum. They can also be easily mode-locked. In these lasers a synthetically produced sapphire crystal with chromium or titanium impurities is irradiated with intense light from a special lamp, or another laser, to create stimulated emission.

Synthetic sapphire—sometimes referred to as "sapphire glass"—is commonly used as a window material, because it is both highly transparent to wavelengths of light between 150 nm (UV) and 5500 nm (IR) (the visible spectrum extends about 380 nm to 750 nm), and extraordinarily scratch-resistant.

The key benefits of sapphire windows are:
Some sapphire-glass windows are made from pure sapphire boules that have been grown in a specific crystal orientation, typically along the optical axis, the c-axis, for minimum birefringence for the application.

The boules are sliced up into the desired window thickness and finally polished to the desired surface finish. Sapphire optical windows can be polished to a wide range of surface finishes due to its crystal structure and its hardness. The surface finishes of optical windows are normally called out by the scratch-dig specifications in accordance with the globally adopted MIL-O-13830 specification.

The sapphire windows are used in both high pressure and vacuum chambers for spectroscopy, crystals in various watches, and windows in grocery store barcode scanners since the material's exceptional hardness and toughness makes it very resistant to scratching.

It is used for end windows on some high-powered laser tubes as its wide-band transparency and thermal conductivity allow it to handle very high power densities in the infra-red or UV spectrum without degrading due to heating.

Along with zirconia and aluminium oxynitride, synthetic sapphire is used for shatter resistant windows in armored vehicles and various military body armor suits, in association with composites.

One type of xenon arc lamp – originally called the "Cermax" and now known generically as the "ceramic body xenon lamp" – uses sapphire crystal output windows. This product tolerates higher thermal loads and thus higher output powers when compared with conventional Xe lamps with pure silica window.

Thin sapphire wafers were the first successful use of an insulating substrate upon which to deposit silicon to make the integrated circuits known as silicon on sapphire or "SOS"; now other substrates can also be used for the class of circuits known more generally as silicon on insulator. Besides its excellent electrical insulating properties, sapphire has high thermal conductivity. CMOS chips on sapphire are especially useful for high-power radio-frequency (RF) applications such as those found in cellular telephones, public-safety band radios, and satellite communication systems. "SOS" also allows for the monolithic integration of both digital and analog circuitry all on one IC chip, and the construction of extremely low power circuits.

In one process, after single crystal sapphire boules are grown, they are core-drilled into cylindrical rods, and wafers are then sliced from these cores.

Wafers of single-crystal sapphire are also used in the semiconductor industry as substrates for the growth of devices based on gallium nitride (GaN). The use of sapphire significantly reduces the cost, because it has about one-seventh the cost of germanium. Gallium nitride on sapphire is commonly used in blue light-emitting diodes (LEDs).

Monocrystalline sapphire is fairly biocompatible and the exceptionally low wear of sapphire–metal pairs has led to the introduction (in Ukraine) of sapphire monocrystals for hip 
joint endoprostheses.




 


</doc>
<doc id="29471" url="https://en.wikipedia.org/wiki?curid=29471" title="Slack voice">
Slack voice

Slack voice (or lax voice) is the pronunciation of consonant or vowels with a glottal opening slightly wider than that occurring in modal voice. Such sounds are often referred to informally as lenis or half-voiced in the case of consonants. In some Chinese varieties, such as Wu, and in many Austronesian languages, the 'intermediate' phonation of slack stops confuses listeners of languages without these distinctions, so that different transcription systems may use or for the same consonant. In Xhosa, slack-voiced consonants have usually been transcribed as breathy voice. Although the IPA has no dedicated diacritic for slack voice, the voiceless diacritic (the under-ring) may be used with a voiced consonant letter, though this convention is also used for partially voiced consonants in languages such as English.

Wu Chinese "muddy" consonants are slack voice word-initially, the primary effect of which is a slightly breathy quality of the following vowel.

Javanese contrasts slack and stiff voiced bilabial, dental, retroflex, and velar stops.

Parauk contrasts slack voicing in its vowels. The contrast is between "slightly stiff" and "slightly breathy" vowels; the first are between modal and stiff voice, while the latter are captured by slack voice.


</doc>
<doc id="29472" url="https://en.wikipedia.org/wiki?curid=29472" title="SADC">
SADC

SADC may stand for:



</doc>
<doc id="29473" url="https://en.wikipedia.org/wiki?curid=29473" title="Salvation">
Salvation

Salvation (; ; ; ) is being saved or protected from harm or being saved or delivered from a dire situation. In religion, salvation is saving of the soul from sin and its consequences.

The academic study of salvation is called soteriology.

In religion, salvation is the saving of the soul from sin and its consequences. It may also be called "deliverance" or "redemption" from sin and its effects. Historically, salvation is considered to be caused either by the grace of a deity (i.e. unmerited and unearned); by the independent choices of a free will and personal effort (i.e. earned and/or merited); or by some combination of the two. Religions often emphasize the necessity of both personal effort—for example, repentance and asceticism—and divine action (e.g. grace).

In contemporary Judaism, redemption (Hebrew "ge'ulah"), refers to God redeeming the people of Israel from their various exiles. This includes the final redemption from the present exile.

Judaism holds that adherents do not need personal salvation as Christians believe. Jews do not subscribe to the doctrine of original sin. Instead, they place a high value on individual morality as defined in the law of God — embodied in what Jews know as the Torah or The Law, given to Moses by God on biblical Mount Sinai.

In Judaism, salvation is closely related to the idea of redemption, a saving from the states or circumstances that destroy the value of human existence. God, as the universal spirit and Creator of the World, is the source of all salvation for humanity, provided an individual honours God by observing his precepts. So redemption or salvation depends on the individual. Judaism stresses that salvation cannot be obtained through anyone else or by just invoking a deity or believing in any outside power or influence.

When examining Jewish intellectual sources throughout history, there is clearly a spectrum of opinions regarding death versus the afterlife. Possibly an over-simplification, one source says salvation can be achieved in the following manner: Live a holy and righteous life dedicated to Yahweh, the God of Creation. Fast, worship, and celebrate during the appropriate holidays.
By origin and nature, Judaism is an ethnic religion. Therefore, salvation has been primarily conceived in terms of the destiny of Israel as the elect people of Yahweh (often referred to as “the Lord”), the God of Israel. In the biblical text of Psalms, there is a description of death, when people go into the earth or the "realm of the dead" and cannot praise God. The first reference to resurrection is collective in Ezekiel's vision of the dry bones, when all the Israelites in exile will be resurrected. There is a reference to individual resurrection in the Book of Daniel (165 BCE), the last book of the Hebrew Bible. It was not until the 2nd century BCE that there arose a belief in an afterlife, in which the dead would be resurrected and undergo divine judgment. Before that time, the individual had to be content that his posterity continued within the holy nation.

The salvation of the individual Jew was connected to the salvation of the entire people. This belief stemmed directly from the teachings of the Torah. In the Torah, God taught his people sanctification of the individual. However, he also expected them to function together (spiritually) and be accountable to one another. The concept of salvation was tied to that of restoration for Israel.

Christianity’s primary premise is that the incarnation and death of Jesus Christ formed the climax of a divine plan for humanity’s salvation. This plan was conceived by God consequent on the Fall of Adam, the progenitor of the human race, and it would be completed at the Last Judgment, when the Second Coming of Christ would mark the catastrophic end of the world.

For Christianity, salvation is only possible through Jesus Christ. Christians believe that Jesus' death on the cross was the once-for-all sacrifice that atoned for the sin of humanity.

The Christian religion, though not the exclusive possessor of the idea of redemption, has given to it a special definiteness and a dominant position. Taken in its widest sense, as deliverance from dangers and ills in general, most religions teach some form of it. It assumes an important position, however, only when the ills in question form part of a great system against which human power is helpless.
According to Christian belief, sin as the human predicament is considered to be universal. For example, in the Apostle Paul declared everyone to be under sin—Jew and Gentile alike. Salvation is made possible by the life, death, and resurrection of Jesus, which in the context of salvation is referred to as the "atonement". Christian soteriology ranges from exclusive salvation to universal reconciliation concepts. While some of the differences are as widespread as Christianity itself, the overwhelming majority agrees that salvation is made possible by the work of Jesus Christ, the Son of God, dying on the cross.

Variant views on salvation are among the main fault lines dividing the various Christian denominations, both between Roman Catholicism and Protestantism and within Protestantism, notably in the Calvinist–Arminian debate, and the fault lines include conflicting definitions of depravity, predestination, atonement, but most pointedly justification.

Salvation is believed to be a process that begins when a person first becomes a Christian, continues through that person's life, and is completed when they stand before Christ in judgment. Therefore, according to Catholic apologist James Akin, the faithful Christian can say in faith and hope, "I "have been" saved; I "am being" saved; and I "will be" saved."

Christian salvation concepts are varied and complicated by certain theological concepts, traditional beliefs, and dogmas. Scripture is subject to individual and ecclesiastical interpretations. While some of the differences are as widespread as Christianity itself, the overwhelming majority agrees that salvation is made possible by the work of Jesus Christ, the Son of God, dying on the cross.

The purpose of salvation is debated, but in general most Christian theologians agree that God devised and implemented his plan of salvation because he loves them and regards human beings as his children. Since human existence on Earth is said to be "given to sin", salvation also has connotations that deal with the liberation of human beings from sin, and the suffering associated with the punishment of sin—i.e., "the wages of sin are death."

Christians believe that salvation depends on the grace of God. Stagg writes that a fact assumed throughout the Bible is that humanity is in, "serious trouble from which we need deliverance…. The fact of sin as the human predicament is implied in the mission of Jesus, and it is explicitly affirmed in that connection". By its nature, salvation must answer to the plight of humankind as it actually is. Each individual's plight as sinner is the result of a fatal choice involving the whole person in bondage, guilt, estrangement, and death. Therefore, salvation must be concerned with the total person. "It must offer redemption from bondage, forgiveness for guilt, reconciliation for estrangement, renewal for the marred image of God".

According to doctrine of the Latter Day Saint movement, the plan of salvation is a plan that God created to save, redeem, and exalt humankind. The elements of this plan are drawn from various sources, including the Bible, Book of Mormon, Doctrine & Covenants, Pearl of Great Price, and numerous statements made by the leadership of The Church of Jesus Christ of Latter-day Saints (LDS Church). The first appearance of the graphical representation of the plan of salvation is in the 1952 missionary manual entitled "A Systematic Program for Teaching the Gospel."

In Islam, salvation refers to the eventual entrance to heaven. Islam teaches that people who die disbelieving in God do not receive salvation. It also teaches that non-Muslims who die believing in the God but disbelieving in his message (Islam), are left to his will. Those who die believing in the One God and his message (Islam) receive salvation.

Narrated Anas that Muhammad said,
Islam teaches that all who enter into Islam must remain so in order to receive salvation.
For those who have not been granted Islam or to whom the message has not been brought;
Belief in the “One God”, also known as the "Tawhid" (التَوْحيدْ) in Arabic, consists of two parts (or principles):

Islam also stresses that in order to gain salvation, one must also avoid sinning along with performing good deeds. Islam acknowledges the inclination of humanity towards sin. Therefore, Muslims are constantly commanded to seek God's forgiveness and repent. Islam teaches that no one can gain salvation simply by virtue of their belief or deeds, instead it is the Mercy of God, which merits them salvation. However, this repentance must not be used to sin any further. Islam teaches that God is Merciful.

Islam describes a true believer to have Love of God and Fear of God. Islam also teaches that every person is responsible for their own sins. The Quran states;

Al-Agharr al-Muzani, a companion of Mohammad, reported that Ibn 'Umar stated to him that Mohammad said,

Sin in Islam is not a state, but an action (a bad deed); Islam teaches that a child is born sinless, regardless of the belief of his parents, dies a Muslim; he enters heaven, and does not enter hell. 

There are acts of worship that Islam teaches to be mandatory. Islam is built on five principles. Narrated Ibn 'Umar that Muhammad said,
Not performing the mandatory acts of worship may deprive Muslims of the chance of salvation.

Hinduism, Buddhism, Jainism and Sikhism share certain key concepts, which are interpreted differently by different groups and individuals. In these religions one is not liberated from sin and its consequences, but from the "saṃsāra" (cycle of rebirth) perpetuated by passions and delusions and its resulting karma. They differ however on the exact nature of this liberation. Salvation is called "moksha" or "mukti" which mean liberation and release respectively. This state and the conditions considered necessary for its realization is described in early texts of Indian religion such as the Upanishads and the Pāli Canon, and later texts such the Yoga Sutras of Patanjali and the Vedanta tradition. "Moksha" can be attained by sādhanā, literally "means of accomplishing something". It includes a variety of disciplines, such as yoga and meditation.

Nirvana is the profound peace of mind that is acquired with moksha (liberation). In Buddhism and Jainism, it is the state of being free from suffering. In Hindu philosophy, it is union with the Brahman (Supreme Being). The word literally means "blown out" (as in a candle) and refers, in the Buddhist context, to the blowing out of the fires of desire, aversion, and delusion, and the imperturbable stillness of mind acquired thereafter.

In Theravada Buddhism the emphasis is on one's own liberation from samsara. The Mahayana traditions emphasize the bodhisattva path, in which "each Buddha and Bodhisattva is a redeemer", assisting the Buddhist in seeking to achieve the redemptive state. The assistance rendered is a form of self-sacrifice on the part of the teachers, who would presumably be able to achieve total detachment from worldly concerns, but have instead chosen to remain engaged in the material world to the degree that this is necessary to assist others in achieving such detachment.

In Jainism, "salvation", "moksa" and "nirvana" are one and the same. When a soul ("atman") achieves moksa, it is released from the cycle of births and deaths, and achieves its pure self. It then becomes a "siddha" (literally means one who has accomplished his ultimate objective). Attaining Moksa requires annihilation of all "karmas", good and bad, because if karma is left, it must bear fruit.





</doc>
<doc id="29475" url="https://en.wikipedia.org/wiki?curid=29475" title="Lockheed S-3 Viking">
Lockheed S-3 Viking

The Lockheed S-3 Viking is a four-seat, twin-engine turbofan-powered jet aircraft that was used by the U.S. Navy primarily for anti-submarine warfare. In the late 1990s, the S-3B's mission focus shifted to surface warfare and aerial refueling. The Viking also provided electronic warfare and surface surveillance capabilities to the carrier battle group. A carrier-based, subsonic, all-weather, multi-mission aircraft with long range; it carried automated weapon systems, and was capable of extended missions with in-flight refueling. Because of the Viking's engines' characteristic sound, it was nicknamed the "Hoover" after the vacuum cleaner brand.

The S-3 was retired from front-line US Navy fleet service aboard aircraft carriers in January 2009, with its missions being assumed by other platforms such as the P-3C Orion, Sikorsky SH-60 Seahawk, and Boeing F/A-18E/F Super Hornet. Several aircraft were flown by Air Test and Evaluation Squadron Thirty (VX-30) at Naval Base Ventura County / NAS Point Mugu, California, for range clearance and surveillance operations on the NAVAIR Point Mugu Range until 2016, and one S-3 is operated by the National Aeronautics and Space Administration (NASA) at the NASA Glenn Research Center.

In the mid-1960s, the U.S. Navy developed the VSX (Heavier-than-air, Anti-submarine, Experimental) requirement for a replacement for the piston-engined Grumman S-2 Tracker as an anti-submarine aircraft to fly off the Navy's aircraft carriers. In August 1968, a team led by Lockheed and a Convair/Grumman team were asked to further develop their proposals to meet this requirement. Lockheed recognised that it had little recent experience in designing carrier based aircraft, so Ling-Temco-Vought (LTV) was brought into the team, being responsible for the folding wings and tail, the engine nacelles, and the landing gear, which was derived from LTV A-7 Corsair II (nose) and Vought F-8 Crusader (main). Sperry Univac Federal Systems was assigned the task of developing the aircraft's onboard computers which integrated input from sensors and sonobuoys.

On 4 August 1969, Lockheed's design was selected as the winner of the contest, and eight prototypes, designated YS-3A were ordered. The first prototype was flown on 21 January 1972 by military test pilot John Christiansen, and the S-3 entered service in 1974. During the production run from 1974 to 1978, a total of 186 S-3As were built. The majority of the surviving S-3As were later upgraded to the S-3B variant, with sixteen aircraft converted into ES-3A Shadow electronic intelligence (ELINT) collection aircraft.

The S-3 is a conventional monoplane with a cantilever shoulder wing, swept at an angle of 15°. The two GE TF-34 high-bypass turbofan engines mounted in nacelles under the wings provide excellent fuel efficiency, giving the Viking the required long range and endurance, while maintaining docile engine-out characteristics.
The aircraft can seat four crew members, three officers and one enlisted aircrewman, with the pilot and the copilot/tactical coordinator (COTAC) in the front of the cockpit and the tactical coordinator (TACCO) and sensor operator (SENSO) in the back. Entry is by an entry door / ladder which folds out of the side of the fuselage. When the aircraft's anti-submarine warfare (ASW) role ended in the late 1990s, the enlisted SENSOs were removed from the crew. In the tanking crew configuration, the S-3B typically flew with a pilot and co-pilot/COTAC. The wing is fitted with leading edge and Fowler flaps. Spoilers are fitted to both the upper and the lower surfaces of the wings. All control surfaces are actuated by dual hydraulically boosted irreversible systems. In the event of dual hydraulic failures, an Emergency Flight Control System (EFCS) permits manual control with greatly increased stick forces and reduced control authority.

Unlike many tactical jets which required ground service equipment, the S-3 was equipped with an auxiliary power unit (APU) and capable of unassisted starts. The aircraft's original APU could provide only minimal electric power and pressurized air for both aircraft cooling and for the engines' pneumatic starters. A newer, more powerful APU could provide full electrical service to the aircraft. The APU itself was started from a hydraulic accumulator by pulling a mechanical handle in the cockpit. The APU accumulator was fed from the primary hydraulic system, but could also be pumped up manually (with much effort) from the cockpit.

All crew members sit on forward-facing, upward-firing Douglas Escapac zero-zero ejection seats. In "group eject" mode, initiating ejection from either front seat ejects the entire crew in sequence, with the back seats ejecting 0.5 seconds before the front in order to provide safe separation. The rear seats are capable of self ejection, and the ejection sequence includes a pyrotechnic charge that stows the rear keyboard trays out of the occupants' way immediately before ejection. Safe ejection requires the seats to be weighted in pairs, and when flying with a single crewman in the back the unoccupied seat is fitted with ballast blocks.

At the time it entered the fleet, the S-3 introduced an unprecedented level of systems integration. Previous ASW aircraft like the Lockheed P-3 Orion and S-3's predecessor, the Grumman S-2 Tracker, featured separate instrumentation and controls for each sensor system. Sensor operators often monitored paper traces, using mechanical calipers to make precise measurements and annotating data by writing on the scrolling paper. Beginning with the S-3, all sensor systems were integrated through a single General Purpose Digital Computer (GPDC). Each crew station had its own display, the co-pilot/COTAC, TACCO and SENSO displays were Multi-Purpose Displays (MPD) capable of displaying data from any of a number of systems. This new level of integration allowed the crew to consult with each other by examining the same data at multiple stations simultaneously, to manage workload by assigning responsibility for a given sensor from one station to another, and to easily combine clues from each sensor to classify faint targets. Because of this, the four-man S-3 was considered roughly equivalent in capability to the much larger P-3 with a crew of 12.

The aircraft has two underwing hardpoints that can be used to carry fuel tanks, general purpose and cluster bombs, missiles, rockets, and storage pods. It also has four internal bomb bay stations that can be used to carry general purpose bombs, aerial torpedoes, and special stores (B57 and B61 nuclear weapons). Fifty-nine sonobuoy chutes are fitted, as well as a dedicated Search and Rescue (SAR) chute. The S-3 is fitted with the ALE-39 countermeasure system and can carry up to 90 rounds of chaff, flares, and expendable jammers (or a combination of all) in three dispensers. A retractable magnetic anomaly detector (MAD) Boom is fitted in the tail.

In the late 1990s, the S-3B's role was changed from anti-submarine warfare (ASW) to anti-surface warfare (ASuW). At that time, the MAD Boom was removed, along with several hundred pounds of submarine detection electronics. With no remaining sonobuoy processing capability, most of the sonobuoy chutes were faired over with a blanking plate.

On 20 February 1974, the S-3A officially became operational with the Air Antisubmarine Squadron FORTY-ONE (VS-41), the "Shamrocks," at NAS North Island, California, which served as the initial S-3 Fleet Replacement Squadron (FRS) for both the Atlantic and Pacific Fleets until a separate Atlantic Fleet FRS, VS-27, was established in the 1980s. The first operational cruise of the S-3A took place in 1975 with the VS-21 "Fighting Redtails" aboard .

Starting in 1987, some S-3As were upgraded to S-3B standard with the addition of a number of new sensors, avionics, and weapons systems, including the capability to launch the AGM-84 Harpoon anti-ship missile. The S-3B could also be fitted with "buddy stores", external fuel tanks that allowed the Viking to refuel other aircraft. In July 1988, VS-30 became the first fleet squadron to receive the enhanced capability Harpoon/ISAR equipped S-3B, based at NAS Cecil Field in Jacksonville, Florida. 16 S-3As were converted to ES-3A Shadows for carrier-based electronic intelligence (ELINT) duties. Six aircraft, designated US-3A, were converted for a specialized utility and limited cargo COD requirement. Plans were also made to develop the KS-3A carrier-based tanker aircraft, but this program was ultimately cancelled after the conversion of just one early development S-3A.

With the collapse of the Soviet Union and the breakup of the Warsaw Pact, the Soviet-Russian submarine threat was perceived as much reduced, and the Vikings had the majority of their antisubmarine warfare equipment removed. The aircraft's mission subsequently changed to sea surface search, sea and ground attack, over-the-horizon targeting, and aircraft refueling. As a result, the S-3B after 1997 was typically crewed by one pilot and one copilot [NFO]; the additional seats in the S-3B could still support additional crew members for certain missions. To reflect these new missions the Viking squadrons were redesignated from "Air Antisubmarine Warfare Squadrons" to "Sea Control Squadrons."
Prior to the aircraft's retirement from front-line fleet use aboard US aircraft carriers, a number of upgrade programs were implemented. These include the Carrier Airborne Inertial Navigation System II (CAINS II) upgrade, which replaced older inertial navigation hardware with ring laser gyroscopes with a Honeywell EGI (Enhanced GPS Inertial Navigation System) and added digital electronic flight instruments (EFI). The Maverick Plus System (MPS) added the capability to employ the AGM-65E laser-guided or AGM-65F infrared-guided air-to-surface missile, and the AGM-84H/K Stand-off Land Attack Missile Expanded Response (SLAM/ER). The SLAM/ER is a GPS/inertial/infrared guided cruise missile derived from the AGM-84 Harpoon that can be controlled by the aircrew in the terminal phase of flight if an AWW-13 data link pod is carried by the aircraft.

The S-3B saw extensive service during the 1991 Gulf War, performing attack, tanker, and ELINT duties, and launching ADM-141 TALD decoys. This was the first time an S-3B was employed overland during an offensive air strike. The first mission occurred when an aircraft from VS-24, from the , attacked an Iraqi Silkworm missile site. The aircraft also participated in the Yugoslav wars in the 1990s and in Operation Enduring Freedom in 2001.

The first ES-3A was delivered in 1991, entering service after two years of testing. The Navy established two squadrons of eight ES-3A aircraft each in both the Atlantic and Pacific Fleets to provide detachments of typically two aircraft, ten officers, and 55 enlisted aircrew, maintenance and support personnel (which comprised/supported four complete aircrews) to deploying carrier air wings. The Pacific Fleet squadron, Fleet Air Reconnaissance Squadron FIVE (VQ-5), the "Sea Shadows," was originally based at the former NAS Agana, Guam but later relocated to NAS North Island in San Diego, California, with the Pacific Fleet S-3 Viking squadrons when NAS Agana closed in 1995 as a result of a 1993 Base Realignment and Closure (BRAC) decision. The Atlantic Fleet squadron, the VQ-6 "Black Ravens," were originally based with all Atlantic Fleet S-3 Vikings at the former NAS Cecil Field in Jacksonville, Florida, but later moved to NAS Jacksonville, approximately to the east, when NAS Cecil Field was closed in 1999 as a result of the same 1993 BRAC decision that closed NAS Agana.
The ES-3A operated primarily with carrier battle groups, providing organic 'Indications and Warning' support to the group and joint theater commanders. In addition to their warning and reconnaissance roles, and their extraordinarily stable handling characteristics and range, Shadows were a preferred recovery tanker (aircraft that provide refueling for returning aircraft). They averaged over 100 flight hours per month while deployed. Excessive utilization caused earlier than expected equipment replacement when Naval aviation funds were limited, making them an easy target for budget-driven decision makers. In 1999, both ES-3A squadrons and all 16 aircraft were decommissioned and the ES-3A inventory placed in Aerospace Maintenance and Regeneration Group (AMARG) storage at Davis-Monthan AFB, Arizona.

In March 2003, during Operation Iraqi Freedom, an S-3B Viking from Sea Control Squadron 38 (The "Red Griffins") piloted by Richard McGrath Jr. launched from . The crew successfully executed a time sensitive strike and fired a laser-guided Maverick missile to neutralize a significant Iraqi naval and leadership target in the port city of Basra, Iraq. This was one of the few times in its operational history that the S-3B Viking had been employed overland on an offensive combat air strike and the first time it launched a laser-guided Maverick missile in combat.
On 1 May 2003, US President George W. Bush flew in the co-pilot seat of a VS-35 Viking from NAS North Island, California, to off the California coast. There, he delivered his "Mission Accomplished" speech announcing the end of major combat in the 2003 invasion of Iraq. During the flight, the aircraft used the customary presidential callsign of "Navy One". The aircraft that President Bush flew in was retired shortly thereafter and on 15 July 2003 was accepted as an exhibit at the National Museum of Naval Aviation at NAS Pensacola, Florida.

Between July and December 2008 the VS-22 Checkmates, the last sea control squadron, operated a detachment of four S-3Bs from the Al Asad Airbase in Al Anbar Province, west of Baghdad. The planes were fitted with LANTIRN pods and they performed non-traditional intelligence, surveillance, and reconnaissance (NTISR). After more than 350 missions, the Checkmates returned to NAS Jacksonville, Florida, on 15 December 2008, prior to disestablishing on 29 January 2009.

Though a proposed airframe known as the Common Support Aircraft was once advanced as a successor to the S-3, E-2 and C-2, this plan failed to materialize. As the surviving S-3 airframes were forced into sundown retirement, a Lockheed Martin full scale fatigue test was performed and extended the service life of the aircraft by approximately 11,000 flight-hours. This supported Navy plans to retire all Vikings from front-line fleet service by 2009 so new strike fighter and multi-mission aircraft could be introduced to recapitalize the aging fleet inventory, with former Viking missions assumed by other fixed-wing and rotary-wing aircraft.

The final carrier based S-3B Squadron, VS-22 was decommissioned at NAS Jacksonville on 29 January 2009. Sea Control Wing Atlantic was decommissioned the following day on 30 January 2009, concurrent with the U.S. Navy retiring the last S-3B Viking from front-line Fleet service.

In June 2010 the first of three aircraft to patrol the Pacific Missile Test Center's range areas off of California was reactivated and delivered. The jet aircraft's higher speed, 10-hour endurance, modern radar, and a LANTIRN targeting pod allowed it to quickly confirm the test range being clear of wayward ships and aircraft before tests commence. These S-3Bs are flown by Air Test and Evaluation Squadron Thirty (VX-30) based out of NAS Point Mugu, California. Also, the NASA Glenn Research Center acquired four S-3Bs in 2005. Since 2009, one of these aircraft (USN BuNo 160607) has also carried the civil registration N601NA and is used for various tests.

By late 2015, the U.S. Navy had three Vikings remaining operational in support roles. One was moved to The Boneyard in November 2015, and the final two were retired, one stored and the other transferred to NASA, on 11 January 2016, officially retiring the S-3 from Navy service.

Naval analysts have suggested returning the stored S-3s to service with the U.S. Navy to fill gaps it left in the carrier air wing when it was retired. This is in response to the realization that the Chinese navy is producing new weapons that can threaten carriers beyond the range their aircraft can strike them. Against the DF-21D anti-ship ballistic missile, carrier-based F/A-18 Super Hornets and F-35C Lightning IIs have about half the unrefueled strike range, so bringing the S-3 back to aerial tanking duties would extend their range against it, as well as free up more Super Hornets that were forced to fill the role. Against submarines armed with anti-ship cruise missiles like the Klub and YJ-18, the S-3 would restore area coverage for ASW duties. Bringing the S-3 out of retirement could at least be a stop-gap measure to increase the survivability and capabilities of aircraft carriers until new aircraft can be developed for such purposes.

In October 2013, the Republic of Korea Navy expressed an interest in acquiring up to 18 ex-USN S-3s to augment their fleet of 16 Lockheed P-3 Orion aircraft. In August 2015, a military program review group approved a proposal to incorporate 12 mothballed S-3s to perform ASW duties; the Viking plan will be sent to the Defense Acquisition Program Administration for further assessment before final approval by the national defense system committee. Although the planes are old, being in storage kept them serviceable and using them is a cheaper way to fulfill short-range airborne ASW capabilities left after the retirement of the S-2 Tracker than buying newer aircraft. Refurbished S-3s could be returned to use by 2019. In 2017, the Republic of Korea Navy canceled plans to purchase refurbished and upgraded Lockheed S-3 Viking aircraft for maritime patrol and anti-submarine duties, leaving offers by Airbus, Boeing, Lockheed Martin and Saab on the table.

In April 2014, Lockheed Martin announced that they would offer refurbished and remanufactured S-3s, dubbed the C-3, as a replacement for the Northrop Grumman C-2A Greyhound for Carrier onboard delivery. The requirement for 35 aircraft would be met from the 91 S-3s currently in storage. In February 2015, the Navy announced that the Bell Boeing V-22 Osprey had been selected to replace the C-2 for the COD mission.





Notes
Bibliography



</doc>
<doc id="29476" url="https://en.wikipedia.org/wiki?curid=29476" title="Kaman SH-2 Seasprite">
Kaman SH-2 Seasprite

The Kaman SH-2 Seasprite is a ship-based helicopter originally developed and produced by American manufacturer Kaman Aircraft Corporation. It has been typically used as a compact and fast-moving rotorcraft for utility and anti-submarine warfare missions.

Development of the Seasprite had been initiated during the late 1950s in response to a request from the United States Navy, calling for a suitably fast and compact naval helicopter for utility missions. Kaman's submission, internally designated as the "K-20", was favourably evaluated, leading to the issuing of a contract for the construction of four prototypes and an initial batch of 12 production helicopters, designated as the "HU2K-1". Beyond the U.S. Navy, the company had also made efforts to acquire other customers for export sales, in particular the Royal Canadian Navy; however, the initial interest of the Canadians was quelled as a result of Kaman's demand for price increases and the Seasprite performing below company projections during its sea trials. Due to its unsatisfactory performance, from 1968 onwards, the U.S. Navy's existing UH-2s were remanufactured from their originally-delivered single-engine arrangement to a more powerful twin-engine configuration.

During October 1970, the Seasprite was selected by the U.S. Navy as the platform for the interim Light Airborne Multi-Purpose System (LAMPS) helicopter, which resulted in greatly enhanced anti-submarine and anti-surface threat capabilities being developed and installed upon a new variant of the type, designated as the "SH.2F". Accordingly, during the 1970s and 1980s, the majority of the existing UH-2 helicopters were remanufactured into the improved SH-2F model. In this configuration, the Seasprite extended and increased shipboard sensor and weapon capabilities against several types of enemy threats, including submarines of all types, surface ships and patrol craft that may be armed with anti-ship missiles.

The Seasprite served for many decades with the U.S. Navy. Highlights of its service life included operations during the lengthy Vietnam War, in which the type was primarily used to rescue downed friendly aircrews within the theatre of operations, and its deployment during the Gulf War, where Seasprites conducted combat support and surface warfare operations against hostile Iraqi forces. In more routine operations, the Seasprite was operated in a number of roles, including anti-submarine warfare (ASW), search and rescue (SAR), utility and plane guard (the latter being performed when on attachment to aircraft carriers). The type was finally withdrawn during 2001 when the last examples of the final variant, known as the SH-2G Super Seasprite were retired. During the 1990s and 2000s, ex-U.S. Navy Seasprites were offered to various nations as a form of foreign aid, which typically met with mixed interest and a limited uptake.

During 1956, the U.S. Navy launched a new competition with the intent of meeting its requirements for a compact, all-weather multipurpose naval helicopter, encouraging private companies to submit their proposals. American manufacturer Kaman Aircraft Corporation decided to produce its own response for the competition, their submitted design, which was given the internal company decision of "K-20", was of a relatively conventional helicopter powered by a single General Electric T58-8F turboshaft engine which drove a 44-foot four-bladed main rotor and a four-bladed tail rotor. Following an evaluation of the designs that had been bid in response, the U.S. Navy decided to select the submission by Kaman to proceed with further development. Accordingly, during late 1957, Kaman was promptly awarded with a contract calling for the construction of four prototypes and an initial batch of 12 production helicopters, designated as the "HU2K-1".

During 1960, the Royal Canadian Navy announced that the HU2K has been identified as the frontrunner for their own requirement for an anti-submarine warfare helicopter; this choice was confirmed when the Treasury Board of the Canadian government gave its approval for the initial procurement of 12 rotorcraft from Kaman at a price of $14.5 million. However, the Canadian purchase was disrupted by multiple factors, including Kaman's decision to abruptly raise the estimated price of the initial batch to $23 million; as the same time, there were concerns amongst officials that the manufacturer's projections of both the weight and performance criteria has been overly optimistic. In response, the Canadian Naval Board decided to hold off on issuing its approval to proceed with the HU2K purchase until after the US Navy had conducted sea trials with the type. During these sea trials, it was revealed that the HU2K was indeed overweight and underpowered; in light of this inferior performance, the HU2K was deemed to be incapable of meeting the Canadian requirements. Accordingly, during late 1961, the competing Sikorsky Sea King was selected to fulfil the intended role instead.

Having been unable to achieve any follow-on orders for the type, during the late 1960s, Kaman decided to terminate production following the completion of the delivery of 184 SH-2s to the U.S. Navy. However, during 1971, production was later restarted by Kaman in order to manufacture an improved variant of the helicopter, designated as the "SH-2F". A significant factor in the reopening of the production line was that the Navy's Sikorsky SH-60 Sea Hawk, which was both newer and more capable in anti-submarine operations, had been determined to be too large to allow it to be safely operated from the smaller flight decks present upon the older frigates then in service.

Upon the enactment of the 1962 United States Tri-Service aircraft designation system, the HU2K-1 had been redesignated as the "UH-2A", while the "HU2K-1U" model was redesignated as the "UH-2B". During its service, the UH-2 Seasprite would be subject to several modifications and improvements, such as the addition of fixtures for the mounting of external stores. Beginning in 1968, the Navy's remaining UH-2s were extensively remanufactured; perhaps the most extensive alteration performance was the replacement of their original single-engine arrangement with a more powerful twin-engine configuration.

During October 1970, the UH-2 was selected to be the platform to function as the interim Light Airborne Multi-Purpose System (LAMPS) helicopter. During the course of the 1960s, LAMPS had evolved out of an urgent requirement to develop a manned helicopter that would be capable of supporting a non-aviation vessel and serve as its tactical Anti-Submarine Warfare arm. Widely referred to as "LAMPS Mark I", the advanced sensors, processors, and display capabilities aboard the helicopter enabled such equipped ships to extend their situational awareness beyond the line-of-sight limitations that unavoidably hampered the performance of shipboard radars, as well as the short distances involved in the acoustic detection and prosecution of underwater threats associated with hull-mounted sonars. Those H-2s that were reconfigured to perform the LAMPS mission were accordingly re-designated as "SH-2D"s.

On 16 March 1971, the first SH-2D LAMPS prototype conducted its first flight. Beginning in 1973, production deliveries of the latest variant of the rotorcraft, designated as the "SH-2F", commenced. Amongst the features of the "SH-2F" model was the full suite of LAMPS I equipment, along with various other improvements, such as upgraded engines, an extended life main rotor, and an elevated take-off weight. During 1981, the Navy placed an order for 60 production SH-2Fs. From 1987 onwards, a total of 16 SH-2Fs were upgraded with a chin-mounted forward-looking infrared (FLIR) sensor, chaff/flare launchers, dual rear-mounted infrared countermeasures, and missile/mine detecting equipment.

Eventually, all but two H-2s that were then in the U.S. Navy inventory were remanufactured into the SH-2F configuration. The final production procurement of the SH-2F was in Fiscal Year 1986. The final six orders for production SH-2Fs were converted to the more extensive and newer SH-2G Super Seasprite variant.

During 1962, the initial UH-2 model commenced its operational service with the U.S. Navy. The U.S. Navy quickly determined that the helicopter's capabilities were greatly restricted by its single engine; thus, the service ordered Kaman to retrofit all of its Seasprites into a more capable twin-engine arrangement instead; when furnished with a pair of engines, the Seasprite was capable of attaining an airspeed of 130 knots and operating at a range of up to 411 nautical miles. The U.S. Navy would operate a total fleet of nearly 200 Seasprites to perform a variety of missions, ranging from anti-submarine warfare (ASW) operations, search and rescue (SAR) and utility transport. Under typical operational conditions, several UH-2s would be deployed upon each of the U.S. Navy's aircraft carriers in order to perform plane guard and SAR missions.

The UH-2 was introduced in time to see action in the Tonkin Gulf incident in August 1964. The Seasprite's principal contribution to what would escalate into the lengthy Vietnam War between the Soviet-backed North Vietnamese and the United States-backed South Vietnamese, was the retrieval of downed aircrews, both from the sea and from inside enemy territory. The type was increasingly relied upon to perform the retrieval mission as the conflict intensified, such as during Operation Rolling Thunder in 1965. During October 1966 alone, out of 269 downed pilots, helicopter-based SAR teams were recorded as having enabled the recovery of 103 men.
During the 1970s, the conversion of UH-2s to the SH-2 anti-submarine configuration provided the U.S. Navy with its first dedicated ASW helicopter capable of operating from vessels other than its aircraft carriers. The compact size of the SH-2 allowed the type to be operated from flight decks that were too small for the majority of helicopters; this factor would later play a role in the U.S. Navy's decision to acquire the improved SH-2F during the early 1980s.

The SH-2F fleet was utilized to enforce and support Operation Earnest Will in July 1987, Operation Praying Mantis in April 1988, and Operation Desert Storm during January 1991 in the Persian Gulf region. The countermeasures and additional equipment present upon the SH-2F allowed the type to conduct combat support and surface warfare missions within these hostile environments, which had an often-minimal submarine threat. During April 1994, the SH-2F was retired from active service with the U.S. Navy; the timing corresponded with the retirement of the last of the Vietnam-era Knox Class Frigates that were unable to accommodate the new and larger SH-60 Sea Hawks, which were used to replace the aging Seasprites.

During 1991, the U.S. Navy had begun to receive deliveries of the new SH-2G Super Seasprite; a total of 18 converted SH-2Fs and 6 new-built SH-2Gs were produced. These were assigned to Naval Reserve squadrons, the SH-2G entered service with HSL-84 in 1993. The SH-2 served in some 600 deployments and flew 1.5 million flight hours before the last of the type were finally retired in mid-2001.

The Royal New Zealand Navy (RNZN) replaced its Westland Wasps with an initial batch of four interim SH-2F Seasprites (formerly operated by the U.S. Navy), operated and maintained by a mix of Navy and Air Force personnel known as No. 3 Squadron RNZAF Naval Support Flight, to operate with ANZAC class frigates until the fleet of five new SH-2G(NZ) Super Seasprites were delivered. During October 2005, the Navy air element was transferred to No. 6 Squadron RNZAF at RNZAF Base Auckland in Whenuapai. RNZN Seasprites have seen service in East Timor. 10 of the 11 SH-2G(A)s rejected by the Royal Australian Navy were purchased in 2014 to replace the five RNZN SH-2G(NZ) Seasprites that had required either a MLU (Mid Life Upgrade) or replacement due to corrosion issues, maintenance problems and obsolescence. Kaman modified the ex-Australian aircraft and renamed them SH-2G(I), with the last one being delivered to New Zealand in early 2016. Eight of the aircraft are flying with the ninth and tenth aircraft being attritional aircraft used for spares etc. The 11th aircraft is held by Kaman as a prototype and test aircraft. The five SH-2G(NZ) have been sold to Peru. A SH-2F (ex-RNZN, NZ3442) is preserved in the Royal New Zealand Air Force Museum, donated to the museum by Kaman Aircraft Corporation after an accident while in service with the RNZN.

During the late 1990s, the United States decided to offer the surplus U.S. Navy SH-2Fs as foreign aid to a number of overseas countries. Amongst those to be offered the type included Greece, which had been offered six, and Turkey, which had been offered 14, but they rejected the offer. Egypt opted to acquire four SH-2F under this aid program, they were mainly used for spares in to support of their existing fleet of ten SH-2Gs. Poland chose to acquire the later SH-2G variant.






</doc>
<doc id="29480" url="https://en.wikipedia.org/wiki?curid=29480" title="Stop consonant">
Stop consonant

In phonetics, a stop, also known as a plosive or oral occlusive, is a consonant in which the vocal tract is blocked so that all airflow ceases.

The occlusion may be made with the tongue blade (, ) or body (, ), lips (, ), or glottis (). Stops contrast with nasals, where the vocal tract is blocked but airflow continues through the nose, as in and , and with fricatives, where partial occlusion impedes but does not block airflow in the vocal tract.

The terms "stop, occlusive," and "plosive" are often used interchangeably. Linguists who distinguish them may not agree on the distinction being made. The terms refer to different features of the consonant. "Stop" refers to the airflow that is stopped. "Occlusive" refers to the articulation, which occludes (blocks) the vocal tract. "Plosive" refers to the release burst (plosion) of the consonant. Some object to the use of "plosive" for inaudibly released stops, which may then instead be called "applosives".

Either "occlusive" or "stop" may be used as a general term covering the other together with nasals. That is, 'occlusive' may be defined as oral occlusives (stops/plosives) plus nasal occlusives (nasals such as , ), or 'stop' may be defined as oral stops (plosives) plus nasal stops (nasals). Ladefoged and Maddieson (1996) prefer to restrict 'stop' to oral occlusives. They say,

In addition, they use "plosive" for a pulmonic stop; "stops" in their usage include ejective and implosive consonants.

If a term such as 'plosive' is used for oral obstruents, and nasals are not called nasal stops, then a "stop" may mean the glottal stop; 'plosive' may even mean non-glottal stop. In other cases, however, it may be the word 'plosive' that is restricted to the glottal stop. Note that, generally speaking, stops do not have plosion (a release burst). In English, for example, there are stops with no audible release, such as the in "apt". However, pulmonic stops do have plosion in other environments.

In Ancient Greek, the term for stop was ("áphōnon"), which means "unpronounceable", "voiceless", or "silent", because stops could not be pronounced without a vowel. This term was calqued into Latin as , and from there borrowed into English as "mute". "Mute" was sometimes used instead for voiceless consonants, whether stops or fricatives, a usage that was later replaced with "surd", from Latin "deaf" or "silent", a term still occasionally seen in the literature. For more information on the Ancient Greek terms, see .

All spoken natural languages in the world have stops, and most have at least the voiceless stops , , and . However, there are exceptions: Colloquial Samoan lacks the coronal , and several North American languages, such as the northern Iroquoian and southern Iroquoian languages (i.e., Cherokee), lack the labial . In fact, the labial is the least stable of the voiceless stops in the languages of the world, as the unconditioned sound change → (→ → Ø) is quite common in unrelated languages, having occurred in the history of Classical Japanese, Classical Arabic, and Proto-Celtic, for instance. Formal Samoan has only one word with velar ; colloquial Samoan conflates and to . Ni‘ihau Hawaiian has for to a greater extent than Standard Hawaiian, but neither distinguish a from a . It may be more accurate to say that Hawaiian and colloquial Samoan do not distinguish velar and coronal stops than to say they lack one or the other.

See Common occlusives for the distribution of both stops and nasals.

In the articulation of the stop, three phases can be distinguished:

In many languages, such as Malay and Vietnamese, word-final stops lack a release burst, even when followed by a vowel, or have a nasal release. See no audible release.

Nasal occlusives are somewhat similar. In the catch and hold, airflow continues through the nose; in the release, there is no burst, and final nasals are typically unreleased across most languages.

In affricates, the catch and hold are those of a stop, but the release is that of a fricative. That is, affricates are stop–fricative contours.

Voiced stops are pronounced with vibration of the vocal cords, voiceless stops without. Stops are commonly voiceless, and many languages, such as Mandarin Chinese and Hawaiian, have only voiceless stops. Others, such as most Australian languages, are indeterminate: stops may vary between voiced and voiceless without distinction.

In aspirated stops, the vocal cords (vocal folds) are abducted at the time of release. In a prevocalic aspirated stop (a stop followed by a vowel or sonorant), the time when the vocal cords begin to vibrate will be delayed until the vocal folds come together enough for voicing to begin, and will usually start with breathy voicing. The duration between the release of the stop and the voice onset is called the "voice onset time" (VOT) or the "aspiration interval". Highly aspirated stops have a long period of aspiration, so that there is a long period of voiceless airflow (a phonetic ) before the onset of the vowel. In tenuis stops, the vocal cords come together for voicing immediately following the release, and there is little or no aspiration (a voice onset time close to zero). In English, there may be a brief segment of breathy voice that identifies the stop as voiceless and not voiced. In voiced stops, the vocal folds are set for voice before the release, and often vibrate during the entire hold, and in English, the voicing after release is not breathy. A stop is called "fully voiced" if it is voiced during the entire occlusion. In English, however, initial voiced stops like or may have no voicing during the period of occlusion, or the voicing may start shortly before the release and continue after release, and word-final stops tend to be fully devoiced: In most dialects of English, the final /b/, /d/ and /g/ in words like "rib", "mad" and "dog" are fully devoiced. Initial voiceless stops, like the "p" in "pie", are aspirated, with a palpable puff of air upon release, whereas a stop after an "s", as in "spy", is tenuis (unaspirated). When spoken near a candle flame, the flame will flicker more after the words "par, tar," and "car" are articulated, compared with "spar, star," and "scar". In the common pronunciation of "papa", the initial "p" is aspirated whereas the medial "p" is not.

In a geminate or long consonant, the occlusion lasts longer than in simple consonants. In languages where stops are only distinguished by length (e.g., Arabic, Ilwana, Icelandic), the long stops may be held up to three times as long as the short stops. Italian is well known for its geminate stops, as the double "t" in the name "Vittoria" takes just as long to say as the "ct" does in English "Victoria". Japanese also prominently features geminate consonants, such as in the minimal pair 来た "kita" 'came' and 切った "kitta" 'cut'.

Note that there are many languages where the features voice, aspiration, and length reinforce each other, and in such cases it may be hard to determine which of these features predominates. In such cases, the terms fortis is sometimes used for aspiration or gemination, whereas lenis is used for single, tenuous, or voiced stops. Be aware, however, that the terms "fortis" and "lenis" are poorly defined, and their meanings vary from source to source.

Simple nasals are differentiated from stops only by a lowered velum that allows the air to escape through the nose during the occlusion. Nasals are acoustically sonorants, as they have a non-turbulent airflow and are nearly always voiced, but they are articulatorily obstruents, as there is complete blockage of the oral cavity. The term occlusive may be used as a cover term for both nasals and stops.

A prenasalized stop starts out with a lowered velum that raises during the occlusion. The closest examples in English are consonant clusters such as the [nd] in "candy", but many languages have prenasalized stops that function phonologically as single consonants. Swahili is well known for having words beginning with prenasalized stops, as in "ndege" 'bird', and in many languages of the South Pacific, such as Fijian, these are even spelled with single letters: "b" [mb], "d" [nd].

A postnasalized stop begins with a raised velum that lowers during the occlusion. This causes an audible nasal "release", as in English "sudden". This could also be compared to the /dn/ cluster found in Russian and other Slavic languages, which can be seen in the name of the Dnieper River.

Note that the terms "prenasalization" and "postnasalization" are normally used only in languages where these sounds are phonemic: that is, not analyzed into sequences of stop plus nasal.

Stops may be made with more than one airstream mechanism. The normal mechanism is pulmonic egressive, that is, with air flowing outward from the lungs. All languages have pulmonic stops. Some languages have stops made with other mechanisms as well: ejective stops (glottalic egressive), implosive stops (glottalic ingressive), or click consonants (lingual ingressive).

A fortis stop (in the narrow sense) is produced with more muscular tension than a lenis stop (in the narrow sense). However, this is difficult to measure, and there is usually debate over the actual mechanism of alleged fortis or lenis consonants.

There are a series of stops in the Korean language, sometimes written with the IPA symbol for ejectives, which are produced using "stiff voice", meaning there is increased contraction of the glottis than for normal production of voiceless stops. The indirect evidence for stiff voice is in the following vowels, which have a higher fundamental frequency than those following other stops. The higher frequency is explained as a result of the glottis being tense. Other such phonation types include breathy voice, or murmur; slack voice; and creaky voice.

The following stops have been given dedicated symbols in the IPA.

Many subclassifications of stops are transcribed by adding a diacritic or modifier letter to the IPA symbols above.





</doc>
<doc id="29482" url="https://en.wikipedia.org/wiki?curid=29482" title="Stayman convention">
Stayman convention

Stayman is a bidding convention in the card game contract bridge. It is used by a partnership to find a 4-4 or 5-3 trump fit in a suit after making a one (1NT) opening bid and it has been adapted for use after a 2NT opening, a 1NT overcall, and many other natural notrump bids.

The convention is named for Sam Stayman, who wrote the first published description in 1945, but its inventors were two other players: the British expert Jack Marx in 1939, who published it only in 1946, and Stayman's regular partner George Rapée in 1944.

A bid and made in a major suit (i.e. 4 or 4 ) scores better than a game contract bid and made in a minor suit (i.e. 5 or 5 ) or in notrump (i.e. 3NT). Also, the success rate for a game contract in a major suit when a partnership has a combined holding of 26 points and eight cards in the major is about 80%, whereas a game contract in 3NT with 26 (HCP) has a success rate of only 60%, or 50% with 25 HCP; the success rate for a minor suit game contract when holding 26 points is about 30%.

Accordingly, partnership priority is to find an eight card or better major suit fit when jointly holding sufficient values for a game contract. 5-3 and 6-2 fits are easy to find in basic methods as responder can bid 3 or 3 over 1NT, and opener will not normally have a 5 card major to bid 1NT. However, finding 4-4 fits presents a problem. The 2 and 2 bids cannot be used for this as they are weak takeouts, a sign-off bid.

After an opening bid or an overcall of 1NT (2NT), or bids an artificial 2 (3) to ask opener or overcaller if he holds a four- or five-card major suit; some partnership agreements may require the major to be headed by an honor of at least a specified rank, such as the queen. The artificial club bid typically promises four cards in at least one of the major suits (promissory Stayman) and, "in standard form", enough strength to continue bidding after partner's response (8 HCP for an invitational bid opposite a standard strong 1NT opening or overcall showing 15-17 HCP, 11 HCP opposite a weak notrump of 12-14 HCP, or 5 HCP to go to game opposite a standard 2NT showing 20-21 points). It also promises distribution that is not 4333. By invoking the Stayman convention, the responder takes control of the bidding since strength and distribution of the opener's hand is already known within a limited range. The opener responds with the following rebids.
A notrump opener should have neither a suit longer than five cards nor more than one 5-card suit since an opening notrump bid shows a balanced hand. A notrump bidder who has at least four cards in each major suit normally responds in hearts, as this can still allow a spade fit to be found. Variant methods are to bid the longer or stronger major, with a preference given to spades, or to use 2NT to show both majors.

In the standard form of Stayman over 1NT, the responder has a number of options depending on his partner's answer:
Over these bids, the notrump bidder (1) with a maximum hand (17 HCP), goes to game over an invitational bid and (2) with four (or more) cards in each major suit, corrects to the previously unbid major suit.

In the standard form of Stayman over 2NT, the responder has only two normal rebids.

In either case, a responder who rebids notrump over a response in a major suit promises four cards of the other major suit. Thus, a notrump opener who holds at least four cards in each major suit should "correct" by bidding the other major suit at the lowest level.

Of course, once a fit is found, responder who has sufficient strength also may bid 4 (Gerber) or 4NT (Blackwood), or cue bid aces, depending upon partnership agreement, to explore slam in any of the above sequences. Some partnerships also admit responder's rebids of a major suit that the notrump bidder did not name.

A bid of 4 over an opening bid of 3NT may be either Stayman or Gerber, depending upon the partnership agreement.

If an adverse suit bid is inserted immediately after a 1NT opening, Stayman may be employed via a double (by partnership agreement) or a cue bid, depending on the strength of his hand. The cue bid, which is conventional, is completely artificial and means nothing other than invoking Stayman. For example, if South opens 1NT, and West overcalls 2, North, if he has adequate values, may call 3, invoking Stayman. South would then show his major or bid game in notrump. Alternatively, North, if his hand lacks the values for game in notrump, may double, which by partnership agreement employs Stayman. This keeps the Stayman bidding at second level.

Partnerships who have not yet learned Stayman but choose to adopt Stayman (without having yet learned or having chosen not to use Jacoby Transfers) will need to adjust their use of normal two-level responses after a 1NT opening, because the availability of this convention changes the nature of what had been normal 1NT responses. When the notrump bidder's partner does not invoke Stayman but instead calls 2 or 2, it is a sign of relative weakness (since if responder held 8 HCP or more, he would have invoked Stayman). These bids are commonly referred to as "drop dead bids", as the opening notrump bidder is requested to withdraw from the auction. If opener has maximum values, a fit, and strong support, he may raise to the 3-level, but under no circumstances may he take any other action. This provides the partnership with an advantage that the non-Stayman partnership doesn't enjoy. For example, a responder may have no honors at all; that is, a total of zero HCP. His partner is likely to be set if he passes. A non-Stayman responder would have to pass, because to bid would provoke a rebid. But a Stayman responder can respond to his partner's 1NT opening at level 2 if he has a 6-card non-club suit. The responder with 3 HCP and a singleton can make a similar call with a 5-card non-club suit. This gives the partnership a better than even chance of success in making the contract, whereas without a response (and without Stayman), the contract would likely be set.

Similarly, a response of 2 indicates less than 8 HCP and should usually be passed. In rare cases, when the opener has maximum values and a fit in diamonds with at least two of the top three honors, he may raise diamonds, and responder may see a chance for game in notrump.

There are many variations on this basic theme, and partnership agreement may alter the details of its use. It is one of the most widely used conventions in bridge.

Some partnerships play that 2 Stayman does not absolutely promise a four-card major (non promissory Stayman). For example, if responder has a short suit and wishes to know if opener has four-card cover in it, so as to play in notrumps. If opener shows hearts initially, 2 can be used to find a fit in spades when the 2 does not promise a four-card major.

1NT - 2, 2 -
Alternatively 2 can be used for all hands with four spades and not four hearts, either invitational or game values, while 3NT denies four spades.

Today, most players use Stayman in conjunction with Jacoby transfers. With Stayman in effect, the responder practically denies having a five-card major, as otherwise he would transfer to the major immediately. The only exception is when responder has 5-4 in the majors; in that case, he could use Stayman, and in the case of a 2 response, bid the five-card major at the two level (weakness take-out / Garbage Stayman) or at the three level (forcing to game). However, the latter hand can also be bid by first using a transfer and then showing the second suit naturally. The Smolen convention provides an alternative method to show a five-card major and game-going values. A minor drawback of Jacoby transfers is that a 2 contract is not possible.

The Smolen convention is an adjunct to Stayman for situations in which the notrump opener has denied holding a four-card major and responder has a five-card major and a four-card major with game-going values.

If the notrump opener responds to the Stayman 2 asking bid with 2, denying a four-card major, responder initiates the Smolen Transfer with a jump shift to three of his four-card major. The jump shift shows which is the four-card major and promises five in the other major. The notrump opener then bids four of the other major with three cards in the suit or 3NT with fewer than three.

Smolen may also be used when responder has a six-card major and a four-card major with game-going values; after the 2 negative response by opener, responder double jump shifts to four in the suit just below his six-card major and the notrump opener transfers to four of his partner's six-card major.

This convention allows a partnership to find either a 5-3 fit, 6-3 and 6-2 fit while ensuring that the notrump opener, who has the stronger hand, will be declarer.

"Garbage" Stayman (or "Weak Stayman") and "Crawling" Stayman are adaptations of Stayman frequently used for damage control when holding a weak hand opposite a 1NT opening bid. Suppose you hold the following hand.

Your partner opens 1NT (15-17), and your right hand opponent passes. Now, what?

In this scenario, opener has about 16 HCP and the opponents have about 24 HCP. Thus, 1NT is virtually certain to go down by at least three or four tricks. Indeed, in Notrump, this dummy will be completely worthless.

But consider what happens if you bid 2 Stayman rather than passing on the first round, and then "pass opener's response". If opener rebids a major suit you have found a 4-4 fit and ability to trump club losers (or, alternately, to sluff the other major on club winners and then to trump losers in the other major). Likewise, a response of 2 guarantees no worse than a 5-2 fit in diamonds and, with a fifth trump, a potential additional ruff. The ability to reach dummy with a couple ruffs also may allow the declarer to take a couple finesses or execute a squeeze that otherwise would not be possible, and which might yield another trick or two. The result is a contract that will go down fewer tricks or that might even make, especially with a somewhat better hand than the example, rather than a contract that is virtually certain to go down at least three or four tricks. The practice of bidding Stayman with a relatively weak hand of this (or similar) shape and then passing the Notrump bidder's reply is often called "Garbage Stayman" because it is bidding Stayman with a "garbage" hand.

"Crawling Stayman" is an optional extension of "Garbage Stayman" for situations in which the responder's diamond suit is short. In "Crawling Stayman", the responder rebids 2 over the Notrump bidder's 2 reply. This conventional bid shows a weak hand with at least four cards in each major suit, asking the Notrump bidder to choose between the major suits at the cheapest level by either passing the 2 bid or correcting to 2. The name "Crawling Stayman" comes from the fact that the bidding "crawls" at the slowest possible pace: (pass) – 1NT – (pass) – 2; (pass) – 2 – (pass) – 2; (pass) – 2; (pass) – pass – (pass).

Alternatively, responder's 2 and 2 bids after the 2 rebid can be weak sign-offs. This allows responder to effectively bid hands which are 5-4 in the majors, by looking first for a 4-4 fit and, if none is found, signing off in his 5 card suit.

"Garbage Stayman" and "Crawling Stayman" bids over a 2NT bid work the same way, but occur at the "three" level.

If Jacoby transfers are not played, there are two approaches to resolve the situation when responder has a 5-card major but only invitational values. In one, more common, referred to as "non-forcing Stayman", in the sequence:
responder's simple rebid of a major suit is invitational, showing 8-9 points and a 5-card spade suit. In the "forcing Stayman" variant, the bid is one-round forcing.

In the original Precision Club system, forcing and non-forcing Stayman are differentiated in the start: 2 by responder shows only invitational values (and the continuation is the same as in basic Stayman), while 2 is forcing to game (responder bids 2NT without majors).

This allows responder to find exact shape of 1NT opener. Developed for use with weak 1 NT opening. Relay bids over opener's rebids of 2, 2, 2, 2NT, 3 allow shape to be defined further if attempting to find 5-3 major fits. Advantages are responder's shape, which may be any distribution, is undisclosed, and responder is able to locate suit shortage holdings not suitable for no trumps. Disadvantage is 2 can't be used as a damage control bid.

1NT – 2♣

Developed to be used in combination with following other responses to 1NT: 2, 2 Jacoby transfers to majors; 2 range finder/transfer to minors (opener's rebids: 2NT 12-13 HCP, 3 14 HCP. Responder passes or corrects to 3 or 3 sign off if weak. After opener's 3 rebid responder bids 3 to show 4 hearts or 3 to show 4 spades both game forcing. Responder's rebid of 3NT denies 4 card major); 2NT invitational hand with both 4 card majors (opener's rebids: no bid no 4 card major 12-13 HCP, 3 4 hearts 12-13 HCP, 3 4 spades 12-13 HCP, 3 4 hearts 14 HCP, 3 4 spades 14 HCP, 3NT 14 HCP no 4 card major)

This allows responder to find exact shape of 1NT opener that may only contain a four-card major. Developed for use with weak 1 NT opening. Relay bids over opener's rebids of 2, 2, 2 allow shape to be defined further if attempting to find 5-3 major fits. Advantages are responder's shape, which may be any distribution, is undisclosed, and responder is able to locate suit shortage holdings not suitable for notrumps. May be also used as a damage control bid, and for both invitational, and game forcing hands.

1NT – 2♣

1NT – 3♣ weak sign off.

Opener's rebids of 2, 2, 2 may all be passed if responder is weak.

Developed to be used in combination with following other responses to 1NT: 2, 2 Jacoby transfers to majors; 2 five spades four hearts 10-11 HCP; 2NT invitational hand with 5,5 minors 10-11 HCP.

This allows responder to find exact shape of 1NT opener that may contain a 5 card major. Developed for use with weak 1NT opening. Relay bids over opener's rebids of 2D, 2H, 2S allow shape to be defined further if attempting to find 5-3 major fits. Advantages are responder's shape, which may be any distribution, is undisclosed, and responder is able to locate suit shortage holdings not suitable for no trumps. May be also used as a damage control bid, and for both invitational, and game forcing hands.

1NT – 2C

Opener's rebids of 2D, 2H, 2S may all be passed if responder is weak.

Developed to be used in combination with following other responses to 1NT: 2D, 2H Jacoby transfers to majors; 2S range finder/transfer C; 2NT invitational hand with 5,5 minors 10-11 HCP.

This allows responder to check for 5-3 major fits where it is possible that opener's 1NT or 2NT might include a five card major. As described by Australian Ron Klinger, it can be played with a weak or strong 1NT.

1NT - 2

1NT - 2, 2 OR 2NT

After a transfer, accept it with any 4333, bid 3NT with only two trumps, otherwise bid 4M.

1NT - 2, 2 OR 2NT - 3 = Stayman

1NT - 2, (2 OR 2NT) - 3, 3

An alternative, simpler version of 5 card Stayman is:

1NT - 2

This structure permits use by weak hands with 5+ diamonds and 2+ cards in each major.

After 1NT - 2, 2

If responder has a five-card major, he begins with a transfer. After completion of the transfer, bidding the other major at the three level shows four cards in it and a game forcing hand, in line with the 1NT - 2, 2 structure above (1NT - 2, 2 - 2 = invitational 5-4).

Similarly after 2NT - 3, 3

A drawback of Five Card Major Stayman (particularly the simpler version) is that the weaker hand may become declarer in a 4-4 major fit.

Puppet Stayman is similar to Five Card Stayman. It is more complex but has the major advantage that the strong hand virtually always becomes declarer.

Initially developed by Neil Silverman and refined by Kit Woolsey and Steve Robinson in 1977-78, is a variation of the Stayman convention designed to find a 5-3 fit in a major, augmenting the search for a 4-4 major fit by standard Stayman. In 1977, Woolsey wrote that Puppet Stayman has several advantages over standard Stayman:

As in standard Stayman, Puppet Stayman begins with a 2 response to a 1NT opening and is at least game invitational; this asks opener to bid a 5-card major if he has one and otherwise to bid 2. Over a 2 response, rebids by responder are intended to disclose his distributional features in the majors as well as his strength. The original 1977 and 1978 revised rebids described by Woolsey are tabulated below: 
Opener and responder continue the bidding having a clearer understanding of each other's distributional features and are better positioned to select the ultimate and level of the contract.

Many variations to the Puppet Stayman bidding structure have been devised since Woolsey's 1978 summary; partnership review and agreement on the preferred modern treatment is required.

Some no longer advocate use of Puppet Stayman over a 1NT opening preferring to use the concept exclusively over a 2NT opening and reserving other Stayman variations and conventions such Jacoby Transfers and Smolen Transfers in search of major-suit fits after a 1NT opening.

Puppet Stayman is more commonly used after a 2NT opening than after a 1NT opening. Responses to a 2NT opening or very strong 2NT rebid (20-22 or 23-24):

Responder bids 3 seeking information about opener's major suit holding. Opener replies:

By this means all 5-3 and 4-4 major suit fits can be found.

An alternative pattern frees up the 2NT-3 sequence as a slam try in the minors. To allow 3-5 spade fits to be found when responder holds 5 spades and 4 hearts, some of the responses change:

2 Checkback Stayman (or simply Checkback) is used after a 1NT rebid by opener rather than a 1NT opening. It is used to "check back" if opener has major suit support, saying nothing additional about the club suit. It can find 3-5 fits, 4-4 fits (in Standard American) and 5-3 fits (in Acol), and also shows whether opener was maximum or minimum strength for his notrump bid. In five-card major systems, bidding Checkback implies that the responder has five cards in his major, and may have four in the other.

1m – 1M; 1NT – 2

The 2 is "Checkback Stayman". Responses by opener shows the following:

Partnership agreement is required on how to handle the case of holding four of the other major and three of partner's suit. One could agree to bid up the line, or support partner's suit first. If partner cannot support your first suit, he will invite with 2NT or bid game with 3NT and you will then correct to your other suit.

In Acol, if the opening bid was a major, opener can rebid his major after a Checkback inquiry to show that it has five cards rather than four and find 5-3 fits. Moreover, 1M – 2m; 2NT – 3 can also be used as Checkback Stayman. It is useful also to include an indication of range, particularly if opener's 2NT rebid is forcing to game and shows a wide points range (15-19). This is achieved by using 3 for minimum hands and 3/3/3NT for maximum hands, or vice versa. After 3, responder can still bid 3/3 to look for a 5-3 fit.

New Minor Forcing is an alternative to Checkback Stayman where either 2 or 2 can be used as the checkback bid. It can be used by responder with invitational values or better to find three-card support for his major or to find a 4-4 heart fit if holding five spades and four hearts); it also allows a return to the minor to play.



</doc>
<doc id="29483" url="https://en.wikipedia.org/wiki?curid=29483" title="Saks Fifth Avenue">
Saks Fifth Avenue

Saks Fifth Avenue is an American luxury department store owned by the oldest commercial corporation in North America, the Hudson's Bay Company. Its main flagship store is located on Fifth Avenue in Midtown Manhattan, New York City.

Saks Fifth Avenue is the successor of a business founded by Andrew Saks in 1867 and incorporated in New York in 1902 as Saks & Company. Saks died in 1912, and in 1923 Saks & Co. merged with Gimbel Brothers, Inc., which was owned by a cousin of Horace Saks, Bernard Gimbel, operating as a separate autonomous subsidiary. On September 15, 1924, Horace Saks and Bernard Gimbel opened Saks Fifth Avenue in New York City, with a full-block avenue frontage south of St. Patrick's Cathedral, facing what would become Rockefeller Center. The architects were Starrett & van Vleck, who developed a reticent, genteel Anglophile classicizing facade similar to their Gimbels Department Store in Pittsburgh (1914).

When Bernard's brother, Adam Gimbel, became president of Saks Fifth Avenue in 1926 after Horace Saks's sudden passing, the company expanded, opening seasonal resort branches in Palm Beach, Florida and Southampton, New York, in 1928. The first full-line year-round Saks store opened in Chicago, in 1929, followed by another resort store in Miami Beach, Florida. In 1938, Saks expanded to the West Coast, opening in Beverly Hills, California. By the end of the 1930s, Saks Fifth Avenue had a total of 10 stores, including resort locations such as Sun Valley, Idaho, Mount Stowe, and Newport, Rhode Island. More full-line stores followed with Detroit, Michigan, in 1940 and Pittsburgh, Pennsylvania, in 1949. In Downtown Pittsburgh, the company moved to its own freestanding location approximately one block from its former home on the fourth floor in the downtown Gimbel's flagship. The San Francisco location opened in 1952, competing locally with I. Magnin. BATUS Inc. acquired Gimbel Bros., Inc. and its Saks Fifth Avenue subsidiary in 1973 as part of its diversification strategy. More expansion followed from the 1960s through the 1990s including the Midwest, and the South, particularly in Texas. In 1990, BATUS sold Saks to Investcorp S.A., which took Saks public in 1996 as Saks Holdings, Inc.

In 1990, "Saks Off 5th" was launched, an outlet store offshoot of the main brand, with 107 stores worldwide by 2016.
In 1998, Proffitt's, Inc. the parent company of Proffitt's and other department stores, acquired Saks Holdings Inc. Upon completing the acquisition, Proffitt's, Inc. changed its name to Saks, Inc.

Since 2000 Saks has opened international locations in Saudi Arabia, United Arab Emirates, Bahrain, Kazakhstan, Canada, and Mexico City.

In August 2007, the United States Postal Service began an experimental program selling the plus zip code extension to businesses. The first company to do so was Saks Fifth Avenue, which received the zip code of 10022-7463 ("SHOE") for the eighth-floor shoe department in its flagship Fifth Avenue store.

During the 2007–2009 recession, Saks Fifth Avenue had to close some stores and to cut prices and profit margins, thus according to Reuters "training shoppers to expect discounts. It took three years before it could start selling at closer to full price". In the following years, the company closed stores in locations including Orange County (2010), Denver (2011), Pittsburgh (2012), Chicago (2012/13) and in June 2013 its last Dallas store to implement the "strategy of employing our resources in our most productive locations".
As of 2013, the New York flagship store, whose real estate value was estimated between $800 million and over $1 billion at the time, generated around 20% of Saks' annual sales at $620 million, with other stores being less profitable according to analysts.
On July 29, 2013, the Hudson's Bay Company (HBC), owner of the competing chain Lord & Taylor, announced it would acquire Saks Fifth Avenue's parent company for US$2.9 billion. Plans called for up to seven Saks Fifth Avenues to open in major Canadian markets. Expansion into Canada is expected to compete with Canadian Holt Renfrew chain and challenge Nordstrom's expansion into Canada, which began in summer 2014 with the opening of a Nordstrom store in Calgary. In January 2014, HBC announced the first Saks store in Canada would occupy in its flagship Queen Street building in downtown Toronto, connected to the Toronto Eaton Centre via sky bridge. The store opened in February 2016 with a second Toronto area location in the Sherway Gardens shopping center opening in spring 2016. On February 22, 2018, Saks Fifth Avenue opened its third Canadian store in Calgary, Alberta.

Starting in 2015 Saks began a $250 million, three-year restoration of its Fifth Avenue flagship store. In the summer of 2015, it was announced that Saks will debut a new location in Greenwich, Connecticut. In the fall of 2015, Saks was planning to replace its existing store at the Houston Galleria with a new store.

In February 2017, Saks was reported to be in advanced talks with Indian retailer Aditya Birla Fashion Retail Ltd. to open two stores in India. The stores are expected to be located at Aerocity in Delhi, and the Bandra Kurla Complex in Mumbai.

In September 2017, Saks Fifth Avenue would be introducing new futuristic salon concept at stores through a partnership with the Warren Tricomi. 

In 2005, vendors filed against Saks alleging unlawful chargebacks. The U.S. Securities and Exchange Commission (SEC) investigated the complaint for years and, according to the "New York Times", "exposed a tangle of illicit tactics that let Saks... keep money it owed to clothing makers", inflating Saks' yearly earnings up to 43% and abusively collecting around $30 million from suppliers over seven years. Saks settled with the SEC in 2007, after firing three or more executives involved in the fraudulent activities.

In 2014, Saks fired transgender employee Leyth Jamal after she was allegedly "belittled by coworkers, forced to use the men's room and repeatedly referred to by male pronouns (he and him)". After Jamal submitted a lawsuit for unfair dismissal, the company stated in a motion to dismiss that "it is well settled that transsexuals are not protected by Title VII." In a court filing, the United States Department of Justice rebuked Saks' argument, stating that "discrimination against an individual based on gender identity is discrimination because of sex." The company was removed from the Human Rights Campaign's list of "allies" during the controversy. The lawsuit was later settled amicably, without disclosing the terms of the settlement.

In 2017, following the events of Hurricane Maria in Puerto Rico, Saks's San Juan store located in Mall of San Juan suffered major damages along with its neighboring anchor store Nordstrom. Taubman Centers, the company who owns the mall, filed a lawsuit against Saks for failing to provide an estimated reopening date and failing to restore damages after the hurricane.




</doc>
<doc id="29484" url="https://en.wikipedia.org/wiki?curid=29484" title="Seabee">
Seabee

United States Naval Construction Battalions, better known as the Seabees, form the Naval Construction Force (NCF) of the United States Navy. Their nickname is a heterograph of the first initials "C.B." from the words Construction Battalion. Depending upon the use of the word, "Seabee" can refer to one of three things: all the enlisted personnel in the USN's occupational field-7 (OF-7), all officers and enlisted assigned to the Naval Construction Force, or the U.S. Naval Construction Battalions (CBs). 
Naval Construction Battalions were conceived of as a replacement for civilian construction companies working for the US Navy after the United States was drawn into World War II with the Japanese attack on Pearl Harbor on 7 December 1941. At that time the U.S. had roughly 70,000 civilians working on military installations overseas. International law made it illegal for them to resist enemy attack, as to do so would classify them as guerrillas, for which they could be summarily executed which is exactly what happened when the Japanese invaded Wake Island.

The Seabees would consist of skilled workers that would be trained to drop their tools if necessary and take up their weapons at a moment's notice to defend themselves. The concept model was that of a USMC–trained battalion of construction tradesmen (a military equivalent of those civilian companies) that would be capable of any type of construction, anywhere needed, under any conditions or circumstance. It was quickly realized that this model could be utilized in every theater of operations, as it was seen to be flexible and adaptable.

The use of USMC organization allowed for smooth co-ordination, integration or interface of both the NCF and Marine Corps elements. In addition, Seabee Battalions could be deployed individually or in multiples as the project scope and scale dictated. What distinguishes Seabees from Combat Engineers are the skill sets. Combat Engineering is but a sub-set in the Seabee toolbox. They have a storied legacy of creative field ingenuity, stretching from Normandy and Okinawa to Iraq and Afghanistan. Admiral Ernest King wrote to the Seabees on their second anniversary, ""Your ingenuity and fortitude have become a legend in the naval service."" Seabees believe that anything they are tasked with they "Can Do" (the CB motto). They were unique at conception and remain so today. In the October 1944 issue of "Flying" magazine the Seabees are described as ""a phenomenon of World War II"". In 2017, the Seabees celebrated their 75 years of service without having changed from Admiral Ben Moreell's conceptual model. An acronym used today that is descriptive of the Seabee model is STEM.

In 1917, the Twelfth Regiment (Public Works) was organized at Naval Training Station Great Lakes. When the US entered World War I in April 1917, the Navy had an immediate requirement to expand the Great Lakes Station in order to house, process, and train 20,000 naval recruits, this number would rise to 50,000 by the end of the year. Lieutenant Malachi Elliott, a graduate of the US Naval Academy, was appointed Public Works Officer at Great Lakes on 18 June 1917, at which time about 500 enlisted men had been assigned to the Public Works Department. Seeing that the department would need to expand with skilled craftsmen, architects, draftsmen, designers, and other professional and technical people, he began to screen incoming recruits with these skills. Finding many, but not enough, he expanded to recruiting civilians outside of the installation, getting many men willing to join the Navy as petty officers, with the understanding that qualified men could later apply for commissions. This allowed the Public Works Department to grow to nearly 600 men by July 1917. They were organized into the Twelfth Regiment (Public Works), which was essentially the Public Works Department because staff officers could not exercise military command. Lieutenant William C. Davis was appointed commanding officer of the regiment, he exercised military control, but the Public Works Officers exercised technical control. In October 1917, the regiment began building Camp Paul Jones at San Diego. With its completion, on 30 December 1917, the regiment became "fully operational" with 1,500 men organized into three battalions. By April 1918, the regiment consisted of 2,400 in five battalions. Men were withdrawn for assignments in the US and abroad. In spring of 1918, 100 men were given special mechanics and ordnance training before being sent to St. Nazaire, France, to assemble Naval Railway Batteries. Later they would join the gun crews and perform combat duties along the railway lines in proximity to the German lines. The Twelfth Regiment reached its peak strength 5 November 1918; 55 officers and 6,211 enlisted men formed into 11 battalions. However, with the end of the war on 11 November 1918, the regiment gradually faded away by the end of 1918.

In the early 1930s, the idea that the Twelfth Regiment pioneered was still in the minds of many Navy Civil Engineers. The planners of the Bureau of Yards and Docks (BuDocks) began providing for "Navy Construction Battalions" in their contingency war plans. In 1934 Captain Carl Carlson's version of the plan was circulated to the Navy Yards, this idea of "Navy Construction Battalions" would later be tentatively approved by Chief of Naval Operations, Admiral William Harrison Standley. In 1935, Rear Admiral Norman Smith, Chief of BuDocks, selected Captain Walter Allen, the War Plans Officer, to represent BuDocks on the War Plans Board. Captain Allen presented the bureau's concept of "Naval Construction Battalions" to the Board. The concept was later adopted for inclusion in the Rainbow war plans. However, a major weakness to this "Navy Construction Battalions" concept was that there would be dual control of the battalions; military control would be exercised by Navy officers while the construction side would be controlled the Navy Civil Engineer Corps officers. There would be no provision for good military organization and military training, which was felt to be requisite to creating high morale, discipline, and cooperation among the men. The plans also only allowed for the battalions to be formed to build training stations throughout the US and only on completion be moved to forward areas. Rear Admiral Ben Moreell became the Chief of BuDocks in December 1937, a post he would hold through the war. With tensions rising in both Europe and Asia, authorization was sought, and quickly received, by the United States Congress for expansion of naval shore bases. New construction was started in the Caribbean and Central Pacific in 1939. These were awarded to private construction firms that would perform the work with civilian personnel under the administrative direction of a Navy Officer in Charge of Construction.

By summer of 1941 civilian contractors were working on large naval bases at Guam, Midway, Wake, Pearl Harbor, Iceland, Newfoundland, Bermuda, and many other places. BuDocks decided there was a need to improve the Navy's supervision of these projects through the creation of "Headquarters Construction Companies". The men in these companies would report to the Officers in Charge of Construction and would be draftsmen and engineering aids needed for the administrative functions of the inspectors and supervisors overseeing the contracted work. These companies would consist of two officers and 99 enlisted men, but were not to do any actual construction. Rear Admiral Chester Nimitz, Chief of the Bureau of Navigation, authorized the formation of the first Headquarters Construction Company, on 31 October 1941. Recruitment started in November and as history would have it the company was formed on 7 December with the men undergoing boot training at Naval Station Newport, Rhode Island. By 16 December 1941, four additional companies had been authorized, but 7 December happened, plans changed and with them the ratings needed by a change in mission. The first HQ Construction Company provided the nucleus for the formation of the 1st Naval Construction Detachment sent to Bora Bora in January 1942. Those men were part of Operation Bobcat and are known in Seabee history as the "Bobcats". In December 1941, Rear Admiral Ben Moreell, Chief of BuDocks, recommended establishing Navy Construction Battalions and on the 28th requested authority to carry this out. On 5 January 1942, he got the go-ahead from the Navy's Bureau of Navigation to recruit construction tradesmen for three Naval Construction Battalions. When Admiral Moreell submitted his request to form those Battalions the other four HQ Construction Companies had been approved and authorized, so HQ Companies 2 & 3 were combined to form the 1st Naval Construction Battalion (and then were deployed as the 2nd & 3rd Construction Detachments) followed by HQ Companies 4 & 5 being combined to form the 2nd Naval Construction Battalion (and deployed as the 4th and 5th Construction Detachments). While those four HQ Companies provided the nucleus for two Construction Battalions they were all deployed in a manner similar to the First Construction Detachment and this sort of thing continued through the 5th NCB. It was 6 NCB that was the first Battalion to deploy as a unit to the same place.

Before all this could happen, a major problem still confronting BuDocks was who would command the Construction Battalions. Naval regulations stated that military command of naval personnel was strictly limited to line officers, yet BuDocks deemed it essential that these Construction Battalions be commanded by officers of the Civil Engineer Corps, who were trained in the skills required for construction work. The newly formed Bureau of Naval Personnel (BuPers), successor to the Navy's Bureau of Navigation, strongly opposed this proposal. Admiral Moreell took the question personally to the Secretary of the Navy, Frank Knox, who, on 19 March 1942, gave authority for officers of the Civil Engineer Corps to exercise military authority over all officers and enlisted men assigned to construction units. Two weeks prior on March 5, all Construction Battalion personnel were officially named Seabees by the Department of the Navy. Seabees have since observed that date as their birth date.

The first men in the Seabees were not raw recruits trade wise, they were recruited for their experience and skills and were given advanced rank for it. As a group they were the highest paid the United States had in uniform during WWII. To find the men with the necessary qualifications, physical standards were less rigid than other branches of the armed forces. The age range was 18–50, with the average of 37, during the first years of the war. Even so- they all were put through the same physical training. These first men had helped build Hoover Dam, the national highways, and New York's skyscrapers; who had worked in mines and quarries and dug subway tunnels; who had worked in shipyards and built docks and wharfs and even ocean liners and aircraft carriers. After December 1942, President Franklin D. Roosevelt ordered that men for the Construction Battalions had to be obtained through the Selective Service System. By that time 60 CBs had been formed. However, men could enlist and then volunteer for the Seabees with a written statement that they were trade qualified. This lasted until October 1943 when voluntary enlistment in the Seabees ceased until December 1944. During this period the recruits were generally younger and had much less developed skill sets due to their age. By the end of the war 325,000 had enlisted in the Seabees, with training in more than 60 skilled trades. Almost 11,400 officers would join the Civil Engineer Corps during World War II with 7,960 of them having served with the Seabees.

Recruits would receive three weeks of training at Camp Allen, Norfolk, Virginia, later Camp Bradford, Little Creek, Virginia and later still Camp Peary NTC, in Williamsburg, Virginia(Fig. 5). The first five battalions were sent directly overseas because of the urgent need of immediate construction of war dictated infra-structure. The newly formed battalions that followed, would be sent to one of the Advance Base Depots and Naval Training Centers (NTC) at Davisville, Rhode Island, Gulfport, Mississippi, or Port Hueneme, California. The Davisville Advanced Base Depot became operational in June 1942, and on 11 August 1942, the Naval Construction Training Center (NTC), known as Camp Endicott, was commissioned. That Camp trained over 100,000 Seabees during World War II. Camp Thomas, a personnel-receiving station on the base, was established in October. Camp Rousseau at Port Hueneme became operational in May 1942. This base was responsible for staging about 175,000 Seabees directly to the efforts in the Pacific. The other CB Camps were: Camp Hollyday, Gulfport Mississippi, Camp Parks, Livermore, California, and Camp Lee-Stephenson, Quoddy, Maine.
CBs sent to the Pacific were attached to one of the four Amphibious Corps: I, III, and V were U.S. Marine Corps (under Admiral Nimitz - Pearl Harbor, Territory of Hawaii) while VII was U.S. Army (under General MacArthur - Brisbane, Australia).

The original purpose of the Seabees was the construction of Advance Bases in the Pacific as laid out by the Office of Naval Operations. These bases were code-named: i.e. BOBCAT (this was the small first Advance Base Operation at Bora Bora), and then came LIONs, CUBs, OAKs and ACORNs. The names were metaphors for base size with LION being a Main Fleet Advance Base (these were numbered 1–4 with Lion 1 on Espiritu Santo). A CUB was a Secondary Fleet Base (these were numbered 1–12, starting with Efate, Tongatabu,and Munda) and were 1/4 the size of a Lion. OAK and ACORN were the names given repurposed enemy air bases captured in an amphibious assault. (CBs constructed, repaired or upgraded 111 major airfields with the number of Acorn fields unknown) Acorn 1 was built at Aola, Guadalcanal, Acorn 8 was on Munda, Acorn 15 was Bougainville, Acorn 17 was on Tarawa. and Acorn 23 was on Kwajalein When these plans were drawn up it was thought that two CBs would be what was needed to construct a Lion installation. This basic idea so grew and evolved that with the invasion of Okinawa the U.S. Navy put 4 Naval Construction Brigades of 55,000 Seabees on that island. This was not Combat Engineering. This was building the infra-structure required to take the War to Japan. Along the way, the Navy had realised that it also needed Advance Base Construction Depots (ABCDs) to get the job done. So the Seabees built them at: 1. Nouméa, 2. Pearl Harbor, 3. Brisbane, 4. Milne Bay, 5. Samar, 6. Subic Bay, and 7.Okinawa). By the end of 1943 the Seabees had constructed over 300 different advanced bases on as many islands. More than 325,000 men served with the Seabees in World War II, fighting and building on six continents and hundreds of islands. In the Pacific, they built 111 major airstrips, 441 piers, bridges, roads, tanks for the storage of of fuel, hospitals for 700,000 patients, and housing for 1.5 million men.

USMC historian Gordon L. Rottman wrote "that one of the biggest contributions the Navy made to the Marine Corps during WWII was the creation of the Seabees". In turn, the Corps would be influential upon the CB organization and its history. In 1942 the Marines issued USMC duffel bags and uniforms to Battalions 17-20. In the records of both the 18th and 19th NCBs they each claim to have been the first CB authorized to wear the USMC uniform. They both received their military training and USMC issue at Marine Training Center, New River, N.C. Marine Corps Base Camp Lejeune. How many other Battalions received the USMC issue is not recorded but it is known that the 25th, 31st, 43rd, 76th, 121st and 133rd NCBs did also. The Marine Corps listed CBs on their Table of organization: "D-Series Division" for 1942, "E-Series Division" for 1943, and "Amphibious Corps" for 1944/45 But, starting with the 1st Naval Construction Detachment (a.k.a. Bobcats), the Marines redesignated them the 3rd Battalion 22nd Marines. They were the very first Seabees and that was only the beginning. Right after them part of the 4th Naval Construction Detachment was assigned to the 5th Marine Defense Battalion on Funafuti for two years. The Bureau of Yards and Docks original request of 28 December 1941 was for the authorization of three Naval Construction Battalions. When those three Battalions were formed the Seabees did not have a fully functional base of their own. So, upon leaving Navy boot camp, those men were sent to National Youth Administration camps in Illinois, New Jersey, New York and Virginia to receive military training from the Marine Corps.

It is written that at that time the Marine Corps wanted a Seabee Battalion for each Division in the Pacific, but was told no because of war priorities. However, by autumn 1942, things changed with NCBs 18, 19 and 25 being assigned to Marine Divisions as combat engineers. Those Battalions were posted to composite Engineer Regiments and redesignated as the 3rd Battalion in their Regiment. (see 16th Marine Regiment, 17th Marine Regiment(Fig. 8), 18th Marine Regiment, 19th Marine Regiment, and 20th Marine Regiment) In August 1942 C Company 18th NCB was transferred to the C.B. Replacement Group, Fleet Marine Force, San Diego. The rest of the 18th embarked from the Fleet Marine Force Base Depot, Norfolk, VA, en route to Guadalcanal where they would replace the 6th NCB with the 1st Marine Division. In the fall of 1943 two sections of the 6th Special NCB were sent to the Russell Islands with the 4th Marines Advance Depot. In November the 14th NCB landed with the 2nd Raider Battalion on Guadalcanal. Earlier in June the 24th NCB supported the landing of the 9th Marine Defense Battalion on Rendova. The 33rd CB had 202 men posted to the 1st Pioneers as shore party to the 5th Marines of the 1st Marine Division on Peleliu. Along with them were 241 from the 73rd CB. Also attached to the 1st Pioneers was the entire 17th Special NCB (segregated). The 47th sent a detachment to Enogi Inlet on Munda attached to the 1st and 4th Marine Raiders. On Bougainville Commander Brockenbrough of the 71st CB was named the shore party commander for the 3rd Marine Division with his Battalion supported by elements of the 25th, 53rd, and the 75th CBs (and as well as the Marines). The 75th had a 100-man detachment volunteer to land with a Company of 3rd Marines at Cape Torokina, Bougainville. The 53rd also had detachments land as shore parties for the 2nd Raider Battalion on green beach and the 3rd Raider Battalion over on Puruata Island.(Fig. 2) The 121st was formed at NCB Training Center of the Marine Training Center - Camp Lejuene, New River, NC. There it was attached to the 4th Marine Division and assigned as the 3rd Battalion 20th Marines. In 1944 the Marine Engineer Regiments were inactivated. Even with the Engineer Regiments inactivated Marine Divisions still had a CB Battalion posted to them. For Iwo Jima, the 133rd and 31st NCBs were on temporary duty assignment (TAD) to the 4th and 5th Marine Divisions. For Okinawa it was the 58th, 71st, and 145th NCBs that were TAD to the 6th, 2nd, and 1st Marine Divisions. But, going back again to Iwo Jima, there the 31st and 133rd were not re-designated. The Marines were short of Marines and the Seabees were ordered to fill in. C Co 31st NCB was a component of the 5th Shore Party Regiment and was on the beach on D-day. The 31st NCB's Demolitions echelon was under divisional control through D+10 with the 5th Marine Division. The 133rd was posted to the 23rd Marines as their Shore Party. The Battalion had each Company detached and tasked to the assault as follows: A Co – 1/23, B Co – 2/23, C Co – 3/23, and D Co – 2/25 (see Naval Mobile Construction Battalion 133).

With Iwo Jima secured the 5th Marine Division returned to Camp Tarawa where it was joined by the 116th NCB. In August Japan fell and 116th NCB went with the 5th Marine Division as part of the occupation force. V-J day found thousands of Japanese troops still in China and the Third Marine Amphibious Corps was sent there to get them back to Japan. A portion of the 33rd Naval Construction Regiment was assigned to III Marine Amphibious Corps for this mission: i.e. the 83rd, 96th, 122nd CBs and the 33rd Special CB.

As just mentioned CB Battalions were also attached to the various Amphibious Corps. The 19th NCB was assigned to the I Marine Amphibious Corps (I MAC) prior to being assigned to the 17th Marines. The 53rd NCB was also posted to I MAC as an element of the 1st Provisional Marine Brigade. For Guam, III Amphibious Corps had the 2nd Special NCB and 25th NCB. V Amphibious Corps (VAC) had the 23rd Special and 62nd NCBs on Iwo Jima. Back on Tinian the 6th Naval Construction Brigade incorporated VAC's insignia as a part of the Brigade's indicating the entire brigade was attached to V Amphibious Corps. (the 6th Brigade was composed of: the 29th Rgt. with CBs; 18, 50, 92, 107, & 135, the 30th Rgt. with CBs: 13, 67, 121,& 123, and the 49th Rgt. with CBs: 9, 38, 110, & 112th (and the 27th Special).

With the war over the Seabees ended up with the most unique standing any U.S. military component has with the U.S. Marine Corps. Seabee historian William Bradford Huie wrote "that the two have a camaraderie unknown else-wheres in the United States military". It should be added that even though they are "Navy" the Seabees adopted USMC fatigues with a Seabee insignia in place of the globe and anchor. During WWII a number of CBs adapted USMC insignia for their units, these included CBs 19, 25, 53, 117 and the 6th Brigade. The insignia modified were the globe and anchor, bulldog, gator with three stars, and a divisional crest.


Naval Combat Demolition Units: NCDUs – Underwater Demolition Teams: UDTs

In early May 1943, a two-phase "Naval Demolition Project" was directed by the Chief of Naval Operations "to meet a present and urgent requirement". The first phase began at Amphibious Training Base (ATB) Solomons, Maryland with the establishment of Operational Naval Demolition Unit No. 1. Six Officers and eighteen enlisted men reported from NTC Camp Peary dynamiting and demolition school, for a four-week course. Those Seabees were immediately sent to participate in the invasion of Sicily. Naval Combat Demolition Units (NCDUs) consisted of one officer and five enlisted and were numbered 1–212. After that first group had been trained Lt. Commander Draper Kauffman was selected to Command the program that had been set up in Camp Peary's "Area E" close to the Seabee Dynamiting and Demolition school. Six classes were graduated from Camp Peary before the program was moved to Fort Pierce. Another reason for the initial NCDU location being at Camp Peary was that the Joint Army Navy Experimental Testing(JANET) site, for beach obstacle removal Project DM-361, was located at Camp Bradford temporarily late 1942-43. Later, despite the move to Fort Pierce, Camp Peary was Kauffman's manpower source. "He would go up to Camp Peary's Dynamite School, assemble the (Seabees) in the auditorium and say, 'I need volunteers for hazardous, prolonged and distant duty." Fort Pierce had Construction Battalion Unit 1011 assigned to the school. Its job was to construct and maintain the various obstacles needed for the demolitions class to practice their training. The men in those first classes referred to themselves as "Demolitioneers". The NCDUs had 34 teams in England for the invasion of Normandy.(all told they suffered 53 percent casualties on Normandy). While waiting for D-day the NCDUs trained with the 146th, 277th and 299th Combat Engineer Battalions. Each NCDU had 5 men from a Combat Engineer Battlion attached to the team. In the beginning the first 10 NCDUs were split into 3 groups. The whole thing was a bit ad-hoc as they had no Commanding Officer, but the Senior officer was the leader of group III, Lt Smith (CEC). He served in that capacity unofficially for the entire group. His group III did a lot of experimental demolitions work and developed the Hagensen Pack. As more teams arrived a NCDU Command was created for the invasion. With Europe invaded most of the NCDUs were sent to Fort Pierce and integrated into the UDTs for the Pacific campaign. However, 30 NCDUs were also sent to the Pacific with NCDUs 1–10 staged at Turner City, Florida Island in the Solomons during January 1944. A few were temporarily attached to UDTs. Later NCDUs 1–10 were combined to form Underwater Demolition Team Able. This team was disbanded with NCDUs 2 and 3 plus three others assigned to MacArthur's 7th Amphibious force and were the only NCDUs remaining at the war's end. The other men from Team Able were assigned to numeric UDTs.

In November 1943 the Navy learned a hard lesson with the invasion of Tarawa. Admiral Kelly Turner ordered the formation of nine Underwater Demolition Teams. UDTs 1 & 2 were completely Seabees according to the UDT Archives with Seabees making up the vast majority of the men in teams 1–9 and 13 (typically referred to as the Seabee Teams). Seabees were roughly 20% of UDT 11. The officers were mostly CEC. The first Underwater Demolition Team commander was Cmdr. E.D. Brewster (CEC) UDT 1. When Teams 1 and 2 were initially formed they were "provisional" with 180 men total. The teams wore fatigues with life-vests and were not expected to leave their boats similar to the NCDUs. However, at Kwajalein Fort Pierce protocol was changed. Admiral Turner ordered daylight reconnaissance, and CEC Ens. Lewis F. Luehrs and Seabee Chief Bill Acheson wore swim trunks under their fatigues. They stripped down, spent 45 minutes in the water in broad daylight. When they got out were taken directly to Admiral Turner's flagship, still in their trunks, to report. Admiral Turner concluded that daylight reconnaissance by individual swimmers was the way to get accurate information on coral and underwater obstacles for upcoming landings. This is what he reported to Admiral Nimitz. The success of those UDT 1 Seabees not following Fort Pierce protocol rewrote the UDT mission model and training regimen. Ensign Luehrs and Chief Acheson were each awarded a Silver Star for their exploit. As a result of UDT 1. the Naval Combat Demolition Training & Experimental Base was created at Kihei, Hawaii on Maui and was distinctly different from Fort Pierce. The first head of training was Seabee Lt. T.C. Crist, first Commander of UDT 2 (Roi-Namur and awarded silver star for there). He was in that position for a very short time before being named Commander of UDT 3(Fig 11). The actions of seabees Luehrs and Acheson created the image of UDTs as the "naked warriors". Later, UDT 13 would be on the beach at Iwo Jima. They scouted prior to D-day, helped direct the first landing craft to the correct beaches on D-day and helped clear the beaches of debris on D-plus 2. UDT 14 was the first all Fleet team, the first of three from the Pacific fleet. After July 1944 new UDTs were completely USN with no Army or USMC. At wars end 34 teams had been formed with teams 1-21 actually being deployed. The Seabees provided half of the men in the teams that saw service.

The Seabees were officially organized in the Naval Reserve on 31 December 1947. With the general demobilization following the war, the Naval Construction Battalions (NCBs) were reduced to 3,300 men on active duty by 1950. Between 1949 and 1953, Naval Construction Battalions were organized into two types of units: Amphibious Construction Battalions (ACBs) and Mobile Construction Battalions (MCBs), which were later re-designated Naval Mobile Construction Battalions (NMCBs) in 1968.

African American Service: the Seabee stevedores
In February 1942 CNO Admiral Harold Rainsford Stark recommended African Americans for ratings in the construction trades. In April the Navy announced it would enlist African Americans in the Seabees. Even so, there were just two regular CBs that were segregated units, the 34th and 80th NCBs. Both had white Southern officers and black enlisted. Both battalions experienced problems with that arrangement that led to the replacement of the officers. 

The Navy had a huge need for cargo handlers. The lack of stevedores for unloading ships in combat zones was creating a problem. On 18 September 1942 authorization was granted for the formation of a different type of CB denoted by the tag "Special" for cargo handling. By wars end 41 Special Construction Battalions were commissioned of which 15 were segregated. Those Special CBs later became the first fully integrated units in the U.S. Navy. The wars end also brought the decommissioning of every one of those units. The Navy's contemporary version of these units are Navy Cargo Handling Battalions of the Navy Expeditionary Logistics Support Group (United States).

Of particular note were the actions of the 17th Special at Peleliu 15-18 Sept 1944 (Fig. 13). The Japanese mounted a counter-attack 0200 D-day nite. By the time it was over nearly the entire 17th had volunteered to hump ammo to the front lines, bring wounded back, fill where in where the wounded had been and man 37mm that had lost their crews. C Co 1st Battalion 7th Marines was hit hardest that night and was very much in need of the 17ths support. The 17th remained with the 7th Marines until the right flank had been secured D-plus3.

NCF made permanent USN component

"On 13 February 1945: Chief of Naval Operations Fleet Admiral Ernest J. King approved the retention of construction battalions as a permanent and integral part of the postwar Navy. When originally established in the Second World War, the Seabee organization was meant to be only a wartime expedient." By Wars end that expedient had grown to 325,000 men, roughly half the size of the Marine Corps 669,000.


In early 1946 the 53rd NCB was still attached to III Marine Amphibious Corps and was sent to Bikini atoll to assist in the preparations for the nuclear tests of Operation Crossroads. At Bikini the battalion was a component of Task Group 1.8 and designated TU 1.8.6. On 3 August the battalion was decommissioned with the men transferred to CBD 1156 that was commissioned on Bikini. The TU 1.8.6 designation continued with them. The Battalion remained on the atoll for nine days after the second nuclear test when it was detached from the Marine Corps and deactivated there.
UDT 3 was designated TU 1.1.3 for the operation. On 27 April 1946, 7 officers and 51 enlisted embarked the USS "Begor" (APD-127) at the Seabee's base, Port Hueneme, for transit to Bikini. Afterwards in 1948 the displaced natives put in a request to the U.S.Navy to blast a channel for access to the island Kili they had been relocated on. This was given to the Seabee detachment on Kwajelin who placed a request for UDT 3. The King of the Bikinians was so pleased he held a going-away feast for the UDTs.

In June 1950 Seabee strength had dropped to 2,800. That changed with the outbreak of the Korean War which led to a call-up of more than 10,000 men. The expansion of the Seabees came from the Naval Reserve Seabee program where individuals volunteered for active duty. The Seabees landed at Inchon with the assault troops. They fought enormous tides as well as enemy fire and provided causeways within hours of the initial landings. Their action here and at other landings emphasized the role of the Seabees, and there was no Seabee demobilization when the truce was declared.

During the Korean War, the Navy realized they needed a naval air station in this region. Cubi Point in the Philippines was selected, and civilian contractors were initially selected for the project. After seeing the forbidding Zambales Mountains and the maze of jungle, they claimed it could not be done. The Navy then turned to the Seabees. The first Seabees to arrive were Naval Mobile Construction Battalion Three (MCB 3) on 2 October 1951; followed by MCB 5 on 5 November 1951. Over the next five years, MCBs 2, 7, 9, 11 and 13 also deployed to Cubi Point. Seabees leveled a mountain to make way for a nearly two-mile-long runway. NAS Cubi Point turned out to be one of the largest earth-moving projects in the world, equivalent to the construction of the Panama Canal (Fig. 15). Seabees there moved 20 million cubic yards of dry fill plus another 15 million that was hydraulic fill. The $100 million facility was commissioned on 25 July 1956, and comprised an air station and an adjacent pier that was capable of docking the Navy's largest carriers. Adjusted for inflation, today's price-tag for what the Seabees built at Cubi Point would be $906,871,323.53. (that excludes the rest of the U.S.Naval Base at Subic Bay).

During the Korean War the Service School Command U.S.N.T.C. Great Lakes, Il. was the location of the U.S. Naval School Construction Battalion Reserve.

Seabee Teams also called Civic Action Teams or CAT 

A product of the Cold War, Seabee Teams were an idea of the U.S. State Department for making "good use" of the Seabees. They could be sent as "U.S. Good Will Ambassadors" to third world nations as a means to combat the spread of Communism and promote "good Will", a military version of the Peace Corps. These 13 man teams would construct schools, drill wells or build clinics creating a positive image or rapport for the U.S. in the developing world. They were utilized by the United States Agency for International Development and were in S.E. Asia by the mid 1950s. Then in the early sixties the U.S.Army Special Forces were being sent into rural areas of South Vietnam to develop a self-defense force to counter the Communist threat and making use of the Seabee teams at these same places made perfect sense to the CIA. So twelve "Seabee teams with Secret Clearances were sent to Vietnam to assist the U.S. Army's Special Forces in the CIA funded Civilian Irregular Defense Group program (CIDG)" in the years 1963,64,65. By 1965 the U.S. Army had enough engineers in theater to end the Seabees involvement with the Special Forces At first they were called Seabee Technical Assistance Teams STAT. Teams after STAT 1104 were renamed Seabee Teams. In total 128 teams were sent to Vietnam with STAT 1104 being the most decorated group of Seabees ever. As a group the Seabee Teams received many awards for heroism. While Vietnam went on the teams were still being sent to other nations. The Royal Thai government requested STATs in 1963 and since then nearly every time a CB has left the United States for a deployment a Seabee team has been sent somewhere.

Construction Civic Action Details or CCAD

CCADs or "See-Kads" are larger civic action units of 20–25 Seabees with the same purpose as Seabee Teams (Fig. 16). The CCAD designation is not found in the record prior to 2013 and there is no explanation stating why these construction crews are called "details" and not "detachments".


In January 1947 166 Seabees sailed from Port Hueneme, California for Antarctica as a component of Operation Highjump. They were part of Admiral Richard E. Byrd's Antarctic expedition. The Admiral was "used" for his name recognition. The U.S.Navy was in charge with "Classified" orders "to do all it could to establish a basis for a (U.S.) land claim in Antarctica". The Navy sent the Seabees to do the job. They would build the camp known as Little America (exploration base) IV as well as a runway for aerial mapping flights. This Operation was vastly larger than IGY Operation Deep Freeze that followed.

In 1955, Seabees began deploying yearly to the continent of Antarctica as participants in Operation Deep Freeze. Their mission was to build and expand scientific bases located on the frozen continent and further establish a land claim for the U.S.(Fig. 19). The first "wintering over" party included 200 Seabees who distinguished themselves by constructing a ice runway on McMurdo Sound. Despite a blizzard that undid the entire project, the airstrip was completed in time for the advance party of Deep Freeze II to become the first to fly into the South Pole by plane. MCB 1 was assigned for Deep Freeze II.

Over the following years and under adverse conditions, Seabees added to their list of accomplishments such things as snow-compacted roads, underground storage, laboratories, and living areas. One of the most notable achievements took place in 1962, when MCB 1 constructed Antarctica's first nuclear power plant, which got them a Navy Unit Commendation. Another, in 1975, was the construction of the Buckminster Fuller Geodesic dome at Amundsen-Scott South Pole Station by NMCB 71. with a diameter of and high. This became a symbolic icon of the United States Antarctic Program until it was replaced (Fig. 18).

During the Cold War, the Seabees undertook a number of other missions, including constructing the Distant Early Warning Line in the Arctic. Again operating often under extreme conditions, the Seabees successfully completed every mission assigned to them.


Seabees deployed to Vietnam twice in the 1950s. First in June 1954 as elements of Operation Passage to Freedom and then two years later to map and survey the nations roads. Seabee teams 501 and 502 arrived on 25 Jan 1963 and are regarded as the first Seabees of the Vietnam conflict. They were sent to Dam Pau and Tri Ton to build camps for the Special Forces. In June 1965, Construction Mechanic 3rd Class Marvin G. Shields was a member of Seabee Technical Assistance Team 1104, that took part in the Battle of Dong Xoai. He was posthumously awarded the Medal of Honor for his actions there and is the only Seabee ever to be awarded the Medal of Honor. Those Seabee "Civic Action Teams" continued throughout the Vietnam War and often were fending off enemy forces alongside their Marine and Army counterparts. Teams typically built schools, clinics or drilled wells. In 1964 ACB 1 was the first CB in the theatre. Beginning in 1965 Naval Construction Regiments (NCRs) deployed throughout Vietnam. The Seabees supported the Marines by building a staggering number of aircraft-support facilities, roads, and bridges. Seabees also worked with and taught construction skills to the Vietnamese people. In 1968 the Marine Corps requested that the Navy make a name change to the CBs. The Marines were using "MCB" for Marine Corps Base and the Navy was using "MCB" for Mobile Construction Battalions. The Navy then added "Naval" to MCB creating the NMCBs that now exist. In May 1968 two reserve battalions were activated (RNMCBs 12 and 22) which brought the total number of battalions rotating to Vietnam to 21 (not including ACBs 1 and 2 or the two Construction Battalion Maintenance Units (CBMUs) that were there too). During 1969 the total number of Seabees that had deployed topped out at 29,000 and then their draw-down began. The last battalion withdrew the end of 1971 which left 3 Seabee teams. They were out by at the end of 1972.


During the Vietnam conflict there were a couple of uniform variations of note. Across the back of the field jacket M-65 the unit number would be stenciled between the shoulders e.g. MCB 1. Another variation was the collar and cover devices for E4 – E6 enlisted (Fig. 21). The Navy authorized that the "crow" for the construction group be replaced by the rating insignia for each trade. These devices were made in gold and black (subdued).

Seabees were first assigned to the State Department in 1964 because listening devices were found in the construction of the new Embassy of the United States in Moscow. The U.S. had just constructed a new embassy in Warsaw and Seabees were dispatched to check that one out and found lots of "bugs". This led to the creation of the support unit in 1966. There are a limited number of special billets for select senior NCOs. These Seabees are assigned to the Department of State and attached to Diplomatic Security. Those chosen can be assigned individually or be part of a regional team traveling from one embassy to the next. Duties include the installation of alarm systems, closed-circuit cameras, electromagnetic locks, safes, vehicle barriers, and securing compounds(Fig. 22). They can also assist security engineers in sweeping embassies (electronic counter-intelligence). They are tasked with new construction or renovations in security sensitive areas and supervise private contractors in non-sensitive areas. Due to Diplomatic protocol the Support Unit is required to wear civilian clothes most of the time they are on duty and receive a supplemental clothing allowance for this. The information regarding this assignment is very scant, but State Department records in 1985 indicate Department security had 800 employees, plus 1,200 Marines and 115 Seabees. That Seabee number is roughly the same today.

On 28 January 1969 a detachment of 50 men from Amphibious Construction Battalion 2 augmented by an additional 17 Seabee divers from both the Atlantic and Pacific fleets as well as the 21st NCR began the installation of the Tektite habitat in Great Lameshur Bay at Lameshur, U.S. Virgin Islands. The Tektite program was funded by NASA and was the first scientists-in-the-sea program sponsored by the U.S. government.(Fig. 23). The Seabees also constructed a 12-hut base camp at Viers that is used today as the Virgin Islands Environmental Resource Station. The Tektite project was a product of the Cold War and caused the U.S. Navy to realize it needed a permanent Underwater Construction capability. "It was this project that led to the formation the Seabee Underwater Construction Teams".

As the Cold War wound down, new challenges were presented by the increased incidence of terrorism. There were also ongoing support missions to Diego Garcia, Guam, Okinawa, Navy and Marine Bases in Japan, the Philippines, Puerto Rico, Guantanamo Bay, Guatemala, the Naval Support Facility for Polaris and Poseidon submarines in Holy Loch, Scotland, Rota, Spain, Naples, Italy, and Souda Bay, Crete.
In 1971, the Seabees began their second huge peacetime construction on Diego Garcia, a small atoll in the Indian Ocean. This project began in 1971 and was completed in 1987 at a cost of $200 million. Because of the extended time-frame, it is difficult to inflation-adjust that cost into today's dollars. The complex accommodates the Navy's largest ships and cargo planes. This base proved invaluable when Iraq invaded Kuwait in August 1990 and Operations Desert Shield and Desert Storm were launched.

Seabee construction efforts led to the expansion and improvement of Naval Air Facility, Sigonella, Sicily, turning this into a major base for the United States Sixth Fleet aviation activities.

There were combat roles as well. In 1983, a truck bomb demolished the barracks the Marines had secured in Beirut, Lebanon. After moving to the Beirut International Airport and setting up quarters there, Druse militia artillery began harassing the Marines. After consultations with the theater commander, Marine amphibious command and combat engineers, the forward-deployed battalion, NMCB-1 in Rota, Spain, sent in a 70-man AirDet working party with heavy equipment. Construction of artillery-resistant quarters went on from December 1983 until the Marines' withdrawal in February 1984. Only one casualty occurred when an equipment operator using a bulldozer to clear fields of fire was wounded by an RPG attack. Seabee EO2 Kirt May received the first Purple Heart awarded to a Seabee since Vietnam.

Robert Stethem was murdered by the Lebanese Shia militia Hezbollah when they hijacked TWA Flight 847 in 1985. Stethem was a Steelworker Second Class (SW2), a Seabee diver and member of Underwater Construction Team One. The is named in his honor. On 24 August 2010, on board USS "Stethem" in Yokosuka, Kanagawa, Japan, Stethem was posthumously made an honorary Master Chief Constructionman (CUCM) by the Master Chief Petty Officer of the Navy.

During the Persian Gulf War, more than 5,000 Seabees (4,000 active and 1,000 reservists) served in the Middle East. In Saudi Arabia, Seabees built ten camps for more than 42,000 personnel; fourteen galleys capable of feeding 75,000 people; and 6 million ft² (600,000 m²) of aircraft parking apron and runways as well as over 200 helicopter landing zones. They built and maintained two 500-bed Fleet Hospitals near the port city of Al-Jubayl.

Seabees continue to provide critical construction skills in connection with the effort to rebuild the infrastructure of Afghanistan. All active and reserve Naval Mobile Construction Battalions (NMCBs) and Naval Construction Regiments (NCRs) have been deployed to both Iraq and Afghanistan. The Seabees have been deployed since the beginning of the invasion of Afghanistan in 2001 and Iraq in 2003. One of their most high-profile tasks in Iraq has been the removal of statues of Saddam Hussein in Baghdad. In Afghanistan, the Seabees' main task has been the construction of multiple forward operating bases for U.S. and coalition forces.

Since 2002, Seabees have provided critical and tactical construction skills in an effort to win the hearts and minds of locals in the Philippines. Their efforts have begun to deter the rising influence of radical terrorists in the southern Philippines, most notably the Abu Sayyaf's jungle training area. Seabees work along with Army, Marines, and Air Force under Joint Special Operations Task Force-Philippines.


On 1 March 1942 the Chief of BuDocks recommended that as a means to promote "esprit de corps" in the new branch of construction battalions, that an insignia be created for use on equipment similar to what air squadrons used on their aircraft(Fig. 26). This was not something for the uniform. Frank J. Iafrate, a civilian plan file clerk at Quonset Point Advance Naval Base, Davisville, Rhode Island, was the artist who designed the original "Disney Style" Seabee in early 1942 with a large capital letter Q around the edge as border. This design was sent to Admiral Moreell who made a single request: that this reference to Quonset Point be changed to a hawser rope and it would be officially adopted. That design remains in use to this day, predominantly unchanged. In late 1942, after designing the logo, Iafrate enlisted in the Seabees. The Camp Post exchanges (PXs) sold pennants with a different Seabee design on them, that was stylistically similar to the Mosquito boat rating insignia.

The Seabees also had a second Logo that much less has been written about(Fig. 9). It was that of a shirtless construction worker holding a sledge hammer with a rifle strapped across his back standing upon the words "Construimus Batuimus USN". The figure is typically on a shield with a blue field across the top and vertical red and white stripes. A small CEC logo is left of the figure and a small anchor is to the right. The Camp's PXs sold two versions of brass badges with this logo, enameled or non-enameled. Despite little being written about this logo it is incorporated into many CB Unit insignias (or variations of it). A partial list of these CBs would be: 9, 15, 17, 23, 29, 41, 45, 50, 68, 75, 77, 86, 87, 90, 93, 95, 99, 145 & 18th Special, Construction Battalion Units (CBUs) 408, 504, 535 and the 7th Brigade. Other units simply used it like 133 NCB did on the front cover of their unit history the "Rain Makers Log".

During World War II, artists working for Walt Disney in the Insignia Department designed logos for about ten Naval Construction units, including the 60th NCB, the 78th NCB the 112th NCB and the 133rd NCB Good candidates, though unknown, are the logos of the 1st NCB, 53rd NCB, 615th CBMU, 30th Regiment and the 6th Brigade There are two Seabee logos in the book on WWII Disney insignia entitled "Disney Don's Dogtags" that are not identified with any unit. Disney did not create the original Seabee insignia.


The end of WWII brought the decommissioning of nearly every Seabee Battalion. The Construction Battalions had been in existence less than four years when this happened and the Navy had not created a Historical Branch or Archive for the NCF. So, there was no central record of the Seabees History or archive for the insignia of the individual units. As history passed, first with Korea and them Vietnam, Construction Battalions were reactivated with the units having no idea what the WWII insignia had been so they made new ones, NMCB One has had three. NMCB 8 is the exception. That Battalion has an insignia very similar to what it had during WWII.

A small number of Seabees support Navy Special Warfare (NSW) units based out of Coronado, CA, and Virginia Beach, VA. Seabees provide services such as power generation/distribution, logistical movement, vehicle repair, construction and maintenance of encampments, water facilities and purification. Seabees assigned to support NSW receive extra training in first aid, small arms, driving, and specialized equipment. The Seabees assigned to NSW are expected to qualify as Expeditionary Warfare Specialists. If desired or required by the unit, Seabees assigned to NSW are eligible to receive the following Naval Enlisted Classifications upon filling the requirements: 5306 – Naval Special Warfare (Combat Service Support) or 5307 – Naval Special Warfare (Combat Support). They also can apply for selection to support the NSW Development Group.

The battalion is the fundamental unit of the Naval Construction Force (NCF). Seabee battalions are constituted in such a way as to be self-sustaining in the field. The nomenclature for NCF battalions has evolved over the years. During World War II, there were more than 140 battalions commissioned(fig. 27). Since then, battalions have been activated and deactivated using WWII unit numbers.
Detachment: This is a construction crew that is sent to smaller construction projects "detached" from the main body's deployment site. They tend to be self-contained construction units capable of independently completing the assigned project.

From the early 1960s through 1991, reserve battalions were designated as Reserve Naval Mobile Construction Battalions (RNMCBs). After 1991, the word "reserve" was dropped, signifying the integration of reserve units with the active units of the NCF.

During the rapid build-up of the Seabees during World War II, the number of battalions in a given area increased and larger construction programs were undertaken.

This necessitated a higher command echelon to plan, coordinate, and assign the work of several battalions in one area. As a result, Naval Construction Regiments (NCRs) were established in December 1942.

In April 1943, Naval Construction Brigades (NCB) were organized to coordinate the work of regiments.

Brigades were the highest NCF command echelon, until early in the 21st Century. At that time, the last two brigades were the Second Naval Construction Brigade (2nd NCB) and the Third Naval Construction Brigade (3rd NCB). The 2nd NCB commanded Atlantic Fleet Seabee units and the 3rd NCB commanded Pacific Fleet Seabee units.

Both brigades were decommissioned in August 2002 and are no longer part of the NCF structure.

Shortly after the commencement of the Global War on Terror, it was realized that a single command interface for global Seabee operations would be required. On August 9, 2002, the First Naval Construction Division (1NCD) was stood-up and commissioned at NAB Little Creek in Virginia.

Since January 2006, 1NCD has been a subordinate unit of Navy Expeditionary Combat Command (NECC). First Naval Construction Division (1NCD) was decommissioned May 31, 2013. The 1NCD staff will be integrated into NECC. Some 1NCD functions have been transferred to the newly created Naval Construction Groups (NCGs) in Gulfport, Mississippi, and Port Hueneme, California, which are now the East and West Coast community for the NCF.

When first organized during World War II, these units consisted of approximately one fourth of the personnel of an NCB, and were intended to take over the maintenance of bases on which major construction had been completed.

Today, CBMU's provide public works support at Naval Support Activities, Forward Operating Bases, and Fleet Hospital/Expeditionary Medical Facilities during wartime or contingency operations. They also provide disaster recovery support to Naval Regional Commanders in CONUS.

UCTs deploy worldwide to conduct underwater construction, inspection, repair, and demolition operations of ocean facilities, to include repair of battle damage. They maintain a capability to support a Fleet Marine Force amphibious assault, subsequent combat service support ashore, and self-defense for their camp and facilities under construction.

UCT1 is home ported at Virginia Beach, Virginia, while UCT2 is at Port Hueneme, California.

In 2013, the Seabee Readiness Groups (SRGs) were decommissioned, and re-formed into Naval Construction Groups One and Two. They are regimental-level command groups, tasked with administrative and tactical control of Seabee Battalions, as well as conducting pre-deployment training of NCF units in the NCGs' respective home port locations. Currently, Naval Construction Group Two (NCG-2) is based at CBC Gulfport, and Naval Construction Group One (NCG-1) is based at CBC Port Hueneme.

ACBs (also abbreviated as PHIBCB) evolved out of pontoon assembly battalions formed as part of the Seabees during World War II. On 31 October 1950, MCBs 104 and 105 were re-designated ACB One and ACB Two, and assigned to Naval Beach Groups.

Today, the ACBs are not part of the NCF, reporting to surface TYCOMs. Additionally, an ACB has a different personnel-mix to an NMCB, with half the enlisted personnel being traditional Seabee rates and the other half being fleet rates.

These units have CEC officers leading them and enlisted Seabees for the various crews. About one-third of new Seabees are assigned to Public Works Departments (PWD) at naval installations both within the United States and overseas. While stationed at a Public Works Department, a Seabee has the opportunity to get specialized training and extensive experience in one or more facets of their rating.

Some bases have civilians that augment the Seabees, but the department is a military organization.

Seabee Engineer Reconnaissance Teams are ten-person teams, developed during Operation Iraqi Freedom (Fig.29). SERTs are divided into three components: a liaison element, a security element and a reconnaissance element. The liaison (LNO) element has a CEC officer and two petty officers who are communications specialists. The LNO element is responsible for communications with higher echelons, both in transferring engineering assessments and intelligence and in receiving engineering reach-back solutions. The reconnaissance element has a CEC officer, who is the SERT Officer-in-Charge (OIC), a Builder or Steelworker chief petty officer who has some bridge construction experience, and petty officers of varying Seabee ratings. The OIC is normally a licensed professional engineer with a civil/structural engineering background.

A SERT unit will include a corpsman or corpsman-trained member, with the rest of the team being selected from the best of their trades in their battalion. All are qualified Seabee Combat Warfare Specialists. The UCTs proved the SERT concept was viable and they have led the way to the concept's adoption throughout OIF.


The newcomers begin "A" School (preliminary training) fresh out of boot camp, or they come from the fleet after their service term is met, spending about 75% of the twelve weeks immersed in hands-on training. The remaining 25% is spent in classroom instruction. From "A" School, new Seabees most often report to an NMCB command for their first tour of duty. For training, the new Seabees attend a four-week course of Expeditionary Combat Skills (ECS) at the Naval Construction Battalion Center in Gulfport, Mississippi, and Port Hueneme, California. ECS is also being taught to all personnel who report to a unit in the Navy Expeditionary Combat Command. ECS is a basic combat-skills course in learning map reading and land navigation, battlefield first aid, formulating defensive plans, conducting reconnaissance, and other combat-related skills. Half of each course is spent at a shooting range learning basic rifle-marksmanship and qualifying with a M16A2 or M16A3 rifle and the M9 service pistol. Those that are posted Alfa Co of an NMCB may be assigned to a crew-served weapon, such as the MK 19 40 mm grenade launcher, the M2HB .50-caliber machine gun, or the M240 machine gun. Many reserve units still field variants of the M60 machine gun. Until 2012, Seabees wore the U.S. Woodland camouflage uniform or the legacy tri-color Desert Camouflage Uniform, the last members of the entire U.S. military to do so, but have now transitioned to the Navy Working Uniform NWU Type III. Seabees use ALICE field gear, as well as some units working with Marines using USMC-issue Improved Load Bearing Equipment (ILBE) gear.

WWII training

Camp Endicott had roughly 45 different vocational schools plus additional specialized classes. These included Air compressors, Arc welding, Bridge building, Bulldozer operation, Camouflage, Carpentry, Concrete construction, Crane operation, Dam building, Deep sea diving, Diesel engines, Distillation and water purification, Dock building, Drafting, Drilling, Dry docks, Dynamite and demolition, Electricity, Electric motors, Fire fighting, Gasoline engines, Generators, Grading roads and airfields, Ice makers, Ignition systems, Lubrication, Marine engines, Marston Matting, Mosquito control, Photography, Pile driving, Pipe-fitting/plumbing, Pontoons, Power-shovel operation, Pumps, Radio, Refrigeration, Riveting, Road building, Road Scrapers, Sheet metal, Soil testing, Steelworking, Storage tanks wood or steel, Tire repair, Tractor operation, Transformers, Vulcanizing, and Well-drilling.

These indicate the construction trade in which a Seabee is skilled. During WWII, the Seabees were the highest-paid component in the U.S. Military, due to all the skilled journeymen in their ranks.

WWII


Current


The Seabee ranks of E-1 through E-3 use the designation "Constructionman" and wear sky-blue stripes on their dress and service uniforms. This blue was adopted in 1899 as a uniform trim color designating the Civil Engineer Corps, but was later given up. Its use by the junior enlisted is a bit of Naval Heritage in the NCF.

At E9 the ratings are reduced to three: EQCM for equipment operators and construction mechanics, CUCM for builders, steelworkers and engineering aids, UCCM for construction electricians and utilitiesmen.

The military qualification badge for the Seabees is known as the Seabee combat warfare specialist insignia (SCW)(Fig. 35). It was created in 1993 for issue to both officers and enlisted personnel that fulfill the training requirements. Only members attached to a qualifying NCF unit are eligible for the SCW pin. The qualifying units include: Naval Mobile Construction Battalions (NMCB), Amphibious Construction Battalions (ACB), Naval Construction Force Support Units (NCFSU), Underwater Construction Teams (UCT), and, since the end of 2008, Naval Construction Regiments (NCR).

The Fleet Marine Force Insignia(Fig. 36), also known as the Fleet Marine Force pin or FMF pin, are three military badges of the United States Navy which are issued to those U.S. Navy officers and sailors who are trained and qualified to perform duties in support of the United States Marine Corps. Those Seabees that draw an assignment with the Fleet Marine Force can earn the Fleet Marine Force Insignia, also known as the Fleet Marine Force pin or FMF pin. the United States Navy has authorized these badges for U.S. Navy officers and sailors who are trained and qualified to perform duties in support of the United States Marine Corps. There are currently three classes of the Fleet Marine Force pin: enlisted, officer, and chaplain. For the requirements, see: Fleet Marine Force Warfare Specialist (EFMFWS) Program per OPNAV Instruction 1414.4B.

At present, there are six active-duty Naval Mobile Construction Battalions (NMCBs) in the United States Navy, split between the Pacific Fleet (Port Hueneme, CA) and the Atlantic Fleet (Gulfport, MS).





see: Seabee (barge)

The first ship of a series of six "Sea Bee" ships, was the SS "Cape Mendocino" (T-AKR-5064), followed by and (Fig. 39), three of which were operated by Lykes Brothers Steamship Company. "The U.S. Navy’s Naval Construction Force, or SeaBees, primarily use the SEABEE barges. The barges are loaded with cargo and floated to and from a mother ship, which allows loading and unloading of containerized cargo off-shore. SEABEE barge ships are equipped with a stern cargo elevator for loading the barges from the water onto the vessel; loaded barges can then be moved toward the vessel’s bow using an internal track system. The barges are stowed on internal decks and are not stacked. The "Sea Bee" vessels had three decks and could transport 38 lighters (12 on the lower decks and 14 on the upper deck). SEABEE barges are larger and heavier than their counterpart, LASH barges." The dual function of the ship is noteworthy, as it had storage tanks with a capacity of nearly 36000 m³ volume built into its sides and unusually large double hull, allowing it to be used also as a product tanker. The ships were later purchased by Military Sealift Command.

 (U.S. Navy).]]

The U.S. Navy Seabee Museum is located at Naval Base Ventura County, Port Hueneme, California near the entrance, but outside the main gate. Due to the location, visitors are able to visit the museum without having to enter the base proper. The museum re-opened on 22 July 2011 in a new building built by Carlsbad-based RQ Construction. The design of the single-story, 38,833 square foot structure was inspired by the Seabee Quonset hut. Inside are galleries for exhibition space, a grand hall, a theater for 45 people, collections storage, and research areas.

On 7 February 2011, the museum was certified as LEED Silver for utilizing a number of sustainable design and construction strategies. Features include the use of low-maintenance landscaping; a "cool" roofing system with high solar reflectance and thermal emittance; use of photocell-controlled light fixtures and energy-efficient lighting fixtures; 30% use of regional materials and 80% construction debris was recycled and diverted from landfills; low-volatility organic compounds (VOCs); and, use of dual-flush toilets and low-flow aerator faucets.

The Seabee Heritage Center is located in Building 446 at the Naval Construction Battalion Center. The Heritage Center is the Atlantic Coast Annex of the Seabee Museum in Port Hueneme. Opened in 1995, the museum annex commemorates the history and achievements of the Atlantic Coast Naval Construction Force (Seabees) and the Navy's Civil Engineer Corps. Exhibits at the Gulfport Annex are provided by the Seabee Museum in Port Hueneme.

The Seabee Museum and Memorial Park in Davisville, Rhode Island was opened in the late 1990s by a group of former Seabees. The Fighting Seabee Statue is located there(Fig. 41).



Other U.S. military construction/engineering organizations:





</doc>
