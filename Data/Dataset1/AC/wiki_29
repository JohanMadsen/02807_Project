<doc id="28027" url="https://en.wikipedia.org/wiki?curid=28027" title="Skateboarding">
Skateboarding

Skateboarding is an action sport which involves riding and performing tricks using a skateboard, as well as a recreational activity, an art form, a entertainment industry job, and a method of transportation. Skateboarding has been shaped and influenced by many skateboarders throughout the years. A 2009 report found that the skateboarding market is worth an estimated $4.8 billion in annual revenue with 11.08 million active skateboarders in the world. In 2016, it was announced that skateboarding will be represented at the 2020 Summer Olympics in Tokyo.

Since the 1970s, skateparks have been constructed specifically for use by skateboarders, Freestyle BMXers, aggressive skaters, and very recently, scooters. However, skateboarding has become controversial in areas in which the activity, although legal, has damaged curbs, stoneworks, steps, benches, plazas and parks.

The first skateboards started with wooden boxes, or boards, with roller skate wheels attached to the bottom. Crate scooters preceded skateboards, having a wooden crate attached to the nose (front of the board), which formed rudimentary handlebars. The boxes turned into planks, similar to the skateboard decks of today. An American WAC, Betty Magnuson, reported seeing French children in the Montmartre section of Paris riding on boards with roller skate wheels attached to them in late 1944. 
Skateboarding, as we know it, was probably born sometime in the late 1940s, or early 1950s, when surfers in California wanted something to do when the waves were flat. This was called "sidewalk surfing" – a new wave of surfing on the sidewalk as the sport of surfing became highly popular. No one knows who made the first board; it seems that several people came up with similar ideas at around the same time. The first manufactured skateboards were ordered by a Los Angeles, California surf shop, meant to be used by surfers in their downtime. The shop owner, Bill Richard, made a deal with the Chicago Roller Skate Company to produce sets of skate wheels, which they attached to square wooden boards. Accordingly, skateboarding was originally denoted "sidewalk surfing" and early skaters emulated surfing style and maneuvers, and performed barefoot.
By the 1960s a small number of surfing manufacturers in Southern California such as Jack's, Kips', Hobie, Bing's and Makaha started building skateboards that resembled small surfboards, and assembled teams to promote their products. One of the earliest Skateboard exhibitions was sponsored by Makaha's founder, Larry Stevenson, in 1963 and held at the Pier Avenue Junior High School in Hermosa Beach, California. Some of these same teams of skateboarders were also featured on a television show called "Surf's Up" in 1964, hosted by Stan Richards, that helped promote skateboarding as something new and fun to do.

As the popularity of skateboarding began expanding, the first skateboarding magazine, "The Quarterly Skateboarder" was published in 1964. John Severson, who published the magazine, wrote in his first editorial:

The magazine only lasted four issues, but resumed publication as "Skateboarder" in 1975. The first broadcast of an actual skateboarding competition was the 1965 National Skateboarding Championships, which were held in Anaheim, California and aired on ABC’s “Wide World of Sports. Because skateboarding was a new sport during this time, there were only two original disciplines during competitions: flatland freestyle and slalom downhill racing.

One of the earliest sponsored skateboarders, Patti McGee, was paid by Hobie and Vita Pak to travel around the country to do skateboarding exhibitions and to demonstrate skateboarding safety tips. McGee made the cover of "Life" magazine in 1965 and was featured on several popular television programs—"The Mike Douglas Show", "What's My Line?" and "The Tonight Show Starring Johnny Carson"—which helped make skateboarding even more popular at the time. Some other well known surfer-style skateboarders of the time were Danny Bearer, Torger Johnson, Bruce Logan, Bill and Mark Richards, Woody Woodward, & Jim Fitzpatrick.

The growth of the sport during this period can also be seen in sales figures for Makaha, which quoted $10 million worth of board sales between 1963 and 1965 (Weyland, 2002:28). By 1966 a variety of sources began to claim that skateboarding was dangerous, resulting in shops being reluctant to sell them, and parents being reluctant to buy them. In 1966 sales had dropped significantly (ibid) and Skateboarder Magazine had stopped publication. The popularity of skateboarding dropped and remained low until the early 1970s.

In the early 1970s, Frank Nasworthy started to develop a skateboard wheel made of polyurethane, calling his company Cadillac Wheels. Prior to this new material, skateboards wheels were metal or "clay" wheels. The improvement in traction and performance was so immense that from the wheel's release in 1972 the popularity of skateboarding started to rise rapidly again, causing companies to invest more in product development. Nasworthy commissioned artist Jim Evans to do a series of paintings promoting Cadillac Wheels, they were featured as ads and posters in the resurrected Skateboarder magazine, and proved immensely popular in promoting the new style of skateboarding.

In the early 1970s skateparks hadn't been invented yet, so skateboarders would flock and skateboard in such urban places as The Escondido reservoir in San Diego, California. Skateboarding magazine would publish the location and Skateboarders made up nicknames for each location such as the Tea Bowl, the Fruit Bowl, Bellagio, the Rabbit Hole, Bird Bath, the Egg Bowl, Upland Pool and the Sewer Slide. Some of the development concepts in the terrain of skateparks were actually taken from the Escondido reservoir. Many companies started to manufacture trucks (axles) specially designed for skateboarding, reached in 1976 by Tracker Trucks. As the equipment became more maneuverable, the decks started to get wider, reaching widths of and over, thus giving the skateboarder even more control. A banana board is a skinny, flexible skateboard made of polypropylene with ribs on the underside for structural support. These were very popular during the mid-1970s and were available in a myriad of colors, bright yellow probably being the most memorable, hence the name.

In 1975 skateboarding had risen back in popularity enough to have one of the largest skateboarding competitions since the 1960s, the Del Mar National Championships, which is said to have had up to 500 competitors. The competition lasted two days and was sponsored by Bahne Skateboards & Cadillac Wheels. While the main event was won by freestyle spinning skate legend Russ Howell, a local skate team from Santa Monica, California, the Zephyr team, ushered in a new era of surfer style skateboarding during the competition that would have a lasting impact on skateboarding's history. With a team of 12, including skating legends such as Jay Adams, Tony Alva, Peggy Oki & Stacy Peralta, they brought a new progressive style of skateboarding to the event, based on the style of Hawaiian surfers Larry Bertlemann, Buttons Kaluhiokalani and Mark Liddell. Craig Stecyk, a photo journalist for Skateboarder Magazine, wrote about and photographed the team, along with Glen E. Friedman, and shortly afterwards ran a series on the team called the Dogtown articles, which eventually immortalized the Zephyr skateboard team. The team became known as the Z-Boys and would go on to become one of the most influential teams in skateboarding's history.

Soon, skateboarding contests for cash and prizes, using a professional tier system, began to be held throughout California, such as the California Free Former World Professional Skateboard Championships, which featured Freestyle and Slalom competitions.

A precursor to the extreme sport of street luge, that was sanctioned by the United States Skateboarding Association (USSA), also took place during the 1970s in Signal Hill, California. The competition was called "The Signal Hill Skateboarding Speed Run", with several competitors earning entries into the Guinness Book of World Records, at the time clocking speeds of over 50 mph on a skateboard. Due to technology and safety concerns at the time, when many competitors crashed during their runs, the sport did not gain popularity or support during this time.

In March 1976, Skateboard City skatepark in Port Orange, Florida and Carlsbad Skatepark in San Diego County, California would be the first two skateparks to be opened to the public, just a week apart. They were the first of some 200 skateparks that would be built through 1982. This was due in part to articles that were running in the investment journals at the time, stating that skateparks were a good investment. Notable skateboarders from the 1970s also include Ty Page, Tom Inouye, Laura Thornhill, Ellen O'Neal, Kim Cespedes, Bob Biniak, Jana Payne, Waldo Autry, Robin Logan, Bobby Piercy, Russ Howell, Ellen Berryman, Shogo Kubo, Desiree Von Essen, Henry Hester, Robin Alaway, Paul Hackett, Michelle Matta, Bruce Logan, Steve Cathey, Edie Robertson, Mike Weed, David Hackett, Gregg Ayres, Darren Ho, and Tom Sims.

Manufacturers started to experiment with more exotic composites and metals, like fiberglass and aluminium, but the common skateboards were made of maple plywood. The skateboarders took advantage of the improved handling of their skateboards and started inventing new tricks. Skateboarders, most notably Ty Page, Bruce Logan, Bobby Piercy, Kevin Reed, and the Z-Boys started to skate the vertical walls of swimming pools that were left empty in the 1976 California drought. This started the "vert" trend in skateboarding. With increased control, vert skaters could skate faster and perform more dangerous tricks, such as slash grinds and frontside/backside airs. This caused liability concerns and increased insurance costs to skatepark owners, and the development (first by Norcon, then more successfully by Rector) of improved knee pads that had a hard sliding cap and strong strapping proved to be too-little-too-late. During this era, the "freestyle" movement in skateboarding began to splinter off and develop into a much more specialized discipline, characterized by the development of a wide assortment of flat-ground tricks.

As a result of the "vert" skating movement, skate parks had to contend with high liability costs that led to many park closures. In response, vert skaters started making their own ramps, while freestyle skaters continued to evolve their flatland style. Thus, by the beginning of the 1980s, skateboarding had once again declined in popularity.

This period was fueled by skateboard companies that were run by skateboarders. The focus was initially on vert ramp skateboarding. The invention of the no-hands aerial (later known as the ollie) by Alan Gelfand in Florida in 1976, and the almost parallel development of the grabbed aerial by George Orton and Tony Alva in California, made it possible for skaters to perform airs on vertical ramps. While this wave of skateboarding was sparked by commercialized vert ramp skating, a majority of people who skateboarded during this period didn't ride vert ramps. As most people could not afford to build vert ramps, or did not have access to nearby ramps, street skating increased in popularity.

Freestyle skating remained healthy throughout this period, with pioneers such as Rodney Mullen inventing many of the basic tricks that would become the foundation of modern street skating, such as the "Impossible" and the "kickflip". The influence that freestyle exerted upon street skating became apparent during the mid-1980s; however, street skating was still performed on wide vert boards with short noses, slide rails, and large soft wheels. In response to the tensions created by this confluence of skateboarding "genres", a rapid evolution occurred in the late 1980s to accommodate the street skater. Since few skateparks were available to skaters at this time, street skating pushed skaters to seek out shopping centers and public and private property as their "spot" to skate. (Public opposition, in which businesses, governments, and property owners have banned skateboarding on properties under their jurisdiction or ownership, would progressively intensify over the following decades.) By 1992, only a small fraction of skateboarders continuing to take part in a highly technical version of street skating, combined with the decline of vert skating, produced a sport that lacked the mainstream appeal to attract new skaters.

Skateboarding during the 1990s became dominated by street skateboarding. Most boards are about wide and long. The wheels are made of an extremely hard polyurethane, with hardness (durometer) approximately 99A. The wheel sizes are relatively small so that the boards are lighter, and the wheels' inertia is overcome quicker, thus making tricks more manageable. Board styles have changed dramatically since the 1970s but have remained mostly alike since the mid-1990s. The contemporary shape of the skateboard is derived from the freestyle boards of the 1980s with a largely symmetrical shape and relatively narrow width. This form had become standard by the mid '90s.

By 2001 skateboarding had gained so much popularity that more people under the age of 18 rode skateboards (10.6 million) than played baseball (8.2 million), although traditional organized team sports still dominated youth programs overall. Skateboarding and skateparks began to be viewed and used in a variety of new ways to complement academic lessons in schools, including new non-traditional physical education skateboarding programs, like Skatepass and Skateistan, to encourage youth to have better attendance, self-discipline and confidence. This was also based on the healthy physical opportunities skateboarding was understood to bring participants for muscle & bone strengthening and balance, as well as the positive impacts it can have on youth in teaching them mutual respect, social networking, artistic expression and an appreciation of the environment.

In 2003 Go Skateboarding Day was founded in southern California by the International Association of Skateboard Companies to promote skateboarding throughout the world. It is celebrated annually on June 21 "to define skateboarding as the rebellious, creative celebration of independence it continues to be."
According to market research firm American Sports Data the number of skateboarders worldwide increased by more than 60 percent between 1999 and 2002—from 7.8 million to 12.5 million.

Many cities also began implementing recreation plans and statutes during this time period, as part of their vision for local parks and communities to make public lands more available, in particular, for skateboarding, inviting skateboarders to come in off of the city streets and into organized skateboarding activity areas. By 2006 there were over 2,400 skateparks worldwide and the design of skateparks themselves had made a transition, as skaters turned designers. Many new places to skateboard designed specifically for street skaters, such as the "Safe Spot Skate Spot" program, first initiated by professional skateboarder Rob Dyrdek throughout many cities, allowed for the creation of smaller alternative safe skate plazas to be built at a lower cost. One of the largest locations ever built to skateboard in the world, SMP Skatepark in China, at 12,000 square meters in size, was built complete with a 5,000-seat stadium.

In 2009 Skatelab opened the Skateboarding Hall of Fame & Skateboard Museum. Nominees are chosen by the International Association of Skateboard Companies (IASC).

Recently, barefoot skating has been experiencing a revival. Many skaters ride barefoot, particularly in summer and in warmer countries, such as South Africa, Australia, Spain and South America. The plastic penny board is intended to be ridden barefoot, as is the surfboard-inspired hamboard.

In the 2010s, electric skateboards became popular, along with self-balancing unicycles in a board format.

With the evolution of skateparks and ramp skating, the skateboard began to change. Early skate tricks had consisted mainly of two-dimensional freestyle manoeuvres like riding on only two wheels ("wheelie" or "manual"), spinning only on the back wheels (a "pivot"), high jumping over a bar and landing on the board again, also known as a "hippie jump", long jumping from one board to another, (often over small barrels or fearless teenagers), or slalom. Another popular trick was the Bertlemann slide, named after Larry Bertelemann's surfing manoeuvres.

In 1976, skateboarding was transformed by the invention of the ollie by Alan "Ollie" Gelfand. It remained largely a unique Florida trick until the summer of 1978, when Gelfand made his first visit to California. Gelfand and his revolutionary maneuvers caught the attention of the West Coast skaters and the media where it began to spread worldwide. The ollie was adapted to flat ground by Rodney Mullen in 1982. Mullen also invented the "Magic Flip," which was later renamed the kickflip, as well as many other tricks including, the 360 Kickflip, which is a 360 pop shove-it and a kickflip in the same motion. The flat ground ollie allowed skateboarders to perform tricks in mid-air without any more equipment than the skateboard itself, it has formed the basis of many street skating tricks. A recent development in the world of trick skating is the 1080, which was first ever landed by Tom Schaar in 2012.

Skateboarding was popularized by the 1986 skateboarding cult classic "Thrashin'". Directed by David Winters and starring Josh Brolin, it features appearances from many famous skaters such as Tony Alva, Tony Hawk, Christian Hosoi and Steve Caballero. "Thrashin'" also had a direct impact on "Lords of Dogtown", as Catherine Hardwicke, who directed "Lords of Dogtown", was hired by Winters to work on "Thrashin"' as a production designer where she met, worked with and befriended many famous skaters including the real Tony Alva, Tony Hawk, Christian Hosoi and Steve Caballero.

These films have helped improve the reputation of skateboarding youth, depicting individuals of this subculture as having a positive outlook on life, prone to poking harmless fun at each other, and engaging in healthy sportsman's competition. According to the film, lack of respect, egotism and hostility towards fellow skateboarders is generally frowned upon, albeit each of the characters (and as such, proxies of the "stereotypical" skateboarder) have a firm disrespect for authority and for rules in general. "Gleaming the Cube", a 1989 movie starring Christian Slater as a skateboarding teen investigating the death of his adopted Vietnamese brother, was somewhat of an iconic landmark to the skateboarding genre of the era. Many well-known skaters had cameos in the film, including Tony Hawk and Rodney Mullen, where Mullen served as Slater's stunt double.

Skateboarding was, at first, tied to the culture of surfing. As skateboarding spread across the United States to places unfamiliar with surfing or surfing culture, it developed an image of its own. For example, the classic film short "Video Days" (1991) portrayed skateboarders as reckless rebels.

California duo Jan and Dean recorded the song "Sidewalk Surfin'" in 1964, which is the Beach Boys song "Catch a Wave" with new lyrics associated with skateboarding.

Certain cities still oppose the building of skate parks in their neighborhoods, for fear of increased crime and drugs in the area. The rift between the old image of skateboarding and a newer one is quite visible: magazines such as "Thrasher" portray skateboarding as dirty, rebellious, and still firmly tied to punk, while other publications, "Transworld Skateboarding" as an example, paint a more diverse and controlled picture of skateboarding. As more professional skaters use hip hop, reggae, or hard rock music accompaniment in their videos, many urban youths, hip-hop fans, reggae fans, and hard rock fans are also drawn to skateboarding, further diluting the sport's punk image.

Group spirit supposedly influences the members of this community. In presentations of this sort, showcasing of criminal tendencies is absent, and no attempt is made to tie extreme sports to any kind of illegal activity. Female based skateboarding groups also exist, such as Brujas which is based in New York City. Many women use their participation in skate crews to perform an alternative form of femininity. These female skate crews offer a safe haven for women and girls in cities, where they can skate and bond without male expectations or competition.

The increasing availability of technology is apparent within the skateboarding community. Many skateboarders record and edit videos of themselves and friends skateboarding. However, part of this culture is to not merely replicate but to innovate; emphasis is placed on finding new places and landing new tricks.

Skateboarding video games have also become very popular in skateboarding culture. Some of the most popular are the "Tony Hawk" series and "Skate series" for various consoles (including hand-held) and personal computer.

Whilst early skateboarders generally rode barefoot, preferring direct foot-to-board contact, and some skaters continue to do so, one of the early leading trends associated with the sub-culture of skateboarding itself, was the sticky-soled slip-on skate shoe, most popularized by Sean Penn's skateboarding character from the film Fast Times at Ridgemont High. Because early skateboarders were actually surfers trying to emulate the sport of surfing, at the time when skateboards first came out on the market, many skateboarded barefoot. But skaters often lacked traction, which led to foot injuries. This necessitated the need for a shoe that was specifically designed and marketed for skateboarding, such as the Randy "720", manufactured by the Randolph Rubber Company, and Vans sneakers, which eventually became cultural iconic signifiers for skateboarders during the 1970s and '80s as skateboarding became more widespread.

While the skate shoes design afforded better connection and traction with the deck, skaterboarders themselves could often be identified when wearing the shoes, with Tony Hawk once saying, "If you were wearing Vans shoes in 86, you were a skateboarder" Because of its connection with skateboarding, Vans financed the legendary skateboarding documentary "Dogtown and Z-Boys" and was the first sneaker company to endorse a professional skateboarder Stacy Peralta. Vans has a long history of being a major sponsor of many of skateboarding's competitions and events throughout skateboarding's history as well, including the Vans Warped Tour and the Vans Triple Crown Series.

As it eventually became more apparent that skateboarding had a particular identity with a style of shoe, other brands of shoe companies began to specifically design skate shoes for functionality and style to further enhance the experience and culture of skateboarding including such brands as; Converse, Nike, DC Shoes, Globe, Adidas, Zoo York and World Industries. Many professional skateboarders are designed a pro-model skate shoe, with their name on it, once they have received a skateboarding sponsorship after becoming notable skateboarders. Some shoe companies involved with skateboarding, like Sole Technology, an American footwear company that makes the Etnies skate shoe brand, further distinguish themselves in the market by collaborating with local cities to open public Skateparks, such as the etnies skatepark in Lake Forest, California.

Individuality and a self-expressed casual style have always been cultural values for skateboarders, as uniforms and jerseys are not typically worn. This type of personal style for skateboarders is often reflected in the graphical designs illustrated on the bottom of the deck of skateboards, since its initial conception in the mid seventies, when Wes Humpston and Jim Muri first began doing design work for Dogtown Skateboards out of their garage by hand, creating the very first iconic skateboard-deck art with the design of the "Dogtown Cross".

Prior to the mid-seventies many early skateboards were originally based upon the concept of “Sidewalk Surfing” and were tied to the surf culture, skateboards were surfboard like in appearance with little to no graphics located under the bottom of the skateboard-deck. Some of the early manufactured skateboards such as "Roller Derby", the "Duraflex Surfer" and the "Banana board" are characteristic. Some skateboards during that time were manufactured with company logo's or stickers across the top of the deck of the skateboard, as griptape was not initially used for construction. But as skateboarding progressed & evolved, and as artist began to design and add influence to the artwork of skateboards, designs and themes began to change.

There were several artistic skateboarding pioneers that had an influence on the culture of skateboarding during the 1980s, that transformed skateboard-deck art like Jim Phillips, whose edgy comic-book style "Screaming Hand", not only became the main logo for Santa Cruz Skateboards, but eventually transcended into tattoos of the same image for thousands of people and vinyl collectible figurines over the years. Artist Vernon Courtlandt Johnson is said to have used his artwork of skeletons and skulls, for Powell Peralta, during the same time that the music genres of punk rock and new wave music were beginning to mesh with the culture of skateboarding. Some other notable skateboard artists that made contribrutions to the culture of skateboarding also include Andy Jenkins, Todd Bratrud, Neil Blender, Marc McKee, Tod Swank, Mark Gonzales, Lance Mountain, Natas Kaupas and Jim Evans.

Over the years skateboard-deck art has continued to influence and expand the culture of skateboarding, as many people began collecting skateboards based on their artistic value and nostalgia. Productions of limited editions with particular designs and types of collectible prints that can be hung on the wall, have been created by such famous artist as Andy Warhol and Keith Haring. Most professional skateboarders today have their own signature skateboard decks, with their favorite artistic designs printed on them using computer graphics.

Skateboards, along with other small-wheeled transportation such as in-line skates and scooters, suffer a safety problem: riders may easily be thrown from small cracks and outcroppings in pavement, especially where the cracks run across the direction of travel. Hitting such an irregularity is the major cause of falls and injuries. The risk may be reduced at higher travel speeds.

Severe injuries are relatively rare. Commonly, a skateboarder who falls suffers from scrapes, cuts, bruises, and sprains. Among injuries reported to a hospital, about half involve broken bones, usually the long bones in the leg or arm. One-third of skateboarders with reported injuries are very new to the sport, having started skating within one week of the injury. Although less common, involving 3.5–9 percent of reported injuries, traumatic head injuries and death are possible severe outcomes.

Skating as a form of transportation exposes the skateboarder to the dangers of other traffic. Skateboarders on the street may be hit by other vehicles or may fall into vehicular traffic.

Skateboarders also pose a risk to other pedestrians and traffic. If the skateboarder falls, the skateboard may roll or fly into another person. A skateboarder who collides with a person who is walking or biking may injure or, rarely, kill that person.

Many jurisdictions require skateboarders to wear bicycle helmets to reduce the risk of head injuries and death. Other protective gear, such as wrist guards, also reduce injury. Some medical researchers have proposed restricting skateboarding to designated, specially designed areas, to reduce the number and severity of injuries, and to eliminate injuries caused by motor vehicles or to other pedestrians.

The use, ownership and sale of skateboards were forbidden in Norway from 1978 to 1989 because of the high number of injuries caused by boards. The ban led skateboarders to construct ramps in the forest and other secluded areas to avoid the police. There was, however, one legal skatepark in the country in Frogner Park in Oslo.

The use of skateboards solely as a form of transportation is often associated with the longboard. Depending on local laws, using skateboards as a form of transportation outside residential areas may or may not be legal. Backers cite portability, exercise, and environmental friendliness as some of the benefits of skateboarding as an alternative to automobiles.

The United States Marine Corps tested the usefulness of commercial off-the-shelf skateboards during urban combat military exercises in the late 1990s in a program called Urban Warrior '99. Their special purpose was "for maneuvering inside buildings in order to detect tripwires and sniper fire".

Trampboarding is a variant of skateboarding that uses a board without the trucks and the wheels on a trampoline. Using the bounce of the trampoline gives height to perform a tricks, whereas in skateboarding you need to make the height by performing an ollie. Trampboarding is seen on YouTube in numerous videos.

Swing boarding is the activity where a skateboard deck is suspended from a pivot point above the rider which allows the rider to swing about that pivot point. The board swings in an arc which is a similar movement to riding a half pipe. The incorporation of a harness and frame allows the rider to perform turns spins all while flying though the air.

Skateboarding damages urban terrain features such as curbs, benches, and ledges when skateboarders perform "grinds" and other tricks on these surfaces. Private industry has responded to this problem by using skate deterrent devices, such as the Skatestopper, in efforts to prevent further damage and to reduce skateboarding on these surfaces.

The enactment of ordinances and the posting of signs stating "Skateboarding is not allowed" have also become common methods to discourage skateboarding in public areas in many cities, to protect pedestrians and property. In the area of street skating, tickets and arrest from police for trespassing and vandalism are not uncommon.

Skateboarding has become an important problem in Freedom Plaza, a National Park within the Pennsylvania Avenue National Historic Site in Washington, D.C. The Plaza contains copies of portions of Pierre (Peter) Charles L'Enfant's 1791 plan for the nation's capital city that have been inscribed in the park's raised marble surface.

Freedom Plaza has become a popular location for skateboarding, although the activity is illegal and has resulted in police actions. A 2016 National Park Service management plan for the Historic Site states that skateboarding has damaged stonework, sculptures, walls, benches, steps, and other surfaces in some areas of the Plaza. The management plan further states that skateboarding presents a persistent law enforcement and management challenge, as popular websites advertise the Plaza's attractiveness for the activity. The plan notes that vandals have removed "No Skateboarding" signs and recommends the replacement of those signs.

A professional skateboarder promoted on Facebook the use of governmental sites for the prohibited activity during the 2013 federal government shutdown in the United States.

Nonetheless, efforts have been taken to improve recognition of the cultural heritage as well as positive effects of skateboarding in cities. By raising £790,000, the Long Live Southbank initiative managed to curb the destruction of a forty years old spot in London due to urban planning, a salvaging operation whose effect extends beyond skateboarding. The presence of skateboarders on this public space keeps the area under nearly constant watch and drives homeless people away, increasing the feeling of safety in the area. The activity attracts artists such as photographers and film makers, as well as a significant number of tourists, which in turn drives economic activity in the neighborhood.




</doc>
<doc id="28028" url="https://en.wikipedia.org/wiki?curid=28028" title="Speed skating">
Speed skating

Speed skating is a competitive form of ice skating in which the competitors race each other in travelling a certain distance on skates. Types of speed skating are long track speed skating, short track speed skating, and marathon speed skating. In the Olympic Games, long-track speed skating is usually referred to as just "speed skating", while short-track speed skating is known as "short track". The ISU, the governing body of both ice sports, refers to long track as "speed skating" and short track as "short track skating".

An international federation was founded in 1892, the first for any winter sport. The sport enjoys large popularity in the Netherlands, Norway and South Korea. There are top international rinks in a number of other countries, including Canada, the United States, Germany, Italy, Japan, Russia and Kazakhstan. A World Cup circuit is held with events in those countries plus two events in the Thialf ice hall in Heerenveen, Netherlands.

The standard rink for long track is 400 meters long, but tracks of 200, 250 and 333⅓ meters are used occasionally. It is one of two Olympic forms of the sport and the one with the longer history.

International Skating Union rules allow some leeway in the size and radius of curves.

Short track speed skating takes place on a smaller rink, normally the size of an ice hockey rink, on a 111.12 m oval track. Distances are shorter than in long-track racing, with the longest Olympic individual race being 1500 meters (the women's relay is 3000 meters and the men's relay 5000 meters). Event are usually held with a knockout format, with the best two in heats of four or five qualifying for the final race, where medals are awarded. Disqualifications and falls are not uncommon.

There are variations on the mass-start races. In the regulations of roller sports, eight different types of mass starts are described. Among them are elimination races, where one or more competitors are eliminated at fixed points during the course; simple distance races, which may include preliminary races; endurance races with time limits instead of a fixed distance; points races; and individual pursuits.

Races usually have some rules about disqualification if an opponent is unfairly hindered; these rules vary between the disciplines. In long track speed skating, almost any infringement on the pairmate is punished, though skaters are permitted to change from the inner to the outer lane out of the final curve if they are not able to hold the inner curve, as long as they are not interfering with the other skater. In mass-start races, skaters will usually be allowed some physical contact.

Team races are also held; in long track speed skating, the only team race at the highest level of competition is the Team pursuit, though athletics-style relay races are held at children's competitions. Relay races are also held in short track and inline competitions, but here, exchanges may take place at any time during the race, though exchanges may be banned during the last couple of laps.

Most speed skating races are held on an oval course, but there are exceptions. Oval sizes vary; in short track speed skating, the rink must be an oval of 111.12 metres, while long track speed skating uses a similarly standardized 400 m rink. Inline skating rinks are between 125 and 400 metres, though banked tracks can only be 250 metres long. Inline skating can also be held on closed road courses between 400 and 1,000 metres, as well as open-road competitions where starting and finishing lines do not coincide. This is also a feature of outdoor marathons.

In the Netherlands, marathon competitions may be held on natural ice on canals, and bodies of water such as lakes and rivers, but may also be held on artificially frozen 400 m tracks, with skaters circling the track 100 times, for example.

The roots of speed skating date back over a millennium to Scandinavia, Northern Europe and the Netherlands, where the natives added bones to their shoes and used them to travel on frozen rivers, canals and lakes. In contrast to what people think, ice skating has always been an activity of joy and sports and not a matter of transport. For example, winters in the Netherlands have never been stable and cold enough to make ice skating a way of travelling or a mode of transport.
This has already been described in 1194 by William Fitzstephen, who described a sport in London.

Later, in Norway, King Eystein Magnusson, later King Eystein I of Norway, boasts of his skills racing on ice legs.
However, skating and speed skating was not limited to the Netherlands and Scandinavia; in 1592, a Scotsman designed a skate with an iron blade. It was iron-bladed skates that led to the spread of skating and, in particular, speed skating.
By 1642, the first official skating club, The Skating Club Of Edinburgh, was born, and, in 1763, the world saw its first official speed skating race, on the Fens in England organized by the National Ice Skating Association.
While in the Netherlands, people began touring the waterways connecting the 11 cities of Friesland, a challenge which eventually led to the Elfstedentocht.

By 1851, North Americans had discovered a love of the sport, and indeed the all-steel blade was later developed there.
The Netherlands came back to the fore in 1889 with the organization of the first world championships. The ISU (International Skating Union) was also born in the Netherlands in 1892.
By the start of the 20th century, skating and speed skating had come into its own as a major popular sporting activity.

Organized races on ice skates developed in the 19th century. Norwegian clubs hosted competitions from 1863, with races in Christiania drawing five-digit crowds. In 1884, the Norwegian Axel Paulsen was named Amateur Champion Skater of the World after winning competitions in the United States. Five years later, a sports club in Amsterdam held an ice-skating event they called a world championship, with participants from Russia, the United States and the United Kingdom, as well as the host country. The "Internationale Eislauf Vereinigung", now known as the International Skating Union, was founded at a meeting of 15 national representatives in Scheveningen in 1892, the first international winter sports federation. The Nederlandse Schaatsrijderbond was founded in 1882 and organised the world championships of 1890 and 1891. Competitions were held around tracks of varying lengths—the 1885 match between Axel Paulsen and Remke van der Zee was skated on a track of 6/7 mile (1400 metres)—but the 400 metre track was standardised by the ISU in 1892, along with the standard distances for world championships, 500 m, 1500 m, 5000 m and 10,000 m. Skaters started in pairs, each to their own lane, and changed lanes for every lap to ensure that each skater completed the same distance. This is what is now known as long track speed skating. Competitions were exclusively for amateur skaters, which was enforced. Peter Sinnerud was disqualified for professionalism in 1904 and lost his world title.

Long track world records were first registered in 1891 and improved rapidly, Jaap Eden lowering the world 5000-metre record by half a minute during the Hamar European Championships in 1894. The record stood for 17 years, and it took 50 years to lower it by further half a minute.

The Elfstedentocht was organized as a competition in 1909 and has been held at irregular intervals, whenever the ice on the course is deemed good enough. Other outdoor races developed later, with Friesland in the northern Netherlands hosting a race in 1917, but the Dutch natural ice conditions have rarely been conducive to skating. The Elfstedentocht has been held 15 times in the nearly 100 years since 1909, and, before artificial ice was available in 1962, national championships had been held in 25 of the years between 1887, when the first championship was held in Slikkerveer, and 1961. Since artificial ice became common in the Netherlands, Dutch speed skaters have been among the world top in long track ice skating and marathon skating. Another solution to still be able to skate marathons on natural ice became the Alternative Elfstedentocht. The Alternative Elfstedentocht races take part in other countries, such as Austria, Finland or Canada, and all top marathon skaters, as well as thousands of recreative skaters, travel from the Netherlands to the location where the race is held. According to the NRC Handelsblad journalist Jaap Bloembergen, the country "takes a carnival look" during international skating championships.

Speed Skating started in 1924.
At the 1914 Olympic Congress, the delegates agreed to include ice speed skating in the 1916 Olympics, after figure skating had featured in the 1908 Olympics. However, World War I put an end to the plans of Olympic competition, and it was not until the winter sports week in Chamonix in 1924—retroactively awarded Olympic status—that ice speed skating reached the Olympic programme. Charles Jewtraw from Lake Placid, New York, won the first Olympic gold medal, though several Norwegians in attendance claimed Oskar Olsen had clocked a better time. Timing issues on the 500 were a problem within the sport until electronic clocks arrived in the 1960s; during the 1936 Olympic 500–metre race, it was suggested that Ivar Ballangrud's 500-metre time was almost a second too good. Finland won the remaining four gold medals at the 1924 Games, with Clas Thunberg winning 1,500 metres, 5,000 metres, and allround. It was the first and only time an allround Olympic gold medal has been awarded in speed skating. Speed Skating is also a sport in today's Olympics.

Norwegian and Finnish skaters won all the gold medals in world championships between the world wars, with Latvians and Austrians visiting the podium in the European Championships. However, North American races were usually conducted packstyle, similar to the marathon races in the Netherlands, but the Olympic races were to be held over the four ISU-approved distances. The ISU approved the suggestion that the speed skating at the 1932 Winter Olympics should be held as packstyle races, and Americans won all four gold medals. Canada won five medals, all silver and bronze, while defending World Champion Clas Thunberg stayed at home, protesting against this form of racing. At the World Championships held immediately after the games, without the American champions, Norwegian racers won all four distances and occupied the three top spots in the allround standings.

Norwegians, Swedes, Finns, and Japanese skating leaders protested to the USOC, condemning the manner of competition and expressing the wish that mass-start races were never to be held again at the Olympics. However, the ISU adopted the short track speed skating branch, with mass-start races on shorter tracks, in 1967, arranged international competitions from 1976, and brought them back to the Olympics in 1992.

Artificial ices entered the long track competitions with the 1960 Winter Olympics, and the competitions in 1956 on Lake Misurina were the last Olympic competitions on natural ice. 1960 also saw the first Winter Olympic competitions for women. Lidia Skoblikova won two gold medals in 1960 and four in 1964.

More aerodynamic skating suits were also developed, with Swiss skater Franz Krienbühl (who finished 8th on the Olympic 10,000 m at the age of 46) at the front of development. After a while, national teams took over development of body suits, which are also used in short track skating, though without headcover attached to the suit—short trackers wear helmets instead, as falls are more common in mass-start races. Suits and indoor skating, as well as the clap skate, has helped to lower long track world records considerably; from 1971 to 2009, the average speed on the men's 1500 metres has been raised from 45 to 52 km/h. Similar speed increases are shown in the other distances.

After the 1972 season, European long track skaters founded a professional league, International Speedskating League, which included Ard Schenk, three-time Olympic gold medallist in 1972, as well as five Norwegians, four other Dutchmen, three Swedes, and a few other skaters. Jonny Nilsson, 1963 world champion and Olympic gold medallist, was the driving force behind the league, which folded in 1974 for economic reasons, and the ISU also excluded tracks hosting professional races from future international championships. The ISU later organised its own World Cup circuit with monetary prizes, and full-time professional teams developed in the Netherlands during the 1990s, which led them to a dominance on the men's side only challenged by Japanese 500 m racers and American inline skaters who changed to long tracks to win Olympic gold.

During the 20th century, roller skating also developed as a competitive sport. Roller-skating races were professional from an early stage. Professional World Championships were arranged in North America between the competitors on that circuit. Later, roller derby leagues appeared, a professional contact sport that originally was a form of racing. FIRS World Championships of inline speed skating go back to the 1980s, but many world champions, such as Derek Parra and Chad Hedrick, have switched to ice in order to win Olympic medals.

Like roller skating, ice speed skating was also professional in North America. Oscar Mathisen, five-time ISU world champion and three-time European champion, renounced his amateur status in 1916 and travelled to America, where he won many races but was beaten by Bobby McLean of Chicago, four-time American champion, in one of the races. Chicago was a centre of ice speed skating in America; the "Chicago Tribune" sponsored a competition called the Silver Skates from 1912 to 2014.

In 1992, short track speed skating was accepted as an Olympic sport. Short track speed skating had little following in the long track speed skating countries of Europe, such as Norway, the Netherlands and the former Soviet Union, with none of these nations having won official medals (though the Netherlands won two gold medals when the sport was a demonstration event in 1988). The Norwegian publication "Sportsboken" spent ten pages detailing the long track speed skating events at the Albertville Games in 1993, but short track was not mentioned by word, though the results pages appeared in that section.

Although this form of speed skating is newer, it is growing faster than long-track speed skating, largely because short track can be done on an ice hockey rink rather than a long-track oval.

Races are run counter-clockwise on a 111-meter track. Short track races are almost always run in a mass start format in which two to six skaters may race at once. Skaters may be disqualified for false starts, impeding, and cutting inside the track. False starts occur when a skater moves before the gun goes off at the start of a race. Skaters are disqualified for impeding when one skater cuts in front of another skater and causes the first skater to stand up to avoid collision or fall. Cutting inside the track occurs when a skater's skates goes inside the blocks which mark the track on the ice. If disqualified the skater will be given last place in their heat of final.

Races are run counter-clockwise on a 400-meter oval. In all individual competition forms, only two skaters are allowed to race at once. Skaters must change lanes every lap. The skater changing from the outside lane to the inside has right-of-way. Skaters may be disqualified for false starts, impeding, and cutting inside the track. If a skater misses their race or falls they have the option to race their distance again. There are no heats or finals in long track, all rankings are by time.

The starting procedure in long-track speed skating consists of three parts. First, the referee tells the athletes to ""Go to the start"". Second, the referee cues the athletes to get "Ready", and waits until the skaters have stopped moving. Finally, the referee waits for a random duration between 1 and 1.5 seconds, and then fires the starting shot. Some argue that this inherent timing variability could disadvantage athletes that start after longer pauses, due to the alerting effect.

In the only non-individual competition form, the team pursuit, two teams of each three to four skaters are allowed to race at once. Both teams remain in the inner lane for the duration of the race; they start on opposite sides of the rink. If four skaters are racing one skater is allowed to drop off and stop racing. The clock stops when the third skater crosses the finish line.

Speed skates Speed skates differ greatly from hockey skates and figure skates. Unlike hockey skates and figure skates, speed skates cut off at the ankle and are built more like a shoe than a boot to allow for more ankle compression. The blades range in length on average from 30 to 45 cm depending on the age and height of the skater. Short track blades are fixed to the boot in two places once at the heel and the other right behind the ball of the foot. Long track skates, also called clap skates, attach firmly to the boot only at the front. The heel of the boot actually detaches from the blade every stroke. This is accomplished through a spring mechanism located at the front connector. Speed skates cannot be sharpened at a shop like most skates. Instead each skater sharpens his or her own skates. This is accomplished using a jig which is a frame to hold the skates in place while they are manually sharpened.

Short track
All short track skaters must have speed skates, a spandex skin suit, protective helmet, specific cut proof skating gloves, knee pads and shin pads (in suit), neck guard (bib style) and ankle protection. Protective eyewear is mandatory. Many skaters wear smooth ceramic or carbon fiber tips on the left hand glove to reduce friction when their hand is on the ice at corners. All skaters who race at a national level must wear a cutproof kevlar suit to protect against being cut from another skater's blade.

Long track
For long track skaters the same equipment should be worn as short track racers but with the exception of a helmet, shin pads, knee pads, and neck guard which are not required. Protective eyewear is not mandatory. The suit also does not need to be kevlar. Long track skaters wear a hood that is built into the suit.





</doc>
<doc id="28030" url="https://en.wikipedia.org/wiki?curid=28030" title="September 13">
September 13





</doc>
<doc id="28032" url="https://en.wikipedia.org/wiki?curid=28032" title="Square (disambiguation)">
Square (disambiguation)

A square is a regular quadrilateral with four equal sides and four right angles.

Square may also refer to:









</doc>
<doc id="28034" url="https://en.wikipedia.org/wiki?curid=28034" title="Scanning electron microscope">
Scanning electron microscope

A scanning electron microscope (SEM) is a type of electron microscope that produces images of a sample by scanning the surface with a focused beam of electrons. The electrons interact with atoms in the sample, producing various signals that contain information about the surface topography and composition of the sample. The electron beam is scanned in a raster scan pattern, and the position of the beam is combined with the detected signal to produce an image. SEM can achieve resolution better than 1 nanometer. Specimens are observed in high vacuum in conventional SEM, or in low vacuum or wet conditions in variable pressure or environmental SEM, and at a wide range of cryogenic or elevated temperatures with specialized instruments.

The most common SEM mode is detection of secondary electrons emitted by atoms excited by the electron beam. The number of secondary electrons that can be detected depends, among other things, on specimen topography. By scanning the sample and collecting the secondary electrons that are emitted using a special detector, an image displaying the topography of the surface is created.

An account of the early history of SEM has been presented by McMullan. Although Max Knoll produced a photo with a 50 mm object-field-width showing channeling contrast by the use of an electron beam scanner, it was Manfred von Ardenne who in 1937 invented a true microscope with high magnification by scanning a very small raster with a demagnified and finely focused electron beam. Ardenne applied the scanning principle not only to achieve magnification but also to purposefully eliminate the chromatic aberration otherwise inherent in the electron microscope. He further discussed the various detection modes, possibilities and theory of SEM, together with the construction of the . Further work was reported by Zworykin's group, followed by the Cambridge groups in the 1950s and early 1960s headed by Charles Oatley, all of which finally led to the marketing of the first commercial instrument by Cambridge Scientific Instrument Company as the "Stereoscan" in 1965, which was delivered to DuPont.

The signals used by a scanning electron microscope to produce an image result from interactions of the electron beam with atoms at various depths within the sample. Various types of signals are produced including secondary electrons (SE), reflected or back-scattered electrons (BSE), characteristic X-rays and light (cathodoluminescence) (CL), absorbed current (specimen current) and transmitted electrons. Secondary electron detectors are standard equipment in all SEMs, but it is rare for a single machine to have detectors for all other possible signals.

In secondary electron imaging, or SEI, the secondary electrons are emitted from very close to the specimen surface. Consequently, SEI can produce very high-resolution images of a sample surface, revealing details less than 1 nm in size. Back-scattered electrons (BSE) are beam electrons that are reflected from the sample by elastic scattering. They emerge from deeper locations within the specimen and, consequently, the resolution of BSE images is less than SE images. However, BSE are often used in analytical SEM, along with the spectra made from the characteristic X-rays, because the intensity of the BSE signal is strongly related to the atomic number (Z) of the specimen. BSE images can provide information about the distribution, but not the identity, of different elements in the sample. In samples predominantly composed of light elements, such as biological specimens, BSE imaging can image colloidal gold immuno-labels of 5 or 10 nm diameter, which would otherwise be difficult or impossible to detect in secondary electron images. Characteristic X-rays are emitted when the electron beam removes an inner shell electron from the sample, causing a higher-energy electron to fill the shell and release energy. The energy or wavelength of these characteristic X-rays can be measured by Energy-dispersive X-ray spectroscopy or Wavelength-dispersive X-ray spectroscopy and used to identify and measure the abundance of elements in the sample and map their distribution.

Due to the very narrow electron beam, SEM micrographs have a large depth of field yielding a characteristic three-dimensional appearance useful for understanding the surface structure of a sample. This is exemplified by the micrograph of pollen shown above. A wide range of magnifications is possible, from about 10 times (about equivalent to that of a powerful hand-lens) to more than 500,000 times, about 250 times the magnification limit of the best light microscopes.

Samples for SEM have to be prepared to withstand the vacuum conditions and high energy beam of electrons, and have to be of a size that will fit on the specimen stage. Samples are generally mounted rigidly to a specimen holder or stub using a conductive adhesive. SEM is used extensively for defect analysis of semiconductor wafers, and manufacturers make instruments that can examine any part of a 300 mm semiconductor wafer. Many instruments have chambers that can tilt an object of that size to 45° and provide continuous 360° rotation.

Nonconductive specimens collect charge when scanned by the electron beam, and especially in secondary electron imaging mode, this causes scanning faults and other image artifacts. For conventional imaging in the SEM, specimens must be electrically conductive, at least at the surface, and electrically grounded to prevent the accumulation of electrostatic charge. Metal objects require little special preparation for SEM except for cleaning and conductively mounting to a specimen stub. Non-conducting materials are usually coated with an ultrathin coating of electrically conducting material, deposited on the sample either by low-vacuum sputter coating or by high-vacuum evaporation. Conductive materials in current use for specimen coating include gold, gold/palladium alloy, platinum, iridium, tungsten, chromium, osmium, and graphite. Coating with heavy metals may increase signal/noise ratio for samples of low atomic number (Z). The improvement arises because secondary electron emission for high-Z materials is enhanced.

An alternative to coating for some biological samples is to increase the bulk conductivity of the material by impregnation with osmium using variants of the OTO staining method (O-osmium tetroxide, T-thiocarbohydrazide, O-osmium).

Nonconducting specimens may be imaged without coating using an environmental SEM (ESEM) or low-voltage mode of SEM operation. In ESEM instruments the specimen is placed in a relatively high-pressure chamber and the electron optical column is differentially pumped to keep vacuum adequately low at the electron gun. The high-pressure region around the sample in the ESEM neutralizes charge and provides an amplification of the secondary electron signal. Low-voltage SEM is typically conducted in an FEG-SEM because field emission guns (FEG) are capable of producing high primary electron brightness and small spot size even at low accelerating potentials. To prevent charging of non-conductive specimens, operating conditions must be adjusted such that the incoming beam current is equal to sum of outcoming secondary and backscattered electrons currents a condition that is more often met at accelerating voltages of 0.3–4 kV.

Synthetic replicas can be made to avoid the use of original samples when they are not suitable or available for SEM examination due to methodological obstacles or legal issues. This technique is achieved in two steps: (1) a mold of the original surface is made using a silicone-based dental elastomer, and (2) a replica of the original surface is obtained by pouring a synthetic resin into the mold.

Embedding in a resin with further polishing to a mirror-like finish can be used for both biological and materials specimens when imaging in backscattered electrons or when doing quantitative X-ray microanalysis.

The main preparation techniques are not required in the environmental SEM outlined below, but some biological specimens can benefit from fixation.

For SEM, a specimen is normally required to be completely dry, since the specimen chamber is at high vacuum. Hard, dry materials such as wood, bone, feathers, dried insects, or shells (including egg shells) can be examined with little further treatment, but living cells and tissues and whole, soft-bodied organisms require chemical fixation to preserve and stabilize their structure.

Fixation is usually performed by incubation in a solution of a buffered chemical fixative, such as glutaraldehyde, sometimes in combination with formaldehyde and other fixatives, and optionally followed by postfixation with osmium tetroxide. The fixed tissue is then dehydrated. Because air-drying causes collapse and shrinkage, this is commonly achieved by replacement of water in the cells with organic solvents such as ethanol or acetone, and replacement of these solvents in turn with a transitional fluid such as liquid carbon dioxide by critical point drying. The carbon dioxide is finally removed while in a supercritical state, so that no gas–liquid interface is present within the sample during drying.

The dry specimen is usually mounted on a specimen stub using an adhesive such as epoxy resin or electrically conductive double-sided adhesive tape, and sputter-coated with gold or gold/palladium alloy before examination in the microscope. Samples may be sectioned (with a microtome) if information about the organism's internal ultrastructure is to be exposed for imaging.

If the SEM is equipped with a cold stage for cryo microscopy, cryofixation may be used and low-temperature scanning electron microscopy performed on the cryogenically fixed specimens. Cryo-fixed specimens may be cryo-fractured under vacuum in a special apparatus to reveal internal structure, sputter-coated and transferred onto the SEM cryo-stage while still frozen. Low-temperature scanning electron microscopy (LT-SEM) is also applicable to the imaging of temperature-sensitive materials such as ice and fats.

Freeze-fracturing, freeze-etch or freeze-and-break is a preparation method particularly useful for examining lipid membranes and their incorporated proteins in "face on" view. The preparation method reveals the proteins embedded in the lipid bilayer.

Back-scattered electron imaging, quantitative X-ray analysis, and X-ray mapping of specimens often requires grinding and polishing the surfaces to an ultra smooth surface. Specimens that undergo WDS or EDS analysis are often carbon-coated. In general, metals are not coated prior to imaging in the SEM because they are conductive and provide their own pathway to ground.

Fractography is the study of fractured surfaces that can be done on a light microscope or, commonly, on an SEM. The fractured surface is cut to a suitable size, cleaned of any organic residues, and mounted on a specimen holder for viewing in the SEM.

Integrated circuits may be cut with a focused ion beam (FIB) or other ion beam milling instrument for viewing in the SEM. The SEM in the first case may be incorporated into the FIB.

Metals, geological specimens, and integrated circuits all may also be chemically polished for viewing in the SEM.

Special high-resolution coating techniques are required for high-magnification imaging of inorganic thin films.

In a typical SEM, an electron beam is thermionically emitted from an electron gun fitted with a tungsten filament cathode. Tungsten is normally used in thermionic electron guns because it has the highest melting point and lowest vapor pressure of all metals, thereby allowing it to be electrically heated for electron emission, and because of its low cost. Other types of electron emitters include lanthanum hexaboride () cathodes, which can be used in a standard tungsten filament SEM if the vacuum system is upgraded or field emission guns (FEG), which may be of the cold-cathode type using tungsten single crystal emitters or the thermally assisted Schottky type, that use emitters of zirconium oxide.

The electron beam, which typically has an energy ranging from 0.2 keV to 40 keV, is focused by one or two condenser lenses to a spot about 0.4 nm to 5 nm in diameter. The beam passes through pairs of scanning coils or pairs of deflector plates in the electron column, typically in the final lens, which deflect the beam in the "x" and "y" axes so that it scans in a raster fashion over a rectangular area of the sample surface.

When the primary electron beam interacts with the sample, the electrons lose energy by repeated random scattering and absorption within a teardrop-shaped volume of the specimen known as the interaction volume, which extends from less than 100 nm to approximately 5 µm into the surface. The size of the interaction volume depends on the electron's landing energy, the atomic number of the specimen and the specimen's density. The energy exchange between the electron beam and the sample results in the reflection of high-energy electrons by elastic scattering, emission of secondary electrons by inelastic scattering and the emission of electromagnetic radiation, each of which can be detected by specialized detectors. The beam current absorbed by the specimen can also be detected and used to create images of the distribution of specimen current. Electronic amplifiers of various types are used to amplify the signals, which are displayed as variations in brightness on a computer monitor (or, for vintage models, on a cathode ray tube). Each pixel of computer video memory is synchronized with the position of the beam on the specimen in the microscope, and the resulting image is therefore a distribution map of the intensity of the signal being emitted from the scanned area of the specimen. In older microscopes images may be captured by photography from a high-resolution cathode ray tube, but in modern machines they are digitised and saved as digital images.

Magnification in a SEM can be controlled over a range of about 6 orders of magnitude from about 10 to 500,000 times. Unlike optical and transmission electron microscopes, image magnification in an SEM is not a function of the power of the objective lens. SEMs may have condenser and objective lenses, but their function is to focus the beam to a spot, and not to image the specimen. Provided the electron gun can generate a beam with sufficiently small diameter, an SEM could in principle work entirely without condenser or objective lenses, although it might not be very versatile or achieve very high resolution. In an SEM, as in scanning probe microscopy, magnification results from the ratio of the dimensions of the raster on the specimen and the raster on the display device. Assuming that the display screen has a fixed size, higher magnification results from reducing the size of the raster on the specimen, and vice versa. Magnification is therefore controlled by the current supplied to the x, y scanning coils, or the voltage supplied to the x, y deflector plates, and not by objective lens power.

The most common imaging mode collects low-energy (<50 eV) secondary electrons that are ejected from the k-shell of the specimen atoms by inelastic scattering interactions with beam electrons. Due to their low energy, these electrons originate within a few nanometers from the sample surface. The electrons are detected by an Everhart-Thornley detector, which is a type of scintillator-photomultiplier system. The secondary electrons are first collected by attracting them towards an electrically biased grid at about +400 V, and then further accelerated towards a phosphor or scintillator positively biased to about +2,000 V. The accelerated secondary electrons are now sufficiently energetic to cause the scintillator to emit flashes of light (cathodoluminescence), which are conducted to a photomultiplier outside the SEM column via a light pipe and a window in the wall of the specimen chamber. The amplified electrical signal output by the photomultiplier is displayed as a two-dimensional intensity distribution that can be viewed and photographed on an analogue video display, or subjected to analog-to-digital conversion and displayed and saved as a digital image. This process relies on a raster-scanned primary beam. The brightness of the signal depends on the number of secondary electrons reaching the detector. If the beam enters the sample perpendicular to the surface, then the activated region is uniform about the axis of the beam and a certain number of electrons "escape" from within the sample. As the angle of incidence increases, the interaction volume increases and the "escape" distance of one side of the beam decreases, resulting in more secondary electrons being emitted from the sample. Thus steep surfaces and edges tend to be brighter than flat surfaces, which results in images with a well-defined, three-dimensional appearance. Using the signal of secondary electrons image resolution less than 0.5 nm is possible.

Backscattered electrons (BSE) consist of high-energy electrons originating in the electron beam, that are reflected or back-scattered out of the specimen interaction volume by elastic scattering interactions with specimen atoms. Since heavy elements (high atomic number) backscatter electrons more strongly than light elements (low atomic number), and thus appear brighter in the image, BSE are used to detect contrast between areas with different chemical compositions. The Everhart-Thornley detector, which is normally positioned to one side of the specimen, is inefficient for the detection of backscattered electrons because few such electrons are emitted in the solid angle subtended by the detector, and because the positively biased detection grid has little ability to attract the higher energy BSE. Dedicated backscattered electron detectors are positioned above the sample in a "doughnut" type arrangement, concentric with the electron beam, maximizing the solid angle of collection. BSE detectors are usually either of scintillator or of semiconductor types. When all parts of the detector are used to collect electrons symmetrically about the beam, atomic number contrast is produced. However, strong topographic contrast is produced by collecting back-scattered electrons from one side above the specimen using an asymmetrical, directional BSE detector; the resulting contrast appears as illumination of the topography from that side. Semiconductor detectors can be made in radial segments that can be switched in or out to control the type of contrast produced and its directionality.

Backscattered electrons can also be used to form an electron backscatter diffraction (EBSD) image that can be used to determine the crystallographic structure of the specimen.

The nature of the SEM's probe, energetic electrons, makes it uniquely suited to examining the optical and electronic properties of semiconductor materials. The high-energy electrons from the SEM beam will inject charge carriers into the semiconductor. Thus, beam electrons lose energy by promoting electrons from the valence band into the conduction band, leaving behind holes.

In a direct bandgap material, recombination of these electron-hole pairs will result in cathodoluminescence; if the sample contains an internal electric field, such as is present at a p-n junction, the SEM beam injection of carriers will cause electron beam induced current (EBIC) to flow. Cathodoluminescence and EBIC are referred to as "beam-injection" techniques, and are very powerful probes of the optoelectronic behavior of semiconductors, in particular for studying nanoscale features and defects.

Cathodoluminescence, the emission of light when atoms excited by high-energy electrons return to their ground state, is analogous to UV-induced fluorescence, and some materials such as zinc sulfide and some fluorescent dyes, exhibit both phenomena. Over the last decades, cathodoluminescence was most commonly experienced as the light emission from the inner surface of the cathode ray tube in television sets and computer CRT monitors. In the SEM, CL detectors either collect all light emitted by the specimen or can analyse the wavelengths emitted by the specimen and display an emission spectrum or an image of the distribution of cathodoluminescence emitted by the specimen in real color.

Characteristic X-rays that are produced by the interaction of electrons with the sample may also be detected in an SEM equipped for energy-dispersive X-ray spectroscopy or wavelength dispersive X-ray spectroscopy. Analysis of the x-ray signals may be used to map the distribution and estimate the abundance of elements in the sample.

SEM is not a camera and the detector is not continuously image-forming like a CCD array or film. Unlike in an optical system, the resolution is not limited by the diffraction limit, fineness of lenses or mirrors or detector array resolution. The focusing optics can be large and coarse, and the SE detector is fist-sized and simply detects current. Instead, the spatial resolution of the SEM depends on the size of the electron spot, which in turn depends on both the wavelength of the electrons and the electron-optical system that produces the scanning beam. The resolution is also limited by the size of the interaction volume, the volume of specimen material that interacts with the electron beam. The spot size and the interaction volume are both large compared to the distances between atoms, so the resolution of the SEM is not high enough to image individual atoms, as is possible with transmission electron microscope (TEM). The SEM has compensating advantages, though, including the ability to image a comparatively large area of the specimen; the ability to image bulk materials (not just thin films or foils); and the variety of analytical modes available for measuring the composition and properties of the specimen. Depending on the instrument, the resolution can fall somewhere between less than 1 nm and 20 nm. As of 2009, The world's highest resolution conventional (≤30 kV) SEM can reach a point resolution of 0.4 nm using a secondary electron detector.

Conventional SEM requires samples to be imaged under vacuum, because a gas atmosphere rapidly spreads and attenuates electron beams. As a consequence, samples that produce a significant amount of vapour, e.g. wet biological samples or oil-bearing rock, must be either dried or cryogenically frozen. Processes involving phase transitions, such as the drying of adhesives or melting of alloys, liquid transport, chemical reactions, and solid-air-gas systems, in general cannot be observed. Some observations of living insects have been possible however.

The first commercial development of the ESEM in the late 1980s allowed samples to be observed in low-pressure gaseous environments (e.g. 1–50 Torr or 0.1–6.7 kPa) and high relative humidity (up to 100%). This was made possible by the development of a secondary-electron detector capable of operating in the presence of water vapour and by the use of pressure-limiting apertures with differential pumping in the path of the electron beam to separate the vacuum region (around the gun and lenses) from the sample chamber.

The first commercial ESEMs were produced by the ElectroScan Corporation in USA in 1988. ElectroScan was taken over by Philips (who later sold their electron-optics division to FEI Company) in 1996.

ESEM is especially useful for non-metallic and biological materials because coating with carbon or gold is unnecessary. Uncoated Plastics and Elastomers can be routinely examined, as can uncoated biological samples. Coating can be difficult to reverse, may conceal small features on the surface of the sample and may reduce the value of the results obtained. X-ray analysis is difficult with a coating of a heavy metal, so carbon coatings are routinely used in conventional SEMs, but ESEM makes it possible to perform X-ray microanalysis on uncoated non-conductive specimens; however some specific for ESEM artifacts are introduced in X-ray analysis. ESEM may be the preferred for electron microscopy of unique samples from criminal or civil actions, where forensic analysis may need to be repeated by several different experts.

It is possible to study specimens in liquid with ESEM or with other liquid-phase electron microscopy methods.

The SEM can also be used in transmission mode by simply incorporating an appropriate detector below a thin specimen section. Both bright and dark field imaging has been reported in the generally low accelerating beam voltage range used in SEM, which increases the contrast of unstained biological specimens at high magnifications with a field emission electron gun. This mode of operation has been abbreviated by the acronym tSEM.

Electron microscopes do not naturally produce color images, as an SEM produces a single value per pixel; this value corresponds to the number of electrons received by the detector during a small period of time of the scanning when the beam is targeted to the (x, y) pixel position.

This single number is usually represented, for each pixel, by a grey level, forming a "black-and-white" image. However, several ways have been used to get color electron microscopy images.

The easiest way to get color is to associate to this single number an arbitrary color, using a color look-up table (i.e. each grey level is replaced by a chosen color). This method is known as false color. On a BSE image, false color may be performed to better distinguish the various phases of the sample.

As an alternative to simply replacing each grey level by a color, a sample observed by an oblique beam allows researchers to create an approximative topography image (see further section "Photometric 3D rendering from a single SEM image"). Such topography can then be processed by 3D-rendering algorithms for a more natural rendering of the surface texture
Very often, published SEM images are artificially colored. This may be done for aesthetic effect, to clarify structure or to add a realistic appearance to the sample and generally does not add information about the specimen.

Coloring may be performed manually with photo-editing software, or semi-automatically with dedicated software using feature-detection or object-oriented segmentation.

In some configurations more information is gathered per pixel, often by the use of multiple detectors.

As a common example, secondary electron and backscattered electron detectors are superimposed and a color is assigned to each of the images captured by each detector, with an end result of a combined color image where colors are related to the density of the components. This method is known as density-dependent color SEM (DDC-SEM). Micrographs produced by DDC-SEM retain topographical information, which is better captured by the secondary electrons detector and combine it to the information about density, obtained by the backscattered electron detector.

Measurement of the energy of photons emitted from the specimen is a common method to get analytical capabilities. Examples are the energy-dispersive X-ray spectroscopy (EDS) detectors used in elemental analysis and cathodoluminescence microscope (CL) systems that analyse the intensity and spectrum of electron-induced luminescence in (for example) geological specimens. In SEM systems using these detectors it is common to color code these extra signals and superimpose them in a single color image, so that differences in the distribution of the various components of the specimen can be seen clearly and compared. Optionally, the standard secondary electron image can be merged with the one or more compositional channels, so that the specimen's structure and composition can be compared. Such images can be made while maintaining the full integrity of the original signal data, which is not modified in any way.

SEMs do not naturally provide 3D images contrary to SPMs. However 3D data can be obtained using an SEM with different methods as follows.


This method typically uses a four-quadrant BSE detector (alternatively for one manufacturer, a 3-segment detector). The microscope produces four images of the same specimen at the same time, so no tilt of the sample is required. The method gives metrological 3D dimensions as far as the slope of the specimen remains reasonable. Most SEM manufacturers now (2018) offer such built-in or optional four-quadrant BSE detector, together with proprietary software allowing to calculate a 3D image in real time.
Other approaches use more sophisticated (and sometimes GPU-intensive) methods like the optimal estimation algorithm and offer much better results at the cost of high demands on computing power.

In all instances, this approach works by integration of the slope, so vertical slopes and overhangs are ignored; for instance, if an entire sphere lies on a flat, little more than the upper hemisphere is seen emerging above the flat, resulting in wrong altitude of the sphere apex. The prominence of this effect depends on the angle of the BSE detectors with respect to the sample, but these detectors are usually situated around (and close to) the electron beam, so this effect is very common.

This method requires an SEM image obtained in oblique low angle lighting. The grey-level is then interpreted as the slope, and the slope integrated to restore the specimen topography. This method is interesting for visual enhancement and the detection of the shape and position of objects ; however the vertical heights cannot usually be calibrated, contrary to other methods such as photogrammetry.


One possible application is measuring the roughness of ice crystals. This method can combine variable-pressure environmental SEM and the 3D capabilities of the SEM to measure roughness on individual ice crystal facets, convert it into a computer model and run further statistical analysis on the model. Other measurements include fractal dimension, examining fracture surface of metals, characterization of materials, corrosion measurement, and dimensional measurements at the nano scale (step height, volume, angle, flatness, bearing ratio, coplanarity, etc.).

The following are examples of images taken using an SEM.





</doc>
<doc id="28044" url="https://en.wikipedia.org/wiki?curid=28044" title="Timeline of the September 11 attacks">
Timeline of the September 11 attacks

The September 11 attacks timeline is a chronological list of all the major events leading up to, during, and immediately following the terrorist attacks on New York and Washington that day. The timeline starts with the completion of the first World Trade Center tower in 1970 through the first anniversary of the attacks in 2002.



All times are in local time (EDT or UTC − 4).







</doc>
<doc id="28045" url="https://en.wikipedia.org/wiki?curid=28045" title="Hijackers in the September 11 attacks">
Hijackers in the September 11 attacks

The hijackers in the September 11 attacks were 19 men affiliated with al-Qaeda. Fifteen of the 19 were citizens of Saudi Arabia, and the others were from the United Arab Emirates (2), Egypt, and Lebanon. The hijackers were organized into four teams, each led by a pilot-trained hijacker with three or four "muscle hijackers," who were trained to help subdue the pilots, passengers, and crew.

The first hijackers to arrive in the United States were Khalid al-Mihdhar and Nawaf al-Hazmi, who settled in San Diego County, California, in January 2000. They were followed by three hijacker-pilots, Mohamed Atta, Marwan al-Shehhi, and Ziad Jarrah in mid-2000 to undertake flight training in south Florida. The fourth hijacker-pilot, Hani Hanjour, arrived in San Diego in December 2000. The rest of the "muscle hijackers" arrived in early- and mid-2001.

Khalid al-Mihdhar and Nawaf al-Hazmi were both experienced and respected jihadists in the eyes of al-Qaeda leader, Osama bin Laden.

As for the pilots who would go on to participate in the attacks, three of them were original members of the Hamburg cell (Mohamed Atta, Marwan al-Shehhi and Ziad Jarrah). Following their training at Al-Qaeda training camps in Afghanistan, they were chosen by Bin Laden and Al-Qaeda's military wing due to their extensive knowledge of western culture and language skills, increasing the mission's operational security and its chances for success. The fourth intended pilot, Ramzi bin al-Shibh, a member of the Hamburg cell, was also chosen to participate in the attacks yet was unable to obtain a visa for entry into the United States. He was later replaced by Hani Hanjour, a Saudi national.

Mihdhar and Hazmi were also potential pilot hijackers, but did not do well in their initial pilot lessons in San Diego. Both were kept on as "muscle" hijackers, who would help overpower the passengers and crew and allow the pilot hijackers to take control of the flights. In addition to Mihdhar and Hazmi, thirteen other muscle hijackers were selected in late 2000 or early 2001. All were from Saudi Arabia, with the exception of Fayez Banihammad, who was from the United Arab Emirates.

Bold letters note the hijackers who piloted the planes.

Hijackers: Mohamed Atta (Egyptian), Abdulaziz al-Omari (Saudi Arabian), Wail al-Shehri (Saudi Arabian), Waleed al-Shehri (Saudi Arabian), Satam al-Suqami (Saudi Arabian).

Two flight attendants called the American Airlines reservation desk during the hijacking. Betty Ong reported that "the five hijackers had come from first-class seats: 2A, 2B, 9A, 9C and 9B." Flight attendant Amy Sweeney called a flight services manager at Logan Airport in Boston and described them as Middle Eastern. She gave the staff the seat numbers and they pulled up the ticket and credit card information of the hijackers, identifying Mohamed Atta.

Mohamed Atta's voice was heard over the air traffic control system, broadcasting messages thought to be intended for the passengers.

Hijackers: Marwan al-Shehhi (United Arab Emirates), Fayez Banihammad (United Arab Emirates), Mohand al-Shehri (Saudi Arabian), Hamza al-Ghamdi (Saudi Arabian), Ahmed al-Ghamdi (Saudi Arabian).

A United Airlines mechanic was called by a flight attendant who stated the crew had been murdered and the plane hijacked.

Hijackers: Hani Hanjour (Saudi Arabian), Khalid al-Mihdhar (Saudi Arabian), Majed Moqed (Saudi Arabian), Nawaf al-Hazmi (Saudi Arabian), Salem al-Hazmi (Saudi Arabian).

Two hijackers, Hani Hanjour and Majed Moqed were identified by clerks as having bought single, first-class tickets for Flight 77 from Advance Travel Service in Totowa, New Jersey with $1,842.25 in cash. Renee May, a flight attendant on Flight 77, used a cell phone to call her mother in Las Vegas. She said her flight was being hijacked by six individuals who had moved them to the rear of the plane. Unlike the other flights, there was no report of stabbings or bomb threats. According to the 9/11 Commission Report, it is possible that pilots were not stabbed to death and were sent to the rear of the plane. One of the hijackers, most likely Hanjour, announced on the intercom that the flight had been hijacked.
Passenger Barbara Olson called her husband, Theodore Olson, the Solicitor General of the United States, stating the flight had been hijacked and the hijackers had knives and box cutters. Two of the passengers had been on the FBI's terrorist-alert list: Khalid al-Mihdhar and Nawaf al-Hazmi.

Hijackers: Ziad Jarrah (Lebanese), Ahmed al-Haznawi (Saudi Arabian), Ahmed al-Nami (Saudi Arabian), Saeed al-Ghamdi (Saudi Arabian).

Passenger Jeremy Glick stated that the hijackers were Arabic-looking, wearing red headbands, and carrying knives.

Spoken messages (from Ziad Jarrah) intended for passengers were broadcast over the air traffic control system, presumably by mistake:

Jarrah is also heard on the cockpit voice recorder. In addition, DNA samples submitted by his girlfriend were matched to remains recovered in Shanksville.

Before the attacks, FBI agent Robert Wright, Jr. had written vigorous criticisms of FBI's alleged incompetence in investigating terrorists residing within the United States. Wright was part of the Bureau's Chicago counter-terrorism task force and involved in project Vulgar Betrayal, which was linked to Yasin al-Qadi.

According to James Bamford, the NSA had picked up communications of al-Mihdhar and al-Hazmi back in 1999, but had been hampered by internal bureaucratic conflicts between itself and the CIA and did not do a full analysis of the information it passed on to the agency. For example, it only passed on the first names, Nawaf and Khalid.

Bamford also claims that the CIA's Alec Station (a unit assigned to bin Laden) knew that al-Mihdhar was planning to come to New York as far back as January 2000. Doug Miller, one of three FBI agents working inside the CIA station, tried to send a message (a CIR) to the FBI to alert them about this, so they could put al-Mihdhar on a watch list. His CIA boss, Tom Wilshire, deputy station chief, allegedly denied permission to Miller. Miller asked his associate Mark Rossini for advice; Rossini pressed Wilshire's deputy but was rebuffed also.

Bamford also claims that al-Mihdhar and Hazmi wound up living with Abdussattar Shaikh for a time to save money. Shaikh was, coincidentally, an FBI informant, but since they never acted suspiciously around him, he never reported them. The CIA Bangkok station told Alec Station that Hazmi had gone to Los Angeles. None of this information made it back to the FBI headquarters.

Within minutes of the attacks, the Federal Bureau of Investigation opened the largest FBI investigation in United States history, operation PENTTBOM. The suspects were identified within 72 hours because few made any attempt to disguise their names on flight and credit card records. They were also among the few non-U.S. citizens and nearly the only passengers with Arabic names on their flights, enabling the FBI to identify them using such details as dates of birth, known or possible residences, visa status, and specific identification of the suspected pilots. On September 27, 2001, the FBI released photos of the 19 hijackers, along with information about many of their possible nationalities and aliases. The suspected hijackers were from Saudi Arabia (fifteen hijackers), United Arab Emirates (two hijackers), Lebanon (one hijacker) and Egypt (one hijacker).

The passport of Satam al-Suqami was reportedly recovered "a few blocks from where the World Trade Center's twin towers once stood"; a passerby picked it up and gave it to a NYPD detective shortly before the towers collapsed. The passports of two other hijackers, Ziad Jarrah and Saeed al-Ghamdi, were recovered from the crash site of United Airlines Flight 93 in Pennsylvania, and a fourth passport, that of Abdulaziz al-Omari was recovered from luggage that did not make it onto American Airlines Flight 11.

According to the 9/11 Commission Report, 26 al-Qaeda terrorist conspirators sought to enter the United States to carry out a suicide mission. In the end, the FBI reported that there were 19 hijackers in all: five on three of the flights, and four on the fourth. On September 14, three days after the attacks, the FBI announced the names of 19 persons. After a controversy about an earlier remark, U.S. Homeland Security Secretary Janet Napolitano stated in May 2009 that the 9/11 Commission found that none of the hijackers entered the United States through Canada.

Nawaf al-Hazmi and Hani Hanjour, attended the Dar al-Hijrah Falls Church, Virginia, Islamic Center where the Imam Anwar al-Awlaki preached, in early April 2001. Through interviews with the FBI, it was discovered that Awlaki had previously met Nawaf al-Hazmi several times while the two lived in San Diego. At the time, Hazmi was living with Khalid al-Mihdhar, another 9/11 hijacker. The hijackers of the same plane often had very strong ties as many of them attended school together or lived together prior to the attacks

Soon after the attacks and before the FBI had released the pictures of all the hijackers, several reports claimed some of the men named as hijackers on 9/11 were alive and had their identities stolen.





</doc>
<doc id="28046" url="https://en.wikipedia.org/wiki?curid=28046" title="Closings and cancellations following the September 11 attacks">
Closings and cancellations following the September 11 attacks

Many closings and cancellations followed the September 11 attacks, including major landmarks, buildings, restrictions on access to Lower Manhattan, and postponement or cancellation of major sporting and other events. Landmarks were closed primarily because of fears that they may be attacked. At some places, streets leading up to the institutions were also closed. When they reopened, there was heightened security. Many states declared a state of emergency.

Speaking at a press conference at 11:02am on the morning of the attacks, Mayor Giuliani told New Yorkers: "If you are south of Canal Street, get out. Walk slowly and carefully. If you can’t figure what else to do, just walk north." The neighborhood was covered in dust and debris, and electrical failures caused traffic light outages. Emergency vehicles were given priority to respond to ongoing fires, building collapses, and expected mass casualties. Over a million workers and residents south of Canal Street evacuated, and police stopped pedestrians from entering lower Manhattan. With subways shut down, vehicle traffic restricted, and tunnels closed, they mainly fled on foot, pouring over bridges and ferries to Brooklyn and New Jersey.

On September 12, vehicle traffic was banned south of 14th Street, subway stations south of Canal Street were bypassed, and pedestrians were not permitted below Chambers Street. Vehicle traffic below Canal Street was not allowed until October 13.

The New York Stock Exchange did not open on September 11 even as CNBC showed futures numbers early in the day. As Wall Street was covered in debris from the World Trade Center and suffered infrastructure damage, it remained closed until September 17.

For at least a full day after the attacks, bridges and tunnels to Manhattan were closed to non-emergency traffic in both directions. Among other things, this interrupted scheduled deliveries of food and other perishables, leading to shortages in restaurants. From September 27, 2001, one-occupant cars were banned from crossing into Lower Manhattan from Midtown on weekday mornings in an effort to relieve some of the crush of traffic in the city (the morning rush hour lasts from 5:30 a.m. to 12:00 p.m.), caused largely by the increased security measures and closure of major vehicle and transit crossings.

The tracks and station under the WTC were shut down within minutes of the first plane crash. All remaining New York City Subway service was suspended from 10:20am to 12:48pm. Immediately after the attacks and more so after the collapses of the Twin Towers, many trains running in Lower Manhattan lost power and had to be evacuated through the tunnels. Some trains had power but the signals did not, requiring special operating procedures to ensure safety.

The IRT Broadway–Seventh Avenue Line, which ran below the World Trade Center between Chambers Street and Rector Street, was the most crippled. This section of the tunnel, including Cortlandt Street station (located directly underneath the World Trade Center), was badly damaged, and had to be rebuilt. Service was immediately suspended south of Chambers Street and then cut back to 14th Street. There was also subsequent flooding on the line south of 34th Street–Penn Station. After the flood was cleaned up, express service was able to resume on September 17 with trains running between Van Cortlandt Park–242nd Street and 14th Street, making local stops north of and express stops south of 96th Street, while and trains made all stops in Manhattan (but bypassed all stations between Canal Street and Fulton Street until October 1). 1/9 skip-stop service was suspended.

After a few switching delays at 96th Street, service was changed on September 19. The train resumed local service in Manhattan, but was extended to New Lots Avenue in Brooklyn (switching onto the express tracks at Chambers Street) to replace the 3, which now terminated at 14th Street as an express. The train continued to make local stops in Manhattan and service between Chambers Street and South Ferry as well as skip-stop service remained suspended. Normal service on all four trains was restored September 15, 2002, but Cortlandt Street will remain closed while the World Trade Center site is redeveloped.

Service on the BMT Broadway Line was also disrupted because the tracks from the Montague Street Tunnel run adjacent to the World Trade Center and there were concerns that train movements could cause unsafe settling of the debris pile. Cortlandt Street station, which sits under Church Street, sustained significant damage in the collapse of the towers. It was closed until September 15, 2002 for removal of debris, structural repairs, and restoration of the track beds, which had suffered flood damage in the aftermath of the collapse. Starting September 17, 2001, and service was suspended and respectively replaced by the (which was extended to Coney Island–Stillwell Avenue via the BMT Montague Street Tunnel, BMT Fourth Avenue Line, and BMT Sea Beach Line) and the (also extended via Fourth Avenue to Bay Ridge–95th Street). In Queens, the replaced the while the replaced the . All service on the BMT Broadway Line ran local north of Canal Street except for the <Q>, which ran normally from 57th Street to Brighton Beach via Broadway and Brighton Express. J/Z skip-stop service was suspended at this time. Normal service on all seven trains resumed on October 28.

The only subway line running between Midtown and Lower Manhattan was the IRT Lexington Avenue Line, which was overcrowded before the attacks and at crush density until the BMT Broadway Line reopened. Wall Street was closed until September 21.

The IND Eighth Avenue Line, which has a stub terminal serving the train under Five World Trade Center, was undamaged, but covered in soot. E trains were extended to Euclid Avenue, Brooklyn, replacing the then suspended train (the and trains replaced it as the local north of 59th Street–Columbus Circle on nights and weekends, respectively. The train, which ran normally from 145th Street or Bedford Park Boulevard to 34th Street–Herald Square via Central Park West Local, also replaced C trains on weekdays). Service was cut back to Canal Street when C service resumed on September 21, but Chambers Street and Broadway–Nassau Street remained closed until October 1. World Trade Center remained closed until January 2002.

There were no reported casualties on the subway or loss of train cars, but an MCI coach bus was destroyed. Another bus was damaged, but repaired and is back in normal service with a special commemoration livery.

PATH started evacuating passengers from its Manhattan trains and tracks within minutes of the first plane crash. The PATH station at World Trade Center was heavily damaged (a train parked in the station was crushed by debris and was removed during the excavation process in January 2002) and all service there was suspended. For several hours, PATH did not run any trains to Manhattan, but was able to restore service on the Uptown Hudson Tubes to 33rd Street by the afternoon. Exchange Place was unusable since the switch configuration at the time required all trains to continue to World Trade Center. As a result, PATH ran a modified service: Hoboken-Journal Square, Hoboken-33rd Street, and Newark-33rd Street. Exchange Place reopened with modifications on June 29, 2003; a temporary station replacing World Trade Center opened on November 23.

Liberty Water Taxi and NY Waterway had a ferry terminal at the World Financial Center. As the area around the terminal was in the restricted zone, NY Waterway suspended service to the terminal with alternate service going to Midtown and Wall Street and Liberty Water Taxi service was suspended. Free ad-hoc ferry service to New Jersey, Brooklyn, and Queens began by evening, with about half a million evacuees transported by Circle Line Tours, NY Waterway, privately owned dining boats, tug boats, and at least one fire boat.

MTA buses were temporarily suspended south of Canal Street, and MTA and NJ Transit buses were re-routed to serve passengers arriving in Brooklyn and New Jersey by walking and taking ferries out of Manhattan.

The Port Authority Bus Terminal was closed until September 13. Amtrak suspended all of its rail service nationwide until 6pm. Greyhound Bus Lines cancelled its bus service in the Northeast, but was running normally by September 13.

The entire airspaces of the United States and Canada were closed ("ground stop") except for military, police, and medical flights. (The unprecedented implementation of Security Control of Air Traffic and Air Navigation Aids (SCATANA) was the first unplanned closure in the U.S.; military exercises known as Operation Skyshield had temporarily closed the airspace in the early 1960s.) Domestic planes were diverted to the nearest available airport. All non-military flights needed specific approval from United States Air Force and FAA. There were only a few dozen private aircraft which received the approval in that time period. Civil Air Patrol's aerial photography unit was the earliest non military flight granted approval. United Airlines cancelled all flights worldwide temporarily. Grounded passengers and planes were searched for security threats. Amtrak was closed until 6pm on September 11, but by September 13 it had increased capacity 30% to deal with an influx of stranded plane passengers.

Many incoming international flights were diverted to Atlantic Canada to avoid proximity to potential targets in the U.S. and large cities in Canada. Some of the international flights that departed from South America were diverted to Mexico as well, however, its airspace was not shut down. On Thursday night, the New York area airports (JFK, LaGuardia, and Newark) were closed again and reopened the next morning. The only traffic from LaGuardia during the closure was a single C-9C government VIP jet, departing at approximately 5:15 p.m. on the 12th.

Civilian air traffic was allowed to resume on September 13, 2001, with stricter airport security checks, disallowing for example the box cutting knives that were used by the hijackers. (Reinforcement of cockpit doors began in October 2001, and was required for larger airlines by 2003.) First, the stranded planes were allowed to go to their intended destinations, then limited service resumed. The backlog of delayed passengers took several days to clear.

Due to a translation error controllers believed Korean Air Flight 85 might have been hijacked. Canadian Prime Minister Jean Chrétien and U.S. authorities ordered the United States Air Force to surround the plane and force it to land in Whitehorse, Yukon and to shoot down the plane if the pilots did not cooperate. Alaska Governor Tony Knowles ordered the evacuation of large hotels and government buildings in Anchorage. At nearby Valdez, (also in Alaska), the U.S. Coast Guard ordered all tankers filling up with oil to head out to sea. Canadian officials evacuated all schools and large buildings in Whitehorse before the plane landed safely.

Many businesses across the United States closed after the intentional nature of the events became clear, and many national landmarks and financial district skyscrapers were evacuated out of fear of further attacks.



In an atmosphere reminiscent of the assassination of John F. Kennedy in 1963, everyday life in the United States came to a standstill in the days after the September 11 attacks. There was a widespread perception immediately following the attacks that recreational events and sports were not appropriate out of respect for the dead and wounded. For this reason, as well as for reasons of perceived threat associated with large gatherings, many events were postponed or cancelled. Other events were also cancelled, postponed, or modified:


</doc>
<doc id="28047" url="https://en.wikipedia.org/wiki?curid=28047" title="Memorials and services for the September 11 attacks">
Memorials and services for the September 11 attacks

The first memorials to the victims of the September 11 attacks in 2001 began to take shape online, as hundreds of webmasters posted their own thoughts, links to the Red Cross and other rescue agencies, photos, and eyewitness accounts. Numerous online September 11 memorials began appearing a few hours after the attacks, although many of these memorials were only temporary. Around the world, U.S. embassies and consulates became makeshift memorials as people came out to pay their respects.

The "Tribute in Light" was the first major physical memorial at the World Trade Center site. A permanent memorial and museum, the National September 11 Memorial & Museum at the World Trade Center, were built as part of the design for overall site redevelopment. The Memorial consists of two massive pools set within the original footprints of the Twin Towers with waterfalls cascading down their sides. The names of the victims of the attacks are inscribed around the edges of the waterfalls. Other permanent memorials are being constructed around the world.

One of the places that saw many memorials and candlelight vigils was Pier A in Hoboken, New Jersey. There was also a memorial service on March 11, 2002, at dusk on Pier A when the "Tribute in Light" first turned on, marking the half-year anniversary of the terrorist attack. A permanent September 11 memorial for Hoboken, called Hoboken Island, was chosen in September 2004.

Soon after the attacks, temporary memorials were set up in New York and elsewhere.








The Raoul Wallenberg Award was given to New York City in 2001 "For all of its citizens who searched for the missing, cared for the injured, gave comfort to loved ones of the missing or lost, and provided sustenance and encouragement to those who searched through the rubble at Ground Zero."

On February 3, 2002, during the Halftime Show of Super Bowl XXXVI, rock group U2 performed Where the Streets Have No Name, while the names of the victims were projected onto banners. Bono opened his jacket to reveal a U.S. flag pattern sewn in the inside lining.

On February 23, 2003, the 45th Annual Grammy Awards were held at Madison Square Garden and paid tribute to those who died during the 9/11 attacks, to whom the ceremony was dedicated. Ceremony host Bruce Springsteen performed "The Rising" at the Awards.

American country singer Darryl Worley paid tribute to the people with his 2003 single, "Have You Forgotten?" from the album of the same name.

Newark International Airport was renamed "Newark Liberty International Airport".

On September 11, 2002, representatives from over 90 countries came to Battery Park City as New York City Mayor Michael Bloomberg lit an eternal flame to mark the first anniversary of the attacks. Leading the dignitaries were Canadian Prime Minister Jean Chrétien, U.N. Secretary General Kofi Annan, Bloomberg, and Secretary of State Colin Powell. The same day, the Victims of Terrorist Attack on the Pentagon Memorial was dedicated at Arlington National Cemetery near the Pentagon. The memorial is dedicated to the five individuals at the Pentagon whose remains were never found, and the partial remains of another 25 victims are buried beneath the memorial. The names of the 184 victims of the Pentagon attack are inscribed on the memorial's side.

Many organizations held memorial services and events for the 10th anniversary of the attacks.

Every year on September 11 a commemoration is held at the National September 11 Memorial. Family members read the names of victims of the attacks, as well as victims of the 1993 World Trade Center truck bombing. Elected officials and other dignitaries attend, but since the 2012 event they have not given speeches.


The National 9/11 Flag was made from a tattered remains of a American flag found by recovery workers in the early morning of September 12, 2001. It was hanging precariously from some scaffolding at a construction site next to Ground Zero. Because of safety reasons the flag could not be taken down until late October 2001. Charlie Vitchers, a construction superintendent for the Ground Zero cleanup effort, had a crew recover the flag. It was placed in storage for seven years.

The flag has made a number appearances across the country including a Boston Red Sox Game, a New York Giants Home Opener, and the USS "New York" Commissioning Ceremony. It also appeared on the CBS Evening News and on ABC World News Tonight "Persons of the Week."

The flag began a national tour on Flag day, which was on June 14, 2009. It will visit all 50 states where service heroes, veterans, and other honorees will each add stitching and material from other retired American flags in order to restore the original 13 stripes of the flag. The flag will have a permanent home at the National September 11 Memorial and Museum.

The 9-11 Remembrance Flag was created to be a permanent reminder of the thousands of people lost in the September 11 attacks. The purpose of keeping the memories of September 11 alive is not to be forever mourning, but for "learning from the circumstances and making every effort to prevent similar tragedies in our future." The flag is also meant to be a reminder of how the people of this country came together to help each other after the attacks. The red background of the flag represents the blood shed by Americans for their country. The stars represent the lost airplanes and their passengers. The blue rectangles stand for the twin towers and the white pentagon represents the Pentagon building. The blue circle symbolizes the unity of this country after the attacks.

The 9/11 National Remembrance Flag was designed by Stephan and Joanne Galvin soon after September 11, 2001. They wanted to do something to help and were inspired by a neighbor's POW/MIA flag. They wanted to sell the flag so people would remember the September 11 attacks and in order to raise money for relief efforts. The blue represents the colors of the state flags that were involved in the attacks. The black represents sorrow for innocent lives lost. The four stars stand for the four planes that crashed and the lives lost, both in the crash and in the rescue efforts, as well as the survivors. The blue star is a representation of American Airlines Flight 77 and the Pentagon. The two white stars represent American Airlines Flight 11 and United Airlines flight 175, as well as the twin towers. The red star stands for United Flight 93 that crashed in Shanksville, Pennsylvania and all those who sacrifice their lives to protect the innocent. The colors of the stars represent the American flag. The four stars are touching each other and the blue parts of the flag in order to symbolize the unity of the people of the United States.

The National Flag of Honor and the National Flag of Heroes were created by John Michelotti for three main reasons: (1)"To immortalize the individual victims that were killed in the terrorist attacks of September 11, 2001." (2)"To give comfort to the families left behind knowing that their loved one will be forever honored and remembered." (2)"To create an enduring symbol, recognized by the world, of the human sacrifice that occurred on September 11, 2001."

The Flag of Honor and the Flag of Heroes are based on the American flag. They both have the names of all the innocent people who were killed in the September 11 attacks printed on the red and white stripes of the American Flag. Both flags have a white space across the bottom with the name of the flag and a description printed in black. The Flag of Honor reads: "This flag contains the names of those killed in the terrorist attacks of September 11. Now and forever it will represent their immortality. We shall never forget them" The Flag of Heroes reads: " This flag contains the names of the emergency service personnel who gave their lives to save others in the terrorist attacks of September 11. Now and forever it will represent their immortality. We shall never forget them."

The Flag of Honor and the Flag of Heroes were featured at the NYC 9/11 Memorial Field 5th Anniversary in Manhattan's Inwood Hill Park September 8–12, 2006. There 3,000 flags which represented those who died in the September 11 attacks. The flags were also featured on the msnbc Today Show and on ABC 13 News, Norfolk, VA.

The Remembrance Flag has a white background with large, black Roman numerals IX/XI in the center and four black stars across the top. The IX/XI are the Roman numerals for 9/11. The four stars represent World Trade Center North, World Trade Center South, the Pentagon, and Shanksville, PA.

The 10th Anniversary September 11 Memorial Flag was designed by Carrot-Top Industries, a privately owned company in Hillsborough, NC. The exclusive 9/11 memorial flag was designed with the two World Trade Towers set inside a pentagon decorated with a ribbon to commemorate all of the Americans that lost their lives on September 11, 2001.

The growing popularity of virtual worlds such as Secondlife has led to the construction of permanent virtual memorials and exhibits. Examples include:

On September 11, 2007, a virtual reality World Trade Center Memorial will be presented to the people of the world. The location is in Second Life, on the island we have named after the original design: Celestial Requiem NYC. We have built this memorial because, to be blunt, the world needed it done years ago, and the two years longer to await the completion of the "Reflected Absence" memorial in New York city (by Michael Arad and Peter Walker) was in our opinion two years too long.






</doc>
<doc id="28051" url="https://en.wikipedia.org/wiki?curid=28051" title="Airport security repercussions due to the September 11 attacks">
Airport security repercussions due to the September 11 attacks

After the September 11 attacks, questions were raised regarding the effectiveness of airport security at the time, as all 19 hijackers involved in 9/11 managed to pass existing checkpoints and board the airplanes without incident. In the months and years following September 11, 2001, security at many airports worldwide was escalated to deter similar terrorist plots.

Prior to September 11, 2001, airport screening was provided in the U.S. by private companies contracted by the airline or airport. In November 2001, the Transportation Security Administration (TSA) was introduced to take over all of the security functions of the FAA, the airlines, and the airports. Among other changes introduced by TSA, bulletproof and locked cockpit doors became standard on commercial passenger aircraft.

In some countries, for example Sweden, Norway and Finland, there were no or only random security checks for domestic flights in 2001 and before that. On or quickly after September 11, decisions were made to introduce full security checks there. It was immediately implemented where possible, but took one to two years to implement everywhere since terminals were often not prepared with room for it.

Cockpit doors on many aircraft are now reinforced and bulletproof to prevent unauthorized access. Passengers are now prohibited from entering the cockpit during flight. Some aircraft are also equipped with CCTV cameras, so the pilots can monitor cabin activity. Pilots are now allowed to carry firearms, but they must be trained and licensed. In the U.S., more air marshals have been placed on flights to improve security.

On September 11, hijackers Khalid al-Mihdhar, Majed Moqed, Nawaf al-Hazmi and Salem al-Hazmi all set off the metal detector. Despite being scanned with a hand-held detector, the hijackers were passed through. Security camera footage later showed some hijackers had what appeared to be box cutters clipped to their back pockets. Box cutters and similar small knives were allowed on board aircraft at the time.

Airport checkpoint screening has been significantly tightened since 2001, and security personnel are more thoroughly trained to detect weapons or explosives. In addition to standard metal detectors, many U.S. airports now employ full-body scanning machines, in which passengers are screened with millimeter wave technology to check for potential hidden weapons or explosives on their persons. Initially, early body scanners provoked quite a bit of controversy because the images produced by the machines were deemed graphic and intrusive. Many considered this an invasion of personal privacy, as TSA screeners were essentially shown an image of each passenger's naked body. Newer body scanners have since been introduced which do not produce an image, but rather alert TSA screeners of areas on the body where an unknown item or substance may be hidden. A TSA security screener then inspects the indicated area(s) manually.

On September 11, some hijackers lacked proper identification, yet they were allowed to board. That is because they were on domestic aircraft. After 9/11, all passengers 18 years or older must now have valid, government-issued identification in order to fly. Airports may check the ID of any passenger (and staff member) at any time to ensure the details on the ID match those on the printed boarding pass. Only under exceptional circumstances may an individual fly without a valid ID. If approved for flying without an ID, the individual will be subject to extra screening of their person and their carry-on items. TSA does not have the capability to conduct background checks on passengers at checkpoints. Sensitive areas in airports, including airport ramps and operational spaces, are restricted from the general public. Called a SIDA (Security Identification Display Area) in the U.S., these spaces require special qualifications to enter.

A European Union regulation demanded airlines make sure that the individual boarding the aircraft is the same individual who checked in his or her luggage; this was implemented by verifying an individual's identification both at luggage check-in and when boarding.

Some countries also fingerprint travellers or use retina and iris scanning to help detect potential criminals. 

With regard to the 2015 Germanwings flight 9525 crash incident, some have stated that security features added to commercial airliners after 9/11 actually work against the safety of such planes.
In 2003 John Gilmore sued United Airlines, Southwest Airlines and U.S. Attorney General John Ashcroft, arguing that requiring passengers to show identification before boarding domestic flights is tantamount to an internal passport, and is unconstitutional. Gilmore lost the case, known as "Gilmore v. Gonzales", and an appeal to the U.S. Supreme Court was denied.



</doc>
<doc id="28061" url="https://en.wikipedia.org/wiki?curid=28061" title="U.S. government response to the September 11 attacks">
U.S. government response to the September 11 attacks

The response of the U.S. government to the September 11 attacks sparked investigations into the motivations and execution of the attacks, as well as the ongoing War on Terrorism in Afghanistan The response included funds for affected families, plans for the War on Terrorism, rebuilding of Lower-East Manhattan, and the invasion and investigation of Iraq and Afghanistan.

Within hours of the attack, a massive search and rescue (SAR) operation was launched, which included over 350 search and rescue dogs. Initially, only a handful of wounded people were found at the site, and in the weeks that followed it became evident that there weren't any survivors to be found. Only twenty survivors were found alive in the rubble.

Rescue and recovery efforts took months to complete. It took several weeks to simply put out the fires burning in the rubble of the buildings, and the clean-up was not completed until May, 2002. Temporary wooden "viewing platforms" were set up for tourists to view construction crews clearing out the gaping holes where the towers once stood. All of these platforms were closed on May 30, 2002.

Many relief funds were immediately set up to assist victims of the attacks, with the task of providing financial assistance to the survivors and the families of victims. By the deadline for victim's compensation, September 11, 2003, 2,833 applications had been received from the families of those killed.

In the aftermath of the attacks, many U.S. citizens held the view that the attacks had "changed the world forever." The Bush administration announced a war on terrorism, with the goal of bringing Osama bin Laden and al-Qaeda to justice and preventing the emergence of other terrorist networks. These goals would be accomplished by means including economic and military sanctions against states perceived as harboring terrorists and increasing global surveillance and intelligence sharing. Immediately after the September 11 attacks U.S. officials speculated on possible involvement by Saddam Hussein. Although unfounded, the association contributed to public support for the 2003 invasion of Iraq. On October 7, 2001, the War in Afghanistan began when U.S. and British forces initiated aerial bombing campaigns in Afghanistan targeting Taliban and Al-Qaeda camps, then later invaded Afghanistan with ground troops of the Special Forces. This was the second-largest operation of the U.S. Global War on Terrorism outside of the United States, and the largest directly connected to terrorism, resulting in the overthrow of Taliban rule in Afghanistan, by a U.S.-led coalition. The U.S. was not the only nation to increase its military readiness, with other notable examples being the Philippines and Indonesia, countries that have their own internal conflicts with Islamist terrorism. 

Because the attacks on the United States were judged to be within the parameters of its charter, NATO declared that Article 5 of the NATO agreement was satisfied on September 12, 2001, making the US war on terrorism the first time since its inception that NATO would actually participate in a "hot" war.

Following the attacks, 762 suspects were taken into custody in the United States. On December 12, 2001, Fox News reported that some 60 Israelis were among them. Federal investigators were reported to have described them as part of a long-running effort to spy on American government officials. A "handful" of these Israelis were described as active Israeli military or intelligence operatives.

In a letter to the editor, Ira Glaser, former head of the ACLU, claimed that none of those 762 detainees were charged with terrorism. "The Justice Department inspector general's report implies more than the violation of the civil liberties of 762 non-citizens. It also implies a dysfunctional and ineffective approach to protecting the public after Sept. 11, 2001... No one can be made safer by arresting the wrong people".

Immediately after opening the hunt on Osama bin Laden, President Bush also visited the Islamic Center of Washington and asked the public to view Arabs and Muslims living in the United States as American patriots.

Congress passed and President Bush signed the Homeland Security Act of 2002, creating the Department of Homeland Security, representing the largest restructuring of the U.S. government in contemporary history. Congress passed the USA PATRIOT Act, stating that it would help detect and prosecute terrorism and other crimes. Civil liberties groups have criticized the PATRIOT Act, saying that it allows law enforcement to invade the privacy of citizens and eliminates judicial oversight of law-enforcement and domestic intelligence gathering. The Bush Administration also invoked 9/11 as the reason to have the National Security Agency initiate a secret operation, "to eavesdrop on telephone and e-mail communications between the United States and people overseas without a warrant."

On June 6, 2002, Attorney General Ashcroft proposed regulations that would create a special registration program that required males aged 16 to 64 who were citizens of designated foreign nations resident in the U.S. to register with the Immigration and Naturalization Service (INS), have their identity verified, and be interviewed, photographed and fingerprinted. Called the National Security Entry-Exit Registration System (NSEERS), it comprised two programs, the tracking of arrivals and departures on the one hand, and voluntary registrations of those already in the U.S., known as the "call-in" program. The DOJ acted under the authority of the Immigration and Nationality Act of 1952, which had authorized a registration system but was allowed to lapse in the 1980s because of budget concerns. Ashcroft identified those required to register as "individuals of elevated national security concern who stay in the country for more than 30 days."

The processing of arrivals as part of their customs screening began in October 2002. It first focused on arrivals from Iran, Iraq, Libya, Sudan, and Syria. It handled 127,694 people before being phased out as universal screening processes were put in place.

The "call-in" registrations began in December. It initially applied to nationals of five countries, Iran, Iraq, Syria, Libya and Sudan, who were required to register by December 16, 2002. On November 6, the United States Department of Justice (DOJ) set a deadline of January 10 for those from another 13 countries: Afghanistan, Algeria, Bahrain, Eritrea, Lebanon, Morocco, North Korea, Oman, Qatar, Somalia, Tunisia, the United Arab Emirates, and Yemen. On December 16, it set a deadline of February 21 for those from Armenia, Pakistan and Saudi Arabia. It later included those from Egypt, Jordan, Kuwait, Indonesia, and Bangladesh. It eventually included citizens of 23 nations with majority Muslim populations, as well as Eritrea, which has a large Muslim population, and North Korea. Failure to register at an INS office resulted in deportation. Those found in violation of their visa were allowed to post bail while processed for deportation. The program registered 82,880 people, of whom 13,434 were found in violation of their visas. Because nationality and Muslim affiliation are only approximations for one another, the program extended to such non-Muslims as Iranian Jews. The program was phased out beginning in May 2003.

The program received a mixed response. Some government officials pronounced the program a success. They said in the course of the combined programs, registration upon entry and that of residents, they had arrested 11 suspected terrorists, found more than 800 criminal suspects or deportable convicts, and identified more than 9,000 illegal aliens. DOJ general counsel Kris Kobach said: "I regard this as a great success. Sept. 11th awakened the country to the fact that weak immigration enforcement presents a huge vulnerability that terrorists can exploit." DOJ officials said fewer than 5% of those who came in to INS offices to register were detained. James W. Ziglar, former head of INS who left the agency early in 2002, in part because of his differing opinions about the program with Ashcroft, said his objections to it had been proven correct: "The people who could be identified as terrorists weren't going to show up. This project was a huge exercise and caused us to use resources in the field that could have been much better deployed." "As expected, we got nothing out of it." Although Homeland Security officials said that six men allegedly linked to terrorism were arrested as a result of the call-in program, that contention was challenged by the Sept. 11 commission, which found little evidence to support that claim.

A federal technical building and fire safety investigation of the collapses of the Twin Towers was conducted by the United States Department of Commerce's National Institute of Standards and Technology (NIST). The goals of this investigation, completed on April 6, 2005, were to investigate the building construction, the materials used, and the technical conditions that contributed to the outcome of the WTC disaster. The investigation was to serve as the basis for:

The report concludes that the fireproofing on the Twin Towers' steel infrastructures was blown off by the initial impact of the planes and that, if this had not occurred, the towers would likely have remained standing. The fires weakened the trusses supporting the floors, making the floors sag. The sagging floors pulled on the exterior steel columns to the point where exterior columns bowed inward. With the damage to the core columns, the buckling exterior columns could no longer support the buildings, causing them to collapse. In addition, the report asserts that the towers' stairwells were not adequately reinforced to provide emergency escape for people above the impact zones. NIST stated that the final report on the collapse of 7 WTC will appear in a separate report.

The Inspector General of the CIA conducted an internal review of the CIA's performance prior to 9/11, and was harshly critical of senior CIA officials for not doing everything possible to confront terrorism, including failing to stop two of the 9/11 hijackers, Nawaf al-Hazmi and Khalid al-Mihdhar, as they entered the United States and failing to share information on the two men with the FBI.

The "National Commission on Terrorist Attacks Upon the United States" (9/11 Commission), chaired by former New Jersey Governor Thomas Kean, was formed in late 2002 to prepare a full and complete account of the circumstances surrounding the attacks, including preparedness for, and the immediate response to, the attacks. On July 22, 2004, the "9/11 Commission Report" was released. The commission has been subject to criticism.

For the first time in history, all nonemergency civilian aircraft in the United States and several other countries including Canada were immediately grounded, stranding tens of thousands of passengers across the world. The order was given at 9:42 by Federal Aviation Administration Command Center national operations manager Ben Sliney. According to the 9/11 Commission Report, "This was an unprecedented order. The air traffic control system handled it with great skill, as about 4,500 commercial and general aviation aircraft soon landed without incident.

Contingency plans for the continuity of government and the evacuation of leaders were implemented almost immediately after the attacks. Congress, however, was not told that the US was under a continuity of government status until February 2002.




</doc>
<doc id="28064" url="https://en.wikipedia.org/wiki?curid=28064" title="Financial assistance following the September 11 attacks">
Financial assistance following the September 11 attacks

Charities and relief agencies raised over $657 million in the three weeks following the September 11, 2001 attacks, the vast bulk going to immediate survivors and victims' families.

On September 21, 2001, the Congress approved a bill to aid the airline industry and establish a federal fund for victims. The cost of the mostly open-ended fund reached $7 billion (the average payout was $1.8 million per family). Victims of earlier terrorist attacks, including those linked to al-Qaida, were not included in the fund—nor were those who would not surrender the right to hold the airlines legally responsible.

From the donations to the Emergency Relief Fund, as of 19 November 2001, the American Red Cross granted 3,165 checks to 2,776 families totaling $54.3 million.

172,612 cases were referred to mental health contacts. The 866-GET INFO number received 29,820 calls. As of 3:10 p.m. November 20, 2001, there had been 1,592,295 blood donations since September 11.

"Fire Donations" took charitable contributions on behalf of firefighters, EMS, and rescue workers.


On Thursday and Friday, September 14–15 September 2001, various relief supplies for the World Trade Center relief effort were collected from the New York City area, and dropped off at the Javits Convention Center or at a staging area at Union Square. By Saturday morning, enough supplies (and volunteers) were collected.

Many families and friends of victims have set up memorial funds and projects to give back to their communities and change the world in honor of their loved ones' lives. Examples include:



</doc>
<doc id="28066" url="https://en.wikipedia.org/wiki?curid=28066" title="Rescue and recovery effort after the September 11 attacks on the World Trade Center">
Rescue and recovery effort after the September 11 attacks on the World Trade Center

The local, state, federal and global reaction to the September 11 attacks on the World Trade Center was unprecedented. The equally unsurpassed events of that day elicited the largest response of local emergency and rescue personnel to assist in the evacuation of the two towers and also contributed to the largest loss of the same personnel when the towers collapsed. After the attacks, the media termed the World Trade Center site "Ground Zero", while rescue personnel referred to it as "the Pile".

In the ensuing recovery and cleanup efforts, personnel related to metalwork and construction professions would descend on the site to offer their services and remained until the site was cleared in May 2002. In the years since, investigations and studies have examined effects upon those who participated, noting a variety of afflictions attributed to the debris and stress.

After American Airlines Flight 11 crashed into the North Tower (1 WTC) of the World Trade Center, a standard announcement was given to tenants in the South Tower (2 WTC) to stay put and that the building was secure. However, many defied those instructions and proceeded to evacuate the South Tower (most notably, Rick Rescorla, Morgan Stanley Security Director, evacuated 2687 of the 2700 Morgan Stanley employees in the building). All people evacuating were ordered through a door on the mezzanine level that led to a bridge to another building, and everyone was evacuated through the neighboring building. The firefighters in charge did not want anyone going through the front doors at first due to falling debris, and then because of falling people who had jumped from the towers.

Standard evacuation procedures for fires in the World Trade Center called for evacuating only the floors immediately above and below the fire, as simultaneous evacuation of up to 50,000 workers would be too chaotic.

Firefighters from the New York City Fire Department rushed to the World Trade Center minutes after the first plane struck the north tower. Chief Joseph Pfeifer and his crew with Battalion 1 were among the first on the scene. At 8:50 a.m., an Incident Command Post was established in the lobby of the North Tower. By 9:00 a.m., shortly before United Airlines Flight 175 hit the South Tower, the FDNY chief had arrived and took over command of the response operations. Due to falling debris and safety concerns, he moved the incident command center to a spot located across West Street, but numerous fire chiefs remained in the lobby which continued to serve as an operations post where alarms, elevators, communications systems, and other equipment were operated. The initial response by the FDNY was on rescue and evacuation of building occupants, which involved sending firefighters up to assist people that were trapped in elevators and elsewhere. Firefighters were also required to ensure all floors were completely evacuated.

Numerous staging areas were set up near the World Trade Center, where responding fire units could report and get deployment instructions. However, many firefighters arrived at the World Trade Center without stopping at the staging areas. As a result, many chiefs could not keep track of the whereabouts of their units. Numerous firefighters reported directly to the building lobbies, and were ordered by those commanding the operating post to proceed into the building.

Problems with radio communication caused commanders to lose contact with many of the firefighters who went into the buildings. The repeater system in the World Trade Center, which was required for portable radio signals to transmit reliably, was malfunctioning after the impact of the planes. As a result, firefighters were unable to report to commanders on their progress, and were unable to hear evacuation orders. Also, many off-duty firefighters arrived to help, without their radios. FDNY commanders lacked communication with the NYPD, who had helicopters at the scene, or with Emergency Medical Service (EMS) dispatchers. The firefighters on the scene also did not have access to television reports or other outside information, which could help in assessing the situation. When the South Tower collapsed at 9:59 a.m., firefighters in the North Tower were not aware of exactly what had happened. The battalion chief in the North Tower lobby immediately issued an order over the radio for firefighters in the tower to evacuate, but many did not hear the order, due to the faulty radios. Because of this, 343 firefighters died in the collapse of the towers.

The command post located across West Street was taken out when the South Tower collapsed, making command and control even more difficult and disorganized. When the North Tower collapsed, falling debris killed Peter Ganci, the FDNY chief. Following the collapse of the World Trade Center, a command post was set up at a firehouse in Greenwich Village.

The FDNY deployed 200 units (half of all units) to the site, with more than 400 firefighters on the scene when the buildings collapsed. This included a total of 121 engine companies, 62 ladder companies, and other special units. The FDNY also received assistance from fire departments in Nassau, Suffolk, Westchester County, and other neighboring jurisdictions, but with limited ability to manage and coordinate efforts.
Besides assisting with recovery operations at Ground Zero, volunteer firefighters from Long Island and Westchester manned numerous firehouses throughout the city to assist with other fire and emergency calls.

FDNY Emergency medical technicians (EMTs), along with 9-1-1 system ambulances operated by voluntary hospitals and volunteer ambulance corps, began arriving at 8:53 a.m., and quickly set up a staging area outside the North Tower, at West Street, which was quickly moved over to the corner of Vesey and West Streets. As more EMTs responded to the scene, five triage areas were set up around the World Trade Center site. EMS chiefs experienced difficulties communicating via their radios, due to the overwhelming volume of radio traffic. At 9:45, an additional dispatch channel was set aside for use by chiefs and supervisors only, but many did not know about this and continued to operate on the other channel. The communication difficulties meant that commanders lacked good situational awareness.

Dispatchers at the 9-1-1 call center, who coordinate EMS response and assign units, were overwhelmed with incoming calls, as well as communications over the radio system. Dispatchers were unable to process and make sense of all the incoming information, including information from people trapped in the towers, about conditions on the upper floors. Overwhelmed dispatchers were unable to effectively give instructions and manage the situation.

EMS personnel were in disarray after the collapse of the South Tower at 9:59 a.m. Following the collapse of the North Tower at 10:28 a.m., EMS commanders regrouped on the North End of Battery Park City, at the Embassy Suites Hotel. Around 11:00 a.m., EMS triage centers were relocated and consolidated at the Chelsea Piers and the Staten Island Ferry Terminal. Throughout the early afternoon, the soundstages at the pier were separated into two areas, one for the more seriously injured and one for the walking wounded. On the acute side, multiple makeshift tables, each with a physician, nurse, and other health care workers, and non-emergency service volunteers, were set up for the arrival of mass casualties.

Supplies, including equipment for airway and vascular control, were obtained from neighboring hospitals. Throughout the afternoon, local merchants arrived to donate food. Despite this, few patients arrived for treatment, the earliest at about 5 p.m., and were not seriously injured, being limited to smoke inhalation. An announcement was made around 6–7 p.m. that a second shift of providers would cover the evening shift, and that an area was being set up for the day personnel to sleep. Soon after, when it was realized that few would have survived the collapse and be brought to the piers, many decided to leave and the area was closed down.

The New York City Police Department quickly responded with the Emergency Service Units (ESU) and other responders after the crash of American Airlines Flight 11 into the North Tower. The NYPD set up its incident command center at Church Street and Vesey Street, on the opposite side of the World Trade Center from where the FDNY was commanding its operations. NYPD helicopters were soon at the scene, reporting on the status of the burning buildings. When the buildings collapsed, 23 NYPD officers were killed, along with 37 Port Authority Police Department officers. The NYPD helped facilitate the evacuation of civilians out of Lower Manhattan, including approximately 5,000 civilians evacuated by the Harbor Unit to Staten Island and to New Jersey. In ensuing days, the police department worked alternating 12-hour shifts to help in the rescue and recovery efforts.

Immediately after the first attack, the captains and crews of a large number of local boats steamed into the attack zone to assist in evacuation. These ships had responded by a request from the U.S. Coast Guard to help evacuate those stranded on Manhattan Island. Others, such as the "John J. Harvey", provided supplies and water, which became urgently needed after the Towers' collapse severed downtown water mains. Estimates of the number of people evacuated by water from Lower Manhattan that day in the eight-hour period following the attacks range from 500,000 to 1,000,000. Norman Mineta, Secretary of Transportation during the attacks, called the efforts "the largest maritime evacuation conducted in the United States." The evacuation was the largest maritime evacuation in history by most estimates, passing the nine-day evacuation of Dunkirk during World War II. As many as 2,000 people injured in the attacks were evacuated by this means.

Amateur radio played a role in the rescue and clean-up efforts. Amateur radio operators established communications, maintained emergency networks, and formed bucket brigades with hundreds of other volunteer personnel. Approximately 500 amateur radio operators volunteered their services during the disaster and recovery.

The New Jersey Legislature honored the role of Amateur Radio operators in a proclamation on December 12, 2002.

Note: "Government exhibits are from the trial of Zacarias Moussaoui"

On the day following the attacks, 11 people were rescued from the rubble, including six firefighters and three police officers. One woman was rescued from the rubble, near where a West Side Highway pedestrian bridge had been. Two PAPD officers, John McLoughlin and Will Jimeno, were also rescued. Discovered by former U.S. Marines Jason Thomas and Dave Karnes, McLoughlin and Jimeno were pulled out alive after spending nearly 24 hours beneath 30 feet of rubble. Their rescue was later portrayed in the Oliver Stone film, "World Trade Center". In total, only twenty survivors were pulled out of the rubble. The final survivor, Port Authority secretary Genelle Guzman-McMillan, was rescued 27 hours after the collapse of the North Tower.

Some firefighters and civilians who survived made cell phone calls from voids beneath the rubble, though the amount of debris made it difficult for rescue workers to get to them.

By Wednesday night, 82 deaths had been confirmed by officials in New York City.

Rescue efforts were paused numerous times in the days after the attack, due to concerns that nearby buildings, including One Liberty Plaza, were in danger of collapsing.

The search and rescue effort in the immediate aftermath at the World Trade Center site involved ironworkers, structural engineers, heavy machinery operators, asbestos workers, boilermakers, carpenters, cement masons, construction managers, electricians, insulation workers, machinists, plumbers and pipefitters, riggers, sheet metal workers, steelworkers, truckers and teamsters, American Red Cross volunteers, and many others.
Lower Manhattan, south of 14th Street, was off-limits, except for rescue and recovery workers. There were also about 400 working dogs, the largest deployment of dogs in the nation's history.

New York City Office of Emergency Management was the agency responsible for coordination of the City's response to the attacks. Headed by then-Director Richard Sheirer, the agency was forced to vacate its headquarters, located in 7 World Trade Center, within hours of the attack. The building later collapsed. OEM reestablished operations temporarily at the police academy, where Mayor Giuliani gave many press conferences throughout the afternoon and evening of September 11. By Friday, rescue and reliefs were organized and administered from Pier 92 on the Hudson River.

Volunteers quickly descended on Ground Zero to help in the rescue and recovery efforts. At Jacob Javits Convention Center, thousands showed up to offer help, where they registered with authorities. Construction projects around the city came to a halt, as workers walked off the jobs to help at Ground Zero. Ironworkers, welders, steel burners, and others with such skills were in high demand. By the end of the first week, over one thousand ironworkers from across North America had arrived to help, along with countless others.

The New York City Department of Design & Construction oversaw the recovery efforts. Beginning on September 12, the Structural Engineers Association of New York (SEAoNY) became involved in the recovery efforts, bringing in experts to review the stability of the rubble, evaluate safety of hundreds of buildings near the site, and designing support for the cranes brought in to clear the debris. The City of New York hired the engineering firm, LZA-Thornton Tomasetti, to oversee the structural engineering operations at the site.

To make the effort more manageable, the World Trade Center site was divided into four quadrants or zones. Each zone was assigned a lead contractor, and a team of three structural engineers, subcontractors, and rescue workers.


The Federal Emergency Management Agency (FEMA), the United States Army Corps of Engineers, the Occupational Safety and Health Administration (OSHA), and the New York City Office of Emergency Management (OEM) provided support. Forestry incident management teams (IMTs) also provided support beginning in the days after the attacks to help manage operations.

A nearby Burger King restaurant was used as a center for police operations. Given that workers worked at the site, or "The Pile", for shifts as long as twelve hours, a specific culture developed at the site, leading to workers developing their own argot.

"The Pile" was the term coined by the rescue workers to describe the 1.8 million tons of wreckage left from the collapse of the World Trade Center. They avoided the use of "ground zero", which describes the epicenter of a bomb explosion.

Numerous volunteers organized to form "bucket brigades", which passed 5-gallon buckets full of debris down a line to investigators, who sifted through the debris in search of evidence and human remains. Ironworkers helped cut up steel beams into more manageable sizes for removal. Much of the debris was hauled off to the Fresh Kills Landfill on Staten Island where it was further searched and sorted.

Some of the steel was reused for memorials. New York City firefighters donated a cross made of steel from the World Trade Center to the Shanksville Volunteer Fire Company in Shanksville, Pennsylvania. The beam, mounted atop a platform shaped like the Pentagon, was erected outside the Shanksville's firehouse near the crash site of United Airlines Flight 93.

Twenty-four tons of the steel used in construction of USS "New York" (LPD-21) came from the small amount of rubble from the World Trade Center preserved for posterity.

Hazards at the World Trade Center site included a diesel fuel tank buried seven stories below. Approximately 2,000 automobiles that had been in the parking garage also presented a risk, with each containing, on average, at least five gallons of gasoline. Once recovery workers reached down to the parking garage level, they found some cars that had exploded and burned. The United States Customs Service, which was housed in 6 World Trade Center, had 1.2 million rounds of ammunition and weapons in storage in a third-floor vault, to support their firing range.

In the hours immediately after the attacks on the World Trade Center, three firefighters raised an American flag over the rubble. The flag was taken from a yacht, and the moment, which was captured on a well-known photograph, evoked comparisons to the iconic Iwo Jima photograph. Morale of rescue workers was boosted on September 14, 2001 when President George W. Bush paid a visit to Ground Zero. Standing with retired firefighter Bob Beckwith, Bush addressed the firefighters and rescue workers with a bullhorn and thanked them. Bush remarked, "I'm shocked at the size of the devastation, It's hard to describe what it's like to see the gnarled steel and broken glass and twisted buildings silhouetted against the smoke. I said that this was the first act of war on America in the 21st century, and I was right, particularly having seen the scene." After some workers shouted that they could not hear the President, Bush famously responded by saying "I can hear you! The rest of the world hears you. And the people who knocked these buildings down will hear all of us soon!"

At some point, rescue workers realized that they were not going to find any more survivors. After a couple of weeks, the conditions at Ground Zero remained harsh, with lingering odors of decaying human remains and smoke. Morale among workers was boosted by letters they received from children around the United States and the world, as well as support from thousands of neighbors in TriBeCa and other Lower Manhattan neighborhoods.

This support continued to spread and eventually led to the founding of over 250 non-profit organizations of which raised almost $700 million within their first two years of operation. One of the nonprofits included One Day's Pay, later changed to MyGoodDeed, which championed the effort to designate September 11 as an official National Day of Service (9/11 Day).

By 2012, many of the 250 plus organizations had disbanded due to lack of funding as the years progressed. Of the ones that remain, a handful remained functioning for those who remain in need. One of these organizations, Tuesday's Children, was founded the day after September 11 in hopes of supporting the children immediately affected by the attacks. The founder of this non-profit, David Weild IV, now calls them one of the "last men standing" in that they are now one of the few remaining organizations who "provide direct services for what social-service groups and survivors of the attacks call the '9-11 Community.'"

Other notable non-profits who are "still standing" include:

Immediately following the attacks, members of the Civil Air Patrol (CAP) were called up to help respond. Northeast Region placed their region personnel and assets on alert mere moments after they learned of the attack. With the exception of CAP, civilian flights were grounded by the Federal Aviation Administration. CAP flew aerial reconnaissance missions over Ground Zero, to provide detailed analysis of the wreckage and to aid in recovery efforts, including transportation of blood donations.

Elements of the New York Army National Guard's 1-101st Cavalry (Staten Island), 258th Field Artillery, and 69th Infantry Regiment based in Manhattan were the first military force to secure Ground Zero on September 11th. The 69th Infantry's armory on Lexington Avenue became the Family Information Center to assist persons in locating missing family members.
The National Guard supplemented the NYPD and FDNY, with 2,250 guard members on the scene by the next morning. Eventually thousands of New York Army and Air National Guardsmen participated in the rescue/recovery efforts. They conducted site security at the WTC, and at other locations. They provided the NYPD with support for traffic control, and they participated directly in recovery operations providing manpower in the form of "bucket brigades" sorting through the debris by hand.

Additionally service members provided security at a variety of location throughout the city and New York State to deter further attacks and reassure the public.

Members of the Air National Guard's 109th Airlift Wing out of Scotia, and Syracuse's 174th Fighter Wing immediately responded to New York City, setting up camp at places such as Fort Hamilton. Mostly civil engineers, firefighters and military police, they greatly aided in the clean-up effort. F-16s from the 174th Fighter Wing also ramped up their flying sorties and patrolled the skies.

The New Jersey National Guard assisted the New York National Guard's efforts following the attacks.

U.S. Marines were also present to assist in the rescue efforts. No official numbers of men who helped out was released but there was evidence that they were there.
Films such as 2006 docudrama "World Trade Center" talked of two Marines who rescued two trapped police officers in the rubble. US Marines were headquartered at 340 Westside Hwy Bloomberg News Building. The commanding officer was Navy Commander Hardy, and executive officer was Maj Priester. These two oversaw 110 military personnel of various branches, various police departments and EMTs.

U.S. Navy deployed a hospital ship USNS "Comfort" (T-AH-20) to Pier 92 in Manhattan. Crew members provided food and shelter for more than 10,000 relief workers. Comfort's 24-hour galley also provided an impressive 30,000 meals. Its medical resources were also used to provide first-aid and sick call services to nearly 600 people. The ship's psychological response team also saw more than 500 patients.

A May 14, 2007, "New York Times" article, "Ground Zero Illness Clouding Giuliani's Legacy," gave the interpretation that thousands of workers at Ground Zero have become sick and that "many regard Mr. Giuliani's triumph of leadership as having come with a human cost." The article reported that the mayor seized control of the cleanup of Ground Zero, taking control away from established federal agencies, such as the Federal Emergency Management Agency, the U.S. Army Corps of Engineers and the Occupational Safety and Health Administration. He instead handed over responsibility to the "largely unknown" city Department of Design and Construction. Documents indicate that the Giuliani administration never enforced federal requirements requiring the wearing of respirators. Concurrently, the administration threatened companies with dismissal if cleanup work slowed.

Workers at the Ground Zero pit worked without proper respirators. They wore painters' masks or no facial covering. Specialists claim that the only effective protection against toxins, such as airborne asbestos, is a special respirator. New York Committee for Occupational Safety and Health industrial hygienist David Newman said, "I was down there watching people working without respirators." He continued, "Others took off their respirators to eat. It was a surreal, ridiculous, unacceptable situation."

The local EPA office sidelined the regional EPA office. Dr. Cate Jenkins, a whistle-blower EPA scientist, said that on September 12, 2001, a regional EPA office offered to dispatch 30 to 40 electron microscopes to the WTC pit to test bulk dust samples for the presence of asbestos fibers. Instead, the local office chose the less effective polarized light microscopy testing method. Dr. Jenkins alleged that the local office refused, and said, "We don't want you fucking cowboys here. The best thing they could do is reassign you to Alaska."

There were many health problems caused by the toxins. 99% of exposed firefighters reported at least one new respiratory problem while working at the World Trade Center site that they had not experienced before. Chronic airway disease is the main lung injury among firefighters who were exposed to toxins during 9/11. Six years after the attacks, among those who never smoked, approximately 13% of firefighters and 22% of EMS had lungs that did not function as well as others around the same age. Steep declines in pulmonary lung function has been a problem since first detected among firefighters and EMS within a year of 9/11 have persisted. 
Increasing numbers of Ground Zero workers are getting illnesses, such as cancer. Between September 11, 2001 through 2008, there were 263 new cases of cancer found in 8,927 male firefighters who responded to 9/11 attacks. This number is 25 more than what is expected from men from a similar age group and race. There is a 19% increase in cancer overall, between firefighters who responded to the attacks and those who were not exposed to toxins from responding to the attacks on September 11.

On January 30, 2007, Ground Zero workers and groups such as Sierra Club and Unsung Heroes Helping Heroes met at the Ground Zero site and urged President George Bush to spend more money on aid for sick Ground Zero workers. They said that the $25 million that Bush promised for the ill workers was inadequate. A Long Island iron-worker, John Sferazo, at the protest rally said, "Why has it taken you 5½ years to meet with us, Mr. President?"

Firefighters, police and their unions, have criticized Mayor Rudy Giuliani over the issue of protective equipment and illnesses after the attacks. An October study by the National Institute of Environmental Safety and Health said that cleanup workers lacked adequate protective gear. The Executive Director of the National Fraternal Order of Police reportedly said of Giuliani: "Everybody likes a Churchillian kind of leader who jumps up when the ashes are still falling and takes over. But two or three good days don't expunge an eight-year record." Sally Regenhard, said, "There's a large and growing number of both FDNY families, FDNY members, former and current, and civilian families who want to expose the true failures of the Giuliani administration when it comes to 9/11." She told the "New York Daily News" that she intends to "Swift Boat" Giuliani.

Various health programs arose after the attacks to provide treatment for 9/11-related illnesses among responders, recovery workers, and other survivors. When the James Zadroga 9/11 Health and Compensation Act became federal law in January 2011, these programs were replaced by the World Trade Center Health Program.

Soon after the attacks, New York City commissioned McKinsey & Company to investigate the response of both the New York City Fire Department and New York City Police Department and make recommendations on how to respond more effectively to such large-scale emergencies in the future.

Officials with the International Association of Fire Fighters have also criticized Rudy Giuliani for failing to support modernized radios that might have spared the lives of more firefighters. Some firefighters never heard the evacuation orders and died in the collapse of the towers.

Estimated total costs, as of October 3, 2001

Plans for the World Trade Center rebuilding started in July 2002 which was headed by the Lower Manhattan Development Corporation. There were many proposals on how to build the World Trade Center back however many lacked creativity. Several architects were chosen throughout this construction process, but all of them ran into many problems with the design. The financial crisis in 2008 also forced the construction process over to the Port Authority; however, the Port Authority construction is not going as smoothly as planned. City officials are looking for better ways to lower the problems and delays. The World Trade Center completion of construction was scheduled for 2016. , three of seven planned buildings were completed, as well as the transportation hub, 9/11 Memorial, and Liberty Park.


Notes
Bibliography

New York Times:

Other:


</doc>
<doc id="28070" url="https://en.wikipedia.org/wiki?curid=28070" title="Communication during the September 11 attacks">
Communication during the September 11 attacks

Communication problems and successes played an important role in the September 11, 2001 attacks and their aftermath. Systems were variously destroyed or overwhelmed by loads greater than they were designed to carry, or failed to operate as intended or desired.
The organizers of the September 11, 2001 attacks apparently planned and coordinated their mission in face to face meetings and used little or no electronic communication. This "radio silence" made their plan more difficult to detect.

According to 9/11 Commission staff statement No. 17 there were several communications failures at the federal government level during and after the 9/11 attacks. Perhaps the most serious occurred in an "Air Threat Conference Call" initiated by the National Military Command Center (NMCC) after two planes had crashed into the World Trade Center, but shortly before The Pentagon was hit. The participants were unable to include the Federal Aviation Administration (FAA) air traffic control command center, which had the most information about the hijackings, in the call.

According to the staff report: 
Operators worked feverishly to include the FAA in this teleconference, but they had equipment problems and difficulty finding secure phone numbers. NORAD asked three times before 10:03 to confirm the presence of FAA on the conference, to provide an update on hijackings. The FAA did not join the call until 10:17. The FAA representative who joined the call had no familiarity with or responsibility for a hijack situation, had no access to decision makers, and had none of the information available to senior FAA officials by that time.

We found no evidence that, at this critical time, during the morning of September 11, NORAD’s top commanders, in Florida or Cheyenne Mountain Complex, ever coordinated with their counterparts at FAA headquarters to improve situational awareness and organize a common response. Lower-level officials improvised—the FAA’s Boston Center bypassing the chain of command to contact NEADS. But the highest level Defense Department officials relied on the NMCC’s Air Threat Conference, in which FAA did not meaningfully participate.

After the 1993 World Trade Center bombing, radio repeaters for New York City Fire Department communication were installed in the tower complex. Because they were unaware that several controls needed to be operated to fully activate the repeater system, fire chiefs at their command post in the lobby of the North Tower thought the repeater was not functioning and did not use it, though it did work and was used by some firefighters. When police officials concluded the twin towers were in danger of collapsing and ordered police to leave the complex, fire officials were not notified. Fire officials on the scene were not monitoring broadcast news reports and did not immediately understand what had happened when the first (South) tower did collapse. 

There was little communication between New York City Police Department and fire department commands even though an Office of Emergency Management (OEM) had been created in 1996 in part to provide such coordination. A primary reason for OEM's inability to coordinate communications and information-sharing in the early hours of the WTC response was the loss of its emergency operations center, located on the twenty third floor of 7 World Trade Center which had been evacuated after debris from tower's collapse struck the building, igniting several fires.

Emergency relief efforts in both Lower Manhattan and at the Pentagon were augmented by volunteer amateur radio operators in the weeks after the attacks.

Cell phones and in-plane credit card phones played a major role during and after the attack, starting with hijacked passengers who called family or notified the authorities about what was happening. Passengers and crew who made calls include: Sandra Bradshaw, Todd Beamer, Tom Burnett, Mark Bingham, Peter Hanson, Jeremy Glick, Barbara K. Olson, Renee May, Madeline Amy Sweeney, Betty Ong, Robert Fangman, Brian David Sweeney, and Ed Felt. Innocent occupants aboard United Airlines Flight 93 were able to assess their situation based on these conversations and plan a revolt that resulted in the aircraft crashing. According to the commission staff: "Their actions saved the lives of countless others, and may have saved either the U.S. Capitol or the White House from destruction."

According to the 9/11 Commission Report, 13 passengers from Flight 93 made a total of over 30 calls to both family and emergency personnel (twenty-two confirmed air phone calls, two confirmed cell phone and eight not specified in the report). Brenda Raney, Verizon Wireless spokesperson, said that Flight 93 was supported by several cell sites. There were reportedly three phone calls from Flight 11, five from Flight 175, and three calls from Flight 77. Two calls from these flights were recorded, placed by flight attendants: Betty Ong on Flight 11 and CeeCee Lyles on Flight 93 

Alexa Graf, an AT&T spokesperson, said it was almost a fluke
that the calls reached their destinations. Marvin Sirbu, professor of Engineering and Public Policy at Carnegie Mellon University said on September 14, 2001, that "The fact of the matter is that cell phones can work in almost all phases of a commercial flight." Other industry experts said that it is possible to use cell phones with varying degrees of success during the ascent and descent of commercial airline flights.

After each of the hijacked aircraft struck the World Trade Center, people inside the towers made calls to family and loved ones; for the victims, this was their last communication. Other callers directed their pleas for help to 9-1-1. Over nine hours of the 9-1-1 calls were eventually released after petitioning by The "New York Times" and families of the WTC victims. In 2001, U.S. cell phones did not yet have texting or photography capabilities that came by the mid-2000s.

After the attack, the cell phone network of New York City was rapidly overloaded (a mass call event) as traffic doubled over normal levels. Cell phone traffic also overloaded across the East Coast, leading to crashes of the cell phone network. Verizon's downtown wire phone service was interrupted for days and weeks because of cut subscriber cables, and to the 140 West Street exchange being shut for days. Capacity between Brooklyn and Manhattan was also diminished by cut trunk cables.

Following the attacks, the issues with the cell network weren't resolved until 36 cellular COWs (cell towers on wheels) were deployed by September 14, 2001, in Lower Manhattan to support the U.S. Federal Emergency Management Agency (FEMA) and provide critical phone service to rescue and recovery workers.

Since three of the major television broadcast network owned-and-operated stations had their transmission towers atop the North Tower (One World Trade Center), coverage was limited after the collapse of the tower. The FM transmitter of National Public Radio station WNYC was also destroyed in the collapse of the North Tower and its offices evacuated. For an interim period, it continued broadcasting on its AM frequency and used NPR's New York offices to produce its programming.
The satellite feed of one television station, WPIX, froze on the last image received from the WTC mast; the image (a remote-camera shot of the burning towers), viewable across North America (as WPIX is available on cable TV in many areas), remained on the screen for much of the day until WPIX was able to set up alternate transmission facilities. It shows the WTC at the moment power cut off to the WPIX transmitter, prior to the towers' collapse.

During the September 11 attacks, WCBS-TV channel 2 and WXTV-TV channel 41 stayed on the air. Unlike most other major New York television stations, WCBS-TV maintained a full-powered backup transmitter at the Empire State Building after moving its main transmitter to the North Tower of the World Trade Center. The station was also simulcasted nationally on Viacom (which at the time owned CBS) cable network VH1 that day. In the immediate aftermath of the attacks, the station lent transmission time to the other stations who had lost their transmitters, until they found suitable backup equipment and locations.

The Emergency Alert System was never activated in the terrorist attacks, as the extensive media coverage made it unnecessary.

AT&T eliminated any costs for domestic calls originating from the New York City area (phones using area codes 212, 718, 917, 646, and 347) in the days following 9/11.

Radio communications during the September 11 attacks served a vital role in coordinating rescue efforts by New York Police Department, New York Fire Department, Port Authority Police Department, and emergency medical services.

While radio communications were modified to address problems discovered after the 1993 World Trade Center bombing, investigations into the radio communications during the September 11th attacks discovered that communication systems and protocols that distinguished each department was hampered by the lack of interoperability, damaged or failed network infrastructure during the attack, and overwhelmed by simultaneous communication between superiors and subordinates.

A rough time line of the incident could include:

The scale of the incident was described in the National Commission report on the attacks as "unprecedented". In roughly seventeen minutes from 8:46 to 9:03 am, over a thousand police, fire, and emergency medical services (EMS) staff arrived at the scene. At some point during a large incident, any agency will reach a point where they find their resources overrun by needs. For example, the Port Authority Police could not schedule staff as if a September 11 attack would occur every shift. There is always a balance struck between readiness and costs. There is conflicting data but some sources suggest there may have been 2,000 to 3,000 workers involved in the rescue operation. It would be rare for most agencies to see an event where there were that many people to be rescued.

There is some level of confusion present in any large incident. The National Institute for Standards and Technology (NIST) asserts commanders did not have adequate information and interagency information sharing was inadequate. For example, on September 11, persons in the New York City Police Department (NYPD) 9-1-1 center told callers from the World Trade Center to remain in place and wait for instruction from firefighters and police officers. This was the plan for managing a fire incident in the building and the 9-1-1 center staff were following the plan. This was partly countered by public safety workers going floor-by-floor and telling people to evacuate. The Commission report suggests people in the NYPD 9-1-1 center and New York City Fire Department (FDNY) dispatch would benefit from better situation awareness. The Commission described the call centers as not "fully integrated" with line personnel at the WTC. The report suggests the NYPD 9-1-1 center and FDNY dispatch were overrun by call volumes that had never been seen before. Adding to the confusion, radio coverage problems, radio traffic blocking, and building system problems occurred inside the burning towers. The facts show that much of the equipment worked as designed and users made the best of what was available to them.

Typical of any large fire, many 9-1-1 calls with conflicting information were received beginning at 8:46 am. In addition to reports that a plane had hit the World Trade Center, the EMS computer-aided dispatch (CAD) log shows reports of a helicopter crash, explosions, and a building fire. Throughout the incident, people at different locations had very different views of the situation. After the collapse of the first tower, many firefighters in the remaining tower had no idea the first tower had fallen.

A factor in radio communications problems included the fact that off-duty personnel self-dispatched to the incident scene. Some off-duty staff went into the towers without radios. According to the Commission report and news coverage, this was true of NYPD, Port Authority Police Department (PAPD), and FDNY personnel. Regardless of any radio coverage problems, these persons could not be commanded or informed by radio. In any incident of this scale, self-dispatched staff without radios would likely be a problem. Even if a cache of radios were brought to the scene to hand out, the scale of this incident would be likely to overrun the number of radios in the cache.

NIST concluded, at the beginning of the incident, there was an approximate factor of five (peak) increase in radio communications traffic over a normal level. After the initial peak, radio traffic through the incident followed an approximate factor of three steady increase. FDNY recordings suggest the dispatch personnel were overloaded: both fire and EMS dispatch were often delayed in responding to radio calls. Many 9-1-1 telephone calls to dispatch were disconnected or routed to "all circuits are busy now", intercept recordings.

NIST calculated that about one third of radio messages transmitted during the surge of communications were incomplete or unintelligible. Documentary footage suggests the tactical channels were also overloaded; some footage captured audio of two or three conversations occurring simultaneously on a particular channel.

In this study of WTC incident communications, radio systems used at the site had problems but were generally effective in that users were able to communicate with one another. A 2002 video documentary "9/11" by Gedeon and Jules Naudet, (referred to as "the documentary") was reviewed. It captured audio from hand-held radios in use at the incident and showed users communicating over radios from the lobby command post in the North Tower. 26 Red Book audio CDs of New York City Fire Department radio transmissions, covering the incident's initial dispatch and the tower failures, were reviewed. These CDs were digitized versions of audio from the Fire Department's logging recorders. In addition, text on an oral history CD with transcripts of fire personnel debriefed on the incident were reviewed.

In 2001, the NYPD used Ultra High Frequency (UHF) radios and divided the city into 35 radio zones. Most hand-held radios had at least 20 channels: while not all officers had all channels, all officers had the ability to communicate citywide. As a characteristic of physics, UHF signals penetrate buildings better than lower Very High Frequency (VHF) frequencies used by the FDNY fire units but generally cover shorter distances over open terrain. The Commission report did not cite any technical flaws with the NYPD radio system.

PAPD has systems described as "low-power UHF". The Commission report says the systems were specific to a single site with the exception of one channel which was Port-Authority-wide. It's unclear whether the PAPD systems were interstitial and limited to 2 watts output, used normal local-control channels but were limited in power output by the frequency coordinator, or used leaky cable systems which were solely intended to work inside the Port Authority buildings. The report says there were 7, site-specific Port Authority Police channels. In 2001, officers at one site could not, (in all cases), carry their radios to another site and use them. Not all radios had all channels.

Recordings of Citywide, Brooklyn, and Manhattan channels for Fire and Citywide, Brooklyn, and Manhattan channels for Emergency medical services were reviewed. Systems generally performed well. The audio coupling point for the logging recorder on Manhattan Fire made the dispatcher's voice difficult to hear. An anonymous fire dispatcher who identifies as "Dispatcher 416" is noteworthy.

The Commission report says that, in 2001, FDNY used a system with 5 repeater channels: one for each of the boroughs of Manhattan, Brooklyn, Queens, with the Bronx and Staten Island sharing a single frequency using different Private Line (PL) tones, and a citywide channel. There were also five simplex channels in FDNY radios.

Observation shows, back in 2001, that the citywide EMS channel was voting more frequently than normal, signals were noisy, interfering signals were present, and that some receiver sites had equalization differences. Some transmissions had choppy audio possibly representative of interference from FSK paging or intermittent microwave radio paths to one or more receiver sites. For example, if a microwave radio path fails for half-second intervals, the voting comparator may vote out that receiver site until silence is detected. This can cause dropped syllables in the voted audio. Some transmissions were noisy, although transactions show the dispatcher was understanding radio traffic in spite of audio drop-outs in almost every case.

The Port Authority repeater, intended to allow communications inside the towers, did not appear to work as intended on September 11. The system, also called "Port Authority Channel 30", was installed after the 1993 World Trade Center attack. News accounts said the system had been turned off for unspecified technical reasons. The Commission report said it was customary to turn the system off because it somehow caused interference to radios in use at fire operations in other parts of the city. The documentary film gives different information, with a Fire Department member from Engine 7/Ladder 1 claiming that the aircraft's impact caused the system to fail. Evidence suggests the remote control console in the lobby command was not working but the repeater was. The radio repeater was located in 5 World Trade Center. A remote control console was connected to the repeater allowing staff at the North Tower lobby command post to communicate without using a hand-held radio.

In a review of the logging recorder track of the Port Authority repeater, someone arrived early during the incident and began to establish a command post. From the command post in the lobby of the North Tower (1 World Trade Center), the user can be heard trying to transmit using a remote control unit. After several failed attempts to communicate with a user on the channel, the user steps through every channel selection on the remote, trying each one. The recording contains the tone remote control console stepping through all of its eight function tones. Someone says, "... the wireline isn't working", over the Port Authority channel. Something that looks like a Motorola T-1380-series remote is shown in the documentary. The fact that users pressing buttons on the remote control can clearly be heard on the logging recorder shows the transmit audio path was working. The recording does not reveal whether or not the console function tones were keying the transmitter.

Some users in the North Tower lobby interpreted the remote control unit not working as a failure of the entire channel. Other fire units, not knowing the channel had failed, arrived and began using it successfully. The recordings show at least some units were successfully using the repeater to communicate inside the South Tower until the moment it collapsed. The Commission report says the North Tower lobby command may not have worked because of a technical problem, the volume control turned all the way down, or because a button that must be pressed to enable it had not been pushed.

On the audio track, an outside agency, possibly in New Jersey and using a repeater, comes through the receive audio on the Port Authority Repeater 7 system. An ambulance being dispatched by the outside (non-FDNY) agency is heard. This may be what the FDNY had described as interference caused when the repeater was left enabled at all times. The distant user appears to be repeated through the system, (possibly on the same CTCSS tone as was configured in Repeater 7). This appears to be a distant co-channel user on the same input frequency as Repeater 7. It's possible that by the random button pressing, a user sent a function tone that temporarily put the base station in "monitor" and that's what caused the outside agency's traffic to be heard. This is unlikely because subsequent transmit function tones should have toggled the receiver from monitor back to CTCSS-enabled.

An oral history interview revealed the Port Authority UHF radios were normally used at incidents inside the World Trade Center. The interviewee said in normal, day-to-day calls, the WTC staff handed Port Authority UHF radios to firefighters on their arrival and that these radios, "worked all over." This implies, but does not prove, that it was common knowledge among department members that FDNY radios had coverage problems inside the buildings. The 9-11 Commission uses the phrase, "performed poorly" to describe FDNY radios during the incident.

Oral history files show that at least four channels were employed at WTC:

One officer said a channel named "Command 3" was used for the North Tower. To those unfamiliar with the details of the FDNY system, it is unclear whether the interviewee meant Tactical 3 or a fifth channel.

FDNY personnel are seen using radios during the documentary footage of the WTC lobby area. Analysis of these scenes showed the radios all appeared to be receiving properly. Oral history files confirm radio communications were at least partly functional.

A problem that shows up in these types of incidents is that receivers in hand-held radios are subjected to signal levels that are likely to overload the receiver. Several radios may be transmitting within feet of one another on different channels. If overloading occurs, only very strong signals can be received while weaker signals disappear and are not received. The hand held radio receivers shown in the documentary appeared to work properly even though several other hand-held radios were transmitting only feet away. This is a hostile environment and suggests the hand-held equipment used by FDNY had good quality receivers, though in this case, the suggestion is incorrect. Second-hand observation is hardly the proper way to 'test' radio receivers or to distinguish 'good quality' from 'bad' and this is likely a source of continued misunderstanding; particularly when these same radios were operating at higher floors, in closer proximity to, and in direct line-of-sight of digital cellular repeaters. Those repeaters were likely operating at unlicensed power levels, which was a common practice of cellular providers at the time, and continues to this day. Footage reveals intelligible recovered audio coming out of the radios and shows radio users communicating with others. This may not have been true of the entire WTC complex but was true of radio users in the crowded lobby.

Analysis of the 26 FDNY audio CDs showed the radios seemed to transmit into the radio systems okay. Radios calling dispatch got through. Calling units were intelligible. Users spoke with dispatchers. Dispatchers answered in ways that suggest they understood what was said. There were no noisy or truncated transmissions heard on any channel, (the equivalent of a dropped cellular call). This suggests the Fire Department's radio backbone is soundly designed and working properly. It is possible that system coverage problems are present; problems that could have been mitigated had the Command Post radio (with greater transmit power) been used. It is also likely that some transmissions did not reach any of the receivers in the system and therefore would not be a detectable problem when listening to the recordings. At the same time those recordings were made, the cellular system was operating at or near full-capacity, meaning every cellular repeater was transmitting. The dense RF interference environment created in NYC that day was essentially a 'perfect storm'; one in which a radio designed 25 years prior could not possibly contend with.

In some scenes, captured documentary audio showed the channels were busy. In some cases, two or more conversations were taking place over a single radio channel at the same time. Users on Tactical 1 may have been close enough to one another to communicate because signals in proximity to each other would overpower weaker signals. At any incident of this size, there is likely to be some overlapping radio traffic. In the same way that large incidents exhaust all the firefighting vehicles and staff, the radio channel resources may become taxed to their limits. NIST says about one third of the fire department radio transmissions were not complete or not understandable.

Some radio users had selected the wrong channels. For example, on the Repeater 7 channel, a unit was heard to call "Manhattan" dispatch and "Citywide". Although the circumstances that lead to the user selecting the wrong channel are not known, this can occur when the user is trapped in darkness or smoke and cannot see the radio. Users will typically try to count steps in a rotary switch channel selector starting from one end of the switch's travel.

A communications van operated by FDNY responded to the incident. Its radio identifier was, "Field Comm." A backup van was in use on the day of the incident because the primary van was out-of-service. The backup van was destroyed and audio recordings of tactical channels used at the incident site were lost.

One annoyance with the fire systems was the presence of "unit ID" data bursts. These constant squawks, heard at the end of transmissions, are decoded at dispatch to identify the calling radio. The annoyance of the data bursts is a trade-off that could help find a firefighter who has been injured or needs help. It also automatically displays the unit ID at the dispatch console. In most systems, it also saves dispatch personnel from typing the unit ID. They press one key and the calling unit's ID is inserted into the current CAD screen or command line.

Recordings show radios were programmed to send unit ID on tactical channels. Radios accept unit ID on a per-channel basis. When mobile or hand-held radios are programmed, the unit ID encoders should be disabled on all channels where the feature is not used. This saves air time for about two to three syllables of speech per push-to-talk press. For example, unless the communications van or chief's vehicles had push-to-talk unit ID decoders, or the channels were recorded for later analysis where unit IDs were decoded from the recordings, the encoders should be turned off for tactical channels to reduce air time used.

It also sounded like some vehicle radios may have had "status buttons" using the data bursts. If true, the operator presses a button on the vehicle radio which sends a short data burst to dispatch. Dispatch gets the unit identity and the new status from a data decoder. These can cause interruptions in voice traffic but cut down on total air time required to conduct business because they occupy the channel for less time than it takes to say, "Engine fifty on scene."

This channel was the primary method of communication in the North tower. It was a simplex channel. Users complained it would only reach from the lobby to floors in the thirties. Tactical 1 was a default channel for use at some fire scenes. Some users who realized Repeater 7 was functional switched to that channel and were afforded better coverage than simplex users on Tactical 1. Audio recordings on the documentary film and NIST analysis show Tactical 1 was overloaded with heavy radio traffic. In contrast, the audio CD of Repeater 7 shows the channel was mostly idle.

The 9-11 commission report said a new portable repeater system had been developed to address shortcomings of Tactical 1 at a large incident. The system, called, "the post", is carried to an area near the incident and set up for the duration to augment weak signals.

The command channel used by officers at the incident was either called "Channel 5" or "Command 5" in documentation. Documents suggest the channel had a repeater but it was not clear if the repeater was citywide, installed in the Field Comm van, or housed in a battalion chief's vehicle. Recordings of this channel were lost when the Field Comm van was destroyed. The documentary film and oral history records show the channel being used effectively.

The federal 9/11 Commission Report included recommendations on communications systems used by police, fire, and emergency medical services (EMS) at the WTC incident. In the report and in appearances on television news programs, commissioners said the capabilities of communications systems lacked the ability to communicate across department lines. That is to say, police units could not communicate with fire units directly by radio. Ambulances could not talk with police units directly by radio. Commission member Lee Hamilton, in several television appearances related to a 2006 book on the topic of the WTC incident, reiterated this factually correct view.

An example that was cited by Hamilton: during the incident the Police Department helicopter was unable to communicate with Fire Department units in order to warn them of the towers' imminent collapse. The NIST document suggests the helicopter may have been able to offer several minutes warning. "Several minutes" may have been enough to get some people from the lower floors outside. This warning of imminent collapse went out over at least one police radio channel but there is nothing showing it was relayed to other people or channels. FDNY operates at least two communications vans: one of which was brought to the scene at the WTC incident. The Commission report reveals the primary FDNY van was equipped to talk to NYPD helicopters but the backup van (which had no NYPD helicopter capability) was in use on September 11, 2001.

In practice, many US helicopters used in emergency services are equipped with radios that allow communications on nearly any conventional two-way radio system, so long as the aircrew know the frequency and associated signaling tones. The radios usually have presets, like a car's broadcast radio, that allow some channels to be configured ahead of need. There was no information in the Commission report suggesting NYPD helicopters had such a capability.

While it is technically possible to implement communications across departments, doing so introduces a host of new training and incident command problems. These are problems that would need to be managed in addition to the existing set of issues present at any large incident. The ability to maintain command, and monitor the safety of, groups working at an incident is diminished if a group of firefighters cannot be reached because they've switched over to the EMS channel. This could cause people to be sent to rescue them when there was no need. Similarly, if the Manhattan EMS dispatcher can't reach an ambulance because they are on one of the fire channels, patient care is affected. New York City Police Commissioner Raymond Kelly, appearing on the "Charlie Rose" show, expressed his view that the existing radio systems performed satisfactorily during the WTC incident. In his view, the interoperability desired by the 9-11 Commission was not needed.

These problems are not new to the World Trade Center incident; cross-department and cross-discipline communication has been a hotly contested and long-identified issue. For example, at the Oklahoma City federal building bombing incident, the inability to communicate among departments was also cited as a problem. Firefighters heard an evacuation order on their radio channel because of the reported presence of a second bomb. Police and EMS workers reportedly did not know of the order.

In Hurricane Katrina's wake, a sergeant in the Louisiana Department of Wildlife and Fisheries appeared on national television to describe not being able to reach persons from other agencies who were assisting with the recovery. She described seeing the people in a nearby boat but not being able to communicate with them.

Even if the technical problems are solved, the issue is more complicated than just adding radio channels or talk groups. It is also a cultural problem. In one local incident, a large number of officers from three police agencies were fielded to search for a violent criminal who had evaded officers from one of the agencies. The officers did not coordinate by switching to a shared radio channel. After the incident, one participant said the users thought their radios were incompatible and did not understand how the shared channel worked. This possibly reflects a training problem or a technology literacy problem. The problem seems to have been remedied since then.

In another instance, a fire agency had thoroughly trained for interoperability scenarios. During an incident where two agencies with different radio channels responded, a decision-maker said personnel from his agency would stay on their own channel. Decision-makers may occasionally act in unpredictable ways, even if technology literate and well-trained. It is not solely a technical problem, but an operational problem as well. Changes to ICS command structure, or operational changes in how the command post for an incident is set up, may produce better results than buying equipment or adding channels. Sometimes there are interoperability problems even where a structure for interoperability exists.

One view of the Incident Command System is that units across department lines would communicate with their own representative at the command post or division level. That representative would relay any needs to another department. For example, a fire unit requesting five paramedic ambulances would identify the magnitude of a medical problem to their fire officer at the command post. This request would add to their commander's operational picture of the division or incident command as she called EMS to request the ambulances. Situation awareness is an important part of effective command and is easy to lose at a large incident. Bypassing incident commanders can contribute to a decomposing of command.

One approach to cross-department netting is the capability of some modern trunked systems to provide a function called "dynamic regrouping"; a feature that Motorola doesn't support in simplex (e.g. 'fireground') operations. It is therefore necessary for a disaster to be near enough the infrastructure to allow for repeater access/operation. Many agencies with Motorola trunked systems already have this capability but it's hardly ever used; even in a crisis. The difficulty of operating such a system is often too great for poorly educated dispatchers who often have no college - much less any particular training in computers or communications systems - other than the 'cursory' training they receive in a 3 or 5 day class the vendors offer. The feature allows the dispatch center personnel to send units from different agencies who are responding to the same incident to a common talk group or virtual channel. This assumes the agencies all share a capability to operate over the same trunked radio system, which is rare. In an informal survey of three agencies with trunked systems that included this feature, users at two sites reported they did not think their system included the feature. A representative from a third site said he "...thought they had the feature but never used it." Of the three agencies with the feature, no one knew how to use it. This would suggest, (in at least the three agencies contacted,) that "dynamic regrouping" was not valuable. Like other disaster readiness processes, users would have to practice using the feature in order for it to be useful during an incident.

Some agencies use commercial two-way radio as an adjunct to their own communications networks. One professional engineering evaluation of public safety radio systems explains that commercial systems such as Nextel's are not built to the same standards of coverage and non-blocking as public safety trunked systems. Like toy walkie talkies marketed to children, they are usable and helpful for non-urgent communications but should not be considered reliable enough for life safety uses. It is also true that most trunked radio system users are likely to hear busy signals, (error tones showing no channels are available,) for the first time during a large disaster. All systems have a finite capacity.

"We don't want or need trunking" is what Chief Charles Dowd (NYPD) was heard to say at an APCO convention in Orlando (2006). NYPD operates a large, conventional repeater network with many legacy channels in the UHF band; and a technology developed "so a large number of users can share a small number of channels" (e.g. trunking) is clearly unnecessary and a frivolous waste of money.

With sufficient channels, there is no need for trunking. There are no 'busy' tones in a conventional repeater system. In the event an individual needs to chime in, he simply waits his turn - just as he would do in a trunked system.

All 911 ambulances and other FDNY vehicles have data terminals, sometimes referred to by staff in recordings and transcripts as MDTs. These terminals are connected to the computer-aided dispatch (CAD) back end or server. They can display text, page through screens describing jobs, and display lists of units assigned to a job.

A thorough analysis of data communications is not possible. What recordings show is that data terminals in at least some field units did not work properly during at least a portion of the incident. At 09:11:14, "Division 3" told Manhattan Fire dispatch, referring to the "summary" screen, "Summary is only giving me a few units. You're going to have to give it to me over the radio. I'm ready to write." This means the terminal was not displaying the entire list of units assigned to Division 3, as it would under normal conditions. The work-around: the Chief had to hand-write the list of units responding. In this one instance, the dispatcher reading the list of about 29 units tied up the Manhattan Fire channel for 53 seconds. During the reading of the list of units responding, one can hear several FDNY units try to interrupt the dispatcher. Their radio traffic was delayed until the entire list was read. This need to read lists of units because of slow or inoperable terminals occurred in at least three or four cases.

It's unclear what caused data delays and incomplete screens on the mobile data terminals. Evidenced by the dispatcher reading the list of units assigned to Division 3, the CAD system was working properly at dispatch positions. At least some field units experienced problems. Possible causes of problems with data terminals in vehicles may have included:

Data terminals are partly purchased and installed to reduce load on dispatch staff and to reduce traffic on voice channels. When they work properly, they have a significant operational benefit. A data outage during an occurrence of high call traffic can quickly overrun dispatch and voice channel capacity in cases where a routine level of calls for service requires both data terminals and voice channels.

New York City Council member Eric Gioia introduced a measure to have the Council investigate the issue of FDNY radio problems.





</doc>
<doc id="28082" url="https://en.wikipedia.org/wiki?curid=28082" title="Timeline for October following the September 11 attacks">
Timeline for October following the September 11 attacks

This article summarizes the events in October 2001 that were related to the September 11 attacks. All times, except where otherwise noted, are in Eastern Daylight Time (EDT), or .
















</doc>
<doc id="28113" url="https://en.wikipedia.org/wiki?curid=28113" title="Timeline for September following the September 11 attacks">
Timeline for September following the September 11 attacks

This article summarizes the events in the remaining days of September 2001 following the September 11 attacks which relate to the attacks. All times, except where otherwise noted, are in Eastern Daylight Time (EDT), or .



The National Day of Prayer and Remembrance





















</doc>
<doc id="28117" url="https://en.wikipedia.org/wiki?curid=28117" title="SAC">
SAC

SAC or Sac may refer to:
















</doc>
<doc id="28118" url="https://en.wikipedia.org/wiki?curid=28118" title="Strategic Air Command">
Strategic Air Command

Strategic Air Command (SAC) was both a Department of Defense Specified Command and a United States Air Force (USAF) Major Command (MAJCOM), responsible for Cold War command and control of two of the three components of the U.S. military's strategic nuclear strike forces, the so-called "nuclear triad," with SAC having control of land-based strategic bomber aircraft and intercontinental ballistic missiles or ICBMs (the third leg of the triad being submarine-launched ballistic missiles (SLBM) of the U.S. Navy).

SAC also operated all strategic reconnaissance aircraft, all strategic airborne command post aircraft, and all USAF aerial refueling aircraft, to include those in the Air Force Reserve (AFRES) and Air National Guard (ANG).

However, SAC did not operate the KB-50, WB-50 and WB-47 weather reconnaissance aircraft operated through the mid and late 1960s by the Air Weather Service, nor did SAC operate the HC-130 or MC-130 operations aircraft capable of aerial refueling helicopters that were assigned to Tactical Air Command (TAC), then Military Airlift Command (MAC), and from 1990 onward, those MC-130 aircraft operated by the Air Force Special Operations Command (AFSOC), or any AFRES (now Air Force Reserve Command (AFRC)) or ANG tactical aerial refueling aircraft (e.g., HC-130, MC-130) operationally gained by TAC, MAC or AFSOC.

SAC primarily consisted of the Second Air Force (2AF), Eighth Air Force (8AF) and the Fifteenth Air Force (15AF), while SAC headquarters (HQ SAC) included Directorates for Operations & Plans, Intelligence, Command & Control, Maintenance, Training, Communications, and Personnel. At a lower echelon, SAC headquarters divisions included Aircraft Engineering, Missile Concept, and Strategic Communications.

In 1992, as part of an overall post-Cold War reorganization of the U.S. Air Force, SAC was disestablished as both a Specified Command and as a MAJCOM, and its and equipment redistributed among the Air Combat Command (ACC), Air Mobility Command (AMC), Pacific Air Forces (PACAF), United States Air Forces in Europe (USAFE), and Air Education and Training Command (AETC), while SAC's central headquarters complex at Offutt AFB, Nebraska was concurrently transferred to the newly created United States Strategic Command (USSTRATCOM), which was established as a joint Unified Combatant Command to replace SAC's Specified Command role.

In 2009, SAC's previous USAF MAJCOM role was reactivated and redesignated as the Air Force Global Strike Command (AFGSC), with AFGSC eventually acquiring claimancy and control of all USAF bomber aircraft and the USAF strategic ICBM force.

The Strategic Air Forces of the United States during World War II included General Carl Spaatz's European command, United States Strategic Air Forces in Europe (USSTAF), consisting of the 8AF and 15AF, and the United States Strategic Air Forces in the Pacific (USASTAF) and its Twentieth Air Force (20AF).

The Operation Overlord air plan for the strategic bombing of both Germany and German military forces in continental Europe prior to the 1944 invasion of France used several Air Forces, primarily those of the USAAF and those of the Royal Air Force (RAF), with command of air operations transferring to the Supreme Commander of the Allied Expeditionary Force on 14 April 1944.

Planning to reorganize for a separate and independent postwar U.S. Air Force had begun by the fall of 1945, with the Simpson Board tasked to plan, "...the reorganization of the Army and the Air Force...". In January 1946, Generals Eisenhower and Spaatz agreed on an Air Force organization composed of the Strategic Air Command, the Air Defense Command, the Tactical Air Command, the Air Transport Command and the supporting Air Technical Service Command, Air Training Command, the Air University, and the Air Force Center.

Strategic Air Command was originally established in the U.S. Army Air Forces on 21 March 1946, acquiring part of the personnel and facilities of the Continental Air Forces (CAF), the World War II command tasked with the air defense of the continental United States (CONUS). At the time, CAF headquarters was located at Bolling Field (later Bolling AFB) in the District of Columbia and SAC assumed occupancy of its headquarters facilities until relocating SAC headquarters (HQ SAC) to nearby Andrews Field (later Andrews AFB), Maryland as a tenant activity until assuming control of Andrews Field in October 1946.

SAC initially totaled 37,000 USAAF personnel. In addition to Bolling Field and, seven months later, Andrews Field, SAC also assumed responsibility for:

SAC also had seven additional CAF bases transferred on 21 March 1946 which remained in SAC through the 1947 establishment of the U.S. Air Force as an independent service. Those installations included:

On 31 March 1946, the following additional installation was also assigned to SAC:

Under the first SAC Commander in Chief, General George C. Kenney, initial units reporting to the Strategic Air Command headquarters on 21 March 1946 included the Second Air Force, the IX Troop Carrier Command and the 73d Air Division.

Fifteenth Air Force was assigned to SAC on 31 March (15th AF's 263rd Army Air Force Base Unit—with —transferred the same date directly under HQ SAC ), while the IX Troop Carrier Command was inactivated the same date and its assets redistributed within SAC.

With postwar demobilization still underway, eight of the ten assigned bomb groups were inactivated before the Eighth Air Force was assigned to SAC on 7 June 1946

Despite the pressures of demobilization, SAC continued the training and evaluation of bomber crews and units still on active duty in the postwar Army Air Forces. Radar Bomb Scoring became the preferred method of evaluating bomber crews, with the last of 888 simulated bomb runs scored against a bombing site near San Diego, California during 1946, subsequently increasing to 2,449 bomb runs by 1947. In the wake of the successful employment of air-dropped nuclear weapons against Hiroshima and Nagasaki to effectively end World War II, SAC became the focus of the nation's nuclear strike capability, to the extent that Joint Chiefs of Staff (JCS) Publication 1259/27 on 12 December 1946 identified that, "...the 'air atomic' strategic air force should only come under the orders of the JCS."

In addition to the strategic bombing mission, SAC also devoted significant resources to aerial reconnaissance. In 1946, SAC's reconnaissance aircraft inventory consisted of F-2 photo variants of the C-45 Expeditor support aircraft, but by 1947 SAC had acquired an F-9C squadron consisting of twelve photo-reconnaissance variants of the B-17G Flying Fortress. An F-13 squadron, the F-13 later re-designated as the RB-29 Superfortress, was also established. SAC conducted routine aerial reconnaissance missions near the Soviet borders or near the 12-mile international waters limit, although some missions actually penetrated into Soviet airspace. The flight profiles of these missions—above 30,000 feet and in excess of 300 knots—made interception by Soviet air forces difficult until the Soviet's 1948 introduction of the MiG-15 jet fighter. Project Nanook, the Cold War’s first Top Secret reconnaissance effort, used the first RB-29 missions for mapping and visual reconnaissance in the Arctic and along the northern Soviet coast. Later missions were Project LEOPARD along the Chukchi Peninsula, followed by Projects RICKRACK, STONEWORK, and COVERALLS.

In 1946, the US possessed only nine atomic bombs and twenty-seven B-29s capable at any one time of delivering them. Furthermore, it was later determined that an attack by the 509th Composite Bomb Group during the 1947 to 1948 time frame would have required at least five to six days just to transfer custody of the bombs from United States Atomic Energy Commission (AEC) sites to SAC and deploy the aircraft and weapons to forward operating bases before launching nuclear strikes.

Unfortunately, postwar budget and personnel cuts had had an insidious effect on SAC as its Deputy Commander, Major General Clements McMullen, implemented mandated force reductions. This continued to wear down SAC as a command and morale plummeted. As a result, by the end of 1947, only two of SAC's eleven groups were combat ready. After the 1948 Bikini Atoll nuclear tests, the "Half Moon" Joint Emergency War Plan developed in May 1948 proposed dropping 50 atomic bombs on twenty Soviet cities, with President Harry S. Truman approving "Half Moon" during the June 1948 Berlin Blockade, (Truman sent B-29s to Europe in July). SAC also ordered special ELINT RB-29s to detect improved Soviet radars and, in cooperation with the 51st Air Force Base Unit, SAC also monitored radioactive fallout from Soviet atomic testing on Novaya Zemlya.

In terms of overall Air Force basing and infrastructure, SAC continued to acquire an ever-increasing share of USAF infrastructure and the USAF associated budget. In 1947, before the USAF was established as an independent service, construction commenced on Limestone AAF, Maine (later renamed Loring AFB), a new SAC installation specifically designed to accommodate the B-36 Peacemaker. Fort Dix AAF, New Jersey (later McGuire AFB); Spokane AAF, Washington (later Fairchild AFB); and Wendover Field, Utah (later Wendover AFB) were also transferred to SAC between 30 April and 1 September 1947. Following establishment of the USAF as a separate service, SAC bases in the United States consisted of:

Those bases subsequently added to SAC in the United States included:

In addition to bases under its operational control, SAC also maintained tenant wings at several bases under the control of other USAF MAJCOMs. These non-SAC bases with SAC tenants included

SAC also often maintained a tenant presence at former SAC bases that the command subsequently transferred and relinquished to other MAJCOMs, to include but not limited to:

SAC transferred to the United States Air Force on 26 September 1947, concurrent with the latter's establishment as a separate military service. Units directly under SAC HQ included the 8AF and 15AF, as well as the 311th Air Division, 4th Fighter Wing, 82nd Fighter Wing, 307th Bomb Wing, and two reconnaissance units, the 311th Reconnaissance Wing and the 46th Reconnaissance Squadron. The 56th Fighter Wing was subsequently assigned to SAC on 1 October 1947.

Following the establishment of the U.S. Air Force, most SAC installations on U.S. territory were renamed as "Air Force Base" during late 1947 and into 1948, while non-U.S. installations were renamed as "Air Base."

In May 1948, in an exercise versus Air Defense Command's "Blue" force, a SAC "Red" strike force simulated attacks on Eastern Seaboard targets as far south as Virginia. After a "scathing" 1948 Lindbergh review of SAC operations in the air and at six SAC bases, General Kenney was removed as Commanding General on 15 October 1948 and replaced on 19 October 1948 by 8AF's commander, Lieutenant General Curtis LeMay. Upon Lemay's assumption of command, SAC had only 60 nuclear-capable aircraft, none of which possessed a realistic long range capability against the Soviet Union.

The B-29D, which had become the B-50 in December 1945, was first delivered to SAC in June 1948. This was followed by SAC's first Convair B-36 Peacemaker bomber arriving at Kirtland AFB, New Mexico in September 1948.

In November 1948, LeMay had SAC's headquarters and its command post moved from Andrews AFB, Maryland to Offutt AFB, Nebraska. At Offutt, the command moved into the "A Building," a three-story facility which had previously been used by the Glenn L. Martin Company during World War II. Concurrent with establishment of this new headquarters facility, Lemay also increased SAC Radar Bomb Scoring (RBS) runs the same year to 12,084. SAC also enhanced its organic fighter escort capability by initiating replacement of its World War II vintage piston-engine F-51D Mustang and F-82E Twin Mustang fighter aircraft with F-84G Thunderjets.

In January 1949, SAC conducted simulated raids on Wright-Patterson AFB, Ohio. Assessments of these simulated raids by, "...LeMay's entire command...were appalling," despite the SAC deputy commander, Major General McMullen, having instructed all bomber units to improve their effectiveness. To motivate crews and improve operational effectiveness command-wide, SAC established a competition, the first so-called "Bomb Comp" in 1948. Winners of this inaugural event were the 43rd Bombardment Group (unit) and, for aircrew award, a B-29 team from the 509th Bombardment Group.

Given its global operating environment, SAC also opened its own survival school at Camp Carson, Colorado in 1949, later moving this school to Stead AFB, Nevada in 1952 before transferring the school to the Air Training Command in 1954.

SAC also created Emergency War Plan 1–49 (EWP 1–49), which outlined the means for delivering 133 atomic bombs, "...the entire stockpile...in a single massive attack..." on 70 Soviet cities over a 30-day period.

The first Soviet atomic bomb test occurred on 29 August 1949 and the Joint Chiefs of Staff (JCS) subsequently identified SAC's primary objective was to damage or destroy the Soviet Union's ability to deliver nuclear weapons. The JCS further defined SAC's secondary objective was to stop any Soviet advances into Western Europe, and its tertiary objective was the previous EWP 1–49 industrial mission.

In July 1950, in response to combat operations on the Korean peninsula, SAC dispatched ten nuclear-capable bombers to Guam and deployed four B-29 bomber wings in Korea for tactical operations, although this action caused SAC commander Lemay to comment, “...too many splinters were being whittled off the [deterrence] stick”..

Initial SAC B-29 successes against North Korea in the summer of 1950 were countered by subsequent Soviet MiG-15 fighter-interceptors, and SAC's 27th Fighter Escort Wing began escorting the bombers with F–84 Thunderjets. Ground-directed bombing (GDB) was subsequently used for close air support (CAS) missions after three SAC radar bomb scoring (RBS) squadron detachments (Dets C, K, & N) arrived at Pusan in September 1950. In 1951, SAC "began to eliminate its combat groups", transferring medium bombardment groups "to Far East Air Forces (FEAF) Bomber Command for combat." In 1951, LeMay convinced the Air Staff to allow SAC to approve nuclear targets, and he continued refusing to submit war plans for JCS review, which the JCS eventually came to accept (of 20,000 candidates in 1960, SAC designated 3,560 as bombing targets—mostly Soviet air defense: airfields and suspected missile sites.)

Although experimented with prior to World War II, SAC refined aerial refueling to a fine art. SAC's in-flight refueling mission began in July 1952 when its 31st Fighter-Escort Wing refueled sixty F-84G Thunderjets from Turner AFB, Georgia to Travis AFB, California non-stop with fuel from twenty-four KB-29P Superfortresses modified into aerial tankers. Exercise FOX PETER ONE followed with 31st FEW fighters being refueled Hickam AFB en route to Hawaii.

On 15 March 1953, a 38th Strategic Reconnaissance Squadron RB-50 returned fire on a Soviet MiG-15, while a 343d Strategic Reconnaissance Squadron RB-50 was shot down over the Sea of Japan 2 days after the Korean Armistice, while on 7 November 1954, an RB-29 was shot down near Hokkaido Island in northern Japan. By the time of the 27 July 1953 Korean War cease-fire, SAC B-29s had flown over 21,000 sorties and dropped nearly 167,000 tons of bombs, with thirty-four B-29s lost in combat and forty-eight B-29s were lost to damage or crashes.

SAC's first jet strategic bomber was the swept-wing B-47 medium bomber, which first entered service in 1951 and became operational within SAC in 1953. The B-47 was a component of the October 1953 "New Look" strategy, which articulated, in part, that: ""...to minimize the threat...the major purpose of air defense was not to shoot down enemy bombers—it was to allow SAC...to get into the air"[--and]" not be destroyed on the ground"[--to allow]" massive retaliation".".

Concern of a bomber gap grew after the 1955 Soviet Aviation Day and the Soviets rejected the "Open Skies" Treaty proposed at the Geneva Summit on 21 July 1955. US bomber strength peaked with "over 2,500 bombers" after production "of over 2,000 B-47s and almost 750 B-52s" (circa 1956, 50% of SAC aircraft & 80% of SAC bombers were B-47s).

In an effort to concurrently enhance it reconnaissance capabilities, SAC also received several RB-57D Canberra aircraft in April 1956, with the aircraft initially based at Turner AFB, Georgia. In 1957, these aircraft were forward deployed to Rhein-Main Air Base, West Germany, in order to conduct reconnaissance missions along the borders of the Soviet Union and other Warsaw Pact nations. However, an unintended consequence of this deployment was that Hawker Hunter fighters of the Royal Air Force stationed in the United Kingdom and in continental Europe often intercepted these classified RB-57 missions as they returned to Rhein-Main AB from over the Baltic.

Since it was designed as a medium bomber, SAC's B-47 Stratojet traded speed for range. Because of this shorter range, and in order to better enable the B-47 fleet to reach its target sets in the Soviet Union, SAC routinely deployed its US-based B-47 wings to overseas forward operating bases in North Africa, Spain and Turkey. This program, in effect from 1957 to 1966, was known as "Reflex" with Sixteenth Air Force (16AF), a SAC numbered air force permanently stationed in Europe, having tactical and administrative control of the forward deployed aircraft and units.

Beginning in 1955, SAC also moved a portion of its bomber and aerial refueling aircraft to a 24-hour alert status, either on the ground or airborne. By 1960, fully one third of SAC's bombers and aerial refueling aircraft were on 24-hour alert, with those crews and aircraft not already airborne ready to take off from designated alert sites at their respective bases within fifteen minutes. Bomber aircraft on ground alert were armed with nuclear weapons while aerial tanker aircraft were sufficiently fueled to provide maximum combat fuel offload to the bombers.

Concurrent with this increased alert posture and in order to better hone strategic bombing skillsets, the 1955 SAC Bombing and Navigation Competition was characterized by radar bomb scoring (RBS) runs on Amarillo, Denver, Salt Lake City, Kansas City, San Antonio and Phoenix; and the 1957 competition (nicknamed "Operation Longshot") had three targets: Atlanta, Kansas City, and St. Louis. This use of RBS with simulated target areas utilizing mobile and fixed bomb scoring sites adjacent to major cities, industrial areas, military installations and dedicated bombing ranges throughout the United States. This format would continue through successive SAC Bombing and Navigation Competitions through the remainder of the 1950s, 1960s, 1970s and 1980s. Commencing in the late 1950s, in addition to representation from every SAC wing with a bombing and/or air refueling mission, later SAC competitions would also include participating bomber and aerial refueling units from the Royal Air Force's Bomber Command and (after 30 April 1968) its successor, RAF Strike Command.

It was described as the "Western Pentagon," specifically a, "...four-story, reinforced concrete and masonry office building..." above ground and a "...segregated, adjacent three-story below ground command post." This was the description of what would become Building 500 at Offutt AFB and the new headquarters complex built expressly for SAC, with construction commencing in 1955. SAC headquarters moved from the A Building at Offutt AFB to Building 500 in 1957. The underground nuclear bunker had 24-inch thick walls and base floor, 10-inch thick intermediate floors, and 24-to-42-inch thick roof. It also contained a war room with six 16-foot data display screens and the capacity to sustain up to 800 people underground for two weeks. The below ground bunker portion of the headquarters complex also contained an IBM 704 computer, which was used to develop monthly weather forecasts at targets, as well as for computing fuel consumption and fallout cloud patterns for planning strike routes and egress routes (e.g., determining the timing as to which targets to bomb first).

In 1957, SAC also constructed The Notch, a facility alternatively known as the 8th Air Force Combat Operations Center (COC) and the Westover Communications Annex, since it was a sub-post of nearby Westover AFB. A 3-story nuclear bunker located on Bare Mountain, Massachusetts, The Notch was built with three-foot thick walls, 1.5 foot thick steel blast doors, and 20 feet underground to protect 350 people for 35 days. The Notch was shut down as a SAC facility in 1970 when 8th Air Force was relocated to Barksdale AFB, Louisiana.

Despite this investment in "hardened" headquarters and command and control facilities, the 1957 Gaither Commission identified, "...little likelihood of SAC's bombers surviving [a Soviet first strike] since there was no way to detect an incoming attack until the first [Soviet nuclear weapon] warhead landed." As a result, SAC's bombers and tankers began sitting armed ground alert at their respective bases on 1 Oct 57.

In another organizational change during this time period, SAC's fighter escort wings were transferred to Tactical Air Command (TAC) during 1957 and 1958. Finally, during January 1958's Exercise Fir Fly, SAC "faker" aircraft (twelve B-47s) simulated bombing strikes against metropolitan areas and military installations in the United States defended by Air Defense Command's 28th Air Division.

After SAC's 1st Missile Division was activated on 18 March 1957, SAC HQ established the Office of Assistant CINCSAC (SAC MIKE) at the Air Force Ballistic Missile Division in California on 1 January 1958. SAC MIKE was responsible for missile development liaison, the intermediate range Jupiter and Thor missiles having been transferred to SAC for alert in 1958.

Beginning on 1 February 1958, a SAC Liaison Team was also located at the NORAD Command Post at Ent AFB, Colorado, and the two commands agreed that direct land line communications should connect SAC bases with NORAD's Air Defense Direction Centers. Also in the late 1950s, SAC continued to enhance its intelligence collection activities and develop innovative means of improving the survivability of its forces to surprise attack. From 1958–, a SAC Detachment (TUSLOG Det 50) operated at Incirlik AB, Turkey, monitoring Soviet missile telemetry from the Kapustin Yar and Tyuratam launch complexes, while in 1959, SAC's Operation Big Star studied, prototyped and evaluated the potential of deploying of Minuteman I ICBMs on civilian railroad tracks via USAF-operated locomotives and trains.

President Eisenhower approved the first Atlas ICBM launch by a SAC crew for 9 September 1959 at Vandenberg AFB.

While missile operations continued to ramp up, robust training for flight crews to ensure survivability for strike missions also continued. In some instances SAC bombers would oppose ADC fighter-interceptors simulating Soviet interceptors. Conversely, SAC assisted ADC readiness by simulating Soviet bomber threats to the continental United States that ADC fighters would respond to. However, following a mid-air collision between an ADC F-102 and a SAC B-47 during a 17 December 1959 Quick Kick exercise, simulated NORAD fighter attacks were prohibited against SAC bombers.

On 18 March 1960, SAC intercontinental missiles began alert at Maine's Snark Missile Launch Complex adjacent to Presque Isle AFB. The following month, on 22 April 1960, SAC turned over the last British-based PGM-17 Thor IRBM to the Royal Air Force. This was soon followed by SAC's first Titan I ICBMs at Lowry AFB's Titan I Missile Complex 1A in Colorado being placed on alert that June.

Beginning in November 1959, in order to counter Soviet surface-to-air missile threats, SAC began adding low-altitude bombing training for its manned bomber force as an adjunct to its legacy high-altitude training. Use of low level flight route corridors known as "Oil Burner" routes (later renamed "Olive Branch" routes in the 1970s), and the first of three SAC RBS trains were utilized starting in 1960. On 30 June 1960, SAC had 696 aircraft on alert in the Zone of Interior, also known as the ZI (referred to today as the Continental United States, or CONUS) and at overseas bases. These 696 aircraft were 113 B-52s, 346 B-47s, 85 KC-135s, and 152 KC-97s. SAC's Emergency War Order (EWO) required the first aircraft to be airborne within 8 minutes and all aircraft to be airborne within 15 minutes after notification.

During the mid-1950s, having recalled numerous World War II USAAF and Korean War USAF combat veteran pilots, navigators, bombardiers and aircrewmen from inactive reserve status back to various lengths of active duty, SAC took the lead in integrating the Air Force's reserve components into the overall SAC structure. By the beginning of the 1960s, SAC had also engineered the assignment of KC-97 Stratotanker aerial refueling aircraft to Air National Guard groups and wings and having them fall under SAC's operational claimancy.

On 11 August 1960, President Eisenhower approved the creation of the Joint Strategic Target Planning Staff (JSTPS), co-located at SAC headquarters at Offutt AFB.) JSTPS also included non-SAC agencies tasked with preparing the Single Integrated Operation Plan, or SIOP, and the National Strategic Target List for nuclear war.

On 1 July 1960, a SAC RB-47 with a six-man crew was shot down in international airspace over the Barents Sea by a Soviet MiG-19. Four of the crewmen were killed and two surviving crewmen were captured and held in Lubyanka Prison in Moscow for seven months.

On 3 February 1961, SAC's Boeing EC-135 Looking Glass, began operations as the Airborne Command Post for the Nuclear Triad and the Post-Attack Command and Control System. From this date and for the next 29 1/2 years, until 24 July 1990, SAC would maintain at least one Looking Glass aircraft continuously aloft 24 hours a day, 365 days a year, with an embarked SAC general officer and battle staff, ready to assume command of all strategic nuclear strike forces in the event that SAC headquarters was destroyed in a Soviet first strike. 

SAC's airborne alerts during this period also included Operation Chrome Dome for the bomber and tanker force. Although ostensibly a peacetime mission, Chrome Dome placed heavy demands on flight crews and five B-52 aircraft were lost to airborne mishaps during the operation's eight-year period.

On 11 May 1961, SAC took delivery of its first B-58 Hustler supersonic medium bomber, assigning it to the 305th Bombardment Wing at Bunker Hill AFB. Optimized for high-altitude, high-speed penetration into Soviet territory prior to Soviet advancements in high-altitude surface-to-air missiles, the B-58 was expensive to operate and inefficient at lower altitudes. Its service in SAC would be comparatively short, eventually being replaced by the FB-111 by 1970.

After an early 1961 development by SAC of a Radar Bomb Scoring (RBS) field kit for use in the U.S. Army's Nike surface-to-air missile systems, SAC aircraft flew several mock penetrations into Air Defense Command sectors in the 1961 SAGE/Missile Master test program, as well as the joint SAC-NORAD Sky Shield II exercise followed by Sky Shield III on 2 September 1962.

In 1961, following the Berlin Crisis, President John F. Kennedy increased the number of SAC aircraft on alert to 50 percent and during periods of increased tensions SAC kept some B-52 airborne in the event of a surprise attack.

In 1962, SAC gained full control of the various "Q Areas" developed by Sandia Laboratories for nuclear weapon storage adjacent to Loring AFB (Site E (Maine)/Caribou AFS), Ellsworth AFB (Site F (South Dakota)/Rushmore AFS), Fairchild AFB (Site G (Washington)/Deep Creek AFS), Travis AFB (Site H (California)/Fairfield AFS), and Westover AFB (Site I (Massachusetts)/Stony Brook AFS). These adjunct sites were subsequently converted to USAF-operated and maintained weapon storage areas (WSAs) in the same manner as WSAs on other SAC bases.

The solid fuel LGM-30A Minuteman I was first deployed in 1962 and the LGM-25C Titan II reached operational service in 1963. Project Added Effort phased out all first-generation ICBMs beginning on 1 May 1964 when Atlas-D were taken off alert at Vandenberg AFB's 576th SMS (LGM-30F Minuteman II replaced Minuteman I in 1965).

In October 1962, a SAC BRASS KNOB mission U-2 piloted by Major Richard S. Heyser detected Soviet intermediate range ballistic missiles in Cuba. BRASS KNOB operations involving multiple U-2 aircraft were subsequently commenced at a forward operating location at McCoy AFB, Florida the same month. On the morning of 27 October, a SAC RB-47H of the 55th Strategic Reconnaissance Wing, forward deployed to Kindley AFB, Bermuda crashed on takeoff, killing all four crewmembers, while later that afternoon, a 4028th Strategic Reconnaissance Squadron U-2 forward deployed to McCoy AFB for BRASS KNOB operations was shot down over Cuba by an SA-2 Guideline missile, killing the pilot, Major Rudolf Anderson.

Throughout the early 1960s, the Kennedy Administration, under the aegis of Secretary of Defense McNamara, cancelled numerous SAC modernization programs. This included the Mach 3 North American B-70 Valkyrie in 1961, the GAM-87 Skybolt missile in 1962, and the Rocky Mountain Deep Underground Support Center in 1963. The B-70's demise came due to its design as a high-altitude bomber with very limited low-altitude performance, making it vulnerable to rapid advances in Soviet high altitude surface-to-air missile defense systems. The following year, Skybolt, an air-launched ballistic missile, was cancelled following numerous test failures and the perceived greater reliability of land-based and submarine-based ballistic missile systems. Although initially entering service in 1957, SAC's 2nd-generation aerial refueling aircraft, the KC-135 Stratotanker, had reached sufficient inventory numbers to allow SAC to begin divestiture of its KC-97 Stratofreighter tankers, transferring them to SAC-gained Air Force Reserve and Air National Guard units. As the KC-135 became the primary aerial tanker in active service, SAC employed the aircraft for several non-stop B-52 and KC-135 flights around the world, demonstrating that SAC no longer needed to depend on Reflex stations at air bases in Spain and Britain.)

After the Secretary of Defense rejected LeMay's November 1964 proposal for a "...strategic air campaign against 94 targets in North Vietnam...", thirty SAC B-52Fs were deployed to Andersen AFB, Guam on 17 February 1965, representing the first increment of SAC aircraft forward deployed for the Vietnam War. The following month, in March 1965, the Strategic Air Command Advanced Echelon (SACADVON) was established as a "...liaison unit for CINCSAC [was] located at MACV Headquarters to assist with the B-52 effort.".

On 23 May 1965, SAC B-52Fs began unarmed missions for radar mapping "...and later to test bombing with the assistance of ground homing beacons...". SAC began saturation bombing on 18 June 1965 (8000 tons per month in 1966) and conducted Operation Arc Light missions from 1965 until the end of hostilities involving U.S. forces in 1973.

All B-52F missions in 1965 were against targets in South Vietnam (RVN) except for the December "...Duck Flight mission [that] hit a suspected VC supply storage area [for which] part of the target box was in Laos." In April 1966, Vietnam operations began with the B-52D model, a 1956 model designed to use the AGM-28 Hound Dog cruise missile and the ADM-20 Quail aerial decoys for low altitude operations and modified in late 1965 by Project Big Belly to increase conventional bomb capacity.

SAC's RBS Squadrons were discontinued when most detachment personnel transferred to Vietnam from 1966 to 1973 for Combat Skyspot ground-directed bombing operations. The first "Quick Reaction" bombing was the "Pink Lady" mission on 6 July 1966 using SAC B-52D/Fs to support the U.S. Army's 1st Air Cavalry Division. The 1972 Operation Linebacker II also used Skyspot for Hanoi/Haiphong bombings in North Vietnam which resulted in the loss of 25 SAC aircrew members.

By May 1967, SACADVON had moved to Seventh Air Force headquarters at Tan Son Nhut Air Base, South Vietnam to schedule and coordinate "...strikes for the 7th AF and MACV.". From a level of 161,921 military and 20,215 civilian assigned to SAC in June 1968, SAC lost 13,698 first term airmen from November 1968 to May 1969 in a three phase drawdown known as Project 693 to comply with Public Law 90-364.

While conventional bombing, air refueling and strategic air reconnaissance operations in Southeast Asia increasingly occupied SAC's operational commitments, SAC's primary mission of nuclear deterrence continued to remain its primary focus. In 1969, "...SAC's B-52s and B-58s could carry B28, B41, B43, B53, and BA53 nuclear weapons" (SAC had 311 nuclear AGM-28 Hound Dog| missiles at the end of the year.) This also coincided with the B-58 Hustler's in-progress retirement from SAC's active inventory and its replacement with the FB-111.

On 18 March 1969, along the South Vietnamese border, SAC first bombed Cambodia (Operation Menu through 26 May 1970 was controlled by Skyspot). On 17 February 1970, SAC conducted the first "GOOD LOOK" bombing of Laos at the Plaine des Jarres after B-52 photorecon missions (GOOD LOOK ALPHA in August 1969 and GOOD LOOK BRAVO ) and the observations of a Skyspot installation in Thailand. SAC transferred "...HQ 8th AF...to Andersen AFB, Guam on 1 April 1970 to oversee B-52D/G operations and to complement SACADVON". 8th AF took over from Third Air Division the generation of "frag" orders based on daily strike requests and amendments from COMUSMACV.

In 1970, SAC deployed the LGM-30G Minuteman III ICBM with multiple independently targetable reentry vehicle or MIRVs, for striking 3 targets, while concurrently retiring the B-58 Hustler supersonic bomber.

1972 saw the commencement of Operation Linebacker II, a combined Seventh Air Force and U.S. Navy Task Force 77 aerial bombing campaign, conducted against targets in North Vietnam during the final period of US involvement in the Vietnam War. Linebacker II was conducted from 18 December to 29 December 1972, leading to several informal names such as "The December Raids" and "The Christmas Bombings." Unlike the previous Operation Rolling Thunder and Operation Linebacker interdiction operations, Linebacker II, would be a "maximum effort" bombing campaign to destroy major target complexes in the Hanoi and Haiphong areas which could only be accomplished by SAC B-52D/Gs. It saw the largest heavy bomber strikes launched by the U.S. Air Force since the end of World War II. Linebacker II was a modified extension of the Operation Linebacker bombings conducted from May to October 1972, with the emphasis of the new campaign shifted to attacks by B-52 Stratofortress heavy bombers rather than smaller tactical fighter aircraft. During Linebacker II, a total of 741 B-52D/G sorties were dispatched from bases in Thailand and Guam to bomb North Vietnam and 729 actually completed their missions. Overall SAC losses during Linebacker II numbered fifteen B-52s. The U.S. government claimed that the operation had succeeded in forcing North Vietnam's Politburo to return to the negotiating table, with the Paris Peace Accords signed shortly after the operation.

By early 1973, offensive SAC air operations in Southeast Asia ceased and numerous SAC aircrewmen who had been shot down and captured as prisoners of war by North Vietnam were repatriated to the United States.

SAC aircraft used during the Vietnam War included B-52D, B-52F, B-52G, KC-135A, KC-135Q, various versions of the RC-135, SR-71, U-2, and EC-135.

During the Vietnam War, due to the escalating costs of combat operations in Southeast Asia, SAC was required to close several SAC bases, consolidate other bases, or transfer several bases to other MAJCOMs, other services, or the Air Reserve Component in order to remain within budgetary constraints. This included:

With the Vietnam War draw-down following the Paris Peace Treaty in 1973, reduced defense budgets forced SAC to inactivate several more wings, close still more bases in CONUS and Puerto Rico, transfer still additional bases to other MAJCOMS or the Air Reserve Component, and retire older B-52B, B-52C, B-52E and B-52F aircraft:

In 1973, the National Emergency Airborne Command Post, or NEACP, aircraft entered SAC's inventory. Consisting of four Boeing E-4 aircraft, these highly modified Boeing 747 airframes were assigned to the 55th Strategic Reconnaissance Wing at Offutt AFB and were forward deployed as necessary to support the National Command Authority.

By 1975, SAC's manned bomber strength included several hundred B-52D, B-52G, B-52H and FB-111A aircraft, and "...SAC's first major exercise in 23 years" was Exercise Global Shield 79. As for the ICBM force, SAC reached a peak strength of 1000 Minuteman II and III and 54 Titan II ICBMs on active status before seeing reductions and retirements through a combination of obsolescing systems and various arms reduction treaties with the Soviet Union.

By 1977, SAC had been pinning its hopes for a new manned strategic bomber in the form of the Rockwell B-1A Lancer. However, on 30 June 1977, President Jimmy Carter Carter announced that the B-1A would be canceled in favor of ICBMs, submarine-launched ballistic missiles (SLBMs), and a fleet of modernized B-52s armed with air-launched cruise missiles (ALCMs).

On 1 December 1979, SAC assumed control of the ballistic missile warning system (BMEWS) and all Space Surveillance Network facilities from the inactivating Aerospace Defense Command (ADC). These activities would later be (transferred to Air Force Space Command (AFSPC) when the latter was established in 1982. SAC also continued to operate the Air Force's entire KC-135 aerial refueling fleet, its EC-135 LOOKING GLASS and E-4 NEACAP command post aircraft, as well the entire strategic reconnaissance aircraft fleet consisting of the U-2, SR-71, RC-135, and WC-135.

In 1981, SAC received a new air refueling tanker aircraft to supplement the aging KC-135 Stratotanker force. Based on the McDonnell Douglas DC-10 commercial airliner, the KC-10A Extender was deployed equipped with improved military avionics, aerial refueling, and satellite communications equipment. That same year, President Ronald Reagan reversed the 1977 Carter administration decision regarding the B-1, directing that 100 examples of a refined version of the aircraft, now designated the B-1B Lancer, be procured as a long-range combat aircraft for SAC.

The LGM-118A Peacekeeper ICBM reached SAC in 1986, and the 114 Peacekeepers had a total warhead yield of about 342 megatons. This also served to offset the retirement of the obsolescent and maintenance-intensive LGM-25C Titan II ICBM, the last example of which was deactivated in May 1987. An additional underground "16,000 square-foot, two-story reinforced concrete" command post for HQ SAC was also constructed at Offutt AFB from 1986 to 1989 from a design by Leo A. Daly, who had designed the adjoining 1957 bunker. The first Rockwell B-1B Lancer was also delivered to SAC in 1987. 

On 22 November 1988, the Northrop Grumman B-2 Spirit, under development as the Advanced Technology Bomber (ATB), a so-called "black program" since 1979, was officially acknowledged and rolled out for the first time for public display. The first "stealth bomber" designed for SAC, the aircraft made its first flight in May 1989.

SAC reorganization at the end of the Cold War began as early as 1988 when the Carlucci Commission planned the closure of:

The closures were the beginning of a post-Cold War process that would later become known as Base Realignment and Closure or BRAC. Although Mather AFB's navigator training mission would relocate to Randolph AFB, Texas, the Mather bomber/tanker wing would inactivate and the AFRES tanker group would relocate to nearby McClellan AFB, relocating again four years later to Beale AFB when another BRAC process would close McClellan AFB.

Concurrently, the Pease AFB bomber/tanker wing would lose its FB-111 aircraft and transfer to Whiteman AFB, Missouri in preparation for transition to the B-2 Spirit while a portion of Pease would be transferred to the New Hampshire Air National Guard for its ANG air refueling wing and be renamed Pease Air National Guard Base.

Additional closures and divestments of SAC bases would continue throughout the late 1980s and early 1990s, accelerating even more so as a result the START I Treaty's mandated elimination of both the entire B-52G fleet and the inactivation of all Minuteman II and Peacekeeper ICBMs, as well as the 1992 reorganization of the Air Force that disestablished SAC and dispersed its assets to other new or existing MAJCOMs, primarily ACC and AMC. In addition to closures of Mather AFB and Pease AFB, this would eventually include the following subsequent closure and realignment actions, primarily due to BRAC:

On 1 July 1989, the 1st Combat Evaluation Group reporting directly to SAC headquarters was split with most HQ 1CEVG organizations transferring to SAC HQ (e.g., the Command Instrument Flight Division) and RBS personnel, equipment, and becoming the 1st Electronic Combat Range Group. Airborne NEACP alerts ended in 1990 and during 1991's Operation Desert Storm to liberate Kuwait from Iraqi invasion and occupation, SAC bomber, tanker and reconnaissance aircraft flew operations (e.g., B-52s with conventional bombs and conventional warhead AGM-86 ALCMs) near Iraq from bases in Great Britain, Turkey, Cyprus, Diego Garcia, Saudi Arabia, and the United Arab Emirates.

Following Operation Desert Storm, the dissolution of the Soviet Union and the "de facto" end of the Cold War, President George H. W. Bush and Secretary of Defense Dick Cheney directed SAC to take all bomber and refueling aircraft and Minuteman II ICBM's off of continuous nuclear alert on 27 September 1991 and placing said aircraft on quick reaction ground alert.

The 31 May 1992 major reorganization of the USAF organizational structure subsequently disestablished SAC, moving its bomber, reconnaissance and aerial command post aircraft and all SAC ICBMs, along with all Tactical Air Command aircraft, to the newly established Air Combat Command (ACC). The newly established Air Mobility Command (AMC) inherited most of SAC's KC-135 Stratotanker aircraft and the entire KC-10 Extender aerial refueling tanker force, while some KC-135s were reassigned directly to USAFE and PACAF, with one additional air refueling wing assigned to the Air Education and Training Command (AETC) as the KC-135 formal training unit.

Land-based ICBMs were later transferred from ACC to Air Force Space Command (AFSPC), while manned bombers remained in ACC. USAF nuclear forces in ACC and AFSPC were then combined with the United States Navy's Fleet Ballistic Missile submarine forces to form the United States Strategic Command (USSTRATCOM), which took over the SAC Headquarters complex at Offutt AFB.

In 2009, the entire land-based USAF ICBM force and that portion of the USAF manned bomber force that was still nuclear-capable, e.g., the B-2 Spirit and B-52 Stratofortress, was transferred to the newly established Air Force Global Strike Command (AFGSC), while the B-1 Lancer conventional bomber force remained in ACC. In 2015, these B-1 units were also transferred to Air Force Global Strike Command, which assumed responsibility for all current and future USAF bomber forces.

The SAC Museum located adjacent to Offutt AFB was moved in 1998 to a site near Ashland, Nebraska and renamed as the Strategic Air and Space Museum in 2001.

Organizations commemorating SAC include the Strategic Air Command Veterans Association, the SAC Society, the B-47 Stratojet Association, the B-52 Stratofortress Association, the FB-111 Association, the SAC Airborne Command Control Association, the Association of Air Force Missileers, the SAC Elite Guard Association and the Strategic Air Command Memorial Amateur Radio Club. After the Cold War, SAC histories included a 1996 almanac and a 2006 organizational history.

In 2009, the Air Force Global Strike Command (AFGSC) was activated with the lineage of Strategic Air Command. AFGSC, headquartered at Barksdale AFB, Louisiana, is one of two USAF component commands assigned to United States Strategic Command (USSTRATCOM). AFGSC currently consists of Eighth Air Force (8AF), responsible for the nuclear-capable manned heavy bomber force, and Twentieth Air Force (20AF), responsible for the ICBM force.



Strategic Air Command in the United Kingdom was among the command's largest overseas concentrations of forces, with additional forces under SAC's 16th Air Force at air bases in North Africa, Spain and Turkey during the 1950s and 1960s.

SAC "Provisional" wings were also located in Kadena AB, Okinawa and U-Tapao Royal Thai Navy Airfield / U-Tapao AB, Thailand during the Vietnam War

SAC also maintained bomber, tanker, and/or reconnaissance aircraft assets at the former Ramey AFB, Puerto Rico in the 1950s, 1960s and 1970s, and at Andersen AFB, Guam; RAF Mildenhall, RAF Fairford and RAF Alconbury in the United Kingdom; Moron AB, Spain; Lajes Field, Azores (Portugal); Diego Garcia, BIOT; and the former NAS Keflavik, Iceland through the 1990s.

SAC also conducted operations from RAF Fairford, RAF Alconbury and RAF Mildenhall in the United Kingdom, Moron AB in Spain, Lajes Field in the Azores (Portugal), RAF Akrotiri in Cyprus, Incirlik AB in Turkey, Diego Garcia in the British Indian Ocean Territory, and from multiple air bases in Egypt, Saudi Arabia, Oman, and the United Arab Emirates during the first Gulf War (Operations Desert Shield and Desert Storm) from 1990 to 1991.





</doc>
<doc id="28119" url="https://en.wikipedia.org/wiki?curid=28119" title="Scheme (programming language)">
Scheme (programming language)

Scheme is a programming language that supports multiple paradigms, including functional programming and imperative programming, and is one of the two main dialects of Lisp. Unlike Common Lisp, the other main dialect, Scheme follows a minimalist design philosophy specifying a small standard core with powerful tools for language extension.

Scheme was created during the 1970s at the MIT AI Lab and released by its developers, Guy L. Steele and Gerald Jay Sussman, via a series of memos now known as the Lambda Papers. It was the first dialect of Lisp to choose lexical scope and the first to require implementations to perform tail-call optimization, giving stronger support for functional programming and associated techniques such as recursive algorithms. It was also one of the first programming languages to support first-class continuations. It had a significant influence on the effort that led to the development of Common Lisp.

The Scheme language is standardized in the official IEEE standard and a "de facto" standard called the "Revised Report on the Algorithmic Language Scheme" (R"n"RS). The most widely implemented standard is R5RS (1998); a new standard, R6RS, was ratified in 2007. Scheme has a diverse user base due to its compactness and elegance, but its minimalist philosophy has also caused wide divergence between practical implementations, so much that the Scheme Steering Committee calls it "the world's most unportable programming language" and "a "family" of dialects" rather than a single language.

Scheme started in the 1970s as an attempt to understand Carl Hewitt's Actor model, for which purpose Steele and Sussman wrote a "tiny Lisp interpreter" using Maclisp and then "added mechanisms for creating actors and sending messages". Scheme was originally called "Schemer", in the tradition of other Lisp-derived languages like Planner or "Conniver". The current name resulted from the authors' use of the ITS operating system, which limited filenames to two components of at most six characters each. Currently, "Schemer" is commonly used to refer to a Scheme programmer.

A new language standardization process began at the 2003 Scheme workshop, with the goal of producing an R6RS standard in 2006. This process broke with the earlier R"n"RS approach of unanimity.

R6RS features a standard module system, allowing a split between the core language and libraries. A number of drafts of the R6RS specification were released, the final version being R5.97RS. A successful vote resulted in the ratification of the new standard, announced on August 28, 2007.

Currently the newest releases of various Scheme implementations support the R6RS standard. There is a portable reference implementation of the proposed implicitly phased libraries for R6RS, called psyntax, which loads and bootstraps itself properly on various older Scheme implementations.

R6RS introduces numerous significant changes to the language. The source code is now specified in Unicode, and a large subset of Unicode characters may now appear in Scheme symbols and identifiers, and there are other minor changes to the lexical rules. Character data is also now specified in Unicode. Many standard procedures have been moved to the new standard libraries, which themselves form a large expansion of the standard, containing procedures and syntactic forms that were formerly not part of the standard. A new module system has been introduced, and systems for exception handling are now standardized. Syntax-rules has been replaced with a more expressive syntactic abstraction facility (syntax-case) which allows the use of all of Scheme at macro expansion time. Compliant implementations are now "required" to support Scheme's full numeric tower, and the semantics of numbers have been expanded, mainly in the direction of support for the IEEE 754 standard for floating point numerical representation.

The R6RS standard has caused controversy because it is seen to have departed from the minimalist philosophy. In August 2009, the Scheme Steering Committee which oversees the standardization process announced its intention to recommend splitting Scheme into two languages: a large modern programming language for programmers; and a small version, a subset of the large version retaining the minimalism praised by educators and casual implementors. Two working groups were created to work on these two new versions of Scheme. The Scheme Reports Process site has links to the working groups' charters, public discussions and issue tracking system.

The ninth draft of R7RS (small language) was made available on April 15, 2013. A vote ratifying this draft closed on May 20, 2013, and the final report has been available since August 6, 2013, 
describing "the 'small' language of that
effort: therefore it cannot be considered in isolation as the
successor to R6RS".

Scheme is primarily a functional programming language. It shares many characteristics with other members of the Lisp programming language family. Scheme's very simple syntax is based on s-expressions, parenthesized lists in which a prefix operator is followed by its arguments. Scheme programs thus consist of sequences of nested lists. Lists are also the main data structure in Scheme, leading to a close equivalence between source code and data formats (homoiconicity). Scheme programs can easily create and evaluate pieces of Scheme code dynamically.

The reliance on lists as data structures is shared by all Lisp dialects. Scheme inherits a rich set of list-processing primitives such as codice_1, codice_2 and codice_3 from its Lisp progenitors. Scheme uses strictly but dynamically typed variables and supports first class procedures. Thus, procedures can be assigned as values to variables or passed as arguments to procedures.

This section concentrates mainly on innovative features of the language, including those features that distinguish Scheme from other Lisps. Unless stated otherwise, descriptions of features relate to the R5RS standard.

"In examples provided in this section, the notation "===> result" is used to indicate the result of evaluating the expression on the immediately preceding line. This is the same convention used in R5RS."

This subsection describes those features of Scheme that have distinguished it from other programming languages from its earliest days. These are the aspects of Scheme that most strongly influence any product of the Scheme language, and they are the aspects that all versions of the Scheme programming language, from 1973 onward, share.

Scheme is a very simple language, much easier to implement than many other languages of comparable expressive power. This ease is attributable to the use of lambda calculus to derive much of the syntax of the language from more primitive forms. For instance of the 23 s-expression-based syntactic constructs defined in the R5RS Scheme standard, 14 are classed as derived or library forms, which can be written as macros involving more fundamental forms, principally lambda. As R5RS says (R5RS sec. 3.1): "The most fundamental of the variable binding constructs is the lambda expression, because all other variable binding constructs can be explained in terms of lambda expressions."

Example: a macro to implement codice_4 as an expression using codice_5 to perform the variable bindings.
(define-syntax let
Thus using codice_4 as defined above a Scheme implementation would rewrite "codice_7" as "codice_8", which reduces implementation's task to that of coding procedure instantiations.

In 1998 Sussman and Steele remarked that the minimalism of Scheme was not a conscious design goal, but rather the unintended outcome of the design process. "We were actually trying to build something complicated and discovered, serendipitously, that we had accidentally designed something that met all our goals but was much simpler than we had intended...we realized that the lambda calculus—a small, simple formalism—could serve as the core of a powerful and expressive programming language."

Like most modern programming languages and unlike earlier Lisps such as Maclisp, Scheme is lexically scoped: all possible variable bindings in a program unit can be analyzed by reading the text of the program unit without consideration of the contexts in which it may be called. This contrasts with dynamic scoping which was characteristic of early Lisp dialects, because of the processing costs associated with the primitive textual substitution methods used to implement lexical scoping algorithms in compilers and interpreters of the day. In those Lisps, it was perfectly possible for a reference to a free variable inside a procedure to refer to quite distinct bindings external to the procedure, depending on the context of the call.

The impetus to incorporate lexical scoping, which was an unusual scoping model in the early 1970s, into their new version of Lisp, came from Sussman's studies of ALGOL. He suggested that ALGOL-like lexical scoping mechanisms would help to realize their initial goal of implementing Hewitt's Actor model in Lisp.

The key insights on how to introduce lexical scoping into a Lisp dialect were popularized in Sussman and Steele's 1975 Lambda Paper, "Scheme: An Interpreter for Extended Lambda Calculus", where they adopted the concept of the lexical closure (on page 21), which had been described in an AI Memo in 1970 by Joel Moses, who attributed the idea to Peter J. Landin.

Alonzo Church's mathematical notation, the lambda calculus, has inspired Lisp's use of "lambda" as a keyword for introducing a procedure, as well as influencing the development of functional programming techniques involving the use of higher-order functions in Lisp. But early Lisps were not suitable expressions of the lambda calculus because of their treatment of free variables.

The introduction of lexical scope resolved the problem by making an equivalence between some forms of lambda notation and their practical expression in a working programming language. Sussman and Steele showed that the new language could be used to elegantly derive all the imperative and declarative semantics of other programming languages including ALGOL and Fortran, and the dynamic scope of other Lisps, by using lambda expressions not as simple procedure instantiations but as "control structures and environment modifiers". They introduced continuation-passing style along with their first description of Scheme in the first of the Lambda Papers, and in subsequent papers, they proceeded to demonstrate the raw power of this practical use of lambda calculus.

Scheme inherits its block structure from earlier block structured languages, particularly ALGOL. In Scheme, blocks are implemented by three "binding constructs": codice_4, codice_10 and codice_11. For instance, the following construct creates a block in which a symbol called codice_12 is bound to the number 10:
(let ((var 10))
Blocks can be nested to create arbitrarily complex block structures according to the need of the programmer. The use of block structuring to create local bindings alleviates the risk of namespace collision that can otherwise occur.

One variant of codice_4, codice_10, permits bindings to refer to variables defined earlier in the same construct, thus:

The other variant, codice_11, is designed to enable mutually recursive procedures to be bound to one another.

===> ((1 . 0) (1 . 0) (2 . 1) (2 . 2) (3 . 2) (3 . 3) (4 . 4) (5 . 4) (5 . 5))

All procedures bound in a single codice_11 may refer to one another by name, as well as to values of variables defined earlier in the same codice_11, but they may not refer to "values" defined later in the same codice_11.

A variant of codice_4, the "named let" form, has an identifier after the codice_4 keyword. This binds the let variables to the argument of a procedure whose name is the given identifier and whose body is the body of the let form. The body may be repeated as desired by calling the procedure. The named let is widely used to implement iteration.

Example: a simple counter

===> (1 2 3 4 5 6 7 8 9 10)
Like any procedure in Scheme, the procedure created in the named let is a first class object.

Scheme has an iteration construct, codice_21, but it is more idiomatic in Scheme to use tail recursion to express iteration. Standard-conforming Scheme implementations are required to optimize tail calls so as to support an unbounded number of active tail calls (R5RS sec. 3.5)—a property the Scheme report describes as "proper tail recursion"—making it safe for Scheme programmers to write iterative algorithms using recursive structures, which are sometimes more intuitive. Tail recursive procedures and the "named codice_4" form provide support for iteration using tail recursion.

===> (0 1 4 9 16 25 36 49 64 81)

Continuations in Scheme are first-class objects. Scheme provides the procedure codice_23 (also known as codice_24) to capture the current continuation by packing it up as an escape procedure bound to a formal argument in a procedure provided by the programmer. (R5RS sec. 6.4) First-class continuations enable the programmer to create non-local control constructs such as iterators, coroutines, and backtracking.

Continuations can be used to emulate the behavior of return statements in imperative programming languages. The following function codice_25, given function codice_26 and list codice_27, returns the first element codice_28 in codice_27 such that codice_30 returns true.

===> 7
===> #f
The following example, a traditional programmer's puzzle, shows that Scheme can handle continuations as first-class objects, binding them to variables and passing them as arguments to procedures.
(let* ((yin

When executed this code displays a counting sequence: codice_31
In contrast to Common Lisp, all data and procedures in Scheme share a common namespace, whereas in Common Lisp functions and data have separate namespaces making it possible for a function and a variable to have the same name, and requiring special notation for referring to a function as a value. This is sometimes known as the "Lisp-1 vs. Lisp-2" distinction, referring to the unified namespace of Scheme and the separate namespaces of Common Lisp.

In Scheme, the same primitives that are used to manipulate and bind data can be used to bind procedures. There is no equivalent of Common Lisp's codice_32 and codice_33 primitives.
(define f 10)
f
===> 10
(set! f (+ f f 6))
f
===> 26
(set! f (lambda (n) (+ n 12)))
===> 18
(set! f (f 1))
f
===> 13
(apply + '(1 2 3 4 5 6))
===> 21
===> (101 102 103)
This subsection documents design decisions that have been taken over the years which have given Scheme a particular character, but are not the direct outcomes of the original design.

Scheme specifies a comparatively full set of numerical datatypes including complex and rational types, which is known in Scheme as the numerical tower (R5RS sec. 6.2). The standard treats these as abstractions, and does not commit the implementor to any particular internal representations.

Numbers may have the quality of exactness. An exact number can only be produced by a sequence of exact operations involving other exact numbers—inexactness is thus contagious. The standard specifies that any two implementations must produce equivalent results for all operations resulting in exact numbers.

The R5RS standard specifies procedures codice_34 and codice_35 which can be used to change the exactness of a number. codice_35 produces "the exact number that is numerically closest to the argument". codice_34 produces "the inexact number that is numerically closest to the argument". The R6RS standard omits these procedures from the main report, but specifies them as R5RS compatibility procedures in the standard library (rnrs r5rs (6)).

In the R5RS standard, Scheme implementations are not required to implement the whole numerical tower, but they must implement "a coherent subset consistent with both the purposes of the implementation and the spirit of the Scheme language" (R5RS sec. 6.2.3). The new R6RS standard does require implementation of the whole tower, and "exact integer objects and exact rational number objects of practically unlimited size and precision, and to implement certain procedures...so they always return exact results when given exact arguments" (R6RS sec. 3.4, sec. 11.7.1).

Example 1: exact arithmetic in an implementation that supports exact 
rational complex numbers.

(define x (+ 1/3 1/4 -1/5 -1/3i 405/50+2/3i))
x
===> 509/60+1/3i
(exact? x)
===> #t
Example 2: Same arithmetic in an implementation that supports neither exact 
rational numbers nor complex numbers but does accept real numbers in rational notation.

(define xr (+ 1/3 1/4 -1/5 405/50))
(define xi (+ -1/3 2/3))
xr
===> 8.48333333333333
xi
===> 0.333333333333333
(exact? xr)
===> #f
===> #f
Both implementations conform to the R5RS standard but the second does not conform to R6RS because it does not implement the full numerical tower.

Scheme supports delayed evaluation through the codice_38 form and the procedure codice_39.

===> 22
===> 70
===> 22

The lexical context of the original definition of the promise is preserved, and its value is also preserved after the first use of codice_39. The promise is only ever evaluated once.

These primitives, which produce or handle values known as promises, can be used to implement advanced lazy evaluation constructs such as streams.

In the R6RS standard, these are no longer primitives, but instead, are provided as part of the R5RS compatibility library (rnrs r5rs (6)).

In R5RS, a suggested implementation of codice_38 and codice_39 is given, implementing the promise as a procedure with no arguments (a thunk) and using memoization to ensure that it is only ever evaluated once, irrespective of the number of times codice_39 is called (R5RS sec. 6.4).

SRFI 41 enables the expression of both finite and infinite sequences with extraordinary economy. For example, this is a definition of the fibonacci sequence using the functions defined in SRFI 41:
(define fibs
(stream-ref fibs 99)
===> 218922995834555169026
Most Lisps specify an order of evaluation for procedure arguments. Scheme does not. Order of evaluation—including the order in which the expression in the operator position is evaluated—may be chosen by an implementation on a call-by-call basis, and the only constraint is that "the effect of any concurrent evaluation of the operator and operand expressions is constrained to be consistent with some sequential order of evaluation." (R5RS sec. 4.1.3)

===> 3

ev is a procedure that describes the argument passed to it, then returns the value of the argument. In contrast with other Lisps, the appearance of an expression in the operator position (the first item) of a Scheme expression is quite legal, as long as the result of the expression in the operator position is a procedure.

In calling the procedure "+" to add 1 and 2, the expressions (ev +), (ev 1) and (ev 2) may be evaluated in any order, as long as the effect is not as if they were evaluated in parallel. Thus the following three lines may be displayed in any order by standard Scheme when the above example code is executed, although the text of one line may not be interleaved with another because that would violate the sequential evaluation constraint.

In the R5RS standard and also in later reports, the syntax of Scheme can easily be extended via the macro system. The R5RS standard introduced a powerful hygienic macro system that allows the programmer to add new syntactic constructs to the language using a simple pattern matching sublanguage (R5RS sec 4.3). Prior to this, the hygienic macro system had been relegated to an appendix of the R4RS standard, as a "high level" system alongside a "low level" macro system, both of which were treated as extensions to Scheme rather than an essential part of the language.

Implementations of the hygienic macro system, also called codice_44, are required to respect the lexical scoping of the rest of the language. This is assured by special naming and scoping rules for macro expansion and avoids common programming errors that can occur in the macro systems of other programming languages. R6RS specifies a more sophisticated transformation system, codice_45, which has been available as a language extension to R5RS Scheme for some time.
(define-syntax when
Invocations of macros and procedures bear a close resemblance—both are s-expressions—but they are treated differently. When the compiler encounters an s-expression in the program, it first checks to see if the symbol is defined as a syntactic keyword within the current lexical scope. If so, it then attempts to expand the macro, treating the items in the tail of the s-expression as arguments without compiling code to evaluate them, and this process is repeated recursively until no macro invocations remain. If it is not a syntactic keyword, the compiler compiles code to evaluate the arguments in the tail of the s-expression and then to evaluate the variable represented by the symbol at the head of the s-expression and call it as a procedure with the evaluated tail expressions passed as actual arguments to it.

Most Scheme implementations also provide additional macro systems. Among popular ones are syntactic closures, explicit renaming macros and codice_46, a non-hygienic macro system similar to codice_47 system provided in Common Lisp.

Prior to R5RS, Scheme had no standard equivalent of the codice_48 procedure which is ubiquitous in other Lisps, although the first Lambda Paper had described codice_49 as "similar to the LISP function EVAL" and the first Revised Report in 1978 replaced this with codice_50, which took two arguments. The second, third and fourth revised reports omitted any equivalent of codice_48.

The reason for this confusion is that in Scheme with its lexical scoping the result of evaluating an expression depends on where it is evaluated. For instance, it is not clear whether the result of evaluating the following expression should be 5 or 6:

If it is evaluated in the outer environment, where codice_52 is defined, the result is the sum of the operands. If it is evaluated in the inner environment, where the symbol "+" has been bound to the value of the procedure "*", the result is the product of the two operands.

R5RS resolves this confusion by specifying three procedures that return environments and providing a procedure codice_48 that takes an s-expression and an environment and evaluates the expression in the environment provided. (R5RS sec. 6.5) R6RS extends this by providing a procedure called codice_54 by which the programmer can specify exactly which objects to import into the evaluation environment.

In most dialects of Lisp including Common Lisp, by convention the value codice_55 evaluates to the value false in a boolean expression. In Scheme, since the IEEE standard in 1991, all values except #f, including codice_55's equivalent in Scheme which is written as '(), evaluate to the value true in a boolean expression. (R5RS sec. 6.3.1)

Where the constant representing the boolean value of true is codice_57 in most Lisps, in Scheme it is codice_58.

In Scheme the primitive datatypes are disjoint. Only one of the following predicates can be true of any Scheme object: codice_59, codice_60, codice_61, codice_62, codice_63, codice_64, codice_65, codice_66, codice_67. (R5RS sec 3.2)

Within the numerical datatype, by contrast, the numerical values overlap. For example, an integer value satisfies all of the codice_68, codice_69, codice_70, codice_71 and codice_62 predicates at the same time. (R5RS sec 6.2)

Scheme has three different types of equivalence between arbitrary objects denoted by three different "equivalence predicates", relational operators for testing equality, codice_73, codice_74 and codice_75:

Type dependent equivalence operations also exist in Scheme: codice_83 and codice_84 compare two strings (the latter performs a case-independent comparison); codice_85 and codice_86 compare characters; codice_87 compares numbers.

Up to the R5RS standard, the standard comment in Scheme was a semicolon, which makes the rest of the line invisible to Scheme. Numerous implementations have supported alternative conventions permitting comments to extend for more than a single line, and the R6RS standard permits two of them: an entire s-expression may be turned into a comment (or "commented out") by preceding it with codice_88 (introduced in SRFI 62) and a multiline comment or "block comment" may be produced by surrounding text with codice_89 and codice_90.

Scheme's input and output is based on the "port" datatype. (R5RS sec 6.6) R5RS defines two default ports, accessible with the procedures codice_91 and codice_92, which correspond to the Unix notions of standard input and standard output. Most implementations also provide codice_93. Redirection of input and standard output is supported in the standard, by standard procedures such as codice_94 and codice_95. Most implementations provide string ports with similar redirection capabilities, enabling many normal input-output operations to be performed on string buffers instead of files, using procedures described in SRFI 6. The R6RS standard specifies much more sophisticated and capable port procedures and many new types of port.

The following examples are written in strict R5RS Scheme.

Example 1: With output defaulting to (current-output-port):

Example 2: As 1, but using optional port argument to output procedures

Example 3: As 1, but output is redirected to a newly created file

(let ((hello0 (lambda () (display "Hello world") (newline))))
Example 4: As 2, but with explicit file open and port close to send output to file

Example 5: As 2, but with using call-with-output-file to send output to a file.

Similar procedures are provided for input. R5RS Scheme provides the predicates codice_96 and codice_97. For character input and output, codice_98, codice_99, codice_100 and codice_101 are provided. For writing and reading Scheme expressions, Scheme provides codice_102 and codice_103. On a read operation, the result returned is the end-of-file object if the input port has reached the end of the file, and this can be tested using the predicate codice_104.

In addition to the standard, SRFI 28 defines a basic formatting procedure resembling Common Lisp's codice_105 function, after which it is named.

In Scheme, procedures are bound to variables. At R5RS the language standard formally mandated that programs may change the variable bindings of built-in procedures, effectively redefining them. (R5RS "Language changes") For example, one may extend codice_106 to accept strings as well as numbers by redefining it:
(set! +
===> 6
===> "123"
In R6RS every binding, including the standard ones, belongs to some library, and all exported bindings are immutable. (R6RS sec 7.1) Because of this, redefinition of standard procedures by mutation is forbidden. Instead, it is possible to import a different procedure under the name of a standard one, which in effect is similar to redefinition.

In Standard Scheme, procedures that convert from one datatype to another contain the character string "->" in their name, predicates end with a "?", and procedures that change the value of already-allocated data end with a "!". These conventions are often followed by Scheme programmers.

In formal contexts such as Scheme standards, the word "procedure" is used in preference to "function" to refer to a lambda expression or primitive procedure. In normal usage, the words "procedure" and "function" are used interchangeably. Procedure application is sometimes referred to formally as "combination".

As in other Lisps, the term "thunk" is used in Scheme to refer to a procedure with no arguments. The term "proper tail recursion" refers to the property of all Scheme implementations, that they perform tail-call optimization so as to support an indefinite number of active tail calls.

The form of the titles of the standards documents since R3RS, "Revised Report on the Algorithmic Language Scheme", is a reference to the title of the ALGOL 60 standard document, "Revised Report on the Algorithmic Language Algol 60," The Summary page of R3RS is closely modeled on the Summary page of the ALGOL 60 Report.

The language is formally defined in the standards R5RS (1998) and R6RS (2007). They describe standard "forms": keywords and accompanying syntax, which provide the control structure of the language, and standard procedures which perform common tasks.

This table describes the standard forms in Scheme. Some forms appear in more than one row because they cannot easily be classified into a single function in the language.

Forms marked "L" in this table are classed as derived "library" forms in the standard and are often implemented as macros using more fundamental forms in practice, making the task of implementation much easier than in other languages.

Note that codice_107 is defined as a library syntax in R5RS, but the expander needs to know about it to achieve the splicing functionality. In R6RS it is no longer a library syntax.

The following two tables describe the standard procedures in R5RS Scheme. R6RS is far more extensive and a summary of this type would not be practical.

Some procedures appear in more than one row because they cannot easily be classified into a single function in the language.
String and character procedures that contain "-ci" in their names perform case-independent comparisons between their arguments: upper case and lower case versions of the same character are taken to be equal.

Implementations of - and / that take more than two arguments are defined but left optional at R5RS.

Because of Scheme's minimalism, many common procedures and syntactic forms are not defined by the standard. In order to keep the core language small but facilitate standardization of extensions, the Scheme community has a "Scheme Request for Implementation" (SRFI) process by which extension libraries are defined through careful discussion of extension proposals. This promotes code portability. Many of the SRFIs are supported by all or most Scheme implementations.

SRFIs with fairly wide support in different implementations include:


A full list of accepted (finalized) SRFIs is available at http://srfi.schemers.org/final-srfis.html

The elegant, minimalist design has made Scheme a popular target for language designers, hobbyists, and educators, and because of its small size, that of a typical interpreter, it is also a popular choice for embedded systems and scripting. This has resulted in scores of implementations, most of which differ from each other so much that porting programs from one implementation to another is quite difficult, and the small size of the standard language means that writing a useful program of any great complexity in standard, portable Scheme is almost impossible. The R6RS standard specifies a much broader language, in an attempt to broaden its appeal to programmers.

Almost all implementations provide a traditional Lisp-style read–eval–print loop for development and debugging. Many also compile Scheme programs to executable binary. Support for embedding Scheme code in programs written in other languages is also common, as the relative simplicity of Scheme implementations makes it a popular choice for adding scripting capabilities to larger systems developed in languages such as C. The Gambit, Chicken, and Bigloo Scheme interpreters compile Scheme to C, which makes embedding particularly easy. In addition, Bigloo's compiler can be configured to generate JVM bytecode, and it also features an experimental bytecode generator for .NET.

Some implementations support additional features. For example, Kawa and JScheme provide integration with Java classes, and the Scheme to C compilers often make it easy to use external libraries written in C, up to allowing the embedding of actual C code in the Scheme source. Another example is Pvts, which offers a set of visual tools for supporting the learning of Scheme.

Scheme is widely used by a number of schools; in particular, a number of introductory Computer Science courses use Scheme in conjunction with the textbook "Structure and Interpretation of Computer Programs" (SICP). For the past 12 years, PLT has run the ProgramByDesign (formerly TeachScheme!) project, which has exposed close to 600 high school teachers and thousands of high school students to rudimentary Scheme programming. MIT's old introductory programming class 6.001 was taught in Scheme, Although 6.001 has been replaced by more modern courses, SICP continues to be taught at MIT.
The textbook "How to Design Programs" by Matthias Felleisen, currently at Northeastern University, is used by some institutes of higher education for their introductory computer science courses. Both Northeastern University and Worcester Polytechnic Institute use Scheme exclusively for their introductory courses Fundamentals of Computer Science (CS2500) and Introduction to Program Design (CS1101), respectively. Rose-Hulman uses Scheme in its more advanced Programming Language Concepts course. Indiana University's introductory class, C211, is taught entirely in Scheme. The introductory class at UC Berkeley, CS 61A, was until recently taught entirely in Scheme, save minor diversions into Logo to demonstrate dynamic scope; all course materials, including lecture webcasts, are available online free of charge. The introductory computer science courses at Yale and Grinnell College are also taught in Scheme. Programming Design Paradigms, a mandatory course for the Computer science Graduate Students at Northeastern University, also extensively uses Scheme.
The introductory Computer Science course at the University of Minnesota - Twin Cities, CSCI 1901, also uses Scheme as its primary language, followed by a course that introduces students to the Java programming language. In the software industry, Tata Consultancy Services, Asia's largest software consultancy firm, uses Scheme in their month-long training program for fresh college graduates.

Scheme is/was also used for the following:






</doc>
<doc id="28122" url="https://en.wikipedia.org/wiki?curid=28122" title="Society for Psychical Research">
Society for Psychical Research

The Society for Psychical Research (SPR) is a nonprofit organisation in the United Kingdom. Its stated purpose is to understand events and abilities commonly described as psychic or paranormal. It describes itself as the "first society to conduct organised scholarly research into human experiences that challenge contemporary scientific models." It does not, however, since its inception in 1882, hold any corporate opinions: SPR members assert a variety of beliefs with regard to the nature of the phenomena studied.

The Society for Psychical Research (SPR) originated from a discussion between journalist Edmund Rogers and the physicist William F. Barrett in autumn 1881. This led to a conference on the 5 and 6 January 1882 at the headquarters of the British National Association of Spiritualists which the foundation of the Society was proposed. The committee included Barrett, Rogers, Stainton Moses, Charles Massey, Edmund Gurney, Hensleigh Wedgwood and Frederic W. H. Myers. The SPR was formally constituted on the 20 February 1882 with philosopher Henry Sidgwick as its first president.

The SPR was the first organisation of its kind in the world, its stated purpose being "to approach these varied problems without prejudice or prepossession of any kind, and in the same spirit of exact and unimpassioned enquiry which has enabled science to solve so many problems, once not less obscure nor less hotly debated."

Other early members included the chemist William Crookes, physicist Oliver Lodge, Nobel laureate Charles Richet and psychologist William James.

Members of the SPR initiated and organised the International Congresses of Physiological/Experimental psychology.

Areas of study included hypnotism, dissociation, thought-transference, mediumship, Reichenbach phenomena, apparitions and haunted houses and the physical phenomena associated with séances. The SPR were to introduce a number of neologisms which have entered the English language, such as 'telepathy', which was coined by Frederic Myers.

The Society is run by a President and a Council of twenty members, and is open to interested members of the public to join. The organisation is based at 1 Vernon Mews, London, with a library and office open to members, and with large book and archival holdings in Cambridge University Library, Cambridgeshire, England. It publishes the peer reviewed quarterly "Journal of the Society for Psychical Research" ("JSPR"), the irregular "Proceedings" and the magazine "Paranormal Review". It holds an annual conference, regular lectures and two study days per year and supports the "LEXSCIEN" on-line library project.

Among the first important works was the two-volume publication in 1886, "Phantasms of the Living", concerning telepathy and apparitions, co-authored by Gurney, Myers and Frank Podmore. This text, and subsequent research in this area, was received negatively by the scientific mainstream, though Gurney and Podmore provided a defense of the society's early work in this area in mainstream publications.

The SPR "devised methodological innovations such as randomized study designs" and conducted "the first experiments investigating the psychology of eyewitness testimony (Hodgson and Davey, 1887), [and] empirical and conceptual studies illuminating mechanisms of dissociation and hypnotism"

In 1894, the "Census of Hallucinations" was published which sampled 17,000 people. Out of these, 1, 684 persons reported having experienced a hallucination of an apparition. Such efforts were claimed to have undermined "the notion of dissociation and hallucinations as intrinsically pathological phenomena"

The SPR investigated many spiritualist mediums such as Eva Carrière and Eusapia Palladino.

During the early twentieth century, the SPR studied a series of automatic scripts and trance utterances from a group of automatic writers, known as the cross-correspondences.

Famous cases investigated by the Society include Borley Rectory and the Enfield Poltergeist.

Much of the early work involved investigating, exposing and in some cases duplicating fake phenomena. In the late 19th century, SPR investigations into séance phenomena led to the exposure of many fraudulent mediums.

Richard Hodgson distinguished himself in that area. In 1884, Hodgson was sent by the SPR to India to investigate Helena Blavatsky and concluded that her claims of psychic power were fraudulent.

In 1886 and 1887 a series of publications by S. J. Davey, Hodgson and Sidgwick in the SPR journal exposed the slate writing tricks of the medium William Eglinton. Hodgson with his friend, S. J. Davey had staged fake séances for educating the public (including SPR members). Davey gave sittings under an assumed name, duplicating the phenomena produced by Eglinton, and then proceeded to point out to the sitters the manner in which they had been deceived. Because of this, some spiritualist members such as Stainton Moses resigned from the SPR.

In 1891, Alfred Russel Wallace requested for the Society to properly investigate spirit photography. Eleanor Sidgwick responded with a critical paper in the SPR which cast doubt on the subject and discussed the fraudulent methods that spirit photographers such as Édouard Isidore Buguet, Frederic Hudson and William H. Mumler had utilised.

Due to the exposure of William Hope and other fraudulent mediums, Arthur Conan Doyle led a mass resignation of eighty-four members of the Society for Psychical Research, as they believed the Society was opposed to spiritualism. Science historian William Hodson Brock has noted that "By the 1900s most avowed spiritualists had left the SPR and gone back to the BNAS (the London Spiritualist Alliance since 1884), having become upset by the sceptical tone of most of the SPR's investigations."

The Society has been criticised by both spiritualists and sceptics.

Prominent spiritualists at first welcomed the SPR and cooperated fully. But relations soured when spiritualists discovered that the SPR would not accept outside testimony as proof, and the society accused some prominent mediums of fraud. Spiritualist Arthur Conan Doyle resigned from the SPR in 1930, to protest what he regarded as the SPR's overly restrictive standards of proof. Psychic investigator and believer in spiritualism Nandor Fodor criticised the SPR for its "strong bias" against physical manifestations of spiritualism.

Sceptics have criticised members of the SPR for having motives liable to impair scientific objectivity. According to SPR critics John Grant and Eric Dingwall (a member of the SPR), early SPR members such as Henry Sidgwick, Frederic W. H. Myers, and William Barrett hoped to cling to something spiritual through psychical research. Myers stated that "[T]he Society for Psychical Research was founded, with the establishment of thought-transference--already rising within measurable distance of proof--as its primary aim." Defenders of the SPR have stated in reply that "a ‘will to believe’ in post-mortem survival, telepathy and other scientifically unpopular notions, does not necessarily exclude a "will to know" and thus the capacity for thorough self-criticism, methodological rigour and relentless suspicion of errors."

The sceptic and physicist Victor J. Stenger has written:

Ivor Lloyd Tuckett an author of an early sceptical work on psychical research wrote that although the SPR have collected some valuable work, most of its active members have "no training in psychology fitting them for their task, and have been the victims of pronounced bias, as sometimes they themselves have admitted." Trevor H. Hall, an ex-member of the Society for Psychical Research, criticised SPR members as "credulous and obsessive wish... to believe." Hall also claimed SPR members "lack knowledge of deceptive methods."

Writer Edward Clodd asserted that the SPR members William F. Barrett and Oliver Lodge had insufficient competence for the detection of fraud and suggested that their spiritualist beliefs were based on magical thinking and primitive superstition. Clodd described the SPR as offering "barbaric spiritual philosophy", and characterised the language of SPR members as using such terms as "subliminal consciousness" and "telepathic energy," as a disguise for "bastard supernaturalism."

A 2004 psychological study involving 174 members of the Society for Psychical Research completed a delusional ideation questionnaire and a deductive reasoning task. As predicted, the study showed that "individuals who reported a strong belief in the paranormal made more errors and displayed more delusional ideation than sceptical individuals". There was also a reasoning bias which was limited to people who reported a belief in, rather than experience of, paranormal phenomena. The results suggested that reasoning abnormalities may have a causal role in the formation of paranormal belief.

Some sceptical members have resigned from the SPR. Eric Dingwall resigned and wrote " After sixty years' experience and personal acquaintance with most of the leading parapsychologists of that period I do not think I could name half a dozen whom I could call objective students who honestly wished to discover the truth. The great majority wanted to prove something or other: They wanted the phenomena into which they were inquiring to serve some purpose in supporting preconceived theories of their own."

The following is a list of presidents:

The Society publishes "Proceedings of the Society for Psychical Research", the "Journal of the Society for Psychical Research", and the "Paranormal Review", as well as the online Psi Encyclopedia.

First published in 1882 as a public record of the activities of the SPR, the "Proceedings" are now reserved for longer pieces of work, such as Presidential Addresses, and are only occasionally published. The current editor is Dr David Vernon.

The "Journal of the Society for Psychical Research" has been published quarterly since 1884. It was introduced as a private, members-only periodical to supplement the "Proceedings". It now focuses on current laboratory and field research, but also includes theoretical, methodological and historical papers on parapsychology. It also publishes book reviews and correspondence. The current editor is Dr David Vernon.

The "Paranormal Review" is the magazine of the Society for Psychical Research. Formerly known as the "Psi Researcher", it has been published since 1996. Previous editors have included Dr Nicola J. Holt. The current editor is Dr Leo Ruickbie.

A number of other psychical research organisations use the term 'Society for Psychical Research' in their name.



SPR histories


Scholarly studies


Criticism




</doc>
<doc id="28123" url="https://en.wikipedia.org/wiki?curid=28123" title="Sniper">
Sniper

A sniper is a military/paramilitary marksman who operates to maintain effective visual contact with the enemy and engage targets from concealed positions or at distances exceeding their detection capabilities. Snipers generally have specialized training and are equipped with high-precision rifles and high-magnification optics, and often feed information back to their units or command headquarters.

In addition to marksmanship and long range shooting, military snipers are trained in a variety of tactical techniques: detection, stalking, and target range estimation methods, camouflage, field craft, infiltration, special reconnaissance and observation, surveillance and target acquisition.

The verb "to snipe" originated in the 1770s among soldiers in British India in reference to shooting snipe, which was considered a challenging target for marksmen.
The agent noun "sniper" appears by the 1820s. The term "sniper" was first attested in 1824 in the sense of the word "sharpshooter".

A somewhat older term is "sharp shooter", a calque of 18th-century German "Scharfschütze", in use in British newspapers as early as 1801.

Different countries use different military doctrines regarding snipers in military units, settings, and tactics.

Generally, a sniper's primary function in modern warfare is to provide detailed reconnaissance from a concealed position and, if necessary, to reduce the enemy's fighting ability by shooting high-value targets (especially officers and other key personnel) and in the process pinning down and demoralizing the enemy. Typical sniper missions include managing intelligence information they gather during reconnaissance and surveillance, target acquisition for air-strikes and artillery, assist employed combat force with fire support and counter-sniper tactics, killing enemy commanders, selecting targets of opportunity, and even destruction of military equipment, which tend to require use of anti-materiel rifles in the larger calibers such as the .50 BMG, like the Barrett M82, McMillan Tac-50, and Denel NTW-20.

Soviet- and Russian-derived military doctrines include squad-level snipers. Snipers have increasingly been demonstrated as useful by US and UK forces in the recent Iraq campaign in a fire support role to cover the movement of infantry, especially in urban areas.

Military snipers from the US, UK, and other countries that adopt their military doctrine are typically deployed in two-man sniper teams consisting of a shooter and spotter. A common practice is for a shooter and a spotter to take turns in order to avoid eye fatigue. In most recent combat operations occurring in large densely populated towns, such as Fallujah, Iraq, two teams would be deployed together to increase their security and effectiveness in an urban environment. A sniper team would be armed with its long-range weapon and a shorter-ranged weapon in case of close contact combat.

The German doctrine of largely independent snipers and emphasis on concealment, developed during the Second World War, has been most influential on modern sniper tactics, and is currently used throughout Western militaries (examples are specialized camouflage clothing, concealment in terrain and emphasis on coup d'œil).

Sniper rifles are classified as crew-served, as the term is used in the United States military. A sniper team (or sniper cell) consists of a combination of one or more "shooters" with force protection elements and support personnel: such as a "spotter" or a "flanker". Within the Table of Organization and Equipment for both the United States Army and the U.S. Marine Corps, the operator of the weapon has an assistant trained to fulfill multiple roles, in addition to being sniper-qualified in the operation of the weapon.

The shooter fires the shot while the spotter assists in observation of targets, atmospheric conditions and handles ancillary tasks as immediate security of their location, communication with other parties; including directing artillery fire and close air support. A flanker's task is to observe areas not immediately visible to the sniper or spotter and assist with the team's perimeter and rear security, therefore flankers are usually armed with an assault rifle or battle rifle. Both spotter and flanker carry additional ammunition and associated equipment.

The spotter detects, observes, and assigns targets and watches for the results of the shot. Using a spotting scope or a rangefinder, the spotter will also read the wind by using physical indicators and the mirage caused by the heat on the ground. Also, in conjunction with the shooter, the spotter will make calculations for distance, angle shooting (slant range), mil dot related calculations, correction for atmospheric conditions and leads for moving targets. It is not unusual for the spotter to be equipped with a notepad and a laptop computer specifically for performing these calculations.

Law enforcement snipers, commonly called police snipers, and military snipers differ in many ways, including their areas of operation and tactics. A police sharpshooter is part of a police operation and usually takes part in relatively short missions. Police forces typically deploy such sharpshooters in hostage scenarios. This differs from a military sniper, who operates as part of a larger army, engaged in warfare. Sometimes as part of a SWAT team, police snipers are deployed alongside negotiators and an assault team trained for close quarters combat. As policemen, they are trained to shoot only as a last resort, when there is a direct threat to life; the police sharpshooter has a well-known rule: "Be prepared to take a life to save a life." Police snipers typically operate at much shorter ranges than military snipers, generally under and sometimes even less than . Both types of snipers do make difficult shots under pressure, and often perform one-shot kills.
Police units that are unequipped for tactical operations may rely on a specialized SWAT team, which may have a dedicated sniper. Some police sniper operations begin with military assistance. Police snipers placed in vantage points, such as high buildings, can provide security for events. In one high-profile incident, Mike Plumb, a SWAT sniper in Columbus, Ohio, prevented a suicide by shooting a revolver out of the individual's hand, leaving him unharmed.
The need for specialized training for police sharpshooters was made apparent in 1972 during the Munich massacre when the German police could not deploy specialized personnel or equipment during the standoff at the airport in the closing phase of the crisis, and consequently all of the Israeli hostages were killed. While the German army did have snipers in 1972, the use of army snipers in the scenario was impossible due to the German constitution's explicit prohibition of the use of the military in domestic matters. This lack of trained snipers who could be used in civilian roles was later addressed with the founding of the specialized police counter-terrorist unit GSG 9.

The longest confirmed sniper kill in combat was achieved by an undisclosed member of the Canadian JTF2 special forces in June 2017 at a distance of .

The previous record holder was Craig Harrison, a Corporal of Horse (CoH) in the Blues and Royals RHG/D of the British Army. In November 2009, Harrison struck two Taliban machine gunners consecutively south of Musa Qala in Helmand Province in Afghanistan at a range of or 1.54 miles using a L115A3 Long Range Rifle.
The QTU Lapua external ballistics software, using continuous doppler drag coefficient (C) data provided by Lapua, predicts that such shots traveling would likely have struck their targets after nearly 6.0 seconds of flight time, having lost 93% of their kinetic energy, retaining of their original velocity, and having dropped or 2.8° from the original bore line. Due to the extreme distances and travel time involved, even a light cross-breeze of would have diverted such shots off target, which would have required compensation.

The calculation assumes a "flat-fire scenario" (a situation where the shooting and target positions are at equal elevation), utilizing British military custom high-pressure .338 Lapua Magnum cartridges, loaded with 16.2 g (250 gr) Lapua LockBase B408 bullets, fired at 936 m/s (3,071 ft/s) muzzle velocity under the following on-site (average) atmospheric conditions: barometric pressure: at sea-level equivalent or on-site, humidity: 25.9%, and temperature: in the region for November 2009, resulting in an air density ρ = 1.0854 kg/m at the elevation of Musa Qala. Harrison mentions in reports that the environmental conditions were perfect for long range shooting, "... no wind, mild weather, clear visibility." In a BBC interview, Harrison reported it took about nine shots for him and his spotter to initially range the target successfully.

Before the development of rifling, firearms were smoothbore and inaccurate over long distance. Barrel rifling was invented at the end of the fifteenth century, but was only employed in large cannons. Over time, rifling, along with other gunnery advances, has increased the performance of modern firearms.

Early forms of sniping or marksmanship were used during the American Revolutionary War. For instance, in 1777 at the battle of Saratoga the Colonists hid in the trees and used early model rifles to shoot British officers. Most notably, Timothy Murphy shot and killed General Simon Fraser of Balnain on 7 October 1777 at a distance of about 400 yards. During the Battle of Brandywine, Capt. Patrick Ferguson had a tall, distinguished American officer in his rifle's iron sights. Ferguson did not take the shot, as the officer had his back to Ferguson; only later did Ferguson learn that George Washington had been on the battlefield that day.
A special unit of marksmen was established during the Napoleonic Wars in the British Army. While most troops at that time used inaccurate smoothbore muskets, the British "Green Jackets" (named for their distinctive green uniforms) used the famous Baker rifle. Through the combination of a leather wad and tight grooves on the inside of the barrel (rifling), this weapon was far more accurate, though slower to load. These Riflemen were the elite of the British Army, and served at the forefront of any engagement, most often in skirmish formation, scouting out and delaying the enemy. Another term, "sharp shooter" was in use in British newspapers as early as 1801. In the "Edinburgh Advertiser", 23 June 1801, can be found the following quote in a piece about the North British Militia; "This Regiment has several Field Pieces, and two companies of Sharp Shooters, which are very necessary in the modern Stile of War". The term appears even earlier, around 1781, in Continental Europe, translated from the German Scharfschütze.

The Whitworth rifle was arguably the first long-range sniper rifle in the world. A muzzleloader designed by Sir Joseph Whitworth, a prominent British engineer, it used polygonal rifling instead, which meant that the projectile did not have to bite into grooves as was done with conventional rifling. The Whitworth rifle was far more accurate than the Pattern 1853 Enfield, which had shown some weaknesses during the recent Crimean War. At trials in 1857 which tested the accuracy and range of both weapons, Whitworth's design outperformed the Enfield at a rate of about three to one. The Whitworth rifle was capable of hitting the target at a range of 2,000 yards, whereas the Enfield could only manage it at 1,400 yards.

During the Crimean War, the first optical sights were designed to fit onto rifles. Much of this pioneering work was the brainchild of Colonel D. Davidson, using optical sights produced by Chance Brothers of Birmingham. This allowed a marksman to observe and target objects more accurately at a greater distance than ever before. The telescopic sight, or scope, was originally fixed and could not be adjusted, which therefore limited its range.

Despite its success at the trials, the rifle was not adopted by the British Army. However, the Whitworth Rifle Company was able to sell the weapon to the French army, and also to the Confederacy during the American Civil War, where both the Union and Confederate armies employed sharpshooters. The most notable incident was during the Battle of Spotsylvania Court House, where on 9 May 1864, Union General John Sedgwick was killed by a Confederate Whitworth sharpshooter at a range of about after saying the enemy "couldn't hit an elephant at this distance".

During the Boer War the latest breech-loading rifled guns with magazines and smokeless powder were used by both sides. The British were equipped with the Lee–Metford rifle, while the Boers had received the latest Mauser rifles from Germany. In the open terrain of South Africa the marksmen were a crucial component to the outcome of the battle.

The first British sniper unit began life as the Lovat Scouts, a Scottish Highland regiment formed in 1899, that earned high praise during the Second Boer War (1899–1902). The unit was formed by Lord Lovat and reported to an American, Major Frederick Russell Burnham, the British Army Chief of Scouts under Lord Roberts. Burnham fittingly described these scouts as "half wolf and half jackrabbit.". Just like their Boer scout opponents, these scouts were well practised in the arts of marksmanship, field craft, map reading, observation, and military tactics. They were skilled woodsmen and practitioners of discretion: "He who shoots and runs away, lives to shoot another day." They were also the first known military unit to wear a ghillie suit. 
Hesketh Hesketh-Prichard said of them that "keener men never lived", and that "Burnham was the greatest scout of our time." Burnham distinguished himself in wars in South Africa, Rhodesia, and in Arizona fighting the Apaches, and his definitive work, "Scouting on Two Continents," provides a dramatic and enlightening picture of what a sniper was at the time and how he operated.

After the war, this regiment went on to formally become the first official sniper unit, then better known as "sharpshooters".

During World War I, snipers appeared as deadly sharpshooters in the trenches. At the start of the war, only Imperial Germany had troops that were issued scoped sniper rifles. Although sharpshooters existed on all sides, the Germans specially equipped some of their soldiers with scoped rifles that could pick off enemy soldiers showing their heads out of their trench. At first the French and British believed such hits to be coincidental hits, until the German scoped rifles were discovered. During World War I, the German army received a reputation for the deadliness and efficiency of its snipers, partly because of the high-quality lenses that German industry could manufacture.
Soon the British army began to train their own snipers in specialized sniper schools. Major Hesketh Hesketh-Prichard was given formal permission to begin sniper training in 1915, and founded the First Army School of Sniping, Observation, and Scouting at Linghem in France in 1916. Starting with a first class of only six, in time he was able to lecture to large numbers of soldiers from different Allied nations, proudly proclaiming in a letter that his school was turning out snipers at three times the rate of any such other school in the world.

He also devised a metal-armoured double loophole that would protect the sniper observer from enemy fire. The front loophole was fixed, but the rear was housed in a metal shutter sliding in grooves. Only when the two loopholes were lined up—a one-to-twenty chance—could an enemy shoot between them. Another innovation was the use of a dummy head to find the location of an enemy sniper. The papier-mâché figures were painted to resemble soldiers to draw sniper fire. Some were equipped with rubber surgical tubing so the dummy could "smoke" a cigarette and thus appear realistic. Holes punched in the dummy by enemy sniper bullets then could be used for triangulation purposes to determine the position of the enemy sniper, who could then be attacked with artillery fire. He developed many of the modern techniques in sniping, including the use of spotting scopes and working in pairs, and using Kim's Game to train observational skills.

In 1920, he wrote his account of his war time activities in his book "", to which reference is still made by modern authors regarding the subject.

The main sniper rifles used during the First World War were the German Mauser Gewehr 98; the British Pattern 1914 Enfield and Lee–Enfield SMLE Mk III, the Canadian Ross Rifle, the American M1903 Springfield, and the Russian M1891 Mosin–Nagant.

During the interbellum, most nations dropped their specialized sniper units, notably the Germans. Effectiveness and dangers of snipers once again came to the fore during the Spanish Civil War. The only nation that had specially trained sniper units during the 1930s was the Soviet Union. Soviet snipers were trained in their skills as marksmen, in using the terrain to hide themselves from the enemy and the ability to work alongside regular forces. This made the Soviet sniper training focus more on "normal" combat situations than those of other nations.

Snipers reappeared as important factors on the battlefield from the first campaign of World War II. During Germany's 1940 campaigns, lone, well-hidden French and British snipers were able to halt the German advance for a considerable amount of time. For example, during the pursuit to Dunkirk, British snipers were able to significantly delay the German infantry's advance. This prompted the British once again to increase training of specialized sniper units. Apart from marksmanship, British snipers were trained to blend in with the environment, often by using special camouflage clothing for concealment. However, because the British Army offered sniper training exclusively to officers and non-commissioned officers, the resulting small number of trained snipers in combat units considerably reduced their overall effectiveness.

During the Winter War, Finnish snipers took a heavy toll of the invading Soviet army. Simo Häyhä is credited with 505 confirmed kills, most with the Finnish version of the iron-sighted bolt-action Mosin–Nagant.

One of the best known battles involving snipers, and the battle that made the Germans reinstate their specialized sniper training, was the Battle of Stalingrad. Their defensive position inside a city filled with rubble meant that Soviet snipers were able to inflict significant casualties on the Wehrmacht troops. Because of the nature of fighting in city rubble, snipers were very hard to spot and seriously dented the morale of the German attackers. The best known of these snipers was probably Vasily Zaytsev, featured in the novel "War of the Rats" and the subsequent film "Enemy At The Gates".
German "Scharfschützen" were prepared before the war, equipped with Karabiner 98 and later Gewehr 43 rifles, but there were often not enough of these weapons available, and as such some were armed with captured scoped Mosin–Nagant 1891/30, SVT or Czech Mauser rifles. The Wehrmacht re-established its sniper training in 1942, drastically increasing the number of snipers per unit with the creation of an additional 31 sniper training companies by 1944. German snipers were at the time the only snipers in the world issued with purpose-manufactured sniping ammunition, known as the 'effect-firing' sS round. The 'effect-firing' sS round featured an extra carefully measured propellant charge and seated a heavy 12.8 gram (198 gr) full-metal-jacketed boat-tail projectile of match-grade build quality, lacking usual features such as a seating ring to improve the already high ballistic coefficient of .584 (G1) further. For aiming optics German snipers used the Zeiss Zielvier 4x (ZF39) telescopic sight which had bullet drop compensation in 50 m increments for ranges from 100 m up to 800 m or in some variations from 100 m up to 1000 m or 1200 m. There were ZF42, Zielfernrohr 43 (ZF 4), Zeiss Zielsechs 6x and other telescopic sights by various manufacturers like the Ajack 4x, Hensoldt Dialytan 4x and Kahles Heliavier 4x with similar features employed on German sniper rifles. Several different mountings produced by various manufacturers were used for mounting aiming optics to the rifles. In February 1945 the Zielgerät 1229 active infrared aiming device was issued for night sniping with the StG 44 assault rifle.

A total of 428,335 individuals received Red Army sniper training, including Soviet and non-Soviet partisans, with 9,534 receiving the sniping 'higher qualification'. During World War ІІ, two six-month training courses for women alone trained nearly 55,000 snipers, of which more than two thousand later served in the army. On average there was at least one sniper in an infantry platoon and one in every reconnaissance platoon, including in tank and even artillery units. Some used the PTRD anti-tank rifle with an adapted scope as an early example of an anti-materiel rifle.

In the United States Armed Forces, sniper training was only very elementary and was mainly concerned with being able to hit targets over long distances. Snipers were required to be able to hit a body over 400 meters away, and a head over 200 meters away. There was almost no instruction in blending into the environment. Sniper training varied from place to place, resulting in wide variation in the qualities of snipers. The main reason the US did not extend sniper training beyond long-range shooting was the limited deployment of US soldiers until the Normandy Invasion. During the campaigns in North Africa and Italy, most fighting occurred in arid and mountainous regions where the potential for concealment was limited, in contrast to Western and Central Europe.

The U.S. Army's lack of familiarity with sniping tactics proved disastrous in Normandy and the campaign in Western Europe where they encountered well trained German snipers. In Normandy, German snipers remained hidden in the dense vegetation and were able to encircle American units, firing at them from all sides. The American and British forces were surprised by how near the German snipers could approach in safety and attack them, as well as by their ability to hit targets at up to 1,000m. A notable mistake made by inexperienced American soldiers was to lie down and wait when targeted by German snipers, allowing the snipers to pick them off one after another. German snipers often infiltrated Allied lines and sometimes when the front-lines moved, they continued to fight from their sniping positions, refusing to surrender until their rations and munitions were exhausted.

Those tactics were also a consequence of changes in German enlistment. After several years of war and heavy losses on the Eastern Front, the German army was forced to rely more heavily on enlisting teenage soldiers. Due to lack of training in more complex group tactics, and thanks to rifle training provided by the Hitlerjugend, those soldiers were often used as autonomous left-behind snipers. While an experienced sniper would take a few lethal shots and retreat to a safer position, those young boys, due both to a disregard for their own safety and to lack of tactical experience would frequently remain in a concealed position and fight until they ran out of ammunition or were killed or wounded. While this tactic generally ended in the demise of the sniper, giving rise to the nickname "Suicide Boys" that was given to those soldiers, this irrational behavior proved quite disruptive to the Allied forces' progress. After World War II, many elements of German sniper training and doctrine were copied by other countries.
In the Pacific War, the Empire of Japan trained snipers. In the jungles of Asia and the Pacific Islands, snipers posed a serious threat to U.S., British, and Commonwealth troops. Japanese snipers were specially trained to use the environment to conceal themselves. Japanese snipers used foliage on their uniforms and dug well-concealed hide-outs that were often connected with small trenches. There was no need for long range accuracy because most combat in the jungle took place within a few hundred meters. Japanese snipers were known for their patience and ability to remain hidden for long periods. They almost never left their carefully camouflaged hiding spots. This meant that whenever a sniper was in the area, the location of the sniper could be determined after the sniper had fired a few shots. The Allies used their own snipers in the Pacific, notably the U.S. Marines, who used M1903 Springfield rifles.

Common sniper rifles used during the Second World War include: the Soviet M1891/30 Mosin–Nagant and, to a lesser extent, the SVT-40; the German Mauser Karabiner 98k and Gewehr 43; the British Lee–Enfield No. 4 and Pattern 1914 Enfield; the Japanese Arisaka 97; the American M1903A4 Springfield and M1C Garand. The Italians trained few snipers and supplied them with a scoped Carcano Model 1891.

Military sniper training aims to teach a high degree of proficiency in camouflage and concealment, stalking, observation and map reading as well as precision marksmanship under various operational conditions. Trainees typically shoot thousands of rounds over a number of weeks, while learning these core skills.

Snipers are trained to squeeze the trigger straight back with the ball of their finger, to avoid jerking the gun sideways. The most accurate position is prone, with a sandbag supporting the stock, and the stock's cheek-piece against the cheek. In the field, a bipod can be used instead. Sometimes a sling is wrapped around the weak arm (or both) to reduce stock movement. Some doctrines train a sniper to breathe deeply before shooting, then hold their lungs empty while they line up and take their shot. Some go further, teaching their snipers to shoot between heartbeats to minimize barrel motion.

The key to sniping is accuracy, which applies to both the weapon and the shooter. The weapon should be able to consistently place shots within tight tolerances. The sniper in turn must utilize the weapon to accurately place shots under varying conditions.

A sniper must have the ability to accurately estimate the various factors that influence a bullet's trajectory and point of impact such as: range to the target, wind direction, wind velocity, altitude and elevation of the sniper and the target and ambient temperature. Mistakes in estimation compound over distance and can decrease lethality or cause a shot to miss completely.

Snipers zero their weapons at a target range or in the field. This is the process of adjusting the scope so that the bullet's points-of-impact is at the point-of-aim (centre of scope or scope's cross-hairs) for a specific distance. A rifle and scope should retain its zero as long as possible under all conditions to reduce the need to re-zero during missions.

A sandbag can serve as a useful platform for shooting a sniper rifle, although any soft surface such as a rucksack will steady a rifle and contribute to consistency. In particular, bipods help when firing from a prone position, and enable the firing position to be sustained for an extended period of time. Many police and military sniper rifles come equipped with an adjustable bipod. Makeshift bipods known as shooting sticks can be constructed from items such as tree branches or ski poles.

Range and accuracy vary depending on the cartridge and specific ammunition types that are used. Typical ranges for common battle field cartridges are as follows:

Servicemen volunteer for the rigorous sniper training and are accepted on the basis of their aptitude, physical ability, marksmanship, patience and mental stability. Military snipers may be further trained as forward air controllers (FACs) to direct air strikes or forward observers (FOs) to direct artillery or mortar fire.

From 2011, the Russian armed forces has run newly developed sniper courses in military district training centres. In place of the Soviet practice of mainly squad sharpshooters, which were often designated during initial training (and of whom only few become snipers "per se"), "new" Army snipers are to be trained intensively for 3 months (for conscripts) or longer (for contract soldiers). The training program includes theory and practice of countersniper engagements, artillery spotting and coordination of air support. The first instructors are the graduates of the Solnechnogorsk sniper training centre.

The method of sniper deployment, according to the Ministry of Defence, is likely to be one three-platoon company at the brigade level, with one of the platoons acting independently and the other two supporting the battalions as needed.

The range to the target is measured or estimated as precisely as conditions permit and correct range estimation becomes absolutely critical at long ranges, because a bullet travels with a curved trajectory and the sniper must compensate for this by aiming higher at longer distances. If the exact distance is not known the sniper may compensate incorrectly and the bullet path may be too high or low. As an example, for a typical military sniping cartridge such as 7.62×51mm NATO (.308 Winchester) M118 Special Ball round this difference (or “drop”) from is . This means that if the sniper incorrectly estimated the distance as 700 meters when the target was in fact 800 meters away, the bullet will be 200 millimeters lower than expected by the time it reaches the target.

Laser rangefinders may be used, and range estimation is often the job of both parties in a team. One useful method of range finding without a laser rangefinder is comparing the height of the target (or nearby objects) to their size on the mil dot scope, or taking a known distance and using some sort of measure (utility poles, fence posts) to determine the additional distance. The average human head is in width, average human shoulders are apart and the average distance from a person's pelvis to the top of their head is .

To determine the range to a target without a laser rangefinder, the sniper may use the mil dot reticle on a scope to accurately find the range. Mil dots are used like a slide rule to measure the height of a target, and if the height is known, the range can be as well. The height of the target (in yards) ×1000, divided by the height of the target (in mils), gives the range in yards. This is only in general, however, as both scope magnification (7×, 40×) and mil dot spacing change. The USMC standard is that 1 mil (that is, 1 milliradian) equals 3.438 MOA (minute of arc, or, equivalently, minute of angle), while the US Army standard is 3.6 MOA, chosen so as to give a diameter of 1 yard at a distance of 1,000 yards (or equivalently, a diameter of 1 meter at a range of 1 kilometer.) Many commercial manufacturers use 3.5, splitting the difference, since it is easier to work with.

It is important to note that angular mil ("mil") is only an approximation of a milliradian and different organizations use different approximations.

At longer ranges, bullet drop plays a significant role in targeting. The effect can be estimated from a chart, which may be memorized or taped to the rifle, although some scopes come with Bullet Drop Compensator (BDC) systems that only require the range be dialed in. These are tuned to both a specific class of rifle and specific ammunition. Every bullet type and load will have different ballistics. .308 Federal 175 grain (11.3 g) BTHP match shoots at . Zeroed at , a 16.2 MOA adjustment would have to be made to hit a target at . If the same bullet was shot with 168 grain (10.9 g), a 17.1 MOA adjustment would be necessary.

Shooting uphill or downhill is confusing for many because gravity does not act perpendicular to the direction the bullet is traveling. Thus, gravity must be divided into its component vectors. Only the fraction of gravity equal to the cosine of the angle of fire with respect to the horizon affects the rate of fall of the bullet, with the remainder adding or subtracting negligible velocity to the bullet along its trajectory. To find the correct zero, the sniper multiplies the actual distance to the range by this fraction and aims as if the target were that distance away. For example, a sniper who observes a target 500 meters away at a 45-degree angle downhill would multiply the range by the cosine of 45 degrees, which is 0.707. The resulting distance will be 353 meters. This number is equal to the horizontal distance to the target. All other values, such as windage, time-to-target, impact velocity, and energy will be calculated based on the actual range of 500 meters. Recently, a small device known as a cosine indicator has been developed. This device is clamped to the tubular body of the telescopic sight, and gives an indicative readout in numerical form as the rifle is aimed up or down at the target. This is translated into a figure used to compute the horizontal range to the target.

Windage plays a significant role, with the effect increasing with wind speed or the distance of the shot. The slant of visible convections near the ground can be used to estimate crosswinds, and correct the point of aim. All adjustments for range, wind, and elevation can be performed by aiming off the target, called "holding over" or Kentucky windage. Alternatively, the scope can be adjusted so that the point of aim is changed to compensate for these factors, sometimes referred to as "dialing in". The shooter must remember to return the scope to zeroed position. Adjusting the scope allows for more accurate shots, because the cross-hairs can be aligned with the target more accurately, but the sniper must know exactly what differences the changes will have on the point-of-impact at each target range.

For moving targets, the point-of-aim is ahead of the target in the direction of movement. Known as "leading" the target, the amount of "lead" depends on the speed and angle of the target's movement as well as the distance to the target. For this technique, holding over is the preferred method. Anticipating the behavior of the target is necessary to accurately place the shot.

The term "hide site" refers to a covered and concealed position from which a sniper and his team can conduct surveillance or fire at targets. A good hide conceals and camouflages the sniper effectively, provides cover from enemy fire and allows a wide view of the surrounding area.

The main purpose of ghillie suits and hide sites is to break up the outline of a person with a rifle.

Many snipers use ghillie suits to hide and stay hidden. Ghillie suits vary according to the terrain into which the sniper wishes to blend. For example, in dry grassland the sniper will typically wear a ghillie suit covered in dead grass.

Shot placement, which is where on the body the sniper is aiming, varies with the type of sniper. Military snipers, who generally do not shoot at targets at less than , usually attempt body shots, aiming at the chest. These shots depend on tissue damage, organ trauma, and blood loss to kill the soldier. Body shots are used because the chest is a larger target.

Police snipers, who generally shoot at much shorter distances, may attempt a more precise shot at particular parts of body or particular devices: in one incident in 2007 in Marseille, a GIPN sniper took a shot from at the pistol of a police officer threatening to commit suicide, destroying the weapon and preventing the police officer from killing himself.

In a high-risk or hostage-taking situation where a suspect is imminently threatening to kill a hostage, police snipers may take head shots to ensure an instant kill. The snipers aim for the medulla oblongata to sever the spine from the brain. While this is believed to prevent the target from reflexively firing their weapon, there is evidence that any brain hit is sufficient.

Snipers are trained for the detection, identification, and location of a targeted soldier in sufficient detail to permit the effective employment of lethal and non-lethal means. Since most kills in modern warfare are by crew-served weapons, reconnaissance is one of the most effective uses of snipers. They use their aerobic conditioning, infiltration skills and excellent long-distance observation equipment (optical scopes) and tactics to approach and observe the enemy. In this role, their rules of engagement typically let them shoot at high-value targets of opportunity, such as enemy officers.

The targets may be personnel or high-value materiel (military equipment and weapons) but most often they target the most important enemy personnel such as officers or specialists (e.g. communications operators) so as to cause maximum disruption to enemy operations. Other personnel they might target include those who pose an immediate threat to the sniper, like dog handlers, who are often employed in a search for snipers. A sniper identifies officers by their appearance and behavior such as symbols of rank, talking to radio operators, sitting as a passenger in a car, sitting in a car with a large radio antenna, having military servants, binoculars/map cases or talking and moving position more frequently. If possible, snipers shoot in descending order by rank, or if rank is unavailable, they shoot to disrupt communications.

Some rifles, such as the Denel NTW-20 and Vidhwansak, are designed for a purely anti-materiel (AM) role, e.g. shooting turbine disks of parked aircraft, missile guidance packages, expensive optics, and the bearings, tubes or wave guides of radar sets. A sniper equipped with the correct rifle can target radar dishes, water containers, the engines of vehicles, and any number of other targets. Other rifles, such as the .50 caliber rifles produced by Barrett and McMillan, are not designed exclusively as AM rifles, but are often employed in such a way, providing the range and power needed for AM applications in a lightweight package compared to most traditional AM rifles. Other calibers, such as the .408 Cheyenne Tactical and the .338 Lapua Magnum, are designed to be capable of limited AM application, but are ideally suited as long range anti-personnel rounds.

Often in situations with multiple targets, snipers use relocation. After firing a few shots from a certain position, snipers move unseen to another location before the enemy can determine where they are and mount a counter-attack. Snipers will frequently use this tactic to their advantage, creating an atmosphere of chaos and confusion. In other, rarer situations, relocation is used to eliminate the factor of wind.

As sniper rifles are often extremely powerful and consequently loud, it is common for snipers to use a technique known as sound masking. When employed by a highly skilled marksman, this tactic can be used as a substitute for a noise suppressor. Very loud sounds in the environment, such as artillery shells air bursting or claps of thunder, can often mask the sound of the shot. This technique is frequently used in clandestine operations, infiltration tactics, and guerrilla warfare.

Due to the surprise nature of sniper fire, high lethality of aimed shots and frustration at the inability to locate and counterattack snipers, sniper tactics have a significant negative effect on morale. Extensive use of sniper tactics can be used to induce constant stress and fear in opposing forces, making them afraid to move about or leave cover. In many ways, the psychological impact imposed by snipers is quite similar to those of landmines, booby-traps, and IEDs (constant threat, high "per event" lethality, inability to strike back).

Historically, captured snipers are often summarily executed. This happened during World War I, and World War II, for example the second Biscari Massacre when 36 suspected snipers were lined up and shot on 14 July 1943.

As a result, if a sniper is in imminent danger of capture, he may discard any items (sniper rifle, laser rangefinder, etc.) which might indicate his status as a sniper. The risk of captured snipers being summarily executed is explicitly referred to in Chapter 6 of US Army doctrine document FM 3-060.11 entitled "SNIPER AND COUNTERSNIPER TACTICS, TECHNIQUES, AND PROCEDURES":

The negative reputation and perception of snipers can be traced back to the American Revolution, when American "Marksmen" intentionally targeted British officers, an act considered uncivilized by the British Army at the time (this reputation was cemented during the Battle of Saratoga, when Benedict Arnold allegedly ordered his marksmen to target British General Simon Fraser, an act that won the battle and French support). The British side used specially selected sharpshooters as well, often German mercenaries.

To demoralize enemy troops, snipers can follow predictable patterns. During the 26th of July Movement in the Cuban Revolution, the revolutionaries led by Fidel Castro always killed the foremost man in a group of President Batista's soldiers. Realizing this, none of Batista's men would walk first, as it was suicidal. This effectively decreased the army's willingness to search for rebel bases in the mountains. An alternative approach to this psychological process is to kill the second man in the row, leading to the psychological effect of nobody wanting to follow the "leader".

The occurrence of sniper warfare has led to the evolution of many counter-sniper tactics in modern military strategies. These aim to reduce the damage caused by a sniper to an army, which can often be harmful to both combat capabilities and morale.

The risk of damage to a chain of command can be reduced by removing or concealing features that would otherwise indicate an officer's rank. Modern armies tend to avoid saluting officers in the field, and eliminate rank insignia on battle dress uniforms (BDU). Officers can seek maximum cover before revealing themselves as good candidates for elimination through actions such as reading maps or using radios.

Friendly snipers can be used to hunt the enemy sniper. Besides direct observation, defending forces can use other techniques. These include calculating the trajectory of a bullet by triangulation. Traditionally, triangulation of a sniper's position was done manually, though radar-based technology has recently become available. Once located, the defenders can attempt to approach the sniper from cover and overwhelm them. The United States military is funding a project known as RedOwl (Robot Enhanced Detection Outpost With Lasers), which uses laser and acoustic sensors to determine the exact direction from which a sniper round has been fired.

The more rounds fired by a sniper, the greater the chance the target has of locating him. Thus, attempts to draw fire are often made, sometimes by offering a helmet slightly out of concealment, a tactic successfully employed in the Winter War by the Finns known as "Kylmä-Kalle" (Cold Charlie). They used a shop mannequin or other doll dressed as a tempting target, such as an officer. The doll was then presented as if it were a real man sloppily covering himself. Usually, Soviet snipers were unable to resist the temptation of an apparently easy kill. Once the angle where the bullet came from was determined, a large caliber gun, such as a Lahti L-39 "Norsupyssy" ("Elephant rifle") anti-tank rifle was fired at the sniper to kill him.

Other tactics include directing artillery or mortar fire onto suspected sniper positions, the use of smoke screens, placing tripwire-operated munitions, mines, or other booby-traps near suspected sniper positions. Even dummy trip-wires can be placed to hamper sniper movement. If anti-personnel mines are unavailable, it is possible to improvise booby-traps by connecting trip-wires to hand grenades, smoke grenades or flares. Though these may not kill a sniper, they will reveal their location. Booby-trap devices can be placed near likely sniper hides, or along the probable routes to and from positions. Knowledge of sniper field-craft will assist in this task.

One very old counter-sniper tactic is to tie rags onto bushes or similar items in suspected sniper hides. These rags flutter in the breeze creating movements in the corner of the sniper's eye, which they will often find distracting. The greatest virtue of this tactic is its simplicity and ease of implementation; however, it is unlikely to prevent a skilled sniper from selecting targets, and may in fact provide a sniper with additional information about the wind near the target.

The use of canine units had been very successful, especially during the Vietnam War.

The use of sniping (in the sense of shooting at relatively long range from a concealed position) to murder came to public attention in a number of sensational U.S. criminal cases, including the Austin sniper incident of 1966 (Charles Whitman), the John F. Kennedy assassination (Lee Harvey Oswald), and the Beltway sniper attacks of late 2002 (Lee Boyd Malvo). However, these incidents usually do not involve the range or skill of military snipers; in all three cases the perpetrators had U.S. military training, but in other specialties. News reports will often (inaccurately) use the term sniper to describe anyone shooting with a rifle at another person.

Sniping has been used in asymmetric warfare situations, for example in the Northern Ireland Troubles, where in 1972, the bloodiest year of the conflict, the majority of the soldiers killed were shot by concealed IRA riflemen. There were some instances in the early 1990s of British soldiers and RUC personnel being shot with .50 caliber Barrett rifles by sniper teams collectively known as the South Armagh sniper.

The sniper is particularly suited to combat environments where one side is at a disadvantage. A careful sniping strategy can use a few individuals and resources to thwart the movement or other progress of a much better equipped or larger force. Sniping enables a few persons to instil terror in a much larger regular force — regardless of the size of the force the snipers are attached to. It is widely accepted that sniping, while effective in specific instances, is much more effective as a broadly deployed psychological attack or as a force-multiplier.

Snipers are less likely to be treated mercifully than non-snipers if captured by the enemy. The rationale for this is that ordinary soldiers shoot at each other at 'equal opportunity' whilst snipers take their time in tracking and killing individual targets in a methodical fashion with a relatively low risk of retaliation.

In 2003, the U.S.-led multinational coalition composed of primarily U.S. and UK troops occupied Iraq and attempted to establish a new government in the country. However, shortly after the initial invasion, violence against coalition forces and among various sectarian groups led to asymmetric warfare with the Iraqi insurgency and civil war between many Sunni and Shia Iraqis.

Through November 2005, when the Pentagon had last reported a sniper fatality, the Army had attributed 28 of 2,100 U.S. deaths to enemy snipers. More recently, since 2006, insurgent snipers such as "Juba" have caused problems for American troops. Claims have been made that Juba have shot up to 37 American soldiers in Iraq as of October 2006.

In 2006, training materials obtained by U.S. intelligence showed that snipers fighting in Iraq were urged to single out and attack engineers, medics, and chaplains on the theory that those casualties would demoralize entire enemy units. Among the training materials, there included an insurgent sniper training manual that was posted on the Internet. Among its tips for shooting U.S. troops, there read: "Killing doctors and chaplains is suggested as a means of psychological warfare."

Some sniper teams in Afghanistan have killed large numbers of Taliban in quite short periods of time. For example, while in Helmand Province, two British snipers (part of the Welsh Guards Battle group) shot dead a total of 75 Taliban in only 40 days during the summer of 2009. In one session of duty, lasting just two hours, they shot and killed eight Taliban. On another occasion, the same team scored a "Quigley" (i.e., killing two Taliban with a single bullet) at a range of 196 metres.

Taliban snipers have themselves caused problems for coalition forces. For example, over a four-month period in early 2011, two Taliban snipers shot dead two British soldiers and wounded six others at an outpost in Qadrat, Helmand province. In one unusual incident, an unnamed 55-year-old ex-Mujahideen fighter with a motorbike and an old British-made Enfield rifle killed two British soldiers with a single shot, hitting the first in the head and the second in the neck.

Sniper activity has been reported during the Arab Spring civil unrest in Libya in 2011, both from anti-governmental and pro-governmental supporters, and in Syria at least from pro-government forces.

Even before firearms were available, soldiers such as archers were specially trained as elite marksmen.







</small>


</doc>
<doc id="28130" url="https://en.wikipedia.org/wiki?curid=28130" title="Sign">
Sign

A sign is an object, quality, event, or entity whose presence or occurrence indicates the probable presence or occurrence of something else. A natural sign bears a causal relation to its object—for instance, thunder is a sign of storm, or medical symptoms signify a disease. A conventional sign signifies by agreement, as a full stop signifies the end of a sentence; similarly the words and expressions of a language, as well as bodily gestures, can be regarded as signs, expressing particular meanings. The physical objects most commonly referred to as signs (notices, road signs, etc., collectively known as signage) generally inform or instruct using written text, symbols, pictures or a combination of these.

The philosophical study of signs and symbols is called semiotics; this includes the study of semiosis, which is the way in which signs (in the semiotic sense) operate.

Semiotics, epistemology, logic, and philosophy of language are concerned about the nature of signs, what they are and how they signify. The nature of signs and symbols and significations, their definition, elements, and types, is mainly established by Aristotle, Augustine, and Aquinas. According to these classic sources, significance is a relationship between two sorts of things: signs and the kinds of things they signify (intend, express or mean), where one term necessarily causes something else to come to the mind. Distinguishing natural signs and conventional signs, the traditional theory of signs (Augustine) sets the following threefold partition of things:
all sorts of indications, evidences, symptoms, and physical signals, there are signs which are "always" signs (the entities of the mind as ideas and images, thoughts and feelings, constructs and intentions); and there are signs that "have" to get their signification (as linguistic entities and cultural symbols). So, while natural signs serve as the source of signification, the human mind is the agency through which signs signify naturally occurring things, such as objects, states, qualities, quantities, events, processes, or relationships. Human language and discourse, communication, philosophy, science, logic, mathematics, poetry, theology, and religion are only some of fields of human study and activity where grasping the nature of signs and symbols and patterns of signification may have a decisive value.

A sign can denote any of the following:

St. Augustine was the first man who synthesized the classical and Hellenistic theories of signs. For him a sign is a thing which is used to signify other things and to make them come to mind ("De Doctrina Christiana" (hereafter DDC) 1.2.2; 2.1.1). The most common signs are spoken and written words (DDC 1.2.2; 2.3.4-2.4.5). Although God cannot be fully expressible, Augustine gave emphasis to the possibility of God’s communication with humans by signs in Scripture (DDC 1.6.6). Augustine endorsed and developed the classical and Hellenistic theories of signs. Among the mainstream in the theories of signs, i.e., that of Aristotle and that of Stoics, the former theory filtered into the works of Cicero (106-43 BC, "De inventione rhetorica" 1.30.47-48) and Quintilian (circa 35-100, "Institutio Oratoria" 5.9.9-10), which regarded the sign as an instrument of inference. In his commentary on Aristotle’s "De Interpretatione", Ammonius said, "according to the division of the philosopher Theophrastus, the relation of speech is twofold, first in regard to the audience, to which speech signifies something, and secondly in regard to the things about which the speaker intends to persuade the audience." If we match DDC with this division, the first part belongs to DDC Book IV and the second part to DDC Books I-III. Augustine, although influenced by these theories, advanced his own theological theory of signs, with whose help one can infer the mind of God from the events and words of Scripture.

Books II and III of DDC enumerate all kinds of signs and explain how to interpret them. Signs are divided into natural ("naturalia") and conventional ("data"); the latter is divided into animal ("bestiae") and human ("homines"); the latter is divided into non-words ("cetera") and words ("verba"); the latter is divided into spoken words ("voces") and written words ("litterae"); the latter is divided into unknown signs ("signa ignota") and ambiguous signs ("signa ambigua"); both the former and the latter are divided respectively into particular signs ("signa propria") and figurative signs ("signa translata"), among which the unknown figurative signs belong to the pagans.
In addition to exegetical knowledge (Quintilian, "Institutio Oratoria" 1.4.1-3 and 1.8.1-21) which follows the order of reading ("lectio"), textual criticism ("emendatio"), explanation ("enarratio"), and judgment ("iudicium"), one needs to know the original language (Hebrew and Greek) and broad background information on Scripture (DDC 2.9.14-2.40.60).

Augustine’s understanding of signs includes several hermeneutical presuppositions as important factors. First, the interpreter should proceed with humility, because only a humble person can grasp the truth of Scripture (DDC 2.41.62). Second, the interpreter must have a spirit of active inquiry and should not hesitate to learn and use pagan education for the purpose of leading to Christian learning, because all truth is God’s truth (DDC 2.40.60-2.42.63). Third, the heart of interpreter should be founded, rooted, and built up in love which is the final goal of the entire Scriptures (DDC 2.42.63).

The sign does not function as its own goal, but its purpose lies in its role as a signification ("res significans", DDC 3.9.13). God gave signs as a means to reveal himself; Christians need to exercise hermeneutical principles in order to understand that divine revelation. Even if the Scriptural text is obscure, it has meaningful benefits. For the obscure text prevents us from falling into pride, triggers our intelligence (DDC 2.6.7), tempers our faith in the history of revelation (DDC 3.8.12), and refines our mind to be suitable to the holy mysteries (DDC 4.8.22). When interpreting signs, the literal meaning should first be sought, and then the figurative meaning (DDC 3.10.14-3.23.33). Augustine suggests the hermeneutical principle that the obscure Scriptural verse is interpreted with the help of plain and simple verses, which formed the doctrine of "scriptura scripturae interpres" (Scripture is the Interpreter of Scripture) in the Reformation Era. Moreover, he introduces the seven rules of Tyconius the Donatist to interpret the obscure meaning of the Bible, which demonstrates his understanding that all truth belongs to God (DDC 3.3.42-3.37.56). In order to apply Augustine's hermeneutics of the sign appropriately in modern times, every division of theology must be involved and interdisciplinary approaches must be taken.


</doc>
<doc id="28131" url="https://en.wikipedia.org/wiki?curid=28131" title="Standard Alphabet by Lepsius">
Standard Alphabet by Lepsius

The Standard Alphabet is a Latin-script alphabet developed by Karl Richard Lepsius. Lepsius initially used it to transcribe Egyptian hieroglyphs and extended it to write African languages, published in 1854 and 1855, and in a revised edition in 1863. The alphabet was comprehensive but was not used much as it contained a lot of diacritic marks and was difficult to read and typeset at that time. It was, however, influential in later projects such as Ellis's Paleotype, and diacritics such as the acute accent for palatalization, under-dot for retroflex, underline for Arabic emphatics, and the click letters continue in modern use.

Vowel length is indicated by a macron ("ā") or a breve ("ă") for long and short vowels, respectively. Open vowels are marked by a line under the letter ("e̱"), while a dot below the letter makes it a close vowel ("ẹ"). Rounded front vowels are written with an umlaut ("ö" and "ü" ), either on top or below, when the space above the letter is needed for vowel length marks (thus "ṳ̄" or "ṳ̆"). Unrounded back vowels are indicated by a corner (˻) below "e" or "i". (Central vowels may be written as one of these series, or as reduced vowels.) 

As in the , nasal vowels get a tilde ("ã"). 

A small circle below a letter is used to mark both the schwa ("e̥", also "ḁ" etc. for other reduced vowels) and syllabic consonants ("r̥" or "l̥", for instance). 

Diphthongs do not receive any special marking, they are simply juxtaposed ("ai" ). A short sign can be used to distinguish which element of the diphthong is the on- or off-glide ("uĭ, ŭi") Vowels in hiatus can be indicated with a diaeresis when necessary ("aï" ).

Other vowels are "a" with a subscript "e" for ; "a" with a subscript "o" for , and "o̩" for or maybe . The English syllabic is "ṙ̥".

Word stress is marked with an acute accent on a long vowel ("á") and with a grave accent on a short vowel ("à").

The Lepsius letters without predictable diacritics are as follows:

Other consonant sounds may be derived from these. For example, palatal and palatalized consonants are marked with an acute accent: "ḱ" , "ǵ" , "ń" , "χ́" , "š́" , "γ́" , "ž́" , "ĺ" , "‘ĺ" , "ǀ́" , "ṕ" , etc. These can also be written "ky, py" etc.

Labialized velars are written with an over-dot: "ġ" , "n̈" , etc. (A dot on a non-velar letter, as in "ṅ" and "ṙ" in the table above, indicates a guttural articulation.)

Retroflex consonants are marked with an under-dot: "ṭ" , "ḍ" , "ṇ" , "ṣ̌" , "ẓ̌" , "ṛ" , "ḷ" , and "ǀ̣" .

The Semitic "emphatic" consonants are marked with an underline: "ṯ" , "ḏ" , "s̱" , "ẕ" , "δ̱" , "ḻ" .

Aspiration is typically marked by "h": "kh" , but a turned apostrophe (Greek "spiritus asper") is also used: "k̒" , "ģ" . Either convention may be used for voiceless sonorants: "m̒" , "‘l" .

Affricates are generally written as sequences, e.g. "tš" for . But the single letters "č" , "ǰ" , "c̀" , "j̀" , "ț" , and "d̦" are also used.

Implosives are written with a macron: "b̄" , "d̄" , "j̄" , "ḡ" . As with vowels, long (geminate) consonants may also be written with a macron, so this transcription can be ambiguous. 

Lepsius typically characterized ejective consonants as tenuis, as they are completely unaspirated, and wrote them with the Greek "spiritus lenis" ("p’", "t’", etc.), which may be the source of the modern convention for ejectives in the IPA. However, when his sources made it clear that there was some activity in the throat, he transcribed them as emphatics. 

When transcribing consonant letters which are pronounced the same but are etymologically distinct, as in Armenian, diacritics from the original alphabet or roman transliteration may be carried over. Similarly, unique sounds such as Czech "ř" may be carried over into Lepsius transcription. Lepsius used a diacritic "r" under "t᷊" and "d᷊" for some poorly described sounds in Dravidian languages. 

Standard capitalization is used. For example, when written in all caps, "γ" becomes "Γ" (as in "AFΓAN" "Afghan").

Tones are marked with an acute and grave accents (backticks) to the right and near the top or the bottom of the corresponding vowel. The diacritic may be underlined for a lower pitch, distinguishing in all eight possible tones.

Tone is not written directly, but rather needs to be established separately for each language. For example, the acute accent may indicate a high tone, a rising tone, or, in the case of Chinese, any tone called "rising" (上) for historical reasons. 

Low rising and falling tones can be distinguished from high rising and falling tones by underlining the accent mark: . The underline also transcribes the Chinese "yin" tones, under the mistaken impression that these tones are actually lower. Two additional tone marks, without any defined phonetic value, are used for Chinese: "level" maˏ (平) and checked maˎ (入); these may also be underlined.




</doc>
<doc id="28132" url="https://en.wikipedia.org/wiki?curid=28132" title="Sidehill gouger">
Sidehill gouger

In American folklore, a Sidehill gouger is a fearsome critter adapted to living on hillsides by having legs on one side of their body shorter than the legs on the opposite side. This peculiarity allows them to walk on steep hillsides, although only in one direction; when lured or chased into the plain, they are trapped in an endless circular path. The creature is variously known as the Sidehill Dodger, Sidehill Hoofer, Sidehill Ousel, Sidehill Loper, Gyascutus, Sidewinder, Wampus, Gudaphro, Hunkus, Rickaboo Racker, Prock, Gwinter, or Cutter Cuss.

Sidehill gougers are mammals who dwell in hillside burrows, and are occasionally depicted as laying eggs There are usually 6 to 8 pups to a litter. Since the gouger is footed for hillsides, it cannot stand up on level ground. If by accident a gouger falls from a hill, it can easily be captured or starve to death. When a clockwise gouger meets a counter-clockwise gouger, they have to fight to the death since they can only go in one direction. The formation of terracettes has been attributed to gouger activity.

Gougers are said to have migrated to the west from New England, a feat accomplished by a pair of gougers who clung to each other in a fashion comparable to "a pair of drunks going home from town with their longer legs on the outer sides. 
A Vermont variation is known as the Wampahoofus. It was reported that farmers crossbreed them with their cows so they could graze easily on mountain sides. 

Frank C. Whitmore and Nicholas Hotton, in their joint tongue-in-cheek response to an article in "Smithsonian Magazine", expounded the taxonomy of sidehill gougers ("Membriinequales declivitous"), noting in particular "the sidehill dodger, which inhabits the Driftless Area of Wisconsin; the dextrosinistral limb ratio approaches unity although the metapodials on the downhill side are noticeably stouter."





</doc>
<doc id="28133" url="https://en.wikipedia.org/wiki?curid=28133" title="Strike">
Strike

Strike may refer to:













</doc>
<doc id="28134" url="https://en.wikipedia.org/wiki?curid=28134" title="Second Vatican Council">
Second Vatican Council

The Second Vatican Council (), informally known as , addressed relations between the Catholic Church and the modern world. It was the twenty-first and most recent ecumenical council of the Catholic Church, and the second to be held at St. Peter's Basilica in the Vatican. The council, through the Holy See, formally opened under the pontificate of Pope John XXIII on 11 October 1962 and closed under Pope Paul VI on the Feast of the Immaculate Conception on 8 December 1965.

Several changes resulted from the council, including the renewal of consecrated life with a revised charism, ecumenical efforts towards dialogue with other religions, and the universal call to holiness, which according to Pope Paul VI was "the most characteristic and ultimate purpose of the teachings of the Council".

According to Pope Benedict XVI, the most important and essential message of the council is "the Paschal Mystery as the center of what it is to be Christian and therefore of the Christian life, the Christian year, the Christian seasons". Other changes which followed the council included the widespread use of vernacular languages in the Mass instead of Latin, the subtle disuse of ornate clerical regalia, the revision of Eucharistic prayers, the abbreviation of the liturgical calendar, the ability to celebrate the Mass "versus populum" (with the officiant facing the congregation), as well as "ad orientem" (facing the "East" and the Crucifix), and modern aesthetic changes encompassing contemporary Catholic liturgical music and artwork. Many of these changes remain divisive among the Catholic faithful.

Of those who took part in the council's opening session, four have become popes: Cardinal Giovanni Battista Montini, who on succeeding John XXIII took the name Pope Paul VI; Bishop Albino Luciani, the future Pope John Paul I; Bishop Karol Wojtyła, who became Pope John Paul II; and Joseph Ratzinger, present as a theological consultant, who became Pope Benedict XVI.

In the 1950s, theological and biblical studies in the Catholic Church had begun to sway away from the Neo-Scholasticism and biblical literalism which a reaction to Catholic modernism had enforced since the First Vatican Council. This shift could be seen in theologians such as Karl Rahner, Michael Herbert, and John Courtney Murray who looked to integrate modern human experience with church principles based on Jesus Christ, as well as others such as Yves Congar, Joseph Ratzinger and Henri de Lubac, who looked to an accurate understanding of scripture and the early Church Fathers as a source of renewal ("ressourcement").

At the same time, the world's bishops faced challenges driven by political, social, economic, and technological change. Some of these bishops sought new ways of addressing those challenges. The First Vatican Council had been held nearly a century before but had been cut short in 1870 when the Italian Army entered the city of Rome at the end of Italian unification. As a result, only deliberations on the role of the papacy and the congruent relationship of faith and reason were completed, with examination of pastoral issues concerning the direction of the Church left unaddressed.

Pope John XXIII, however, gave notice of his intention to convene the Council on 25 January 1959, less than three months after his election in October 1958. This sudden announcement, which caught the Curia by surprise, caused little initial official comment from Church insiders. Reaction to the announcement was widespread and largely positive from both religious and secular leaders outside the Catholic Church, and the council was formally summoned by the apostolic constitution "Humanae Salutis" on 25 December 1961. In various discussions before the Council convened, John XXIII said that it was time to "open the windows [of the Church] and let in some fresh air." He invited other Christians outside the Catholic Church to send observers to the Council. Acceptances came from both the Eastern Orthodox Church and Protestant denominations as internal observers, but these observers did not cast votes in the approbation of the conciliar documents.

Pope John XXIII's announcement on 25 January 1959 of his intention to call a general council came as a surprise even to the cardinals present. The Pontiff pre-announced the council under a full moon when the faithful with their candlelights gathered in St. Peter's square and jokingly noted about the brightness of the moon.

He had tested the idea only ten days before with one of them, his Cardinal Secretary of State Domenico Tardini, who gave enthusiastic support to the idea. Although the Pope later said the idea came to him in a flash in his conversation with Tardini, two cardinals had earlier attempted to interest him in the idea. They were two of the most conservative, Ernesto Ruffini and Alfredo Ottaviani, who had already in 1948 proposed the idea to Pope Pius XII and who put it before John XXIII on 27 October 1958.

Actual preparations for the Council took more than two years, and included work from 10 specialised commissions, people for mass media and Christian Unity, and a Central Commission for overall coordination. These groups, composed mostly of members of the Roman Curia, produced 987 proposed constituting sessions, making it the largest gathering in any council in church history. (This compares to Vatican I, where 737 attended, mostly from Europe.) Attendance varied in later sessions from 2,100 to over 2,300. In addition, a varying number of "periti" () were available for theological consultation—a group that turned out to have a major influence as the council went forward. Seventeen Orthodox Churches and Protestant denominations sent observers. More than three dozen representatives of other Christian communities were present at the opening session, and the number grew to nearly 100 by the end of the 4th Council Sessions.

Pope John XXIII opened the Council on 11 October 1962 in a public session and read the declaration "Gaudet Mater Ecclesia" before the Council Fathers.

What is needed at the present time is a new enthusiasm, a new joy and serenity of mind in the unreserved acceptance by all of the entire Christian faith, without forfeiting that accuracy and precision in its presentation which characterized the proceedings of the Council of Trent and the First Vatican Council. What is needed, and what everyone imbued with a truly Christian, Catholic and apostolic spirit craves today, is that this doctrine shall be more widely known, more deeply understood, and more penetrating in its effects on men's moral lives. What is needed is that this certain and immutable doctrine, to which the faithful owe obedience, be studied afresh and reformulated in contemporary terms. For this deposit of faith, or truths which are contained in our time-honored teaching is one thing; the manner in which these truths are set forth (with their meaning preserved intact) is something else. .

13 October 1962 marked the initial working session of the Council. That day's agenda included the election for members of the ten conciliar commissions. Each would have sixteen elected and eight appointed members, and were expected to do most of the work of the Council. It had been expected that the members of the preparatory commissions, where the Curia was heavily represented, would be confirmed as the majorities on the conciliar commissions. Senior French Cardinal Achille Liénart addressed the Council, saying that the bishops could not intelligently vote for strangers. He asked that the vote be postponed to give all the bishops a chance to draw up their own lists. German Cardinal Josef Frings seconded that proposal, and the vote was postponed. The first meeting of the Council adjourned after only fifteen minutes.

The bishops met to discuss the membership of the commissions, along with other issues, both in national and regional groups, as well as in gatherings that were more informal. The "schemata" (Latin for drafts) from the preparatory sessions were thrown out, and new ones were created. When the council met on 16 October 1962, a new slate of commission members was presented and approved by the Council. One important change was a significant increase in membership from Central and Northern Europe, instead of countries such as Spain or Italy. More than 100 bishops from Africa, Asia, and Latin America were Dutch or Belgian and tended to associate with the bishops from those countries. These groups were led by Cardinals Bernardus Johannes Alfrink of the Netherlands and Leo Suenens of Belgium.

Eleven commissions and three secretariats were established, with their respective presidents:

After adjournment on 8 December, work began on preparations for the sessions scheduled for 1963. These preparations, however, were halted upon the death of Pope John XXIII on 3 June 1963, since an ecumenical council is automatically interrupted and suspended upon the death of the Pope who convened it, until the next Pope orders the council to be continued or dissolved. Pope Paul VI was elected on 21 June 1963 and immediately announced that the Council would continue.

In the months prior to the second period, Pope Paul VI worked to correct some of the problems of organization and procedure that had been discovered during the first period. This included inviting additional lay Catholic and non-Catholic observers, reducing the number of proposed schemata to seventeen (which were made more general, in keeping with the pastoral nature of the council) and later eliminating the requirement of secrecy surrounding general sessions.

Pope Paul's opening address on 29 September 1963 stressed the pastoral nature of the council, and set out four purposes for it:

During this period, the bishops approved the constitution on the liturgy, "Sacrosanctum Concilium", and the decree on social communication, "Inter mirifica". Work went forward with the schemata on the Church, bishops and dioceses, and ecumenism. On 8 November 1963, Josef Frings criticized the Holy Office, and drew an articulate and impassioned defense by its Secretary, Alfredo Ottaviani. This exchange is often considered the most dramatic of the council (Cardinal Frings' theological adviser was the young Joseph Ratzinger, who would later as a Cardinal head the same department of the Holy See, and from 2005–13 reign as Pope Benedict XVI). The second period ended on 4 December.

In the time between the second and third periods, the proposed schemata were further revised on the basis of comments from the Council Fathers. A number of topics were reduced to statements of fundamental propositions that could gain approval during the third period, with postconciliar commissions handling implementation of these measures.

At the end of the second period, Cardinal Leo Joseph Suenens of Belgium had asked the other bishops: "Why are we even discussing the reality of the church when half of the church is not even represented here?," referring to women. In response, 15 women were appointed as auditors in September 1964. Eventually 23 women were auditors at the Second Vatican Council, including 10 women religious. The auditors had no official role in the deliberations, although they attended the meetings of subcommittees working on council documents, particularly texts that dealt with the laity. They also met together on a weekly basis to read draft documents and comment on them.

During the third period, which began on 14 September 1964, the Council Fathers worked through a large volume of proposals. Schemata on ecumenism ("Unitatis redintegratio"); the official view on Protestant and Eastern Orthodox "separated brethren", the Eastern Rite churches ("Orientalium Ecclesiarum"); and the Dogmatic Constitution of the Church ("Lumen gentium") 'were approved and promulgated by the Pope′.

Schemata on the life and ministry of priests and the missionary activity of the Church were rejected and sent back to commissions for complete rewriting. Work continued on the remaining schemata, in particular those on the Church in the modern world and religious freedom. There was controversy over revisions of the decree on religious freedom and the failure to vote on it during the third period, but Pope Paul promised that this schema would be the first to be reviewed in the next period.

Pope Paul closed the third period on 21 November by announcing a change in the Eucharistic fast and formally reaffirming Mary as "Mother of the Church".

Eleven schemata remained unfinished at the end of the third period, and commissions worked to give them their final form. Schema 13, on the Church in the modern world, was revised by a commission that worked with the assistance of laymen.

Pope Paul VI opened the last period of the Council on 14 September 1965 with the establishment of the Synod of Bishops. This more permanent structure was intended to preserve close cooperation of the bishops with the Pope after the council.

The first business of the fourth period was the consideration of the decree on religious freedom, "Dignitatis humanae", one of the more controversial of the conciliar documents. The vote was 1,997 for to 224 against, a margin that widened even further by the time the bishops finally signed the decree. The principal work of the other part of the period was work on three documents, all of which were approved by the Council Fathers. The lengthened and revised pastoral constitution on the Church in the modern world, "Gaudium et spes", was followed by decrees on missionary activity, "Ad gentes" and the ministry and life of priests, "Presbyterorum ordinis".

The council also gave final approval to other documents that had been considered in earlier sessions. They included the Dogmatic Constitution on Divine Revelation ("Dei verbum"), decrees on the pastoral office of bishops ("Christus Dominus"), the life of persons in religious orders (expanded and modified from earlier sessions, finally titled "Perfectae caritatis"), education for the priesthood ("Optatam totius"), Christian education ("Gravissimum educationis"), and the role of the laity ("Apostolicam actuositatem").

One of the more controversial documents was "Nostra aetate", which stated that the Jews of the time of Christ, taken indiscriminately, and all Jews today are no more responsible for the death of Christ than Christians.

A major event of the final days of the council was the act of Pope Paul and Orthodox Patriarch Athenagoras of a joint expression of regret for many of the past actions that had led up to the Great Schism between the western and eastern churches.

"The old story of the Samaritan has been the model of the spirituality of the council" (Paul VI., address, 7 December): On 8 December, the Council was formally closed, with the bishops professing their obedience to the Council's decrees. To help carry forward the work of the Council, Pope Paul:

The first matter covered by the council was the liturgy, to emphasize "the primacy of God" and "the primacy of adoration," according to Pope Benedict XVI. He said that the most important essential idea of the Council itself is "Paschal Mystery (Christ's passion, death and resurrection) as the center of what it is to be Christian and therefore of the Christian life, the Christian year, the Christian seasons, expressed in Eastertide and on Sunday which is always the day of the Resurrection." Thus, the liturgy, especially the Eucharist, which makes the Paschal Mystery present, is "the summit toward which the activity of the Church is directed; at the same time it is the font from which all her power flows."

The matter that had the most immediate effect on the lives of individual Catholics, was the revision of the liturgy. The central idea was that there ought to be lay participation in the liturgy which means they "take part fully aware of what they are doing, actively engaged in the rite, and enriched by its effects." (SC 11) In the mid-1960s, permissions were granted to celebrate most of the Mass in vernacular languages, including the canon from 1967 onwards. The amount of Scripture read during Mass was greatly expanded, through the introduction of multiple year lectionaries.
Neither the Second Vatican Council nor the subsequent revision of the Roman Missal abolished Latin as the liturgical language of the Roman Rite: the official text of the Roman Missal, on which translations into vernacular languages are to be based, continues to be in Latin and it can still be used in the celebration.

The Dogmatic Constitution on the Church produced by the Council is entitled "Lumen gentium".

In its first chapter, titled ""The Mystery of the Church,"" is the statement that: 

The document adds, "Nevertheless, many elements of sanctification and of truth are found outside its visible confines." The other characteristics of that period were described by Belgian Bishop as "legalism" and "clericalism," in what has been described as "one of the most dramatic moments of Vatican II."

According to Pope Paul VI, "the most characteristic and ultimate purpose of the teachings of the Council" is the universal call to holiness: John Paul II calls this "an intrinsic and essential aspect of [the Council Fathers'] teaching on the Church."

In his plan for the new millennium, "Novo millennio ineunte", John Paul II said that "all pastoral initiatives must be set in relation to holiness" as the first priority of the Church.

The council sought to revive the central role of Scripture in the theological and devotional life of the Church, building upon the work of earlier popes in crafting a modern approach to Scriptural analysis and interpretation. A new approach to interpretation was approved by the bishops. The Church was to continue to provide versions of the Bible in the "mother tongues" of the faithful, and both clergy and laity were to continue to make Bible study a central part of their lives. This affirmed the importance of Sacred Scripture as attested by Providentissimus Deus by Pope Leo XIII and the writings of the Saints, Doctors, and Popes throughout Church history but also approved historically conditioned interpretation of Scripture as presented in Pius XII's 1943 encyclical "Divino afflante Spiritu".

The role of the bishops was brought into renewed prominence, especially when seen collectively, as a college that has succeeded to that of the apostles in teaching and governing the Church. This college was headed by the Pope.

The questioning of the nature of and even validity of the Second Vatican Council continues to be a contending point of rejection and conflict among various religious communities, some of which are not in communion with the Roman Catholic Church. In particular, two schools of thought may be discerned:


The most recent edition of 1983 Code of Canon Law states that Catholics may not disregard the teaching of an ecumenical council even if it does not propose such as definitive. Accordingly, it also maintains that the present living Pope alone judges the criterion of membership for being "in communio" with the Church. The present canon law further articulates: 
In addition to general spiritual guidance, the Second Vatican Council produced very specific recommendations, such as in the document "Gaudiem et Spes": "Any act of war aimed indiscriminately at the destruction of entire cities of extensive areas along with their population is a crime against God and man himself. It merits unequivocal and unhesitating condemnation."

By "the spirit of Vatican II" is often meant promoting teachings and intentions attributed to the Second Vatican Council in ways not limited to literal readings of its documents, spoken of as the "letter" of the Council (cf. Saint Paul's phrase, "the letter kills, but the Spirit gives life").

The spirit of Vatican II is invoked for a great variety of ideas and attitudes. Bishop John Tong Hon of Hong Kong used it with regard merely to an openness to dialogue with others, saying: "We are guided by the spirit of Vatican II: only dialogue and negotiation can solve conflicts."

In contrast, Michael Novak described it as a spirit that: 

To mark the fiftieth anniversary of the beginning of Vatican II, in October 2011, Pope Benedict XVI declared the period from October 2012 to the Solemnity of Christ the King at the end of November 2013 a "Year of Faith", as:








</doc>
<doc id="28135" url="https://en.wikipedia.org/wiki?curid=28135" title="Slovene language">
Slovene language

Slovene ( or ) or Slovenian (; "slovenski jezik" or "slovenščina") belongs to the group of South Slavic languages. It is spoken by approximately 2.5 million speakers worldwide, the majority of whom live in Slovenia. It is the first language of about 2.1 million Slovenian people and is one of the 24 official and working languages of the European Union.

Standard Slovene is the national standard language that was formed in the 18th century, mostly based on Upper and Lower Carniolan dialect groups, the latter being a dialect spoken by Primož Trubar. Unstandardized dialects are more preserved in regions of the Slovene Lands where compulsory schooling was in languages other than Standard Slovene, as was the case with the Carinthian Slovenes in Austria, and the Slovene minority in Italy. For example, the Resian and Torre (Ter) dialects in the Italian Province of Udine differ most from other Slovene dialects.

The distinctive characteristics of Slovene are dual grammatical number, two accentual norms (one characterized by pitch accent), and abundant inflection (a trait shared with many Slavic languages). Although Slovene is basically an SVO language, word order is very flexible, often adjusted for emphasis or stylistic reasons. Slovene has a T-V distinction: second-person plural forms are used for individuals as a sign of respect.

Slovene and Slovak are the only two modern Slavic languages whose names for themselves literally mean "Slavic" ("slověnьskъ" in old Slavonic).

Slovene is an Indo-European language belonging to the Western subgroup of the South Slavic branch of the Slavic languages, together with Serbo-Croatian. It is close to the Chakavian and especially Kajkavian dialects of Serbo-Croatian, but further from the Shtokavian dialect, the basis for the Bosnian, Croatian, Montenegrin, and Serbian standard languages. Furthermore, Slovene shares certain linguistic characteristics with all South Slavic languages, including those of the Eastern subgroup, such as Bulgarian. 

Although Slovene is almost completely intelligible with the Kajkavian dialects of Serbo-Croatian (especially the variant spoken in Hrvatsko Zagorje on the border with Slovenia), mutual intelligibility with other varieties of Serbo-Croatian is hindered by differences in vocabulary, grammar, and pronunciation. The Slovene language also has many commonalities with the West Slavic languages.

Like all Slavic languages, Slovene traces its roots to the same proto-Slavic group of languages that produced Old Church Slavonic. The earliest known examples of a distinct, written Slovene dialect are from the "Freising Manuscripts," known in Slovene as "Brižinski spomeniki". The consensus estimate of their date of origin is between 972 and 1039 (most likely before 1000). These religious writings are among the oldest surviving manuscripts in any Slavic language.

The "Freising Manuscripts" are a record of a proto-Slovene language that was spoken in a much larger territory than modern Slovene, which included most of the present-day Austrian states of Carinthia and Styria, as well as East Tyrol, the Val Pusteria in South Tyrol, and some areas of Upper and Lower Austria. 

By the 15th century, most of the northern areas were gradually Germanized: the northern border of the Slovene-speaking territory stabilized on the line going from north of Klagenfurt to south of Villach and east of Hermagor in Carinthia, while in Styria it was pretty much identical with the current Austrian-Slovenian border. 

This linguistic border remained almost unchanged until the late 19th century, when a second process of Germanization took place, mostly in Carinthia. Between the 9th and 12th century, proto-Slovene spread into northern Istria and in the areas around Trieste.

During most of the Middle Ages, Slovene was a vernacular language of the peasantry, although it was also spoken in most of the towns on Slovene territory, together with German or Italian. Although during this time, German emerged as the spoken language of the nobility, Slovene had some role in the courtly life of the Carinthian, Carniolan and Styrian nobility, as well. This is proved by the survival of certain ritual formulas in Slovene (such as the ritual installation of the Dukes of Carinthia). The words "Buge waz primi, gralva Venus!" ("God be With You, Queen Venus!"), with which Bernhard von Spanheim greeted the poet Ulrich von Liechtenstein upon his arrival in Carinthia in 1227 (or 1238), is another example of some level of Slovene knowledge among high nobility in the region.

The first printed Slovene words, "stara pravda" (meaning 'old justice'), appeared in 1515 in Vienna in a poem of the German mercenaries who suppressed the Slovene peasant revolt. Standard Slovene emerged in the second half of the 16th century, thanks to the works of Slovene Lutheran authors, who were active during the Protestant Reformation. The most prominent authors from this period are Primož Trubar, who wrote the first books in Slovene; Adam Bohorič, the author of the first Slovene grammar; and Jurij Dalmatin, who translated the entire Bible into Slovene.

From the high Middle Ages up to the dissolution of the Austro-Hungarian Empire in 1918, in the territory of present-day Slovenia, German was the language of the elite, and Slovene was the language of the common people. During this period, German had a strong influence on Slovene, and many Germanisms are preserved in contemporary colloquial Slovene. Many Slovene scientists before the 1920s also wrote in foreign languages, mostly German, which was the "lingua franca" of science throughout Central Europe at the time.

During the rise of Romantic Nationalism in the 19th century, the cultural movements of Illyrism and Pan-Slavism brought words from Serbo-Croatian and Czech into standard Slovene, mostly to replace words previously borrowed from German. Most of these innovations have remained, although some were dropped in later development. In the second half of the 19th century, many nationalist authors made an abundant use of Serbo-Croatian words: among them were Fran Levstik and Josip Jurčič, who wrote the first novel in Slovene in 1866. This tendency was reversed in the Fin de siècle period by the first generation of modernist Slovene authors (most notably the writer Ivan Cankar), who resorted to a more "pure" and simple language without excessive Serbo-Croatian borrowings.

During the Kingdom of Yugoslavia in the 1920s and 1930s, the influence of Serbo-Croatian increased again. This was opposed by the younger generations of Slovene authors and intellectuals; among the most fierce opponents of an excessive Serbo-Croatian influence on Slovene were the intellectuals associated with the leftist journal "Sodobnost", as well as some younger Catholic activists and authors. After 1945, numerous Serbo-Croatian words that had been used in the previous decades were dropped. The result was that a Slovene text from the 1910s is frequently closer to modern Slovene than a text from the 1920s and 1930s.

Between 1920 and 1941, the official language of the Kingdom of Yugoslavia was defined as "Serbian-Croatian-Slovene". In practice, Slovene was used in Slovenia, both in education and administration. Many state institutions used only Serbo-Croatian, and a Slovene–Serbo-Croatian bilingualism was applied in many spheres of public life in Slovenia. For examples, at the post offices, railways and in administrative offices, Serbo-Croatian was used together with Slovene. However, state employees were expected to be able to speak Slovene in Slovenia.

During the same time, western Slovenia (the Slovenian Littoral and the western districts of Inner Carniola) was under Italian administration and submitted to a violent policy of Fascist Italianization; the same policy was applied to Slovene speakers in Venetian Slovenia, Gorizia and Trieste. Between 1923 and 1943, all public use of Slovene language in these territories was strictly prohibited, and Slovene language activists were persecuted by the state.

After the Carinthian Plebiscite of 1920, a less severe policy of Germanization took place in the Slovene-speaking areas of southern Carinthia which remained under Austrian administration. After the Anschluss of 1938, the use of Slovene was strictly forbidden in Carinthia, as well. This accelerated a process of language shift in Carinthia, which continued throughout the second half of the 20th century: according to the Austro-Hungarian census of 1910, around 17% of inhabitants of Carinthia spoke Slovene in their daily communication; in 1951, this figure dropped under 10%, and by 2001 to a mere 2.8%.

During World War II, Slovenia was divided among the Axis Powers of Fascist Italy, Nazi Germany, and Hungary. Each of the occupying powers tried to either discourage or entirely suppress the Slovene language.

Following World War II, Slovenia became part of the Socialist Federal Republic of Yugoslavia. Slovene was one of the official languages of the federation. In the territory of Slovenia, it was commonly used in almost all areas of public life. One important exception was the Yugoslav army, where Serbo-Croatian was used exclusively, even in Slovenia.

National independence has revitalized the language: since 1991, when Slovenia gained independence, Slovene has been used as an official language in all areas of public life. In 2004 it became one of the official languages of the European Union upon Slovenia's admission.

Joža Mahnič, a literary historian and president of the publishing house "Slovenska matica", said in February 2008 that Slovene is a language rich enough to express everything, including the most sophisticated and specialised texts. In February 2010, Janez Dular, a prominent Slovenian linguist, commented that, although Slovene is not an endangered language, its scope has been shrinking, especially in science and higher education.

The language is spoken by about 2.5 million people, mainly in Slovenia, but also by Slovene national minorities in Friuli-Venezia Giulia, Italy (around 90,000 in Venetian Slovenia, Resia Valley, Canale Valley, Province of Trieste and in those municipalities of the Province of Gorizia bordering with Slovenia), in southern Carinthia and some parts of Styria in Austria (25,000). It is also spoken in Croatia, especially in Istria, Rijeka and Zagreb (11,800-13,100), in southwestern Hungary (3-5,000), in Serbia (5,000), and by the Slovene diaspora throughout Europe and the rest of the world (around 300,000), particularly in the United States (most notably Ohio, home to an estimated 3,400 speakers), Canada, Argentina, Australia and South Africa.

Slovene is sometimes characterized as the most diverse Slavic language in terms of dialects, with different degrees of mutual intelligibility. Accounts of the number of dialects range from as few as seven dialects, often considered dialect groups or dialect bases that are further subdivided into as many as 50 dialects. Other sources characterize the number of dialects as nine or eight. The Slovene proverb "Every village has its own voice" ("Vsaka vas ima svoj glas") depicts the differences in dialects. Although pronunciation differs greatly from area to area, those differences do not pose major obstacles to understanding. The standard language is mainly used in public presentations or on formal occasions.

The Prekmurje dialect used to have a written norm of its own at one point. The Resian dialects have an independent written norm that is used by their regional state institutions. Speakers of those two dialects have considerable difficulties with being understood by speakers of other varieties of Slovene, needing code-switching to Standard Slovene. Other dialects are mutually intelligible when speakers avoid the excessive usage of regionalisms.

Regionalisms are mostly limited to culinary and agricultural expressions, although there are many exceptions. Some loanwords have become so deeply rooted in the local language that people have considerable difficulties in finding a standard expression for the dialect term (for instance, "kovter" meaning blanket is "prešita odeja" in Standard Slovene, but the latter term is "never" used in speech). Southwestern dialects incorporate a great deal of calques and loanwords from Italian, whereas eastern and northwestern dialects are replete with lexemes of German origin. Usage of such words hinders intelligibility between dialects and is greatly discouraged in formal situations.

Slovene has a phoneme set consisting of 21 consonants and 8 vowels.

Slovene has 21 distinctive consonant phonemes.

All voiced obstruents are devoiced at the end of words unless immediately followed by a word beginning with a vowel or a voiced consonant. In consonant clusters, voicing distinction is neutralized and all consonants assimilate the voicing of the rightmost segment. In this context, , and may occur as voiced allophones of , and , respectively (e.g. "vŕh drevésa" ).


The sequences , and occur only before a vowel. Before a consonant or word-finally, they are reduced to , and respectively. This is reflected in the spelling in the case of , but not for and .

Under certain (somewhat unpredictable) circumstances, at the end of a syllable may become , merging with the allophone of in that position.

Slovene has an eight-vowel (according to Peter Jurgec nine-vowel) system, in comparison to the five-vowel system of Serbo-Croatian.

Slovene nouns retain six of the seven Slavic noun cases: nominative, accusative, genitive, dative, locative and instrumental. There is no distinct vocative; the nominative is used in that role. Nouns, adjectives and pronouns have three numbers: singular, plural and a special dual form that indicates exactly two objects.

Nouns in Slovene are either masculine, feminine or neuter gender. In addition, there is a distinction between animate and inanimate nouns, although this is only relevant for masculine nouns and only in the singular. Animate nouns have an accusative singular form that is identical to the genitive, while for inanimate nouns the accusative singular is the same as the nominative. Animacy is based mostly on semantics and is less rigid than gender. Generally speaking a noun is animate if it refers to something that is generally thought to have free will or the ability to move of its own accord. This includes all nouns for people and animals. All other nouns are inanimate, including plants and other non-moving life forms, and also groups of people or animals. However, there are some nouns for inanimate objects that are generally animate, which mostly include inanimate objects that are named after people or animals. This includes:

Slovene, like most other European languages, has a T–V distinction, or two forms of 'you' for formal and informal situations. Although informal address using the 2nd person singular "ti" form (known as "tikanje") is officially limited to friends and family, talk among children, and addressing animals, it is increasingly used among the middle generation to signal a relaxed attitude or lifestyle instead of its polite or formal counterpart using the 2nd person plural "vi" form (known as "vikanje").

An additional nonstandard but widespread use of a singular participle combined with a plural auxiliary verb (known as "polvikanje") signals a somewhat more friendly and less formal attitude while maintaining politeness:


The use of nonstandard forms ("polvikanje") might be frowned upon by many people and would not likely be used in a formal setting.

The use of the 3rd person plural "oni" ('they') form (known as "onikanje" in both direct address and indirect reference; this is similar to using "Sie" in German) as an ultra-polite form is now archaic or dialectal. It is associated with servant-master relationships in older literature, the child-parent relationship in certain conservative rural communities, and parishioner-priest relationships.

Foreign words used in Slovene are of various types depending on the assimilation they have undergone. The types are:
The loanwords are mostly from German and Italian, while the more recently borrowed and less assimilated words are typically from English.

There are no definite or indefinite articles as in English ("a", "an", "the") or German ("der", "die", "das", "ein", "eine"). A whole verb or a noun is described without articles and the grammatical gender is found from the word's termination. It is enough to say "barka" ("a" or "the barge"), "Noetova barka" ('Noah's ark'). The gender is known in this case to be feminine. In declensions, endings are normally changed; see below. If one should like to somehow distinguish between definiteness or indefiniteness of a noun, one would say "(prav/natanko/ravno) tista barka" ('that (exact) barge') for "the barge" and "neka/ena barka" ('one barge') for "a barge".

Definiteness of a noun phrase can also be discernible through the ending of the accompanying adjective. One should say "rdeči šotor" ([exactly that] red tent) or "rdeč šotor" ([a] red tent). This difference is observable only for masculine nouns in nominative or accusative case. Because of the lack of article in Slovene and audibly insignificant difference between the masculine adjective forms, most dialects do not distinguish between definite and indefinite variants of the adjective, leading to hypercorrection when speakers try to use Standard Slovenian.

This alphabet () was derived in the mid-1840s from the system created by Croatianist Ljudevit Gaj. Intended for the Serbo-Croatian language (in all its varieties), it was patterned on the Czech alphabet of the 1830s. Before that was, for example, written as , or ; as , , or ; sometimes as as a relic from the now modern Russian yery character, usually transliterated as "y"; as ; as ; as ; as , or .

The standard Slovene orthography, used in almost all situations, uses only the letters of the ISO basic Latin alphabet plus , , and :

The orthography thus underdifferentiates several phonemic distinctions:


In the tonemic varieties of Slovene, the ambiguity is even worse: "e" in a final syllable can stand for any of (although is rare).

The reader is expected to gather the interpretation of the word from the context, as in these examples:


To compensate for the shortcomings of the standard orthography, Slovene also uses standardized diacritics or accent marks to denote stress, vowel length and pitch accent, much like the closely related Serbo-Croatian. However, as in Serbo-Croatian, use of such accent marks is restricted to dictionaries, language textbooks and linguistic publications. In normal writing, the diacritics are almost never used, except in a few minimal pairs where real ambiguity could arise.

Two different and mutually incompatible systems of diacritics are used. The first is the simpler non-tonemic system, which can be applied to all Slovene dialects. It is more widely used and is the standard representation in dictionaries such as SSKJ. The tonemic system also includes tone as part of the representation. However, neither system reliably distinguishes schwa from the front mid-vowels, nor vocalised l from regular l . Some sources write these as "ə" and "ł" respectively, but this is not as common.

In the non-tonemic system, the distinction between the two mid-vowels is indicated, as well as the placement of stress and length of vowels:


The tonemic system uses the diacritics somewhat differently from the non-tonemic system. The high-mid vowels and are written "ẹ ọ" with a subscript dot, while the low-mid vowels and are written as plain "e o".

Pitch accent and length is indicated by four diacritical marks:


The schwa vowel is written ambiguously as "e", but its accentuation will sometimes distinguish it: a long vowel mark can never appear on a schwa, while a grave accent can appear only on a schwa. Thus, only "ȅ" and unstressed "e" are truly ambiguous.

Standard Slovene spelling and grammar are defined by the Orthographic Committee and the Fran Ramovš Institute of the Slovenian Language, which are both part of the Slovenian Academy of Sciences and Arts ("Slovenska akademija znanosti in umetnosti", SAZU). The newest reference book of standard Slovene spelling (and to some extent also grammar) is the "Slovenski pravopis" ("SP2001"; Slovene Normative Guide). The latest printed edition was published in 2001 (reprinted in 2003 with some corrections) and contains more than 130,000 dictionary entries. In 2003, an electronic version was published.

The official dictionary of modern Slovene, which was also prepared by SAZU, is "Slovar slovenskega knjižnega jezika" ("SSKJ"; Standard Slovene Dictionary). It was published in five volumes by Državna Založba Slovenije between 1970 and 1991 and contains more than 100,000 entries and subentries with accentuation, part-of-speech labels, common collocations, and various qualifiers. In the 1990s, an electronic version of the dictionary was published and it is available online.

The SAZU considers SP2001 to be the normative source on Slovenian language. When dictionary entries in SP2001 and SSKJ differ, the SP2001 entry takes precedence. SP2001 is called a Spelling Dictionary by the European Network of e-Lexicography.







</doc>
<doc id="28136" url="https://en.wikipedia.org/wiki?curid=28136" title="Slovak language">
Slovak language

Slovak () is an Indo-European language that belongs to the West Slavic languages (together with Czech, Polish, and Sorbian). It is called ' () or ' () in the language itself.

Slovak is the official language of Slovakia, where it is spoken by approximately 5.51 million people (2014). Slovak speakers are also found in the United States, the Czech Republic, Argentina, Serbia, Ireland, Romania, Poland, Canada, Hungary, Croatia, the United Kingdom, Australia, Austria, Ukraine and many other countries worldwide.

Slovak should not be confused with Slovene, or Slovenian (' or '), the main language of Slovenia.

Slovak uses the Latin script with small modifications that include the four diacritics (ˇ, ´, ¨, ˆ) placed above certain letters (a-á,ä; c-č; d-ď; dz-dž; e-é; i-í; l-ľ,ĺ; n-ň; o-ó,ô; r-ŕ; s-š; t-ť; u-ú; y-ý; z-ž)

The primary principle of Slovak spelling is the phonemic principle. The secondary principle is the morphological principle: forms derived from the same stem are written in the same way even if they are pronounced differently. An example of this principle is the assimilation rule (see below). The tertiary principle is the etymological principle, which can be seen in the use of "i" after certain consonants and of "y" after other consonants, although both "i" and "y" are pronounced almost, but usually the same way.

Finally, the rarely applied grammatical principle is present when, for example, the basic singular form and plural form of masculine adjectives are written differently with no difference in pronunciation (e.g. "pekný" = nice – singular versus "pekní" = nice – plural).

In addition, the following rules are present:

Most foreign words receive Slovak spelling immediately or after some time. For example, "weekend" is spelled "víkend", "software" – "softvér", "gay" – "gej" (both not exclusively), and "quality" is spelled "kvalita." Personal and geographical names from other languages using Latin alphabets keep their original spelling unless a fully Slovak form of the name exists (e.g. "Londýn" for "London").

Slovak features some heterophonic homographs (words with identical spelling but different pronunciation and meaning), the most common examples being "krásne" (beautiful) versus "krásne" (beautifully).




The main features of Slovak syntax are as follows:

Some examples include the following:


Word order in Slovak is relatively free, since strong inflection enables the identification of grammatical roles (subject, object, predicate, etc.) regardless of word placement. This relatively free word order allows the use of word order to convey topic and emphasis.

Some examples are as follows:

The unmarked order is subject–verb–object. Variation in word order is generally possible, but word order is not completely free.
In the above example, the noun phrase "ten veľký muž" cannot be split up, so that the following combinations are not possible:

And the following is stylistically not correct:

Slovak does not have articles. The demonstrative pronoun "ten" (fem: "tá", neuter: "to") may be used in front of the noun in situations where definiteness must be made explicit.

Slovak nouns are inflected for case and number. There are six cases: nominative, genitive, dative, accusative, locative, and instrumental. The vocative is no longer morphologically marked. There are two numbers: singular and plural. Nouns have inherent gender. There are three genders: masculine, feminine, and neuter. Adjectives agree with nouns in case, number, and gender.

The numerals 0–10 have unique forms, with numerals 1–4 requiring specific gendered representations. Numerals 11–19 are formed by adding "násť" to the end of each numeral. The suffix "dsať" is used to create numerals 20, 30 and 40; for numerals 50, 60, 70, 80 and 90, "desiat" is used. Compound numerals (21, 1054) are combinations of these words formed in the same order as their mathematical symbol is written (e.g. 21 = dvadsaťjeden, literally "twenty-one").

The numerals are as follows:
Some higher numbers: (200) dvesto... (300) tristo... (900) deväťsto... (1,000) tisíc... (1,100) tisícsto... (2,000) dvetisíc... (100,000) stotisíc... (200,000) dvestotisíc... (1,000,000) milión... (1,000,000,000) miliarda...

Counted nouns have two forms. The most common form is the plural genitive (e.g. "päť domov" = five houses or "stodva žien" = one hundred two women), while the plural form of the noun when counting the amounts of 2-4, etc., is usually the nominative form without counting (e.g. "dva domy" = two houses or "dve ženy" = two women) but gender rules do apply in many cases.

Verbs have three major conjugations. Three persons and two numbers (singular and plural) are distinguished. Several conjugation paradigms exist as follows:













Adverbs are formed by replacing the adjectival ending with the ending -"o" or -"e" / -"y". Sometimes both -"o" and -"e" are possible. Examples include the following:

The comparative/superlative of adverbs is formed by replacing the adjectival ending with a comparative/superlative ending -"(ej)ší" or -"(ej)šie". Examples include the following:

Each preposition is associated with one or more grammatical cases. The noun governed by a preposition must appear in the case required by the preposition in the given context (e.g. from friends = "od priateľov"). "Priateľov" is the genitive case of "priatelia". It must appear in this case because the preposition "od" (= from) always calls for its objects to be in the genitive.
"Po" has a different meaning depending on the case of its governed noun.

The Slovak language is a descendant of Proto-Slavic, itself a descendant of Proto-Indo-European. It is closely related to the other West Slavic languages, primarily to Czech and Polish. Czech also influenced the language in its later development. To lesser degrees, moreover, Slovak has been influenced by German, Latin, Hungarian, and recently English.

Although most dialects of Czech and Slovak are mutually intelligible (see Comparison of Slovak and Czech), eastern Slovak dialects are less intelligible to speakers of Czech and more so closer to Polish and mutual contact between speakers of Czech and speakers of the eastern dialects is limited.

Since the dissolution of Czechoslovakia it has been permitted to use Czech in TV broadcasting and—like any other language of the world—during court proceedings (Administration Procedure Act 99/1963 Zb.). From 1999 to August 2009, the Minority Language Act 184/1999 Z.z., in its section (§) 6, contained the variously interpreted unclear provision saying that "When applying this act, it holds that the use of the Czech language fulfills the requirement of fundamental intelligibility with the state language"; the state language is Slovak and the Minority Language Act basically refers to municipalities with more than 20% ethnic minority population (no such Czech municipalities are found in Slovakia). Since 1 September 2009 (due to an amendment to the State Language Act 270/1995 Z.z.) a language "fundamentally intelligible with the state language" (i.e. the Czech language) may be used in contact with state offices and bodies by its native speakers, and documents written in it and issued by bodies in the Czech Republic are officially accepted. Regardless of its official status, Czech is used commonly both in Slovak mass media and in daily communication by Czech natives as an equal language.

Czech and Slovak have a long history of interaction and mutual influence well before the creation of Czechoslovakia in 1918, a state which existed until 1993. Literary Slovak shares significant orthographic features with Czech, as well as technical and professional terminology dating from the Czechoslovak period, but phonetic, grammatical, and vocabulary differences do exist.

Slavic language varieties tend to be closely related, and have had a large degree of mutual influence, due to the complicated ethnopolitical history of their historic ranges. This is reflected in the many features Slovak shares with neighboring language varieties. Standard Slovak shares high degrees of mutual intelligibility with many Slavic varieties. Despite this closeness to other Slavic varieties, significant variation exists among Slovak dialects. In particular, eastern varieties differ significantly from the standard language, which is based on central and western varieties.

Eastern Slovak dialects have the greatest degree of mutual intelligibility with Polish of all the Slovak dialects followed by Rusyn, but both lack technical terminology and upper register expressions. Polish and Sorbian also differ quite considerably from Czech and Slovak in upper registers, but non-technical and lower register speech is readily intelligible. Some mutual intelligibility occurs with spoken Rusyn, Ukrainian, and even Russian (in this order), although their orthographies are based on the Cyrillic script.


Sports:

Food:

Clothing:

Exclamations:

Nouns:

Verbs:

Greetings:

"Servus" is commonly used as a greeting or upon parting in Slovak-speaking regions and some German-speaking regions, particularly Austria. "Papa" is also commonly used upon parting in these regions. Both "servus" and "papa" are used in colloquial, informal conversation"."

Hungarians and Slovaks have had a language interaction ever since the settlement of Hungarians in the Carpathian area. Hungarians also adopted many words from various Slavic languages related to agriculture and administration, and a number of Hungarian loanwords are found in Slovak. Some examples are as follows:

There are many Slovak dialects, which are divided into the following four basic groups:

The fourth group of dialects is often not considered a separate group, but a subgroup of Central and Western Slovak dialects (see e.g. Štolc, 1968), but it is currently undergoing changes due to contact with surrounding languages (Serbo-Croatian, Romanian, and Hungarian) and long-time geographical separation from Slovakia (see the studies in "Zborník Spolku vojvodinských slovakistov", e.g. Dudok, 1993).

For an external map of the three groups in Slovakia see here.

The dialect groups differ mostly in phonology, vocabulary, and tonal inflection. Syntactic differences are minor. Central Slovak forms the basis of the present-day standard language. Not all dialects are fully mutually intelligible. It may be difficult for an inhabitant of the western Slovakia to understand a dialect from eastern Slovakia and the other way around.

The dialects are fragmented geographically, separated by numerous mountain ranges. The first three groups already existed in the 10th century. All of them are spoken by the Slovaks outside Slovakia (USA, Canada, Croatian Slavonia, and elsewhere), and central and western dialects form the basis of the lowland dialects (see above).

The western dialects contain features common with the Moravian dialects in the Czech Republic, the southern central dialects contain a few features common with South Slavic languages, and the eastern dialects a few features common with Polish and the East Slavonic languages (cf. Štolc, 1994). Lowland dialects share some words and areal features with the languages surrounding them (Serbo-Croatian, Hungarian, and Romanian).





</doc>
<doc id="28142" url="https://en.wikipedia.org/wiki?curid=28142" title="Supercluster">
Supercluster

A supercluster is a large group of smaller galaxy clusters or galaxy groups; it is among the largest-known structures of the cosmos. The Milky Way is part of the Local Group galaxy group (which contains more than 54 galaxies), which in turn is part of the Laniakea Supercluster. This supercluster spans over 500 million light-years, while the Local Group spans over 10 million light-years. The number of superclusters in the observable universe is estimated to be 10 million.

The existence of superclusters indicates that the galaxies in the Universe are not uniformly distributed; most of them are drawn together in groups and clusters, with groups containing up to some dozens of galaxies and clusters up to several thousand galaxies. Those groups and clusters and additional isolated galaxies in turn form even larger structures called superclusters.

Their existence was first postulated by George Abell in his 1958 Abell catalogue of galaxy clusters. He called them "second-order clusters", or clusters of clusters.

Superclusters form massive structures of galaxies, called "filaments", "supercluster complexes", "walls" or "sheets", that may span between several hundred million light-years to 10 billion light-years, covering more than 5% of the observable universe. These are the largest known structures to date. Observations of superclusters can give information about the initial condition of the universe, when these superclusters were created. The directions of the rotational axes of galaxies within superclusters may also give insight and information into the early formation process of galaxies in the history of the Universe.
Interspersed among superclusters are large voids of space where few galaxies exist. Superclusters are frequently subdivided into groups of clusters called galaxy groups and clusters.



</doc>
<doc id="28143" url="https://en.wikipedia.org/wiki?curid=28143" title="Salicylic acid">
Salicylic acid

Salicylic acid (from Latin "salix", "willow tree") is a lipophilic monohydroxybenzoic acid, a type of phenolic acid, and a beta hydroxy acid (BHA). It has the formula CHO. This colorless crystalline organic acid is widely used in organic synthesis and functions as a plant hormone. It is derived from the metabolism of salicin. In addition to serving as an important active metabolite of aspirin ("acetylsalicylic acid"), which acts in part as a prodrug to salicylic acid, it is probably best known for its use as a key ingredient in topical anti-acne products. The salts and esters of salicylic acid are known as salicylates.
It is on the WHO Model List of Essential Medicines, the most important medications needed in a basic health system.

Salicylic acid as a medication is used most commonly to help remove the outer layer of the skin. As such, it is used to treat warts, psoriasis, acne, ringworm, dandruff, and ichthyosis. 

As with other hydroxy acids, salicylic acid is a key ingredient in many skincare products for the treatment of seborrhoeic dermatitis, acne, psoriasis, calluses, corns, keratosis pilaris, acanthosis nigricans, ichthyosis and warts.

Salicylic acid is used in the production of other pharmaceuticals, including 4-aminosalicylic acid, sandulpiride, and landetimide (via Salethamide).

Salicylic acid was one of the original starting materials for making acetylsalicylic acid (aspirin) in 1897.

Bismuth subsalicylate, a salt of bismuth and salicylic acid, is the active ingredient in stomach relief aids such as Pepto-Bismol, is the main ingredient of Kaopectate and "displays anti-inflammatory action (due to salicylic acid) and also acts as an antacid and mild antibiotic".

Other derivatives include methyl salicylate used as a liniment to soothe joint and muscle pain and choline salicylate used topically to relieve the pain of mouth ulcers.

Salicylic acid has the formula CH(OH)COOH, where the OH group is "ortho" to the carboxyl group. It is also known as 2-hydroxybenzoic acid. It is poorly soluble in water (2 g/L at 20 °C). Aspirin (acetylsalicylic acid or ASA) can be prepared by the esterification of the phenolic hydroxyl group of salicylic acid with the acetyl group from acetic anhydride or acetyl chloride. Salicylic acid can also be prepared using the Kolbe-Schmitt reaction.

Salicylic acid is used as a food preservative, a bactericidal and an antiseptic.

Sodium salicylate is a useful phosphor in the vacuum ultraviolet spectral range, with nearly flat quantum efficiency for wavelengths between 10 and 100 nm. It fluoresces in the blue at 420 nm. It is easily prepared on a clean surface by spraying a saturated solution of the salt in methanol followed by evaporation.

As a topical agent and as a beta-hydroxy acid (and unlike alpha-hydroxy acids), salicylic acid is capable of penetrating and breaking down fats and lipids, causing moderate chemical burns of the skin at very high concentrations. It may damage the lining of pores if the solvent is alcohol, acetone or an oil. Over-the-counter limits are set at 2% for topical preparations expected to be left on the face and 3% for those expected to be washed off, such as acne cleansers or shampoo. 

17% and 27% salicylic acid, which is sold for wart removal, should not be applied to the face and should not be used for acne treatment. Even for wart removal, such a solution should be applied once or twice a day – more frequent use may lead to an increase in side-effects without an increase in efficacy.

Some people are hypersensitive to salicylic acid and related compounds.

Salicylic acid is biosynthesized from the amino acid phenylalanine. In "Arabidopsis thaliana" it can be synthesized via a phenylalanine-independent pathway.

Sodium salicylate is commercially prepared by treating sodium phenolate (the sodium salt of phenol) with carbon dioxide at high pressure (100 atm) and high temperature (390 K) – a method known as the Kolbe-Schmitt reaction. Acidification of the product with sulfuric acid gives salicylic acid:

It can also be prepared by the hydrolysis of aspirin (acetylsalicylic acid) or methyl salicylate (oil of wintergreen) with a strong acid or base.

Hippocrates, Galen, Pliny the Elder and others knew that willow bark could ease pain and reduce fevers. It was used in Europe and China to treat these conditions. This remedy is mentioned in texts from ancient Egypt, Sumer and Assyria. The Cherokee and other Native Americans used an infusion of the bark for fever and other medicinal purposes.

In 2014, archaeologists identified traces of salicylic acid on 7th century pottery fragments found in east central Colorado. The Reverend Edward Stone, a vicar from Chipping Norton, Oxfordshire, England, noted in 1763 that the bark of the willow was effective in reducing a fever.

The active extract of the bark, called "salicin", after the Latin name for the white willow ("Salix alba"), was isolated and named by the German chemist Johann Andreas Buchner in 1828. A larger amount of the substance was isolated in 1829 by Henri Leroux, a French pharmacist. Raffaele Piria, an Italian chemist, was able to convert the substance into a sugar and a second component, which on oxidation becomes salicylic acid.

Salicylic acid was also isolated from the herb meadowsweet ("Filipendula ulmaria", formerly classified as "Spiraea ulmaria") by German researchers in 1839. While their extract was somewhat effective, it also caused digestive problems such as gastric irritation, bleeding, diarrhea and even death when consumed in high doses.

Salicylic acid occurs in plants as free salicylic acid and its carboxylated esters and phenolic glycosides. Several studies suggest that humans metabolize salicylic acid in measurable quantities from these plants; one study found that vegetarians not taking aspirin had urinary levels of salicylic acid higher than the non-vegetarians. Dietary sources of salicylic acid and their interaction with drugs such as aspirin have not been well studied. Ongoing clinical studies of people with aspirin-induced asthma have shown some benefits of a diet low in salicylic acid.

Studies on the salicylic content of foods are sparse and have produced distinctly different results, giving rise to controversy. Possible causes for the discrepancies include uncontrolled and often unreported factors such as cultivation area, crop species, and harvest methods, as well as inherent issues with the studies such as poor extraction and measurement procedures and limited or low-accuracy equipment. A recent study using best practice measurement methodology significantly reduced intra-sample measurement variability but has not yet been replicated or extended.

Some results have been consistently reported. Meat, poultry, fish, eggs, oils, dairy products, sugar, cereals, and flour all have little to no salicylates. Measurable levels of salicylic acid have been found in fruits, vegetables, herbs, spices, nuts, and teas.

Salicylic acid is a phenolic phytohormone and is found in plants with roles in plant growth and development, photosynthesis, transpiration, ion uptake and transport. SA is involved in endogenous signaling, mediating in plant defense against pathogens. It plays a role in the resistance to pathogens by inducing the production of pathogenesis-related proteins. It is involved in the systemic acquired resistance in which a pathogenic attack on one part of the plant induces resistance in other parts. The signal can also move to nearby plants by salicylic acid being converted to the volatile ester methyl salicylate.




</doc>
<doc id="28144" url="https://en.wikipedia.org/wiki?curid=28144" title="Seaborgium">
Seaborgium

Seaborgium is a synthetic chemical element with symbol Sg and atomic number 106. It is named after the American nuclear chemist Glenn T. Seaborg. As a synthetic element, it can be created in a laboratory but is not found in nature. It is also radioactive; the most stable known isotope, Sg, has a half-life of approximately 3.1 minutes.

In the periodic table of the elements, it is a d-block transactinide element. It is a member of the 7th period and belongs to the group 6 elements as the fourth member of the 6d series of transition metals. Chemistry experiments have confirmed that seaborgium behaves as the heavier homologue to tungsten in group 6. The chemical properties of seaborgium are characterized only partly, but they compare well with the chemistry of the other group 6 elements.

In 1974, a few atoms of seaborgium were produced in laboratories in the Soviet Union and in the United States. The priority of the discovery and therefore the naming of the element was disputed between Soviet and American scientists, and it was not until 1997 that International Union of Pure and Applied Chemistry (IUPAC) established seaborgium as the official name for the element. It is one of only two elements named after a living person at the time of naming, the other being oganesson, element 118.

Two groups claimed discovery of the element. Evidence of element 106 was first reported in 1974 by a Russian research team in Dubna led by Yuri Oganessian, in which targets of lead-208 and lead-207 were bombarded with accelerated ions of chromium-54. In total, fifty-one spontaneous fission events were observed with a half-life between four and ten milliseconds. After having ruled out nucleon transfer reactions as a cause for these activities, the team concluded that the most likely cause of the activities was the spontaneous fission of isotopes of element 106. The isotope in question was first suggested to be seaborgium-259, but was later corrected to seaborgium-260.

A few months later in 1974, researchers including Glenn T. Seaborg and Albert Ghiorso at the University of California, Berkeley, and E. Kenneth Hulet from the Lawrence Livermore National Laboratory, also synthesized the element by bombarding a californium-249 target with oxygen-18 ions, using equipment similar to that which had been used for the synthesis of element 104 five years earlier, observing at least seventy alpha decays, seemingly from the isotope seaborgium-263m with a half-life of seconds. The alpha daughter rutherfordium-259 and granddaughter nobelium-255 had previously been synthesised and the properties observed here matched with those previously known, as did the intensity of their production. The cross-section of the reaction observed, 0.3 nanobarns, also agreed well with theoretical predictions. These bolstered the assignment of the alpha decay events to seaborgium-263m.

A dispute thus arose from the initial competing claims of discovery, though unlike the case of the synthetic elements up to element 105, neither team of discoverers chose to announce proposed names for the new elements, thus averting an element naming controversy temporarily. The dispute on discovery, however, dragged on until 1992, when the IUPAC/IUPAP Transfermium Working Group (TWG), formed to put an end to the controversy by making conclusions regarding discovery claims for elements 101 to 112, concluded that the Soviet synthesis of seaborgium-260 was not convincing enough, "lacking as it is in yield curves and angular selection results", whereas the American synthesis of seaborgium-263 was convincing due to its being firmly anchored to known daughter nuclei. As such, the TWG recognised the Berkeley team as official discoverers in their 1993 report.

Seaborg had previously suggested to the TWG that if Berkeley was recognised as the official discoverer of elements 104 and 105, they might propose the name "kurchatovium" (symbol Kt) for element 106 to honour the Dubna team, which had proposed this name for element 104 after Igor Kurchatov, the former head of the Soviet nuclear research programme. However, due to the worsening relations between the competing teams after the publication of the TWG report (because the Berkeley team vehemently disagreed with the TWG's conclusions, especially regarding element 104), this proposal was dropped from consideration by the Berkeley team. After being recognized as official discoverers, the Berkeley team started deciding on a name in earnest:

Seaborg's son Eric remembered the naming process as follows:

The name "seaborgium" and symbol "Sg" were announced at the 207th national meeting of the American Chemical Society in March 1994 by Kenneth Hulet, one of the co-discovers. However, IUPAC resolved in August 1994 that an element could not be named after a living person, and Seaborg was still alive at the time. Thus, in September 1994, IUPAC recommended a set of names in which the names proposed by the three laboratories (the third being the GSI Helmholtz Centre for Heavy Ion Research in Darmstadt, Germany) with competing claims to the discovery for elements 104 to 109 were shifted to various other elements, in which "rutherfordium" (Rf), the Berkeley proposal for element 104, was shifted to element 106, with "seaborgium" being dropped entirely as a name.

This decision ignited a firestorm of worldwide protest for disregarding the historic discoverer's right to name new elements, and against the new retroactive rule against naming elements after living persons; the American Chemical Society stood firmly behind the name "seaborgium" for element 106, together with all the other American and German naming proposals for elements 104 to 109, approving these names for its journals in defiance of IUPAC. At first, IUPAC defended itself, with an American member of its committee writing: "Discoverers don't have a right to name an element. They have a right to suggest a name. And, of course, we didn't infringe on that at all." However, Seaborg responded:

Bowing to public pressure, IUPAC proposed a different compromise in August 1995, in which the name "seaborgium" was reinstated for element 106 in exchange for the removal of all but one of the other American proposals, which met an even worse response. Finally, IUPAC rescinded these previous compromises and made a final, new recommendation in August 1997, in which the American and German proposals for elements 104 to 109 were all adopted, including "seaborgium" for element 106, with the single exception of element 105, named "dubnium" to recognise the contributions of the Dubna team to the experimental procedures of transactinide synthesis. This list was finally accepted by the American Chemical Society, which wrote:

Seaborg commented regarding the naming:

Seaborg died a year and a half later, on 25 February 1999, at the age of 86.

Super-heavy elements such as seaborgium are produced by bombarding lighter elements in particle accelerators that induces fusion reactions. Whereas most of the isotopes of seaborgium can be synthesized directly this way, some heavier ones have only been observed as decay products of elements with higher atomic numbers.

Depending on the energies involved, fusion reactions that generate superheavy elements are separated into "hot" and "cold". In hot fusion reactions, very light, high-energy projectiles are accelerated toward very heavy targets (actinides), giving rise to compound nuclei at high excitation energy (~40–50 MeV) that may either fission or evaporate several (3 to 5) neutrons. In cold fusion reactions, the produced fused nuclei have a relatively low excitation energy (~10–20 MeV), which decreases the probability that these products will undergo fission reactions. As the fused nuclei cool to the ground state, they require emission of only one or two neutrons, and thus, allows for the generation of more neutron-rich products. The latter is a distinct concept from that of where nuclear fusion claimed to be achieved at room temperature conditions (see cold fusion).

Seaborgium has no stable or naturally occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Twelve different isotopes of seaborgium have been reported with atomic masses 258–267, 269, and 271, three of which, seaborgium-261, 263, and 265, have known metastable states. All of these decay only through alpha decay and spontaneous fission, with the single exception of seaborgium-261 that can also undergo electron capture to dubnium-261.

There is a trend toward increasing half-lives for the heavier isotopes; thus the heaviest three known isotopes, Sg, Sg, and Sg, are also the longest-lived, having half-lives in minutes. Some other isotopes in this region are predicted to have comparable or even longer half-lives, with the longest-lived predicted isotope being Sg which is expected to have a half-life of about an hour. Additionally, Sg, Sg, Sg, as well as the predicted Sg have or should have half-lives measured in seconds. All the remaining isotopes have half-lives measured in milliseconds, with the exception of the shortest-lived isotope, Sg, with a half-life of only 92 microseconds.

The proton-rich isotopes from Sg to Sg were directly produced by cold fusion; all heavier isotopes were produced from the repeated alpha decay of the heavier elements hassium, darmstadtium, and flerovium, with the exceptions of the isotopes Sg, Sg, Sg, and Sg, which were directly produced by hot fusion through irradiation of actinide targets. The twelve isotopes of seaborgium have half-lives ranging from 92 microseconds for Sg to 3.1 minutes for Sg.

Seaborgium is expected to be a solid under normal conditions and assume a body-centered cubic crystal structure, similar to its lighter congener tungsten. It should be a very heavy metal with a density of around 35.0 g/cm, which would be the fourth-highest of any of the 118 known elements, lower only than bohrium (37.1 g/cm), meitnerium (37.4 g/cm) and hassium (41 g/cm), the three following elements in the periodic table. In comparison, the densest known element that has had its density measured, osmium, has a density of only 22.61 g/cm. This results from seaborgium's high atomic weight, the lanthanide and actinide contractions, and relativistic effects, although production of enough seaborgium to measure this quantity would be impractical, and the sample would quickly decay.

Seaborgium is the fourth member of the 6d series of transition metals and the heaviest member of group 6 in the periodic table, below chromium, molybdenum, and tungsten. All the members of the group form a diversity of oxoanions. They readily portray their group oxidation state of +6, although this is highly oxidising in the case of chromium, and this state becomes more and more stable to reduction as the group is descended: indeed, tungsten is the last of the 5d transition metals where all four 5d electrons participate in metallic bonding. As such, seaborgium should have +6 as its most stable oxidation state, both in the gas phase and in aqueous solution, and this is the only oxidation state that is experimentally known for it; the +5 and +4 states should be less stable and the +3 state, the most common for chromium, would be the least stable for seaborgium. Experimental chemical investigation has been hampered due to the need to produce seaborgium one atom at a time, its short half-life, and the resulting necessary harshness of the experimental conditions. The isotope Sg and its isomer Sg are advantageous for radiochemistry: they are produced in the Cm(Ne,5n) reaction.

This stabilisation of the highest oxidation state occurs in the early 6d elements because of the similarity between the energies of the 6d and 7s orbitals, since the 7s orbitals are relativistically stabilised and the 6d orbitals are relativistically destabilised. This effect is so large in the seventh period that seaborgium is expected to lose its 6d electrons before its 7s electrons (Sg, [Rn]5f6d7s; Sg, [Rn]5f6d7s; Sg, [Rn]5f6d7s; Sg, [Rn]5f6d; Sg, [Rn]5f). Because of the great destabilisation of the 7s orbital, Sg should be even more unstable than W and should be very readily oxidised to Sg. The predicted ionic radius of the hexacoordinate Sg ion is 65 pm, while the predicted atomic radius of seaborgium is 128 pm. Nevertheless, the stability of the highest oxidation state is still expected to decrease as Lr > Rf > Db > Sg. Some predicted standard reduction potentials for seaborgium ions in aqueous acidic solution are as follows:

Seaborgium should form a very volatile hexafluoride (SgF) as well as a moderately volatile hexachloride (SgCl), pentachloride (SgCl), and oxychlorides SgOCl and SgOCl. SgOCl is expected to be the most stable of the seaborgium oxychlorides and to be the least volatile of the group 6 oxychlorides, with the sequence MoOCl > WOCl > SgOCl.

The volatile seaborgium(VI) compounds SgCl and SgOCl are expected to be unstable to decomposition to seaborgium(V) compounds at high temperatures, analogous to MoCl and MoOCl; this should not happen for SgOCl due to the much higher energy gap between the highest occupied and lowest unoccupied molecular orbitals, despite the similar Sg–Cl bond strengths (similarly to molybdenum and tungsten). Thus, in the first experimental chemical studies of seaborgium in 1995 and 1996, seaborgium atoms were produced in the reaction Cm(Ne,4n)Sg, thermalised, and reacted with an O/HCl mixture. The adsorption properties of the resulting oxychloride were measured and compared with those of molybdenum and tungsten compounds. The results indicated that seaborgium formed a volatile oxychloride akin to those of the other group 6 elements, and confirmed the decreasing trend of oxychloride volatility down group 6:

In 2001, a team continued the study of the gas phase chemistry of seaborgium by reacting the element with O in a HO environment. In a manner similar to the formation of the oxychloride, the results of the experiment indicated the formation of seaborgium oxide hydroxide, a reaction well known among the lighter group 6 homologues as well as the pseudohomologue uranium.

Molybdenum and tungsten are very similar to each other and show important differences to the smaller chromium, and seaborgium is expected to follow the chemistry of tungsten and molybdenum quite closely, forming an even greater variety of oxoanions, the simplest among them being seaborgate, , which would form from the rapid hydrolysis of , although this would take place less readily than with molybdenum and tungsten as expected from seaborgium's greater size. Seaborgium should hydrolyse less readily than tungsten in hydrofluoric acid at low concentrations, but more readily at high concentrations, also forming complexes such as SgOF and : complex formation competes with hydrolysis in hydrofluoric acid. These predictions have largely been confirmed. In experiments conducted in 1997 and 1998, seaborgium was eluted from cation-exchange resin using a HNO/HF solution, most likely as neutral SgOF or the anionic complex ion [SgOF] rather than . In contrast, in 0.1 M nitric acid, seaborgium does not elute, unlike molybdenum and tungsten, indicating that the hydrolysis of [Sg(HO)] only proceeds as far as the cationic complex [Sg(OH)(HO)] or [Sg(OH)(HO)], while that of molybdenum and tungsten proceeds to neutral [MO(OH))].

The only other oxidation state known for seaborgium other than the group oxidation state of +6 is the zero oxidation state. Similarly to its three lighter congeners, forming chromium hexacarbonyl, molybdenum hexacarbonyl, and tungsten hexacarbonyl, seaborgium has been shown in 2014 to also form seaborgium hexacarbonyl, Sg(CO). Like its molybdenum and tungsten homologues, seaborgium hexacarbonyl is a volatile compound that reacts readily with silicon dioxide.



</doc>
<doc id="28145" url="https://en.wikipedia.org/wiki?curid=28145" title="September 15">
September 15





</doc>
<doc id="28146" url="https://en.wikipedia.org/wiki?curid=28146" title="September 18">
September 18





</doc>
<doc id="28147" url="https://en.wikipedia.org/wiki?curid=28147" title="September 19">
September 19





</doc>
<doc id="28148" url="https://en.wikipedia.org/wiki?curid=28148" title="September 20">
September 20





</doc>
<doc id="28149" url="https://en.wikipedia.org/wiki?curid=28149" title="Serpens">
Serpens

Serpens ("the Serpent", Greek ) is a constellation of the northern hemisphere. One of the 48 constellations listed by the 2nd-century astronomer Ptolemy, it remains one of the 88 modern constellations defined by the International Astronomical Union. It is unique among the modern constellations in being split into two non-contiguous parts, Serpens Caput (Serpent Head) to the west and Serpens Cauda (Serpent Tail) to the east. Between these two halves lies the constellation of Ophiuchus, the "Serpent-Bearer". In figurative representations, the body of the serpent is represented as passing behind Ophiuchus between Mu Serpentis in "Serpens Caput" and Nu Serpentis in "Serpens Cauda".

The brightest star in Serpens is the red giant star Alpha Serpentis, or Unukalhai, in Serpens Caput, with an apparent magnitude of 2.63. Also located in Serpens Caput are the naked-eye globular cluster Messier 5 and the naked-eye variables R Serpentis and Tau Serpentis. Notable extragalactic objects include Seyfert's Sextet, one of the densest galaxy clusters known; Arp 220, the prototypical ultraluminous infrared galaxy; and Hoag's Object, the most famous of the very rare class of galaxies known as ring galaxies.

Part of the Milky Way's galactic plane passes through Serpens Cauda, which is therefore rich in galactic deep-sky objects, such as the Eagle Nebula (IC 4703) and its associated star cluster Messier 16. The nebula measures 70 light-years by 50 light-years and contains the Pillars of Creation, three dust clouds that became famous for the image taken by the Hubble Space Telescope. Other striking objects include the Red Square Nebula, one of the few objects in astronomy to take on a square shape; and Westerhout 40, a massive nearby star-forming region consisting of a molecular cloud and an H II region.

In Greek mythology, Serpens represents a snake held by the healer Asclepius. Represented in the sky by the constellation Ophiuchus, Asclepius once killed a snake, but the animal was subsequently resurrected after a second snake placed a revival herb on it before its death. As snakes shed their skin every year, they were known as the symbol of rebirth in ancient Greek society, and legend says Asclepius would revive dead humans using the same technique he witnessed. Although this is likely the logic for Serpens' presence with Ophiuchus, the true reason is still not fully known. Sometimes, Serpens was depicted as coiling around Ophiuchus, but the majority of atlases showed Serpens passing either behind Ophiuchus' body or between his legs.

In some ancient atlases, the constellations Serpens and Ophiuchus were depicted as two separate constellations, although more often they were shown as a single constellation. One notable figure to depict Serpens separately was Johann Bayer; thus, Serpens' stars are cataloged with separate Bayer designations from those of Ophiuchus. When Eugène Delporte established modern constellation boundaries in the 1920s, he elected to depict the two separately. However, this posed the problem of how to disentangle the two constellations, with Deporte deciding to split Serpens into two areas—the head and the tail—separated by the continuous Ophiuchus. These two areas became known as Serpens Caput and Serpens Cauda, "caput" being the Latin word for head and "cauda" the Latin word for tail.

In Chinese astronomy, most of the stars of Serpens represented part of a wall surrounding a marketplace, known as Tianshi, which was in Ophiuchus and part of Hercules. Serpens also contains a few Chinese constellations. Two stars in the tail represented part of Shilou, the tower with the market office. Another star in the tail represented Liesi, jewel shops. One star in the head (Mu Serpentis) marked Tianru, the crown prince's wet nurse, or sometimes rain.

There were two "serpent" constellations in Babylonian astronomy, known as Mušḫuššu and Bašmu. It appears that Mušḫuššu was depicted as a hybrid of a dragon, a lion and a bird, and loosely corresponded to Hydra. Bašmu was a horned serpent (c.f. Ningishzida) and roughly corresponds to the Ὄφις constellation of Eudoxus of Cnidus on which the Ὄφις ("Serpens") of Ptolemy is based.

Serpens is the only one of the 88 modern constellations to be split into two disconnected regions in the sky: "Serpens Caput" (the head) and "Serpens Cauda" (the tail). The constellation is also unusual in that it depends on another constellation for context; specifically, it is being held by the Serpent Bearer Ophiuchus.

Serpens Caput is bordered by Libra to the south, Virgo and Boötes to the east, Corona Borealis to the north, and Ophiuchus and Hercules to the west; Serpens Cauda is bordered by Sagittarius to the south, Scutum and Aquila to the east, and Ophiuchus to the north and west. Covering 636.9 square degrees total, it ranks 23rd of the 88 constellations in size. It appears prominently in both the northern and southern skies during the Northern Hemisphere's summer. Its main asterism consists of 11 stars, and 108 stars in total are brighter than magnitude 6.5, the traditional limit for naked-eye visibility.

Serpens Caput's boundaries, as set by Eugène Delporte in 1930, are defined by a 15-sided polygon, while Serpens Cauda's are defined by a 25-sided polygon. In the equatorial coordinate system, the right ascension coordinates of Serpens Caput's borders lie between and , while the declination coordinates are between and . Serpens Cauda's boundaries lie between right ascensions of and and declinations of and . The International Astronomical Union (IAU) adopted the three-letter abbreviation "Ser" for the constellation in 1922.

Marking the heart of the serpent is the constellation's brightest star, Alpha Serpentis. Traditionally called Unukalhai, is a red giant of spectral type K2III located approximately 23 parsecs distant with a visual magnitude of 2.630 ± 0.009, meaning it can easily be seen with the naked eye even in areas with substantial light pollution. A faint companion is in orbit around the red giant star, although it is not visible to the naked eye. Situated near Alpha is Lambda Serpentis, a magnitude 4.42 ± 0.05 star rather similar to the Sun positioned only 12 parsecs away. Another solar analog in Serpens is the primary of Psi Serpentis, a binary star located slightly further away at approximately 14 parsecs.

Beta, Gamma, and Iota Serpentis form a distinctive triangular shape marking the head of the snake, with Kappa Serpentis being roughly midway between Gamma and Iota. The brightest of the four with an apparent magnitude of roughly 3.67, Beta Serpentis is a white main-sequence star roughly 160 parsecs distant. It is likely that a nearby 10th-magnitude star is physically associated with Beta, although it is not certain. The Mira variable R Serpentis, situated between Beta and Gamma, is visible to the naked eye at its maximum of 5th-magnitude, but, typical of Mira variables, it can fade to below magnitude 14. Gamma Serpentis itself is an F-type subgiant located only 11 parsecs distant and thus is quite bright, being of magnitude 3.84 ± 0.05. The star is known to show solar-like oscillations.

Delta Serpentis, forming part of the body of the snake between the heart and the head, is a multiple star system positioned around 70 parsecs from Earth. Consisting of four stars, the system has a total apparent magnitude of 3.79 as viewed from Earth, although two of the stars, with a combined apparent magnitude of 3.80, provide nearly all the light. The primary, a white subgiant, is a Delta Scuti variable with an average apparent magnitude of 4.23. Positioned very near Delta, both in the night sky and likely in actual space at an estimated distance of around 70 parsecs, is the barium star 16 Serpentis. Another notable variable star visible to the naked eye is Chi Serpentis, an Alpha² Canum Venaticorum variable situated midway between Delta and Beta which varies from its median brightness of 5.33 by 0.03 magnitudes over a period of approximately 1.5 days.

The two stars in Serpens Caput that form part of the Snake's body below the heart are Epsilon and Mu Serpentis, both third-magnitude A-type main-sequence stars. Both have a peculiarity: Epsilon is an Am star, while Mu is a binary. Located slightly northwest of Mu is 36 Serpentis, another A-type main-sequence star. This star also has a peculiarity; it is a binary with the primary component being a Lambda Boötis star, meaning that it has solar-like amounts of carbon, nitrogen, and oxygen, while containing very low amounts of iron peak elements. 25 Serpentis, positioned a few degrees northeast of Mu Serpentis, is a spectroscopic binary consisting of a hot B-type giant and an A-type main-sequence star. The primary is a slowly pulsating B star, which causes the system to vary by 0.03 magnitudes.

Serpens Caput contains many RR Lyrae variables, although most are too faint to be seen without professional photography. The brightest is VY Serpentis, only of 10th magnitude. This star's period has been increasing by approximately 1.2 seconds per century. A variable star of a different kind is Tau Serpentis, a cool red giant that pulsates between magnitudes 5.89 and 7.07 in 87 days. This star has been found to display an inverse P Cygni profile, where cold infalling gas on to the star creates redshifted hydrogen absorption lines next to the normal emission lines.

Several stars in Serpens have been found to have planets. The brightest, Omega Serpentis, located between Epsilon and Mu, is an orange giant with a planet of at least 1.7 Jupiter-masses. NN Serpentis, an eclipsing post-common-envelope binary consisting of a white dwarf and a red dwarf, is very likely to have two planets causing variations in the period of the eclipses. Although it does not have a planet, the solar analog HD 137510 has been found to have a brown dwarf companion within the brown-dwarf desert.

PSR B1534+11 is a system consisting of two neutron stars orbiting each other, one of which is a pulsar with a period of 37.9 milliseconds. Situated approximately 1000 parsecs distant, the system was used to test Albert Einstein's theory of general relativity, validating the system's relativistic parameters to within 0.2% of values predicted by the theory. The X-ray emission from the system has been found to be present when the non-pulsar star intersects the equatorial pulsar wind of the pulsar, and the system's orbit has been found to vary slightly.

The brightest star in the tail, Eta Serpentis, is similar to Alpha Serpentis' primary in that it is a red giant of spectral class K. This star, however, is known to exhibit solar-like oscillations over a period of approximately 2.16 hours. The other two stars in Serpens Cauda forming its asterism are Theta and Xi Serpentis. Xi, where the asterism crosses over to Mu Serpentis in the head, is a triple star system located approximately 105 parsecs away. Two of the stars, with a combined apparent magnitude of around 3.5, form a spectroscopic binary with an angular separation of only 2.2 milliarcseconds, and thus cannot be resolved with modern equipment. The primary is a white giant with an excess of strontium. Theta, forming the tip of the tail, is also a multiple system, consisting of two A-type main-sequence stars with a combined apparent magnitude of around 4.1 separated by almost half an arcminute.

Lying near the boundary with Ophiuchus are Zeta, Nu, and Omicron Serpentis. All three are 4th-magnitude main-sequence stars, with Nu and Omicron being of spectral type A and Zeta being of spectral type F. Nu is a single star with a 9th-magnitude visual companion, while Omicron is a Delta Scuti variable with amplitude variations of 0.01 magnitudes. In 1909, the symbiotic nova RT Serpentis appeared near Omicron, although it only reached a maximum magnitude of 10.

The star system 59 Serpentis, also known as d Serpentis, is a triple star system consisting of a spectroscopic binary containing an A-type star and an orange giant and an orange giant secondary. The system shows irregular variations in brightness between magnitudes 5.17 and 5.2. In 1970, the nova FH Serpentis appeared just slightly north of 59 Serpentis, reaching a maximum brightness of 4.5. Also near 59 Serpentis in the Serpens Cloud are several Orion variables. MWC 297 is a Herbig Be star that in 1994 exhibited a large X-ray flare and increased in X-ray luminosity by five times before returning to the quiescent state. The star also appears to possess a circumstellar disk. Another Orion variable in the region is VV Serpentis, a Herbig Ae star that has been found to exhibit Delta Scuti pulsations. VV Serpentis has also, like MWC 297, been found to have a dusty disk surrounding it, and is also a UX Orionis star, meaning that it shows irregular variations in its brightness.

The star HR 6958, also known as MV Serpentis, is an Alpha Canum Venaticorum variable that is faintly visible to the naked eye. The star's metal abundance is ten times higher than the Sun for most metals at the iron peak and up to 1,000 times more for heavier elements. It has also been found to contain excess silicon. Barely visible to the naked eye is HD 172365, a likely post-blue straggler in the open cluster IC 4756 that contains a large excess of lithium. HD 172189, also located in IC 4756, is an Algol variable eclipsing binary with a 5.70 day period. The primary star in the system is also a Delta Scuti variable, undergoing multiple pulsation frequencies, which, combined with the eclipses, causes the system to vary by around a tenth of a magnitude.

As the galactic plane passes through it, Serpens Cauda contains many massive OB stars. Several of these are visible to the naked eye, such as NW Serpentis, an early Be star that has been found to be somewhat variable. The variability is interesting; according to one study, it could be one of the first discovered hybrids between Beta Cephei variables and slowly pulsating B stars. Although not visible to the naked eye, HD 167971 (MY Serpentis) is a Beta Lyrae variable triple system consisting of three very hot O-type stars. A member of the cluster NGC 6604, the two eclipsing stars are both blue giants, with one being of the very early spectral type O7.5III. The remaining star is either a blue giant or supergiant of a late O or early B spectral type. Also an eclipsing binary, the HD 166734 system consists of two O-type blue supergiants in orbit around each other. Less extreme in terms of mass and temperature is HD 161701, a spectroscopic binary consisting of a B-type primary and an Ap secondary, although it is the only known spectroscopic binary to consist of a star with excess of mercury and manganese and an Ap star.

South of the Eagle Nebula on the border with Sagittarius is the eclipsing binary W Serpentis, whose primary is a white giant that is interacting with the secondary. The system has been found to contain an accretion disk, and was one of the first discovered Serpentids, which are eclipsing binaries containing exceptionally strong far-ultraviolet spectral lines. It is suspected that such Serpentids are in an earlier evolutionary phase, and will evolve first into double periodic variables and then classical Algol variables. Also near the Eagle Nebula is the eclipsing Wolf–Rayet binary CV Serpentis, consisting of a Wolf–Rayet star and a hot O-type subgiant. The system is surrounded by a ring-shaped nebula, likely formed during the Wolf–Rayet phase of the primary. The eclipses of the system vary erratically, and although there are two theories as to why, neither of them is completely consistent with current understanding of stars.

Serpens Cauda contains a few X-ray binaries. One of these, GX 17+2, is a low-mass X-ray binary consisting of a neutron star and, as in all low-mass X-ray binaries, a low-mass star. The system has been classified as a Sco-like Z source, meaning that its accretion is near the Eddington limit. The system has also been found to approximately every 3 days brighten by around 3.5 K-band magnitudes, possibly due to the presence of a synchrotron jet. Another low-mass X-ray binary, Serpens X-1, undergoes occasional X-ray bursts. One in particular lasted nearly four hours, possibly explained by the burning of carbon in "a heavy element ocean".

As the galactic plane does not pass through this part of Serpens, a view to many galaxies beyond it is possible. However, a few structures of the Milky Way Galaxy are present in Serpens Caput, such as Messier 5, a globular cluster positioned approximately 8° southwest of α Serpentis, next to the star 5 Serpentis. Barely visible to the naked eye under good conditions, and is located approximately 25,000 ly distant. Messier 5 contains a large number of known RR Lyrae variable stars, and is receding from us at over 50 km/s. The cluster contains two millisecond pulsars, one of which is in a binary, allowing the proper motion of the cluster to be measured. The binary could help our understanding of neutron degenerate matter; the current median mass, if confirmed, would exclude any "soft" equation of state for such matter. The cluster has been used to test for magnetic dipole moments in neutrinos, which could shed light on some hypothetical particles such as the axion. Another globular cluster is Palomar 5, found just south of Messier 5. Many stars are leaving this globular cluster due to the Milky Way's gravity, forming a tidal tail over 30000 light-years long.

The L134/L183 is a dark nebula complex that, along with a third cloud, is likely formed by fragments of a single original cloud located 36 degrees away from the galactic plane, a large distance for dark nebulae. The entire complex is thought to be around 140 parsecs distant. L183, also referred to as L134N, is home to several infrared sources, indicating pre-stellar sources thought to present the first known observation of the contraction phase between cloud cores and prestellar cores. The core is split into three regions, with a combined mass of around 25 solar masses.

Outside of the Milky Way, there are no bright deep-sky objects for amateur astronomers in Serpens Caput, with nothing else above 10th magnitude. The brightest is NGC 5962, a spiral galaxy positioned around 28 megaparsecs distant with an apparent magnitude of 11.34. Slightly fainter is NGC 5921, a barred spiral galaxy with a LINER-type active galactic nucleus situated somewhat closer at a distance of 21 megaparsecs. A type II supernova was observed in this galaxy in 2001 and was designated SN 2001X. Fainter still are the spirals NGC 5964 and NGC 6118, with the latter being host to the supernova SN 2004dk.
Hoag's Object, located 600 million light-years from Earth, is a member of the very rare class of galaxies known as ring galaxies. The outer ring is largely composed of young blue stars while the core is made up of older yellow stars. The predominant theory regarding its formation is that the progenitor galaxy was a barred spiral galaxy whose arms had velocities too great to keep the galaxy's coherence and therefore detached. Arp 220 is another unusual galaxy in Serpens. The prototypical ultraluminous infrared galaxy, Arp 220 is somewhat closer than Hoag's Object at 250 million light-years from Earth. It consists of two large spiral galaxies in the process of colliding with their nuclei orbiting at a distance of 1,200 light-years, causing extensive star formation throughout both components. It possesses a large cluster of more than a billion stars, partially covered by thick dust clouds near one of the galaxies' core. Another interacting galaxy pair, albeit in an earlier stage, consists of the galaxies NGC 5953 and NGC 5954. In this case, both are active galaxies, with the former a Seyfert 2 galaxy and the latter a LINER-type galaxy. Both are undergoing a burst of star formation triggered by the interaction.

Seyfert's Sextet is a group of six galaxies, four of which are interacting gravitationally and two of which simply appear to be a part of the group despite their greater distance. The gravitationally bound cluster lies at a distance of 190 million light-years from Earth and is approximately 100,000 light-years across, making Seyfert's Sextet one of the densest galaxy group known. Astronomers predict that the four interacting galaxies will eventually merge to form a large elliptical galaxy. The radio source 3C 326 was originally though to emanate from a giant elliptical galaxy. However, in 1990, it was shown that the source is instead a brighter, smaller galaxy a few arcseconds north. This object, designated 3C 326 N, has enough gas for star formation, but is being inhibited due to the energy from the radio galaxy nucleus.

A much larger galaxy cluster is the redshift-0.0354 Abell 2063. The cluster is thought to be interacting with the nearby galaxy group MKW 3s, based on radial velocity measurements of galaxies and the positioning of the cD galaxy at the center of Abell 2063. The active galaxy at the center of MKW 3s—NGC 5920—appears to be creating a bubble of hot gas from its radio activity. Near the 5th-magnitude star Pi Serpentis lies AWM 4, a cluster containing an excess of metals in the intracluster medium. The central galaxy, NGC 6051, is a radio galaxy that is probably responsible for this enrichment. Similar to AWM 4, the cluster Abell 2052 has central cD radio galaxy, 3C 317. This radio galaxy is believed to have restarted after a period of inactivity less than 200 years ago. The galaxy has over 40,000 known globular clusters, the highest known total of any galaxy as of 2002.
Consisting of two quasars with a separation of less than 5 arcseconds, the quasar pair 4C 11.50 is one of the visually closest pairs of quasars in the sky. The two have markedly different redshifts, however, and are thus unrelated. The foreground member of the pair (4C 11.50 A) does not have enough mass to refract light from the background component (4C 11.50 B) enough to produce a lensed image, although it does have a true companion of its own. An even stranger galaxy pair is 3C 321. Unlike the previous pair, the two galaxies making up 3C 321 are interacting with each other and are in the process of merging. Both members appear to be active galaxies; the primary radio galaxy may be responsible for the activity in the secondary by means of the former's jet driving material onto the latter's supermassive black hole.

An example of gravitational lensing is found in the radio galaxy 3C 324. First thought to be a single overluminous radio galaxy with a redshift of "z" = 1.206, it was found in 1987 to actually be two galaxies, with the radio galaxy at the aforementioned redshift being lensed by another galaxy at redshift "z" = 0.845. The first example of a multiply-imaged radio galaxy discovered, the source appears to be an elliptical galaxy with a dust lane obscuring our view of the visual and ultraviolet emission from the nucleus. In even shorter wavelengths, the BL Lac object PG 1553+113 is a heavy emitter of gamma rays. This object is the most distant found to emit photons with energies in the TeV range as of 2007. The spectrum is unique, with hard emission in some ranges of the gamma-ray spectrum in stark contrast to soft emission in others. In 2012, the object flared in the gamma-ray spectrum, tripling in luminosity for two nights, allowing the redshift to be accurately measured as "z" = 0.49.

Several gamma-ray bursts (GRBs) have been observed in Serpens Caput, such as GRB 970111, one of the brightest GRBs observed. An optical transient event associated with this GRB has not been found, despite its intensity. The host galaxy initially also proved elusive, however it now appears that the host is a Seyfert I galaxy located at redshift "z" = 0.657. The X-ray afterglow of the GRB has also been much fainter than for other dimmer GRBs. More distant is GRB 060526 (redshift "z" = 3.221), from which X-ray and optical afterglows were detected. This GRB was very faint for a long-duration GRB.

Part of the galactic plane passes through the tail, and thus Serpens Cauda is rich in deep-sky objects within our own galaxy. The Eagle Nebula and its associated star cluster, Messier 16 lie 7,000 light-years from Earth in the direction of the galactic center. The nebula measures 70 light-years by 50 light-years and contains the Pillars of Creation, three dust clouds that became famous for the image taken by the Hubble Space Telescope. The stars being born in the Eagle Nebula, added to those with an approximate age of 5 million years have an average temperature of 45,000 kelvins and produce prodigious amounts of radiation that will eventually destroy the dust pillars. Despite its fame, the Eagle Nebula is fairly dim, with an integrated magnitude of approximately 6.0. The star-forming regions in the nebula are often evaporating gaseous globules; unlike Bok globules they only hold one protostar.

North of Messier 16, at a distance of approximately 2000 parsecs, is the OB association Serpens OB2, containing over 100 OB stars. Around 5 million years old, the association appears to still contain star-forming regions, and the light from its stars is illuminating the HII region S 54. Within this HII region is the open cluster NGC 6604, which is the same age as the surrounding OB association, and the cluster is now thought to simply be the densest part of it. The cluster appears to be producing a thermal chimney of ionized gas, caused by the interaction of the gas from the galactic disk with the galactic halo.

Another open cluster in Serpens Cauda is IC 4756, containing at least one naked-eye star, HD 172365 (another naked-eye star in the vicinity, HD 171586, is most likely unrelated). Positioned approximately 440 parsecs distant, the cluster is estimated to be around 800 million years old, quite old for an open cluster. Despite the presence of the Milky Way in Serpens Cauda, one globular cluster can be found: NGC 6535, although invisible to the naked eye, can be made out in small telescopes just north of Zeta Serpentis. Rather small and sparse for a globular cluster, this cluster contains no known RR Lyrae variables, which is unusual for a globular cluster.

MWC 922 is a star surrounded by a planetary nebula. Dubbed the Red Square Nebula due to its similarities to the Red Rectangle Nebula, the planetary nebula appears to be a nearly perfect square with a dark band around the equatorial regions. The nebula contains concentric rings, which are similar to those seen in the supernova SN 1987A. MWC 922 itself is an FS Canis Majoris variable, meaning that it is a Be star containing exceptionally bright hydrogen emission lines as well as select forbidden lines, likely due to the presence of a close binary. East of Xi Serpentis is another planetary nebula, Abell 41, containing the binary star MT Serpentis at its center. The nebula appears to have a bipolar structure, and the axis of symmetry of the nebula has been found to be within 5° of the line perpendicular to the orbital plane of the stars, strengthening the link between binary stars and bipolar planetary nebulae. On the other end of the stellar age spectrum is L483, a dark nebula which contains the protostar IRAS 18418-0440. Although classified as a class 0 protostar, it has some unusual features for such an object, such as a lack of high-velocity stellar winds, and it has been proposed that this object is in transition between class 0 and class I. A variable nebula exists around the protostar, although it is only visible in infrared light.
The Serpens cloud is a massive star-forming molecular cloud situated in the southern part of Serpens Cauda. Only two million years old and 420 parecs distant, the cloud is known to contain many protostars such as Serpens FIRS 1 and Serpens SVS 20. The Serpens South protocluster was uncovered by NASA's Spitzer Space Telescope in the southern portion of the cloud, and it appears that star formation is still continuing in the region. Another site of star formation is the Westerhout 40 complex, consisting of a prominent HII region adjacent to a molecular cloud. Located around 500 parsecs distant, it is one of the nearest massive regions of star formation, but as the molecular cloud obscures the HII region, rendering it and its embedded cluster tough to see visibly, it is not as well-studied as others. The embedded cluster likely contains over 600 stars above 0.1 solar masses, with several massive stars, including at least one O-type star, being responsible for lighting the HII region and the production of a bubble.

Despite the presence of the Milky Way, several active galaxies are visible in Serpens Cauda as well, such as PDS 456, found near Xi Serpentis. The most intrinsically luminous nearby active galaxy, this AGN has been found to be extremely variable in the X-ray spectrum. This has allowed light to be shed on the nature of the supermassive black hole at the center, likely a Kerr black hole. It is possible that the quasar is undergoing a transition from an ultraluminous infrared galaxy to a classical radio-quiet quasar, but there are problems with this theory, and the object appears to be an exceptional object that does not completely lie within current classification systems. Nearby is NRAO 530, a blazar that has been known to flare in the X-rays occasionally. One of these flares was for less than 2000 seconds, making it the shortest flare ever observed in a blazar as of 2004. The blazar also appears to show periodic variability in its radio wave output over two different periods of six and ten years.

There are two daytime meteor showers that radiate from Serpens, the Omega Serpentids and the Sigma Serpentids. Both showers peak between December 18 and December 25.



</doc>
<doc id="28150" url="https://en.wikipedia.org/wiki?curid=28150" title="Sculptor Group">
Sculptor Group

The Sculptor Group is a loose group of galaxies near the south galactic pole. The group is one of the closest groups of galaxies to the Local Group; the distance to the center of the group from the Milky Way is approximately .

The Sculptor Galaxy (NGC 253) and a few other galaxies form a gravitationally-bound core in the center of this group. A few other galaxies at the periphery may be associated with the group but may not be gravitationally bound. Because most of the galaxies in this group are actually weakly gravitationally bound, the group may also be described as a filament. It is considered to be at an early stage of evolution in which galaxies are still falling into the group along filamentary structures.

The table below lists galaxies that have been identified as associated with the Sculptor Galaxy (and hence associated with the group) by I. D. Karachentsev and collaborators.

The object names used in the above table differ from the names used by Karachentsev and collaborators. NGC, IC, UGC, and PGC numbers have been used when possible to allow for easier referencing.

The irregular galaxy NGC 55, the spiral galaxy NGC 300, and their companion galaxies have been considered by many researchers to be part of this group. However, recent distance measurements to these and other galaxies in the same region of the sky show that NGC 55, NGC 300, and their companions may simply be foreground galaxies that are physically unassociated with the Sculptor Group.


</doc>
<doc id="28151" url="https://en.wikipedia.org/wiki?curid=28151" title="State (polity)">
State (polity)

A state is a compulsory political organization with a centralized government that maintains a monopoly of the legitimate use of force within a certain geographical territory.

Many human societies have been governed by states for millennia, however for most of pre-history people lived in stateless societies. The first states arose about 5,500 years ago in conjunction with rapid growth of cities, invention of writing, and codification of new forms of religion. Over time, a variety of different forms developed, employing a variety of justifications for their existence (such as divine right, the theory of the social contract, etc.). Today, however, the modern nation-state is the predominant form of state to which people are subject.

Some states are sovereign. Some states are subject to external sovereignty or hegemony where ultimate sovereignty lies in another state. The term state is also applied to federated states that are members of a federal union, which is the sovereign state.

Speakers of American English often use the terms "state" and "government" as , with both words referring to an organized political group that exercises authority over a particular territory.

The word "state" and its cognates in some other European languages ("stato" in Italian, "estado" in Spanish and Portuguese, "état" in French, "Staat" in German) ultimately derive from the Latin word "status", meaning "condition, circumstances".

The English noun "state" in the generic sense "condition, circumstances" predates the political sense. It is introduced to Middle English c. 1200 both from Old French and directly from Latin.

With the revival of the Roman law in 14th-century Europe, the term came to refer to the legal standing of persons (such as the various "estates of the realm" – noble, common, and clerical), and in particular the special status of the king. The highest estates, generally those with the most wealth and social rank, were those that held power.
The word also had associations with Roman ideas (dating back to Cicero) about the ""status rei publicae"", the "condition of public matters". In time, the word lost its reference to particular social groups and became associated with the legal order of the entire society and the apparatus of its enforcement.

The early 16th-century works of Machiavelli (especially "The Prince") played a central role in popularizing the use of the word "state" in something similar to its modern sense.
The contrasting of church and state still dates to the 16th century.
The North American colonies were called "states" as early as the 1630s.
The expression "L'Etat, c'est moi" ("I am the State") attributed to Louis XIV of France is probably apocryphal, recorded in the late 18th century.

There is no academic consensus on the most appropriate definition of the state. The term "state" refers to a set of different, but interrelated and often overlapping, theories about a certain range of political phenomena. The act of defining the term can be seen as part of an ideological conflict, because different definitions lead to different theories of state function, and as a result validate different political strategies. According to Jeffrey and Painter, "if we define the 'essence' of the state in one place or era, we are liable to find that in another time or space something which is also understood to be a state has different 'essential' characteristics".

The most commonly used definition is Max Weber's, which describes the state as a compulsory political organization with a centralized government that maintains a monopoly of the legitimate use of force within a certain territory. General categories of state institutions include administrative bureaucracies, legal systems, and military or religious organizations.

Another commonly accepted definition of the state is the one given at the Montevideo Convention on Rights and Duties of States in 1933. It defined state as a space that possess the following : A permanent population, a defined territory and a government that is capable of maintaining effective control over the corresponding territory and of conducting International relations with other states.

According to the "Oxford English Dictionary", a state is "a. an organized political community under one government; a commonwealth; a nation. b. such a community forming part of a federal republic, esp the United States of America".

Confounding the definition problem is that "state" and "government" are often used as synonyms in common conversation and even some academic discourse. According to this definition schema, the states are nonphysical persons of international law, governments are organizations of people. The relationship between a government and its state is one of representation and authorized agency.

States may be classified by Political philosophers as sovereign if they are not dependent on, or subject to any other power or state. Other states are subject to external sovereignty or hegemony where ultimate sovereignty lies in another state. Many states are federated states which participate in a federal union. A federated state is a territorial and constitutional community forming part of a federation. (Compare confederacies or confederations such as Switzerland.) Such states differ from sovereign states in that they have transferred a portion of their sovereign powers to a federal government.

One can commonly and sometimes readily (but not necessarily usefully) classify states according to their apparent make-up or focus. The concept of the nation-state, theoretically or ideally co-terminous with a "nation", became very popular by the 20th century in Europe, but occurred rarely elsewhere or at other times. In contrast, some states have sought to make a virtue of their multi-ethnic or multi-national character (Hapsburg Austria-Hungary, for example, or the Soviet Union), and have emphasised unifying characteristics such as autocracy, monarchical legitimacy, or ideology. Imperial states have sometimes promoted notions of racial superiority.
Other states may bring ideas of commonality and inclusiveness to the fore: note the "res publica" of ancient Rome and the "Rzeczpospolita" of Poland-Lithuania which finds echoes in the modern-day republic. The concept of temple states centred on religious shrines occurs in some discussions of the ancient world.
Relatively small city-states, once a relatively common and often successful form of polity in the days before folk worried about failed states,
have become rarer and comparatively less prominent in modern times, although a number of them survive as federated states, like the present day German city-states, or as otherwise autonomous entities with limited sovereignty, like Hong Kong, Gibraltar and Ceuta. To some extent, urban secession, the creation of a new city-state (sovereign or federated), continues to be discussed in the early 21st century in cities such as London.

A state can be distinguished from a government. The government is the particular group of people, the administrative bureaucracy that controls the state apparatus at a given time. That is, governments are the means through which state power is employed. States are served by a continuous succession of different governments. States are immaterial and nonphysical social objects, whereas governments are groups of people with certain coercive powers.

Each successive government is composed of a specialized and privileged body of individuals, who monopolize political decision-making, and are separated by status and organization from the population as a whole. Their function is to enforce existing laws, legislate new ones, and arbitrate conflicts. In some societies, this group is a self-perpetuating or hereditary class. In other societies, such as democracies, the political roles remain, but there is frequent turnover of the people actually filling the positions.

States can also be distinguished from the concept of a "nation", where "nation" refers to a cultural-political community of people. A nation-state refers to a situation where a single ethnicity is associated with a specific state.

In the classical thought, the state was identified with both political society and civil society as a form of political community, while the modern thought distinguished the nation state as a political society from civil society as a form of economic society.
Thus in the modern thought the state is contrasted with civil society.

Antonio Gramsci believed that civil society is the primary locus of political activity because it is where all forms of "identity formation, ideological struggle, the activities of intellectuals, and the construction of hegemony take place." and that civil society was the nexus connecting the economic and political sphere. Arising out of the collective actions of civil society is what Gramsci calls "political society", which Gramsci differentiates from the notion of the state as a polity. He stated that politics was not a "one-way process of political management" but, rather, that the activities of civil organizations conditioned the activities of political parties and state institutions, and were conditioned by them in turn. Louis Althusser argued that civil organizations such as church, schools, and the family are part of an "ideological state apparatus" which complements the "repressive state apparatus" (such as police and military) in reproducing social relations.

Jürgen Habermas spoke of a public sphere that was distinct from both the economic and political sphere.

Given the role that many social groups have in the development of public policy, and the extensive connections between state bureaucracies and other institutions, it has become increasingly difficult to identify the boundaries of the state. Privatization, nationalization, and the creation of new regulatory bodies also change the boundaries of the state in relation to society. Often the nature of quasi-autonomous organizations is unclear, generating debate among political scientists on whether they are part of the state or civil society. Some political scientists thus prefer to speak of policy networks and decentralized governance in modern societies rather than of state bureaucracies and direct state control over policy.

Most political theories of the state can roughly be classified into two categories. The first are known as "liberal" or "conservative" theories, which treat capitalism as a given, and then concentrate on the function of states in capitalist society. These theories tend to see the state as a neutral entity separated from society and the economy. Marxist theories on the other hand, see politics as intimately tied in with economic relations, and emphasize the relation between economic power and political power. They see the state as a partisan instrument that primarily serves the interests of the upper class.

Anarchism is a political philosophy which considers the state immoral, unnecessary, and harmful and instead promotes a stateless society, or anarchy.

Anarchists believe that the state is inherently an instrument of domination and repression, no matter who is in control of it. Anarchists note that the state possesses the monopoly on the legal use of violence. Unlike Marxists, anarchists believe that revolutionary seizure of state power should not be a political goal. They believe instead that the state apparatus should be completely dismantled, and an alternative set of social relations created, which are not based on state power at all.

Various Christian anarchists, such as Jacques Ellul, have identified the State and political power as the Beast in the Book of Revelation.

Marx and Engels were clear in that the communist goal was a classless society in which the state would have "withered away". Their views are scattered throughout the Marx/Engels Collected Works and address past or the then extant state forms from an analytical or tactical viewpoint, not future social forms, speculation about which is generally anathema to groups considering themselves Marxist but who, not having conquered the existing state power(s) are not in the situation of supplying the institutional form of an actual society. To the extent that it makes sense, there is no single "Marxist theory of state", but rather many different "Marxist" theories that have been developed by adherents of Marxism.

Marx's early writings portrayed the state as "parasitic", built upon the superstructure of the economy, and working against the public interest. He also wrote that the state mirrors class relations in society in general, acts as a regulator and repressor of class struggle, and acts as a tool of political power and domination for the ruling class. The "Communist Manifesto" claimed that the state is nothing more than "a committee for managing the common affairs of the "bourgeoisie".

For Marxist theorists, the role of the non-socialist state is determined by its function in the global capitalist order. Ralph Miliband argued that the ruling class uses the state as its instrument to dominate society by virtue of the interpersonal ties between state officials and economic elites. For Miliband, the state is dominated by an elite that comes from the same background as the capitalist class. State officials therefore share the same interests as owners of capital and are linked to them through a wide array of social, economic, and political ties.

Gramsci's theories of state emphasized that the state is only one of the institutions in society that helps maintain the hegemony of the ruling class, and that state power is bolstered by the ideological domination of the institutions of civil society, such as churches, schools, and mass media.

Pluralists view society as a collection of individuals and groups, who are competing for political power. They then view the state as a neutral body that simply enacts the will of whichever groups dominate the electoral process. Within the pluralist tradition, Robert Dahl developed the theory of the state as a neutral arena for contending interests or its agencies as simply another set of interest groups. With power competitively arranged in society, state policy is a product of recurrent bargaining. Although pluralism recognizes the existence of inequality, it asserts that all groups have an opportunity to pressure the state. The pluralist approach suggests that the modern democratic state's actions are the result of pressures applied by a variety of organized interests. Dahl called this kind of state a polyarchy.

Pluralism has been challenged on the ground that it is not supported by empirical evidence. Citing surveys showing that the large majority of people in high leadership positions are members of the wealthy upper class, critics of pluralism claim that the state serves the interests of the upper class rather than equitably serving the interests of all social groups.

Jürgen Habermas believed that the base-superstructure framework, used by many Marxist theorists to describe the relation between the state and the economy, was overly simplistic. He felt that the modern state plays a large role in structuring the economy, by regulating economic activity and being a large-scale economic consumer/producer, and through its redistributive welfare state activities. Because of the way these activities structure the economic framework, Habermas felt that the state cannot be looked at as passively responding to economic class interests.

Michel Foucault believed that modern political theory was too state-centric, saying "Maybe, after all, the state is no more than a composite reality and a mythologized abstraction, whose importance is a lot more limited than many of us think." He thought that political theory was focusing too much on abstract institutions, and not enough on the actual practices of government. In Foucault's opinion, the state had no essence. He believed that instead of trying to understand the activities of governments by analyzing the properties of the state (a reified abstraction), political theorists should be examining changes in the practice of government to understand changes in the nature of the state. Foucault argues that it is technology that has created and made the state so elusive and successful instead of looking at the state as something to be toppled we should look at the state as technological manifestation or system with many heads; Foucault argues instead of something to be overthrown as in the sense of the Marxist and Anarchist understanding of the state. Every single scientific technological advance has come to the service of the state Foucault argues and it is with the emergence of the Mathematical sciences and essentially the formation of Mathematical statistics that you get a understanding of the complex technology of producing how the modern state was so successfully created. The nation (Nation state) wasn't an historical accident insist Foucault but a deliberate production in which the modern state had to now manage coincidentally with the emerging practice of the Police (Cameral science) 'allowing' the population to now 'come in' into "jus gentium" and "civitas" (Civil society) after deliberately being excluded for several millennia. Democracy wasn't (the newly formed voting franchise) as is always painted by both political revolutionaries and political philosophers as a cry for political freedom or wanting to be accepted by the 'ruling elite', Foucault insists, but was a part of a skilled endeavour of switching over new technology such as; Translatio imperii, Plenitudo potestatis and "extra" "Ecclesiam nulla salus" readily available from the past Medieval period, into mass persuasion for the future industrial 'political' population(deception over the population) in which the political population was now asked to insist upon itself “the president must be elected”. Where these political symbol agents, represented by the pope and the president are now democratised. Foucault calls these new forms of technology Biopower and form part of our political inheritance which he calls Biopolitics.

Heavily influenced by Gramsci, Nicos Poulantzas, a Greek neo-Marxist theorist argued that capitalist states do not always act on behalf of the ruling class, and when they do, it is not necessarily the case because state officials consciously strive to do so, but because the 'structural' position of the state is configured in such a way to ensure that the long-term interests of capital are always dominant. Poulantzas' main contribution to the Marxist literature on the state was the concept of 'relative autonomy' of the state. While Poulantzas' work on 'state autonomy' has served to sharpen and specify a great deal of Marxist literature on the state, his own framework came under criticism for its 'structural functionalism'.

State autonomy theorists believe that the state is an entity that is impervious to external social and economic influence, and has interests of its own.

"New institutionalist" writings on the state, such as the works of Theda Skocpol, suggest that state actors are to an important degree autonomous. In other words, state personnel have interests of their own, which they can and do pursue independently of (at times in conflict with) actors in society. Since the state controls the means of coercion, and given the dependence of many groups in civil society on the state for achieving any goals they may espouse, state personnel can to some extent impose their own preferences on civil society.

States generally rely on a claim to some form of political legitimacy in order to maintain domination over their subjects.

The rise of the modern day state system was closely related to changes in political thought, especially concerning the changing understanding of legitimate state power and control. Early modern defenders of absolutism (Absolute monarchy), such as Thomas Hobbes and Jean Bodin undermined the doctrine of the divine right of kings by arguing that the power of kings should be justified by reference to the people. Hobbes in particular went further to argue that political power should be justified with reference to the individual(Hobbes wrote in the time of the English Civil war), not just to the people understood collectively. Both Hobbes and Bodin thought they were defending the power of kings, not advocating for democracy, but their arguments about the nature of sovereignty were fiercely resisted by more traditional defenders of the power of kings, such as Sir Robert Filmer in England, who thought that such defenses ultimately opened the way to more democratic claims.

Max Weber identified three main sources of political legitimacy in his works. The first, legitimacy based on traditional grounds is derived from a belief that things should be as they have been in the past, and that those who defend these traditions have a legitimate claim to power. The second, legitimacy based on charismatic leadership is devotion to a leader or group that is viewed as exceptionally heroic or virtuous. The third is rational-legal authority, whereby legitimacy is derived from the belief that a certain group has been placed in power in a legal manner, and that their actions are justifiable according to a specific code of written laws. Weber believed that the modern state is characterized primarily by appeals to rational-legal authority.

The earliest forms of the state emerged whenever it became possible to centralize power in a durable way. Agriculture and writing are almost everywhere associated with this process: agriculture because it allowed for the emergence of a social class of people who did not have to spend most of their time providing for their own subsistence, and writing (or an equivalent of writing, like Inca quipus) because it made possible the centralization of vital information.

The first known states were created in the Fertile Crescent, India, China, Mesoamerica, the Andes, and others, but it is only in relatively modern times that states have almost completely displaced alternative "stateless" forms of political organization of societies all over the planet. Roving bands of hunter-gatherers and even fairly sizable and complex tribal societies based on herding or agriculture have existed without any full-time specialized state organization, and these "stateless" forms of political organization have in fact prevailed for all of the prehistory and much of the history of the human species and civilization.

Initially states emerged over territories built by conquest in which one culture, one set of ideals and one set of laws have been imposed by force or threat over diverse nations by a civilian and military bureaucracy. Currently, that is not always the case and there are multinational states, federated states and autonomous areas within states.

Since the late 19th century, virtually the entirety of the world's inhabitable land has been parcelled up into areas with more or less definite borders claimed by various states. Earlier, quite large land areas had been either unclaimed or uninhabited, or inhabited by nomadic peoples who were not organised as states. However, even within present-day states there are vast areas of wilderness, like the Amazon rainforest, which are uninhabited or inhabited solely or mostly by indigenous people (and some of them remain uncontacted). Also, there are states which do not hold de facto control over all of their claimed territory or where this control is challenged. Currently the international community comprises around 200 sovereign states, the vast majority of which are represented in the United Nations.

For most of human history, people have lived in stateless societies, characterized by a lack of concentrated authority, and the absence of large inequalities in economic and political power.

The anthropologist Tim Ingold writes:
During the Neolithic period, human societies underwent major cultural and economic changes, including the development of agriculture, the formation of sedentary societies and fixed settlements, increasing population densities, and the use of pottery and more complex tools.

Sedentary agriculture led to the development of property rights, domestication of plants and animals, and larger family sizes. It also provided the basis for the centralized state form by producing a large surplus of food, which created a more complex division of labor by enabling people to specialize in tasks other than food production. Early states were characterized by highly stratified societies, with a privileged and wealthy ruling class that was subordinate to a monarch. The ruling classes began to differentiate themselves through forms of architecture and other cultural practices that were different from those of the subordinate laboring classes.

In the past, it was suggested that the centralized state was developed to administer large public works systems (such as irrigation systems) and to regulate complex economies. However, modern archaeological and anthropological evidence does not support this thesis, pointing to the existence of several non-stratified and politically decentralized complex societies.

Mesopotamia is generally considered to be the location of the earliest civilization or complex society, meaning that it contained cities, full-time division of labor, social concentration of wealth into capital, unequal distribution of wealth, ruling classes, community ties based on residency rather than kinship, long distance trade, monumental architecture, standardized forms of art and culture, writing, and mathematics and science. It was the world's first literate civilization, and formed the first sets of written laws.

Although state-forms existed before the rise of the Ancient Greek empire, the Greeks were the first people known to have explicitly formulated a political philosophy of the state, and to have rationally analyzed political institutions. Prior to this, states were described and justified in terms of religious myths.

Several important political innovations of classical antiquity came from the Greek city-states and the Roman Republic. The Greek city-states before the 4th century granted citizenship rights to their free population, and in Athens these rights were combined with a directly democratic form of government that was to have a long afterlife in political thought and history.

During Medieval times in Europe, the state was organized on the principle of feudalism, and the relationship between lord and vassal became central to social organization. Feudalism led to the development of greater social hierarchies.

The formalization of the struggles over taxation between the monarch and other elements of society (especially the nobility and the cities) gave rise to what is now called the Standestaat, or the state of Estates, characterized by parliaments in which key social groups negotiated with the king about legal and economic matters. These estates of the realm sometimes evolved in the direction of fully-fledged parliaments, but sometimes lost out in their struggles with the monarch, leading to greater centralization of lawmaking and military power in his hands. Beginning in the 15th century, this centralizing process gives rise to the absolutist state.

Cultural and national homogenization figured prominently in the rise of the modern state system. Since the absolutist period, states have largely been organized on a national basis. The concept of a national state, however, is not synonymous with nation state. Even in the most ethnically homogeneous societies there is not always a complete correspondence between state and nation, hence the active role often taken by the state to promote nationalism through emphasis on shared symbols and national identity.

Some states are often labeled as weak or failed. In David Samuels's words "...a failed state occurs when sovereignty over claimed territory has collapsed or was never effectively at all". Authors like Samuels and Joel S. Migdal have explored the emergence of weak states, how they are different from Western "strong" states and its consequences to the economic development of developing countries.

Early state formation

To understand the formation of weak states, Samuels compares the formation of European states in the 1600 with the conditions under which more recent states were formed in the twentieth century. In this line of argument, the state allows a population to resolve a collective action problem, in which citizens recognize the authority of the state and this exercise the power of coercion over them. This kind of social organization required a decline in legitimacy of traditional forms of ruling (like religious authorities) and replaced them with an increase in the legitimacy of depersonalized rule; an increase in the central government's sovereignty; and an increase in the organizational complexity of the central government (bureaucracy).

The transition to this modern state was possible in Europe around 1600 thanks to the confluence of factors like the technological developments in warfare, which generated strong incentives to tax and consolidate central structures of governance to respond to external threats. This was complemented by the increasing on the production of food (as a result of productivity improvements), which allowed to sustain a larger population and so increased the complexity and centralization of states. Finally, cultural changes challenged the authority of monarchies and paved the way to the emergence of modern states.

Late state formation

The conditions that enabled the emergence of modern states in Europe were different for other countries that started this process later. As a result, many of these states lack effective capabilities to tax and extract revenue from their citizens, which derives in problems like corruption, tax evasion and low economic growth. Unlike the European case, late state formation occurred in a context of limited international conflict that diminished the incentives to tax and increase military spending. Also, many of these states emerged from colonization in a state of poverty and with institutions designed to extract natural resources, which have made more difficult to form states. European colonization also defined many arbitrary borders that mixed different cultural groups under the same national identities, which has made difficult to build states with legitimacy among all the population, since some states have to compete for it with other forms of political identity.

As a complement of this argument, Migdal gives a historical account on how sudden social changes in the Third World during the Industrial Revolution contributed to the formation of weak states. The expansion of international trade that started around 1850, brought profound changes in Africa, Asia and Latin America that were introduced with the objective of assure the availability of raw materials for the European market. These changes consisted in: i) reforms to landownership laws with the objective of integrate more lands to the international economy, ii) increase in the taxation of peasants and little landowners, as well as collecting of these taxes in cash instead of in kind as was usual up to that moment and iii) the introduction of new and less costly modes of transportation, mainly railroads. As a result, the traditional forms of social control became obsolete, deteriorating the existing institutions and opening the way to the creation of new ones, that not necessarily lead these countries to build strong states. This fragmentation of the social order induced a political logic in which these states were captured to some extent by "strongmen", who were capable to take advantage of the above-mentioned changes and that challenge the sovereignty of the state. As a result, these decentralization of social control impedes to consolidate strong states.





</doc>
<doc id="28152" url="https://en.wikipedia.org/wiki?curid=28152" title="Stevia">
Stevia

Stevia () is a sweetener and sugar substitute extracted from the leaves of the plant species "Stevia rebaudiana".

The active compounds of stevia are steviol glycosides (mainly stevioside and rebaudioside), which have up to 150 times the sweetness of sugar, are heat-stable, pH-stable, and not fermentable. Stevia's taste has a slower onset and longer duration than that of sugar, and some of its extracts may have a bitter or licorice-like aftertaste at high concentrations.

The legal status of stevia as a food additive or dietary supplement varies from country to country. In the United States, high-purity stevia glycoside extracts are generally recognized as safe (GRAS) and allowed as ingredients in food products, but stevia leaf and crude extracts do not have GRAS or Food and Drug Administration (FDA) approval for use in food. The European Union approved stevia additives in 2011, and in Japan, stevia has been widely used as a sweetener for decades.

The plant "Stevia rebaudiana" has been used for more than 1,500 years by the Guaraní peoples of South America, who called it "ka'a he'ê" ("sweet herb"). The leaves have been used traditionally for hundreds of years in both Brazil and Paraguay to sweeten local teas and medicines, and as a "sweet treat". The genus was named for Spanish botanist and physician Petrus Jacobus Stevus (Pedro Jaime Esteve 1500–1556), a professor of botany at the University of Valencia.

In 1899, Swiss botanist Moisés Santiago Bertoni, while conducting research in eastern Paraguay, first described the plant and the sweet taste in detail. Only limited research was conducted on the topic until in 1931 two French chemists isolated the glycosides that give stevia its sweet taste.

During the 1990s, the United States Food and Drug Administration (FDA) received two petitions requesting that stevia be classified as generally recognized as safe (GRAS), but the FDA "disagreed with [the] conclusions [detailed in the petitions]". In 2015, the FDA still regarded stevia as "not an approved food additive", and stated that it "has not been affirmed as GRAS in the United States due to inadequate toxicological information". Stevia remained banned for all uses until the Dietary Supplement Health and Education Act of 1994, after which the FDA revised its stance and permitted stevia to be used as a dietary supplement, although still not as a food additive.

In December 2008, the FDA gave a "no objection" approval for GRAS status to Truvia (developed by Cargill and the Coca-Cola Company) and PureVia (developed by PepsiCo and the Whole Earth Sweetener Company, a subsidiary of Merisant), both of which use rebaudioside A derived from the stevia plant. However, the FDA said that these products are not stevia, but a highly purified product. As of 2017, high-purity stevia glycosides are considered safe and allowable as ingredients in food products sold in the United States.

In June 2016, the U.S. Customs and Border Protection issued an order of detention for stevia products made in China based on information that the products were made using prison labor.

Early studies prompted the European Commission in 1999 to ban stevia's use in food in the European Union pending further research. In 2006, research data compiled in the safety evaluation released by the World Health Organization found no adverse effects.

In the early 1970s, sweeteners such as cyclamate and saccharin were gradually decreased or removed from a variant formulation of Coca-Cola. Consequently, use of stevia as an alternative began in Japan, with the aqueous extract of the leaves yielding purified steviosides developed as sweeteners. The first commercial stevia sweetener in Japan was produced by the Japanese firm Morita Kagaku Kogyo Co., Ltd. in 1971. The Japanese have been using stevia in food products and soft drinks, (including Coca Cola), and for table use. In 2006, Japan consumed more stevia than any other country, with stevia accounting for 40% of the sweetener market.

In the mid-1980s, stevia became popular in U.S. natural foods and health food industries, as a noncaloric natural sweetener for teas and weight-loss blends. The makers of the synthetic sweetener NutraSweet (at the time Monsanto) asked the FDA to require testing of the herb. In 2007, the Coca-Cola Company announced plans to obtain approval for its stevia-derived sweetener, Rebiana, for use as a food additive within the United States by 2009, as well as plans to market Rebiana-sweetened products in 12 countries that allow stevia's use as a food additive. In May 2008, Coca Cola and Cargill announced the availability of Truvia, a consumer brand stevia sweetener containing erythritol and Rebiana, which the FDA permitted as a food additive in December 2008. Coca-Cola announced intentions to release stevia-sweetened beverages in late December 2008. From 2013 onwards, Coca-Cola Life, containing stevia as a sweetener, was launched in various countries around the world.

Shortly afterward, PepsiCo and Pure Circle announced PureVia, their brand of stevia-based sweetener, but withheld release of beverages sweetened with rebaudioside A until receipt of FDA confirmation. Since the FDA permitted Truvia and PureVia, both Coca Cola and PepsiCo have introduced products that contain their new sweeteners.

As of 2006, China was the world's largest exporter of stevioside products.

Rebaudioside A has the least bitterness of all the steviol glycosides in the "Stevia rebaudiana" plant. To produce rebaudioside A commercially, stevia plants are dried and subjected to a water extraction process. This crude extract contains about 50% rebaudioside A. The various glycosides are separated and purified via crystallization techniques, typically using ethanol or methanol as solvent.

Stevia extracts and derivatives are produced industrially and marketed under different trade names.


Glycosides are molecules that contain glucose residues bound to other non-sugar substances called aglycones (molecules with other sugars are polysaccharides). Preliminary experiments deduce that the tongue's taste receptors react to the glycosides and transduce the sweet taste sensation and the lingering bitter aftertaste by direct activation of sweet and bitter receptors.

According to basic research, steviol glycosides and steviol interact with a protein channel called TRPM5, potentiating the signal from the sweet or bitter receptors, amplifying the taste of other sweet, bitter and umami tastants. The synergetic effect of the glycosides on the sweet receptor and TRPM5 explains the sweetness sensation. Some steviol glycosides (rebaudioside A) are perceived sweeter than others (stevioside).

Steviol cannot be further digested in the digestive tract and is taken up into the bloodstream, metabolised by the liver to steviol glucuronide, and excreted in the urine.

A 2011 review found that the use of stevia sweeteners as replacements for sugar might benefit people with diabetes, children, and those wishing to lower their intake of calories.

Although both steviol and rebaudioside A have been found to be mutagenic in laboratory "in vitro" testing, these effects have not been demonstrated for the doses and routes of administration to which humans are exposed. Two 2010 review studies found no health concerns with stevia or its sweetening extracts.

The WHO's Joint Experts Committee on Food Additives has approved, based on long-term studies, an acceptable daily intake of steviol glycoside of up to 4 mg/kg of body weight. In 2010, The European Food Safety Authority established an acceptable daily intake of 4 mg/kg/day of steviol, in the form of steviol glycosides. Meanwhile, the Memorial Sloan Kettering Cancer Center warns that "steviol at high dosages may have weak mutagenic activity," and a review "conducted for" the Center for Science in the Public Interest notes that there are no published carcinogenicity results for rebaudioside A (or stevioside).

The plant may be grown legally in most countries, although some countries restrict its use as a sweetener. The legally allowed uses and maximum dosage of the extracts and derived products vary widely from country to country.





</doc>
<doc id="28153" url="https://en.wikipedia.org/wiki?curid=28153" title="Search for extraterrestrial intelligence">
Search for extraterrestrial intelligence

The search for extraterrestrial intelligence (SETI) is a collective term for scientific searches for intelligent extraterrestrial life, for example, monitoring electromagnetic radiation for signs of transmissions from civilizations on other planets.

Scientific investigation began shortly after the advent of radio in the early 1900s, and focused international efforts have been going on since the 1980s. In 2015 Stephen Hawking and Russian billionaire Yuri Milner announced a well-funded effort called the Breakthrough Initiatives.

There have been many earlier searches for extraterrestrial intelligence within the Solar System. In 1896, Nikola Tesla suggested that an extreme version of his wireless electrical transmission system could be used to contact beings on Mars. In 1899, while conducting experiments at his Colorado Springs experimental station, he thought he had detected a signal from that planet since an odd repetitive static signal seemed to cut off when Mars set in the night sky. Analysis of Tesla's research has ranged from suggestions that Tesla detected nothing, he simply misunderstood the new technology he was working with, to claims that Tesla may have been observing signals from Marconi's European radio experiments and even that he could have picked up naturally occurring Jovian plasma torus signals. In the early 1900s, Guglielmo Marconi, Lord Kelvin and David Peck Todd also stated their belief that radio could be used to contact Martians, with Marconi stating that his stations had also picked up potential Martian signals.

On August 21–23, 1924, Mars entered an opposition closer to Earth than at any time in the century before or the next 80 years. In the United States, a "National Radio Silence Day" was promoted during a 36-hour period from August 21–23, with all radios quiet for five minutes on the hour, every hour. At the United States Naval Observatory, a radio receiver was lifted above the ground in a dirigible tuned to a wavelength between 8 and 9 km, using a "radio-camera" developed by Amherst College and Charles Francis Jenkins. The program was led by David Peck Todd with the military assistance of Admiral Edward W. Eberle (Chief of Naval Operations), with William F. Friedman (chief cryptographer of the United States Army), assigned to translate any potential Martian messages.

A 1959 paper by Philip Morrison and Giuseppe Cocconi first pointed out the possibility of searching the microwave spectrum, and proposed frequencies and a set of initial targets.

In 1960, Cornell University astronomer Frank Drake performed the first modern SETI experiment, named "Project Ozma", after the Queen of Oz in L. Frank Baum's fantasy books. Drake used a radio telescope in diameter at Green Bank, West Virginia, to examine the stars Tau Ceti and Epsilon Eridani near the 1.420 gigahertz marker frequency, a region of the radio spectrum dubbed the "water hole" due to its proximity to the hydrogen and hydroxyl radical spectral lines. A 400 kilohertz band around the marker frequency was scanned, using a single-channel receiver with a bandwidth of 100 hertz. He found nothing of interest.

Soviet scientists took a strong interest in SETI during the 1960s and performed a number of searches with omnidirectional antennas in the hope of picking up powerful radio signals. Soviet astronomer Iosif Shklovsky wrote the pioneering book in the field, "Universe, Life, Intelligence" (1962), which was expanded upon by American astronomer Carl Sagan as the best-selling book "Intelligent Life in the Universe" (1966).

In the March 1955 issue of "Scientific American", John D. Kraus described an idea to scan the cosmos for natural radio signals using a flat-plane radio telescope equipped with a parabolic reflector. Within two years, his concept was approved for construction by Ohio State University. With a total of US$71,000 in grants from the National Science Foundation, construction began on an plot in Delaware, Ohio. This Ohio State University Radio Observatory telescope was called "Big Ear". Later, it began the world's first continuous SETI program, called the Ohio State University SETI program.

In 1971, NASA funded a SETI study that involved Drake, Bernard M. Oliver of Hewlett-Packard Corporation, and others. The resulting report proposed the construction of an Earth-based radio telescope array with 1,500 dishes known as "Project Cyclops". The price tag for the Cyclops array was US$10 billion. Cyclops was not built, but the report formed the basis of much SETI work that followed.

The Ohio State SETI program gained fame on August 15, 1977, when Jerry Ehman, a project volunteer, witnessed a startlingly strong signal received by the telescope. He quickly circled the indication on a printout and scribbled the exclamation "Wow!" in the margin. Dubbed the "Wow! signal", it is considered by some to be the best candidate for a radio signal from an artificial, extraterrestrial source ever discovered, but it has not been detected again in several additional searches.

In 1980, Carl Sagan, Bruce Murray, and Louis Friedman founded the U.S. Planetary Society, partly as a vehicle for SETI studies.

In the early 1980s, Harvard University physicist Paul Horowitz took the next step and proposed the design of a spectrum analyzer specifically intended to search for SETI transmissions. Traditional desktop spectrum analyzers were of little use for this job, as they sampled frequencies using banks of analog filters and so were restricted in the number of channels they could acquire. However, modern integrated-circuit digital signal processing (DSP) technology could be used to build autocorrelation receivers to check far more channels. This work led in 1981 to a portable spectrum analyzer named "Suitcase SETI" that had a capacity of 131,000 narrow band channels. After field tests that lasted into 1982, Suitcase SETI was put into use in 1983 with the Harvard/Smithsonian radio telescope at Oak Ridge Observatory in Harvard, Massachusetts. This project was named "Sentinel" and continued into 1985.

Even 131,000 channels were not enough to search the sky in detail at a fast rate, so Suitcase SETI was followed in 1985 by Project "META", for "Megachannel Extra-Terrestrial Assay". The META spectrum analyzer had a capacity of 8.4 million channels and a channel resolution of 0.05 hertz. An important feature of META was its use of frequency Doppler shift to distinguish between signals of terrestrial and extraterrestrial origin. The project was led by Horowitz with the help of the Planetary Society, and was partly funded by movie maker Steven Spielberg. A second such effort, META II, was begun in Argentina in 1990, to search the southern sky. META II is still in operation, after an equipment upgrade in 1996.

The follow-on to META was named "BETA", for "Billion-channel Extraterrestrial Assay", and it commenced observation on October 30, 1995. The heart of BETA's processing capability consisted of 63 dedicated fast Fourier transform (FFT) engines, each capable of performing a 2-point complex FFTs in two seconds, and 21 general-purpose personal computers equipped with custom digital signal processing boards. This allowed BETA to receive 250 million simultaneous channels with a resolution of 0.5 hertz per channel. It scanned through the microwave spectrum from 1.400 to 1.720 gigahertz in eight hops, with two seconds of observation per hop. An important capability of the BETA search was rapid and automatic re-observation of candidate signals, achieved by observing the sky with two adjacent beams, one slightly to the east and the other slightly to the west. A successful candidate signal would first transit the east beam, and then the west beam and do so with a speed consistent with Earth's sidereal rotation rate. A third receiver observed the horizon to veto signals of obvious terrestrial origin. On March 23, 1999, the 26-meter radio telescope on which Sentinel, META and BETA were based was blown over by strong winds and seriously damaged. This forced the BETA project to cease operation.

In 1978, the NASA SETI program had been heavily criticized by Senator William Proxmire, and funding for SETI research was removed from the NASA budget by Congress in 1981; however, funding was restored in 1982, after Carl Sagan talked with Proxmire and convinced him of the program's value. In 1992, the U.S. government funded an operational SETI program, in the form of the NASA Microwave Observing Program (MOP). MOP was planned as a long-term effort to conduct a general survey of the sky and also carry out targeted searches of 800 specific nearby stars. MOP was to be performed by radio antennas associated with the NASA Deep Space Network, as well as the radio telescope of the National Radio Astronomy Observatory at Green Bank, West Virginia and the radio telescope at the Arecibo Observatory in Puerto Rico. The signals were to be analyzed by spectrum analyzers, each with a capacity of 15 million channels. These spectrum analyzers could be grouped together to obtain greater capacity. Those used in the targeted search had a bandwidth of 1 hertz per channel, while those used in the sky survey had a bandwidth of 30 hertz per channel.

MOP drew the attention of the United States Congress, where the program was ridiculed and canceled one year after its start. SETI advocates continued without government funding, and in 1995 the nonprofit SETI Institute of Mountain View, California resurrected the MOP program under the name of Project "Phoenix", backed by private sources of funding. Project Phoenix, under the direction of Jill Tarter, is a continuation of the targeted search program from MOP and studies roughly 1,000 nearby Sun-like stars. From 1995 through March 2004, Phoenix conducted observations at the Parkes radio telescope in Australia, the radio telescope of the National Radio Astronomy Observatory in Green Bank, West Virginia, and the radio telescope at the Arecibo Observatory in Puerto Rico. The project observed the equivalent of 800 stars over the available channels in the frequency range from 1200 to 3000 MHz. The search was sensitive enough to pick up transmitters with 1 GW EIRP to a distance of about 200 light-years. According to Prof. Tarter, in 2012 it costs around "$2 million per year to keep SETI research going at the SETI Institute" and approximately 10 times that to support "all kinds of SETI activity around the world".

Many radio frequencies penetrate Earth's atmosphere quite well, and this led to radio telescopes that investigate the cosmos using large radio antennas. Furthermore, human endeavors emit considerable electromagnetic radiation as a byproduct of communications such as television and radio. These signals would be easy to recognize as artificial due to their repetitive nature and narrow bandwidths. If this is typical, one way of discovering an extraterrestrial civilization might be to detect artificial radio emissions from a location outside the Solar System.

Many international radio telescopes are currently being used for radio SETI searches, including the Low Frequency Array (LOFAR) in Europe, the Murchison Widefield Array (MWA) in Australia, and the Lovell Telescope in the United Kingdom.

The SETI Institute collaborated with the Radio Astronomy Laboratory at the Berkeley SETI Research Center to develop a specialized radio telescope array for SETI studies, something like a mini-cyclops array. Formerly known as the One Hectare Telescope (1HT), the concept was renamed the "Allen Telescope Array" (ATA) after the project's benefactor Paul Allen. Its sensitivity would be equivalent to a single large dish more than 100 meters in diameter if completed. Presently, the array under construction has 42 dishes at the Hat Creek Radio Observatory in rural northern California.

The full array (ATA-350) is planned to consist of 350 or more offset-Gregorian radio dishes, each in diameter. These dishes are the largest producible with commercially available satellite television dish technology. The ATA was planned for a 2007 completion date, at a very modest cost of US$25 million. The SETI Institute provided money for building the ATA while University of California, Berkeley designed the telescope and provided operational funding. The first portion of the array (ATA-42) became operational in October 2007 with 42 antennas. The DSP system planned for ATA-350 is extremely ambitious. Completion of the full 350 element array will depend on funding and the technical results from ATA-42.

ATA-42 (ATA) is designed to allow multiple observers simultaneous access to the interferometer output at the same time. Typically, the ATA snapshot imager (used for astronomical surveys and SETI) is run in parallel with the beam forming system (used primarily for SETI). ATA also supports observations in multiple synthesized pencil beams at once, through a technique known as "multibeaming". Multibeaming provides an effective filter for identifying false positives in SETI, since a very distant transmitter must appear at only one point on the sky.

SETI Institute's Center for SETI Research (CSR) uses ATA in the search for extraterrestrial intelligence, observing 12 hours a day, 7 days a week. From 2007-2015, ATA has identified hundreds of millions of technological signals. So far, all these signals have been assigned the status of noise or radio frequency interference because a) they appear to be generated by satellites or Earth-based transmitters, or b) they disappeared before the threshold time limit of ~1 hour. Researchers in CSR are presently working on ways to reduce the threshold time limit, and to expand ATA's capabilities for detection of signals that may have embedded messages.

Berkeley astronomers used the ATA to pursue several science topics, some of which might have turned up transient SETI signals, until 2011, when the collaboration between the University of California and the SETI Institute was terminated.

CNET published an article and pictures about the Allen Telescope Array (ATA) on December 12, 2008.

In April 2011, the ATA was forced to enter an 8-month "hibernation" due to funding shortfalls. Regular operation of the ATA was resumed on December 5, 2011.

In 2012, new life was breathed into the ATA thanks to a $3.6M philanthropic donation by Franklin Antonio, Co-Founder and Chief Scientist of QUALCOMM Incorporated. This gift supports upgrades of all the receivers on the ATA dishes to have dramatically (2x - 10x from 1–8 GHz) greater sensitivity than before and supporting sensitive observations over a wider frequency range from 1–18 GHz, though initially the radio frequency electronics go to only 12 GHz. As of July, 2013 the first of these receivers was installed and proven. Full installation on all 42 antennas is expected in June, 2014. ATA is especially well suited to the search for extraterrestrial intelligence SETI and to discovery of astronomical radio sources, such as heretofore unexplained non-repeating, possibly extragalactic, pulses known as fast radio bursts or FRBs.

SERENDIP (Search for Extraterrestrial Radio Emissions from Nearby Developed Intelligent Populations) is a SETI program launched in 1979 by the Berkeley SETI Research Center. SERENDIP takes advantage of ongoing "mainstream" radio telescope observations as a "piggy-back" or "commensal" program, using large radio telescopes including the NRAO 90m telescope at Green Bank and the Arecibo 305m telescope. Rather than having its own observation program, SERENDIP analyzes deep space radio telescope data that it obtains while other astronomers are using the telescopes.

The most recently deployed SERENDIP spectrometer, SERENDIP V.v, was installed at the Arecibo Observatory in June 2009 and is currently operational. The digital back-end instrument is an FPGA-based 128 million-channel digital spectrometer covering 200 MHz of bandwidth. It takes data commensally with the seven-beam Arecibo L-band Feed Array (ALFA). The program has found around 400 suspicious signals, but there is not enough data to prove that they belong to extraterrestrial intelligence.

"Breakthrough Listen" is a ten-year initiative with $100 million funding begun in July 2015 to actively search for intelligent extraterrestrial communications in the universe, in a substantially expanded way, using resources that had not previously been extensively used for the purpose. It has been described as the most comprehensive search for alien communications to date. The science program for Breakthrough Listen is based at Berkeley SETI Research Center, located in the Astronomy Department at the University of California, Berkeley.

Announced in July 2015, the project is observing for thousands of hours every year on two major radio telescopes, the Green Bank Observatory in West Virginia and the Parkes Observatory in Australia. Previously, only about 24 to 36 hours of telescope per year was used in the search for alien life. Furthermore, the Automated Planet Finder at Lick Observatory is searching for optical signals coming from laser transmissions. The massive data rates from the radio telescopes (24 GB/s at Green Bank) necessitated the construction of dedicated hardware at the telescopes to perform the bulk of the analysis. Some of the data are also analyzed by volunteers in the SETI@home distributed computing network. Founder of modern SETI Frank Drake is one of the scientists on the project's advisory committee.

China's 500 meter Aperture Spherical Telescope (FAST) lists "detecting interstellar communication signals" as part of its science mission. It is funded by the National Development and Reform Commission (NDRC) and managed by the National Astronomical observatories (NAOC) of Chinese Academy of Sciences (CAS). FAST is the first radio observatory built with SETI as a core scientific goal. FAST consists of a fixed diameter spherical dish constructed in a natural depression sinkhole caused by karst processes in the region. It is the world's largest filled-aperture radio telescope.
According to its website, FAST could search out to 28 light-years, and would be able to reach 1400 stars. If the transmitter's radiated power is increased to 1000,000 MW, FAST would be able to reach one million stars. This is compared to the Arecibo 305 meter telescope detection distance of 18 light-years.

SETI@home was conceived by David Gedye along with Craig Kasnoff and is a popular volunteer distributed computing project that was launched by the Berkeley SETI Research Center at the University of California, Berkeley, in May 1999. It was originally funded by The Planetary Society and Paramount Pictures, and later by the state of California. The project is run by director David P. Anderson and chief scientist Dan Werthimer. Any individual can become involved with SETI research by downloading the Berkeley Open Infrastructure for Network Computing (BOINC) software program, attaching to the SETI@home project, and allowing the program to run as a background process that uses idle computer power. The SETI@home program itself runs signal analysis on a "work unit" of data recorded from the central 2.5 MHz wide band of the SERENDIP IV instrument. After computation on the work unit is complete, the results are then automatically reported back to SETI@home servers at University of California, Berkeley. By June 28, 2009, the SETI@home project had over 180,000 active participants volunteering a total of over 290,000 computers. These computers give SETI@home an average computational power of 617 teraFLOPS. In 2004 radio source SHGb02+14a set off speculation in the media that a signal had been detected but researchers noted the frequency drifted rapidly and the detection on three SETI@home computers fell within random chance.

As of 2010, after 10 years of data collection, SETI@home has listened to that one frequency at every point of over 67 percent of the sky observable from Arecibo with at least three scans (out of the goal of nine scans), which covers about 20 percent of the full celestial sphere.

SETI Net is a private search system created by a single individual. It is closely affiliated with the SETI League and is one of the project Argus stations (DM12jw).

The SETI Net station consists of off-the-shelf, consumer-grade electronics to minimize cost and to allow this design to be replicated as simply as possible. It has a 3-meter parabolic antenna that can be directed in azimuth and elevation, an LNA that covers the 1420 MHz spectrum, a receiver to reproduce the wideband audio, and a standard personal computer as the control device and for deploying the detection algorithms.

The antenna can be pointed and locked to one sky location, enabling the system to integrate on it for long periods. Currently the Wow! signal area is being monitored when it is above the horizon. All search data are collected and made available on the Internet archive.

SETI Net started operation in the early 1980s as a way to learn about the science of the search, and has developed several software packages for the amateur SETI community. It has provided an astronomical clock, a file manager to keep track of SETI data files, a spectrum analyzer optimized for amateur SETI, remote control of the station from the Internet, and other packages.

Founded in 1994 in response to the United States Congress cancellation of the NASA SETI program, The SETI League, Inc. is a membership-supported nonprofit organization with 1,500 members in 62 countries. This grass-roots alliance of amateur and professional radio astronomers is headed by executive director emeritus H. Paul Shuch, the engineer credited with developing the world's first commercial home satellite TV receiver. Many SETI League members are licensed radio amateurs and microwave experimenters. Others are digital signal processing experts and computer enthusiasts.

The SETI League pioneered the conversion of backyard satellite TV dishes in diameter into research-grade radio telescopes of modest sensitivity. The organization concentrates on coordinating a global network of small, amateur-built radio telescopes under Project Argus, an all-sky survey seeking to achieve real-time coverage of the entire sky. Project Argus was conceived as a continuation of the all-sky survey component of the late NASA SETI program (the targeted search having been continued by the SETI Institute's Project Phoenix). There are currently 143 Project Argus radio telescopes operating in 27 countries. Project Argus instruments typically exhibit sensitivity on the order of 10 Watts/square metre, or roughly equivalent to that achieved by the Ohio State University Big Ear radio telescope in 1977, when it detected the landmark "Wow!" candidate signal.

The name "Argus" derives from the mythical Greek guard-beast who had 100 eyes, and could see in all directions at once. In the SETI context, the name has been used for radio telescopes in fiction (Arthur C. Clarke, ""Imperial Earth""; Carl Sagan, ""Contact""), was the name initially used for the NASA study ultimately known as "Cyclops," and is the name given to an omnidirectional radio telescope design being developed at the Ohio State University.

While most SETI sky searches have studied the radio spectrum, some SETI researchers have considered the possibility that alien civilizations might be using powerful lasers for interstellar communications at optical wavelengths. The idea was first suggested by R. N. Schwartz and Charles Hard Townes in a 1961 paper published in the journal "Nature" titled "Interstellar and Interplanetary Communication by Optical Masers". However, the 1971 Cyclops study discounted the possibility of optical SETI, reasoning that construction of a laser system that could outshine the bright central star of a remote star system would be too difficult. In 1983, Townes published a detailed study of the idea in the United States journal "Proceedings of the National Academy of Sciences", which was met with widespread agreement by the SETI community.

There are two problems with optical SETI. The first problem is that lasers are highly "monochromatic", that is, they emit light only on one frequency, making it troublesome to figure out what frequency to look for. However, emitting light in narrow pulses results in a broad spectrum of emission; the spread in frequency becomes higher as the pulse width becomes narrower, making it easier to detect an emission.

The other problem is that while radio transmissions can be broadcast in all directions, lasers are highly directional. Interstellar gas and dust is almost transparent to near infrared, so these signals can be seen from greater distances, but the extraterrestrial laser signals would need to be transmitted in the direction of Earth in order to be detected.

Optical SETI supporters have conducted paper studies of the effectiveness of using contemporary high-energy lasers and a ten-meter diameter mirror as an interstellar beacon. The analysis shows that an infrared pulse from a laser, focused into a narrow beam by such a mirror, would appear thousands of times brighter than the Sun to a distant civilization in the beam's line of fire. The Cyclops study proved incorrect in suggesting a laser beam would be inherently hard to see.

Such a system could be made to automatically steer itself through a target list, sending a pulse to each target at a constant rate. This would allow targeting of all Sun-like stars within a distance of 100 light-years. The studies have also described an automatic laser pulse detector system with a low-cost, two-meter mirror made of carbon composite materials, focusing on an array of light detectors. This automatic detector system could perform sky surveys to detect laser flashes from civilizations attempting contact.

Several optical SETI experiments are now in progress. A Harvard-Smithsonian group that includes Paul Horowitz designed a laser detector and mounted it on Harvard's optical telescope. This telescope is currently being used for a more conventional star survey, and the optical SETI survey is "piggybacking" on that effort. Between October 1998 and November 1999, the survey inspected about 2,500 stars. Nothing that resembled an intentional laser signal was detected, but efforts continue. The Harvard-Smithsonian group is now working with Princeton University to mount a similar detector system on Princeton's 91-centimeter (36-inch) telescope. The Harvard and Princeton telescopes will be "ganged" to track the same targets at the same time, with the intent being to detect the same signal in both locations as a means of reducing errors from detector noise.

The Harvard-Smithsonian SETI group lead by Professor Paul Horowitz built a dedicated all-sky optical survey system along the lines of that described above, featuring a 1.8-meter (72-inch) telescope. The new optical SETI survey telescope is being set up at the Oak Ridge Observatory in Harvard, Massachusetts.

The University of California, Berkeley, home of SERENDIP and SETI@home, is also conducting optical SETI searches and collaborates with the NIROSETI program. The optical SETI program at Breakthrough Listen is being directed by Geoffrey Marcy, an extrasolar planet hunter, and it involves examination of records of spectra taken during extrasolar planet hunts for a continuous, rather than pulsed, laser signal. This survey uses the Automated Planet Finder 2.4-m telescope at the Lick Observatory, situated on the summit of Mount Hamilton, east of San Jose, California, USA. The other Berkeley optical SETI effort is being pursued by the Harvard-Smithsonian group and is being directed by Dan Werthimer of Berkeley, who built the laser detector for the Harvard-Smithsonian group. This survey uses a 76-centimeter (30-inch) automated telescope at Leuschner Observatory and an older laser detector built by Werthimer.

In May 2017, astronomers reported studies related to laser light emissions from stars, as a way of detecting technology-related signals from an alien civilization. The reported studies included KIC 8462852, an oddly dimming star in which its unusual starlight fluctuations may be the result of interference by an artificial megastructure, such as a Dyson swarm, made by such a civilization. No evidence was found for technology-related signals from KIC 8462852 in the studies.

Gamma-ray bursts (GRBs) are candidates for extraterrestrial communication. These high-energy bursts are observed about once per day and originate throughout the observable universe. SETI currently omits gamma ray frequencies in their monitoring and analysis because they are absorbed by the Earth's atmosphere and difficult to detect with ground-based receivers. In addition, the wide burst bandwidths pose a serious analysis challenge for modern digital signal processing systems. Still, the continued mysteries surrounding gamma-ray bursts have encouraged hypotheses invoking extraterrestrials. John A. Ball from the MIT Haystack Observatory suggests that an advanced civilization that has reached a technological singularity would be capable of transmitting a two-millisecond pulse encoding bits of information. This is "comparable to the estimated total information content of Earth's biosystem—genes and memes and including all libraries and computer media".

The possibility of using interstellar messenger probes in the search for extraterrestrial intelligence was first suggested by Ronald N. Bracewell in 1960 (see Bracewell probe), and the technical feasibility of this approach was demonstrated by the British Interplanetary Society's starship study Project Daedalus in 1978. Starting in 1979, Robert Freitas advanced arguments for the proposition that physical space-probes are a superior mode of interstellar communication to radio signals. See Voyager Golden Record.

In recognition that any sufficiently advanced interstellar probe in the vicinity of Earth could easily monitor the terrestrial Internet, Invitation to ETI was established by Prof. Allen Tough in 1996, as a Web-based SETI experiment inviting such spacefaring probes to establish contact with humanity. The project's 100 Signatories includes prominent physical, biological, and social scientists, as well as artists, educators, entertainers, philosophers and futurists. Prof. H. Paul Shuch, executive director emeritus of The SETI League, serves as the project's Principal Investigator.

Inscribing a message in matter and transporting it to an interstellar destination can be enormously more energy efficient than communication using electromagnetic waves if delays larger than light transit time can be tolerated. That said, for simple messages such as "hello," radio SETI could be far more efficient. If energy requirement is used as a proxy for technical difficulty, then a solarcentric Search for Extraterrestrial Artifacts (SETA) may be a useful supplement to traditional radio or optical searches.

Much like the "preferred frequency" concept in SETI radio beacon theory, the Earth-Moon or Sun-Earth libration orbits might therefore constitute the most universally convenient parking places for automated extraterrestrial spacecraft exploring arbitrary stellar systems. A viable long-term SETI program may be founded upon a search for these objects.

In 1979, Freitas and Valdes conducted a photographic search of the vicinity of the Earth-Moon triangular libration points and , and of the solar-synchronized positions in the associated halo orbits, seeking possible orbiting extraterrestrial interstellar probes, but found nothing to a detection limit of about 14th magnitude. The authors conducted a second, more comprehensive photographic search for probes in 1982 that examined the five Earth-Moon Lagrangian positions and included the solar-synchronized positions in the stable L4/L5 libration orbits, the potentially stable nonplanar orbits near L1/L2, Earth-Moon , and also in the Sun-Earth system. Again no extraterrestrial probes were found to limiting magnitudes of 17–19th magnitude near L3/L4/L5, 10–18th magnitude for /, and 14–16th magnitude for Sun-Earth .

In June 1983, Valdes and Freitas used the 26 m radiotelescope at Hat Creek Radio Observatory to search for the tritium hyperfine line at 1516 MHz from 108 assorted astronomical objects, with emphasis on 53 nearby stars including all visible stars within a 20 light-year radius. The tritium frequency was deemed highly attractive for SETI work because (1) the isotope is cosmically rare, (2) the tritium hyperfine line is centered in the SETI waterhole region of the terrestrial microwave window, and (3) in addition to beacon signals, tritium hyperfine emission may occur as a byproduct of extensive nuclear fusion energy production by extraterrestrial civilizations. The wideband- and narrowband-channel observations achieved sensitivities of 5–14 x 10 W/m²/channel and 0.7-2 x 10 W/m²/channel, respectively, but no detections were made.

Technosignatures, including all signs of technology with the exception of the interstellar radio messages that define traditional SETI, are a recent avenue in the search for extraterrestrial intelligence. Technosignatures may originate from various sources, from megastructures such as Dyson spheres and space mirrors or space shaders to the atmospheric contamination created by an industrial civilization, or city lights on extrasolar planets, and may be detectable in the future with large hypertelescopes.

Technosignatures can be divided into three broad categories: astroengineering projects, signals of planetary origin, and spacecraft within and outside the Solar System.

An astroengineering installation such as a Dyson sphere, designed to convert all of the incident radiation of its host star into energy, could be detected through the observation of an infrared excess from a solar analog star, or by the star's apparent disappearance in the visible spectrum over several years. After examining some 100,000 nearby large galaxies, a team of researchers has concluded that none of them display any obvious signs of highly advanced technological civilizations.

Another hypothetical form of astroengineering, the Shkadov thruster, moves its host star by reflecting some of the star's light back on itself, and would be detected by observing if its transits across the star abruptly end with the thruster in front. Asteroid mining within the Solar System is also a detectable technosignature of the first kind.

Individual extrasolar planets can be analyzed for signs of technology. Avi Loeb of the Harvard-Smithsonian Center for Astrophysics has proposed that persistent light signals on the night side of an exoplanet can be an indication of the presence of cities and an advanced civilization. In addition, the excess infrared radiation and chemicals produced by various industrial processes or terraforming efforts may point to intelligence.

Light and heat detected from planets need to be distinguished from natural sources to conclusively prove the existence of civilization on a planet. However, as argued by the Colossus team,
a civilization heat signature should be within a "comfortable" temperature range, like terrestrial urban heat islands, i.e. only a few degrees warmer than the planet itself. In contrast, such natural sources as wild fires, volcanoes, etc. are significantly hotter, so they will be well distinguished by their maximum flux at a different wavelength.

Extraterrestrial craft are another target in the search for technosignatures. Magnetic sail interstellar spacecraft should be detectable over thousands of light-years of distance through the synchrotron radiation they would produce through interaction with the interstellar medium; other interstellar spacecraft designs may be detectable at more modest distances. In addition, robotic probes within the Solar System are also being sought out with optical and radio searches.

For a sufficiently advanced civilization, hyper energetic neutrinos from Planck scale accelerators should be detectable at a distance of many Mpc.

Italian physicist Enrico Fermi suggested in the 1950s that if technologically advanced civilizations are common in the universe, then they should be detectable in one way or another. (According to those who were there, Fermi either asked "Where are they?" or "Where is everybody?")

The Fermi paradox is commonly understood as asking why extraterrestrials have not visited Earth, but the same reasoning applies to the question of why signals from extraterrestrials have not been heard. The SETI version of the question is sometimes referred to as "the Great Silence".

The Fermi paradox can be stated more completely as follows:
There are multiple explanations proposed for the Fermi paradox, ranging from analyses suggesting that intelligent life is rare (the "Rare Earth hypothesis"), to analyses suggesting that although extraterrestrial civilizations may be common, they would not communicate, or would not travel across interstellar distances.

Science writer Timothy Ferris has posited that since galactic societies are most likely only transitory, an obvious solution is an interstellar communications network, or a type of library consisting mostly of automated systems. They would store the cumulative knowledge of vanished civilizations and communicate that knowledge through the galaxy. Ferris calls this the "Interstellar Internet", with the various automated systems acting as network "servers". If such an Interstellar Internet exists, the hypothesis states, communications between servers are mostly through narrow-band, highly directional radio or laser links. Intercepting such signals is, as discussed earlier, very difficult. However, the network could maintain some broadcast nodes in hopes of making contact with new civilizations.

Although somewhat dated in terms of "information culture" arguments, not to mention the obvious technological problems of a system that could work effectively for billions of years and requires multiple lifeforms agreeing on certain basics of communications technologies, this hypothesis is actually testable (see below).

A significant problem is the vastness of space. Despite piggybacking on the world's most sensitive radio telescope, Charles Stuart Bowyer said, the instrument could not detect random radio noise emanating from a civilization like ours, which has been leaking radio and TV signals for less than 100 years. For SERENDIP and most other SETI projects to detect a signal from an extraterrestrial civilization, the civilization would have to be beaming a powerful signal directly at us. It also means that Earth civilization will only be detectable within a distance of 100 light-years.

The International Academy of Astronautics (IAA) has a long-standing SETI Permanent Study Group (SPSG, formerly called the IAA SETI Committee), which addresses matters of SETI science, technology, and international policy. The SPSG meets in conjunction with the International Astronautical Congress (IAC) held annually at different locations around the world, and sponsors two SETI Symposia at each IAC. In 2005, the IAA established the SETI: Post-Detection Science and Technology Taskgroup (Chairman, Professor Paul Davies) "to act as a Standing Committee to be available to be called on at any time to advise and consult on questions stemming from the discovery of a putative signal of extraterrestrial intelligent (ETI) origin."

However, the protocols mentioned apply only to radio SETI rather than for METI (Active SETI). The intention for METI is covered under the SETI charter "Declaration of Principles Concerning Sending Communications with Extraterrestrial Intelligence".

On October 2000 astronomers Iván Almár and Jill Tarter presented a paper to The SETI Permanent Study Group in Rio de Janeiro, Brazil which proposed a scale (modelled after the Torino scale) which is an ordinal scale between zero and ten that quantifies the impact of any public announcement regarding evidence of extraterrestrial intelligence; the Rio scale has since inspired the 2005 San Marino Scale (in regard to the risks of transmissions from Earth) and the 2010 London Scale (in regard to the detection of extraterrestrial life) The Rio Scale itself was revised in 2018.

The SETI Institute does not officially recognize the Wow! signal as of extraterrestrial origin (as it was unable to be verified). The SETI Institute has also publicly denied that the candidate signal Radio source SHGb02+14a is of extraterrestrial origin though full details of the signal, such as its exact location have never been disclosed to the public. Although other volunteering projects such as Zooniverse credit users for discoveries, there is currently no crediting or early notification by SETI@Home following the discovery of a signal.

Some people, including Steven M. Greer, have expressed cynicism that the general public might not be informed in the event of a genuine discovery of extraterrestrial intelligence due to significant vested interests. Some, such as Bruce Jakosky have also argued that the official disclosure of extraterrestrial life may have far reaching and as yet undetermined implications for society, particularly for the world's religions.

Active SETI, also known as messaging to extraterrestrial intelligence (METI), consists of sending signals into space in the hope that they will be picked up by an alien intelligence.

In November 1974, a largely symbolic attempt was made at the Arecibo Observatory to send a message to other worlds. Known as the Arecibo Message, it was sent towards the globular cluster M13, which is 25,000 light-years from Earth. Further IRMs Cosmic Call, Teen Age Message, Cosmic Call 2, and A Message From Earth were transmitted in 1999, 2001, 2003 and 2008 from the Evpatoria Planetary Radar.

Physicist Stephen Hawking, in his book "A Brief History of Time", suggests that "alerting" extraterrestrial intelligences to our existence is foolhardy, citing mankind's history of treating his fellow man harshly in meetings of civilizations with a significant technology gap. He suggests, in view of this history, that we "lay low". In one response to Hawking, in September 2016, astronomer Seth Shostak, allays such concerns. Astronomer Jill Tarter also disagrees with Hawking, arguing that aliens developed and long-lived enough to communicate and travel across interstellar distances would have evolved a cooperative and less violent intelligence. She does think it is too soon for humans to attempt active SETI and that humans should be more advanced technologically first but keep listening in the meantime.

The concern over METI was raised by the science journal "Nature" in an editorial in October 2006, which commented on a recent meeting of the International Academy of Astronautics SETI study group. The editor said, "It is not obvious that all extraterrestrial civilizations will be benign, or that contact with even a benign one would not have serious repercussions" (Nature Vol 443 12 October 06 p 606). Astronomer and science fiction author David Brin has expressed similar concerns.

Richard Carrigan, a particle physicist at the Fermi National Accelerator Laboratory near Chicago, Illinois, suggested that passive SETI could also be dangerous and that a signal released onto the Internet could act as a computer virus. Computer security expert Bruce Schneier dismissed this possibility as a "bizarre movie-plot threat".

To lend a quantitative basis to discussions of the risks of transmitting deliberate messages from Earth, the SETI Permanent Study Group of the International Academy of Astronautics adopted in 2007 a new analytical tool, the San Marino Scale. Developed by Prof. Ivan Almar and Prof. H. Paul Shuch, the scale evaluates the significance of transmissions from Earth as a function of signal intensity and information content. Its adoption suggests that not all such transmissions are equal, and each must be evaluated separately before establishing blanket international policy regarding active SETI.

However, some scientists consider these fears about the dangers of METI as panic and irrational superstition; see, for example, Alexander L. Zaitsev's papers. Biologist João Pedro de Magalhães also proposed in 2015 transmitting an invitation message to any extraterrestrial intelligences watching us already in the context of the Zoo Hypothesis and inviting them to respond, arguing this would not put us in any more danger than we are already if the Zoo Hypothesis is correct.

On 13 February 2015, scientists (including Geoffrey Marcy, Seth Shostak, Frank Drake, Elon Musk and David Brin) at a convention of the American Association for the Advancement of Science, discussed Active SETI and whether transmitting a message to possible intelligent extraterrestrials in the Cosmos was a good idea; one result was a statement, signed by many, that a "worldwide scientific, political and humanitarian discussion must occur before any message is sent". On 28 March 2015, a related essay was written by Seth Shostak and published in "The New York Times".

The Breakthrough Message program is an open competition announced in July 2015 to design a digital message that could be transmitted from Earth to an extraterrestrial civilization, with a US$1,000,000 prize pool. The message should be "representative of humanity and planet Earth". The program pledges "not to transmit any message until there has been a wide-ranging debate at high levels of science and politics on the risks and rewards of contacting advanced civilizations".

As various SETI projects have progressed, some have criticized early claims by researchers as being too "euphoric". For example, Peter Schenkel, while remaining a supporter of SETI projects, wrote in 2006 that

SETI has also occasionally been the target of criticism by those who suggest that it is a form of pseudoscience. In particular, critics allege that no observed phenomena suggest the existence of extraterrestrial intelligence, and furthermore that the assertion of the existence of extraterrestrial intelligence has no good Popperian criteria for falsifiability, as explained in a 2009 editorial in "Nature", which said:

"Nature" added that SETI was "marked by a hope, bordering on faith" that aliens were aiming signals at us, that a hypothetical alien SETI project looking at Earth with "similar faith" would be "sorely disappointed" (despite our many untargeted radar and TV signals, and our few targeted Active SETI radio signals denounced by those fearing aliens), and that it had difficulties attracting even sympathetic working scientists and Government funding because it was "an effort so likely to turn up nothing".

However "Nature" also added that "Nonetheless, a small SETI effort is well worth supporting, especially given the enormous implications if it did succeed" and that "happily, a handful of wealthy technologists and other private donors have proved willing to provide that support".

Supporters of the Rare Earth Hypothesis argue that advanced lifeforms are likely to be very rare, and that, if that is so, then SETI efforts will be futile. However the Rare Earth Hypothesis itself faces many criticisms.

In 1993 Roy Mash claimed that "Arguments favoring the existence of extraterrestrial intelligence nearly always contain an overt appeal to big numbers, often combined with a covert reliance on generalization from a single instance" and concluded that "the dispute between believers and skeptics is seen to boil down to a conflict of intuitions which can barely be engaged, let alone resolved, given our present state of knowledge". In 2012 Milan M. Ćirković (who was then research professor at the Astronomical Observatory of Belgrade and a research associate of the Future of Humanity Institute at the University of Oxford claimed that Mash was unrealistically over-reliant on excessive abstraction that ignored the empirical information available to modern SETI researchers.

George Basalla, Emeritus Professor of History at the University of Delaware, is a critic of SETI who argued in 2006 that "extraterrestrials discussed by scientists are as imaginary as the spirits and gods of religion or myth", and has in turn been criticized by Milan M. Ćirković for, among other things, being unable to distinguish between "SETI believers" and "scientists engaged in SETI", who are often sceptical (especially about quick detection), such as Freeman Dyson (and, at least in their later years, Iosif Shklovsky and Sebastian von Hoerner), and for ignoring the difference between the knowledge underlying the arguments of modern scientists and those of ancient Greek thinkers.

Massimo Pigliucci, Professor of Philosophy at CUNY-City College, asked in 2010 whether SETI is "uncomfortably close to the status of pseudoscience" due to the lack of any clear point at which negative results cause the hypothesis of Extraterrestrial Intelligence to be abandoned, before eventually concluding that SETI is "almost-science", which is described by Milan M. Ćirković as Pigliucci putting SETI in "the illustrious company of string theory, interpretations of quantum mechanics, evolutionary psychology and history (of the 'synthetic' kind done recently by Jared Diamond)", while adding that his justification for doing so with SETI "is weak, outdated, and reflecting particular philosophical prejudices similar to the ones described above in Mash and Basalla".




</doc>
<doc id="28154" url="https://en.wikipedia.org/wiki?curid=28154" title="Sextans">
Sextans

Sextans is a minor equatorial constellation which was introduced in 1687 by Johannes Hevelius. Its name is Latin for the astronomical sextant, an instrument that Hevelius made frequent use of in his observations.

Sextans as a constellation covers a rather dim, sparse region of the sky. It has only one star above the fifth magnitude, namely α Sextantis at 4.49. The constellation contains a few double stars, including γ, 35, and 40 Sextantis. There are a few notable variable stars, including β, 25, 23 Sextantis, and LHS 292. NGC 3115, an edge-on lenticular galaxy, is the only noteworthy deep-sky object. It also lies near the ecliptic, which causes the Moon, and some of the planets to occasionally pass through it for brief periods of time.

The constellation is the location of the field studied by the COSMOS project, undertaken by the Hubble Space Telescope.

Sextans B is a fairly bright dwarf irregular galaxy at magnitude 6.6, 4.3 million light-years from Earth. It is part of the Local Group of galaxies.

CL J1001+0220 is as of 2016 the most distant known galaxy cluster at redshift z=2.506, 11.1 billion light-years from Earth.

In June 2015, astronomers reported evidence for Population III stars in the Cosmos Redshift 7 galaxy (at "z" = 6.60) found in Sextans. Such stars are likely to have existed in the very early universe (i.e., at high redshift), and may have started the production of chemical elements heavier than hydrogen that are needed for the later formation of planets and life as we know it.



</doc>
<doc id="28156" url="https://en.wikipedia.org/wiki?curid=28156" title="Salem al-Hazmi">
Salem al-Hazmi

Salem al-Hazmi (, , also transliterated as Alhazmi) (February 2, 1981 – September 11, 2001) was one of five hijackers of American Airlines Flight 77 as part of the September 11 attacks.

Hazmi had a relatively long history with al-Qaeda before being selected for the attacks. He obtained a tourist visa through the Visa Express program and arrived in the United States in June 2001 where he would settle in New Jersey with other American 77 hijackers up until the attacks.

On September 11, 2001, Hazmi boarded American Airlines Flight 77 and helped subdue the passengers and crew for Hani Hanjour, the pilot among the hijackers, to crash the plane into west facade of the Pentagon. His older brother, Nawaf al-Hazmi, was another hijacker aboard the same flight. At the age of 20 years and 221 days, he was the youngest hijacker who participated in the attacks.

Hazmi was born on February 2, 1981 to Muhammad Salim al-Hazmi, a grocer, in Mecca, Saudi Arabia. His father described Salem as a quarrelsome teenager who had problems with alcohol and petty theft. However, he stopped drinking and began to attend the mosque about three months before he left his family.

There are reports that he fought in Afghanistan with his brother, Nawaf al-Hazmi, and other reports say the two fought together in Chechnya. Salem al-Hazmi was an al-Qaeda veteran by the time he was selected for participation in the 9/11 attacks. U.S. intelligence learned of Hazmi's involvement with al-Qaeda as early as 1999, but he was not placed on any watchlists.

Known as "Bilal" during the preparations, both he and Ahmed al-Ghamdi flew to Beirut in November 2000, though on separate flights.

Along with Nawaf al-Hazmi and several other future hijackers, Salem al-Hazmi may have attended the 2000 Al Qaeda Summit in Kuala Lumpur, Malaysia. It was there that the details of the 9/11 attacks were decided upon.
According to the FBI and the 9/11 Commission report, Hazmi first entered the United States on June 29, 2001, although there are numerous unconfirmed reports that he was living in San Antonio, Texas with fellow hijacker Satam al-Suqami much earlier. Hazmi used the controversial Visa Express program to gain entry into the country.

Hazmi moved to Paterson, New Jersey where he lived with Hani Hanjour. Both were among the five hijackers who applied for Virginia identity cards at the Arlington office of the Virginia Department of Motor Vehicles on August 2, 2001, although Salem already held an NJ identity card.

On August 27, brothers Nawaf and Salem purchased flight tickets through Travelocity.com using Nawaf's visa card.

With the four other Flight 77 hijackers, he worked out at a Gold's Gym in Greenbelt, Maryland from September 2 to September 6 of the same year.

On September 11, 2001, Hazmi boarded American Airlines Flight 77. Airport surveillance video from Washington's Dulles Airport shows two of the five hijackers, including Salem al-Hazmi, being pulled aside to undergo additional scrutiny after setting off metal detectors.

The flight was scheduled to depart at 08:10, but ended up departing 10 minutes late from Gate D26 at Dulles. The last normal radio communications from the aircraft to air traffic control occurred at 08:50:51. At 08:54, Flight 77 began to deviate from its normal, assigned flight path and turned south, and then hijackers set the flight's autopilot heading for Washington, D.C. Passenger Barbara Olson called her husband, United States Solicitor General Theodore Olson, and reported that the plane had been hijacked and that the assailants had box cutters and knives. At 09:37, American Airlines Flight 77 crashed into the west facade of the Pentagon, killing all 64 aboard (including the hijackers), along with 125 on the ground in the Pentagon. In the recovery process at the Pentagon, remains of all five Flight 77 hijackers were identified through a process of elimination, as not matching any DNA samples for the victims, and put into custody of the FBI. Forensics teams confirmed that it seemed two of the hijackers were brothers, based on their DNA similarities.

Shortly after the attacks, several sources reported that Salem al-Hazmi, 26, was alive and working at a petrochemical plant in Yanbu, Saudi Arabia. He claimed that his passport had been stolen by a pickpocket in Cairo three years before, and that the pictures and details such as date of birth released to the public by the FBI were his own. He also stated that he had never visited the United States, but volunteered to fly to the U.S. to prove his innocence. On September 19, "Al-Sharq Al-Awsat" published his photograph alongside Badr Alhazmi's, who they claimed was the actual hijacker who had stolen his identity.

Muhammad Salim al-Hazmi, father of the two suspects, Nawaf and Salim Muhammad al-Hazmi, said that the published photos may be doctored or faked somehow. Hazmi continued, "As a father, I have a feeling that the two of them are still alive and unhurt, and will come back home in the near future when the truth is uncovered and the real culprits are found."

After some confusion and doubt Saudi Arabia admitted that in fact the names of the hijackers were correct. "The names that we got confirmed that," Interior Minister Prince Nayef said in an interview with The Associated Press. "Their families have been notified." Nayef said the Saudi leadership was shocked to learn 15 of the hijackers were from Saudi Arabia and said it was natural that the kingdom had not noticed their involvement beforehand.



</doc>
<doc id="28157" url="https://en.wikipedia.org/wiki?curid=28157" title="Satsuma Province">
Satsuma Province

Satsuma's provincial capital was Satsumasendai. During the Sengoku period, Satsuma was a fief of the Shimazu "daimyō", who ruled much of southern Kyūshū from their castle at Kagoshima city. They were the initial patrons of Satsuma ware, which was later widely exported to the West.

In 1871, with the abolition of feudal domains and the establishment of prefectures after the Meiji Restoration, the provinces of Satsuma and Ōsumi were combined to eventually establish Kagoshima Prefecture.

Satsuma was one of the main provinces that rose in opposition to the Tokugawa shogunate in the mid 19th century. Because of this, the oligarchy that came into power after the Meiji Restoration of 1868 had a strong representation from the Satsuma province, with leaders such as Ōkubo Toshimichi and Saigō Takamori taking up key government positions.

Satsuma is well known for its production of sweet potatoes, known in Japan as 薩摩芋 (satsuma-imo or "Satsuma potato"). On the other hand, Satsuma mandarins (known as "mikan" in Japan) do not specifically originate from Satsuma but were imported into the West through this province in the Meiji era.






</doc>
<doc id="28159" url="https://en.wikipedia.org/wiki?curid=28159" title="Scottish">
Scottish

Scottish usually refers to something of, from, or related to Scotland, including:



</doc>
<doc id="28161" url="https://en.wikipedia.org/wiki?curid=28161" title="List of brightest stars">
List of brightest stars

This is a list of the brightest naked eye stars to +2.50 magnitude, as determined by their "maximum", "total", or "combined" apparent visual magnitudes as seen from Earth. Although several of the brightest stars are also known close binary or multiple star systems, they do appear to the naked eye as single stars. The given list below combines/adds the magnitudes of bright individual components. Most of the proper names in this list are those approved by the Working Group on Star Names, and although their usage is recommended, they remain as "preliminary guidelines." Popular star names here, that have not been approved by the IAU, appear with a short note.

Apparent visual magnitudes of the brightest star can also be compared to non-stellar objects in our Solar System. Here the maximum visible magnitudes above the brightest star, Sirius (−1.46), are as follows. Excluding the Sun, the brightest objects are the Moon (−12.7), Venus (−4.89), Jupiter (−2.94), Mars (−2.91), Mercury (−2.45), and Saturn (−0.49).

Any exact order of the visual brightness of stars is not perfectly defined for four reasons:


The source of magnitudes cited in this list is the linked Wikipedia articles—this basic list is a catalog of what Wikipedia itself documents. References can be found in the individual articles.




</doc>
<doc id="28162" url="https://en.wikipedia.org/wiki?curid=28162" title="List of nearest stars and brown dwarfs">
List of nearest stars and brown dwarfs

The following two lists include all the known stars and brown dwarfs that are within of the Sun, or were/will be within in the astronomically near past or future. The easiest way to determine stellar distance to the Sun for these objects is parallax, which measures how much stars appear to move against background objects over the course of Earth's orbit around the Sun. As a parsec (parallax-second) is defined by the distance of an object that would appear to move exactly one second of arc against background objects, stars less than 5 parsecs away will have measured parallaxes of over 0.2 arcseconds, or 200 milliarcseconds.

The first table is based on the most accurate observed parallaxes of the stars. The second table additionally lists stars that in the past have come or in the future will come within . Determining which stars fall within the stated range relies on accurate astrometric measurements of their parallax and total proper motions (how far they move across the sky due to their actual velocity relative to the Sun), along with spectroscopic determined radial velocities (their speed directly towards or away from us, which combined with proper motion defines their true movement through the sky relative to the Sun). Both of these measurements are subject to increasing and significant errors over very long time spans, especially over the several thousand-year time spans it takes for stars to noticeably move relative to each other.

There are 52 stellar systems beyond our own Solar system that currently lie within this 5 parsec distance. These systems contain a total of 63 stars, of which 50 are red dwarfs, by far the most common type of star in the Milky Way. Much more massive stars, such as our own, make up the remaining 13. In addition to these "true" stars, there are 11 brown dwarfs (objects not quite massive enough to fuse hydrogen), and 4 white dwarfs (extremely dense objects that remain after stars such as our Sun exhaust all fusable hydrogen in their core and slowly shed their outer layers while only the collapsed core remains). Despite the relative proximity of these objects to Earth, only nine exceed 6.5 apparent magnitude, meaning only about 12% of these objects can be observed with the naked eye. Besides the Sun, only three are first-magnitude stars: Alpha Centauri, Sirius, and Procyon. All of these objects are located in the Local Bubble, a region within the Orion–Cygnus Arm of the Milky Way.

Based on the latest Gaia DR2 astrometric results released in 2018 of stars brighter than 13.8G apparent magnitude, an estimated 694 solar-like or cooler stars will possibly approach the solar system to less than 5 parsecs over the next 15 million years. Of these, 26 have a good probability to come within 1.0 parsec and another 7 within 0.5 parsecs. The closest encounter to the Sun so far predicted is the low mass orange K7 spectral type star Gliese 710 / HIP 89825. Minimum perihelion distance is 0.0676±0.0157 parsecs or 13900±3200 AU about 1.281 million years from now. This is easily within the predicted maximum size of the Oort cloud, whose approach will likely be disruptive enough to affect the orbits of cometary bodies.

Stars visible to the unaided eye have their magnitude shown in ' below. Brown dwarfs are shown with their designations in '. White dwarfs are shown with their designation in . The classes of the stars and brown dwarfs are shown in the color of their spectral types (these colors are derived from conventional names for the spectral types and do not represent the star's observed color). Many brown dwarfs are not listed by visual magnitude but are listed by near-infrared J band apparent magnitude due to how dim (and often invisible) they are in visible colors. Some of the parallax and distance results are preliminary measurements.

Over long periods of time, the slow independent motion of stars change in both relative position and in their distance from the observer. This can cause other currently distant stars to fall within a stated range, which may be readily calculated and predicted using accurate astromertic measurements of parallax and total proper motions, along with spectroscopic determined radial velocities. Although predictions can be extrapolated back into the past or forward into the future, they are subject to increasing significant cumulative errors over very long periods. Inaccuracies of these measured parameters make determining the true minimum distances of any encountering stars or brown dwarfs fairly difficult.

Examples of notable predicted stellar encounters falling within 5 parsecs from the Sun appear in the list below. A summary of the more likely candidates include: 




</doc>
<doc id="28163" url="https://en.wikipedia.org/wiki?curid=28163" title="Sagitta">
Sagitta

Sagitta is a dim but distinctive constellation in the northern sky. Its name is Latin for "arrow", and it should not be confused with the significantly larger constellation Sagittarius, the archer. Although Sagitta is an ancient constellation, it has no star brighter than 3rd magnitude and has the third-smallest area of all constellations (only Equuleus and Crux are smaller). It was included among the 48 constellations listed by the 2nd century astronomer Ptolemy, and it remains one of the 88 modern constellations defined by the International Astronomical Union. Located to the north of the equator, Sagitta can be seen from every location on Earth except within the Antarctic circle.

The red giant Gamma Sagittae is the constellation's brightest star, with an apparent magnitude of 3.47. Two star systems have been found to have planets.
The Greeks who may have originally identified this constellation called it "Oistos". The Romans named it Sagitta. Sagitta's shape is reminiscent of an arrow, and many cultures have interpreted it thus, among them the Persians, Hebrews, Greeks and Romans. The Arabs called it "as-Sahm", a name that was transferred "Sham" and now refers to Alpha Sagittae only.

In ancient Greece, Sagitta was regarded as the weapon that Hercules used to kill the eagle (Aquila) of Jove that perpetually gnawed Prometheus' liver. The Arrow is located beyond the north border of Aquila, the Eagle. According to R.H. Allen, the Arrow could be the one shot by Hercules towards the adjacent Stymphalian birds (6th labor) who had claws, beaks and wings of iron, and who lived on human flesh in the marshes of Arcadia - Aquila the Eagle, Cygnus the Swan, and Lyra (the Vulture) - and still lying between them, whence the title Herculea (although Allen cites no reference to support this assertion). Eratosthenes claimed it as the arrow with which Apollo exterminated the Cyclopes.

Covering 79.9 square degrees and hence 0.194% of the sky, Sagitta ranks 86th of the 88 modern constellations by area. Its position in the Northern Celestial Hemisphere means that the whole constellation is visible to observers north of 69°S. It is bordered by Vulpecula to the north, Hercules to the west, Aquila to the south, and Delphinus to the east. The three-letter abbreviation for the constellation, as adopted by the International Astronomical Union in 1922, is 'Sge'. The official constellation boundaries, as set by Eugène Delporte in 1930, are defined by a polygon of twelve segments ("illustrated in infobox"). In the equatorial coordinate system, the right ascension coordinates of these borders lie between and , while the declination coordinates are between 16.08° and 21.64°.

Johann Bayer gave Bayer designations to eight stars, labelling them Alpha to Theta. What was viewed by Bayer, Friedrich Wilhelm Argelander and Heis as a single star Theta was in fact three stars, and is now equated to HR 7705. John Flamsteed added the letters x (mistaken as Chi), y and z to 13, 14 and 15 Sagittae in his Catalogus Britannicus. All three were dropped by Bevis and Baily.

In his "Uranometria", Bayer depicted Alpha, Beta and Epsilon Sagittae as the fins of the arrow. Also known as Sham, Alpha is a yellow bright giant star of spectral class G1 II (with an apparent magnitude of 4.38, which lies at a distance of 430 ± 10 light-years from Earth. Originally 4 times as massive as the Sun, it has swollen and brightened to 20 times its diameter and 340 times its luminosity. Also of magnitude 4.38, Beta is a G-type giant located 440 ± 20 light-years distant from Earth. Epsilon Sagittae is G8 III, 5.66m, multiple star (4 components; component B is optical)

Ptolemy saw the constellation's brightest star Gamma Sagittae as marking the arrow's head, while Bayer saw Gamma, Eta and Theta as depicting the arrow's shaft. Gamma Sagittae is a red giant of spectral type M0III, and magnitude 3.47. It lies at a distance of 258 ± 4 light-years from Earth. Eta Sagittae is a star of spectral class K2 III with a magnitude of 5.1, which belongs to the Hyades Stream. Theta Sagittae is a multiple star system.

Delta and Zeta depicted the spike according to Bayer. Delta Sagittae is a suspected visual double - M2 II+A0 V probably single image, composite spectrum), magnitude 3.82. Zeta Sagittae is a triple system, approximately 326 light-years from Earth, the primary an A-type star.

FG Sagittae is a remote highly luminous star around 8000 light years distant from Earth. WR 124 is a Wolf-Rayet star moving at great speed surrounded by a nebula of ejected gas. HD 183143 is a remote blue-white star that has been described as a blue supergiant or hypergiant. It is an Alpha Cygni variable.

R Sagittae is a member of the rare RV Tauri variable class of star. It ranges in magnitude from 8.2 to 10.4.
S Sagittae is a Classical Cepheid variable that varies from magnitude 5.24 to 6.04 in 8.38 days. It is a yellow-white supergiant that varies between spectral types F6Ib and G5Ib. Around 6 or 7 times as massive and 3500 time as luminous as the Sun, it is located around 2300 light-years away from Earth.

Located near 18 Sagittae is V Sagittae, the prototype of the V Sagittae variables, cataclysmic variables that are also super soft X-ray source.

WZ Sagittae is a dwarf nova composed of a white dwarf and low mass star companion. The black widow pulsar (B1957+20) is a millisecond pulsar with a brown dwarf companion.

HD 231701 is a yellow-white main sequence star hotter and larger than our Sun, which was found to have a planet in 2007. 15 Sagittae is a solar analog that has a brown dwarf substellar companion.

Messier 71 is a very loose globular cluster mistaken for quite some time for a dense open cluster. It lies at a distance of about 13,000 light-years from Earth and was first discovered by the French astronomer Philippe Loys de Chéseaux in the year 1745 or 1746.




</doc>
<doc id="28164" url="https://en.wikipedia.org/wiki?curid=28164" title="Simon Ockley">
Simon Ockley

Simon Ockley (16789 August 1720) was a British Orientalist.

Ockley was born at Exeter. He was educated at Queens' College, Cambridge, and graduated B.A. in 1697, MA. in 1701, and B.D. in 1710. He became fellow of Jesus College and vicar of Swavesey, and in 1711 was chosen Adams Professor of Arabic in the university. He had a large family, and his latter days were embittered by pecuniary embarrassments, which form the subject of a chapter in Isaac D'Israeli's "Calamities of Authors". The preface to the second volume of his "History of the Saracens" is dated from Cambridge Castle, where he lay a prisoner for debt.

Ockley maintained that a knowledge of Oriental literature was essential to the proper study of theology, and in the preface to his first book, the "Introductio ad linguas orientales" (1706), he urges the importance of the study.

He died at Swavesey.



</doc>
<doc id="28165" url="https://en.wikipedia.org/wiki?curid=28165" title="Sharable Content Object Reference Model">
Sharable Content Object Reference Model

Sharable Content Object Reference Model (SCORM) is a collection of standards and specifications for web-based electronic educational technology (also called e-learning). It defines communications between client side content and a host system (called "the run-time environment"), which is commonly supported by a learning management system. SCORM also defines how content may be packaged into a transferable ZIP file called "Package Interchange Format."

SCORM is a specification of the Advanced Distributed Learning (ADL) Initiative from the Office of the United States Secretary of Defense.

SCORM 2004 introduced a complex idea called sequencing, which is a set of rules that specifies the order in which a learner may experience content objects. In simple terms, they constrain a learner to a fixed set of paths through the training material, permit the learner to "bookmark" their progress when taking breaks, and assure the acceptability of test scores achieved by the learner. The standard uses XML, and it is based on the results of work done by AICC, IMS Global, IEEE, and Ariadne.

SCORM 1.1 is the first production version. It used a Course Structure Format XML file based on the AICC specifications to describe content structure, but lacked a robust packaging manifest and support for metadata. Quickly abandoned in favor of SCORM 1.2.

This was the first version that was widely used. It is still widely used and is supported by most Learning Management Systems.

This is the current version. It is based on new standards for API and content object-to-runtime environment communication, with many ambiguities of previous versions resolved. Includes ability to specify adaptive sequencing of activities that use the content objects. Includes ability to share and use information about the success status for multiple learning objectives or competencies across content objects and across courses for the same learner within the same learning management system. A more robust test suite helps ensure good interoperability.



The Experience API (also known as xAPI or Tin Can API) was finalized to version 1.0 in April 2013. The Experience API solves many of the problems inherent with older versions of SCORM. Just like SCORM, ADL is the steward of the Experience API. AICC with their cmi5 planned to use xAPI as their transport standard, but AICC membership decided to dissolve the organization and transferred cmi5 to ADL.

The Experience API (Tin Can API) is a web service that allows software clients to read and write experiential data in the form of “statement” objects. In their simplest form, statements are in the form of “I did this”, or more generally “actor verb object”. More complex statement forms can be used. There is also a built-in query API to help filter recorded statements, and a state API that allows for a sort of “scratch space” for consuming applications. Experience API statements are stored in a data store called a Learning Record Store, which can exist on its own or within a Learning Management System.


Server software
Client Interface (no GUI) 
Content editing software



</doc>
<doc id="28167" url="https://en.wikipedia.org/wiki?curid=28167" title="Sejm">
Sejm

The Sejm of the Republic of Poland (; ) is the lower house of the Polish parliament. It consists of 460 deputies (posłowie, literally "envoys", in Polish) elected by universal ballot and is presided over by a speaker called the "Marshal of the "Sejm" of the Republic of Poland" ("Marszałek Sejmu Rzeczypospolitej Polskiej"). In the Kingdom of Poland, ""Sejm"" referred to the entire three-chamber parliament of Poland, comprising the lower house (the Chamber of Envoys; ), the upper house (the Senate; Polish: "Senat") and the King. It was thus a three-estate parliament. Since the Second Polish Republic (1918–1939), ""Sejm"" has referred only to the lower house of the parliament; the upper house is called the "Senat Rzeczypospolitej Polskiej" ("Senate of the Republic of Poland").

""Sejm"" stems from an Old Slavic word meaning "gathering". Its origin was the King's Councils ("wiece"), which gained power during the time of Poland's fragmentation (1146–1295). The 1180 Sejm in Łęczyca (known as the 'First Polish parliament') was the most notable of these councils, in that for the first time in Poland's history it established laws constraining the power of the ruler. It forbade arbitrary sequestration of supplies in the countryside and takeover of bishopric lands after the death of a bishop. These early "Sejm"s were not a regular event, they convened at the King's behest. After the 1493 "Sejm" in Piotrków, it became a regularly convening body, to which indirect elections were held every two years. The bicameral system was also established there. The "Sejm" now comprised two chambers: the "Senat" (Senate) of 81 bishops and other dignitaries; and the Chamber of Envoys, made up of 54 envoys elected by smaller local "sejmik" (assemblies of landed nobility) in each of the Kingdom's provinces. At the time, Poland's nobility, which accounted for around 10% of the state's population (then the highest amount in Europe), was becoming particularly influential, and with the eventual development of the Golden Liberty, the "Sejm"'s powers increased dramatically.

 Over time, the envoys in the lower chamber grew in number and power as they pressed the king for more privileges. The "Sejm" eventually became even more active in supporting the goals of the privileged classes when the King ordered that the landed nobility and their estates (peasants) be drafted into military service. After the Union of Lublin in 1569, the Kingdom of Poland became, through personal union with the Grand Duchy of Lithuania, the Polish–Lithuanian Commonwealth, and thus the "Sejm" was supplemented with new envoys from among the Lithuanian nobility. This "Commonwealth of Both Nations" ensured that the state of affairs surrounding the three-estates system continued, with the "Sejm", Senate and King forming the estates and supreme deliberating body of the state. In the first few decades of the 16th century, the Senate had established its precedence over the "Sejm"; however, from the mid-1500s onwards, the "Sejm" became a very powerful representative body of the "Szlachta" ("middle nobility"). Its chambers reserved the final decisions in legislation, taxation, budget, and treasury matters (including military funding), foreign policy, and the confirment of nobility. The 1573 Warsaw Confederation saw the nobles of the "Sejm" officially sanction and guarantee religious tolerance in Commonwealth territory, ensuring a refuge for those fleeing the ongoing Reformation and Counter-Reformation wars in Europe. Until the end of the 16th century, unanimity was not required, and the majority-voting process was the most commonly used system for voting. Later, with the rise of the Polish magnates and their increasing power, the unanimity principle was re-introduced with the institution of the nobility's right of "liberum veto" (Latin: "I freely forbid"). Additionally, if the envoys were unable to reach a unanimous decision within six weeks (the time limit of a single session), deliberations were declared void and all previous acts passed by that "Sejm" were annulled. From the mid-17th century onward, any objection to a "Sejm" resolution, by either an envoy or a senator, automatically caused the rejection of other, previously approved resolutions. This was because all resolutions passed by a given session of the "Sejm" formed a whole resolution, and, as such, was published as the annual "constituent act" of the "Sejm", e.g. the ""Anno Domini" 1667" act. In the 16th century, no single person or small group dared to hold up proceedings, but, from the second half of the 17th century, the "liberum veto" was used to virtually paralyze the "Sejm", and brought the Commonwealth to the brink of collapse. The "liberum veto" was finally abolished with the adoption of Poland's 3rd May Constitution in 1791, a piece of legislation which was passed as the "Government Act", and for which the "Sejm" required four years to propagate and adopt. The constitution's acceptance, and the possible long-term consequences it may have had, is arguably the reason for which the powers of Austria-Hungary, Russia and Prussia then decided to partition the Polish-Lithuanian Commonwealth, thus putting an end to over 300 years of Polish parliamentary continuity. It is estimated that between 1493 and 1793, a "Sejm" was held 240 times, the total debate-time sum of which was 44 years.

After the fall of the Duchy of Warsaw, which existed as a Napoleonic client state between 1807 and 1815, and its short-lived "Sejm" of the Duchy of Warsaw, the "Sejm" of Congress Poland was established in the "Kongresówka" (Congress Poland) of Russia; it was composed of the king (the Russian emperor), the upper house (Senate), and the lower house (Chamber of Envoys). Overall, during the period from 1795 until the re-establishment of Poland's sovereignty in 1918, little power was actually held by any Polish legislative body and the occupying powers of Russia, Prussia (later united Germany) and Austria-Hungary propagated legislation for their own respective formerly-Polish territories at a national level.

The Chamber of Envoys, despite its name, consisted not only of 77 envoys (sent by local assemblies) from the hereditary nobility, but also of 51 deputies, elected by the non-noble population. All deputies were covered by Parliamentary immunity, with each individual serving for a term of office of six years, with half of the deputies being elected every two years. Candidates for deputy had to be able to read and write, and have a certain amount of wealth. The legal voting age was 21, except for those citizens serving in the military, the personnel of which were not allowed to vote. Parliamentary sessions were initially convened every two years, and lasted for (at least) 30 days. However, after many clashes between liberal deputies and conservative government officials, sessions were later called only four times (1818, 1820, 1826, and 1830, with the last two sessions being secret). The "Sejm" had the right to call for votes on civil and administrative legal issues, and, with permission from the king, it could also vote on matters related to the fiscal policy and the military. It had the right to exercise control over government officials, and to file petitions. The 64-member Senate on the other hand, was composed of "voivodes" and "kasztelans" (both types of provincial governors), Russian "princes of the blood", and nine bishops. It acted as the Parliamentary Court, had the right to control "citizens' books", and had similar legislative rights as did the Chamber of Deputies.

In the Free City of Cracow (1815–1846), a unicameral Assembly of Representatives was established, and from 1827, a unicameral provincial "sejm" existed in the Grand Duchy of Poznań. Poles were elected to and represented the majority in both of these legislatures; however, they were largely powerless institutions and exercised only very limited power. After numerous failures in securing legislative sovereignty in the early 19th century, many Poles simply gave up trying to attain a degree of independence from their foreign master-states. In the Austrian partition, a relatively powerless "Sejm" of the Estates operated until the time of the Spring of Nations. After this, in the mid to late 19th century, only in autonomous Galicia (1861–1914) was there a unicameral and functional National "Sejm", the "Sejm" of the Land. It is recognised today as having played a major and overwhelming positive role in the development of Polish national institutions.

In the second half of the 19th century, Poles were able to become members of the parliaments of Austria, Prussia and Russia, where they formed Polish Clubs. Deputies of Polish nationality were elected to the Prussian "Landtag" from 1848, and then to the German Empire's "Reichstag" from 1871. Polish Deputies were members of the Austrian State Council (from 1867), and from 1906 were also elected to the Russian Imperial State "Duma" (lower chamber) and to the State Council (upper chamber).

After the First World War and re-establishment of Polish independence, the convocation of parliament, under the democratic electoral law of 1918, became an enduring symbol of the new state's wish to demonstrate and establish continuity with the 300-year Polish parliamentary traditions established before the time of the partitions. Maciej Rataj emphatically paid tribute to this with the phrase: "There is Poland there, and so is the "Sejm"".

During the interwar period of Poland's independence, the first Legislative "Sejm" of 1919, a Constituent Assembly, passed the Small Constitution of 1919, which introduced a parliamentary republic and proclaimed the principle of the "Sejm"'s sovereignty. This was then strengthened, in 1921, by the March Constitution, one of the most democratic European constitutions enacted after the end of World War I. The constitution established a political system which was based on Montesquieu's doctrine of separation of powers, and which restored the bicameral "Sejm" consisting of a lower house (to which alone the name of ""Sejm"" was from then on applied) and an upper house, the Senate. In 1919, Roza Pomerantz-Meltzer, a member of the Zionist party, became the first woman ever elected to the "Sejm".

The legal content of the March Constitution allowed for "Sejm" supremacy in the system of state institutions at the expense of the executive powers, thus creating a parliamentary republic out of the Polish state. An attempt to strengthen executive powers in 1926 (through the August Amendment) proved too limited and largely failed in helping avoid legislative grid-lock which had ensued as a result of too-great parliamentary power in a state which had numerous diametrically-opposed political parties sitting in its legislature. In 1935, the parliamentary republic was weakened further when, by way of, Józef Piłsudski's May Coup, the president was forced to sign the April Constitution of 1935, an act through which the head of state assumed the dominant position in legislating for the state and the Senate increased its power at the expense of the "Sejm".

On 2 September 1939, the "Sejm" held its final pre-war session, during which it declared Poland's readiness to defend itself against invading German forces. On 2 November 1939, the President dissolved the "Sejm" and the Senate, which were then, according to plan, to resume their activity within two months after the end of the Second World War; this, however, never happened. During wartime, the National Council (1939–1945) was established to represent the legislature as part of the Polish Government in Exile. Meanwhile, in Nazi-occupied Poland, the Council of National Unity was set up; this body functioned from 1944 to 1945 as the parliament of the Polish Underground State. With the cessation of hostilities in 1945, and subsequent rise to power of the Communist-backed Provisional Government of National Unity, the Second Polish Republic legally ceased to exist.

The "Sejm" in the Polish People's Republic had 460 deputies throughout most of its history. At first, this number was declared to represent one deputy per 60,000 citizens (425 were elected in 1952), but, in 1960, as the population grew, the declaration was changed: The constitution then stated that the deputies were representative "of" the people and could be recalled "by" the people, but this article was never used, and, instead of the "five-point electoral law", a non-proportional, "four-point" version was used. Legislation was passed with majority voting.

Under the 1952 Constitution, the Sejm was defined as "the highest organ of State authority" in Poland, as well as "the highest spokesman of the will of the people in town and country." On paper, it was vested with great lawmaking and oversight powers. For instance, it was empowered with control over "the functioning of other organs of State authority and administration," and ministers were required to answer questions posed by deputies within seven days. In practice, it did little more than rubber-stamp decisions already made by the Communist Polish United Workers Party and its executive bodies. This was standard practice in nearly all Communist regimes due to the principle of democratic centralism.

The "Sejm" voted on the budget and on the periodic national plans that were a fixture of communist economies. The "Sejm" deliberated in sessions that were ordered to convene by the State Council. 

The "Sejm" also chose a "Prezydium" ("presiding body") from among its members; the marshall of which was always a member of the United People's Party. In its preliminary session, the "Sejm" also nominated the Prime Minister, the Council of Ministers of Poland, and members of the State Council. It also chose many other government officials, including the head of the Supreme Chamber of Control and members of the State Tribunal and the Constitutional Tribunal, as well as the Ombudsman (the last three bodies of which were created in the 1980s).

When the Sejm was not in session, the State Council had the power to issue decrees that had the force of law. However, those decrees had to be approved by the Sejm at its next session.

The Senate of Poland was abolished by the Polish people's referendum in 1946, after which the "Sejm" became the sole legislative body in Poland. Even though the "Sejm" was largely subservient to the Communist party, one brave deputy, Romuald Bukowski (an independent) voted against the imposition of martial law in 1982.

After the end of communism in 1989, the Senate was reinstated as the upper house of a bicameral national assembly, while the "Sejm" became the lower house. The "Sejm" is now composed of 460 deputies elected by proportional representation every four years.

Between 7 and 19 deputies are elected from each constituency using the d'Hondt method (with one exception, in 2001, when the Sainte-Laguë method was used), their number being proportional to their constituency's population. Additionally, a threshold is used, so that candidates are chosen only from parties that gained at least 5% of the nationwide vote (candidates from ethnic-minority parties are exempt from this threshold).






</doc>
<doc id="28168" url="https://en.wikipedia.org/wiki?curid=28168" title="Stock exchange">
Stock exchange

A stock exchange, securities exchange or bourse, is a facility where stock brokers and traders can buy and sell securities, such as shares of stock and bonds and other financial instruments. Stock exchanges may also provide facilities for the issue and redemption of such securities and instruments and capital events including the payment of income and dividends. Securities traded on a stock exchange include stock issued by listed companies, unit trusts, derivatives, pooled investment products and bonds. Stock exchanges often function as "continuous auction" markets with buyers and sellers consummating transactions at a central location such as the floor of the exchange. Many stock exchanges today use electronic trading, in place of the traditional floor trading.

To be able to trade a security on a certain stock exchange, the security must be listed there. Usually, there is a central location at least for record keeping, but trade is increasingly less linked to a physical place, as modern markets use electronic networks, which give them advantages of increased speed and reduced cost of transactions. Trade on an exchange is restricted to brokers who are members of the exchange. In recent years, various other trading venues, such as electronic communication networks, alternative trading systems and "dark pools" have taken much of the trading activity away from traditional stock exchanges.

Initial public offerings of stocks and bonds to investors is done in the primary market and subsequent trading is done in the secondary market. A stock exchange is often the most important component of a stock market. Supply and demand in stock markets are driven by various factors that, as in all free markets, affect the price of stocks (see stock valuation).

There is usually no obligation for stock to be issued through the stock exchange itself, nor must stock be subsequently traded on an exchange. Such trading may be "off exchange" or over-the-counter. This is the usual way that derivatives and bonds are traded. Increasingly, stock exchanges are part of a global securities market. Stock exchanges also serve an economic function in providing liquidity to shareholders in providing an efficient means of disposing of shares.

The idea of debt dates back to the ancient world, as evidenced for example by ancient Mesopotamian city clay tablets recording interest-bearing loans. There is little consensus among scholars as to when corporate stock was first traded. Some see the key event as the Dutch East India Company's founding in 1602, while others point to earlier developments. Economist Ulrike Malmendier of the University of California at Berkeley argues that a share market existed as far back as ancient Rome. One of Europe's oldest stock exchanges is the Frankfurt Stock Exchange () established in 1585 in Frankfurt am Main.

In the Roman Republic, which existed for centuries before the Empire was founded, there were "societates publicanorum", organizations of contractors or leaseholders who performed temple-building and other services for the government. One such service was the feeding of geese on the Capitoline Hill as a reward to the birds after their honking warned of a Gallic invasion in 390 B.C. Participants in such organizations had "partes" or shares, a concept mentioned various times by the statesman and orator Cicero. In one speech, Cicero mentions "shares that had a very high price at the time." Such evidence, in Malmendier's view, suggests the instruments were tradable, with fluctuating values based on an organization's success. The "societas" declined into obscurity in the time of the emperors, as most of their services were taken over by direct agents of the state.

Tradable bonds as a commonly used type of security were a more recent innovation, spearheaded by the Italian city-states of the late medieval and early Renaissance periods.

While the Italian city-states produced the first transferable government bonds, they did not develop the other ingredient necessary to produce a fully-fledged capital market: the stock market in its modern sense. In the early 1600s the Dutch East India Company (VOC) became the first company in history to issue bonds and shares of stock to the general public. As Edward Stringham (2015) notes, "companies with transferable shares date back to classical Rome, but these were usually not enduring endeavors and no considerable secondary market existed (Neal, 1997, p. 61)." The VOC, formed to build up the spice trade, operated as a colonial ruler in what is now Indonesia and beyond, a purview that included conducting military operations against the wishes of the exploited natives and of competing colonial powers. Control of the company was held tightly by its directors, with ordinary shareholders not having much influence on management or even access to the company's accounting statements.
However, shareholders were rewarded well for their investment. The company paid an average dividend of over 16 percent per year from 1602 to 1650. Financial innovation in Amsterdam took many forms. In 1609 investors led by one Isaac Le Maire formed history's firstbear syndicate, but their coordinated trading had only a modest impact in driving down share prices, which tended to remain robust throughout the 17th century. By the 1620s the company was expanding its securities issuance with the first use of corporate bonds.

Joseph de la Vega, also known as Joseph Penso de la Vega and by other variations of his name, was an Amsterdam trader from a Spanish Jewish family and a prolific writer as well as a successful businessman in 17th-century Amsterdam. His 1688 book "Confusion of Confusions" explained the workings of the city's stock market. It was the earliest book about stock trading and inner workings of a stock market, taking the form of a dialogue between a merchant, a shareholder and a philosopher, the book described a market that was sophisticated but also prone to excesses, and de la Vega offered advice to his readers on such topics as the unpredictability of market shifts and the importance of patience in investment.

In England, King William III sought to modernize the kingdom's finances to pay for its wars, and thus the first government bonds were issued in 1693 and the Bank of England was set up the following year. Soon thereafter, English joint-stock companies began going public.
London's first stockbrokers, however, were barred from the old commercial center known as the Royal Exchange, reportedly because of their rude manners. Instead, the new trade was conducted from coffee houses along Exchange Alley. By 1698 a broker named John Castaing, operating out of Jonathan's Coffee House, was posting regular lists of stock and commodity prices. Those lists mark the beginning of the London Stock Exchange.

One of history's greatest financial bubbles occurred in the next few decades. At the center of it were the South Sea Company, set up in 1711 to conduct English trade with South America, and the Mississippi Company, focused on commerce with France's Louisiana colony and touted by transplanted Scottish financier John Law, who was acting in effect as France's central banker. Investors snapped up shares in both, and whatever else was available. In 1720, at the height of the mania, there was even an offering of "a company for carrying out an undertaking of great advantage, but nobody to know what it is".

By the end of that same year, share prices had started collapsing, as it became clear that expectations of imminent wealth from the Americas were overblown. In London, Parliament passed the Bubble Act, which stated that only royally chartered companies could issue public shares. In Paris, Law was stripped of office and fled the country. Stock trading was more limited and subdued in subsequent decades. Yet the market survived, and by the 1790s shares were being traded in the young United States.

Stock exchanges have multiple roles in the economy. This may include the following:

A stock exchange provides companies with the facility to raise capital for expansion through selling shares to the investing public.

Besides the borrowing capacity provided to an individual or firm by the banking system, in the form of credit or a loan, there are four common forms of capital raising used by companies and entrepreneurs. Most of these available options might be achieved, directly or indirectly, through a stock exchange.

Capital intensive companies, particularly high tech companies, always need to raise high volumes of capital in their early stages. For this reason, the public market provided by the stock exchanges has been one of the most important funding sources for many capital intensive startups. After the 1990s and early-2000s hi-tech listed companies' boom and bust in the world's major stock exchanges, it has been much more demanding for the high-tech entrepreneur to take his/her company public, unless either the company already has products in the market and is generating sales and earnings, or the company has completed advanced promising clinical trials, earned potentially profitable patents or conducted market research which demonstrated very positive outcomes. This is quite different from the situation of the 1990s to early-2000s period, when a number of companies (particularly Internet boom and biotechnology companies) went public in the most prominent stock exchanges around the world, in the total absence of sales, earnings and any well-documented promising outcome. Anyway, every year a number of companies, including unknown highly speculative and financially unpredictable hi-tech startups, are listed for the first time in all the major stock exchanges – there are even specialized entry markets for these kind of companies or stock indexes tracking their performance (examples include the Alternext, CAC Small, SDAX, TecDAX, or most of the third market good companies).

A number of companies have also raised significant amounts of capital through R&D limited partnerships. Tax law changes that were enacted in 1987 in the United States changed the tax deductibility of investments in R&D limited partnerships. In order for a partnership to be of interest to investors today, the cash on cash return must be high enough to entice investors.

A third usual source of capital for startup companies has been venture capital. This source remains largely available today, but the maximum statistical amount that the venture company firms in aggregate will invest in any one company is not limitless (it was approximately $15 million in 2001 for a biotechnology company).

A fourth alternative source of cash for a private company is a corporate partner, usually an established multinational company, which provides capital for the smaller company in return for marketing rights, patent rights, or equity. Corporate partnerships have been used successfully in a large number of cases.

When people draw their savings and invest in shares (through an IPO or the issuance of new company shares of an already listed company), it usually leads to rational allocation of resources because funds, which could have been consumed, or kept in idle deposits with banks, are mobilized and redirected to help companies' management boards finance their organizations. This may promote business activity with benefits for several economic sectors such as agriculture, commerce and industry, resulting in stronger economic growth and higher productivity levels of firms.

Companies view acquisitions as an opportunity to expand product lines, increase distribution channels, hedge against volatility, increase their market share, or acquire other necessary business assets. A takeover bid or a merger agreement through the stock market is one of the simplest and most common ways for a company to grow by acquisition or fusion.

Both casual and professional stock investors, as large as institutional investors or as small as an ordinary middle-class family, through dividends and stock price increases that may result in capital gains, share in the wealth of profitable businesses. Unprofitable and troubled businesses may result in capital losses for shareholders.

By having a wide and varied scope of owners, companies generally tend to improve management standards and efficiency to satisfy the demands of these shareholders and the more stringent rules for public corporations imposed by public stock exchanges and the government. Consequently, it is alleged that public companies (companies that are owned by shareholders who are members of the general public and trade shares on public exchanges) tend to have better management records than privately held companies (those companies where shares are not publicly traded, often owned by the company founders, their families and heirs, or otherwise by a small group of investors).

Despite this claim, some well-documented cases are known where it is alleged that there has been considerable slippage in corporate governance on the part of some public companies. The dot-com bubble in the late 1990s, and the subprime mortgage crisis in 2007–08, are classical examples of corporate mismanagement. Companies like Pets.com (2000), Enron (2001), One.Tel (2001), Sunbeam (2001), Webvan (2001), Adelphia (2002), MCI WorldCom (2002), Parmalat (2003), American International Group (2008), Bear Stearns (2008), Lehman Brothers (2008), General Motors (2009) and Satyam Computer Services (2009) were among the most widely scrutinized by the media.

To assist in corporate governance many banks and companies worldwide utilize securities identification numbers (USIN) to identify, uniquely, their stocks, bonds and other securities. Adding an ISIN code helps to distinctly identify securities and the ISIN system is used worldwide by funds, companies, and governments.

However, when poor financial, ethical or managerial records are known by the stock investors, the stock and the company tend to lose value. In the stock exchanges, shareholders of underperforming firms are often penalized by significant share price decline, and they tend as well to dismiss incompetent management teams.

As opposed to other businesses that require huge capital outlay, investing in shares is open to both the large and small stock investors because a person buys the number of shares they can afford. Therefore, the Stock Exchange provides the opportunity for small investors to own shares of the same companies as large investors.

Governments at various levels may decide to borrow money to finance infrastructure projects such as sewage and water treatment works or housing estates by selling another category of securities known as bonds. These bonds can be raised through the stock exchange whereby members of the public buy them, thus loaning money to the government. The issuance of such bonds can obviate, in the short term, direct taxation of citizens to finance development—though by securing such bonds with the full faith and credit of the government instead of with collateral, the government must eventually tax citizens or otherwise raise additional funds to make any regular coupon payments and refund the principal when the bonds mature.

At the stock exchange, share prices rise and fall depending, largely, on economic forces. Share prices tend to rise or remain stable when companies and the economy in general show signs of stability and growth. An economic recession, depression, or financial crisis could eventually lead to a stock market crash. Therefore, the movement of share prices and in general of the stock indexes can be an indicator of the general trend in the economy.

Each stock exchange imposes its own listing requirements upon companies that want to be listed on that exchange. Such conditions may include minimum number of shares outstanding, minimum market capitalization, and minimum annual income.

The listing requirements imposed by some stock exchanges include:

Stock exchanges originated as mutual organizations, owned by its member stock brokers. There has been a recent trend for stock exchanges to "demutualize", where the members sell their shares in an initial public offering. In this way the mutual organization becomes a corporation, with shares that are listed on a stock exchange. Examples are Australian Securities Exchange (1998), Euronext (merged with New York Stock Exchange), NASDAQ (2002), Bursa Malaysia (2004), the New York Stock Exchange (2005), Bolsas y Mercados Españoles, and the São Paulo Stock Exchange (2007).
The Shenzhen and Shanghai stock exchanges can be characterized as quasi-state institutions insofar as they were created by government bodies in China and their leading personnel are directly appointed by the China Securities Regulatory Commission.
Another example is Tashkent republican stock exchange (Uzbekistan) established in 1994, three years after the collapse of the Soviet Union, mainly state-owned but has a form of a public corporation (joint stock company). According to an Uzbek government decision (March 2012) 25 percent minus one share of Tashkent stock exchange was expected to be sold to Korea Exchange(KRX) in 2014.

In the 19th century, exchanges were opened to trade forward contracts on commodities. Exchange traded forward contracts are called futures contracts. These "commodity exchanges" later started offering future contracts on other products, such as interest rates and shares, as well as options contracts. They are now generally known as futures exchanges.


Lists:



</doc>
<doc id="28170" url="https://en.wikipedia.org/wiki?curid=28170" title="Son of God">
Son of God

Historically, many rulers have assumed titles such as son of God, son of a god or son of heaven.

The term "son of God" is sometimes used in the Old and New Testaments of the Christian Bible to refer to those with special relationships with God. In the Old Testament, angels, just and pious men, and the kings of Israel are all called "sons of God." In the New Testament, Adam, and, most notably, Jesus Christ are called "son of God," while followers of Jesus are called, "sons of God."

In the New Testament, "Son of God" is applied to Jesus on many occasions. Jesus is declared to be the Son of God on two separate occasions by a voice speaking from Heaven. Jesus is also explicitly and implicitly described as the Son of God by himself and by various individuals who appear in the New Testament. As applied to Jesus, the term is a reference to his role as the Messiah, the King chosen by God. The contexts and ways in which Jesus' title, Son of God, means something more than or other than Messiah remain the subject of ongoing scholarly study and discussion.

The term "Son of God" should not be confused with the term "God the Son" (), the second Person of the Trinity in Christian theology. The doctrine of the Trinity identifies Jesus as God the Son, "identical in essence but distinct in person" with regard to God the Father and God the Holy Spirit (the first and third Persons of the Trinity). Nontrinitarian Christians accept the application to Jesus of the term "Son of God", which is found in the New Testament, but not the term "God the Son", which is not found there.

Throughout history, emperors and rulers ranging from the Western Zhou dynasty (c. 1000 BC) in China to Alexander the Great (c. 360 BC) to the Emperor of Japan (c. 600 AD) have assumed titles that reflect a filial relationship with deities.

The title "Son of Heaven" i.e. 天子 (from 天 meaning sky/heaven/god and 子 meaning child) was first used in the Western Zhou dynasty (c. 1000 BC). It is mentioned in the Shijing book of songs, and reflected the Zhou belief that as Son of Heaven (and as its delegate) the Emperor of China was responsible for the well being of the whole world by the Mandate of Heaven. This title may also be translated as "son of God" given that the word "Ten" or "Tien" in Chinese may either mean sky or god. The Emperor of Japan was also called the Son of Heaven (天子 "tenshi") starting in the early 7th century.

Among the Steppe Peoples, there was also a widespread use of "Son of God/Son of Heaven" for instance, in the Third Century B.C., the ruler was called Chanyü and similar titles were used as late as the 13th Century by Genghis Khan.

Examples of kings being considered the son of god are found throughout the Ancient Near East. Egypt in particular developed a long lasting tradition. Egyptian pharaohs are known to have been referred to as the son of a particular god and their begetting in some cases is even given in sexually explicit detail. Egyptian pharaohs did not have full parity with their divine fathers but rather were subordinate. Nevertheless, in the first four dynasties, the pharaoh was considered to be the embodiment of a god. Thus, Egypt was ruled by direct theocracy, wherein "God himself is recognized as the head" of the state. During the later Amarna Period, Akhenaten reduced the Pharaoh's role to one of coregent, where the Pharaoh and God ruled as father and son. Akhenaten also took on the role of the priest of god, eliminating representation on his behalf by others. Later still, the closest Egypt came to the Jewish variant of theocracy was during the reign of Herihor. He took on the role of ruler not as a god but rather as a high-priest and king.
Jewish kings are also known to have been referred to as "son of the ". The Jewish variant of theocracy can be thought of as a representative theocracy where the king is viewed as God’s surrogate on earth. Jewish kings thus, had less of a direct connection to god than pharaohs. Unlike pharaohs, Jewish kings rarely acted as priests, nor were prayers addressed directly to them. Rather, prayers concerning the king are addressed directly to god. The Jewish philosopher Philo is known to have likened God to a supreme king, rather than likening Jewish kings to gods.

Based on the Bible, several kings of Damascus took the title son of Hadad. From the archaeological record a stela erected by Bar-Rakib for his father Panammuwa II contains similar language. The son of Panammuwa II a king of Sam'al referred to himself as a son of Rakib. Rakib-El is a god who appears in Phoenician and Aramaic inscriptions. Panammuwa II died unexpectedly while in Damascus. However, his son the king Bar-Rakib was not a native of Damascus but rather the ruler of Sam'al it is unknown if other rules of Sam'al used similar language.

In Greek mythology, Heracles (son of Zeus) and many other figures were considered to be sons of gods through union with mortal women. From around 360 BC onwards Alexander the Great may have implied he was a demigod by using the title "Son of Ammon–Zeus".
In 42 BC, Julius Caesar was formally deified as "the divine Julius" ("divus Iulius") after his assassination. His adopted son, Octavian (better known as Augustus, a title given to him 15 years later, in 27 BC) thus became known as "divi Iuli filius" (son of the divine Julius) or simply "divi filius" (son of the god). As a daring and unprecedented move, Augustus used this title to advance his political position in the Second Triumvirate, finally overcoming all rivals for power within the Roman state.

The word applied to Julius Caesar as deified was "divus", not the distinct word "deus". Thus Augustus called himself "Divi filius", and not "Dei filius". The line between been god and god-like was at times less than clear to the population at large, and Augustus seems to have been aware of the necessity of keeping the ambiguity. As a purely semantic mechanism, and to maintain ambiguity, the court of Augustus sustained the concept that any worship given to an emperor was paid to the "position of emperor" rather than the person of the emperor. However, the subtle semantic distinction was lost outside Rome, where Augustus began to be worshiped as a deity. The inscription DF thus came to be used for Augustus, at times unclear which meaning was intended. The assumption of the title "Divi filius" by Augustus meshed with a larger campaign by him to exercise the power of his image. Official portraits of Augustus made even towards the end of his life continued to portray him as a handsome youth, implying that miraculously, he never aged. Given that few people had ever seen the emperor, these images sent a distinct message.

Later, Tiberius (emperor from 14–37 AD) came to be accepted as the son of "divus Augustus" and Hadrian as the son of "divus Trajan". By the end of the 1st century, the emperor Domitian was being called "dominus et deus" (i.e. "master and god").

Outside the Roman Empire, the 2nd century Kushan King Kanishka I used the title "devaputra" meaning "son of God".

Although references to "sons of God", "son of God" and "son of the " are occasionally found in Jewish literature, they never refer to physical descent from God. There are two instances where Jewish kings are figuratively referred to as a god. The king is likened to the supreme king God. These terms are often used in the general sense in which the Jewish people were referred to as "children of the your God".

When used by the rabbis, the term referred to Israel or to human beings in general, and not as a reference to the Jewish mashiach. In Judaism the term "mashiach" has a broader meaning and usage and can refer to a wide range of people and objects, not necessarily related to the Jewish eschaton.

Gabriel's Revelation, also called the Vision of Gabriel or the Jeselsohn Stone, is a three-foot-tall (one metre) stone tablet with 87 lines of Hebrew text written in ink, containing a collection of short prophecies written in the first person and dated to the late 1st century BC. It is a tablet described as a "Dead Sea scroll in stone".

The text seems to talk about a messianic figure from Ephraim who broke evil before righteousness by three days. Later the text talks about a “prince of princes" a leader of Israel who was killed by the evil king and not properly buried. The evil king was then miraculously defeated. The text seems to refer to Jeremiah Chapter 31. The choice of Ephraim as the lineage of the messianic figure described in the text seems to draw on passages in Jeremiah, Zechariah and Hosea. This leader was referred to as a son of God.

The text seems to be based on a Jewish revolt recorded by Josephus dating from 4 BC. Based on its dating the text seems to refer to Simon of Peraea, one of the three leaders of this revolt.

In some versions of Deuteronomy the Dead Sea Scrolls refer to the sons of God rather than the sons of Israel, probably in reference to angels. The Septuagint reads similarly.

4Q174 is a midrashic text in which God refers to the Davidic messiah as his son.

4Q246 refers to a figure who will be called the son of God and son of the Most High. It is debated if this figure represents the royal messiah, a future evil gentile king or something else.

In 11Q13 Melchizedek is referred to as god the divine judge. Melchizedek in the bible was the king of Salem. At least some in the Qumran community seemed to think that at the end of days Melchizedek would reign as their king. The passage is based on Psalm 82.

In both Joseph and Aseneth and the related text The Story of Asenath, Joseph is referred to as the son of God. In the Prayer of Joseph both Jacob and the angel are referred to as angels and the sons of God.

This style of naming is also used for some rabbis in the Talmud.

In Christianity, the title Son of God refers to the status of Jesus as the divine son of God the Father. It derives from several uses in the New Testament and early Christian theology. The terms "son of God" and "son of the " are found in several passages of the Old Testament as well, but generally do not refer to Jesus or the Trinity directly.

In Islam, Jesus is known as "Īsā ibn Maryam" (), and is understood to be the penultimate prophet and messenger of God (Allah) and "al-Masih", the Arabic term for Messiah (Christ), sent to guide the Children of Israel ("banī isrā'īl" in Arabic) with a new revelation, the "al-Injīl" (Arabic for "the gospel"). 

Islam completely denies that Jesus was the begotten son of God (Allah) or "part of" God (Allah). As in Christianity, Islam believes Jesus had no earthly father. In Islam Jesus is believed to be born due to the command of God (Allah) "be". God (Allah) ordered the angel Jibrīl (Holy Spirit) to "blow" the soul of Jesus into Mary and so she gave birth to Jesus.

In the writings of the Bahá'í Faith, the term "Son of God" is applied to Jesus, but does not indicate a literal physical relationship between Jesus and God, but is symbolic and is used to indicate the very strong spiritual relationship between Jesus and God and the source of his authority. Shoghi Effendi, the head of the Bahá'í Faith in the first half of the 20th century, also noted that the term does not indicate that the station of Jesus is superior to other prophets and messengers that Bahá'ís name Manifestations of God, including Buddha, Muhammad and Baha'u'llah among others. Shoghi Effendi notes that, since all Manifestations of God share the same intimate relationship with God and reflect the same light, the term Sonship can in a sense be attributable to all the Manifestations.




</doc>
<doc id="28171" url="https://en.wikipedia.org/wiki?curid=28171" title="SA">
SA

Sa, SA or S.A. may refer to:













</doc>
<doc id="28172" url="https://en.wikipedia.org/wiki?curid=28172" title="Saint Boniface">
Saint Boniface

Saint Boniface (; 675 – 5 June 754 AD), born Winfrid (also spelled Winifred, Wynfrith, Winfrith or Wynfryth) in the kingdom of Wessex in Anglo-Saxon England, was a leading figure in the Anglo-Saxon mission to the Germanic parts of the Frankish Empire during the 8th century. He organized Christianity in many parts of Germania and was made archbishop of Mainz by Pope Gregory III. He was martyred in Frisia in 754, along with 52 others, and his remains were returned to Fulda, where they rest in a sarcophagus which became a site of pilgrimage. Boniface's life and death as well as his work became widely known, there being a wealth of material available—a number of "vitae", especially the near-contemporary "Vita Bonifatii auctore Willibaldi", legal documents, possibly some sermons, and above all his correspondence. He became the patron saint of Germania, known as the "Apostle of the Germans".

Norman F. Cantor notes the three roles Boniface played that made him "one of the truly outstanding creators of the first Europe, as the apostle of Germania, the reformer of the Frankish church, and the chief fomentor of the alliance between the papacy and the Carolingian family." Through his efforts to reorganize and regulate the church of the Franks, he helped shape Western Christianity, and many of the dioceses he proposed remain today. After his martyrdom, he was quickly hailed as a saint in Fulda and other areas in Germania and in England. He is still venerated strongly today by German Catholics. Boniface is celebrated (and criticized) as a missionary; he is regarded as a unifier of Europe, and he is seen (mainly by Catholics) as a Germanic national figure.

The earliest Bonifacian "vita", Willibald's, does not mention his place of birth but says that at an early age he attended a monastery ruled by Abbot Wulfhard in "escancastre", or "Examchester", which seems to denote Exeter, and may have been one of many "monasteriola" built by local landowners and churchmen; nothing else is known of it outside the Bonifacian "vitae". This monastery is believed to have occupied the site of the Church of St Mary Major in the City of Exeter, demolished in 1971, next to which was later built Exeter Cathedral. Later tradition places his birth at Crediton, but the earliest mention of Crediton in connection to Boniface is from the early fourteenth century, in John Grandisson's "Legenda Sanctorum: The Proper Lessons for Saints' Days according to the use of Exeter". In one of his letters Boniface mentions he was "born and reared...[in] the synod of London", but he may have been speaking metaphorically.

According to the "vitae", Winfrid was of a respected and prosperous family. Against his father's wishes he devoted himself at an early age to the monastic life. He received further theological training in the Benedictine monastery and minster of Nhutscelle (Nursling), not far from Winchester, which under the direction of abbot Winbert had grown into an industrious centre of learning in the tradition of Aldhelm. Winfrid taught in the abbey school and at the age of 30 became a priest; in this time, he wrote a Latin grammar, the "Ars Grammatica", besides a treatise on verse and some Aldhelm-inspired riddles. While little is known about Nursling outside of Boniface's "vitae", it seems clear that the library there was significant. In order to supply Boniface with the materials he needed, it would have contained works by Donatus, Priscian, Isidore, and many others. Around 716, when his abbot Wynberth of Nursling died, he was invited (or expected) to assume his position—it is possible that they were related, and the practice of hereditary right among the early Anglo-Saxons would affirm this. Winfrid, however, declined the position and in 716 set out on a missionary expedition to Frisia.

Boniface first left for the continent in 716. He traveled to Utrecht, where Willibrord, the "Apostle of the Frisians," had been working since the 690s. He spent a year with Willibrord, preaching in the countryside, but their efforts were frustrated by the war then being carried on between Charles Martel and Radbod, King of the Frisians. Willibrord fled to the abbey he had founded in Echternach (in modern-day Luxembourg) while Boniface returned to Nursling.

Boniface returned to the continent the next year and went straight to Rome, where Pope Gregory II renamed him "Boniface", after the (legendary) fourth-century martyr Boniface of Tarsus, and appointed him missionary bishop for Germania—he became a bishop without a diocese for an area that lacked any church organization. He would never return to England, though he remained in correspondence with his countrymen and kinfolk throughout his life.

According to the "vitae" Boniface felled the Donar Oak, Latinized by Willibald as "Jupiter's oak," near the present-day town of Fritzlar in northern Hesse. According to his early biographer Willibald, Boniface started to chop the oak down, when suddenly a great wind, as if by miracle, blew the ancient oak over. When the god did not strike him down, the people were amazed and converted to Christianity. He built a chapel dedicated to Saint Peter from its wood at the site—the chapel was the beginning of the monastery in Fritzlar. This account from the "vita" is stylized to portray Boniface as a singular character who alone acts to root out paganism. Lutz von Padberg and others point out that what the "vitae" leave out is that the action was most likely well-prepared and widely publicized in advance for maximum effect, and that Boniface had little reason to fear for his personal safety since the Frankish fortified settlement of Büraburg was nearby. According to Willibald, Boniface later had a church with an attached monastery built in Fritzlar, on the site of the previously built chapel, according to tradition.

The support of the Frankish mayors of the palace (maior domos), and later the early Pippinid and Carolingian rulers, was essential for Boniface's work. Boniface had been under the protection of Charles Martel from 723 on. The Christian Frankish leaders desired to defeat their rival power, the non-Christian Saxons, and to incorporate the Saxon lands into their own growing empire. Boniface's campaign of destruction of indigenous Germanic pagan sites may have benefited the Franks in their campaign against the Saxons.

In 732, Boniface traveled again to Rome to report, and Pope Gregory III conferred upon him the pallium as archbishop with jurisdiction over Germany. Boniface again set out for what is now Germany, continued his mission, and used his authority to resolve the problems of many other Christians who had fallen out of contact with the regular hierarchy of the Roman Catholic Church. During his third visit to Rome in 737–38, he was made papal legate for Germany.

After Boniface's third trip to Rome, Charles Martel erected four dioceses in Bavaria (Salzburg, Regensburg, Freising, and Passau) and gave them to Boniface as archbishop and metropolitan over all Germany east of the Rhine. In 745, he was granted Mainz as metropolitan see. In 742, one of his disciples, Sturm (also known as Sturmi, or Sturmius), founded the abbey of Fulda not far from Boniface's earlier missionary outpost at Fritzlar. Although Sturm was the founding abbot of Fulda, Boniface was very involved in the foundation. The initial grant for the abbey was signed by Carloman, the son of Charles Martel, and a supporter of Boniface's reform efforts in the Frankish church. The saint himself explained to his old friend, Daniel of Winchester, that without the protection of Charles Martel he could "neither administer his church, defend his clergy, nor prevent idolatry."

According to German historian Gunther Wolf, the high point of Boniface's career was the Concilium Germanicum, organized by Carloman in an unknown location in April 743. Although Boniface was not able to safeguard the church from property seizures by the local nobility, he did achieve one goal, the adoption of stricter guidelines for the Frankish clergy, which often hailed directly from the nobility. After Carloman's resignation in 747 he maintained a sometimes turbulent relationship with the king of the Franks, Pepin; the claim that he would have crowned Pepin at Soissons in 751 is now generally discredited.

Boniface balanced this support and attempted to maintain some independence, however, by attaining the support of the papacy and of the Agilolfing rulers of Bavaria. In Frankish, Hessian, and Thuringian territory, he established the dioceses of Würzburg, and Erfurt. By appointing his own followers as bishops, he was able to retain some independence from the Carolingians, who most likely were content to give him leeway as long as Christianity was imposed on the Saxons and other Germanic tribes.

According to the "vitae", Boniface had never relinquished his hope of converting the Frisians, and in 754 he set out with a retinue for Frisia. He baptized a great number and summoned a general meeting for confirmation at a place not far from Dokkum, between Franeker and Groningen. Instead of his converts, however, a group of armed robbers appeared who slew the aged archbishop. The "vitae" mention that Boniface persuaded his (armed) comrades to lay down their arms: "Cease fighting. Lay down your arms, for we are told in Scripture not to render evil for good but to overcome evil by good."

Having killed Boniface and his company, the Frisian bandits ransacked their possessions but found that the company's luggage did not contain the riches they had hoped for: "they broke open the chests containing the books and found, to their dismay, that they held manuscripts instead of gold vessels, pages of sacred texts instead of silver plates." They attempted to destroy these books, the earliest "vita" already says, and this account underlies the status of the Ragyndrudis Codex, now held as a Bonifacian relic in Fulda, and supposedly one of three books found on the field by the Christians who inspected it afterward. Of those three books, the Ragyndrudis Codex shows incisions that could have been made by sword or axe; its story appears confirmed in the Utrecht hagiography, the "Vita altera", which reports that an eye-witness saw that the saint at the moment of death held up a gospel as spiritual protection. The story was later repeated by Otloh's "vita"; at that time, the Ragyndrudis Codex seems to have been firmly connected to the martyrdom.

Boniface's remains were moved from the Frisian countryside to Utrecht, and then to Mainz, where sources contradict each other regarding the behavior of Lullus, Boniface's successor as archbishop of Mainz. According to Willibald's "vita" Lullus allowed the body to be moved to Fulda, while the (later) "Vita Sturmi", a hagiography of Sturm by Eigil of Fulda, Lullus attempted to block the move and keep the body in Mainz.

His remains were eventually buried in the abbey church of Fulda after resting for some time in Utrecht, and they are entombed within a shrine beneath the high altar of Fulda Cathedral, previously the abbey church.

Veneration of Boniface in Fulda began immediately after his death; his grave was equipped with a decorative tomb around ten years after his burial, and the grave and relics became the center of the abbey. Fulda monks prayed for newly elected abbots at the grave site before greeting them, and every Monday the saint was remembered in prayer, the monks prostrating themselves and reciting Psalm 50. After the abbey church was rebuilt to become the Ratgar Basilica (dedicated 791), Boniface's remains were translated to a new grave: since the church had been enlarged, his grave, originally in the west, was now in the middle; his relics were moved to a new apse in 819. From then on Boniface, as patron of the abbey, was regarded as both spiritual intercessor for the monks and legal owner of the abbey and its possessions, and all donations to the abbey were done in his name. He was honored on the date of his martyrdom, 5 June (with a mass written by Alcuin), and (around the year 1000) with a mass dedicated to his appointment as bishop, on 1 December.

Willibald's "vita" describes how a visitor on horseback come to the site of the martyrdom, and a hoof of his horse got stuck in the mire. When it was pulled loose, a well sprang up. By the time of the "Vita altera Bonifatii" (9th century), there was a church on the site, and the well had become a "fountain of sweet water" used to sanctify people. The "Vita Liudgeri", a hagiographical account of the work of Ludger, describes how Ludger himself had built the church, sharing duties with two other priests. According to James Palmer, the well was of great importance since the saint's body was hundreds of miles away; the physicality of the well allowed for an ongoing connection with the saint. In addition, Boniface signified Dokkum's and Frisia's "connect[ion] to the rest of (Frankish) Christendom".

Saint Boniface's feast day is celebrated on 5 June in the Roman Catholic Church, the Lutheran Church, the Anglican Communion and the Eastern Orthodox Church.

A famous statue of Saint Boniface stands on the grounds of Mainz Cathedral, seat of the archbishop of Mainz. A more modern rendition stands facing St. Peter's Church of Fritzlar.

The UK National Shrine is located at the Catholic church at Crediton, Devon, which has a bas-relief of the felling of Thor's Oak, by sculptor Kenneth Carter. The sculpture was unveiled by Princess Margaret in his native Crediton, located in Newcombes Meadow Park. There is also a series of paintings there by Timothy Moore. There are quite a few churches dedicated to St. Boniface in the United Kingdom: Bunbury, Cheshire; Chandler's Ford and Southampton Hampshire; Adler Street, London; Papa Westray, Orkney; St Budeaux, Plymouth (now demolished); Bonchurch, Isle of Wight; Cullompton, Devon.

Bishop George Errington founded St Boniface's Catholic College, Plymouth in 1856. The school celebrates Saint Boniface on 5 June each year.

In 1818, Father Norbert Provencher founded a mission on the east bank of the Red River in what was then Rupert's Land, building a log church and naming it after St. Boniface. The log church was consecrated as Saint Boniface Cathedral after Provencher was himself consecrated as a bishop and the diocese was formed. The community that grew around the cathedral eventually became the city of Saint Boniface, which merged into the city of Winnipeg in 1971. In 1844, four Grey Nuns arrived by canoe in Manitoba, and in 1871, built Western Canada's first hospital: St. Boniface Hospital, where the Assiniboine and Red Rivers meet. Today, St. Boniface Hospital is the second-largest hospital in Manitoba.

Some traditions credit Saint Boniface with the invention of the Christmas tree. The "vitae" mention nothing of the sort. However, it is mentioned on a BBC-Devon website, in an account which places Geismar in Bavaria, and in a number of educational books, including "St. Boniface and the Little Fir Tree", "The Brightest Star of All: Christmas Stories for the Family", "The American normal readers". and a short story by Henry van Dyke, "The First Christmas Tree".

The earliest "Life" of Boniface was written by a certain Willibald, an Anglo-Saxon priest who came to Mainz after Boniface's death, around 765. Willibald's biography was widely dispersed; Levison lists some forty manuscripts. According to his lemma, a group of four manuscripts including Codex Monacensis 1086 are copies directly from the original.

Listed second in Levison's edition is the entry from a late ninth-century Fulda document: Boniface's status as a martyr is attested by his inclusion in the "Fulda Martyrology" which also lists, for instance, the date (1 November) of his translation in 819, when the Fulda Cathedral had been rebuilt. A "Vita Bonifacii" was written in Fulda in the ninth century, possibly by Candidus of Fulda, but is now lost.

The next "vita", chronologically, is the "Vita altera Bonifatii auctore Radbodo", which originates in the Bishopric of Utrecht, and was probably revised by Radboud of Utrecht (899–917). Mainly agreeing with Willibald, it adds an eye-witness who presumably saw the martyrdom at Dokkum. The "Vita tertia Bonifatii" likewise originates in Utrecht. It is dated between 917 (Radboud's death) and 1075, the year Adam of Bremen wrote his "Gesta Hammaburgensis ecclesiae pontificum", which used the "Vita tertia".

A later "vita", written by Otloh of St. Emmeram (1062–1066), is based on Willibald's and a number of other "vitae" as well as the correspondence, and also includes information from local traditions.

Boniface engaged in regular correspondence with fellow churchmen all over Western Europe, including the three popes he worked with, and with some of his kinsmen back in England. Many of these letters contain questions about church reform and liturgical or doctrinal matters. In most cases, what remains is one half of the conversation, either the question or the answer. The correspondence as a whole gives evidence of Boniface's widespread connections; some of the letters also prove an intimate relationship especially with female correspondents.

There are 150 letters in what is generally called the Bonifatian correspondence, though not all them are by Boniface or addressed to him. They were assembled by order of archbishop Lullus, Boniface's successor in Mainz, and were initially organized into two parts, a section containing the papal correspondence and another with his private letters. They were reorganized in the eighth century, in a roughly chronological ordering. Otloh of St. Emmeram, who worked on a new "vita" of Boniface in the eleventh century, is credited with compiling the complete correspondence as we have it.

The correspondence was edited and published already in the seventeenth century, by Nicolaus Serarius. Stephan Alexander Würdtwein's 1789 edition, "Epistolae S. Bonifacii Archiepiscopi Magontini", was the basis for a number of (partial) translations in the nineteenth century. The first version to be published by Monumenta Germaniae Historica (MGH) was the edition by Ernst Dümmler (1892); the most authoritative version until today is Michael Tangl's 1912 "Die Briefe des Heiligen Bonifatius, Nach der Ausgabe in den Monumenta Germaniae Historica", published by MGH in 1916. This edition is the basis of Ephraim Emerton's selection and translation in English, "The Letters of Saint Boniface", first published in New York in 1940; it was republished most recently with a new introduction by Thomas F.X. Noble in 2000.

Some fifteen preserved sermons are traditionally associated with Boniface, but that they were actually his is not generally accepted.

Early in his career, before he left for the continent, Boniface wrote the "Ars Bonifacii", a grammatical treatise presumably for his students in Nursling. Helmut Gneuss reports that one manuscript copy of the treatise originates from (the south of) England, mid-eighth century; it is now held in Marburg, in the Hessisches Staatsarchiv. He also wrote a treatise on verse, the "Caesurae uersuum", and a collection of riddles, the "Enigmata", influenced greatly by Aldhelm and containing many references to works of Vergil (the "Aeneid", the "Georgics", and the "Eclogues"). Three octosyllabic poems written in clearly Aldhelmian fashion (according to Andy Orchard) are preserved in his correspondence, all composed before he left for the continent.

A letter by Boniface charging Aldebert and Clement with heresy is preserved in the records of the Roman Council of 745 that condemned the two. Boniface had an interest in the Irish canon law collection known as "Collectio canonum Hibernensis", and a late 8th/early 9th-century manuscript in Würzburg contains, besides a selection from the "Hibernensis", a list of rubrics that mention the heresies of Clemens and Aldebert. The relevant folios containing these rubrics were most likely copied in Mainz, Würzburg, or Fulda, all places associated with Boniface. Michael Glatthaar suggested that the rubrics should be seen as Boniface's contribution to the agenda for a synod.

Boniface's death (and birth) has given rise to a number of noteworthy celebrations. The dates for some of these celebrations have undergone some changes: in 1805, 1855, and 1905 (and in England in 1955) anniversaries were calculated with Boniface's death dated in 755, the "Mainz tradition"; in Mainz, Michael Tangl's dating of the martyrdom in 754 was not accepted until after 1955. Celebrations in Germany centered on Fulda and Mainz, in the Netherlands on Dokkum and Utrecht, and in England on Crediton and Exeter.

The first German celebration on a fairly large scale was held in 1805 (the 1050th anniversary of his death), followed by a similar celebration in a number of towns in 1855; both of these were predominantly Catholic affairs, which emphasized the role of Boniface in German history as opposed to Protestant views on the role of Martin Luther, and especially the 1855 celebrations were an expression of German Catholic nationalism. In 1905, when strife between Catholic and Protestant factions had eased (one Protestant church published a celebratory pamphlet, Gerhard Ficker's "Bonifatius, der "Apostel der Deutschen""), there were modest celebrations and a publication for the occasion on historical aspects of Boniface and his work, the 1905 "Festgabe" by Gregor Richter and Carl Scherer. In all, the content of these early celebrations showed evidence of the continuing question about the meaning of Boniface for Germany, though the importance of Boniface in cities associated with him was without question.

In 1954, celebrations were widespread, in England, Germany, and the Netherlands, and a number of these celebrations were international affairs. Especially in Germany, these celebrations had a distinctly political note to them and often stressed Boniface as a kind of founder of Europe, such as when Konrad Adenauer, the (Catholic) German chancellor, addressed a crowd of 60,000 in Fulda, celebrating the feast day of the saint in a European context: "Das, was wir in Europa gemeinsam haben, [ist] gemeinsamen Ursprungs" ("What we have in common in Europe comes from the same source").

When Pope John Paul II visited Germany in November 1980, he spent two days in Fulda (17 and 18 November). He celebrated mass in Fulda Cathedral with 30,000 gathered on the square in front of the building, and met with the German Bishops' Conference (held in Fulda since 1867). The pope next celebrated mass outside the cathedral, in front of an estimated crowd of 100,000, and hailed the importance of Boniface for German Christianity: "Der heilige Bonifatius, Bischof und Märtyrer, "bedeutet" den 'Anfang' des Evangeliums und der Kirche in Eurem Land" ("The holy Boniface, bishop and martyr, "signifies" the beginning of the gospel and the church in your country"). A photograph of the pope praying at Boniface's grave became the centerpiece of a prayer card distributed from the cathedral.

In 2004, anniversary celebrations were held throughout Northwestern Germany and Utrecht, and Fulda and Mainz—generating a great amount of academic and popular interest. The event occasioned a number of scholarly studies, esp. biographies (for instance, by Auke Jelsma in Dutch, Lutz von Padberg in German, and Klaas Bruinsma in Frisian), and a fictional completion of the Boniface correspondence (Lutterbach, "Mit Axt und Evangelium"). A German musical proved a great commercial success, and in the Netherlands an opera was staged.

The literature on the saint and his work is extensive. At the time of the various anniversaries, edited collections were published containing essays by some of the best-known scholars of the time, such as the 1954 collection "Sankt Bonifatius: Gedenkgabe zum Zwölfhundertsten Todestag" and the 2004 collection "Bonifatius—Vom Angelsächsischen Missionar zum Apostel der Deutschen". In the modern era, published a number of biographies and articles on the saint focusing on his missionary praxis and his relics. The most authoritative biography is still Theodor Schieffer's "Winfrid-Bonifatius und die Christliche Grundlegung Europas" (1954).




 


</doc>
<doc id="28174" url="https://en.wikipedia.org/wiki?curid=28174" title="Data storage">
Data storage

Data storage is the recording (storing) of information (data) in a storage medium. Recording is accomplished by virtually any form of energy. DNA and RNA, handwriting, phonographic recording, magnetic tape, and optical discs are all examples of storage media. Electronic data storage requires electrical power to store and retrieve data. Data storage in a digital, machine-readable medium is sometimes called "digital data". Computer data storage is one of the core functions of a general purpose computer. Electronic documents can be stored in much less space than paper documents. Barcodes and magnetic ink character recognition (MICR) are two ways of recording machine-readable data on paper.

A recording medium is a physical material that holds information. Newly created information is distributed and can be stored in four storage media–print, film, magnetic, and optical–and seen or heard in four information flows–telephone, radio and TV, and the Internet as well as being observed directly. Digital information is stored on electronic media in many different recording formats.

With electronic media, the data and the recording media are sometimes referred to as "software" despite the more common use of the word to describe computer software. With (traditional art) static media, art materials such as crayons may be considered both equipment and medium as the wax, charcoal or chalk material from the equipment becomes part of the surface of the medium.

Some recording media may be temporary either by design or by nature. Volatile organic compounds may be used to preserve the environment or to purposely make data expire over time. Data such as smoke signals or skywriting are temporary by nature. Depending on the volatility, a gas (e.g. atmosphere, smoke) or a liquid surface such as a lake would be considered a temporary recording medium if at all.

A 2003 UC Berkeley report estimated that about five exabytes of new information were produced in 2002, and that 92% of this data was stored on hard disk drives. This was about twice the data produced in 2000. The amount of data transmitted over telecommunication systems in 2002 was nearly 18 exabytes—three and a half times more than was recorded on non-volatile storage. Telephone calls constituted 98% of the telecommunicated information in 2002. The researchers' highest estimate for the growth rate of newly stored information (uncompressed) was more than 30% per year.

It has been estimated that the year 2002 was the beginning of the digital age for information storage: an age in which more information is stored on digital storage devices than on analog storage devices. In 1986, approximately 1% of the world's capacity to store information was in digital format; this grew to 3% by 1993, to 25% by 2000, and to 97% by 2007. These figures correspond to less than three compressed exabytes in 1986, and 295 compressed exabytes in 2007. The quantity of digital storage doubled roughly every three years.

In a more limited study, the International Data Corporation estimated that the total amount of digital data in 2007 was 281 exabytes, and that the total amount of digital data produced exceeded the global storage capacity for the first time.

A study published in 2011 estimated that the world's technological capacity to store information in analog and digital devices grew from less than three (optimally compressed) exabytes in 1986, to 295 (optimally compressed) exabytes in 2007, and doubles roughly every three years.



</doc>
<doc id="28175" url="https://en.wikipedia.org/wiki?curid=28175" title="Sinn Féin">
Sinn Féin

Sinn Féin ( ; ; is a left-wing Irish republican political party active in both the Republic of Ireland and Northern Ireland.

The Sinn Féin organisation was founded in 1905 by Arthur Griffith. It took its current form in 1970 after a split within the party (with the other side becoming the Workers' Party of Ireland) and has historically been associated with the Provisional Irish Republican Army (IRA). Mary Lou McDonald has been party president since February 2018.

Sinn Féin is a major party in both Northern Ireland and the Republic of Ireland. It is the largest nationalist party in the Northern Ireland Assembly, and the joint-largest overall (alongside the Democratic Unionist Party); it had four ministerial posts in the most recent power-sharing Northern Ireland Executive. It holds seven of Northern Ireland's 18 seats—the second-largest bloc after the Democratic Unionist Party (DUP)—at Westminster, where it follows a policy of abstentionism, refusing to attend parliament or vote on bills. It is the third-largest party, and the largest party on the left, in the Oireachtas, the parliament of the Republic of Ireland.

The phrase "Sinn Féin" is Irish for "Ourselves" or "We Ourselves", although it is frequently mistranslated as "ourselves alone" (from ""Sinn Féin Amháin"", an early 20th century slogan. See also Sinn Féin (slogan)). The meaning of the name itself is an assertion of Irish national sovereignty and self-determination; i.e., the Irish people governing themselves, rather than being part of a political union with Great Britain (England, Scotland and Wales) under the Westminster Parliament.

Around the time of 1969–1970, owing to the split in the republican movement, there were two groups calling themselves Sinn Féin; one under Tomás Mac Giolla, the other under Ruairí Ó Brádaigh. The latter became known as Sinn Féin (Kevin Street) or Provisional Sinn Féin, and the former became known as Sinn Féin (Gardiner Place) or Official Sinn Féin. As the "Officials" dropped all mention of Sinn Féin from their name in 1982, instead calling itself the Workers' Party of Ireland, the Provisionals were now generally known as Sinn Féin. Supporters of Republican Sinn Féin, which came from a 1986 split, still use the term "Provisional Sinn Féin" to refer to the party led by Gerry Adams.

Sinn Féin members have also been referred to as Shinners, a term intended as a pejorative.

Sinn Féin was founded on 28 November 1905, when, at the first annual Convention of the National Council, Arthur Griffith outlined the Sinn Féin policy, "to establish in Ireland's capital a national legislature endowed with the moral authority of the Irish nation". The party contested the 1908 North Leitrim by-election, where it secured 27% of the vote. Thereafter, both support and membership fell. At the 1910 "Ard Fheis" (party conference) the attendance was poor, and there was difficulty finding members willing to take seats on the executive.

In 1914, Sinn Féin members, including Griffith, joined the anti-Redmond Irish Volunteers, which was referred to by Redmondites and others as the "Sinn Féin Volunteers". Although Griffith himself did not take part in the Easter Rising of 1916, many Sinn Féin members, who were also members of both the Volunteers and the Irish Republican Brotherhood, did. Government and newspapers dubbed the Rising "the Sinn Féin Rising". After the Rising, republicans came together under the banner of Sinn Féin, and at the 1917 "Ard Fheis" the party committed itself for the first time to the establishment of an Irish Republic. In the 1918 general election, Sinn Féin won 73 of Ireland's 105 seats, and in January 1919, its MPs assembled in Dublin and proclaimed themselves Dáil Éireann, the parliament of Ireland. The party supported the Irish Republican Army during the War of Independence, and members of the Dáil government negotiated the Anglo-Irish Treaty with the British government in 1921. In the Dáil debates that followed, the party divided on the Treaty. Anti-Treaty members led by Éamon de Valera walked out, and pro- and anti-Treaty members took opposite sides in the ensuing Civil War.

Pro-Treaty Dáil deputies and other Treaty supporters formed a new party, Cumann na nGaedheal, on 27 April 1923 at a meeting in Dublin, where delegates agreed on a constitution and political programme. Cumann na nGaedheal went on to govern the new Irish Free State for nine years. (It merged with two other organisations to form Fine Gael in 1933.) Anti-Treaty Sinn Féin members continued to boycott the Dáil. At a special "Ard Fheis" in March 1926, de Valera proposed that elected members be allowed to take their seats in the Dáil if and when the controversial Oath of Allegiance was removed. When his motion was defeated, de Valera resigned from Sinn Féin; on 16 May 1926, he founded his own party, Fianna Fáil, which was dedicated to republicanising the Free State from within its political structures. He took most Sinn Féin TDs with him. De Valera's resignation meant also the loss of financial support from America. The rump Sinn Féin party could field no more than fifteen candidates, and won only six seats in the June 1927 general election, a level of support not seen since before 1916. Vice-President and "de facto" leader Mary MacSwiney announced that the party simply did not have the funds to contest the second election called that year, declaring "no true Irish citizen can vote for any of the other parties". Fianna Fáil came to power at the 1932 general election (to begin what would be an unbroken 16-year spell in government) and went on to long dominate politics in the independent Irish state.

An attempt in the 1940s to access funds that had been put in the care of the High Court led to the Sinn Féin Funds case, which the party lost and in which the judge ruled that it was not the legal successor to the Sinn Féin of 1917. At the United Kingdom 1955 general election, two Sinn Féin candidates were elected to Westminster, but the party's vote decreased at the following election in 1959, during the IRA's Border Campaign. In 1962, supporters of Marxism–Leninism took control of the Sinn Féin leadership from traditional republicans, and started to take policy in a new direction. The same thing happened in the IRA, with the ascent of Cathal Goulding. These people were influenced by Communist Party of Ireland member Roy Johnston's "National Liberation Strategy" and the theories of C. Desmond Greaves of the Connolly Association (part of the Communist Party of Great Britain). The Garland Commission was set up in 1967, to investigate the possibility of ending abstentionism. Its report angered the already disaffected traditional republican element within the party, notably Seán Mac Stíofáin and Ruairí Ó Brádaigh, who viewed such a policy as treason against the Irish Republic.

The Sinn Féin party split in two at the beginning of 1970. At the party's "Ard Fheis" on 11 January the proposal to end abstentionism and take seats, if elected, in the Dáil, the Parliament of Northern Ireland and the Parliament of the United Kingdom was put before the members. A similar motion had been adopted at an IRA convention the previous month, leading to the formation of a Provisional Army Council by Mac Stíofáin and other members opposed to the leadership. When the motion was put to the "Ard Fheis", it failed to achieve the necessary two-thirds majority. The Executive attempted to circumvent this by introducing a motion in support of IRA policy, at which point the dissenting delegates walked out of the meeting. These members reconvened at another place, appointed a Caretaker Executive and pledged allegiance to the Provisional Army Council. The Caretaker Executive declared itself opposed to the ending of abstentionism, the drift towards "extreme forms of socialism", the failure of the leadership to defend the nationalist people of Belfast during the 1969 Northern Ireland riots, and the expulsion of traditional republicans by the leadership during the 1960s.

At its October 1970 "Ard Fheis", delegates were informed that an IRA convention had been held and had regularised its structure, bringing to an end the 'provisional' period. By then, however, the label "Provisional" or "Provo" was already being applied to them by the media. The opposing, anti-abstentionist party became known as "Official Sinn Féin". It changed its name in 1977 to "Sinn Féin – The Workers' Party", and in 1982 to "The Workers' Party".

Because the "Provisionals" were committed to military rather than political action, Sinn Féin's initial membership was largely confined, in Danny Morrison's words, to men "over military age or women". A Sinn Féin organiser of the time in Belfast described the party's role as "agitation and publicity". New "cumainn" (branches) were established in Belfast, and a new newspaper, "Republican News", was published. Sinn Féin took off as a protest movement after the introduction of internment in August 1971, organising marches and pickets. The party launched its platform, "Éire Nua" ("a New Ireland") at the 1971 "Ard Fheis". In general, however, the party lacked a distinct political philosophy. In the words of Brian Feeney, "Ó Brádaigh would use Sinn Féin "ard fheiseanna" (party conferences) to announce republican policy, which was, in effect, IRA policy, namely that Britain should leave the North or the 'war' would continue". Sinn Féin was given a concrete presence in the community when the IRA declared a ceasefire in 1975. 'Incident centres' were set up to communicate potential confrontations to the British authorities. They were manned by Sinn Féin, which had been legalised the previous year by Merlyn Rees, Secretary of State for Northern Ireland.

Political status for prisoners became an issue after the ending of the truce. Rees released the last of the internees but introduced the Diplock courts, and ended 'Special Category Status' for all prisoners convicted after 1 March 1976. This led first to the blanket protest, and then to the dirty protest. Around the same time, Gerry Adams began writing for "Republican News", calling for Sinn Féin to become more involved politically. Over the next few years, Adams and those aligned with him would extend their influence throughout the republican movement and slowly marginalise Ó Brádaigh, part of a general trend of power in both Sinn Féin and the IRA shifting north. In particular, Ó'Brádaigh's part in the 1975 IRA ceasefire had damaged his reputation in the eyes of Ulster republicans.

The prisoners' protest climaxed with the 1981 hunger strike, during which striker Bobby Sands was elected Member of Parliament for Fermanagh and South Tyrone as an Anti H-Block candidate. After his death on hunger strike, his seat was held, with an increased vote, by his election agent, Owen Carron. Two other Anti H-Block candidates were elected to Dáil Éireann in the general election in the Republic. These successes convinced republicans that they should contest every election. Danny Morrison expressed the mood at the 1981 "Ard Fheis" when he said:
This was the origin of what became known as the Armalite and ballot box strategy. "Éire Nua" was dropped in 1982, and the following year Ó Brádaigh stepped down as leader, and was replaced by Adams.

Under Adams' leadership electoral politics became increasingly important. In 1983 Alex Maskey was elected to Belfast City Council, the first Sinn Féin member to sit on that body. Sinn Féin polled over 100,000 votes in the Westminster elections that year, and Adams won the West Belfast seat that had been held by the Social Democratic and Labour Party (SDLP). By 1985 it had fifty-nine seats on seventeen of the twenty-six Northern Ireland councils, including seven on Belfast City Council.

The party began a reappraisal of the policy of abstention from the Dáil. At the 1983 "Ard Fheis" the constitution was amended to remove the ban on the discussion of abstentionism to allow Sinn Féin to run a candidate in the forthcoming European elections. However, in his address, Adams said, "We are an abstentionist party. It is not my intention to advocate change in this situation." A motion to permit entry into the Dáil was allowed at the 1985 "Ard Fheis", but without the active support of the leadership, and Adams did not speak. The motion failed narrowly. By October of the following year an IRA Convention had indicated its support for elected Sinn Féin Teachtaí Dála (TDs) taking their seats. Thus, when the motion to end abstention was put to the "Ard Fheis" on 1 November 1986, it was clear that there would not be a split in the IRA as there had been in 1970. The motion was passed with a two-thirds majority. Ó Brádaigh and about twenty other delegates walked out, and met in a Dublin hotel with hundreds of supporters to re-organise as Republican Sinn Féin.

Tentative negotiations between Sinn Féin and the British government led to more substantive discussions with the SDLP in the 1990s. Multi-party negotiations began in 1994 in Northern Ireland, without Sinn Féin. The Provisional IRA declared a ceasefire in the autumn of 1994. Sinn Féin then joined the talks, but the Conservative government under John Major soon came to depend on unionist votes to remain in power. It suspended Sinn Féin from the talks, and began to insist that the IRA decommission all of their weapons before Sinn Féin be re-admitted to the talks; this led to the IRA calling off its ceasefire. The new Labour government of Tony Blair wasn't reliant on unionist votes and re-admitted Sinn Féin, leading to another, permanent, ceasefire.

The talks led to the Good Friday Agreement of 10 April 1998 (officially known as the "Belfast Agreement"), which set up an inclusive devolved government in the North, and altered the Dublin government's constitutional claim to the whole island in Articles 2 and 3 of the Constitution of Ireland. Republicans opposed to the direction taken by Sinn Féin in the peace process formed the 32 County Sovereignty Movement in the late 1990s.

The party expelled Denis Donaldson, a party official, in December 2005, with him stating publicly that he had been in the employ of the British government as an agent since the 1980s. Donaldson told reporters that the British security agencies who employed him were behind the collapse of the Assembly and set up Sinn Féin to take the blame for it, a claim disputed by the British Government. Donaldson was found fatally shot in his home in County Donegal on 4 April 2006, and a murder inquiry was launched. In April 2009, the Real IRA released a statement taking responsibility for the killing.

When Sinn Féin and the Democratic Unionist Party (DUP) became the largest parties, by the terms of the Good Friday Agreement no deal could be made without the support of both parties. They nearly reached a deal in November 2004, but the DUP insisted on photographic and/or video evidence that decommissioning had been carried out, which was unacceptable to Sinn Féin.

On 2 September 2006, Martin McGuinness publicly stated that Sinn Féin would refuse to participate in a shadow assembly at Stormont, asserting that his party would only take part in negotiations that were aimed at restoring a power-sharing government. This development followed a decision on the part of members of Sinn Féin to refrain from participating in debates since the Assembly's recall the previous May. The relevant parties to these talks were given a deadline of 24 November 2006 to decide upon whether or not they would ultimately form the executive.

The 86-year Sinn Féin boycott of policing in Northern Ireland ended on 28 January 2007, when the "Ard Fheis" voted overwhelmingly to support the Police Service of Northern Ireland (PSNI). Sinn Féin members began to sit on Policing Boards and join District Policing Partnerships. There was opposition to this decision within Sinn Féin, and some members left, including elected representatives. The most well-known opponent was former IRA prisoner Gerry McGeough, who stood in the 2007 Assembly election against Sinn Féin in the constituency of Fermanagh and South Tyrone, as an Independent Republican. Others who opposed this development left to found the Republican Network for Unity.

Immediately after the June 2017 UK general election, where the Conservatives won 49% of seats but not an overall majority, so that non-mainstream parties could have significant influence, Gerry Adams announced for Sinn Féin that their elected MPs would continue the policy of not swearing allegiance to the Queen, as would be required for them to take their seats in the Westminster Parliament.

In 2017 and 2018 there were allegations of bullying within the party, leading to a number of resignations and expulsions of elected members.

At the "Ard Fheis" on 18 November 2017, Gerry Adams announced he would stand down as president of Sinn Féin in 2018, and would not stand for re-election as TD for Louth.

On 10 February 2018, Mary Lou McDonald was announced as the new president of Sinn Féin at a special Ard Fheis in Dublin. Michelle O’Neill was also elected as Vice President of the party.

Sinn Féin is the largest Irish republican political party, and was historically associated with the IRA, while also having been associated with the Provisional IRA in the party´s modern incarnation. The Irish government alleged that senior members of Sinn Féin have held posts on the IRA Army Council. However, the SF leadership has denied these claims. The US Government has made similar allegations.

A republican document of the early 1980s stated: "Both Sinn Féin and the IRA play different but converging roles in the war of national liberation. The Irish Republican Army wages an armed campaign... Sinn Féin maintains the propaganda war and is the public and political voice of the movement".

The British government stated in 2005 that "we had always said all the way through we believed that Sinn Féin and the IRA were inextricably linked and that had obvious implications at leadership level".

The Northern Bank robbery of £26.5 million in Belfast in December 2004 further delayed a political deal in Northern Ireland. The IRA were widely blamed for the robbery although Sinn Féin denied this and stated that party officials had not known of the robbery nor sanctioned it. Because of the timing of the robbery, it is considered that the plans for the robbery must have been laid whilst Sinn Féin was engaged in talks about a possible peace settlement. This undermined confidence among unionists about the sincerity of republicans towards reaching agreement. In the aftermath of the row over the robbery, a further controversy erupted when, on RTÉ's "Questions and Answers" programme, the chairman of Sinn Féin, Mitchel McLaughlin, insisted that the IRA's controversial killing of a mother of ten young children, Jean McConville, in the early 1970s though "wrong", was not a crime, as it had taken place in the context of the political conflict. Politicians from the Republic, along with the Irish media, strongly attacked McLaughlin's comments.

On 10 February 2005, the government-appointed Independent Monitoring Commission reported that it firmly supported the PSNI and Garda Síochána assessments that the IRA was responsible for the Northern Bank robbery and that certain senior members of Sinn Féin were also senior members of the IRA and would have had knowledge of and given approval to the carrying out of the robbery. Sinn Féin has argued that the IMC is not independent, and that the inclusion of former Alliance Party leader John Alderdice and a British security head was proof of this. The IMC recommended further financial sanctions against Sinn Féin members of the Northern Ireland Assembly. The British government responded by saying it would ask MPs to vote to withdraw the parliamentary allowances of the four Sinn Féin MPs elected in 2001.

Gerry Adams responded to the IMC report by challenging the Irish government to have him arrested for IRA membership—a crime in both jurisdictions—and for conspiracy.

On 20 February 2005, Irish Minister for Justice, Equality and Law Reform Michael McDowell publicly accused three of the Sinn Féin leadership, Gerry Adams, Martin McGuinness and Martin Ferris (TD for Kerry North) of being on the seven-man IRA Army Council; they later denied this.

On 27 February 2005, a demonstration against the murder of Robert McCartney on 30 January 2005 was held in east Belfast. Alex Maskey, a former Sinn Féin Lord Mayor of Belfast, was told by relatives of McCartney to "hand over the 12" IRA members involved. The McCartney family, although formerly Sinn Féin voters themselves, urged witnesses to the crime to contact the PSNI. Three IRA men were expelled from the organisation, and a man was charged with McCartney's murder.

Irish Taoiseach Bertie Ahern subsequently called Sinn Féin and the IRA "both sides of the same coin". The official ostracism of Sinn Féin was shown in February 2005 when Dáil Éireann passed a motion condemning the party's alleged involvement in illegal activity. US President George W. Bush and Senator Edward Kennedy refused to meet Gerry Adams while meeting the family of Robert McCartney.

On 10 March 2005, the House of Commons in London passed without significant opposition a motion, introduced by the British government, to withdraw the allowances of the four Sinn Féin MPs for one year, in response to the Northern Bank Robbery. This measure cost the party approximately £400,000. However, the debate prior to the vote mainly surrounded the more recent events connected with the murder of Robert McCartney. Conservatives and unionists put down amendments to have the Sinn Féin MPs evicted from their offices at the House of Commons but these were defeated.

In March 2005, Mitchell Reiss, the United States Special Envoy for Northern Ireland, condemned the party's links to the IRA, saying "it is hard to understand how a European country in the year 2005 can have a private army associated with a political party".

The October 2015 Assessment on Paramilitary Groups in Northern Ireland concluded that the Provisional IRA still existed "in a much reduced form", and that some IRA members believed its Army Council oversaw both the PIRA and Sinn Féin, although it believed that the leadership "remains committed to the peace process and its aim of achieving a united Ireland by political means".

Most of the party's policies are intended to be implemented on an "all-Ireland" basis which further emphasises their central aim of creating a united Ireland.

Sinn Féin is a democratic socialist and left-wing party. In the European Parliament, the party aligns itself with the European United Left–Nordic Green Left (GUE/NGL) parliamentary group. The party pledges support for minority rights, migrants' rights, and eradicating poverty. Although it is not in favour of the extension of legalised abortion (British 1967 Act) to Northern Ireland, Sinn Féin state they are opposed to the attitudes in society which "pressurise women" to have abortions and "criminalise" women who make this decision. The party does state that in cases of incest, rape, sexual abuse, "fatal foetal abnormalities", or when a woman's life and health are at risk or in danger, the final decision must rest with the woman. In the 2018 Irish abortion referendum, the party campaigned for a 'Yes' vote, but remained opposed to abortions up to 12 weeks. Categorised as "populist socialist" in literature, in 2014 leading party strategist and ideologue Eoin Ó Broin described Sinn Féin's entire political project as unashamedly populist.

Sinn Féin has been considered to be Eurosceptic. The party campaigned for a "No" vote in the Irish referendum on joining the European Economic Community in 1972. Sinn Féin was on the same side of the debate as the DUP and most of the UUP in that they wanted to pull out when UK had its referendum in 1975. The party was critical of the supposed need for an EU constitution as proposed in 2002, and urged a "No" vote in the 2008 referendum on the Lisbon Treaty, although Mary Lou McDonald said that there was "no contradiction in being pro-Europe, but anti-treaty". In its manifesto for the 2015 UK general election, Sinn Féin pledged that the party would campaign for the UK to stay within the European Union (EU), Martin McGuinness saying that an exit "would be absolutely economically disastrous". Gerry Adams said that, if there were to be a referendum on the question, there ought to be a separate and binding referendum for Northern Ireland. Its policy of a "Europe of Equals", and its critical engagement after 2001, together with its engagement with the European Parliament, marks a change from the party's previous opposition to the EU. The party expresses, on one hand, "support for Europe-wide measures that promote and enhance human rights, equality and the all-Ireland agenda", and on the other a "principled opposition" to a European superstate.

Sinn Féin's main political goal is a united Ireland. Other key policies from their most recent election manifesto are listed below:




Sinn Féin supports the creation of a "Minister for Europe", the independence of the Basque Country from Spain and France, and the Palestinians in the Israeli–Palestinian conflict.

Sinn Féin support a policy of "critical engagement with the EU", and have a "principled opposition" to a European superstate. It opposes an EU constitution because it would reduce the sovereignty of the member-states. It also criticises the EU on grounds of neoliberalism. Sinn Féin MEP Matt Carthy says that the "European Union must become a cooperative union of nation states committed to working together on issues such as climate change, migration, trade, and using our common strengths to improve the lives of citizens. If it does not, EU disintegration becomes a real possibility."

Sinn Féin is organised throughout Ireland, and membership is open to all Irish residents over the age of 16. The party is organised hierarchically into "cumainn" (branches), "comhairle ceantair" (district executives), and "cúigí" (regional executives). At national level, the "Coiste Seasta" (Standing Committee) oversees the day-to-day running of Sinn Féin. It is an eight-member body nominated by the Sinn Féin "Ard Chomhairle" (National Executive) and also includes the chairperson of each "cúige". The Sinn Féin "Ard Chomhairle" meets at least once a month. It directs the overall implementation of Sinn Féin policy and activities of the party.

The "Ard Chomhairle" also oversees the operation of various departments of Sinn Féin, viz Administration, Finance, National Organiser, Campaigns, Sinn Féin Republican Youth, Women's Forum, Culture, Publicity and International Affairs. It is made up of the following: Officer Board and nine other members, all of whom are elected by delegates to the "Ard Fheis", fifteen representing the five "Cúige" regions (three delegates each). The "Ard Chomhairle" can co-opt eight members for specific posts and additional members can be co-opted, if necessary, to ensure that at least thirty per cent of "Ard Chomhairle" members are women.

The Ardfheis (national delegate conference) is the ultimate policy-making body of the party, where delegates, directly elected by members of "cumainn", can decide on and implement policy. It is held at least once a year, but a special "Ard Fheis" can be called by the "Ard Chomhairle" or the membership under special circumstances.

2016–2017


Note: As the second largest party, and largest nationalist party, Sinn Féin will fill the deputy First Minister position when a new executive is formed. Other ministerial positions will not be allocated until then.

Sinn Féin returned to Northern Ireland elections at the 1982 Assembly elections, winning five seats with 64,191 votes (10.1%). The party narrowly missed winning additional seats in Belfast North and Fermanagh and South Tyrone. In the 1983 UK general election eight months later, Sinn Féin increased its support, breaking the six-figure vote barrier in Northern Ireland for the first time by polling 102,701 votes (13.4%). Gerry Adams won the Belfast West constituency, and Danny Morrison fell only 78 votes short of victory in Mid Ulster.

The 1984 European elections proved to be a disappointment, with Sinn Féin's candidate Danny Morrison polling 91,476 (13.3%) and falling well behind the SDLP candidate John Hume.

By the beginning of 1985, Sinn Féin had won its first representation on local councils, owing to three by-election wins in Omagh (Seamus Kerr, May 1983) and Belfast (Alex Maskey in June 1983 and Sean McKnight in March 1984). Three sitting councillors also defected to Sinn Féin in Dungannon, Fermanagh and Derry (the last defecting from the SDLP). Sinn Féin succeeded in winning 59 seats in the 1985 local government elections, after it had predicted winning only 40 seats. However, the results continued to show a decline from the peak of 1983, as the party won 75,686 votes (11.8%). The party failed to gain any seats in the 1986 by-elections caused by the resignation of unionist MPs in protest at the Anglo-Irish Agreement. While this was partly due to an electoral pact between unionist candidates, the SF vote fell in the four constituencies they contested.

In the 1987 general election, Gerry Adams held his Belfast West seat, but the party failed to make breakthroughs elsewhere and overall polled 83,389 votes (11.4%). The same year saw the party contest the Dáil election in the Republic of Ireland; however, it failed to win any seats and polled less than 2%.

The 1989 local government elections saw a drop in support for Sinn Féin. Defending 58 seats (the 59 won in 1985, plus two 1987 by-election gains in West Belfast, minus three councillors who had defected to Republican Sinn Féin in 1986), the party lost 15 seats. In the aftermath of the election, Mitchell McLaughlin admitted that recent IRA activity had affected the Sinn Féin vote.

In the 1989 European election, Danny Morrison again failed to win a seat, polling at 48,914 votes (9%).

The nadir for SF in this period came in 1992, with Gerry Adams losing his Belfast West seat to the SDLP, and the SF vote falling in the other constituencies that they had contested relative to 1987.

In the 1997 UK general election, Adams regained Belfast West. Martin McGuinness also won a seat in Mid Ulster. In the Irish general election the same year the party won its first seat since 1957, with Caoimhghín Ó Caoláin gaining a seat in the Cavan-Monaghan constituency. In the Irish local elections of 1999 the party increased its number of councillors from 7 to 23.

The party overtook its nationalist rival, the Social Democratic and Labour Party, as the largest nationalist party in the local elections and UK general election of 2001, winning four Westminster seats to the SDLP's three. The party continues to subscribe, however, to an abstentionist policy towards the Westminster British parliament, on account of opposing that parliament's jurisdiction in Northern Ireland, as well as its oath to the Queen.
Sinn Féin increased its share of the nationalist vote in the 2003, 2007, and 2011 Assembly elections, with Martin McGuinness, former Minister for Education, taking the post of deputy First Minister in the Northern Ireland power-sharing Executive Committee. The party has three ministers in the Executive Committee.

In the 2010 General Election, the party retained its five seats, and for the first time topped the poll at a Westminster Election in Northern Ireland, winning 25.5% of the vote. All Sinn Féin MPs increased their share of the vote and with the exception of Fermanagh and South Tyrone, increased their majorities. In Fermanagh and South Tyrone, Unionist parties agreed a joint candidate, this resulted in the closest contest of the election, with Sinn Féin MP Michelle Gildernew holding her seat by 4 votes after 3 recounts and an election petition challenging the result.

Sinn Féin lost some ground in the 2016 Assembly election, dropping one seat to finish with 28, ten behind the DUP. In the snap election eight months later caused by the resignation of McGuinness as deputy First Minister, however, the party surged, winning 27.9% of the popular vote to 28.1% for the DUP, and 27 seats to the DUP's 28 in an Assembly reduced by 18 seats. The withdrawal of the DUP party whip from Jim Wells in May 2018 meant that Sinn Féin became the joint-largest party in the Assembly alongside the DUP, with 27 seats each.

The party had five TDs elected in the 2002 Irish general election, an increase of four from the previous election. At the general election in 2007 the party had expectations of substantial gains, with poll predictions that they would gain five to ten seats. However, the party lost one of its seats to Fine Gael. Seán Crowe, who had topped the poll in Dublin South–West fell to fifth place, with his first preference vote reduced from 20.28% to 12.16%.

On 26 November 2010, Pearse Doherty won a seat in the Donegal South–West by-election. It was the party's first by-election victory in the Republic of Ireland since 1925. After negotiations with the left-wing Independent TDs Finian McGrath and Maureen O'Sullivan, a Technical Group was formed in the Dáil to give its members more speaking time.

In the 2011 Irish general election the party made significant gains. All its sitting TDs were returned, with Seán Crowe regaining the seat he had lost in 2007 in Dublin South–West. In addition to winning long-targeted seats such as Dublin Central and Dublin North–West, the party gained unexpected seats in Cork East and Sligo–North Leitrim. It ultimately won 14 seats, the best performance for the party's current incarnation. The party went on to win three seats in the Seanad election which followed their success at the general election. In the 2016 election it made further gains, finishing with 23 seats and overtaking the Labour Party as the third-largest party in the Dáil. It ran seven candidates in the Seanad election, all of whom were successful.

Sinn Féin is represented on most county and city councils. It made large gains in the local elections of 2004, increasing its number of councillors from 21 to 54, and replacing the Progressive Democrats as the fourth-largest party in local government. At the local elections of June 2009, the party's vote fell by 0.95% to 7.34%, with no change in the number of seats. Losses in Dublin and urban areas were balanced by gains in areas such as Limerick, Wicklow, Cork, Tipperary and Kilkenny and the border counties . However, three of Sinn Féin's seven representatives on Dublin City Council resigned within six months of the June 2009 elections, one of them defecting to the Labour Party.

In the 2004 European Parliament election, Bairbre de Brún won Sinn Féin's first seat in the European Parliament, at the expense of the Social Democratic and Labour Party (SDLP). She came in second behind Jim Allister, then of the Democratic Unionist Party (DUP). In the 2009 election, de Brún was re-elected with 126,184 first preference votes, the only candidate to reach the quota on the first count. This was the first time since elections began in 1979 that the DUP failed to take the first seat, and was the first occasion Sinn Féin topped a poll in any Northern Ireland election.

Sinn Féin made a breakthrough in the Dublin constituency in 2004. The party's candidate, Mary Lou McDonald, was elected on the sixth count as one of four MEPs for Dublin, effectively taking the seat of Patricia McKenna of the Green Party. In the 2009 election, when Dublin's representation was reduced to three MEPs, she failed to hold her seat. In the South constituency their candidate, Councillor Toiréasa Ferris, managed to nearly double the number of first preference votes, lying third after the first count, but failed to get enough transfers to win a seat.

In the 2014 election, Martina Anderson topped the poll in Northern Ireland, as did Lynn Boylan in Dublin. Liadh Ní Riada was elected in the South constituency, and Matt Carthy in Midlands–North-West.






</doc>
<doc id="28176" url="https://en.wikipedia.org/wiki?curid=28176" title="Willis Tower">
Willis Tower

The Willis Tower, built as and still commonly referred to as the Sears Tower, is a 110-story, skyscraper in Chicago, Illinois. At completion in 1973, it surpassed the World Trade Center towers in New York to become the tallest building in the world, a title it held for nearly 25 years; it remained the tallest building in the Western Hemisphere until the completion of a new building at the World Trade Center site in 2014. The building is considered a seminal achievement for its designer Fazlur Rahman Khan. The Willis Tower is the second-tallest building in the United States and the Western hemisphere – and the 16th-tallest in the world. More than one million people visit its observation deck each year, making it one of Chicago's most-popular tourist destinations. The structure was renamed in 2009 by the Willis Group as part of its lease on a portion of the tower's space.

, the building's largest tenant is United Airlines, which moved its corporate headquarters from the United Building at 77 West Wacker Drive in 2012 and today occupies around 20 floors with its headquarters and operations center.

In 1969, Sears, Roebuck & Co. was the largest retailer in the world, with about 350,000 employees. Sears executives decided to consolidate the thousands of employees in offices distributed throughout the Chicago area into one building on the western edge of Chicago's Loop. Sears asked its outside counsel, Arnstein, Gluck, Weitzenfeld & Minow (now known as Arnstein & Lehr, LLP) to suggest a location. The firm consulted with local and federal authorities and the applicable law, then offered Sears two options: the Goose Island area northwest of downtown, and a two-block area bounded by Franklin Street on the east, Jackson Boulevard on the south, Wacker Drive on the west and Adams Street on the north, with Quincy Street running through the middle from east to west.

This latter site was decided upon, and preliminary inquiries determined that the necessary permits could be obtained and Quincy Street vacated. The next step was to acquire the property; a team of attorneys from the Arnstein law firm, headed by Andrew Adsit, began buying the property parcel by parcel. Sears purchased 15 old buildings from 100 owners and paid $2.7 million to the City of Chicago for the portion of Quincy Street that divided the property.

Sears, which needed of office space for its planned consolidation and predicted that growth would require yet more, commissioned architects Skidmore, Owings & Merrill (SOM) to produce a structure to be one of the largest office buildings in the world. Their team of architect Bruce Graham and structural engineer Fazlur Rahman Khan designed the building as nine square "tubes" (each essentially a separate building), clustered in a 3×3 matrix forming a square base with sides. All nine tubes would rise up to the 50th floor of the building. At the 50th floor, the northwest and southeast tubes end, and the remaining seven continue up. At the 66th floor, the northeast and the southwest tubes end. At the 90th floor, the north, east, and south tubes end. The remaining west and center tubes continue up to the 108th floor.

The Willis Tower was the first building to use Khan's bundled tube structure. This innovative design was structurally efficient and economic: at 1,450 feet, it provided more space and rose higher than the Empire State Building, yet cost much less per unit area. This structural system would prove highly influential in skyscraper construction. It has been used in most supertall buildings since then, including the world's tallest building, the Burj Khalifa. To honor Khan's contributions, the Structural Engineers Association of Illinois commissioned a sculpture of him for the lobby of the Willis Tower.
Sears executives decided that the space they would immediately occupy should be efficiently designed to house their Merchandise Group, and that floor space for future growth would be rented out to smaller firms and businesses until Sears could retake it. The latter floor areas had to be designed to a smaller plate, with a high window-space to floor-space ratio, to be attractive and marketable to prospective lessees. Smaller floorplates required a taller structure to yield sufficient square footage. Skidmore architects proposed a tower with large floors in the lower part of the building, and gradually tapered areas of floorplates in a series of setbacks, which would give the Sears Tower its distinctive look.

As Sears continued to offer optimistic projections for growth, the tower's proposed floor count rapidly increased into the low hundreds, surpassing the height of New York's unfinished World Trade Center to become the world's tallest building. The height was restricted by a limit imposed by the Federal Aviation Administration (FAA) to protect air traffic. The financing of the tower was provided by the Sears company. It was topped with two antennas to permit local television and radio broadcasts. Sears and the City of Chicago approved the design, and the first steel was put in place in April 1971. The structure was completed in May 1973. The construction cost about US$150 million at the time, equivalent to $ million in 2018. By comparison, Taipei 101, built in 2004 in Taiwan, cost around the equivalent of US$2.21 billion in 2018 dollars.

Black bands appear on the tower around the 29th–32nd, 64th–65th, 88th–89th, and 104th–108th floors. These are louvres that allow ventilation for service equipment and obscure the structure's belt trusses. Even though regulations did not require a fire sprinkler system, the building was equipped with one from the beginning. There are around 40,000 sprinkler heads in the building, installed at a cost of $4 million.

In February 1982, two television antennas were added to the structure, increasing its total height to . The western antenna was later extended, bringing the overall height to on June 5, 2000 to improve reception of local NBC station WMAQ-TV.

As the construction of the building neared the 50th floor, lawsuits for an injunction were filed seeking to stop the building from exceeding 67 floors. The suits alleged that above that point television reception would deteriorate and cause property values to plummet. The first suit was filed by the State's Attorney in neighboring Lake County on March 17, 1972. A second suit was filed on March 28 in the Cook County Circuit Court by the Villages of Skokie, Northbrook and Deerfield, Illinois.

Sears filed motions to dismiss the Lake County and the Cook County lawsuits and on May 17, 1972, Judge LaVerne Dickson, Chief of the Lake County Circuit Court dismissed the suit, saying, "I find nothing that gives television viewers the right to reception without interference. They will have to find some other means of ensuring reception such as taller antennas." The Lake County State's Attorney filed a Notice of Appeal and the Supreme Court agreed to permit bypassing the appellate court and to hear the matter on an expedited basis. The State's Attorney then asked the Illinois Supreme Court for a temporary injunction to stop the construction and his request was denied. On June 12, Judge Charles R. Barrett granted Sears' motion to dismiss the suit filed by the three Chicago suburbs on the grounds that interference with television reception caused by construction of the Sears building did not violate constitutional rights and that the suburbs involved in the suit do not have any right to undistorted television reception. This decision was also appealed and consolidated with the Lake County appeal with the Supreme Court of Illinois.

Meanwhile, an Illinois Citizens' Committee for Broadcasting requested the Federal Communications Commission to halt construction so that the building would not interfere with area television reception. On May 26, 1972, the Commission declined to take action on the grounds that it did not have jurisdiction to do so.

On June 30, 1972, the Illinois Supreme Court affirmed the previous rulings by Lake and Cook County Circuit Courts, by a letter order with a written opinion to follow. On September 8, 1972, the United States Court of Appeals for the Seventh Circuit upheld the decision by the Federal Communications Commission to dismiss the complaint brought by the Illinois Citizens Committee for Broadcasting charging that the building would drastically affect reception in the Chicago market and requesting the FCC to halt construction. The Supreme Court of Illinois written opinion was filed on September 20, 1972. In affirming the judgments of lower courts the Court held, "Considering the foregoing, it is clear to us that absent legislation to the contrary defendant has a propriety right to construct a building to its desired height and that completion of the project would not constitute a nuisance under the circumstances of this case." 

Sears' optimistic growth projections were not met. Competition from its traditional rivals (like Montgomery Ward) continued, with new competition by retailing giants such as Kmart, Kohl's, and Walmart. The fortunes of Sears & Roebuck declined in the 1970s as the company lost market share; its management grew more cautious. Nor did the Sears Tower draw as many tenants as Sears had hoped. The tower stood half-vacant for a decade as a surplus of office space was erected in Chicago in the 1980s.

In 1990, the law firm of Keck, Mahin & Cate decided to move out of its space in the Sears Tower and into a development that would become 77 West Wacker Drive, rebuffing Sears' attempts to entice the firm to stay. Two years later, Sears began moving its own offices out of the Sears Tower.

In 1994, Sears sold the building to Boston-based AEW Capital Management, with financing from MetLife. At the time, it was one-third vacant. By 1995, Sears had completely left the building, moving to a new office campus in Hoffman Estates, Illinois.

In 1997, Toronto-based TrizecHahn Corporation (the owner of the CN Tower at the time) purchased the building for $110 million, and assumption of $4 million in liabilities, and a $734 million mortgage. In 2003, Trizec surrendered the building to lender MetLife.

In 2004, MetLife sold the building to a group of investors, including New York-based Joseph Chetrit, Joseph Moinian, Lloyd Goldman, Joseph Cayre and Jeffrey Feil, and Skokie, Illinois-based American Landmark Properties. The quoted price was $840 million, with $825 million held in a mortgage.

In June 2006, seven men were arrested by the FBI and charged with plotting to destroy the tower. Deputy FBI Director John Pistole described their plot as "more aspirational than operational". The case went to court in October 2007; after three trials, five of the suspects were convicted and two were acquitted. The alleged leader of the group, Narseal Batiste, was sentenced to 13½ years in prison in November 2009.

In February 2009, the owners announced they were considering a plan to paint the structure silver; this plan was later dropped. The paint would have "rebranded" the building and highlighted its advances in energy efficiency. The estimated cost was $50 million.

Since 2007, the building owners have been considering building a hotel on the north side of Jackson, between Wacker and Franklin, at the plaza that is the entrance to the tower's observation deck. The tower's parking garage is beneath the plaza. Building owners say the second building was considered in the original design. The plan was eventually cancelled as city zoning does not permit construction of such a tall tower there.

Although Sears' naming rights expired in 2003, the building continued to be called the Sears Tower for several years. In March 2009, London-based insurance broker Willis Group Holdings agreed to lease a portion of the building, and obtained the building's naming rights. On July 16, 2009, the building was officially renamed Willis Tower. On August 13, 2012, United Airlines announced it would move its corporate headquarters from 77 West Wacker Drive to Willis Tower.

In 2015, the Blackstone Group completed purchase of the tower for a reported $1.3 billion, the highest price ever paid for a U.S. property outside New York City. The new owners are considering several plans for further site developments.

The Willis Tower observation deck, called the Skydeck, opened on June 22, 1974. Located on the tower's 103rd floor, it is high, making it the highest observation deck in the United States and one of Chicago's most famous tourist attractions. Tourists can experience how the building sways on a windy day. They also can see far over the plains of Illinois and across Lake Michigan to Indiana, Michigan, and Wisconsin on a clear day. Elevators take tourists to the top in about 60 seconds, and allow tourists to feel the pressure change as they rise up. The Skydeck competes with the John Hancock Center's observation floor a mile and a half away, which is lower. Some 1.7 million tourists visit the Skydeck annually. A second Skydeck on the 99th floor is also used if the 103rd floor is closed. The tourist entrance can be found on the south side of the building along Jackson Boulevard.

In January 2009, Willis Tower's owners began a major renovation of the Skydeck, including the installation of retractable glass balconies, which can be extended approximately from the facade of the 103rd floor, overlooking South Wacker Drive. The all-glass boxes, informally dubbed "The Ledge", allow visitors to look through the glass floor to the street below. The boxes, which can bear of weight, opened to the public on July 2, 2009. On May 29, 2014, the laminated glass covering the floor of one of the glass boxes shattered while visitors were sitting on it, but caused no injuries. The broken glass was replaced within days, and tourist operations resumed as before.

The Willis Tower remains the second tallest building in the Americas (after One World Trade Center) and the Western Hemisphere. With a pinnacle height of , it is the third-tallest freestanding structure in the Americas, as it is shorter than Toronto's CN Tower. Willis Tower is the eighth-tallest freestanding structure in the world by pinnacle height.

At tall, including decorative spires, the Petronas Twin Towers in Kuala Lumpur, Malaysia, laid claim to replacing the Sears Tower as the tallest building in the world in 1998. Not everyone agreed, and in the ensuing controversy, four different categories of "tallest building" were created. Of these, Petronas was the tallest in the first category (height to top of architectural elements, meaning spires but not antennas), giving it the title of world's tallest building.

Taipei 101 in Taiwan claimed the record in three of the four categories in 2004 to become recognized as the tallest building in the world. Taipei 101 surpassed the Petronas Twin Towers in spire height and the Sears Tower in roof height and highest occupied floor. The Sears Tower retained one record: its antenna exceeded Taipei 101's spire in height. In 2008, the Shanghai World Financial Center claimed the records of tallest building by roof and highest occupied floor.

On August 12, 2007, the Burj Khalifa in Dubai, United Arab Emirates was reported by its developers to have surpassed the Sears Tower in all height categories.

Upon completion, One World Trade Center in New York City surpassed the Willis Tower through its structural and pinnacle heights, but not by roof, observation deck elevation, or highest occupied floor.

Until 2000, the Sears Tower did not hold the record for being the tallest building by pinnacle height. From 1969 to 1978, this record was held by the John Hancock Center, whose antenna reached a height of , or taller than the Sears Tower's original height of . In 1978, One World Trade Center became taller by pinnacle height due to the addition of a antenna, which brought its total height to . In 1982, two antennas were installed on top of the Sears Tower which brought its total height to , making it taller than the John Hancock Center but not One World Trade Center. However, the extension of the Sears Tower's western antenna in June 2000 to allowed it to just barely claim the title of tallest building by pinnacle height.

On May 25, 1981, Dan Goodwin, wearing a homemade Spider-Man suit while using suction cups, camming devices, and sky hooks, and despite several attempts by the Chicago Fire Department to stop him, made the first successful outside ascent of the Sears Tower. Goodwin was arrested at the top after the seven-hour climb, and he was later charged with trespassing. Goodwin stated that the reason he made the climb was to call attention to shortcomings in high-rise rescue and firefighting techniques. After a lengthy interrogation by Chicago's District Attorney and Fire Commissioner, Goodwin was officially released from prison.

In August 1999, French urban climber Alain "Spiderman" Robert, using only his bare hands and bare feet, scaled the building's exterior glass and steel wall all the way to the top. A thick fog settled in near the end of his climb, making the last 20 stories of the building's glass and steel exterior slippery.

Although Sears sold the Tower in 1994 and had completely vacated it by 1995, the company retained the naming rights to the building through 2003. The new owners were rebuffed in renaming deals with CDW Corp in 2005 and the U.S. Olympic Committee in 2008. London-based insurance broker Willis Group Holdings, Ltd. leased more than of space on three floors in 2009. A Willis spokesman said the naming rights were obtained as part of the negotiations at no cost to Willis, and the building was renamed Willis Tower on July 16, 2009.

The naming rights are valid for 15 years, so it is possible that the building's name could change again in 2024 or later. The "Chicago Tribune" joked that the building's new name reminded them of the oft-repeated "What you talkin' 'bout, Willis?" catchphrase from the 1980s American television sitcom "Diff'rent Strokes" and considered the name-change ill-advised in "a city with a deep appreciation of tradition and a healthy ego, where some Chicagoans still mourn the switch from Marshall Field's to Macy's". This feeling was confirmed in a July 16, 2009 CNN article in which some Chicago area residents expressed reluctance to accept the Willis Tower name, and in an article that appeared in the October 2010 issue of "Chicago" magazine that ranked the building among Chicago's 40 most important, the author pointedly refused to acknowledge the name change and referred to the building as the "Sears Tower". "Time" magazine called the name change one of the top 10 worst corporate name changes and pointed to negative press coverage by local news outlets and online petitions from angry residents. The naming rights issue continued into 2013, when Eric Zorn noted in the "Chicago Tribune" that "We're stubborn about such things. This month marked four years since the former Sears Tower was re-christened Willis Tower, and the new name has yet to stick."


Many broadcast station transmitters are located at the top of Willis Tower. Each list is ranked by height from the top down. Stations at the same height on the same mast indicate the use of a diplexer into the same shared antenna. Due to its extreme height, FM stations (all class B) are very limited in power output.

Also, NOAA Weather Radio station KWO39 transmits off the top of Willis Tower, at 162.550 MHz. KWO39, programmed by the National Weather Service Weather Forecast Office in Chicago, is equipped with Specific Area Message Encoding (SAME), which sets off a siren on specially-programmed weather radios to alert of an impending hazard, such as a tornado or civil emergency.

The building has appeared in numerous films and television shows set in Chicago such as "Ferris Bueller's Day Off", where Ferris and company watch the streets of Chicago from the observation deck. The television show "Late Night with Conan O'Brien" introduced a character called The Sears Tower Dressed In Sears Clothing when the show visited Chicago in 2006. The building is also featured in History Channel's "Life After People", in which it and other human-made landmarks suffer from neglect without humans around, and it collapses two hundred years after people are gone. In an episode of the television series "Monk", Adrian Monk tries to conquer his fear of heights by imagining that he is on top of the Sears Tower. Also, in an episode of "Kenan and Kel", Kenan Rockmore and Kel Kimble decide to climb to the top of the Sears Tower, so that Kenan can declare his love for a girl.

In the movie "", the tower is damaged by a tornado.

In "1969", a Season 2 episode of the science-fiction series "Stargate SG-1", the SG-1 team accidentally travels back in time to the titular year. At one point, the team travels though Chicago and the Sears Tower is shown (erroneously, since construction did not begin on the tower until two years later in 1971).

In the 2004 film "I, Robot", the tower is shown updated in the year 2035 with new triangular antennas. The tower is shown surpassed in height by the USR (United States Robotics) Building.

In the 2008 film "The Dark Knight", the tower is part of Gotham City.

In the 2011 film "", the tower is featured in a number of scenes. The most notable one is when the N.E.S.T team tries to enter the city using V-22 Osprey helicopters. They use Willis Tower for cover before using wing suits to descend into the city streets. In the movie, the tower is shown to be severely damaged by the Decepticon invasion of the city.

In the 2013 film "Man of Steel", the tower's interior and parts of its exterior portrayed the offices of the "Daily Planet".

In the 2014 film "Divergent", the tower is shown abandoned and decayed in a future Chicago.

In the 2015 film "Jupiter Ascending", the tower is featured prominently as the place where Caine and Jupiter await a spaceship to lift them off the planet.

In the 2018 film "Rampage", the Energyne corporation is headquartered in the building, and uses tower's antenna to broadcast an echolocation signal that would bring three mutated monsters to town. After the antenna is destroyed by the beasts, the entire building topples to the ground.

Older versions of "Microsoft Flight Simulator" would begin with the player on the runway of Meigs Field, facing a virtual version of the tower.

In Sufjan Stevens' 2005 album "Illinois", the tower is referenced in the track "Seer's Tower", whose title is a play on the tower's now-former name, Sears Tower.



</doc>
<doc id="28177" url="https://en.wikipedia.org/wiki?curid=28177" title="Simony">
Simony

Simony is the act of selling church offices and roles. It is named after Simon Magus, who is described in the Acts of the Apostles 8:9–24 as having offered two disciples of Jesus, Peter and John, payment in exchange for their empowering him to impart the power of the Holy Spirit to anyone on whom he would place his hands. The term extends to other forms of trafficking for money in "spiritual things." Simony was one of the important issues during the Investiture Controversy.

Although an offense against canon law, Simony became widespread in the Catholic Church in the 9th and 10th centuries. In the canon law, the word bears a more extended meaning than in English law. "Simony according to the canonists", says John Ayliffe in his "Parergon",

In the "Corpus Juris Canonici" the Decretum and the Decretals deal with the subject. The offender whether "simoniacus" (the perpetrator of a simoniacal transaction) or "simoniace promotus" (the beneficiary of a simoniacal transaction), was liable to deprivation of his benefice and deposition from orders if a secular priest, or to confinement in a stricter monastery if a regular. No distinction seems to have been drawn between the sale of an immediate and of a reversionary interest. The innocent "simoniace promotus" was, apart from dispensation, liable to the same penalties as though he were guilty.

Certain matters were simoniacal by the canon law but would not be regarded as such in English law. So grave was the crime of simony considered that even infamous persons (deprived of citizens' rights due to conviction) could accuse another of it. English provincial and legatine constitutions continually assailed simony.

The Church of England struggled with the practice after its separation from Rome. For the purposes of English law, simony is defined by William Blackstone as "obtain[ing] orders, or a licence to preach, by money or corrupt practices" or, more narrowly, "the corrupt presentation of any one to an ecclesiastical benefice for gift or reward". While English law recognized simony as an offence, it treated it as merely an ecclesiastical matter, rather than a crime, for which the punishment was forfeiture of the office or any advantage from the offence and severance of any patronage relationship with the person who bestowed the office. Both Edward VI of England and Elizabeth I promulgated statutes against simony, in the latter case through the Simony Act 1588. The cases of Bishop of St. David's Thomas Watson in 1699 and of Dean of York William Cockburn in 1841 were particularly notable.

By the Benefices Act 1892, a person guilty of simony is guilty of an offence for which he may be proceeded against under the Clergy Discipline Act 1892. An innocent clerk is under no disability, as he might be by the canon law. Simony may be committed in three ways – in promotion to orders, in presentation to a benefice, and in resignation of a benefice. The common law (with which the canon law is incorporated, as far as it is not contrary to the common or statute law or the prerogative of the Crown) has been considerably modified by statute. Where no statute applies to the case, the doctrines of the canon law may still be of authority.

, simony remains an offence. An unlawfully bestowed office can be declared void by the Crown, and the offender can be disabled from making future appointments and fined up to £1000. Clergy are no longer required to make a declaration as to simony on ordination, but offences are now likely to be dealt with under the Clergy Discipline Measure 2003, r.8.





</doc>
<doc id="28178" url="https://en.wikipedia.org/wiki?curid=28178" title="September 26">
September 26




</doc>
<doc id="28179" url="https://en.wikipedia.org/wiki?curid=28179" title="Samaritans">
Samaritans

The Samaritans (; Samaritan Hebrew: , "" (), "Guardians/Keepers/Watchers (of the Torah)") are an ethnoreligious group of the Levant originating from the Israelites (or Hebrews) of the Ancient Near East.

Ancestrally, Samaritans claim descent from the tribe of Ephraim and tribe of Manasseh (two sons of Joseph) as well as from the Levites, who have links to ancient Samaria (now constituting the majority of the territory known as the West Bank) from the period of their entry into Canaan, while some suggest that it was from the beginning of the Babylonian captivity up to the Samaritan polity under the rule of Baba Rabba. Samaritans used to include descendants who ascribed to the Benjamin tribe, but this line became extinct in the 1960s. According to Samaritan tradition, the split between them and the Judean-led Southern Israelites began during the biblical time of the priest Eli when the Southern Israelites split off from the central Israelite tradition, as they perceive it.

In the Talmud, a central post-exilic religious text of Rabbinic Judaism, the Samaritans are called "Cutheans" (, "Kutim"), referring to the ancient city of Kutha, geographically located in what is today Iraq. In the biblical account, however, Kuthah was one of several cities from which people were brought to Samaria, and they worshiped Nergal. Modern genetics partially support both the claims of the Samaritans and the account in the Hebrew Bible (and Talmud), suggesting that the genealogy of the Samaritans lies in some combination of these two accounts. Genetically, modern Samaritan populations are found to have "much greater affinity" genetically to Jews than to neighbouring Palestinian Arabs. This suggests that the Samaritans remained a genetically isolated population.

The Samaritans are adherents of Samaritanism, a religion closely related to Judaism. Samaritans believe that their worship, which is based on the Samaritan Pentateuch, is the true religion of the ancient Israelites from before the Babylonian captivity, preserved by those who remained in the Land of Israel, as opposed to Judaism, which they see as a related but altered and amended religion, brought back by those returning from the Babylonian Captivity. The Samaritans believe that Mount Gerizim was the original Holy Place of Israel from the time that Joshua conquered Canaan. The major issue between Jews and Samaritans has always been the location of the Chosen Place to worship God: Mount Zion in Jerusalem according to the Jewish faith or Mount Gerizim according to the Samaritan faith.

Once a large community, the Samaritan population appears to have shrunk significantly in the wake of the bloody suppression of the Samaritan Revolts (mainly in 529 CE and 555 CE) against the Byzantine Empire. Conversion to Christianity under the Byzantines also reduced their numbers. Conversions to Islam took place as well, and by the mid–Middle Ages, Benjamin of Tudela estimated only around 1,900 Samaritans remained in Palestine and Syria. , the population was 796, divided between Qiryat Luza on Mount Gerizim and the city of Holon, just outside Tel Aviv. Most Samaritans in Holon and Qiryat Luza today speak Hebrew and Arabic. For liturgical purposes, Samaritan Hebrew, Samaritan Aramaic, and Arabic are used, all written with the Samaritan alphabet, a variant of the Paleo-Hebrew alphabet, which is distinct from the Hebrew alphabet. Hebrew and later Aramaic were languages in use by the Jewish and Samaritan inhabitants of Judea (the name by which Israel was known during part of the Second Temple era) before the Roman exile.

Samaritans have a stand-alone religious status in Israel, and there are occasional conversions from Judaism to Samaritanism and vice versa due to marriages. While the Israeli Rabbinic authorities consider Samaritanism to be a branch of Judaism, the Chief Rabbinate of Israel requires Samaritans to officially go through a formal conversion to Judaism in order to be recognized as Halakhic Jews. One example is Israeli TV personality Sofi Tsedaka, who formally converted to Rabbinic Judaism at the age of 18. Samaritans with Israeli citizenship are obligated to undertake mandatory service in the Israel Defense Forces, while those with dual Israeli-Palestinian citizenship (living in Qiryat Luza) are generally exempted.

There is conflict over the etymology of the name for the Samaritans in Hebrew, stemming from the fact that they are referred to differently in different dialects of Hebrew. This has accompanied controversy over whether the Samaritans are named after the geographic area of Samaria (the northern part of what is now globally known as the West Bank), or whether the area received its name from the group. This distinction is controversial in part because different interpretations can be used to justify or deny claims of ancestry over this region, which has been deeply contested in modern times.

In Samaritan Hebrew, the Samaritans call themselves "Shamerim" (שַמֶרִים), which according to the Anchor Bible Dictionary, is derived from the Ancient Hebrew term meaning "Guardians/Keepers/Watchers [of the Torah/Law]".

Biblical Hebrew "Šomerim" () "Guardians" (singular "Šomer") comes from the Hebrew Semitic root שמר, which means "to watch, guard".

Historically, Samaria was the key geographical concentration of the Samaritan community. Thus, it may suggest the region of Samaria is named after the Samaritans, rather than the Samaritans being named after the region. In Jewish tradition, however, it is sometimes claimed that Mount Samaria, meaning "Watch Mountain", is actually named so because watchers used to watch from those mountains for approaching armies from Egypt in ancient times. In Modern Hebrew, the Samaritans are called , which would appear to simply mean "inhabitants of Samaria". This is a politically sensitive distinction.

That the etymology of the Samaritans' ethnonym in Samaritan Hebrew is derived from "Guardians/Keepers/Watchers [of the Law/Torah]", as opposed to Samaritans being named after the region of Samaria, has in history been supported by a number of Christian Church fathers, including Epiphanius of Salamis in the "Panarion", Jerome and Eusebius in the "Chronicon" and Origen in "The Commentary on Saint John's Gospel", and in some Talmudic commentary of Tanhuma on Genesis 31, and Pirke De-Rabbi Eliezer 38, p. 21.

According to Samaritan tradition, Mount Gerizim was the original Holy Place of the Israelites from the time that Joshua conquered Canaan and the tribes of Israel settled the land. The reference to Mount Gerizim derives from the biblical story of Moses ordering Joshua to take the Twelve Tribes of Israel, to the mountains by Shekhem (Nablus) and place half of the tribes, six in number, on Mount Gerizim, the Mount of the Blessing, and the other half on Mount Ebal, the Mount of the Curse. The two mountains were used to symbolize the significance of the commandments and serve as a warning to whoever disobeyed them (Deut. 11:29; 27:12; Josh. 8:33).

Samaritans claim they are Israelite descendants of the Northern Israelite tribes of Ephraim and Manasseh, who survived the destruction of the Kingdom of Israel (Samaria) by the Assyrians in 722 BCE.

Samaritan historiography places the basic schism from the remaining part of Israel after the tribes of Israel conquered and returned to the land of Canaan, led by Joshua. In its account, after Joshua's death, Eli the priest left the Tabernacle which Moses erected in the desert and established on Mount Gerizim and built another one under his own rule in the hills of Shiloh.

Abu l-Fath, who in the 14th century wrote a major work of Samaritan history, comments on Samaritan origins as follows:

Further, the "Samaritan Chronicle Adler", or New Chronicle, believed to have been composed in the 18th century using earlier chronicles as sources states:

The emergence of the Samaritans as an ethnic and religious community distinct from other Levant peoples appears to have occurred at some point after the Assyrian conquest of the Israelite Kingdom of Israel in approximately 721 BCE. The records of Sargon II of Assyria indicate that he deported 27,290 inhabitants of the former kingdom.

Jewish tradition affirms the Assyrian deportations and replacement of the previous inhabitants by forced resettlement by other peoples but claims a different ethnic origin for the Samaritans. The Talmud accounts for a people called "Cuthim" on a number of occasions, mentioning their arrival by the hands of the Assyrians. According to 2 Kings and Josephus, the people of Israel were removed by the king of the Assyrians (Sargon II) to Halah, to Gozan on the Khabur River and to the towns of the Medes. The king of the Assyrians then brought people from Babylon, Cuthah, Avah, Emath, and Sepharvaim to place in Samaria. Because God sent lions among them to kill them, the king of the Assyrians sent one of the priests from Bethel to teach the new settlers about God's ordinances. The eventual result was that the new settlers worshiped both the God of the land and their own gods from the countries from which they came.

This account is contradicted by the version in Chronicles, where, following Samaria's destruction, King Hezekiah is depicted as endeavouring to draw the Ephraimites and Manassites closer to Judah. Temple repairs at the time of Josiah were financed by money from all "the remnant of Israel" in Samaria, including from Manasseh, Ephraim, and Benjamin. Jeremiah likewise speaks of people from Shekhem, Shiloh, and Samaria who brought offerings of frankincense and grain to the House of YHWH. Chronicles makes no mention of an Assyrian resettlement. Yitzakh Magen argues that the version of Chronicles is perhaps closer to the historical truth and that the Assyrian settlement was unsuccessful, a notable Israelite population remained in Samaria, part of which, following the conquest of Judah, fled south and settled there as refugees.

A Midrash (Genesis Rabbah Sect. 94) relates about an encounter between Rabbi Meir and a Samaritan. The story that developed includes the following dialogue:

<poem>Rabbi Meir: What tribe are you from?
The Samaritan: From Joseph.
Rabbi Meir: No!
The Samaritan: From which one then?
Rabbi Meir: From Issachar.
The Samaritan: How do you figure?
Rabbi Meir: For it is written (Gen 46:13): The sons of Issachar: Tola, Puvah, Iob, and Shimron. These are the Samaritans (shamray).</poem>

Zertal dates the Assyrian onslaught at 721 BCE to 647 BCE and discusses three waves of imported settlers. He shows that Mesopotamian pottery in Samaritan territory cluster around the lands of Menasheh and that the type of pottery found was produced around 689 BCE. Some date their split with the Jews to the time of Nehemiah, Ezra, and the building of the Second Temple in Jerusalem after the Babylonian exile. Returning exiles considered the Samaritans to be non-Israelites and, thus, not fit for this religious work.

The "Encyclopaedia Judaica" (under "Samaritans") summarizes both past and present views on the Samaritans' origins. It says:

Furthermore, to this day the Samaritans claim descent from the tribe of Joseph:

The Dead Sea scroll 4Q372 hopes that the northern tribes will return to the land of Joseph. The current dwellers in the north are referred to as fools, an enemy people. However, they are not referred to as foreigners. It goes on to say that the Samaritans mocked Jerusalem and built a temple on a high place to provoke Israel.

The account of the Assyrian kings, which was among the archaeological discoveries in Babylon, differs from the Samaritan account, and confirms much of the Jewish biblical account but may differ in regard to the ethnicity of the foreigners settled in Samaria by the Assyrians. At one point, it is simply said that they were from Arabia, while at another, that they were brought from a number of countries conquered by Sargon II:

Also,

The narratives in Genesis about the rivalries among the twelve sons of Jacob are viewed by some as describing tensions between north and south. They were temporarily united in the United Monarchy, but after the death of Solomon, the kingdom split in two, the Kingdom of Israel with its last capital city Samaria and the Kingdom of Judah with its capital Jerusalem.

The Deuteronomistic history, written in Judah, portrayed Israel as a sinful kingdom, divinely punished for its idolatry and iniquity by being destroyed by the Assyrians in 720 BCE.

The tensions continued in the postexilic period. The Books of Kings are more inclusive than Ezra–Nehemiah since the ideal is of one Israel with twelve tribes, whereas the Books of Chronicles concentrate on the Kingdom of Judah and ignore the Kingdom of Israel (Samaria).

The Samaritans claimed that they were the true Israel who were descendants of the "Ten Lost Tribes" taken into Assyrian captivity. They had their own sacred precinct on Mount Gerizim and claimed that it was the original sanctuary. Moreover, they claimed that their version of the Pentateuch was the original and that the Jews had a falsified text produced by Ezra during the Babylonian exile.

Both Jewish and Samaritan religious leaders taught that it was wrong to have any contact with the opposite group, and neither was to enter each other's territories or even to speak to one another. During the New Testament period, the tensions were exploited by Roman authorities as they likewise had done between rival tribal factions elsewhere, and Josephus reports numerous violent confrontations between Jews and Samaritans throughout the first half of the first century.

According to historian Lawrence Schiffman, throughout the Persian Period, Judeans and Samaritans fought periodically with one another. The Samaritans were a blend of all kinds of people—made up of Israelites who were not exiled when the Northern Kingdom was destroyed in 722 BCE—of various different nationalities whom the Assyrians had resettled in the area. The Assyrians did this as an attempt to ensure that Israel’s national dream could not come true.

According to the Jewish version of events, when the Judean exile ended in 539 BCE and the exiles began returning home from Babylon, Samaritans found their former homeland of the north populated by other people who claimed the land as their own and Jerusalem, their former glorious capital, in ruins. The inhabitants worshiped the Pagan gods, but when the then-sparsely populated areas became infested with dangerous wild beasts, they appealed to the king of Assyria for Israelite priests to instruct them on how to worship the "God of that country." The result was a syncretistic religion, in which national groups worshiped the Israelite God, but they also served their own gods in accordance with the customs of the nations from which they had been brought.

According to Chronicles 36:22–23, the Persian emperor, Cyrus the Great (reigned 559–530 BCE), permitted the return of the exiles to their homeland and ordered the rebuilding of the Temple (Zion). The prophet Isaiah identified Cyrus as "the Lord's Messiah". The word "Messiah" refers to an anointed individual, such as a king or priest.

During the First Temple, it was possible for foreigners to help the Jewish people in an informal way until tension grew between the Samaritans and Judeans. This meant that foreigners could physically move into Judean land and abide by its laws and religion.

Ezra 4 says that the local inhabitants of the land offered to assist with the building of the new Temple during the time of Zerubbabel, but their offer was rejected. According to Ezra, this rejection precipitated a further interference not only with the rebuilding of the Temple but also with the reconstruction of Jerusalem. The issue surrounding the Samaritans offer to help rebuild the temple was a complicated one that took a while for the Judeans to think over. There had always been a division between the north and the south and this instance perfectly illustrates that. Following Solomon's death, sectionalism formed and inevitably led to the division of the kingdom. This division lead to the Judeans rejecting the offer made by the Samaritans to centralise worship at the Temple.

The text is not clear on this matter, but one possibility is that these "people of the land" were thought of as Samaritans. We do know that Samaritan and Jewish alienation increased and that the Samaritans eventually built their own temple on Mount Gerizim, near Shechem.

The rebuilding of the Jewish Temple in Jerusalem took several decades. The project was first led by Sheshbazzar (ca. 538 BCE), later by Zerubbabel and Jeshua, and later still by Haggai and Zechariah (520–515 BCE). The work was completed in 515 BCE.

The term "Kuthim" applied by Jews to the Samaritans had clear pejorative connotations, implying that they were interlopers brought in from Kutha in Mesopotamia and rejecting their claim of descent from the ancient Tribes of Israel.

According to many scholars, archaeological excavations at Mount Gerizim indicate that a Samaritan temple was built there in the first half of the 5th century BCE. The date of the schism between Samaritans and Jews is unknown, but by the early 4th century BCE the communities seem to have had distinctive practices and communal separation.

Antiochus IV Epiphanes was on the throne of the Seleucid Empire from 175 to 163 BCE. His policy was to Hellenize his entire kingdom and standardize religious observance. According to 1 Maccabees 1:41-50 he proclaimed himself the incarnation of the Greek god Zeus and mandated death to anyone who refused to worship him. In the 2nd century BCE, a series of events led to a revolution by a faction Judeans against Antiochus IV.

The universal peril led the Samaritans, eager for safety, to repudiate all connection and kinship with the Jews. The request was granted. This was put forth as the final breach between the two groups, being alleged at a much later date in the Christian Bible (John 4:9), "For Jews have no dealings with Samaritans"—or not "alleged" if the Greek sunchrasthai merely refers to not sharing utensils (NABRE).

Anderson notes that during the reign of Antiochus IV (175–164 BCE):

Josephus Book 12, Chapter 5 quotes the Samaritans as saying:

During the Hellenistic period, Samaria was largely divided between a Hellenizing faction based in Samaria (Sebastaea) and a pious faction, led by the High Priest and based largely around Shekhem and the rural areas. Samaria was a largely autonomous state nominally dependent on the Seleucid Empire until around 113 BCE, when the Jewish Hasmonean ruler John Hyrcanus destroyed the Samaritan temple and devastated Samaria.

The Hellinized Samaritan Temple at Mount Gerizim was destroyed by John Hyrcanus in 113 BC, having existed about 200 years. Only a few stone remnants of it exist today.

Under the Roman Empire, Samaria became a part of the Herodian Kingdom, Herodian Tetrarchy and with deposition of the Herodian ethnarh Herod Achelaus in early 1st century CE, Samaria became a part of the province of Judaea.

Samaritans appear briefly in the Christian gospels, most notably in the account of the Samaritan woman at the well and the parable of the Good Samaritan. In the latter, it is only the Samaritan who helped the man stripped of clothing, beaten, and left on the road half dead, his Abrahamic covenantal circumcision implicitly evident. The priest and Levite walked past. But the Samaritan helped the naked man regardless of his nakedness (itself religiously offensive to the priest and Levite), his self-evident poverty, or to which Hebrew sect he belonged (which was unclear to any, due to his nakedness).

The Temple of Gerizim was rebuilt after the Bar Kokhba revolt against the Romans, around 136 CE. A building dated to the second century BCE, the Delos Synagogue, is commonly identified as a Samaritan synagogue, which would make it the oldest known Jewish or Samaritan synagogue. On the other hand, Matassa argues that, although there is evidence of Samaritans on Delos, there is no evidence the building was a synagogue.

Much of Samaritan liturgy was set by the high priest Baba Rabba in the 4th century.

There were some Samaritans in the Sasanian Empire, where they served in the army.

This period is considered as something of a golden age for the Samaritan community, the population thought to number up to a million. 

According to Samaritan sources, Eastern Roman emperor Zeno (who ruled 474–491 and whom the sources call "Zait the King of Edom") persecuted the Samaritans. The Emperor went to Neapolis (Shechem), gathered the elders and asked them to convert; when they refused, Zeno had many Samaritans killed, and re-built the synagogue as a church. Zeno then took for himself Mount Gerizim, where the Samaritans worshiped God, and built several edifices, among whom a tomb for his recently deceased son, on which he put a cross, so that the Samaritans, worshiping God, would prostrate in front of the tomb. Later, in 484, the Samaritans revolted. The rebels attacked Sichem, burned five churches built on Samaritan holy places and cut the finger of bishop Terebinthus, who was officiating the ceremony of Pentecost. They elected a Justa (or Justasa/Justasus) as their king and moved to Caesarea, where a noteworthy Samaritan community lived. Here several Christians were killed and the church of St. Sebastian was destroyed. Justa celebrated the victory with games in the circus. According to John Malalas, the "dux Palaestinae" Asclepiades, whose troops were reinforced by the Caesarea-based Arcadiani of Rheges, defeated Justa, killed him and sent his head to Zeno. According to Procopius, Terebinthus went to Zeno to ask for revenge; the Emperor personally went to Samaria to quell the rebellion.

Some modern historians believe that the order of the facts preserved by Samaritan sources should be inverted, as the persecution of Zeno was a consequence of the rebellion rather than its cause, and should have happened after 484, around 489. Zeno rebuilt the church of St. Procopius in Neapolis (Sichem) and the Samaritans were banned from Mount Gerizim, on whose top a signalling tower was built to alert in case of civil unrest.

Under a charismatic, messianic figure named Julianus ben Sabar (or ben Sahir), the Samaritans launched a war to create their own independent state in 529. With the help of the Ghassanids, Emperor Justinian I crushed the revolt; tens of thousands of Samaritans died or were enslaved. The Samaritan faith, which had previously enjoyed the status of "religio licita", was virtually outlawed thereafter by the Christian Byzantine Empire; from a population once at least in the hundreds of thousands, the Samaritan community dwindled to tens of thousands.

Though initially guaranteed religious freedom after the Muslim conquest of Palestine, Samaritan numbers dropped further as a result of massacres and conversions.

By the time of the early Muslim conquests, apart from Palestine, small dispersed communities of Samaritans were living also in Arab Egypt, Syria, and Iran. Like other non-Muslims in the empire, such as Jews, Samaritans were often considered to be People of the Book. Their minority status was protected by the Muslim rulers, and they had the right to practice their religion, but, as dhimmi, adult males had to pay the jizya or "protection tax". This however changed during late Abbasid period, with increasing persecution targeting the Samaritan community and considering them infidels which must convert to Islam. The tradition of men wearing a red tarboosh may go back to an order by the Abbasid Caliph al-Mutawakkil (847-861 CE) that required non-Muslims to be distinguished from Muslims.

During the Crusades, Samaritans, like the non-Latin Christian inhabitants of the Kingdom of Jerusalem, were second-class citizens, but they were tolerated and perhaps favored because they were docile and had been mentioned positively in the Christian New Testament.

While the majority of the Samaritan population in Damascus was massacred or converted during the reign of the Ottoman Pasha Mardam Beq in the early 17th century, the remainder of the Samaritan community there, in particular, the Danafi family, which is still influential today, moved back to Nablus in the 17th century.

The Nablus community endured because most of the surviving diaspora returned, and they have maintained a tiny presence there to this day. In 1624, the last Samaritan High Priest of the line of Eleazar son of Aaron died without issue, but according to Samaritan tradition, descendants of Aaron's other son, Ithamar, remained and took over the office.

By the late Ottoman period, the Samaritan community dwindled to its lowest. In 19th century, with pressure of conversion and persecution from the local rulers and occasional natural disasters, the community fell to just over 100 persons.

The situation of the Samaritan community improved significantly during the British Mandate of Palestine. At that time, they began to work in the public sector, like many other groups. The censuses of 1922 and 1931 recorded 163 and 182 Samaritans in Palestine, respectively. The majority of them lived in Nablus.

After the end of the British Mandate of Palestine and the subsequent establishment of the State of Israel, some of the Samaritans who were living in Jaffa emigrated to Samaria and lived in Nablus. By the late 1950s, around 100 Samaritans left the West Bank for Israel under an agreement with the Jordanian authorities in the West Bank. In 1954, Israeli President Yitzhak Ben-Zvi fostered a Samaritan enclave in Holon, Israel. 

Until the 1990s, most of the Samaritans resided in the West Bank city of (Nablus) below Mount Gerizim. They relocated to the mountain itself near the Israeli village Har Brakha as a result of violence during the First Intifada (1987–1990). Consequently, all that is left of the Samaritan community in Nablus itself is an abandoned synagogue. The Israeli army maintains a presence in the area.

Demographic investigations of the Samaritan community were carried out in the 1960s. Detailed pedigrees of the last 13 generations show that the Samaritans comprise four lineages:

Recently several genetic studies on the Samaritan population were made using haplogroup comparisons as well as wide-genome genetic studies. Of the 12 Samaritan males used in the analysis, 10 (83%) had Y chromosomes belonging to haplogroup J, which includes three of the four Samaritan families. The Joshua-Marhiv family belongs to Haplogroup J-M267 (formerly "J1"), while the Danafi and Tsedakah families belong to haplogroup J-M172 (formerly "J2"), and can be further distinguished by M67, the derived allele of which has been found in the Danafi family. The only Samaritan family not found in haplogroup J was the Cohen family (Tradition: Tribe of Levi) which was found haplogroup E-M78 (formerly "E3b1a M78"). This article predated the change of the classification of haplogroup E3b1-M78 to E3b1a-M78 and the further subdivision of E3b1a-M78 into 6 subclades based on the research of Cruciani, et al.

The 2004 article on the genetic ancestry of the Samaritans by Shen "et al." concluded from a sample comparing Samaritans to several Jewish populations, all currently living in Israel—representing the Beta Israel, Ashkenazi Jews, Iraqi Jews, Libyan Jews, Moroccan Jews, and Yemenite Jews, as well as Israeli Druze and Palestinians—that "the principal components analysis suggested a common ancestry of Samaritan and Jewish patrilineages. Most of the former may be traced back to a common ancestor in what is today identified as the paternally inherited Israelite high priesthood (Cohanim) with a common ancestor projected to the time of the Assyrian conquest of the kingdom of Israel."

Archaeologists Aharoni, et al., estimated that this "exile of peoples to and from Israel under the Assyrians" took place during ca. 734–712 BCE. The authors speculated that when the Assyrians conquered the Northern Kingdom of Israel, resulting in the exile of many of the Israelites, a subgroup of the Israelites that remained in the Land of Israel "married Assyrian and female exiles relocated from other conquered lands, which was a typical Assyrian policy to obliterate national identities." The study goes on to say that "Such a scenario could explain why Samaritan Y chromosome lineages cluster tightly with Jewish Y lineages, while their mitochondrial lineages are closest to Iraqi Jewish and Israeli Arab mtDNA sequences." Non-Jewish Iraqis were not sampled in this study; however, mitochondrial lineages of Jewish communities tend to correlate with their non-Jewish host populations, unlike paternal lineages which almost always correspond to Israelite lineages.

, there were 796 Samaritans, half of whom reside in their modern homes at Kiryat Luza on Mount Gerizim, which is sacred to them, and the rest in the city of Holon, just outside Tel Aviv. There are also four Samaritan families residing in Binyamina-Giv'at Ada, Matan, and Ashdod.

As a small community physically divided between neighbors in a hostile region, Samaritans have been hesitant to overtly take sides in the Arab–Israeli conflict, fearing that doing so could lead to negative repercussions. While the Samaritan communities in both the West Bank's Nablus and Israeli Holon have assimilated to the surrounding respective cultures, Hebrew has become the primary domestic language for Samaritans. Samaritans who are Israeli citizens are drafted into the military, along with the Jewish citizens of Israel.

Relations of Samaritans with Jewish Israelis and Muslim and Christian Palestinians in neighboring areas have been mixed. Samaritans living in both Israel and in the West Bank enjoy Israeli citizenship. Samaritans in the Palestinian Authority-ruled territories are a minority in the midst of a Muslim majority. They had a reserved seat in the Palestinian Legislative Council in the election of 1996, but they no longer have one. Samaritans living in Samaria have been granted passports by both Israel and the Palestinian Authority.

One of the biggest problems facing the community today is the issue of continuity. With such a small population, divided into only four families (Cohen, Tsedakah, Danafi, and Marhiv, a fifth family dying out in the twentieth century) and a general refusal to accept converts, there has been a history of genetic disorders within the group due to the small gene pool. To counter this, the Samaritan community has recently agreed that men from the community marry non-Samaritan (primarily, Israeli Jewish) women, provided that the women agree to follow Samaritan religious practices. There is a six-month trial period before officially joining the Samaritan community to see whether this is a commitment that the woman would like to take. This often poses a problem for the women, who are typically less than eager to adopt the strict interpretation of biblical (Levitical) laws regarding menstruation, by which they must live in a separate dwelling during their periods and after childbirth. There have been a few instances of intermarriage. In addition, all marriages within the Samaritan community are first approved by a geneticist at Tel HaShomer Hospital, in order to prevent the spread of genetic disorders. In meetings arranged by "international marriage agencies", a small number of Ukrainian women have recently been allowed to marry into the community in an effort to expand the gene pool.

The Samaritan community in Israel also faces demographic challenges as young people leave the community and convert to Judaism. A notable example is Israeli television presenter Sofi Tsedaka, who has made a documentary about her leaving the community at age 18.

The head of the community is the Samaritan High Priest, who is selected by age from the priestly family and resides on Mount Gerizim. The current high priest is Aabed-El ben Asher ben Matzliach who assumed the office in 2013.

Much of the local Palestinian population of Nablus is believed to be descended from Samaritans who had converted to Islam. According to the historian Fayyad Altif, large numbers of Samaritans converted due to persecution under various Muslim rulers, and because the monotheistic nature of Islam made it easy for them to accept it. The Samaritans themselves describe the Ottoman period as the worst period in their modern history, as many Samaritan families were forced to convert to Islam during that time. Even today, certain Nabulsi family names such as Maslamani, Yaish, and Shaksheer among others, are associated with Samaritan ancestry.

For the Samaritans in particular, the passing of the al-Hakim Edict by the Fatimid Caliphate in 1021, under which all Jews and Christians in the Fatimid ruled southern Levant were ordered to either convert to Islam or leave, along with another notable forced conversion to Islam imposed at the hands of the rebel ibn Firāsa, would contribute to their rapid unprecedented decrease, and ultimately almost complete extinction as a separate religious community. As a result, they had decreased from nearly a million and a half in late Roman (Byzantine) times to 146 people by the end of the Ottoman Era.

In 1940, the future Israeli president and historian Yitzhak Ben-Zvi wrote an article in which he stated that two thirds of the residents of Nablus and the surrounding neighboring villages were of Samaritan origin. He mentioned the name of several Palestinian Muslim families as having Samaritan origins, including the Buwarda and Kasem families, who protected Samaritans from Muslim persecution in the 1850s. He further claimed that these families had written records testifying to their Samaritan ancestry, which were maintained by their priests and elders.

According to "The Economist", "most ethnic Samaritans are now pious Muslims."

The Samaritan religion is based on some of the same books used as the basis of Judaism but differs from the latter. Samaritan religious works include the Samaritan version of the Torah, the Memar Markah, the Samaritan liturgy, and Samaritan law codes and biblical commentaries. Many claim the Samaritans appear to have a text of the Torah as old as the Masoretic Text; scholars have various theories concerning the actual relationships between these three texts.

According to Samaritans, it was on Mount Gerizim that Abraham was commanded by God to offer Isaac, his son, as a sacrifice. In both narratives, God then causes the sacrifice to be interrupted, explaining that this was the ultimate test of Abraham's obedience, as a result of which all the world would receive blessing.

The Torah mentions the place where God chooses to establish His name (Deut 12:5), and Judaism takes this to refer to Jerusalem. However, the Samaritan text speaks of the place where God "has chosen" to establish His name, and Samaritans identify it as Mount Gerizim, making it the focus of their spiritual values.

The legitimacy of the Samaritan temple was attacked by Jewish scholars including Andronicus ben Meshullam.

In the Christian Bible, the Gospel of John relates an encounter between a Samaritan woman and Jesus in which she says that the mountain was the center of their worship. She poses the question to Jesus when she realizes that he is the Messiah. Jesus affirms the Jewish position, saying "You (that is, the Samaritans) worship what you do not know".


The Samaritans have retained an offshoot of the Ancient Hebrew script, a High Priesthood, the slaughtering and eating of lambs on Passover eve, and the celebration of the first month's beginning around springtime as the New Year. Yom Teru`ah (the biblical name for "Rosh Hashanah"), at the beginning of Tishrei, is not considered a New Year as it is in Rabbinic Judaism. The Samaritan Pentateuch differs from the Jewish Masoretic Text as well. Some differences are doctrinal: for example, the Samaritan Torah explicitly states that Mount Gerizim is "the place that God "has chosen"" to establish His name, as opposed to the Jewish Torah that refers to "the place that God "chooses"". Other differences are minor and seem more or less accidental.

Samaritans refer to themselves as "Benai Yisrael" ("Children of Israel") which is a term used by all Jewish denominations as a name for the Jewish people as a whole. They, however, do not refer to themselves as "Yehudim" (Jews), the standard Hebrew name for Jews.

The Talmudic attitude expressed in tractate Kutim is that they are to be treated as Jews in matters where their practice coincides with Rabbinic Judaism but as non-Jews where their practice differs. Some claim that since the 19th century, Rabbinic Judaism has regarded the Samaritans as a Jewish sect and the term "Samaritan Jews" has been used for them.

Samaritan law is not the same as Halakha (Rabbinic Jewish law). The Samaritans have several groups of religious texts, which correspond to Jewish Halakha. A few examples of such texts are:


Samaria or Samaritans are mentioned in the New Testament books of Matthew, Luke, John and Acts. The Gospel of Mark contains no mention of Samaritans or Samaria. The best known reference to the Samaritans is the Parable of the Good Samaritan, found in the Gospel of Luke. The following references are found:

The rest of the New Testament makes no specific mention of Samaria or Samaritans.

"The Samaritan News", a monthly magazine started in 1969, is written in Samaritan Aramaic, Hebrew, Arabic, and English and deals with current and historical issues with which the Samaritan community is concerned. The "Samaritan Update" is a bi-monthly e-newsletter for Samaritan Studies.

Samaritan view


Jewish view

Independent views


Books and other information


Photographic links



</doc>
<doc id="28180" url="https://en.wikipedia.org/wiki?curid=28180" title="Seneca Lake (New York)">
Seneca Lake (New York)

Seneca Lake is the largest of the glacial Finger Lakes of the U.S. state of New York, and the deepest lake entirely within the state. It is promoted as being the lake trout capital of the world, and is host of the National Lake Trout Derby. Because of its depth and relative ease of access, the US Navy uses Seneca Lake to perform test and evaluation of equipment ranging from single element transducers to complex sonar arrays and systems.
The lake takes its name from the Seneca nation of Native Americans. At the north end of Seneca Lake is the city of Geneva, New York, home of Hobart and William Smith Colleges and the New York State Agricultural Experiment Station, a division of Cornell University. At the south end of the lake is the village of Watkins Glen, New York, famed for auto racing and waterfalls.

Due to Seneca Lake's unique macroclimate it is home to over 50 wineries, many of them farm wineries and is the location of the Seneca Lake AVA. (See Seneca Lake wine trail).

At long, it is the second longest of the Finger Lakes and has the largest volume, estimated at , roughly half of the water in all the Finger Lakes. It has an average depth of , a maximum depth of , and a surface area of .

For comparison, Scotland's famous Loch Ness is long, wide, has a surface area of , an average depth of , a maximum depth of , and total volume of of water.

Seneca's two main inlets are Catharine Creek at the southern end and the Keuka Lake Outlet. Seneca Lake lets out into the Seneca River/ Cayuga-Seneca Canal, which joins Seneca and Cayuga Lakes at their northern ends.

It is fed by underground springs and replenished at a rate of 328,000 gallons (1240 m³) per minute. These springs keep the water moving in a circular motion, giving it little chance to freeze over. Because of Seneca Lake's great depth its temperature remains a near-constant . In summer the top warms to .

Seneca lake has a typical aquatic population for large deep lakes in the northeast, with coldwater fish such as lake trout and Atlantic salmon inhabiting the deeper waters, and warmwater fish such as smallmouth bass and yellow perch inhabiting the shallower areas. The lake is also home to a robust population of "sawbellies," the local term for alewife shad.

Seneca Lake was formed at least two million years ago by glacial carving of streams and valleys. Originally it was a part of a series of rivers that flowed northward. Around this time many continental glaciers moved into the area and started the Pleistocene glaciation also known as the Ice Age. It is presumed that the Finger Lakes were created by many advances and retreats of massive glaciers that were up to 2 miles wide.

Over 200 years ago, there were Iroquois villages on Seneca Lake's surrounding hillsides. During the American Revolutionary War, their villages, including Kanadaseaga ("Seneca Castle"), were wiped out during the 1779 Sullivan Expedition by Continental troops under order by General George Washington (in retaliation of the Wyoming Massacre ) to invade their homeland, destroy their dwellings and crops, and end their threat to the patriots. They destroyed nearly 50 Seneca and Cayuga villages. Today roadside signs trace Sullivan and Clinton's route along the east side of Seneca Lake where the burning of villages and crops occurred.

After the war, the Iroquois were forced to cede their land when Britain was defeated. Their millions of acres were sold and some lands in this area were granted to veterans of the army in payment for their military service. A slow stream of European-American settlers began to arrive circa 1790. Initially the settlers were without a market nearby or a way to get their crops to market. The settlers' isolation ended in 1825 with the opening of the Erie Canal.

The canal linked the Finger Lakes Region to the outside world. Steamships, barges and ferries quickly became Seneca Lake's ambassadors of commerce and trade. The former, short Crooked Lake Canal linked Seneca Lake to Keuka Lake.

Numerous canal barges sank during operations and rest on the bottom of the lake. A collection of barges at the southwest end of the lake, near the village of Watkins Glen, is being preserved and made accessible for scuba diving by the Finger Lakes Underwater Preserve Association.

The lake is a popular fishing destination from all around. Fish species present in the lake include lake trout, rainbow trout, brown trout, landlocked salmon, largemouth bass, smallmouth bass, northern pike, pickerel, and yellow perch.

The painted rocks located at the southern end of the lake on the eastern cliff face depict an American flag, Tee-pee, and several Native Americans. The older paintings, located on the bottom of the cliff, were said to have been drawn in 1779 after the Senecas escaped men from John Sullivan's campaign. However, this account is questioned by historian Barbara Bell, arguing that it is unlikely that the Senecas would have returned to paint the paintings having just escaped from Sullivan's men. She suggests instead that these paintings may have been made much later, for tourists on Seneca Lake boat tours.

It is known that the more visible and prominent paintings of the Native Americans, American flag, and Tee-pee were added in 1929 during the Sullivan Sesquicentennial. There are two mistakes in these 1929 additions: firstly the Native Americans in the Seneca Region used longhouses and not Tee-pees, and secondly the flag is displayed pointing to the left which is never to be done on a horizontal surface.

Seneca Lake is also the site of strange and currently unexplained cannon-like booms and shakes that are heard and felt in the surrounding area. They are known locally as the Seneca Guns, Lake Drums, or Lake Guns, and these types of phenomena are known elsewhere as skyquakes. The term Lake Guns originated in the short story "The Lake Gun" by James Fenimore Cooper in 1851. There is no explanation that takes into account sounds the Iroquois heard before Cooper's time; it is possible sonic booms have been mistaken for natural sounds in modern days.

The east side of Seneca Lake was once home to a military training ground called Sampson Naval Base, primarily used during World War II. It became Sampson Air Force Base during the Korean War and was used for basic training. After Sampson AFB closed, the airfield remained as Seneca Army Airfield but was closed in 2000. The training grounds of Sampson have since been converted to a civilian picnic area called Sampson State Park. 

There is still a Naval facility at Seneca Lake, the Naval Undersea Warfare Center (NUWC) Sonar test facility. A scale model of the sonar section of the nuclear submarine USS Seawolf (SSN 21) was tested during the development of this ship, which was launched in June, 1995.

There is a YSI EMM-2500 Buoy Platform located in the north end of Seneca Lake roughly in the center. Its coordinates are: latitude: 42°41'49.99"N, longitude: 76°55'29.93"W. The buoy has cellular modem communications and measures wind speed and direction, relative humidity, air temperature, barometric pressure, light intensity, and the water's depth and temperature, conductivity, turbidity, and chlorophyll-a levels.

The buoy was initially deployed in June 2006. The water depth where it is located is about .

Viticulture and winemaking in the area date back to the 19th century, with the foundation of the Seneca Lake Wine Company in 1866 marking the first major winery in the area. The modern era of wine production began in the 1970s with the establishment of several wineries and the passage of the New York Farm Winery Act of 1976. The region was established as an American Viticultural Area in 1988.

Seneca Lake Wine Trail hosts many events on and around the lake, annually. With more than 30 wineries currently located on the shores of Seneca Lake, the winter 'Deck the Halls' event is a great time at the lake with participating wineries showcasing their vintages and pairing these wines with distinctive, tasty treats. Wineries also provide participants with an ornament at each stop to commemorate the event.

The Elmira & Seneca Lake Railway opened for operation on June 19, 1900 from Horseheads, New York to Seneca Lake.



</doc>
<doc id="28181" url="https://en.wikipedia.org/wiki?curid=28181" title="Strait of Gibraltar">
Strait of Gibraltar

The Strait of Gibraltar ( , ) is a narrow strait that connects the Atlantic Ocean to the Mediterranean Sea and separates Gibraltar and Peninsular Spain in Europe from Morocco and Ceuta (Spain) in Africa. The name comes from the Rock of Gibraltar, which in turn originates from the Arabic "Jebel Tariq" (meaning "Tariq's mountain") named after Tariq ibn Ziyad. It is also known as the Straits of Gibraltar, the Gut of Gibraltar (although this is mostly archaic), the STROG (Strait Of Gibraltar) in naval use, and Bab Al Maghrib (), "Gate of the West". In the Middle Ages, Muslims called it "Al-Zuqaq", "The Passage", the Romans called it Fretum Gatitanum (Strait of Cadiz), and in the ancient world it was known as the "Pillars of Hercules" (Ancient Greek: αἱ Ἡράκλειοι στῆλαι).

Europe and Africa are separated by of ocean at the strait's narrowest point. The Strait's depth ranges between which possibly interacted with the lower mean sea level of the last major glaciation 20,000 years ago when the level of the sea is believed to have been lower by . Ferries cross between the two continents every day in as little as 35 minutes. The Spanish side of the Strait is protected under El Estrecho Natural Park.

On the northern side of the Strait are Spain and Gibraltar (a British overseas territory in the Iberian Peninsula), while on the southern side are Morocco and Ceuta (a Spanish exclave in Morocco). Its boundaries were known in antiquity as the Pillars of Hercules. There are several islets, such as the disputed Isla Perejil, that are claimed by both Morocco and Spain.

Due to its location, the Strait is commonly used for illegal immigration from Africa to Europe.

The International Hydrographic Organization defines the limits of the Strait of Gibraltar as follows:

The seabed of the Strait is composed of synorogenic Betic-Rif clayey flysch covered by Pliocene and/or Quaternary calcareous sediments, sourced from thriving cold water coral communities. Exposed bedrock surfaces, coarse sediments and local sand dunes attest to the strong bottom current conditions at the present time.

Around 5.9 million years ago, the connection between the Mediterranean Sea and the Atlantic Ocean along the Betic and Rifan Corridor was progressively restricted until its total closure, effectively causing the salinity of the Mediterranean to rise periodically within the gypsum and salt deposition range, during what is known as the Messinian salinity crisis. In this water chemistry environment, dissolved mineral concentrations, temperature and stilled water currents combined and occurred regularly to precipitate many mineral salts in layers on the seabed. The resultant accumulation of various huge salt and mineral deposits about the Mediterranean basin are directly linked to this era. It is believed that this process took a short time, by geological standards, lasting between 500,000 and 600,000 years.

It is estimated that, were the straits closed even at today's higher sea level, most water in the Mediterranean basin would evaporate within only a thousand years, as it is believed to have done then, and such an event would lay down mineral deposits like the salt deposits now found under the sea floor all over the Mediterranean.

After a lengthy period of restricted intermittent or no water exchange between the Atlantic Ocean and Mediterranean basin, approximately 5.33 million years ago, the Atlantic-Mediterranean connection was completely reestablished through the Strait of Gibraltar by the Zanclean flood, and has remained open ever since. The erosion produced by the incoming waters seems to be the main cause for the present depth of the strait (900 m at the narrows, 280 m at the Camarinal Sill). The strait is expected to close again as the African Plate moves northward relative to the Eurasian Plate, but on geological rather than human timescales.

The Strait has been identified as an Important Bird Area by BirdLife International because hundreds of thousands of seabirds use it every year to pass between the Mediterranean and the Atlantic, including large numbers of Cory's and Balearic shearwaters, Audouin's, yellow-legged and lesser black-backed gulls, razorbills and Atlantic puffins.

Evidence of the first human habitation of the area by Neanderthals dates back to 125,000 years ago. It is believed that the Rock of Gibraltar may have been one of the last outposts of Neanderthal habitation in the world, with evidence of their presence there dating to as recently as 24,000 years ago. Archaeological evidence of Homo sapiens habitation of the area dates back c. 40,000 years.

The relatively short distance between the two shores has served as a quick crossing point for various groups and civilizations throughout history, including Carthaginians campaigning against Rome, Romans travelling between the provinces of Hispania and Mauritania, Vandals raiding south from Germania through Western Rome and into North Africa in the 5th century, Moors and Berbers in the 8th–11th centuries, and Spain and Portugal in the 16th century.

Beginning in 1492, the straits began to play a certain cultural role in acting as a barrier against cross-strait conquest and the flow of culture and language that would naturally follow such a conquest. In that year, the last Muslim government north of the straits was overthrown by a Spanish force. Since that time, the straits have come to foster the development of two very distinct and varied cultures on either side of the straits after sharing much the same culture and greater degrees of tolerance for over 300+ years from the 8th century to the early 13th century.

On the northern side, Christian-European culture has remained dominant since the expulsion of the last Muslim kingdom in 1492, along with the Romance Spanish language, while on the southern side, Muslim-Arabic/Mediterranean has been dominant since the spread of Islam into North Africa in the 700s, along with the Arabic language. For the last 500 years, religious and cultural intolerance, more than the small travel barrier that the straits present, has come to act as a powerful enforcing agent of the cultural separation that exists between these two groups.

The small British enclave of the city of Gibraltar presents a third cultural group found in the straits. This enclave was first established in 1704 and has since been used by Britain to act as a surety for control of the sea lanes into and out of the Mediterranean.

Following the Spanish coup of July 1936 the Spanish Republican Navy tried to blockade the Strait of Gibraltar to hamper the transport of Army of Africa troops from Spanish Morocco to Peninsular Spain. On 5 August 1936 the so-called Convoy de la victoria was able to bring at least 2,500 men across the strait, breaking the republican blockade.

The Strait is an important shipping route from the Mediterranean to the Atlantic. There are ferries that operate between Spain and Morocco across the strait, as well as between Spain and Ceuta and Gibraltar to Tangier.

In December 2003, Spain and Morocco agreed to explore the construction of an undersea rail tunnel to connect their rail systems across the Strait. The gauge of the rail would be to match the proposed construction and conversion of significant parts of the existing broad gauge system to standard gauge. While the project remains in a planning phase, Spanish and Moroccan officials have met to discuss it as recently as 2012, and proposals predict it could be completed by 2025.

The Strait of Gibraltar links the Atlantic Ocean directly to the Mediterranean Sea. This direct linkage creates certain unique flow and wave patterns. These unique patterns are created due to the interaction of various regional and global evaporative forces, tidal forces, and wind forces.

Through the strait, water generally flows more or less continually in both an eastward and a westward direction. A smaller amount of deeper saltier and therefore denser waters continually work their way westwards (the Mediterranean outflow), while a larger amount of surface waters with lower salinity and density continually work their way eastwards (the Mediterranean inflow). These general flow tendencies may be occasionally interrupted for brief periods by temporary tidal flows, depending on various lunar and solar alignments. Still, on the whole and over time, the balance of the water flow is eastwards, due to an evaporation rate within the Mediterranean basin higher than the combined inflow of all the rivers that empty into it. At the strait's far western end is the Camarinal Sill, the strait's shallowest point which limits mixing between the cold, less saline Atlantic water and the warm Mediterranean waters.

The Mediterranean waters are so much saltier than the Atlantic waters that they sink below the constantly incoming water and form a highly saline ("thermohaline", both warm and salty) layer of bottom water. This layer of bottom-water constantly works its way out into the Atlantic as the Mediterranean outflow. On the Atlantic side of the strait, a density boundary separates the Mediterranean outflow waters from the rest at about depth. These waters flow out and down the continental slope, losing salinity, until they begin to mix and equilibrate more rapidly, much further out at a depth of about . The Mediterranean outflow water layer can be traced for thousands of kilometres west of the strait, before completely losing its identity.
During the Second World War, German U-boats used the currents to pass into the Mediterranean Sea without detection, by maintaining silence with engines off. From September 1941 to May 1944 Germany managed to send 62 U-boats into the Mediterranean. All these boats had to navigate the British-controlled Strait of Gibraltar where nine U-boats were sunk while attempting passage and 10 more had to break off their run due to damage. No U-boats ever made it back into the Atlantic and all were either sunk in battle or scuttled by their own crews.

Internal waves (waves at the density boundary layer) are often produced by the strait. Like traffic merging on a highway, the water flow is constricted in both directions because it must pass over the Camarinal Sill. When large tidal flows enter the Strait and the high tide relaxes, internal waves are generated at the Camarinal Sill and proceed eastwards. Even though the waves may occur down to great depths, occasionally the waves are almost imperceptible at the surface, at other times they can be seen clearly in satellite imagery. These "internal waves" continue to flow eastward and to refract around coastal features. They can sometimes be traced for as much as , and sometimes create interference patterns with refracted waves.

Except for its far eastern end, the Strait lies within the territorial waters of Spain and Morocco. The United Kingdom claims around Gibraltar on the northern side of the Strait, putting part of it inside British territorial waters. As this is less than the maximum, it means, according to the British claim, that part of the Strait lies in international waters. The ownership of Gibraltar and its territorial waters is disputed by Spain. Similarly, Morocco disputes Spanish sovereignty over Ceuta on the southern coast.

Some studies have proposed the possibility of erecting tidal power generating stations within the strait, to be powered from the predictable current at the strait.

In the 1920s and 1930s, the Atlantropa project proposed damming the strait to generate large amounts of electricity and lower the sea level of the Mediterranean by several hundreds of meters to create large new lands for settlement. This proposal would however have devastating effects on the local climate and ecology and would dramatically change the strength of the West African Monsoon.




</doc>
<doc id="28182" url="https://en.wikipedia.org/wiki?curid=28182" title="Social epistemology">
Social epistemology

Social epistemology refers to a broad set of approaches that can be taken in the study of knowledge that construes human knowledge as a collective achievement. Another way of characterizing social epistemology is as the evaluation of the social dimensions of knowledge or information. It is sometimes simplified to mean a social justification of belief.

One of the enduring difficulties with defining "social epistemology" that arises is the attempt to determine what the word "knowledge" means in this context. There is also a challenge in arriving at a definition of "social" which satisfies academics from different disciplines. Social epistemologists may exist working in many of the disciplines of the humanities and social sciences, most commonly in philosophy and sociology. In addition to marking a distinct movement in traditional and analytic epistemology, social epistemology is associated with the interdisciplinary field of science and technology studies (STS).

The consideration of social dimensions of knowledge in relation to philosophy started in 380 B.C.E with Plato's dialogue: Charmides. In it he questions the degree of certainty an unprofessional in a field can have towards a person's claim to be a specialist in that same field. As the exploration of a dependence on authoritative figures constitutes a part of the study of social epistemology, it confirms the existence of the ideology in minds long before it was given its label. 

In 1936, Karl Mannheim turned Karl Marx's theory of ideology (which interpreted the "social" aspect in epistemology to be of a political or sociological nature) into an analysis of how the human society develops and functions.

It was not until the 1970s that there was a powerful growth of interest amongst philosophers in topics such as epistemic value of testimony, the nature and function of expertise, proper distribution of cognitive labor and resources among individuals in the communities and the status of group reasoning and knowledge.

The term "social epistemology" was firstly used by the library scientists Margaret Egan and Jesse Shera in the 1950s. Steven Shapin also used it in 1979. However, it was not until the late 1980s that its current sense began to emerge. In 1987, the philosophical journal "Synthese" published a special issue on social epistemology which included two authors that have since taken the branch of epistemology in two divergent directions: Alvin Goldman and Steve Fuller. Fuller founded a journal called "Social Epistemology: A journal of knowledge, culture, and policy" in 1987 and published his first book, "Social Epistemology", in 1988. Goldman's "Knowledge in a Social World" came out in 1999. Goldman advocates for a type of epistemology which is sometimes called "veritistic epistemology" because of its large emphasis on truth. This type of epistemology is sometimes seen to side with "essentialism" as opposed to "multiculturalism". But Goldman has argued that this association between veritistic epistemology and essentialism is not necessary. He describes Social Epistemology as knowledge derived from one's interactions with another person, group or society. 

Goldman looks into one of the two strategies of the socialization of epistemology. This strategy includes the evaluation of social factors that impact knowledge formed on true belief. In contrast, Fuller takes preference for the second strategy that defines knowledge influenced by social factors as collectively accepted belief. The difference between the two can be simplified with exemplars e.g.: the first strategy means analyzing how your degree of wealth (a social factor) influences what information you determine to be valid whilst the second strategy occurs when an evaluation is done on wealth’s influence upon your knowledge acquired from the beliefs of the society in which you find yourself.

In 2012, on the occasion of the 25th anniversary of "Social Epistemology", Fuller reflected upon the history and the prospects of the field, including the need for social epistemology to re-connect with the larger issues of knowledge production first identified by Charles Sanders Peirce as "cognitive economy" and nowadays often pursued by library and information science. As for the "analytic social epistemology", to which Goldman has been a significant contributor, Fuller concludes that it has "failed to make significant progress owing, in part, to a minimal understanding of actual knowledge practices, a minimised role for philosophers in ongoing inquiry, and a focus on maintaining the status quo of epistemology as a field."

The basic view of knowledge that motivated the emergence of social epistemology as it is perceived today can be traced to the work of Thomas Kuhn and Michel Foucault, which gained acknowledgment at the end of the 1960s. Both brought historical concerns directly to bear on problems long associated with the philosophy of science. Perhaps the most notable issue here was the nature of truth, which both Kuhn and Foucault described as a relative and contingent notion. On this background, ongoing work in the sociology of scientific knowledge (SSK) and the history and philosophy of science (HPS) was able to assert its epistemological consequences, leading most notably to the establishment of the strong programme at the University of Edinburgh. In terms of the two strands of social epistemology, Fuller is more sensitive and receptive to this historical trajectory (if not always in agreement) than Goldman, whose "veritistic" social epistemology can be reasonably read as a systematic rejection of the more extreme claims associated with Kuhn and Foucault.

As a field within analytic philosophy, social epistemology foregrounds the social aspects of knowledge creation and dissemination. What precisely these social aspects are, and whether they have beneficial or detrimental effects upon the possibilities to create, acquire and spread knowledge is a subject of continuous debate.

Within the field, "the social" is approached in two complementary and not mutually exclusive ways: "the social" character of knowledge can either be approached through inquiries in "inter-individual" epistemic relations or through inquiries focusing on epistemic "communities". The inter-individual approach typically focuses on issues such as testimony, epistemic trust as a form of trust placed by one individual in another, epistemic dependence, epistemic authority, etc. The community approach typically focuses on issues such as community standards of justification, community procedures of critique, diversity, epistemic justice, and collective knowledge.

Social epistemology as a field within analytic philosophy has close ties to, and often overlaps with feminist epistemology and philosophy of science. While parts of the field engage in abstract, normative considerations of knowledge creation and dissemination, other parts of the field are "naturalized epistemology" in the sense that they draw on empirically gained insights---which could mean natural science research from, e.g., cognitive psychology, be that qualitative or quantitative social science research. (For the notion of "naturalized epistemology" see Willard Van Orman Quine.) And while parts of the field are concerned with analytic considerations of rather general character, case-based and domain-specific inquiries in, e.g., knowledge creation in collaborative scientific practice, knowledge exchange on online platforms or knowledge gained in learning institutions play an increasing role.

Important academic journals for social epistemology as a field within analytic philosophy are, e.g., "Episteme", "Hypatia", "Social Epistemology", and "Synthese". However, major works within this field are also published in journals that predominantly address philosophers of science and psychology or in interdisciplinary journals which focus on particular domains of inquiry (such as, e.g., "Ethics and Information Technology").


In both stages, both varieties of social epistemology remain largely "academic" or "theoretical" projects. Yet both emphasize the social significance of knowledge and therefore the cultural value of social epistemology itself. A range of journals publishing social epistemology welcome papers that include a policy dimension. 

More practical applications of social epistemology can be found in the areas of library science, academic publishing, guidelines for scientific authorship and collaboration, knowledge policy and debates over the role over the Internet in knowledge transmission and creation.

Social epistemology is still considered a relatively new addition to philosophy, with its problems and theories still fresh and in rapid movement.






</doc>
<doc id="28184" url="https://en.wikipedia.org/wiki?curid=28184" title="Sound card">
Sound card

A sound card (also known as an audio card) is an internal expansion card that provides input and output of audio signals to and from a computer under control of computer programs. The term "sound card" is also applied to external audio interfaces used for professional audio applications. 

Sound functionality can also be integrated onto the motherboard, using components similar to those found on plug-in cards. The integrated sound system is often still referred to as a "sound card". Sound processing hardware is also present on modern video cards with HDMI to output sound along with the video using that connector; previously they used a SPDIF connection to the motherboard or sound card.

Typical uses of sound cards or sound card functionality include providing the audio component for multimedia applications such as music composition, editing video or audio, presentation, education and entertainment (games) and video projection. Sound cards are also used for computer-based communication such as voice over IP and teleconferencing.

Modern sound cards use a digital-to-analog converter (DAC), which converts recorded or generated digital data into an analog format. The output signal is connected to an amplifier, headphones, or external device using standard interconnects, such as a TRS phone connector or an RCA connector. If the number and size of connectors is too large for the space on the backplate, the connectors will be off-board, typically using a breakout box, an auxiliary backplate, or a panel mounted at the front. More advanced cards usually include more than one sound chip to support higher data rates and multiple simultaneous functionality, for example digital production of synthesized sounds, usually for real-time generation of music and sound effects using minimal data and CPU time.

Most sound cards have a line in connector for an input signal from a cassette tape or other sound source that has higher voltage levels than a microphone. The sound card digitizes this signal. The DMAC transfers the samples to the main memory, from where a recording software may write it to the hard disk for storage, editing, or further processing. Another common external connector is the "microphone" connector, for signals from a microphone or other low-level input device. Input through a microphone jack can be used, for example, by speech recognition or voice over IP applications.

An important sound card characteristic is polyphony, which refers to its ability to process and output multiple "independent" voices or sounds "simultaneously". These distinct "channels" are seen as the number of audio outputs, which may correspond to a speaker configuration such as 2.0 (stereo), 2.1 (stereo and sub woofer), 5.1 (surround), or other configuration. Sometimes, the terms "voice" and "channel" are used interchangeably to indicate the degree of polyphony, not the output speaker configuration.

For example, many older sound chips could accommodate three voices, but only one audio channel (i.e., a single mono output) for output, requiring all voices to be mixed together. Later cards, such as the AdLib sound card, had a 9-voice polyphony combined in 1 mono output channel.

For some years, most PC sound cards have had multiple FM synthesis voices (typically 9 or 16) which were usually used for MIDI music. The full capabilities of advanced cards are often not fully used; only one (mono) or two (stereo) voice(s) and channel(s) are usually dedicated to playback of digital sound samples, and playing back more than one digital sound sample usually requires a software downmix at a fixed sampling rate. Modern low-cost integrated sound cards (i.e., those built into motherboards) such as audio codecs like those meeting the AC'97 standard and even some lower-cost expansion sound cards still work this way. These devices may provide more than two sound output channels (typically 5.1 or 7.1 surround sound), but they usually have no actual hardware polyphony for either sound effects or MIDI reproduction these tasks are performed entirely in software. This is similar to the way inexpensive softmodems perform modem tasks in software rather than in hardware.

Also, in the early days of 'wavetable' sample-based synthesis, some sound card manufacturers advertised polyphony solely on the MIDI capabilities alone. In this case, the card's output channel is irrelevant; typically, the card is only capable of two channels of digital sound. Instead, the polyphony measurement solely applies to the number of MIDI instruments the sound card is capable of producing at one given time.

Today, a sound card providing actual hardware polyphony, regardless of the number of output channels, is typically referred to as a "hardware audio accelerator", although actual voice polyphony is not the sole (or even a necessary) prerequisite, with other aspects such as hardware acceleration of 3D sound, positional audio and real-time DSP effects being more important.

Since digital sound playback has become available and single and provided better performance than synthesis, modern sound cards with hardware polyphony do not actually use DACs with as many channels as voices; instead, they perform voice mixing and effects processing in hardware, eventually performing digital filtering and conversions to and from the frequency domain for applying certain effects, inside a dedicated DSP. The final playback stage is performed by an external (in reference to the DSP chip(s)) DAC with significantly fewer channels than voices (e.g., 8 channels for 7.1 audio, which can be divided among 32, 64 or even 128 voices).

Connectors on the sound cards are color-coded as per the PC System Design Guide. They will also have symbols with arrows, holes and soundwaves that are associated with each jack position, the meaning of each is given below:

Sound cards for IBM PC compatible computers were very uncommon until 1988. For the majority IBM PC users, the internal PC speaker was the only way for early PC software to produce sound and music. The speaker hardware was typically limited to square waves. The resulting sound was generally described as "beeps and boops" which resulted in the common nickname "beeper". Several companies, most notably Access Software, developed techniques for digital sound reproduction over the PC speaker like RealSound. The resulting audio, while functional, suffered from heavily distorted output and low volume, and usually required all other processing to be stopped while sounds were played. Other home computers of the 1980s like the Commodore 64 included hardware support for digital sound playback and/or music synthesis, leaving the IBM PC at a disadvantage when it came to multimedia applications. Early sound cards for the IBM PC platform were not designed for gaming or multimedia applications, but rather on specific audio applications, such as music composition with the AdLib Personal Music System, IBM Music Feature Card, and Creative Music System, or on speech synthesis like Digispeech "DS201", Covox Speech Thing, and Street Electronics "Echo".

In 1988, a panel of computer-game CEOs stated at the Consumer Electronics Show that the PC's limited sound capability prevented it from becoming the leading home computer, that it needed a $49–79 sound card with better capability than current products, and that once such hardware was widely installed their companies would support it. Sierra On-Line, which had pioneered supporting EGA and VGA video, and 3 1/2" disks, promised that year to support the AdLib, IBM Music Feature, and Roland MT-32 sound cards in its games. A 1989 "Computer Gaming World" survey found that 18 of 25 game companies planned to support AdLib, six Roland and Covox, and seven Creative Music System/Game Blaster.

One of the first manufacturers of sound cards for the IBM PC was AdLib, which produced a card based on the Yamaha YM3812 sound chip, also known as the OPL2. The AdLib had two modes: A 9-voice mode where each voice could be fully programmed, and a less frequently used "percussion" mode with 3 regular voices producing 5 independent percussion-only voices for a total of 11. (The percussion mode was considered inflexible by most developers; it was used mostly by AdLib's own composition software.)

Creative Labs also marketed a sound card about the same time called the Creative Music System. Although the "C/MS " had twelve voices to AdLib's nine, and was a stereo card while the AdLib was mono, the basic technology behind it was based on the Philips SAA1099 chip which was essentially a square-wave generator. It sounded much like twelve simultaneous PC speakers would have except for each channel having amplitude control, and failed to sell well, even after Creative renamed it the Game Blaster a year later, and marketed it through RadioShack in the US. The Game Blaster retailed for under $100 and was compatible with many popular games, such as Silpheed.

A large change in the IBM PC compatible sound card market happened when Creative Labs introduced the Sound Blaster card. Recommended by Microsoft to developers creating software based on the Multimedia PC standard, the Sound Blaster cloned the AdLib and added a sound coprocessor for recording and play back of digital audio (likely to have been an Intel microcontroller relabeled by Creative). It was incorrectly called a "DSP" (to suggest it was a digital signal processor), a game port for adding a joystick, and capability to interface to MIDI equipment (using the game port and a special cable). With more features at nearly the same price, and compatibility as well, most buyers chose the Sound Blaster. It eventually outsold the AdLib and dominated the market.

Roland also made sound cards in the late 1980s, most of them being high quality "prosumer" cards, such as the MT-32 and LAPC-I. Roland cards often sold for hundreds of dollars, and sometimes over a thousand. Many games had music written for their cards, such as Silpheed and Police Quest II. The cards were often poor at sound effects such as laughs, but for music were by far the best sound cards available until the mid nineties. Some Roland cards, such as the SCC, and later versions of the MT-32 were made to be less expensive, but their quality was usually drastically poorer than the other Roland cards.

By 1992 one sound card vendor advertised that its product was "Sound Blaster, AdLib, Disney Sound Source and Covox Speech Thing Compatible!". Responding to readers complaining about an article on sound cards that unfavorably mentioned the Gravis Ultrasound, "Computer Gaming World" stated in January 1994 that "The de facto standard in the gaming world is Sound Blaster compatibility ... It would have been unfair to have recommended anything else". The magazine that year stated that "Wing Commander II" was "Probably the game responsible" for making it the standard card. The Sound Blaster line of cards, together with the first inexpensive CD-ROM drives and evolving video technology, ushered in a new era of multimedia computer applications that could play back CD audio, add recorded dialogue to video games, or even reproduce full motion video (albeit at much lower resolutions and quality in early days). The widespread decision to support the Sound Blaster design in multimedia and entertainment titles meant that future sound cards such as Media Vision's Pro Audio Spectrum and the Gravis Ultrasound had to be Sound Blaster compatible if they were to sell well. Until the early 2000s (by which the AC'97 audio standard became more widespread and eventually usurped the SoundBlaster as a standard due to its low cost and integration into many motherboards), Sound Blaster compatibility is a standard that many other sound cards still support to maintain compatibility with many games and applications released.

When game company Sierra On-Line opted to support add-on music hardware in addition to built-in hardware such as the PC speaker and built-in sound capabilities of the IBM PCjr and Tandy 1000, what could be done with sound and music on the IBM PC changed dramatically. Two of the companies Sierra partnered with were Roland and AdLib, opting to produce in-game music for King's Quest 4 that supported the MT-32 and AdLib Music Synthesizer. The MT-32 had superior output quality, due in part to its method of sound synthesis as well as built-in reverb. Since it was the most sophisticated synthesizer they supported, Sierra chose to use most of the MT-32's custom features and unconventional instrument patches, producing background sound effects (e.g., chirping birds, clopping horse hooves, etc.) before the Sound Blaster brought playing real audio clips to the PC entertainment world. Many game companies also supported the MT-32, but supported the Adlib card as an alternative because of the latter's higher market base. The adoption of the MT-32 led the way for the creation of the MPU-401/Roland Sound Canvas and General MIDI standards as the most common means of playing in-game music until the mid-1990s.

Early ISA bus sound cards were half-duplex, meaning they couldn't record and play digitized sound simultaneously, mostly due to inferior card hardware (e.g., DSPs). Later, ISA cards like the SoundBlaster AWE series and Plug-and-play Soundblaster clones eventually became full-duplex and supported simultaneous recording and playback, but at the expense of using up two IRQ and DMA channels instead of one, making them no different from having two half-duplex sound cards in terms of configuration. Towards the end of the ISA bus' life, ISA sound cards started taking advantage of IRQ sharing, thus reducing the IRQs needed to one, but still needed two DMA channels. Many Conventional PCI bus cards do not have these limitations and are mostly full-duplex. It should also be noted that many modern PCI bus cards also do not require free DMA channels to operate.

Also, throughout the years, sound cards have evolved in terms of digital audio sampling rate (starting from 8-bit , to 32-bit, that the latest solutions support). Along the way, some cards started offering 'wavetable' sample-based synthesis, which provides superior MIDI synthesis quality relative to the earlier OPL-based solutions, which uses FM-synthesis. Also, some higher end cards started having their own RAM and processor for user-definable sound samples and MIDI instruments as well as to offload audio processing from the CPU.

For years, sound cards had only one or two channels of digital sound (most notably the Sound Blaster series and their compatibles) with the exception of the E-MU card family, the Gravis GF-1 and AMD Interwave, which had hardware support for up to 32 independent channels of digital audio. Early games and MOD-players needing more channels than a card could support had to resort to mixing multiple channels in software. Even today, the tendency is still to mix multiple sound streams in software, except in products specifically intended for gamers or professional musicians, with a sensible difference in price from "software based" products. Also, in the early era of 'wavetable' sample-based synthesis, sound card companies would also sometimes boast about the card's polyphony capabilities in terms of MIDI synthesis. In this case polyphony solely refers to the count of MIDI notes the card is capable of synthesizing simultaneously at one given time and not the count of digital audio streams the card is capable of handling.

In regards to physical sound output, the number of physical sound channels has also increased. The first sound card solutions were mono. Stereo sound was introduced in the early 1980s, and quadraphonic sound came in 1989. This was shortly followed by 5.1 channel audio. The latest sound cards support up to audio channels in the 7.1 speaker setup.

Most new sound cards no longer have the audio loopback device commonly called "Stereo Mix"/"Wave out mix"/"Mono Mix"/"What U Hear" that was once very prevalent and that allows users to digitally record speaker output to the microphone input.

Lenovo and other manufacturers fail to implement the chipset feature in hardware, while other manufacturers disable the driver from supporting it. In some cases loopback can be reinstated with driver updates (as in the case of some Dell computers); alternatively software (Total Recorder or Virtual Audio Cable) can be purchased to enable the functionality. According to Microsoft, the functionality was hidden by default in Windows Vista (to reduce user confusion), but is still available, as long as the underlying sound card drivers and hardware support it. Ultimately, the user can connect the line out directly to the line in (analog hole).

Professional sound cards are special sound cards optimized for low-latency multichannel sound recording and playback, including studio-grade fidelity. Their drivers usually follow the Audio Stream Input Output protocol for use with professional sound engineering and music software, although ASIO drivers are also available for a range of consumer-grade sound cards.

Professional sound cards are usually described as "audio interfaces", and sometimes have the form of external rack-mountable units using USB, FireWire, or an optical interface, to offer sufficient data rates. The emphasis in these products is, in general, on multiple input and output connectors, direct hardware support for multiple input and output sound channels, as well as higher sampling rates and fidelity as compared to the usual consumer sound card. In that respect, their role and intended purpose is more similar to a specialized multi-channel data recorder and real-time audio mixer and processor, roles which are possible only to a limited degree with typical consumer sound cards.

On the other hand, certain features of consumer sound cards such as support for environmental audio extensions (EAX), optimization for hardware acceleration in video games, or real-time ambience effects are secondary, nonexistent or even undesirable in professional sound cards, and as such audio interfaces are not recommended for the typical home user.

The typical "consumer-grade" sound card is intended for generic home, office, and entertainment purposes with an emphasis on playback and casual use, rather than catering to the needs of audio professionals. In response to this, Steinberg (the creators of audio recording and sequencing software, Cubase and Nuendo) developed a protocol that specified the handling of multiple audio inputs and outputs.

In general, consumer grade sound cards impose several restrictions and inconveniences that would be unacceptable to an audio professional. One of a modern sound card's purposes is to provide an AD/DA converter (analog to digital/digital to analog). However, in professional applications, there is usually a need for enhanced recording (analog to digital) conversion capabilities.

One of the limitations of consumer sound cards is their comparatively large sampling latency; this is the time it takes for the AD Converter to complete conversion of a sound sample and transfer it to the computer's main memory.

Consumer sound cards are also limited in the "effective" sampling rates and bit depths they can actually manage (compare analog versus digital sound) and have lower numbers of less flexible input channels: professional studio recording use typically requires more than the two channels that consumer sound cards provide, and more accessible connectors, unlike the variable mixture of internal—and sometimes virtual—and external connectors found in consumer-grade sound cards.

In 1984, the first IBM PCjr had a rudimentary 3-voice sound synthesis chip (the SN76489) which was capable of generating three square-wave tones with variable amplitude, and a pseudo-white noise channel that could generate primitive percussion sounds. The Tandy 1000, initially a clone of the PCjr, duplicated this functionality, with the Tandy TL/SL/RL models adding digital sound recording and playback capabilities. Many games during the 1980s that supported the PCjr's video standard (described as "Tandy-compatible", "Tandy graphics", or "TGA") also supported PCjr/Tandy 1000 audio.

In the late 1990s many computer manufacturers began to replace plug-in sound cards with a "codec" chip (actually a combined audio AD/DA-converter) integrated into the motherboard. Many of these used Intel's AC'97 specification. Others used inexpensive ACR slot accessory cards.

From around 2001 many motherboards incorporated integrated "real" (non-codec) sound cards, usually in the form of a custom chipset providing something akin to full Sound Blaster compatibility, providing relatively high-quality sound.

However, these features were dropped when AC'97 was superseded by Intel's HD Audio standard, which was released in 2004, again specified the use of a codec chip, and slowly gained acceptance. As of 2011, most motherboards have returned to using a codec chip, albeit a HD Audio compatible one, and the requirement for Sound Blaster compatibility relegated to history.

Various non-IBM PC compatible computers, such as early home computers like the Commodore 64 (1982) and Amiga (1985), NEC's PC-88 and PC-98, Fujitsu's FM-7 and FM Towns, the MSX, Apple's Macintosh, and workstations from manufacturers like Sun, have had their own motherboard integrated sound devices. In some cases, most notably in those of the Amiga, C64, PC-88, PC-98, MSX, FM-7, and FM towns, they provide very advanced capabilities (as of the time of manufacture), in others they are only minimal capabilities. Some of these platforms have also had sound cards designed for their bus architectures that cannot be used in a standard PC.

Several Japanese computer platforms, including the PC-88, PC-98, MSX, and FM-7, featured built-in FM synthesis sound from Yamaha by the mid-1980s. By 1989, the FM Towns computer platform featured built-in PCM sample-based sound and supported the CD-ROM format.

The custom sound chip on Amiga, named Paula, had four digital sound channels (2 for the left speaker and 2 for the right) with 8-bit resolution (although with patches, 14/15-bit was accomplishable at the cost of high CPU usage) for each channel and a 6-bit volume control per channel. Sound playback on Amiga was done by reading directly from the chip-RAM without using the main CPU.

Most arcade games have integrated sound chips, the most popular being the Yamaha OPL chip for BGM coupled with a variety of DACs for sampled audio and sound effects.

The earliest known sound card used by computers was the Gooch Synthetic Woodwind, a music device for PLATO terminals, and is widely hailed as the precursor to sound cards and MIDI. It was invented in 1972.

Certain early arcade machines made use of sound cards to achieve playback of complex audio waveforms and digital music, despite being already equipped with onboard audio. An example of a sound card used in arcade machines is the Digital Compression System card, used in games from Midway. For example, Mortal Kombat II on the Midway T Unit hardware. The T-Unit hardware already has an onboard YM2151 OPL chip coupled with an OKI 6295 DAC, but said game uses an added on DCS card instead. The card is also used in the arcade version of Midway and Aerosmith's Revolution X for complex looping BGM and speech playback (Revolution X used fully sampled songs from the band's album that transparently looped- an impressive feature at the time the game was released).

MSX computers, while equipped with built-in sound capabilities, also relied on sound cards to produce better quality audio. The card, known as Moonsound, uses a Yamaha OPL4 sound chip. Prior to the Moonsound, there were also sound cards called "MSX Music" and "MSX Audio", which uses OPL2 and OPL3 chipsets, for the system.

The Apple II series of computers, which did not have sound capabilities beyond a beep until the IIGS, could use plug-in sound cards from a variety of manufacturers. The first, in 1978, was ALF's Apple Music Synthesizer, with 3 voices; two or three cards could be used to create 6 or 9 voices in stereo. Later ALF created the Apple Music II, a 9-voice model. The most widely supported card, however, was the Mockingboard. Sweet Micro Systems sold the Mockingboard in various models. Early Mockingboard models ranged from 3 voices in mono, while some later designs had 6 voices in stereo. Some software supported use of two Mockingboard cards, which allowed 12-voice music and sound. A 12-voice, single card clone of the Mockingboard called the Phasor was made by Applied Engineering. In late 2005 a company called ReactiveMicro.com produced a 6-voice clone called the Mockingboard v1 and also had plans to clone the Phasor and produce a hybrid card user-selectable between Mockingboard and Phasor modes plus support both the SC-01 or SC-02 speech synthesizers.

The Sinclair ZX Spectrum that initially only had a beeper had some sound cards made for it. One example is the TurboSound. Other examples are the Fuller Box, Melodik for the Didaktik Gamma, AY-Magic et.c. The Zon X-81 for the ZX81 was also possible to use on the ZX Spectrum using an adapter.

Devices such as the Covox Speech Thing could be attached to the parallel port of an IBM PC and feed 6- or 8-bit PCM sample data to produce audio. Also, many types of professional sound cards (audio interfaces) have the form of an external FireWire or USB unit, usually for convenience and improved fidelity.

Sound cards using the PCMCIA Cardbus interface were available before laptop and notebook computers routinely had onboard sound. Cardbus audio may still be used if onboard sound quality is poor. When Cardbus interfaces were superseded by Expresscard on computers since about 2005, manufacturers followed. Most of these units are designed for mobile DJs, providing separate outputs to allow both playback and monitoring from one system, however some also target mobile gamers, providing high-end sound to gaming laptops who are usually well-equipped when it comes to graphics and processing power, but tend to have audio codecs that are no better than the ones found on regular laptops.

USB sound "cards" are external devices that plug into the computer via USB. They are often used in studios and on stage by electronic musicians including live PA performers and DJs. DJs who use DJ software typically use sound cards integrated into DJ controllers or specialized DJ sound cards. DJ sound cards sometimes have inputs with phono preamplifiers to allow turntables to be connected to the computer to control the software's playback of music files with timecode vinyl.

The USB specification defines a standard interface, the USB audio device class, allowing a single driver to work with the various USB sound devices and interfaces on the market. Mac OS X, Windows, and Linux support this standard. However, many USB sound cards do not conform to the standard and require proprietary drivers from the manufacturer.

Even cards meeting the older, slow, USB 1.1 specification are capable of high quality sound with a limited number of channels, or limited sampling frequency or bit depth, but USB 2.0 or later is more capable.

A USB audio interface may also describe a device allowing a computer which has a sound-card, yet lacks a standard audio socket, to be connected to an external device which requires such a socket, via its USB socket.

The main function of a sound card is to play audio, usually music, with varying formats (monophonic, stereophonic, various multiple speaker setups) and degrees of control. The source may be a CD or DVD, a file, streamed audio, or any external source connected to a sound card input.

Audio may be recorded. Sometimes sound card hardware and drivers do not support recording a source that is being played.

A card can also be used, in conjunction with software, to generate arbitrary waveforms, acting as an audio-frequency function generator. Free and commercial software is available for this purpose; there are also online services that generate audio files for any desired waveforms, playable through a sound card.

A card can be used, again in conjunction with free or commercial software, to analyse input waveforms. For example, a very-low-distortion sinewave oscillator can be used as input to equipment under test; the output is sent to a sound card's line input and run through Fourier transform software to find the amplitude of each harmonic of the added distortion. Alternatively, a less pure signal source may be used, with circuitry to subtract the input from the output, attenuated and phase-corrected; the result is distortion and noise only, which can be analysed.

There are programs which allow a sound card to be used as an audio-frequency oscilloscope.

For all measurement purposes a sound card must be chosen with good audio properties. It must itself contribute as little distortion and noise as possible, and attention must be paid to bandwidth and sampling. A typical integrated sound card, the Realtek ALC887, according to its data sheet has distortion of about 80 dB below the fundamental; cards are available with distortion better than -100 dB.

Sound cards with a sampling rate of 192 kHz can be used to synchronize the clock of the computer with a time signal transmitter working on frequencies below 96 kHz like DCF 77 with a special software and a coil at the entrance of the sound card, working as antenna , .

To use a sound card, the operating system (OS) typically requires a specific device driver, a low-level program that handles the data connections between the physical hardware and the operating system. Some operating systems include the drivers for many cards; for cards not so supported, drivers are supplied with the card, or available for download.





</doc>
<doc id="28186" url="https://en.wikipedia.org/wiki?curid=28186" title="Symmetry group">
Symmetry group

In group theory, the symmetry group of an object (image, signal, etc.) is the group of all transformations under which the object is invariant with composition as the group operation. For a space with a metric, it is a subgroup of the isometry group of the space concerned. If not stated otherwise, this article considers symmetry groups in Euclidean geometry, but the concept may also be studied in more general contexts as expanded below.

The "objects" may be geometric figures, images, and patterns, such as a wallpaper pattern. The definition can be made more precise by specifying what is meant by image or pattern, e.g., a function of position with values in a set of colors. For symmetry of physical objects, one may also want to take their physical composition into account. The group of isometries of space induces a group action on objects in it.

The symmetry group is sometimes also called full symmetry group in order to emphasize that it includes the orientation-reversing isometries (like reflections, glide reflections and improper rotations) under which the figure is invariant. The subgroup of orientation-preserving isometries (i.e. translations, rotations, and compositions of these) that leave the figure invariant is called its proper symmetry group. The proper symmetry group of an object is equal to its full symmetry group if and only if the object is chiral (and thus there are no orientation-reversing isometries under which it is invariant).

Any symmetry group whose elements have a common fixed point, which is true for all finite symmetry groups and also for the symmetry groups of bounded figures, can be represented as a subgroup of the orthogonal group O("n") by choosing the origin to be a fixed point. The proper symmetry group is then a subgroup of the special orthogonal group SO("n"), and is therefore also called rotation group of the figure.

A discrete symmetry group is a symmetry group such that for every point of the space the set of images of the point under the isometries in the symmetry group is a discrete set. The number of elements in the group may be either finite or infinite.

Discrete symmetry groups come in three types: (1) finite point groups, which include only rotations, reflections, inversion and rotoinversion – they are just the finite subgroups of O("n"), (2) infinite lattice groups, which include only translations, and (3) infinite space groups which combines elements of both previous types, and may also include extra transformations like screw displacements and glide reflections. There are also "continuous" symmetry groups, which contain rotations of arbitrarily small angles or translations of arbitrarily small distances. The group of all symmetries of a sphere O(3) is an example of this, and in general such continuous symmetry groups are studied as Lie groups. With a categorization of subgroups of the Euclidean group corresponds a categorization of symmetry groups.

Two geometric figures are considered to be of the same symmetry type if their symmetry groups are conjugate subgroups of the Euclidean group E("n") (the isometry group of R), where two subgroups "H", "H" of a group "G" are "conjugate", if there exists such that . For example:

When considering isometry groups, one may restrict oneself to those where for all points the set of images under the isometries is topologically closed. This includes all discrete isometry groups and also those involved in continuous symmetries, but excludes for example in 1D the group of translations by a rational number. A "figure" with this symmetry group is non-drawable and up to arbitrarily fine detail homogeneous, without being really homogeneous.

The isometry groups in one dimension where for all points the set of images under the isometries is topologically closed are:

See also symmetry groups in one dimension.

Up to conjugacy the discrete point groups in two-dimensional space are the following classes:


C is the trivial group containing only the identity operation, which occurs when the figure has no symmetry at all, for example the letter F. C is the symmetry group of the letter Z, C that of a triskelion, C of a swastika, and C, C, etc. are the symmetry groups of similar swastika-like figures with five, six, etc. arms instead of four.

D is the 2-element group containing the identity operation and a single reflection, which occurs when the figure has only a single axis of bilateral symmetry, for example the letter A.

D, which is isomorphic to the Klein four-group, is the symmetry group of a non-equilateral rectangle. This figure has four symmetry operations: the identity operation, one twofold axis of rotation, and two nonequivalent mirror planes.

D, D etc. are the symmetry groups of the regular polygons.

The actual symmetry groups in each of these cases have two degrees of freedom for the center of rotation, and in the case of the dihedral groups, one more for the positions of the mirrors.

The remaining isometry groups in two dimensions with a fixed point, where for all points the set of images under the isometries is topologically closed are:

For non-bounded figures, the additional isometry groups can include translations; the closed ones are:

Up to conjugacy the set of three-dimensional point groups consists of 7 infinite series, and 7 separate ones. In crystallography they are restricted to be compatible with the discrete translation symmetries of a crystal lattice. This crystallographic restriction of the infinite families of general point groups results in 32 crystallographic point groups (27 from the 7 infinite series, and 5 of the 7 others).

The continuous symmetry groups with a fixed point include those of:

For objects and scalar fields the cylindrical symmetry implies vertical planes of reflection. However, for vector fields it does not: in cylindrical coordinates with respect to some axis, 
formula_1 has cylindrical symmetry with respect to the axis if and only if formula_2 and formula_3 have this symmetry, i.e., they do not depend on formula_4. Additionally there is reflectional symmetry if and only if formula_5.

For spherical symmetry there is no such distinction, it implies planes of reflection.

The continuous symmetry groups without a fixed point include those with a screw axis, such as an infinite helix. See also subgroups of the Euclidean group.

In wider contexts, a symmetry group may be any kind of transformation group, or automorphism group. Once we know what kind of mathematical structure we are concerned with, we should be able to pinpoint what mappings preserve the structure. Conversely, specifying the symmetry can define the structure, or at least clarify what we mean by an invariant, geometric language in which to discuss it; this is one way of looking at the Erlangen programme.

For example, automorphism groups of certain models of finite geometries are not "symmetry groups" in the usual sense, although they preserve symmetry. They do this by preserving "families" of point-sets rather than point-sets (or "objects") themselves.

Like above, the group of automorphisms of space induces a group action on objects in it.

For a given geometric figure in a given geometric space, consider the following equivalence relation: two automorphisms of space are equivalent if and only if the two images of the figure are the same (here "the same" does not mean something like e.g. "the same up to translation and rotation", but it means "exactly the same"). Then the equivalence class of the identity is the symmetry group of the figure, and every equivalence class corresponds to one isomorphic version of the figure.

There is a bijection between every pair of equivalence classes: the inverse of a representative of the first equivalence class, composed with a representative of the second.

In the case of a finite automorphism group of the whole space, its order is the order of the symmetry group of the figure multiplied by the number of isomorphic versions of the figure.

Examples:

Compare Lagrange's theorem (group theory) and its proof.




</doc>
<doc id="28187" url="https://en.wikipedia.org/wiki?curid=28187" title="Singular they">
Singular they

Singular "they" is the use in English of the pronoun "they" or its inflected or derivative forms, "them", "their", "theirs", and "themselves" (or "themself"), as an epicene (gender-neutral) singular pronoun. It typically occurs with an antecedent of indeterminate gender, as in sentences such as:


The singular "they" had emerged by the 14th century. Though it is commonly employed in everyday English, it has been the target of criticism since the late 19th century. Its use in formal English has increased with the trend toward gender-inclusive language.

The "singular "they"" permits a singular antecedent, used with the same (plural) verb forms as plural they, and has the same inflected forms as plural "they" (i.e. "them", "their", and "theirs"), except that in the reflexive form, "themself" is sometimes used instead of "themselves". 

"Themself" was common usage from the 14th to 16th centuries. Its use has been increasing since the 1970s or 1980s, though it is sometimes still classified as "a minority form". In 2002, Payne and Huddleston, in "The Cambridge Grammar of the English Language", called its use in standard dialect "rare and acceptable only to a minority of speakers" but "likely to increase with the growing acceptance of "they" as a singular pronoun". It is useful when referring to a single person of indeterminate gender, where the plural form "themselves" might seem incongruous, as in:

The Canadian government recommends "themselves" as the reflexive form of singular "they" for use in Canadian federal legislative texts and advises against using "themself", but "themself" is also found:

"They" with a singular antecedent goes back to the Middle English of the 14th century, and has remained in common use for centuries in spite of its proscription by traditional grammarians beginning in the late 18th century.

Informal spoken English exhibits nearly universal use of the singular "they". An examination by Jürgen Gerner of the British National Corpus published in 1998 found that British speakers regardless of social status, age, sex, or region used the singular "they" overwhelmingly more often than the gender-neutral "he" or other options.

Singular "they" is found in the writings of many respected authors. Here are some examples, arranged chronologically:





Alongside "they", it was acceptable to use the pronoun "he" to refer to an indefinite person of either gender, as in the following:

Such usage is still occasionally found but has lost acceptability in most contexts.

The inherently masculine pronoun "he" has often been used in English in a genderless sense as an alternative way of referring to any person. The earliest known explicit recommendation by a grammarian to use the generic "he" rather than "they" in formal English is Ann Fisher's mid-18th century "A New Grammar" assertion that "The "Masculine Person" answers to the "general Name", which comprehends both "Male" and "Female"; as, "any Person who knows what he says."" 

Nineteenth-century grammarians insisted on "he" as a gender-neutral pronoun on the grounds of number agreement, while rejecting "he or she" as clumsy, and this was widely adopted: e.g. in 1850, the British Parliament passed an act which provided that, when used in acts of Parliament "words importing the masculine gender shall be deemed and taken to include females". Baskervill and Sewell mention the common use of the singular "they" in their "An English Grammar for the Use of High School, Academy and College Class" of 1895, but prefer the generic "he" on the basis of number agreement:

Baskervill gives a number of examples of recognized authors using the singular "they", including:

It has been argued that the real motivation for promoting the "generic" "he" was an androcentric world view, with the default sex of humans being male – and the default gender therefore being masculine. There is some evidence for this: Wilson wrote in 1560:

and Poole wrote in 1646

In spite of continuous attempts on the part of educationalists to proscribe singular "they" in favour of "he", this advice was largely ignored; even writers of the period continued to use "they" (though the proscription may have been observed more by American writers). Use of the purportedly gender-neutral "he" remained acceptable until at least the 1960s, though some uses of "he" were later criticized as being awkward or silly, for instance when referring to:

"He" is still sometimes found in contemporary writing when referring to a generic or indeterminate antecedent.
In some cases it is clear from the situation that the persons potentially referred to are likely to be male, as in
In some cases the antecedent may refer to persons who are only "probably" male or to occupations traditionally thought of as male:
In other situations, the antecedent may refer to:
In 2010, Choy and Clark still recommend the use of generic "he" "in formal speech or writing":
In 2015, "Fowler's Dictionary of Modern English Usage" calls this "the now outmoded use of "he" to mean 'anyone, stating
In 2016, "Garner's Modern English" calls the generic use of masculine pronouns "the traditional view, now widely assailed as sexist".

The earliest known attempt to create gender-neutral pronouns dates back to 1792, when Scottish economist James Anderson advocated for an indeterminate pronoun "ou".

In 1808, poet Samuel Taylor Coleridge wrote:

"whether we may not, nay ought not, to use a neutral pronoun, relative or representative, to the word 'Person', where it hath been used in the sense of ', ', or noun of the common gender, in order to avoid particularising man or woman, or in order to express either sex indifferently? If this be incorrect in syntax, the whole use of the word Person is lost in a number of instances, or only retained by some stiff and strange position of the words, as – 'not letting the person be aware wherein offense has been given' – instead of – 'wherein he or she has offended'. In my [judgment] both the specific intention and general etymon of 'Person' in such sentences fully authorise the use of it and which instead of he, she, him, her, who, whom."— Samuel Taylor Coleridge, as written in "Anima Poetæ: From the Unpublished Note-books of Samuel Taylor Coleridge", edited by Ernest Coleridge (1895), p. 190.

In the second half of the 20th century, people expressed more widespread concern at the use of sexist and male-oriented language.
This included criticism of the use of "man" as a generic term to include men and women and of the use of "he" to refer to any human, regardless of sex (social gender).

It was argued that "he" could not sensibly be used as a generic pronoun understood to include men and women. William Safire in his "On Language" column in "The New York Times" approved of the use of generic "he", mentioning the mnemonic phrase "the male embraces the female".
C. Badendyck from Brooklyn wrote to the "New York Times" in a reply:

By 1980, the movement had gained wide support, and many organizations, including most publishers, had issued guidelines on the use of gender-neutral language.

The use of masculine generic nouns and pronouns in written and spoken language has decreased since the 1970s.
In a corpus of spontaneous speech collected in Australia in the 1990s, singular "they" had become the most frequently used generic pronoun. Similarly, a study from 2002 looking at a corpus of American and British newspapers showed a preference for "they" to be used (rather than generic "he" or "he or she") as a singular epicene pronoun.

The increased use of singular "they" may owe in part to an increasing desire for gender-neutral language. A solution in formal writing has often been to write ""he or she"", or something similar, but this is often considered awkward or overly politically correct, particularly when used excessively. In 2016, the journal "American Speech" published a study by Darren K. LaScotte investigating the pronouns used by native English speakers in informal written responses to questions concerning a subject of unspecified gender, finding that 68% of study participants chose singular "they" to refer to such an antecedent. Some participants noted that they found constructions such as "he or she" inadequate as they do not include people who do not identify as either male or female.

In contemporary usage, singular "they" is used to refer to an indeterminate antecedent, for instance when the notional gender or number of the antecedent is indeterminate or the gender of the real-word entity referred to is unknown or unrevealed.
Examples include different types of usage.

The singular antecedent can be a pronoun such as "someone", "anybody", or "everybody", or an interrogative pronoun such as "who":

Although the pronouns "everybody", "everyone", "nobody", and "no one" are singular in form and are used with a singular verb, these pronouns have an "implied plurality" that is somewhat similar to the implied plurality of collective or group nouns such as "crowd" or "team", and in some sentences where the antecedent is one of these "implied plural" pronouns, the word "they" cannot be replaced by generic "he", suggesting a "notional plural" rather than a "bound variable" interpretation.
In contrast to sentences that involve multiple pairwise relationships and singular "they" such as
There are examples where the antecedent pronoun (such as "everyone") may refer to a collective, with no necessary implication of pairwise relationships. These are examples of plural "they":
Which are apparent because they do not work with a generic "he" or "he or she": 
In addition, for these "notional plural" cases, it would not be appropriate to use "themself" instead of "themselves" as in:

The singular antecedent can also be a noun such as "person", "patient", or "student":

Known individuals may be referred to as "they" if the individual's gender is unknown to the speaker, or if the individual is non-binary or genderqueer, regards male or female pronouns as inappropriate, and prefers "they" instead. Several social media applications permit account holders to choose to identify their gender using one of a variety of non-binary or genderqueer options, such as "gender fluid", "agender", or "bigender", and to designate a pronoun, including "they"/"them", which they wish to be used when referring to them. Though "singular "they"" has long been used with antecedents such as "everybody" or generic persons of unknown gender, this use, which may be chosen by an individual, is recent.

The singular "they" in the meaning "gender-neutral singular pronoun for a known person, as a non-binary identifier" was chosen by the American Dialect Society as the Word of the Year 2015. In 2016, the American Dialect Society wrote:

The vote followed the previous year's approval of this use by "The Washington Post" style guide, when Bill Walsh, the "Post"s copy editor said that the singular "they" is "the only sensible solution to English's lack of a gender-neutral third-person singular personal pronoun".

Though both generic "he" and generic "they" have long histories of use, and both are still used, both are also systematically avoided by particular groups.
Style guides that avoid expressing a preference for either approach sometimes recommend recasting a problem sentence, for instance replacing generic expressions with plurals to avoid the criticisms of either party.

The use of singular "they" may be more accepted in British English than in American English, or vice versa.

"The Handbook of Nonsexist Writing" by Casey Miller and Kate Swift was first published in the United States. Because of differences in culture and vocabulary, separate British editions have since been published. These authors accept or recommend singular uses of "they" not just in cases where there is an element of semantic plurality expressed by a word such as "everyone" but also where an indeterminate "person" is referred to, citing examples of such usage even in formal speech. For instance, they quote Ronald Reagan:

In addition to use of singular "they", they – and others – also suggest a number of ways to avoid "the pronoun problem" in gender-neutral writing.
One strategy is to rewrite the sentence to use a plural "they". For instance, in a newspaper story
could have been changed to
Another strategy is to eliminate the pronoun; so
becomes
Other methods of avoiding gender preference include recasting a sentence to use "one", or (for babies) "it".

"Garner's Modern American Usage" (2003) recommends cautious use of singular "they", and avoidance where possible because its use is stigmatized.
Garner suggests that use of singular "they" is more acceptable in British English:
and apparently regrets the resistance by the American language community:
He regards the trend toward using singular "they" with antecedents like "everybody", "anyone" and "somebody" as inevitable:
In the 14th edition (1993) of "The Chicago Manual of Style", the University of Chicago Press explicitly recommended using singular "they" and "their", noting a "revival" of this usage and citing "its venerable use by such writers as Addison, Austen, Chesterfield, Fielding, Ruskin, Scott, and Shakespeare."
From the 15th edition (2003), this was changed. In Chapter 5 of the 16th edition (2010), now written by Bryan A. Garner, the recommendations are:
and
According to "The American Heritage Book of English Usage", many Americans avoid use of "they" to refer to a singular antecedent out of respect for a "traditional" grammatical rule, despite use of singular "they" by modern writers of note and mainstream publications:
The "Publication Manual of the American Psychological Association" explicitly reject the use of singular "they" and gives the following example as "incorrect" usage:
while also specifically taking the position that generic "he" is unacceptable. The APA recommends using "he or she", recasting the sentence with a plural subject to allow correct use of "they", or simply rewriting the sentence to avoid issues with gender or number.
Strunk & White, the authors of "The Elements of Style", find use of "they" with a singular antecedent unacceptable:
Their assessment, in 1979, was
Joseph M. Williams, who wrote a number of books on writing with "", discusses the advantages and disadvantages of various solutions when faced with the problem of referring to an antecedent such as "someone", "everyone", "no one" or a noun that does not indicate gender and suggests that this will continue to be a problem for some time. He "suspect[s] that eventually we will accept the plural "they" as a correct singular" but states that currently "formal usage requires a singular pronoun".
According to "The Little, Brown Handbook", most experts—and some teachers and employers—find use of singular "they" unacceptable:

It recommends using "he or she" or avoiding the problem by rewriting the sentence to use a plural or omit the pronoun.
The "Purdue Online Writing Lab" (OWL) maintains that singular "they" is incorrect:
"The Washington Post" style manual, , recommends trying to "write around the problem, perhaps by changing singulars to plurals, before using the singular they as a last resort" and specifically permits use of "they" for a "gender-nonconforming person".
The Associated Press Stylebook, , recommends: "They/them/their is acceptable in limited cases as a singular and-or gender-neutral pronoun, when alternative wording is overly awkward or clumsy. However, rewording usually is possible and always is preferable."

In the first edition of "A Dictionary of Modern English Usage" (published in 1926) use of the generic "he" is recommended. It is stated that singular "they" is disapproved of by grammarians. Numerous examples of its use by eminent writers in the past are given, but it is stated that "few good modern writers would flout [grammarians] so conspicuously as Fielding and Thackeray", whose sentences are described as having an "old-fashioned sound".
The second edition of Fowler's, "Fowler's Modern English Usage" (edited by Sir Ernest Gowers and published in 1965) continues to recommend use of the generic "he"; use of the singular "they" is called "the popular solution", which "sets the literary man's teeth on edge". It is stated that singular "they" is disapproved of by grammarians but common in colloquial speech. Numerous examples of its use by eminent writers are given, but it is stated that "few good modern writers would flout [grammarians] so conspicuously as Fielding and Thackeray".
According to the third edition of Fowler's ("The New Fowler's Modern English Usage", edited by Burchfield and published in 1996) singular "they" has not only been widely used by good writers for centuries, but is now generally accepted, except by some conservative grammarians, including the Fowler of 1926, who ignored the evidence:

"The Complete Plain Words" was originally written in 1948 by Sir Ernest Gowers, a civil servant, in an attempt by the British civil service to improve "official English". A second edition, edited by Sir Bruce Fraser, was published in 1973. It refers to "they" or "them" as the "equivalent of a singular pronoun of common sex" as "common in speech and not unknown in serious writing " but "stigmatized by grammarians as usage grammatically indefensible. The book's advice for "official writers" (civil servants) is to avoid its use and not to be tempted by its "greater convenience", though "necessity may eventually force it into the category of accepted idiom".
A new edition of "Plain Words", revised and updated by Sir Ernest Gowers' great granddaughter, Rebecca Gowers, was published in 2014.
It notes that singular "they" and "them" have become much more widespread since Gowers' original comments, but still finds it "safer" to treat a sentence like 'The reader may toss their book aside' as incorrect "in formal English", while rejecting even more strongly sentences like

"The Times Style and Usage Guide" (first published in 2003 by "The Times" of London) recommends avoiding sentences like
by using a plural construction:
"The Cambridge Guide to English Usage" (2004) finds singular "they" "unremarkable":

It expresses several preferences.
"The Economist Style Guide" refers to the use of "they" in sentences like
as "scrambled syntax that people adopt because they cannot bring themselves to use a singular pronoun".
"New Hart's Rules" is aimed at those engaged in copy editing, and the emphasis is on the formal elements of presentation including punctuation and typeface, rather than on linguistic style but—like "The Chicago Manual of Style"—makes occasional forays into matters of usage.
It advises against use of the purportedly gender-neutral "he", and suggests cautious use of "they" where "he or she" presents problems.

The 2011 edition of the "New International Version Bible" uses singular "they" instead of the traditional "he" when translating pronouns that apply to both genders in the original Greek or Hebrew. This decision was based on research by a commission that studied modern English usage and determined that singular "they" ("them"/"their") was by far the most common way that English-language speakers and writers today refer back to singular antecedents such as "whoever", "anyone", "somebody", "a person", "no one", and the like."

The Australian "Federation Press Style Guide for use in preparation of book manuscripts" recommends "gender-neutral language should be used", stating that use of "they" and "their" as singular pronouns is acceptable.

According to "A Comprehensive Grammar of the English Language" (1985):
"The Cambridge Grammar of the English Language" discusses the prescriptivist argument that "they" is a plural pronoun and that the use of "they" with a singular "antecedent" therefore violates the rule of agreement between antecedent and pronoun, but takes the view that "they", though "primarily" plural, can also be singular in a secondary "extended" sense, comparable to the purportedly extended sense of "he" to include female gender.

Use of singular "they" is stated to be "particularly common", even "stylistically neutral" with antecedents such as "everyone", "someone", and "no one", but more restricted when referring to common nouns as antecedents, as in

Use of the pronoun "themself" is described as being "rare" and "acceptable only to a minority of speakers", while use of the morphologically plural "themselves" is considered problematic when referring to "someone" rather that "everyone" (since only the latter implies a plural set).

There are also issues of grammatical acceptability when reflexive pronouns refer to singular noun phrases joined by "or", the following all being problematic:
On the motivation for using singular "they", "A Student's Introduction to English Grammar" states

The alternative "he or she" can be "far too cumbersome", as in
or even "flatly ungrammatical", as in

"Among younger speakers", use of singular "they" even with definite noun-phrase antecedents finds increasing acceptance, "sidestepping any presumption about the sex of the person referred to", as in

One explanation given for some uses of "they" referring to a singular antecedent is "notional agreement", when the antecedent is seen as semantically plural: 
In other words, in the Shakespeare quotation "a mother" is syntactically singular but stands for all mothers, and in the Shaw quotation "no man" is syntactically singular (demonstrated by taking the singular form "goes") but is semantically plural ("all" go [to kill] not to be killed), hence idiomatically requiring "they". Such use, which goes back a long way, includes examples where the sex is known, as in the above examples.

Distributive constructions apply a "single" idea to "multiple" members of a group.
They are typically marked in English by words like "each", "every" and "any". The simplest examples are applied to groups of two, and use words like "either" and "or"—"Would you like tea or coffee?". Since distributive constructions apply an idea relevant to each individual in the group, rather than to the group as a whole, they are most often conceived of as singular, and a singular pronoun is used.
However, many languages, including English, show ambivalence in this regard. Because distribution also requires a group with more than one member, plural forms are sometimes used.

The singular "they", which uses the same verb form plurals do, is typically used to refer to an indeterminate antecedent, for example,
In some sentences, typically those including words like "every" or "any", the morphologically singular antecedent does not refer to a single entity but is "anaphorically linked" to the associated pronoun to indicate a set of pairwise relationships, as in the sentence:

Linguists like Pinker and Huddleston explain sentences like this (and others) in terms of bound variables, a term borrowed from logic. Pinker prefers the terms "quantifier" and "bound variable" to "antecedent" and " pronoun". He suggests that pronouns used as "variables" in this way are more appropriately regarded as homonyms of the equivalent referential pronouns.

The following shows different types of anaphoric reference, using various pronouns, including "they":

A study of whether "singular "they"" is more "difficult" to understand than gendered pronouns ("In Search of Gender Neutrality: Is Singular "They" a Cognitively Efficient Substitute for Generic "He"?" by Foertsch and Gernsbacher) found that "singular "they" is a cognitively efficient substitute for generic "he" or "she", particularly when the antecedent is nonreferential" (e.g. "anybody", "a nurse", or "a truck driver") rather than referring to a specific person (e.g. "a runner I knew" or "my nurse"). Clauses with singular "they" were read "just as quickly as clauses containing a gendered pronoun that matched the stereotype of the antecedent" (e.g. "she" for a nurse and "he" for a truck driver) and "much more quickly than clauses containing a gendered pronoun that went against the gender stereotype of the antecedent".
On the other hand, when the pronoun "they" was used to refer to known individuals ("referential antecedents, for which the gender was presumably known", e.g "my nurse", "that truck driver", "a runner I knew"), reading was slowed when compared with use of a gendered pronoun consistent with the "stereotypic gender" (e.g. "he" for a specific truck driver). 
The study concluded that "the increased use of singular "they" is not problematic for the majority of readers".

The singular and plural use of "they" can be compared with the pronoun "you", which originally was only plural, but by about 1700 replaced "thou" for singular referents, while retaining the plural verb form. For "you", the singular reflexive pronoun ("yourself") is different from its plural reflexive pronoun ("yourselves"); with "they" one can hear either "themself" or "themselves" for the singular reflexive pronoun.

Singular "they" has also been compared to "royal we" (also termed "editorial we"), when a single person uses first-person plural in place of first-person singular pronouns. Similar to singular "you", its singular reflexive pronoun ("ourself") is different from the plural reflexive pronoun ("ourselves").

The pronoun "it", which is typically used for inanimate objects, can also be used for infants of unspecified gender but tends to be "dehumanizing" and is therefore more likely in a clinical context; in a more personal context, the use of "it" to refer to a person might indicate antipathy or other negative emotions.
"It" can also be used for non-human animals of unspecified gender, though "they" is common for pets and other domesticated animals of unspecified gender, especially when referred to by a proper name (e.g. "Rags", "Snuggles").
It is uncommon to use singular "they" instead of "it" for something other than a life form.







</doc>
<doc id="28189" url="https://en.wikipedia.org/wiki?curid=28189" title="Space Shuttle">
Space Shuttle

The Space Shuttle was a partially reusable low Earth orbital spacecraft system operated by the U.S. National Aeronautics and Space Administration (NASA), as part of the Space Shuttle program. Its official program name was "Space Transportation System (STS)", taken from a 1969 plan for a system of reusable spacecraft of which it was the only item funded for development. The first of four orbital test flights occurred in 1981, leading to operational flights beginning in 1982. In addition to the prototype whose completion was cancelled, five complete Shuttle systems were built and used on a total of 135 missions from 1981 to 2011, launched from the Kennedy Space Center (KSC) in Florida. Operational missions launched numerous satellites, interplanetary probes, and the Hubble Space Telescope (HST); conducted science experiments in orbit; and participated in construction and servicing of the International Space Station. The Shuttle fleet's total mission time was 1322 days, 19 hours, 21 minutes and 23 seconds.

Shuttle components included the Orbiter Vehicle (OV) with three clustered Rocketdyne RS-25 main engines, a pair of recoverable solid rocket boosters (SRBs), and the expendable external tank (ET) containing liquid hydrogen and liquid oxygen. The Space Shuttle was launched vertically, like a conventional rocket, with the two SRBs operating in parallel with the OV's three main engines, which were fueled from the ET. The SRBs were jettisoned before the vehicle reached orbit, and the ET was jettisoned just before orbit insertion, which used the orbiter's two Orbital Maneuvering System (OMS) engines. At the conclusion of the mission, the orbiter fired its OMS to de-orbit and re-enter the atmosphere. The orbiter then glided as a spaceplane to a runway landing, usually to the Shuttle Landing Facility at Kennedy Space Center, Florida or Rogers Dry Lake in Edwards Air Force Base, California. After landing at Edwards, the orbiter was flown back to the KSC on the Shuttle Carrier Aircraft, a specially modified version of the Boeing 747.

The first orbiter, "Enterprise", was built in 1976, used in Approach and Landing Tests and had no orbital capability. Four fully operational orbiters were initially built: "Columbia", "Challenger", "Discovery", and "Atlantis". Of these, two were lost in mission accidents: "Challenger" in 1986 and "Columbia" in 2003, with a total of fourteen astronauts killed. A fifth operational (and sixth in total) orbiter, "Endeavour", was built in 1991 to replace "Challenger". The Space Shuttle was retired from service upon the conclusion of "Atlantis"s final flight on July 21, 2011. The U.S. has since relied primarily on the Russian Soyuz spacecraft to transport supplies and astronauts to the International Space Station.

The Space Shuttle was a partially reusable human spaceflight vehicle capable of reaching low Earth orbit, commissioned and operated by the U.S. National Aeronautics and Space Administration (NASA) from 1981 to 2011. It resulted from shuttle design studies conducted by NASA and the U.S. Air Force in the 1960s and was first proposed for development as part of an ambitious second-generation Space Transportation System (STS) of space vehicles to follow the Apollo program in a September 1969 report of a Space Task Group headed by Vice President Spiro Agnew to President Richard Nixon. Nixon's post-Apollo NASA budgeting withdrew support of all system components except the Shuttle, to which NASA applied the STS name.

The vehicle consisted of a spaceplane for orbit and re-entry, fueled from an expendable External Tank containing liquid hydrogen and liquid oxygen, with two reusable strap-on solid rocket boosters. The first of four orbital test flights occurred in 1981, leading to operational flights beginning in 1982, all launched from the Kennedy Space Center, Florida. The system was retired from service in 2011 after 135 missions, with "Atlantis" making the final launch of the three-decade Shuttle program on July 8, 2011. The program ended after "Atlantis" landed at the Kennedy Space Center on July 21, 2011. Major missions included launching numerous satellites and interplanetary probes, conducting space science experiments, and servicing and construction of space stations. The first orbiter vehicle, named "Enterprise", was used in the initial Approach and Landing Tests phase but installation of engines, heat shielding, and other equipment necessary for orbital flight was cancelled. A total of five operational orbiters were built, and of these, two were destroyed in accidents.

It was used for orbital space missions by NASA, the U.S. Department of Defense, the European Space Agency, Japan, and Germany. The United States funded Shuttle development and operations except for the Spacelab modules used on D1 and D2sponsored by Germany. SL-J was partially funded by Japan.

At launch, it consisted of the "stack", including the dark orange external tank (ET) (for the first two launches the tank was painted white); two white, slender solid rocket boosters (SRBs); and the Orbiter Vehicle, which contained the crew and payload. Some payloads were launched into higher orbits with either of two different upper stages developed for the STS (single-stage Payload Assist Module or two-stage Inertial Upper Stage). The Space Shuttle was stacked in the Vehicle Assembly Building, and the stack mounted on a mobile launch platform held down by four frangible nuts on each SRB, which were detonated at launch.

The Shuttle stack launched vertically like a conventional rocket. It lifted off under the power of its two SRBs and three main engines, which were fueled by liquid hydrogen and liquid oxygen from the ET. The Space Shuttle had a two-stage ascent. The SRBs provided additional thrust during liftoff and first-stage flight. About two minutes after liftoff, frangible nuts were fired, releasing the SRBs, which then parachuted into the ocean, to be retrieved by NASA recovery ships for refurbishment and reuse. The orbiter and ET continued to ascend on an increasingly horizontal flight path under power from its main engines. Upon reaching 17,500 mph (7.8 km/s), necessary for low Earth orbit, the main engines were shut down. The ET, attached by two frangible nuts was then jettisoned to burn up in the atmosphere. After jettisoning the external tank, the orbital maneuvering system (OMS) engines were used to adjust the orbit.

The orbiter carried astronauts and payloads such as satellites or space station parts into low Earth orbit, the Earth's upper atmosphere or thermosphere. Usually, five to seven crew members rode in the orbiter. Two crew members, the commander and pilot, were sufficient for a minimal flight, as in the first four "test" flights, STS-1 through STS-4. The typical payload capacity was about but could be increased depending on the choice of launch configuration. The orbiter carried its payload in a large cargo bay with doors that opened along the length of its top, a feature which made the Space Shuttle unique among spacecraft. This feature made possible the deployment of large satellites such as the Hubble Space Telescope and also the capture and return of large payloads back to Earth.

When the orbiter's space mission was complete, it fired its OMS thrusters to drop out of orbit and re-enter the lower atmosphere. During descent, the orbiter passed through different layers of the atmosphere and decelerated from hypersonic speed primarily by aerobraking. In the lower atmosphere and landing phase, it was more like a glider but with reaction control system (RCS) thrusters and fly-by-wire-controlled hydraulically actuated flight surfaces controlling its descent. It landed on a long runway as a conventional aircraft. The aerodynamic shape was a compromise between the demands of radically different speeds and air pressures during re-entry, hypersonic flight, and subsonic atmospheric flight. As a result, the orbiter had a relatively high sink rate at low altitudes, and it transitioned during re-entry from using RCS thrusters at very high altitudes to flight surfaces in the lower atmosphere.

The formal design of what became the Space Shuttle began with the "Phase A" contract design studies issued in the late 1960s. Conceptualization had begun two decades earlier, before the Apollo program of the 1960s. One of the places the concept of a spacecraft returning from space to a horizontal landing originated was within NACA, in 1954, in the form of an aeronautics research experiment later named the X-15. The NACA proposal was submitted by Walter Dornberger.

In 1958, the X-15 concept further developed into a proposal to launch an X-15 into space, and another X-series spaceplane proposal, named X-20 Dyna-Soar, as well as variety of aerospace plane concepts and studies. Neil Armstrong was selected to pilot both the X-15 and the X-20. Though the X-20 was not built, another spaceplane similar to the X-20 was built several years later and delivered to NASA in January 1966 called the HL-10 ("HL" indicated "horizontal landing").

In the mid-1960s, the U.S. Air Force conducted classified studies on next-generation space transportation systems and concluded that semi-reusable designs were the cheapest choice. It proposed a development program with an immediate start on a "Class I" vehicle with expendable boosters, followed by slower development of a "Class II" semi-reusable design and possible "Class III" fully reusable design later. In 1967, George Mueller held a one-day symposium at NASA headquarters to study the options. Eighty people attended and presented a wide variety of designs, including earlier U.S. Air Force designs such as the X-20 Dyna-Soar.

In 1968, NASA officially began work on what was then known as the Integrated Launch and Re-entry Vehicle (ILRV). At the same time, NASA held a separate Space Shuttle Main Engine (SSME) competition. NASA offices in Houston and Huntsville jointly issued a Request for Proposal (RFP) for ILRV studies to design a spacecraft that could deliver a payload to orbit but also re-enter the atmosphere and fly back to Earth. For example, one of the responses was for a two-stage design, featuring a large booster and a small orbiter, called the DC-3, one of several Phase A Shuttle designs. After the aforementioned "Phase A" studies, B, C, and D phases progressively evaluated in-depth designs up to 1972. In the final design, the bottom stage consisted of recoverable solid rocket boosters, and the top stage used an expendable external tank.

In 1969, President Richard Nixon decided to support proceeding with Space Shuttle development. A series of development programs and analysis refined the basic design, prior to full development and testing. In August 1973, the X-24B proved that an unpowered spaceplane could re-enter Earth's atmosphere for a horizontal landing.

Across the Atlantic, European ministers met in Belgium in 1973 to authorize Western Europe's manned orbital project and its main contribution to Space Shuttlethe Spacelab program. Spacelab would provide a multidisciplinary orbital space laboratory and additional space equipment for the Shuttle.

The Space Shuttle was the first operational orbital spacecraft designed for reuse. It carried different payloads to low Earth orbit, provided crew rotation and supplies for the International Space Station (ISS), and performed satellite servicing and repair. The orbiter could also recover satellites and other payloads from orbit and return them to Earth. Each Shuttle was designed for a projected lifespan of 100 launches or ten years of operational life, although this was later extended. The person in charge of designing the STS was Maxime Faget, who had also overseen the Mercury, Gemini, and Apollo spacecraft designs. The crucial factor in the size and shape of the Shuttle orbiter was the requirement that it be able to accommodate the largest planned commercial and military satellites, and have over 1,000 mile cross-range recovery range to meet the requirement for classified USAF missions for a once-around abort from a launch to a polar orbit. The militarily specified cross range requirement was one of the primary reasons for the Shuttle's large wings, compared to modern commercial designs with very minimal control surfaces and glide capability. Factors involved in opting for solid rockets and an expendable fuel tank included the desire of the Pentagon to obtain a high-capacity payload vehicle for satellite deployment, and the desire of the Nixon administration to reduce the costs of space exploration by developing a spacecraft with reusable components.

Each Space Shuttle was a reusable launch system composed of three main assemblies: the reusable OV, the expendable ET, and the two reusable SRBs. Only the OV entered orbit shortly after the tank and boosters are jettisoned. The vehicle was launched vertically like a conventional rocket, and the orbiter glided to a horizontal landing like an airplane, after which it was refurbished for reuse. The SRBs parachuted to splashdown in the ocean where they were towed back to shore and refurbished for later Shuttle missions.

Five operational OVs were built: "Columbia" (OV-102), "Challenger" (OV-099), "Discovery" (OV-103), "Atlantis" (OV-104), and "Endeavour" (OV-105). A mock-up, "Inspiration", currently stands at the entrance to the Astronaut Hall of Fame. An additional craft, "Enterprise" (OV-101), was built for atmospheric testing gliding and landing; it was originally intended to be outfitted for orbital operations after the test program, but it was found more economical to upgrade the structural test article STA-099 into orbiter "Challenger" (OV-099). "Challenger" disintegrated 73 seconds after launch in 1986, and "Endeavour" was built as a replacement from structural spare components. Building "Endeavour" cost about US$1.7 billion. "Columbia" broke apart over Texas during re-entry in 2003. A Space Shuttle launch cost around $450 million.

Roger A. Pielke, Jr. has estimated that the Space Shuttle program cost about US$170 billion (2008 dollars) through early 2008; the average cost per flight was about US$1.5 billion. Two missions were paid for by Germany, Spacelab D1 and D2 (D for "Deutschland") with a payload control center in Oberpfaffenhofen. D1 was the first time that control of a manned STS mission payload was not in U.S. hands.

At times, the orbiter itself was referred to as the Space Shuttle. This was not technically correct as the "Space Shuttle" was the combination of the orbiter, the external tank, and the two solid rocket boosters. These components, once assembled in the Vehicle Assembly Building originally built to assemble the Apollo Saturn V rocket, were commonly referred to as the "stack".

Responsibility for the Shuttle components was spread among multiple NASA field centers. The Kennedy Space Center was responsible for launch, landing and turnaround operations for equatorial orbits (the only orbit profile actually used in the program), the U.S. Air Force at the Vandenberg Air Force Base was responsible for launch, landing and turnaround operations for polar orbits (though this was never used), the Johnson Space Center served as the central point for all Shuttle operations, the Marshall Space Flight Center was responsible for the main engines, external tank, and solid rocket boosters, the John C. Stennis Space Center handled main engine testing, and the Goddard Space Flight Center managed the global tracking network.

The orbiter resembled a conventional aircraft, with double-delta wings swept 81° at the inner leading edge and 45° at the outer leading edge. Its vertical stabilizer's leading edge was swept back at a 50° angle. The four elevons, mounted at the trailing edge of the wings, and the rudder/speed brake, attached at the trailing edge of the stabilizer, with the body flap, controlled the orbiter during descent and landing.

The orbiter's -long payload bay, comprising most of the fuselage, could accommodate cylindrical payloads up to in diameter. Information declassified in 2011 showed that these measurements were chosen specifically to accommodate the KH-9 HEXAGON spy satellite operated by the National Reconnaissance Office. Two mostly-symmetrical lengthwise payload bay doors hinged on either side of the bay comprised its entire top. Payloads were generally loaded horizontally into the bay while the orbiter was standing upright on the launch pad and unloaded vertically in the near-weightless orbital environment by the orbiter's robotic remote manipulator arm (under astronaut control), EVA astronauts, or under the payloads' own power (as for satellites attached to a rocket "upper stage" for deployment.)

Three Space Shuttle Main Engines (SSMEs) were mounted on the orbiter's aft fuselage in a triangular pattern. The engine nozzles could gimbal 10.5 degrees up and down, and 8.5 degrees from side to side during ascent to change the direction of their thrust to steer the Shuttle. The orbiter structure was made primarily from aluminum alloy, although the engine structure was made primarily from titanium alloy.

The operational orbiters built were OV-102 "Columbia", OV-099 "Challenger", OV-103 "Discovery", OV-104 "Atlantis", and OV-105 "Endeavour".

The main function of the Space Shuttle external tank was to supply the liquid oxygen and hydrogen fuel to the main engines. It was also the backbone of the launch vehicle, providing attachment points for the two solid rocket boosters and the orbiter. The external tank was the only part of the Shuttle system that was not reused. Although the external tanks were always discarded, it would have been possible to take them into orbit and re-use them (such as a wet workshop for incorporation into a space station).

Two solid rocket boosters (SRBs) each provided of thrust at liftoff, which was 83% of the total thrust at liftoff. The SRBs were jettisoned two minutes after launch at a height of about , and then deployed parachutes and landed in the ocean to be recovered. The SRB cases were made of steel about ½ inch (13 mm) thick. The solid rocket boosters were re-used many times; the casing used in Ares I engine testing in 2009 consisted of motor cases that had been flown, collectively, on 48 Shuttle missions, including STS-1.

Astronauts who have flown on multiple spacecraft report that Shuttle delivers a rougher ride than Apollo or Soyuz. The additional vibration is caused by the solid rocket boosters, as solid fuel does not burn as evenly as liquid fuel. The vibration dampens down after the solid rocket boosters have been jettisoned.

The orbiter could be used in conjunction with a variety of add-ons depending on the mission. This included orbital laboratories (Spacelab, Spacehab), boosters for launching payloads farther into space (Inertial Upper Stage, Payload Assist Module), and other functions, such as provided by Extended Duration Orbiter, Multi-Purpose Logistics Modules, or Canadarm (RMS). An upper stage called Transfer Orbit Stage (Orbital Science Corp. TOS-21) was also used once with the orbiter. Other types of systems and racks were part of the modular Spacelab system pallets, igloo, IPS, etc., which also supported special missions such as SRTM.

A major component of the Space Shuttle Program was Spacelab, primarily contributed by a consortium of European countries, and operated in conjunction with the United States and international partners. Supported by a modular system of pressurized modules, pallets, and systems, Spacelab missions executed on multidisciplinary science, orbital logistics, and international cooperation. Over 29 missions flew on subjects ranging from astronomy, microgravity, radar, and life sciences, to name a few. Spacelab hardware also supported missions such as Hubble (HST) servicing and space station resupply. STS-2 and STS-3 provided testing, and the first full mission was Spacelab-1 (STS-9) launched on November 28, 1983.

Spacelab formally began in 1973, after a meeting in Brussels, Belgium, by European heads of state. Within the decade, Spacelab went into orbit and provided Europe and the United States with an orbital workshop and hardware system. International cooperation, science, and exploration were realized on Spacelab.

The Shuttle was one of the earliest craft to use a computerized fly-by-wire digital flight control system. This means no mechanical or hydraulic linkages connected the pilot's control stick to the control surfaces or reaction control system thrusters. The control algorithm, which used a classical Proportional Integral Derivative (PID) approach, was developed and maintained by Honeywell. The Shuttle's fly-by-wire digital flight control system was composed of 4 control systems each addressing a different mission phase: Ascent, Descent, On-Orbit and Aborts. Honeywell is also credited with the design and implementation of the Shuttle's Nose Wheel Steering Control Algorithm that allowed the Orbiter to safely land at Kennedy Space Center's Shuttle Runway.

A concern with using digital fly-by-wire systems on the Shuttle was reliability. Considerable research went into the Shuttle computer system. The Shuttle used five identical redundant IBM 32-bit general purpose computers (GPCs), model AP-101, constituting a type of embedded system. Four computers ran specialized software called the Primary Avionics Software System (PASS). A fifth backup computer ran separate software called the Backup Flight System (BFS). Collectively they were called the Data Processing System (DPS).
The design goal of the Shuttle's DPS was fail-operational/fail-safe reliability. After a single failure, the Shuttle could still continue the mission. After two failures, it could still land safely.

The four general-purpose computers operated essentially in lockstep, checking each other. If one computer provided a different result than the other three (i.e. the one computer failed), the three functioning computers "voted" it out of the system. This isolated it from vehicle control. If a second computer of the three remaining failed, the two functioning computers voted it out. A very unlikely failure mode would have been where two of the computers produced result A, and two produced result B (a two-two split). In this unlikely case, one group of two was to be picked at random.

The Backup Flight System (BFS) was separately developed software running on the fifth computer, used only if the entire four-computer primary system failed. The BFS was created because although the four primary computers were hardware redundant, they all ran the same software, so a generic software problem could crash all of them. Embedded system avionic software was developed under totally different conditions from public commercial software: the number of code lines was tiny compared to a public commercial software product, changes were only made infrequently and with extensive testing, and many programming and test personnel worked on the small amount of computer code. However, in theory it could have still failed, and the BFS existed for that contingency. While the BFS could run in parallel with PASS, the BFS never engaged to take over control from PASS during any Shuttle mission.

The software for the Shuttle computers was written in a high-level language called HAL/S, somewhat similar to PL/I. It is specifically designed for a real time embedded system environment.

The IBM AP-101 computers originally had about 424 kilobytes of magnetic core memory each. The CPU could process about 400,000 instructions per second. They had no hard disk drive, and loaded software from magnetic tape cartridges.

In 1990, the original computers were replaced with an upgraded model AP-101S, which had about 2.5 times the memory capacity (about 1 megabyte) and three times the processor speed (about 1.2 million instructions per second). The memory was changed from magnetic core to semiconductor with battery backup.

Early Shuttle missions, starting in November 1983, took along the Grid Compass, arguably one of the first laptop computers. The GRiD was given the name SPOC, for Shuttle Portable Onboard Computer. Use on the Shuttle required both hardware and software modifications which were incorporated into later versions of the commercial product. It was used to monitor and display the Shuttle's ground position, path of the next two orbits, show where the Shuttle had line of sight communications with ground stations, and determine points for location-specific observations of the Earth. The Compass sold poorly, as it cost at least US$8000, but it offered unmatched performance for its weight and size. NASA was one of its main customers.

During its service life, the Shuttle's Control System never experienced a failure. Many of the lessons learned have been used to design today's high speed control algorithms.

The prototype orbiter "Enterprise" originally had a flag of the United States on the upper surface of the left wing and the letters "USA" in black on the right wing. The name "Enterprise" was painted in black on the payload bay doors just above the hinge and behind the crew module; on the aft end of the payload bay doors was the NASA "worm" logotype in gray. Underneath the rear of the payload bay doors on the side of the fuselage just above the wing is the text "United States" in black with a flag of the United States ahead of it.

The first operational orbiter, "Columbia", originally had the same markings as "Enterprise", although the letters "USA" on the right wing were slightly larger and spaced farther apart. "Columbia" also had black markings which "Enterprise" lacked on its forward RCS module, around the cockpit windows, and on its vertical stabilizer, and had distinctive black "chines" on the forward part of its upper wing surfaces, which none of the other orbiters had.

"Challenger" established a modified marking scheme for the shuttle fleet that was matched by "Discovery", "Atlantis" and "Endeavour". The letters "USA" in black above an American flag were displayed on the left wing, with the NASA "worm" logotype in gray centered above the name of the orbiter in black on the right wing. The name of the orbiter was inscribed not on the payload bay doors, but on the forward fuselage just below and behind the cockpit windows. This would make the name visible when the shuttle was photographed in orbit with the doors open.

In 1983, "Enterprise" had its wing markings changed to match "Challenger", and the NASA "worm" logotype on the aft end of the payload bay doors was changed from gray to black. Some black markings were added to the nose, cockpit windows and vertical tail to more closely resemble the flight vehicles, but the name "Enterprise" remained on the payload bay doors as there was never any need to open them. "Columbia" had its name moved to the forward fuselage to match the other flight vehicles after STS-61-C, during the 1986–88 hiatus when the shuttle fleet was grounded following the loss of "Challenger", but retained its original wing markings until its last overhaul (after STS-93), and its unique black wing "chines" for the remainder of its operational life.

Beginning in 1998, the flight vehicles' markings were modified to incorporate the NASA "meatball" insignia. The "worm" logotype, which the agency had phased out, was removed from the payload bay doors and the "meatball" insignia was added aft of the "United States" text on the lower aft fuselage. The "meatball" insignia was also displayed on the left wing, with the American flag above the orbiter's name, left-justified rather than centered, on the right wing. The three surviving flight vehicles, "Discovery", "Atlantis" and "Endeavour", still bear these markings as museum displays. "Enterprise" became the property of the Smithsonian Institution in 1985 and was no longer under NASA's control when these changes were made, hence the prototype orbiter still has its 1983 markings and still has its name on the payload bay doors.

The Space Shuttle was initially developed in the 1970s, but received many upgrades and modifications afterward to improve performance, reliability and safety. Internally, the Shuttle remained largely similar to the original design, with the exception of the improved avionics computers. In addition to the computer upgrades, the original analog primary flight instruments were replaced with modern full-color, flat-panel display screens, called a glass cockpit, which is similar to those of contemporary airliners. To facilitate construction of ISS, the internal airlocks of each orbiter except "Columbia" were replaced with external docking systems to allow for a greater amount of cargo to be stored on the Shuttle's mid-deck during station resupply missions.

The Space Shuttle Main Engines (SSMEs) had several improvements to enhance reliability and power. This explains phrases such as "Main engines throttling up to 104 percent." This did not mean the engines were being run over a safe limit. The 100 percent figure was the original specified power level. During the lengthy development program, Rocketdyne determined the engine was capable of safe reliable operation at 104 percent of the originally specified thrust. NASA could have rescaled the output number, saying in essence 104 percent is now 100 percent. To clarify this would have required revising much previous documentation and software, so the 104 percent number was retained. SSME upgrades were denoted as "block numbers", such as block I, block II, and block IIA. The upgrades improved engine reliability, maintainability and performance. The 109% thrust level was finally reached in flight hardware with the Block II engines in 2001. The normal maximum throttle was 104 percent, with 106 percent or 109 percent used for mission aborts.

For the first two missions, STS-1 and STS-2, the external tank was painted white to protect the insulation that covers much of the tank, but improvements and testing showed that it was not required. The weight saved by not painting the tank resulted in an increase in payload capability to orbit. Additional weight was saved by removing some of the internal "stringers" in the hydrogen tank that proved unnecessary. The resulting "light-weight external tank" was first flown on STS-6 and used on the majority of Shuttle missions. STS-91 saw the first flight of the "super light-weight external tank". This version of the tank was made of the 2195 aluminum-lithium alloy. It weighed 3.4 metric tons (7,500 lb) less than the last run of lightweight tanks, allowing the Shuttle to deliver heavy elements to ISS's high inclination orbit. As the Shuttle was always operated with a crew, each of these improvements was first flown on operational mission flights.

The solid rocket boosters underwent improvements as well. Design engineers added a third O-ring seal to the joints between the segments after the 1986 Space Shuttle "Challenger" disaster.
Several other SRB improvements were planned to improve performance and safety, but never came to be. These culminated in the considerably simpler, lower cost, probably safer and better-performing Advanced Solid Rocket Booster. These rockets entered production in the early to mid-1990s to support the Space Station, but were later canceled to save money after the expenditure of $2.2 billion. The loss of the ASRB program resulted in the development of the Super LightWeight external Tank (SLWT), which provided some of the increased payload capability, while not providing any of the safety improvements. In addition, the U.S. Air Force developed their own much lighter single-piece SRB design using a filament-wound system, but this too was canceled.

STS-70 was delayed in 1995, when woodpeckers bored holes in the foam insulation of "Discovery"'s external tank. Since then, NASA has installed commercial plastic owl decoys and inflatable owl balloons which had to be removed prior to launch. The delicate nature of the foam insulation had been the cause of damage to the Thermal Protection System, the tile heat shield and heat wrap of the orbiter. NASA remained confident that this damage, while it was the primary cause of the Space Shuttle "Columbia" disaster on February 1, 2003, would not jeopardize the completion of the International Space Station (ISS) in the projected time allotted.

A cargo-only, unmanned variant of the Shuttle was variously proposed and rejected since the 1980s. It was called the Shuttle-C, and would have traded re-usability for cargo capability, with large potential savings from reusing technology developed for the Space Shuttle. Another proposal was to convert the payload bay into a passenger area, with versions ranging from 30 to 74 seats, three days in orbit, and cost US$1.5 million per seat.

On the first four Shuttle missions, astronauts wore modified U.S. Air Force high-altitude full-pressure suits, which included a full-pressure helmet during ascent and descent. From the fifth flight, STS-5, until the loss of "Challenger", one-piece light blue nomex flight suits and partial-pressure helmets were worn. A less-bulky, partial-pressure version of the high-altitude pressure suits with a helmet was reinstated when Shuttle flights resumed in 1988. The Launch-Entry Suit ended its service life in late 1995, and was replaced by the full-pressure Advanced Crew Escape Suit (ACES), which resembled the Gemini space suit in design, but retained the orange color of the Launch-Entry Suit.

To extend the duration that orbiters could stay docked at the ISS, the Station-to-Shuttle Power Transfer System (SSPTS) was installed. The SSPTS allowed these orbiters to use power provided by the ISS to preserve their consumables. The SSPTS was first used successfully on STS-118.

Orbiter (for "Endeavour", OV-105)

External tank (for SLWT)

Solid Rocket Boosters

System Stack

All Space Shuttle missions were launched from Kennedy Space Center (KSC). The weather criteria used for launch included, but were not limited to: precipitation, temperatures, cloud cover, lightning forecast, wind, and humidity. The Shuttle was not launched under conditions where it could have been struck by lightning. Aircraft are often struck by lightning with no adverse effects because the electricity of the strike is dissipated through its conductive structure and the aircraft is not electrically grounded. Like most jet airliners, the Shuttle was mainly constructed of conductive aluminum, which would normally shield and protect the internal systems. However, upon liftoff the Shuttle sent out a long exhaust plume as it ascended, and this plume could have triggered lightning by providing a current path to ground. The NASA Anvil Rule for a Shuttle launch stated that an anvil cloud could not appear within a distance of 10 nautical miles. The Shuttle Launch Weather Officer monitored conditions until the final decision to scrub a launch was announced. In addition, the weather conditions had to be acceptable at one of the Transatlantic Abort Landing sites (one of several Space Shuttle abort modes) to launch as well as the solid rocket booster recovery area. While the Shuttle might have safely endured a lightning strike, a similar strike caused problems on Apollo 12, so for safety NASA chose not to launch the Shuttle if lightning was possible (NPR8715.5).

Historically, the Shuttle was not launched if its flight would run from December to January (a year-end rollover or YERO). Its flight software, designed in the 1970s, was not designed for this, and would require the orbiter's computers be reset through a change of year, which could cause a glitch while in orbit. In 2007, NASA engineers devised a solution so Shuttle flights could cross the year-end boundary.

After the final hold in the countdown at T-minus 9 minutes, the Shuttle went through its final preparations for launch, and the countdown was automatically controlled by the Ground Launch Sequencer (GLS), software at the Launch Control Center, which stopped the count if it sensed a critical problem with any of the Shuttle's onboard systems. The GLS handed off the count to the Shuttle's on-board computers at T minus 31 seconds, in a process called auto sequence start.

At T-minus 16 seconds, the massive sound suppression system (SPS) began to drench the Mobile Launcher Platform (MLP) and SRB trenches with of water to protect the Orbiter from damage by acoustical energy and rocket exhaust reflected from the flame trench and MLP during lift off.

At T-minus 10 seconds, hydrogen igniters were activated under each engine bell to quell the stagnant gas inside the cones before ignition. Failure to burn these gases could trip the onboard sensors and create the possibility of an overpressure and explosion of the vehicle during the firing phase. The main engine turbopumps also began charging the combustion chambers with liquid hydrogen and liquid oxygen at this time. The computers reciprocated this action by allowing the redundant computer systems to begin the firing phase.
The three main engines (SSMEs) started at "T"-6.6 seconds. The main engines ignited sequentially via the Shuttle's general purpose computers (GPCs) at 120 millisecond intervals. All three SSMEs were required to reach 90% rated thrust within three seconds, otherwise the onboard computers would initiate an RSLS abort. If all three engines indicated nominal performance by "T"-3 seconds, they were commanded to gimbal to liftoff configuration and the command would be issued to arm the SRBs for ignition at "T"-0. Between "T"-6.6 seconds and "T"-3 seconds, while the SSMEs were firing but the SRBs were still bolted to the pad, the offset thrust caused the entire launch stack (boosters, tank and orbiter) to pitch down measured at the tip of the external tank. The three second delay after confirmation of SSME operation was to allow the stack to return to nearly vertical. At "T"-0 seconds, the 8 frangible nuts holding the SRBs to the pad were detonated, the SSMEs were commanded to 100% throttle, and the SRBs were ignited. By "T"+0.23 seconds, the SRBs built up enough thrust for liftoff to commence, and reached maximum chamber pressure by "T"+0.6 seconds. The Johnson Space Center's Mission Control Center assumed control of the flight once the SRBs had cleared the launch tower.

Shortly after liftoff, the Shuttle's main engines were throttled up to 104.5% and the vehicle began a combined roll, pitch and yaw maneuver that placed it onto the correct heading (azimuth) for the planned orbital inclination and in a heads down attitude with wings level. The Shuttle flew upside down during the ascent phase. This orientation allowed a trim angle of attack that was favorable for aerodynamic loads during the region of high dynamic pressure, resulting in a net positive load factor, as well as providing the flight crew with a view of the horizon as a visual reference. The vehicle climbed in a progressively flattening arc, accelerating as the mass of the SRBs and main tank decreased. To achieve low orbit requires much more horizontal than vertical acceleration. This was not visually obvious, since the vehicle rose vertically and was out of sight for most of the horizontal acceleration. The near circular orbital velocity at the altitude of the International Space Station is , roughly equivalent to Mach 23 at sea level. As the International Space Station orbits at an inclination of 51.6 degrees, missions going there must set orbital inclination to the same value in order to rendezvous with the station.

Around 30 seconds into ascent, the SSMEs were throttled down—usually to 72%, though this varied—to reduce the maximum aerodynamic forces acting on the Shuttle at a point called Max Q. Additionally, the propellant grain design of the SRBs caused their thrust to drop by about 30% by 50 seconds into ascent. Once the Orbiter's guidance verified that Max Q would be within Shuttle structural limits, the main engines were throttled back up to 104.5%; this throttling down and back up was called the "thrust bucket". To maximize performance, the throttle level and timing of the thrust bucket was shaped to bring the Shuttle as close to aerodynamic limits as possible.
At around "T"+126 seconds, pyrotechnic fasteners released the SRBs and small separation rockets pushed them laterally away from the vehicle. The SRBs parachuted back to the ocean to be reused. The Shuttle then began accelerating to orbit on the main engines. Acceleration at this point would typically fall to .9 "g", and the vehicle would take on a somewhat nose-up angle to the horizonit used the main engines to gain and then maintain altitude while it accelerated horizontally towards orbit. At about five and three-quarter minutes into ascent, the orbiter's direct communication links with the ground began to fade, at which point it rolled heads up to reroute its communication links to the Tracking and Data Relay Satellite system.

At about seven and a half minutes into ascent, the mass of the vehicle was low enough that the engines had to be throttled back to limit vehicle acceleration to 3 "g" (29.4 m/s² or 96.5 ft/s², equivalent to accelerating from zero to in a second). The Shuttle would maintain this acceleration for the next minute, and main engine cut-off (MECO) occurred at about eight and a half minutes after launch. The main engines were shut down before complete depletion of propellant, as running dry would have destroyed the engines. The oxygen supply was terminated before the hydrogen supply, as the SSMEs reacted unfavorably to other shutdown modes. (Liquid oxygen has a tendency to react violently, and supports combustion when it encounters hot engine metal.) A few seconds after MECO, the external tank was released by firing pyrotechnic fasteners.

At this point the Shuttle and external tank were on a slightly suborbital trajectory, coasting up towards apogee. Once at apogee, about half an hour after MECO, the Shuttle's Orbital Maneuvering System (OMS) engines were fired to raise its perigee and achieve orbit, while the external tank fell back into the atmosphere and burned up over the Indian Ocean or the Pacific Ocean depending on launch profile. The sealing action of the tank plumbing and lack of pressure relief systems on the external tank helped it break up in the lower atmosphere. After the foam burned away during re-entry, the heat caused a pressure buildup in the remaining liquid oxygen and hydrogen until the tank exploded. This ensured that any pieces that fell back to Earth were small.


The Shuttle was monitored throughout its ascent for short range tracking (10 seconds before liftoff through 57 seconds after), medium range (7 seconds before liftoff through 110 seconds after) and long range (7 seconds before liftoff through 165 seconds after). Short range cameras included 22 16mm cameras on the Mobile Launch Platform and 8 16mm on the Fixed Service Structure, 4 high speed fixed cameras located on the perimeter of the launch complex plus an additional 42 fixed cameras with 16mm motion picture film. Medium range cameras included remotely operated tracking cameras at the launch complex plus 6 sites along the immediate coast north and south of the launch pad, each with 800mm lens and high speed cameras running 100 frames per second. These cameras ran for only 4–10 seconds due to limitations in the amount of film available. Long range cameras included those mounted on the external tank, SRBs and orbiter itself which streamed live video back to the ground providing valuable information about any debris falling during ascent. Long range tracking cameras with 400-inch film and 200-inch video lenses were operated by a photographer at Playalinda Beach as well as 9 other sites from 38 miles north at the Ponce Inlet to 23 miles south to Patrick Air Force Base (PAFB) and additional mobile optical tracking camera was stationed on Merritt Island during launches. A total of 10 HD cameras were used both for ascent information for engineers and broadcast feeds to networks such as NASA TV and HDNet. The number of cameras significantly increased and numerous existing cameras were upgraded at the recommendation of the Columbia Accident Investigation Board to provide better information about the debris during launch. Debris was also tracked using a pair of Weibel Continuous Pulse Doppler X-band radars, one on board the SRB recovery ship MV "Liberty Star" positioned north east of the launch pad and on a ship positioned south of the launch pad. Additionally, during the first 2 flights following the loss of "Columbia" and her crew, a pair of NASA WB-57 reconnaissance aircraft equipped with HD Video and Infrared flew at to provide additional views of the launch ascent. Kennedy Space Center also invested nearly $3 million in improvements to the digital video analysis systems in support of debris tracking.

Once in orbit, the Shuttle usually flew at an altitude of , although the STS-82 mission reached . In the 1980s and 1990s, many flights involved space science missions on the NASA/ESA Spacelab, or launching various types of satellites and science probes. By the 1990s and 2000s the focus shifted more to servicing the space station, with fewer satellite launches. Most missions involved staying in orbit several days to two weeks, although longer missions were possible with the Extended Duration Orbiter add-on or when attached to a space station. STS-80 was the longest at almost 17 days and 16 hours.

Almost the entire Space Shuttle re-entry procedure, except for lowering the landing gear and deploying the air data probes, was normally performed under computer control. However, the re-entry could be flown entirely manually if an emergency arose. The approach and landing phase could be controlled by the autopilot, but was usually hand flown.

The vehicle began re-entry by firing the Orbital maneuvering system engines, while flying upside down, backside first, in the opposite direction to orbital motion for approximately three minutes, which reduced the Shuttle's velocity by about . The resultant slowing of the Shuttle lowered its orbital perigee down into the upper atmosphere. The Shuttle then flipped over, by pushing its nose down (which was actually "up" relative to the Earth, because it was flying upside down). This OMS firing was done roughly halfway around the globe from the landing site.

The vehicle started encountering more significant air density in the lower thermosphere at about , at around Mach 25, . The vehicle was controlled by a combination of RCS thrusters and control surfaces, to fly at a 40-degree nose-up attitude, producing high drag, not only to slow it down to landing speed, but also to reduce reentry heating. As the vehicle encountered progressively denser air, it began a gradual transition from spacecraft to aircraft. In a straight line, its 40-degree nose-up attitude would cause the descent angle to flatten-out, or even rise. The vehicle therefore performed a series of four steep S-shaped banking turns, each lasting several minutes, at up to 70 degrees of bank, while still maintaining the 40-degree angle of attack. In this way it dissipated speed sideways rather than upwards. This occurred during the 'hottest' phase of re-entry, when the heat-shield glowed red and the G-forces were at their highest. By the end of the last turn, the transition to aircraft was almost complete. The vehicle leveled its wings, lowered its nose into a shallow dive and began its approach to the landing site.

The orbiter's maximum glide ratio/lift-to-drag ratio varies considerably with speed, ranging from 1:1 at hypersonic speeds, 2:1 at supersonic speeds and reaching 4.5:1 at subsonic speeds during approach and landing.

In the lower atmosphere, the orbiter flies much like a conventional glider, except for a much higher descent rate, over or 9,800 fpm. At approximately Mach 3, two air data probes, located on the left and right sides of the orbiter's forward lower fuselage, are deployed to sense air pressure related to the vehicle's movement in the atmosphere.

When the approach and landing phase began, the orbiter was at a altitude, from the runway. The pilots applied aerodynamic braking to help slow down the vehicle. The orbiter's speed was reduced from , approximately, at touch-down (compared to for a jet airliner). The landing gear was deployed while the Orbiter was flying at . To assist the speed brakes, a drag chute was deployed either after main gear or nose gear touchdown (depending on selected chute deploy mode) at about . The chute was jettisoned once the orbiter slowed to .

After landing, the vehicle stayed on the runway for several hours for the orbiter to cool. Teams at the front and rear of the orbiter tested for presence of hydrogen, hydrazine, monomethylhydrazine, nitrogen tetroxide and ammonia (fuels and by-products of the reaction control system and the orbiter's three APUs). If hydrogen was detected, an emergency would be declared, the orbiter powered down and teams would evacuate the area. A convoy of 25 specially designed vehicles and 150 trained engineers and technicians approached the orbiter. Purge and vent lines were attached to remove toxic gases from fuel lines and the cargo bay about 45–60 minutes after landing. A flight surgeon boarded the orbiter for initial medical checks of the crew before disembarking. Once the crew left the orbiter, responsibility for the vehicle was handed from the Johnson Space Center back to the Kennedy Space Center.

If the mission ended at Edwards Air Force Base in California, White Sands Space Harbor in New Mexico, or any of the runways the orbiter might use in an emergency, the orbiter was loaded atop the Shuttle Carrier Aircraft, a modified 747, for transport back to the Kennedy Space Center, landing at the Shuttle Landing Facility. Once at the Shuttle Landing Facility, the orbiter was then towed along a tow-way and access roads normally used by tour buses and KSC employees to the Orbiter Processing Facility where it began a months-long preparation process for the next mission.

NASA preferred Space Shuttle landings to be at Kennedy Space Center. If weather conditions made landing there unfavorable, the Shuttle could delay its landing until conditions are favorable, touch down at Edwards Air Force Base, California, or use one of the multiple alternate landing sites around the world. A landing at any site other than Kennedy Space Center meant that after touchdown the Shuttle must be mated to the Shuttle Carrier Aircraft and returned to Cape Canaveral. Space Shuttle "Columbia" (STS-3) once landed at the White Sands Space Harbor, New Mexico; this was viewed as a last resort as NASA scientists believed that the sand could potentially damage the Shuttle's exterior.

There were many alternative landing sites that were never used.

An example of technical risk analysis for a STS mission is SPRA iteration 3.1 top risk contributors for STS-133:

An internal NASA risk assessment study (conducted by the Shuttle Program Safety and Mission Assurance Office at Johnson Space Center) released in late 2010 or early 2011 concluded that the agency had seriously underestimated the level of risk involved in operating the Shuttle. The report assessed that there was a 1 in 9 chance of a catastrophic disaster during the first nine flights of the Shuttle but that safety improvements had later improved the risk ratio to 1 in 90.

Below is a list of major events in the Space Shuttle orbiter fleet.
Sources: NASA launch manifest, NASA Space Shuttle archive

On January 28, 1986, "Challenger" disintegrated 73 seconds after launch due to the failure of the right SRB, killing all seven astronauts on board. The disaster was caused by low-temperature impairment of an O-ring, a mission critical seal used between segments of the SRB casing. Failure of the O-ring allowed hot combustion gases to escape from between the booster sections and burn through the adjacent external tank, causing it to explode. Repeated warnings from design engineers voicing concerns about the lack of evidence of the O-rings' safety when the temperature was below 53 °F (12 °C) had been ignored by NASA managers.

On February 1, 2003, "Columbia" disintegrated during re-entry, killing its crew of seven, because of damage to the carbon-carbon leading edge of the wing caused during launch. Ground control engineers had made three separate requests for high-resolution images taken by the Department of Defense that would have provided an understanding of the extent of the damage, while NASA's chief thermal protection system (TPS) engineer requested that astronauts on board "Columbia" be allowed to leave the vehicle to inspect the damage. NASA managers intervened to stop the Department of Defense's assistance and refused the request for the spacewalk, and thus the feasibility of scenarios for astronaut repair or rescue by "Atlantis" were not considered by NASA management at the time.

NASA retired the Space Shuttle in 2011, after 30 years of service. The Shuttle was originally conceived of and presented to the public as a "Space Truck", which would, among other things, be used to build a United States space station in low earth orbit in the early 1990s. When the U.S. space station evolved into the International Space Station project, which suffered from long delays and design changes before it could be completed, the retirement of the Space Shuttle was delayed several times until 2011, serving at least 15 years longer than originally planned. "Discovery" was the first of NASA's three remaining operational Space Shuttles to be retired.

The final Space Shuttle mission was originally scheduled for late 2010, but the program was later extended to July 2011 when Michael Suffredini of the ISS program said that one additional trip was needed in 2011 to deliver parts to the International Space Station. The Shuttle's final mission consisted of just four astronauts—Christopher Ferguson (Commander), Douglas Hurley (Pilot), Sandra Magnus (Mission Specialist 1), and Rex Walheim (Mission Specialist 2); they conducted the 135th and last space Shuttle mission on board "Atlantis", which launched on July 8, 2011, and landed safely at the Kennedy Space Center on July 21, 2011, at 5:57 AM EDT (09:57 UTC). 

NASA announced it would transfer orbiters to education institutions or museums at the conclusion of the Space Shuttle program. Each museum or institution is responsible for covering the cost of preparing and transporting each vehicle for display. Twenty museums from across the country submitted proposals for receiving one of the retired orbiters. NASA also made Space Shuttle thermal protection system tiles available to schools and universities for less than US$25 each. About 7,000 tiles were available on a first-come, first-served basis, limited to one per institution.

On April 12, 2011, NASA announced selection of locations for the remaining Shuttle orbiters:


In August 2011, the NASA Office of Inspector General (OIG) published a "Review of NASA's Selection of Display Locations for the Space Shuttle Orbiters"; the review had four main findings: 
The NASA OIG had three recommendations, saying NASA should:

In September 2011, the CEO and two board members of Seattle's Museum of Flight met with NASA Administrator Charles Bolden, pointing out "significant errors in deciding where to put its four retiring Space Shuttles"; the errors alleged include inaccurate information on Museum of Flight's attendance and international visitor statistics, as well as the readiness of the Intrepid Sea-Air-Space Museum's exhibit site.


Flight and mid-deck training hardware will be taken from the Johnson Space Center and will go to the National Air and Space Museum and the National Museum of the U.S. Air Force. The full fuselage mockup, which includes the payload bay and aft section but no wings, is to go to the Museum of Flight in Seattle. Mission Simulation and Training Facility's fixed simulator will go to the Adler Planetarium in Chicago, and the motion simulator will go to the Texas A&M Aerospace Engineering Department in College Station, Texas. Other simulators used in Shuttle astronaut training will go to the Wings of Dreams Aviation Museum in Starke, Florida and the Virginia Air and Space Center in Hampton, Virginia.

Until another U.S. manned spacecraft is ready, crews will travel to and from the International Space Station (ISS) exclusively aboard the Russian Soyuz spacecraft.

A planned successor to STS was the "Shuttle II", during the 1980s and 1990s, and later the Constellation program during the 2004–2010 period. CSTS was a proposal to continue to operate STS commercially, after NASA. In September 2011, NASA announced the selection of the design for the new Space Launch System that is planned to launch the Orion spacecraft and other hardware to missions beyond low earth-orbit.

The Commercial Orbital Transportation Services program began in 2006 with the purpose of creating commercially operated unmanned cargo vehicles to service the ISS. The Commercial Crew Development (CCDev) program was started in 2010 to create commercially operated manned spacecraft capable of delivering at least four crew members to the ISS, to stay docked for 180 days, and then return them back to Earth. These spacecraft were to become operational in the 2010s.

Space Shuttles have been features of fiction and nonfiction, from children's movies to documentaries. Early examples include the 1979 "James Bond" film, "Moonraker", the 1982 Activision videogame "Space Shuttle: A Journey into Space" (1982) and G. Harry Stine's 1981 novel "Shuttle Down". In the 1986 film "SpaceCamp", "Atlantis" accidentally launches into space with a group of U.S. Space Camp participants as its crew. A space shuttle named "Intrepid" is featured in the 1989 film "Moontrap".

The 1998 film "Armageddon" portrays a combined crew of offshore oil rig workers and U.S. military staff who pilot two modified Shuttles to avert the destruction of Earth by an asteroid. Retired American test pilots visit a Russian satellite in the 2000 Clint Eastwood adventure film "Space Cowboys". In the 2003 film "The Core", the "Endeavour"s landing is disrupted by the Earth's magnetic core, and its crew is selected to pilot a vehicle designed to restart the core. The 2004 Bollywood movie "Swades", where a Space Shuttle is used to launch a special rainfall monitoring satellite, was filmed at Kennedy Space Center in the year after the "Columbia" disaster that had taken the life of Indian-American astronaut KC Chawla.

On television, the 1996 drama "The Cape" portrays the lives of a group of NASA astronauts as they prepare for and fly Shuttle missions. "Odyssey 5" was a short-lived sci-fi series that features the crew of a Space Shuttle as the last survivors of a disaster that destroys Earth. The 1997–2007 sci-fi series "Stargate SG-1" has a shuttle rescue written into an episode.

The 2013 film "Gravity" features the fictional Space Shuttle "Explorer" during STS-157, whose crew are killed or left stranded after it is destroyed by a shower of high speed orbital debris. The 2017 Lego film "The Lego Batman Movie" features a hybrid between the Batmobile and a Space Shuttle, named "the Bat Space Shuttle" by Dick Grayson. It's clearly based on the Lego City set 3367 ("Space Shuttle"), but is black and weapon-equipped.

The Space Shuttle has also been the subject of toys and models; for example, a large Lego Space Shuttle model was constructed by visitors at Kennedy Space Center, and smaller models have been sold commercially as a standard "LegoLand" set. A 1980 pinball machine "Space Shuttle" was produced by Zaccaria and a 1984 pinball machine "Space Shuttle: Pinball Adventure" was produced by Williams and features a plastic Space Shuttle model among other artwork of astronauts on the play field. The Space Shuttle also appears in a number of flight simulator and space flight simulator games such as "Microsoft Space Simulator", "Orbiter", "FlightGear", "X-Plane" and Space Shuttle Mission 2007. Several Transformers toys were modeled after the Space Shuttle.

The U.S. Postal Service has released several postage issues that depict the Space Shuttle. The first such stamps were issued in 1981, and are on display at the National Postal Museum.







</doc>
<doc id="28191" url="https://en.wikipedia.org/wiki?curid=28191" title="Snow">
Snow

Snow refers to forms of ice crystals that precipitate from the atmosphere (usually from clouds) and undergo changes on the Earth's surface. It pertains to frozen crystalline water throughout its life cycle, starting when, under suitable conditions, the ice crystals form in the atmosphere, increase to millimeter size, precipitate and accumulate on surfaces, then metamorphose in place, and ultimately melt, slide or sublimate away. Snowstorms organize and develop by feeding on sources of atmospheric moisture and cold air. Snowflakes nucleate around particles in the atmosphere by attracting supercooled water droplets, which freeze in hexagonal-shaped crystals. Snowflakes take on a variety of shapes, basic among these are platelets, needles, columns and rime. As snow accumulates into a snowpack, it may blow into drifts. Over time, accumulated snow metamorphoses, by sintering, sublimation and freeze-thaw. Where the climate is cold enough for year-to-year accumulation, a glacier may form. Otherwise, snow typically melts seasonally, causing runoff into streams and rivers and recharging groundwater.

Major snow-prone areas include the polar regions, the upper half of the Northern Hemisphere and mountainous regions worldwide with sufficient moisture and cold temperatures. In the Southern Hemisphere, snow is confined primarily to mountainous areas, apart from Antarctica.

Snow affects such human activities as transportation: creating the need for keeping roadways, wings, and windows clear; agriculture: providing water to crops and safeguarding livestock; sports such as skiing, snowboarding, and snowmachine travel; and warfare. Snow affects ecosystems, as well, by providing an insulating layer during winter under which plants and animals are able to survive the cold.

Snow develops in clouds that themselves are part of a larger weather system. The physics of snow crystal development in clouds results from a complex set of variables that include moisture content and temperatures. The resulting shapes of the falling and fallen crystals can be classified into a number of basic shapes and combinations, thereof. Occasionally, some plate-like, dendritic and stellar-shaped snowflakes can form under clear sky with a very cold temperature inversion present.

Snow clouds usually occur in the context of larger weather systems, the most important of which is the low pressure area, which typically incorporate warm and cold fronts as part of their circulation. Two additional and locally productive sources of snow are lake-effect (also sea-effect) storms and elevation effects, especially in mountains.

Mid-latitude cyclones are low pressure areas which are capable of producing anything from cloudiness and mild snow storms to heavy blizzards. During a hemisphere's fall, winter, and spring, the atmosphere over continents can be cold enough through the depth of the troposphere to cause snowfall. In the Northern Hemisphere, the northern side of the low pressure area produces the most snow. For the southern mid-latitudes, the side of a cyclone that produces the most snow is the southern side.

A cold front, the leading edge of a cooler mass of air, can produce frontal snowsqualls—an intense frontal convective line (similar to a rainband), when temperature is near freezing at the surface. The strong convection that develops has enough moisture to produce whiteout conditions at places which line passes over as the wind causes intense blowing snow. This type of snowsquall generally lasts less than 30 minutes at any point along its path but the motion of the line can cover large distances. Frontal squalls may form a short distance ahead of the surface cold front or behind the cold front where there may be a deepening low pressure system or a series of trough lines which act similar to a traditional cold frontal passage. In situations where squalls develop post-frontally it is not unusual to have two or three linear squall bands pass in rapid succession only separated by 25 miles (40 kilometers) with each passing the same point in roughly 30 minutes apart. In cases where there is a large amount of vertical growth and mixing the squall may develop embedded cumulonimbus clouds resulting in lightning and thunder which is dubbed thundersnow.

A warm front can produce snow for a period, as warm, moist air overrides below-freezing air and creates precipitation at the boundary. Often, snow transitions to rain in the warm sector behind the front.

Lake-effect snow is produced during cooler atmospheric conditions when a cold air mass moves across long expanses of warmer lake water, warming the lower layer of air which picks up water vapor from the lake, rises up through the colder air above, freezes and is deposited on the leeward (downwind) shores.

The same effect also occurs over bodies of salt water, when it is termed "ocean-effect" or "bay-effect snow". The effect is enhanced when the moving air mass is uplifted by the orographic influence of higher elevations on the downwind shores. This uplifting can produce narrow but very intense bands of precipitation, which deposit at a rate of many inches of snow each hour, often resulting in a large amount of total snowfall.

The areas affected by lake-effect snow are called snowbelts. These include areas east of the Great Lakes, the west coasts of northern Japan, the Kamchatka Peninsula in Russia, and areas near the Great Salt Lake, Black Sea, Caspian Sea, Baltic Sea, and parts of the northern Atlantic Ocean.

Orographic or relief snowfall is caused when masses of air pushed by wind are forced up the side of elevated land formations, such as large mountains. The lifting of air up the side of a mountain or range results in adiabatic cooling, and ultimately condensation and precipitation. Moisture is removed by orographic lift, leaving drier, warmer air on the descending, leeward side. The resulting enhanced productivity of snow fall and the decrease in temperature with elevation means that snow depth and seasonal persistence of snowpack increases with elevation in snow-prone areas.

A snowflake consists of roughly 10 water molecules, which are added to its core at different rates and in different patterns, depending on the changing temperature and humidity within the atmosphere that the snowflake falls through on its way to the ground. As a result, snowflakes vary among themselves, while following similar patterns.

Snow crystals form when tiny supercooled cloud droplets (about 10 μm in diameter) freeze. These droplets are able to remain liquid at temperatures lower than , because to freeze, a few molecules in the droplet need to get together by chance to form an arrangement similar to that in an ice lattice. Then the droplet freezes around this "nucleus". In warmer clouds an aerosol particle or "ice nucleus" must be present in (or in contact with) the droplet to act as a nucleus. Ice nuclei are very rare compared to that cloud condensation nuclei on which liquid droplets form. Clays, desert dust and biological particles can be nuclei. Artificial nuclei include particles of silver iodide and dry ice, and these are used to stimulate precipitation in cloud seeding.

Once a droplet has frozen, it grows in the supersaturated environment—one where air is saturated with respect to ice when the temperature is below the freezing point. The droplet then grows by diffusion of water molecules in the air (vapor) onto the ice crystal surface where they are collected. Because water droplets are so much more numerous than the ice crystals due to their sheer abundance, the crystals are able to grow to hundreds of micrometers or millimeters in size at the expense of the water droplets by the Wegener–Bergeron–Findeisen process. The corresponding depletion of water vapor causes the ice crystals to grow at the droplets' expense. These large crystals are an efficient source of precipitation, since they fall through the atmosphere due to their mass, and may collide and stick together in clusters, or aggregates. These aggregates are snowflakes, and are usually the type of ice particle that falls to the ground. Although the ice is clear, scattering of light by the crystal facets and hollows/imperfections mean that the crystals often appear white in color due to diffuse reflection of the whole spectrum of light by the small ice particles.

Micrography of thousands of snowflakes from 1885 onward, starting with Wilson Alwyn Bentley, revealed the wide diversity of snowflakes within a classifiable set of patterns. Closely matching snow crystals have been observed.

Nakaya developed a crystal morphology diagram, relating crystal shapes to the temperature and moisture conditions under which they formed, which is summarized in the following table.

As Nakaya discovered, shape is also a function of whether the prevalent moisture is above or below saturation. Forms below the saturation line trend more towards solid and compact. Crystals formed in supersaturated air trend more towards lacy, delicate and ornate. Many more complex growth patterns also form such as side-planes, bullet-rosettes and also planar types depending on the conditions and ice nuclei. If a crystal has started forming in a column growth regime, at around , and then falls into the warmer plate-like regime, then plate or dendritic crystals sprout at the end of the column, producing so called "capped columns".

Magono and Lee devised a classification of freshly formed snow crystals that includes 80 distinct shapes. They documented each with micrographs.

Snow accumulates from a series of snow events, punctuated by freezing and thawing, over areas that are cold enough to retain snow seasonally or perennially. Major snow-prone areas include the Arctic and Antarctic, the Northern Hemisphere, and alpine regions. The liquid equivalent of snowfall may be evaluated using a snow gauge or with a standard rain gauge, adjusted for winter by removal of a funnel and inner cylinder. Both types of gauges melt the accumulated snow and report the amount of water collected. At some automatic weather stations an ultrasonic snow depth sensor may be used to augment the precipitation gauge.

Snow flurry, snow storm and blizzard describe snow events of progressively greater duration and intensity. A blizzard is a weather condition involving snow and has varying definitions in different parts of the world. In the United States, a blizzard occurs when two conditions are met for a period of three hours or more: A sustained wind or frequent gusts to , and sufficient snow in the air to reduce visibility to less than . In Canada and the United Kingdom, the criteria are similar. While heavy snowfall often occurs during blizzard conditions, falling snow is not a requirement, as blowing snow can create a ground blizzard.

Snowstorm intensity may be categorized by visibility and depth of accumulation. Snowfall's intensity is determined by visibility, as follows:

The "International Classification for Seasonal Snow on the Ground" defines "height of new snow" as the depth of freshly fallen snow, in centimeters as measured with a ruler, that accumulated on a snowboard during an observation period of 24 hours, or other observation interval. After the measurement, the snow is cleared from the board and the board is placed flush with the snow surface to provide an accurate measurement at the end of the next interval. Melting, compacting, blowing and drifting contribute to the difficulty of measuring snowfall.

Glaciers with their permanent snowpacks cover about 10% of the earth's surface, while seasonal snow covers about nine percent, mostly in the Northern Hemisphere, where seasonal snow covers about , according to a 1987 estimate. A 2007 estimate of snow cover over the Northern Hemisphere suggested that, on average, snow cover ranges from a minimum extent of each August to a maximum extent of each January or nearly half of the land surface in that hemisphere. A study of Northern Hemisphere snow cover extent for the period 1972–2006 suggests a reduction of over the 35-year period.

The following are world records regarding snowfall and snowflakes:

After deposition, snow progresses on one of two paths that determine its fate, either "ablation" (mostly by melting) or transitioning from firn (multi-year snow) into "glacier ice". During this transition, snow "is a highly porous, sintered material made up of a continuous ice structure and a continuously connected pore space, forming together the snow microstructure". Almost always near its melting temperature, a snowpack is continually transforming these properties in a process, known as "metamorphism", wherein all three phases of water may coexist, including liquid water partially filling the pore space. Starting as a powdery deposition, snow becomes more granular when it begins to compact under its own weight, be blown by the wind, sinter particles together and commence the cycle of melting and refreezing. Water vapor plays a role as it deposits ice crystals, known as hoar frost, during cold, still conditions.

Over the course of time, a snowpack may settle under its own weight until its density is approximately 30% of water. Increases in density above this initial compression occur primarily by melting and refreezing, caused by temperatures above freezing or by direct solar radiation. In colder climates, snow lies on the ground all winter. By late spring, snow densities typically reach a maximum of 50% of water. Snow that persists into summer evolves into névé, granular snow, which has been partially melted, refrozen and compacted. Névé has a minimum density of , which is roughly half of the density of liquid water.

Firn is snow that has persisted for multiple years and has been recrystallized into a substance denser than névé, yet less dense and hard than glacial ice. Firn resembles caked sugar and is very resistant to shovelling. Its density generally ranges from to , and it can often be found underneath the snow that accumulates at the head of a glacier. The minimum altitude that firn accumulates on a glacier is called the "firn limit", "firn line" or "snowline".

There are four main mechanisms for movement of deposited snow: "drifting" of unsintered snow, "avalanches" of accumulated snow on steep slopes, "snowmelt" during thaw conditions, and the "movement of glaciers" after snow has persisted for multiple years and metamorphosed into glacier ice.

When powdery, snow drifts with the wind from the location where it originally fell, forming deposits with a depth of several meters in isolated locations. After attaching to hillsides, blown snow can evolve into a snow slab, which is an avalanche hazard on steep slopes.

An avalanche (also called a snowslide or snowslip) is a rapid flow of snow down a sloping surface. Avalanches are typically triggered in a starting zone from a mechanical failure in the snowpack (slab avalanche) when the forces on the snow exceed its strength but sometimes only with gradually widening (loose snow avalanche). After initiation, avalanches usually accelerate rapidly and grow in mass and volume as they entrain more snow. If the avalanche moves fast enough some of the snow may mix with the air forming a powder snow avalanche, which is a type of gravity current. They occur in three major mechanisms:

Many rivers originating in mountainous or high-latitude regions receive a significant portion of their flow from snowmelt. This often makes the river's flow highly seasonal resulting in periodic flooding during the spring months and at least in dry mountainous regions like the mountain West of the US or most of Iran and Afghanistan, very low flow for the rest of the year. In contrast, if much of the melt is from glaciated or nearly glaciated areas, the melt continues through the warm season, with peak flows occurring in mid to late summer.

Glaciers form where the accumulation of snow and ice exceeds ablation. The area in which an alpine glacier forms is called a cirque (corrie or cwm), a typically armchair-shaped geological feature, which collects snow and where the snowpack compacts under the weight of successive layers of accumulating snow, forming névé. Further crushing of the individual snow crystals and reduction of entrapped air in the snow turns it into 'glacial ice'. This glacial ice will fill the cirque until it 'overflows' through a geological weakness or vacancy, such as the gap between two mountains. When the mass of snow and ice is sufficiently thick, it begins to move due to a combination of surface slope, gravity and pressure. On steeper slopes, this can occur with as little as 15 m (50 ft) of snow-ice.

Scientists study snow at a wide variety of scales that include the physics of chemical bonds and clouds; the distribution, accumulation, metamorphosis, and ablation of snowpacks; and the contribution of snowmelt to river hydraulics and ground hydrology. In doing so, they employ a variety of instruments to observe and measure the phenomena studied. Their findings contribute to knowledge applied by engineers, who adapt vehicles and structures to snow, by agronomists, who address the availability of snowmelt to agriculture, and those, who design equipment for sporting activities on snow. Scientists develop and others employ snow classification systems that describe its physical properties at scales ranging from the individual crystal to the aggregated snowpack. A sub-specialty is avalanches, which are of concern to engineers and outdoors sports people, alike.

Snow science addresses how snow forms, its distribution, and processes affecting how snowpacks change over time. Scientists improve storm forecasting, study global snow cover and its effect on climate, glaciers, and water supplies around the world. The study includes physical properties of the material as it changes, bulk properties of in-place snow packs, and the aggregate properties of regions with snow cover. In doing so, they employ on-the-ground physical measurement techniques to establish ground truth and remote sensing techniques to develop understanding of snow-related processes over large areas.

In the field snow scientists often excavate a snow pit within which to make basic measurements and observations. Observations can describe features caused by wind, water percolation, or snow unloading from trees.Water percolation into a snowpack can create flow fingers and ponding or flow along capillary barriers, which can refreeze into horizontal and vertical solid ice formations within the snowpack. Among the measurements of the properties of snowpacks that the "International Classification for Seasonal Snow on the Ground" includes are: snow height, snow water equivalent, snow strength, and extent of snow cover. Each has a designation with code and detailed description. The classification extends the prior classifications of Nakaya and his successors to related types of precipitation and are quoted in the following table:
"All are formed in cloud, except for rime, which forms on objects exposed to supercooled moisture."

It also has a more extensive classification of deposited snow than those that pertain to airborne snow. The categories include both natural and man-made snow types, descriptions of snow crystals as they metamorphose and melt, the development of hoar frost in the snow pack and the formation of ice therein. Each such layer of a snowpack differs from the adjacent layers by one or more characteristics that describe its microstructure or density, which together define the snow type, and other physical properties. Thus, at any one time, the type and state of the snow forming a layer have to be defined because its physical and mechanical properties depend on them. Physical properties include microstructure, grain size and shape, snow density, liquid water content, and temperature.

Remote sensing of snowpacks with satellites and other platforms typically includes multi-spectral collection of imagery. Multi-faceted interpretation of the data obtained allows inferences about what is observed. The science behind these remote observations has been verified with ground-truth studies of the actual conditions.

Satellite observations record a decrease in snow-covered areas since the 1960s, when satellites when satellite observations began. In some areas, including China, snow cover has increased. In some regions such as China, a trend of increasing snow cover has been observed from 1978 to 2006. These changes are attributed to global climate change, which may lead to earlier melting and less aea coverage. However, in some areas there may be an increase in snow depth because of higher temperatures for latitudes north of 40°. For the Northern Hemisphere as a whole the mean monthly snow-cover extent has been decreasing by 1.3% per decade.

The most frequently used methods to map and measure snow extent, snow depth and snow water equivalent employ multiple inputs on the visible–infrared spectrum to deduce the presence and properties of snow. The National Snow and Ice Data Center (NSIDC) uses the reflectance of visible and infrared radiation to calculate a normalized difference snow index, which is a ratio of radiation parameters that can distinguish between clouds and snow. Other researchers have developed decision trees, employing the available data to make more accurate assessments. One challenge to this assessment is where snow cover is patchy, for example during periods of accumulation or ablation and also in forested areas. Cloud cover inhibits optical sensing of surface reflectance, which has led to other methods for estimating ground conditions underneath clouds. For hydrological models, it is important to have continuous information about the snow cover. Passive microwave sensors are especially valuable for temporal and spatial continuity because they can map the surface beneath clouds and in darkness. When combined with reflective measurements, passive microwave sensing greatly extends the inferences possible about the snowpack.

Snow science often leads to predictive models that include snow deposition, snow melt, and snow hydrology—elements of the Earth's water cycle—which help describe global climate change.

Global climate change models (GCMs) incorporate snow as a factor in their calculations. Some important aspects of snow cover include its albedo (reflectivity of incident radiation, including light) and insulating qualities, which slow the rate of seasonal melting of sea ice. As of 2011, the melt phase of GCM snow models were thought to perform poorly in regions with complex factors that regulate snow melt, such as vegetation cover and terrain. These models typically derive snow water equivalent (SWE) in some manner from satellite observations of snow cover. The "International Classification for Seasonal Snow on the Ground" defines SWE as "the depth of water that would result if the mass of snow melted completely".

Given the importance of snowmelt to agriculture, hydrological runoff models that include snow in their predictions address the phases of accumulating snowpack, melting processes, and distribution of the meltwater through stream networks and into the groundwater. Key to describing the melting processes are solar heat flux, ambient temperature, wind, and precipitation. Initial snowmelt models used a degree-day approach that emphasized the temperature difference between the air and the snowpack to compute snow water equivalent, SWE. More recent models use an energy balance approach that take into account the following factors to compute "Q", the energy available for melt. This requires measurement of an array of snowpack and environmental factors to compute six heat flow mechanisms that contribute to "Q".

Snow affects human activity in four major areas, transportation, agriculture, structures, and sports. Most transportation modes are impeded by snow on the travel surface. Agriculture often relies on snow as a source of seasonal moisture. Structures may fail under snow loads. Humans find a wide variety of recreational activities in snowy landscapes.

Snow affects the rights of way of highways, airfields and railroads. They share a common tool for clearing snow, the snowplow. However, the application is different in each case—whereas roadways employ anti-icing chemicals to prevent bonding of ice, airfields may not; railroads rely on abrasives to enhance traction on tracks.

In the late 20th century, an estimated $2 billion was spent annually in North America on roadway winter maintenance, owing to snow and other winter weather events, according to a 1994 report by Kuemmel. The study surveyed the practices of jurisdictions within 44 US states and nine Canadian provinces. It assessed the policies, practices, and equipment used for winter maintenance. It found similar practices and progress to be prevalent in Europe.

The dominant effect of snow on vehicle contact with the road is diminished friction. This can be improved with the use of snow tires, which have a tread designed to compact snow in a manner that enhances traction. However, the key to maintaining a roadway that can accommodate traffic during and after a snow event is an effective anti-icing program that employs both chemicals and plowing. The FHWA "Manual of Practice for an Effective Anti-icing Program" emphasizes "anti-icing" procedures that prevent the bonding of snow and ice to the road. Key aspects of the practice include: understanding anti-icing in light of the level of service to be achieved on a given roadway, the climatic conditions to be encountered, and the different roles of deicing, anti-icing, and abrasive materials and applications, and employing anti-icing "toolboxes", one for operations, one for decision-making and another for personnel. The elements to the toolboxes are:
The manual offers matrices that address different types of snow and the rate of snowfall to tailor applications appropriately and efficiently.

Snow fences, constructed upwind of roadways control snow drifting by causing windblown, drifting snow to accumulate in a desired place. They are also used on railways. Additionally, farmers and ranchers use snow fences to create drifts in basins for a ready supply of water in the spring.

In order to keep airports open during winter storms, runways and taxiways require snow removal. Unlike roadways, where chloride chemical treatment is common to prevent snow from bonding to the pavement surface, such chemicals are typically banned from airports because of their strong corrosive effect on aluminum aircraft. Consequently, mechanical brushes are often used to complement the action of snow plows. Given the width of runways on airfields that handle large aircraft, vehicles with large plow blades, an echelon of plow vehicles or rotary snowplows are used to clear snow on runways and taxiways. Terminal aprons may require or more to be cleared.

Properly equipped aircraft are able to fly through snowstorms under Instrument flight rules. Prior to takeoff, during snowstorms they require deicing fluid to prevent accumulation and freezing of snow and other precipitation on wings and fuselages, which may compromise the safety of the aircraft and its occupants. In flight, aircraft rely on a variety of mechanisms to avoid rime and other types of icing in clouds, these include pulsing pneumatic boots, electro-thermal areas that generate heat, and fluid deicers that bleed onto the surface.

Railroads have traditionally employed two types of snow plows for clearing track, the wedge plow, which casts snow to both sides, and the rotary snowplow, which is suited for addressing heavy snowfall and casting snow far to one side or the other. Prior to the invention of the rotary snowplow ca. 1865, it required multiple locomotives to drive a wedge plow through deep snow. Subsequent to clearing the track with such plows, a "flanger" is used to clear snow from between the rails that are below the reach of the other types of plow. Where icing may affect the steel-to-steel contact of locomotive wheels on track, abrasives (typically sand) have been used to provide traction on steeper uphills.

Railroads employ snow sheds—structures that cover the track—to prevent the accumulation of heavy snow or avalanches to cover tracks in snowy mountainous areas, such as the Alps and the Rocky Mountains.


Snow can be compacted to form a snow road and be part of a winter road route for vehicles to access isolated communities or construction projects during the winter. Snow can also be used to provide the supporting structure and surface for a runway, as with the Phoenix Airfield in Antarctica. The snow-compacted runway is designed to withstand approximately 60 wheeled flights of heavy-lift military aircraft a year.

Snowfall can be beneficial to agriculture by serving as a thermal insulator, conserving the heat of the Earth and protecting crops from subfreezing weather. Some agricultural areas depend on an accumulation of snow during winter that will melt gradually in spring, providing water for crop growth, both directly and via runoff through streams and rivers, which supply irrigation canals. The following are examples of rivers that rely on meltwater from glaciers or seasonal snowpack as an important part of their flow on which irrigation depends: the Ganges, many of whose tributaries rise in the Himalayas and which provide much irrigation in northeast India, the Indus River, which rises in Tibet and provides irrigation water to Pakistan from rapidly retreating Tibetan glaciers, and the Colorado River, which receives much of its water from seasonal snowpack in the Rocky Mountains and provides irrigation water to some 4 million acres (1.6 million hectares).

Snow is an important consideration for loads on structures. To address these, European countries employ "Eurocode 1: Actions on structures - Part 1-3: General actions - Snow loads". In North America, ASCE "Minimum Design Loads for Buildings and Other Structures" gives guidance on snow loads. Both standards employ methods that translate maximum expected ground snow loads onto design loads for roofs.

Snow loads and icings are two principal issues for roofs. Snow loads are related to the climate in which a structure is sited. Icings are usually a result of the building or structure generating heat that melts the snow that is on it.

"Snow loads" – The "Minimum Design Loads for Buildings and Other Structures" gives guidance on how to translate the following factors into roof snow loads:
It gives tables for ground snow loads by region and a methodology for computing ground snow loads that may vary with elevation from nearby, measured values. The "Eurocode 1" uses similar methodologies, starting with ground snow loads that are tabulated for portions of Europe.

"Icings" – Roofs must also be designed to avoid ice dams, which result from meltwater running under the snow on the roof and freezing at the eave. Ice dams on roofs form when accumulated snow on a sloping roof melts and flows down the roof, under the insulating blanket of snow, until it reaches below freezing temperature air, typically at the eaves. When the meltwater reaches the freezing air, ice accumulates, forming a dam, and snow that melts later cannot drain properly through the dam. Ice dams may result in damaged building materials or in damage or injury when the ice dam falls off or from attempts to remove ice dams. The melting results from heat passing through the roof under the highly insulating layer of snow.

In areas with trees, utility distribution lines on poles are less susceptible to snow loads than they are subject to damage from trees falling on them, felled by heavy, wet snow. Elsewhere, snow can accrete on power lines as "sleeves" of rime ice. Engineers design for such loads, which are measured in kg/m (lb/ft) and power companies have forecasting systems that anticipate types of weather that may cause such accretions. Rime ice may be removed manually or by creating a sufficient short circuit in the affected segment of power lines to melt the accretions.

Snow figures into many winter sports and forms of recreation, including skiing and sledding. Common examples include cross-country skiing, Alpine skiing, snowboarding, snowshoeing, and snowmobiling. The design of the equipment used, typically relies on the bearing strength of snow, as with skis or snowboards and contends with the coefficient of friction of snow to allow sliding, often enhance by ski waxes.

Skiing is by far the largest form of winter recreation. As of 1994, of the estimated 65–75 million skiers worldwide, there were approximately 55 million who engaged in Alpine skiing, the rest engaged in cross-country skiing. Approximately 30 million skiers (of all kinds) were in Europe, 15 million in the US, and 14 million in Japan. As of 1996, there were reportedly 4,500 ski areas, operating 26,000 ski lifts and enjoying 390 million skier visits per year. The preponderant region for downhill skiing was Europe, followed by Japan and the US.

Increasingly, ski resorts are relying on snowmaking, the production of snow by forcing water and pressurized air through a snow gun on ski slopes. Snowmaking is mainly used to supplement natural snow at ski resorts. This allows them to improve the reliability of their snow cover and to extend their ski seasons from late autumn to early spring. The production of snow requires low temperatures. The threshold temperature for snowmaking increases as humidity decreases. Wet-bulb temperature is used as a metric since it takes air temperature and relative humidity into account. Snowmaking is a relatively expensive process in its energy consumption, thereby limiting its use.

Ski wax enhances the ability of a ski or other runner to slide over snow, which depends on both the properties of the snow and the ski to result in an optimum amount of lubrication from melting the snow by friction with the ski—too little and the ski interacts with solid snow crystals, too much and capillary attraction of meltwater retards the ski. Before a ski can slide, it must overcome the maximum value static friction. Kinetic (or dynamic) friction occurs when the ski is moving over the snow.

Snow affects warfare conducted in winter, alpine environments or at high latitudes. The main factors are "impaired visibility" for acquiring targets during falling snow, "enhanced visibility" of targets against snowy backgrounds for targeting, and mobility for both mechanized and infantry troops. Snowfall can severely inhibit the logistics of supplying troops, as well. Snow can also provide cover and fortification against small-arms fire. Noted winter warfare campaigns where snow and other factors affected the operations include:


Both plant and animal life endemic to snow-bound areas develop ways to adapt. Among the adaptive mechanisms for plants are dormancy, seasonal dieback, survival of seeds; and for animals are hibernation, insulation, anti-freeze chemistry, storing food, drawing on reserves from within the body, and clustering for mutual heat.

Snow interacts with vegetation in two principal ways, vegetation can influence the deposition and retention of snow and, conversely, the presence of snow can affect the distribution and growth of vegetation. Tree branches, especially of conifers intercept falling snow and prevent accumulation on the ground. Snow suspended in trees ablates more rapidly than that on the ground, owing to its greater exposure to sun and air movement. Trees and other plants can also promote snow retention on the ground, which would otherwise be blown elsewhere or melted by the sun. Snow affects vegetation in several ways, the presence of stored water can promote growth, yet the annual onset of growth is dependent on the departure of the snowpack for those plants that are buried beneath it. Furthermore, avalanches and erosion from snowmelt can scour terrain of vegetation.

Snow supports a wide variety of animals both on the surface and beneath. Many invertebrates thrive in snow, including spiders, wasps, beetles, snow scorpionflys and springtails. Such arthropods are typically active at temperatures down to . Invertebrates fall into two groups, regarding surviving subfreezing temperatures: freezing resistant and those that avoid freezing because they are freeze-sensitive. The first group may be cold hardy owing to the ability to produce antifreeze agents in their body fluids that allows survival of long exposure to sub-freezing conditions. Some organisms fast during the winter, which expels freezing-sensitive contents from their digestive tracts. The ability to survive the absence of oxygen in ice is an additional survival mechanism.

Small vertebrates are active beneath the snow. Among vertebrates, alpine salamanders are active in snow at temperatures as low as ; they burrow to the surface in springtime and lay their eggs in melt ponds. Among mammals, those that remain active are typically smaller than . Omnivores are more likely to enter a torpor or be hibernators, whereas herbivores are more likely to maintain food caches beneath the snow. Voles store up to of food and pikas up to . Voles also huddle in communal nests to benefit from one another's warmth. On the surface, wolves, coyotes, foxes, lynx, and weasels rely on these subsurface dwellers for food and often dive into the snowpack to find them.

Extraterrestrial "snow" includes water-based precipitation, but also precipitation of other compounds prevalent on other planets and moons in the Solar System. Examples are:

Lexicon
Notable snow events

Recreation

Related concepts

Science and scientists
Snow structures



</doc>
<doc id="28195" url="https://en.wikipedia.org/wiki?curid=28195" title="Symbolics">
Symbolics

Symbolics refers to two companies: now-defunct computer manufacturer Symbolics, Inc., and a privately held company that acquired the assets of the former company and continues to sell and maintain the Open Genera Lisp system and the Macsyma computer algebra system.

The symbolics.com domain was originally registered on March 15, 1985, making it the first .com-domain in the world. In August 2009, it was sold to napkin.com (formerly XF.com) Investments.

Symbolics, Inc. was a computer manufacturer headquartered in Cambridge, Massachusetts, and later in Concord, Massachusetts, with manufacturing facilities in Chatsworth, California (a suburban section of Los Angeles). Its first CEO, chairman, and founder was Russell Noftsker. Symbolics designed and manufactured a line of Lisp machines, single-user computers optimized to run the Lisp programming language. Symbolics also made significant advances in software technology, and offered one of the premier software development environments of the 1980s and 1990s, now sold commercially as Open Genera for Tru64 UNIX on the HP Alpha. The Lisp Machine was the first commercially available "workstation" (although that word had not yet been coined).

Symbolics was a spinoff from the MIT AI Lab, one of two companies to be founded by AI Lab staffers and associated hackers for the purpose of manufacturing Lisp machines. The other was Lisp Machines, Inc., although Symbolics attracted most of the hackers, and more funding.

Symbolics' initial product, the LM-2 (introduced in 1981), was a repackaged version of the MIT CADR Lisp machine design. The operating system and software development environment, over 500,000 lines, was written in Lisp from the microcode up, based on MIT's Lisp Machine Lisp.

The software bundle was later renamed ZetaLisp, to distinguish the Symbolics' product from other vendors who had also licensed the MIT software. Symbolics' Zmacs text editor, a variant of Emacs, was implemented in a text-processing package named "ZWEI", an acronym for "Zwei was Eine initially", with "Eine" being an acronym for "Eine Is Not Emacs". Both are recursive acronyms and puns on the German words for "One" ("Eins", "Eine") and "Two" ("Zwei").

The Lisp Machine system software was then copyrighted by MIT, and was licensed to both Symbolics and LMI. Until 1981, Symbolics shared all its copyrighted enhancements to the source code with MIT and kept it on an MIT server. According to Richard Stallman, Symbolics engaged in a business tactic in which it forced MIT to make all Symbolics' copyrighted fixes and improvements to the Lisp Machine OS available only to Symbolics (and MIT but not to Symbolics competitors), and thereby choke off its competitor LMI, which at that time had insufficient resources to independently maintain or develop the OS and environment.

Symbolics felt that they no longer had sufficient control over their product. At that point, Symbolics began using their own copy of the software, located on their company servers — while Stallman says that Symbolics did that to prevent its Lisp improvements from flowing to Lisp Machines, Inc. From that base, Symbolics made extensive improvements to every part of the software, and continued to deliver almost all the source code to their customers (including MIT). However, the policy prohibited MIT staff from distributing the Symbolics version of the software to others. With the end of open collaboration came the end of the MIT hacker community. As a reaction to this, Stallman initiated the GNU project to make a new community. Eventually, Copyleft and the GNU General Public License would ensure that a hacker's software could remain free software. In this way Symbolics played a key, albeit adversarial, role in instigating the free software movement.

In 1983, a year later than planned, Symbolics introduced the 3600 family of Lisp machines. Code-named the "L-machine" internally, the 3600 family was an innovative new design, inspired by the CADR architecture but sharing few of its implementation details. The main processor had a 36 bit word (divided up as 4 or 8 bits of tags, and 32 bits of data or 28 bits of memory address). Memory words were 44 bits, the additional 8 bits being used for error-correcting code (ECC). The instruction set was that of a stack machine. The 3600 architecture provided 4,096 hardware registers, of which half were used as a cache for the top of the control stack; the rest were used by the microcode and time-critical routines of the operating system and Lisp run-time environment. Hardware support was provided for virtual memory, which was common for machines in its class, and for garbage collection, which was unique.

The original 3600 processor was a microprogrammed design like the CADR, and was built on several large circuit boards from standard TTL integrated circuits, both features being common for commercial computers in its class at the time. CPU clock speed varied depending on the particular instruction being executed, but was typically around 5 MHz. Many Lisp primitives could be executed in a single clock cycle. Disk I/O was handled by multitasking at the microcode level. A 68000 processor (known as the "Front-End Processor", or FEP) started the main computer up, and handled the slower peripherals during normal operation. An Ethernet interface was standard equipment, replacing the Chaosnet interface of the LM-2.

The 3600 was roughly the size of a household refrigerator. This was partly due to the size of the processor — the cards were widely spaced to allow wire-wrap prototype cards to fit without interference — and partly due to the limitations of the disk drive technology in the early 1980s. At the 3600's introduction, the smallest disk that could support the ZetaLisp software was 14 inches (356 mm) across (most 3600s shipped with the 10½-inch Fujitsu Eagle). The 3670 and 3675 were slightly shorter in height, but were essentially the same machine packed a little tighter. The advent of , and later , disk drives that could hold hundreds of megabytes led to the introduction of the 3640 and 3645, which were roughly the size of a two-drawer file cabinet.

Later versions of the 3600 architecture were implemented on custom integrated circuits, reducing the five cards of the original processor design to two, at a large manufacturing cost savings and with performance slightly better than the old design. The 3650, first of the "G machines", as they were known within the company, was housed in a cabinet derived from the 3640s. Denser memory and smaller disk drives enabled the introduction of the 3620, about the size of a modern full-size tower PC. The 3630 was a "fat 3620" with room for more memory and video interface cards. The 3610 was a lower priced variant of the 3620, essentially identical in every way except that it was licensed for application deployment rather than general development.

The various models of the 3600 family were popular for AI research and commercial applications throughout the 1980s. The AI commercialization boom of the 1980s led directly to Symbolics' success during the decade. Symbolics computers were widely believed to be the best platform available for developing AI software. The LM-2 used a Symbolics-branded version of the complex space-cadet keyboard, while later models used a simplified version (at right), known simply as the . The Symbolics keyboard featured the many modifier keys used in Zmacs, notably Control/Meta/Super/Hyper in a block, but did not feature the complex symbol set of the space-cadet keyboard.

Also contributing to the 3600 series' success was a line of bit-mapped graphics color video interfaces, combined with extremely powerful animation software. Symbolics' Graphics Division, headquartered in Westwood, California, a stone's throw from the major Hollywood movie and television studios, made its S-Render and S-Paint software into industry leaders in the animation business.

Symbolics developed the first workstations capable of processing HDTV quality video, which enjoyed a popular following in Japan. A 3600 — with the standard black-and-white monitor — made a cameo appearance in the movie Real Genius. The company was also referenced in Michael Crichton's novel "Jurassic Park".

Symbolics' Graphics Division was sold to Nichimen Trading Company in the early 1990s, and the S-Graphics software suite (S-Paint, S-Geometry, S-Dynamics, S-Render) ported to Franz Allegro Common Lisp on SGI and PC computers running Windows NT. Today it is sold as Mirai by Izware LLC, and continues to be used in major motion pictures (most famously in New Line Cinema's "The Lord of the Rings"), video games, and military simulations.
Symbolic's 3600-series computers were also used as the first front end "controller" computers for the Connection Machine massively parallel computers manufactured by Thinking Machines Inc., another MIT spinoff based in Cambridge, Massachusetts. The Connection Machine ran a parallel variant of Lisp and, initially, was used primarily by the AI community, so the Symbolics Lisp machine was a particularly good fit as a front-end machine.

For a long time, the operating system didn't have a name, but was finally named "Genera" around 1984. The system included a number of advanced dialects of Lisp. Its heritage was MACLISP on the PDP-10, but it included more data types, and multiple-inheritance object-oriented programming features. This Lisp dialect was called Lisp Machine Lisp at MIT. Symbolics used the name ZetaLisp. Symbolics later wrote new software in "Symbolics Common Lisp", its version of the Common Lisp standard.

In the late 1980s (2 years later than planned), the Ivory family of single-chip Lisp Machine processors superseded the G-Machine 3650, 3620, and 3630 systems. The Ivory 390k transistor VLSI implementation designed in Symbolics Common Lisp using NS, a custom Symbolics Hardware Design Language (HDL), addressed a 40-bit word (8 bits tag, 32 bits data/address). Since it only addressed full words and not bytes or half-words, this allowed addressing of 4 Gigawords (GW) or 16 gigabytes (GB) of memory; the increase in address space reflected the growth of programs and data as semiconductor memory and disk space became cheaper. The Ivory processor had 8 bits of ECC attached to each word, so each word fetched from external memory to the chip was actually 48 bits wide. Each Ivory instruction was 18 bits wide and two instructions plus a 2-bit CDR code and 2-bit Data Type were in each instruction word fetched from memory. Fetching two instruction words at a time from memory enhanced the Ivory's performance. Unlike the 3600's microprogrammed architecture, the Ivory instruction set was still microcoded, but was stored in a 1200 x 180-bit ROM inside the Ivory chip. The initial Ivory processors were fabricated by VLSI Technology Inc in San Jose, California, on a 2 µm CMOS process, with later generations fabricated by Hewlett Packard in Corvallis, Oregon, on 1.25 µm and 1 µm CMOS processes. The Ivory had a stack architecture and operated a 4-stage pipeline: Fetch, Decode, Execute and Write Back. Ivory processors were marketed in stand-alone Lisp Machines (the XL400, XL1200, and XL1201), headless Lisp Machines (NXP1000), and on add-in cards for Sun Microsystems (UX400, UX1200) and Apple Macintosh (MacIvory I, II, III) computers. The Lisp Machines with Ivory processors operated at speeds that were between two and six times faster than a 3600 depending on the model and the revision of the Ivory chip.

The Ivory instruction set was later emulated in software for microprocessors implementing the 64-bit Alpha architecture. The "Virtual Lisp Machine" emulator, combined with the operating system and software development environment from the XL machines, is sold as Open Genera.

Sunstone was a RISC-like processor that was to be released shortly after the Ivory. It was designed by Ron Lebel's group at the Symbolics Westwood office. However, the project was canceled the day it was supposed to tape out.

As quickly as the commercial AI boom of the mid-1980s had propelled Symbolics to success, the "AI Winter" of the late 1980s and early 1990s, combined with the slow down of Reagan's "Star Wars" missile defense program, for which DARPA had invested heavily in AI solutions, severely damaged Symbolics. An internal war between Noftsker and the CEO the board had hired in 1986, Brian Sear, over whether to follow Sun's suggested lead and focus on selling their software, or to re-emphasize their superior hardware, and the ensuing lack of focus when both Noftsker and Sear were fired from the company caused sales to plummet. This fact, combined with some ill-advised real estate deals by company management during the boom years (they had entered into large long-term lease obligations in California), drove Symbolics into bankruptcy. Rapid evolution in mass-market microprocessor technology (the "PC revolution"), advances in Lisp compiler technology, and the economics of manufacturing custom microprocessors severely diminished the commercial advantages of purpose-built Lisp machines. By 1995, the Lisp machine era had ended, and with it Symbolics' hopes for success.

Symbolics continued as an enterprise with very limited revenues, supported mainly by service contracts on the remaining MacIvory, UX-1200, UX-1201, and other machines still used by commercial customers. Symbolics also sold Virtual Lisp Machine (VLM) software for DEC, Compaq, and HP Alpha-based workstations (AlphaStation) and servers (AlphaServer), refurbished MacIvory IIs, and Symbolics keyboards.

In July 2005, Symbolics closed its Chatsworth, California, maintenance facility. The reclusive owner of the company, Andrew Topping, died that same year. The current legal status of Symbolics software is uncertain. An assortment of Symbolics hardware was still available for purchase as of August 2007. The US DoD is still paying Symbolics for regular maintenance work.

On March 15, 1985, symbolics.com became the first (and currently, since it is still registered, the oldest) registered .com domain of the Internet. The symbolics.com domain was purchased by XF.com in 2009.

Genera also featured the most extensive networking interoperability software seen to that point. A local area network system called Chaosnet had been invented for the Lisp Machine (predating the commercial availability of Ethernet). The Symbolics system supported Chaosnet, but also had one of the first TCP/IP implementations. It also supported DECnet and IBM's SNA network protocols. A Dialnet protocol used phone lines and modems. Genera would, using hints from its distributed "namespace" database (somewhat similar to DNS, but more comprehensive, like parts of Xerox's Grapevine), automatically select the best protocol combination to use when connecting to network service. An application program (or a user command) would only specify the name of the host and the desired service. For example, a host name and a request for "Terminal Connection" might yield a connection over TCP/IP using the Telnet protocol (although there were many other possibilities). Likewise, requesting a file operation (such as a Copy File command) might pick NFS, FTP, NFILE (the Symbolics network file access protocol), or one of several others, and it might execute the request over TCP/IP, Chaosnet, or whatever other network was most suitable.

The most popular application program for the Symbolics Lisp Machine was the ICAD computer-aided engineering system. One of the first networked multi-player video games, a version of Spacewar, was developed for the Symbolics Lisp Machine in 1983. Electronic CAD software on the Symbolics Lisp Machine was used to develop the first implementation of the Hewlett-Packard Precision Architecture.

Symbolics' research and development staff (first at MIT, and then later at the company) produced a number of major innovations in software technology:


The Symbolics Graphics Division (SGD, founded in 1982, sold to Nichimen Graphics in 1992) developed the S-Graphics software suite (S-Paint, S-Geometry, S-Dynamics, S-Render) for Symbolics Genera.

This software was also used to create a few computer animated movies and was used for some popular movies.




</doc>
<doc id="28198" url="https://en.wikipedia.org/wiki?curid=28198" title="Surfing">
Surfing

Surfing is a surface water sport in which the wave rider, referred to as a surfer, rides on the forward or deep face of a moving wave, which is usually carrying the surfer towards the shore. Waves suitable for surfing are primarily found in the ocean, but can also be found in lakes or rivers in the form of a standing wave or tidal bore. However, surfers can also utilize artificial waves such as those from boat wakes and the waves created in artificial wave pools.

The term "surfing" refers to the act of riding a wave, regardless of whether the wave is ridden with a board or without a board, and regardless of the stance used. The native peoples of the Pacific, for instance, surfed waves on alaia, paipo, and other such craft, and did so on their belly and knees. The modern-day definition of surfing, however, most often refers to a surfer riding a wave standing up on a surfboard; this is also referred to as stand-up surfing.

Another prominent form of surfing is body boarding, when a surfer rides a wave on a bodyboard, either lying on their belly, drop knee, or sometimes even standing up on a body board. Other types of surfing include knee boarding, surf matting (riding inflatable mats), and using foils. Body surfing, where the wave is surfed without a board, using the surfer's own body to catch and ride the wave, is very common and is considered by some to be the purest form of surfing.

Three major subdivisions within standing-up surfing are stand-up paddling, long boarding and short boarding with several major differences including the board design and length, the riding style, and the kind of wave that is ridden.

In tow-in surfing (most often, but not exclusively, associated with big wave surfing), a motorized water vehicle, such as a personal watercraft, tows the surfer into the wave front, helping the surfer match a large wave's speed, which is generally a higher speed than a self-propelled surfer can produce. Surfing-related sports such as paddle boarding and sea kayaking do not require waves, and other derivative sports such as kite surfing and windsurfing rely primarily on wind for power, yet all of these platforms may also be used to ride waves. Recently with the use of V-drive boats, Wakesurfing, in which one surfs on the wake of a boat, has emerged. The Guinness Book of World Records recognized a wave ride by Garrett McNamara at Nazaré, Portugal as the largest wave ever surfed.

For hundreds of years, surfing was a central part of ancient Polynesian culture. Surfing may have first been observed by British explorers at Tahiti in 1767. Samuel Wallis and the crew members of who were the first Britons to visit the island in June of that year. Another candidate is the botanist Joseph Banks being part of the first voyage of James Cook on , who arrived on Tahiti on 10 April 1769. Lieutenant James King was the first person to write about the art of surfing on Hawaii when he was completing the journals of Captain James Cook upon Cook's death in 1779.

When Mark Twain visited Hawaii in 1866 he wrote,
In one place we came upon a large company of naked natives, of both sexes and all ages, amusing themselves with the national pastime of surf-bathing.

References to surf riding on planks and single canoe hulls are also verified for pre-contact Samoa, where surfing was called "fa'ase'e" or "se'egalu" (see Augustin Krämer, "The Samoa Islands"), and Tonga, far pre-dating the practice of surfing by Hawaiians and eastern Polynesians by over a thousand years.

In July 1885, three teenage Hawaiian princes took a break from their boarding school, St. Mathew’s Hall in San Mateo, and came to cool off in Santa Cruz, California. There, David Kawānanakoa, Edward Keliʻiahonui and Jonah Kūhiō Kalanianaʻole surfed the mouth of the San Lorenzo River on custom-shaped redwood boards, according to surf historians Kim Stoner and Geoff Dunn.

George Freeth (8 November 1883 – 7 April 1919) is often credited as being the "Father of Modern Surfing". He is thought to have been the first modern surfer.

In 1907, the eclectic interests of the land baron Henry E. Huntington brought the ancient art of surfing to the California coast. While on vacation, Huntington had seen Hawaiian boys surfing the island waves. Looking for a way to entice visitors to the area of Redondo Beach, where he had heavily invested in real estate, he hired a young Hawaiian to ride surfboards. George Freeth decided to revive the art of surfing, but had little success with the huge 16-foot hardwood boards that were popular at that time. When he cut them in half to make them more manageable, he created the original "Long board", which made him the talk of the islands. To the delight of visitors, Freeth exhibited his surfing skills twice a day in front of the Hotel Redondo.

In 1975, professional contests started. That year Margo Oberg became the first female professional surfer.

Swell is generated when wind blows consistently over a large area of open water, called the wind's fetch. The size of a swell is determined by the strength of the wind and the length of its fetch and duration. Because of this, surf tends to be larger and more prevalent on coastlines exposed to large expanses of ocean traversed by intense low pressure systems.

Local wind conditions affect wave quality, since the surface of a wave can become choppy in blustery conditions. Ideal conditions include a light to moderate "offshore" wind, because it blows into the front of the wave, making it a "barrel" or "tube" wave. Waves are Left handed and Right Handed depending upon the breaking formation of the wave.

Waves are generally recognized by the surfaces over which they break. For example, there are Beach breaks, Reef breaks and Point breaks.

The most important influence on wave shape is the topography of the seabed directly behind and immediately beneath the breaking wave. The contours of the reef or bar front becomes stretched by diffraction. Each break is different, since each location's underwater topography is unique. At beach breaks, sandbanks change shape from week to week. Surf forecasting is aided by advances in information technology. Mathematical modeling graphically depicts the size and direction of swells around the globe.

Swell regularity varies across the globe and throughout the year. During winter, heavy swells are generated in the mid-latitudes, when the North and South polar fronts shift toward the Equator. The predominantly Westerly winds generate swells that advance Eastward, so waves tend to be largest on West coasts during winter months. However, an endless train of mid-latitude cyclones cause the isobars to become undulated, redirecting swells at regular intervals toward the tropics.

East coasts also receive heavy winter swells when low-pressure cells form in the sub-tropics, where slow moving highs inhibit their movement. These lows produce a shorter fetch than polar fronts, however they can still generate heavy swells, since their slower movement increases the duration of a particular wind direction. The variables of fetch and duration both influence how long wind acts over a wave as it travels, since a wave reaching the end of a fetch behaves as if the wind died.

During summer, heavy swells are generated when cyclones form in the tropics. Tropical cyclones form over warm seas, so their occurrence is influenced by El Niño & La Niña cycles. Their movements are unpredictable.

Surf travel and some surf camps offer surfers access to remote, tropical locations, where tradewinds ensure offshore conditions. Since winter swells are generated by mid-latitude cyclones, their regularity coincides with the passage of these lows. Swells arrive in pulses, each lasting for a couple of days, with a few days between each swell.

The availability of free model data from the NOAA has allowed the creation of several surf forecasting websites.

The value of good surf in attracting surf tourism has prompted the construction of artificial reefs and sand bars. Artificial surfing reefs can be built with durable sandbags or concrete, and resemble a submerged breakwater. These artificial reefs not only provide a surfing location, but also dissipate wave energy and shelter the coastline from erosion. Ships such as Seli 1 that have accidentally stranded on sandy bottoms, can create sandbanks that give rise to good waves.

An artificial reef known as Chevron Reef was constructed in El Segundo, California in hopes of creating a new surfing area. However, the reef failed to produce any quality waves and was removed in 2008. In Kovalam, South West India, an artificial reef has, however, successfully provided the local community with a quality lefthander, stabilized coastal soil erosion, and provided good habitat for marine life. ASR Ltd., a New Zealand-based company, constructed the Kovalam reef and is working on another reef in Boscombe, England.

Even with artificial reefs in place, a tourist's vacation time may coincide with a "flat spell", when no waves are available. Completely artificial Wave pools aim to solve that problem by controlling all the elements that go into creating perfect surf, however there are only a handful of wave pools that can simulate good surfing waves, owing primarily to construction and operation costs and potential liability. Most wave pools generate waves that are too small and lack the power necessary to surf. The Seagaia Ocean Dome, located in Miyazaki, Japan, was an example of a surfable wave pool. Able to generate waves with up to 10-foot faces, the specialized pump held water in 20 vertical tanks positioned along the back edge of the pool. This allowed the waves to be directed as they approach the artificial sea floor. Lefts, Rights, and A-frames could be directed from this pump design providing for rippable surf and barrel rides. The Ocean Dome cost about $2 billion to build and was expensive to maintain. The Ocean Dome was closed in 2007. In England, construction is nearing completion on the Wave, situated near Bristol, which will enable people unable to get to the coast to enjoy the waves in a controlled environment, set in the heart of nature.

There are two main types of artificial waves that exist today. One being artificial or stationary waves which simulate a moving, breaking wave by pumping a layer of water against a smooth structure mimicking the shape of a breaking wave. Because of the velocity of the rushing water the wave and the surfer can remain stationary while the water rushes by under the surfboard. Artificial waves of this kind provide the opportunity to try surfing and learn its basics in a moderately small and controlled environment near or far from locations with natural surf.

Another artificial wave can be made through use of a wave pool such as Kelly Slater's Wave Co. and NLand Surf Park in Austin, TX. These wave pools strive to make a wave that replicates a real ocean wave more than the stationary wave does. In 2018, the first professional surfing tournament in a wave pool was held.

Surfers represent a diverse culture based on riding the waves. Some people practice surfing as a recreational activity while others make it the central focus of their lives. Surfing culture is most dominant in Hawaii and California because these two states offer the best surfing conditions. However, waves can be found wherever there is coastline, and a tight-knit yet far-reaching subculture of surfers has emerged throughout America. Some historical markers of the culture included the woodie, the station wagon used to carry surfers' boards, as well as boardshorts, the long swim shorts typically worn while surfing. Surfers also wear wetsuits in colder regions.

The sport of surfing now represents a multibillion-dollar industry especially in clothing and fashion markets. The World Surf League (WSL) runs the championship tour, hosting top competitors in some of the best surf spots around the globe. A small number of people make a career out of surfing by receiving corporate sponsorships and performing for photographers and videographers in far-flung destinations; they are typically referred to as freesurfers.Sixty-six surfboarders on a 42-foot surfboard set a record in Huntington Beach, California for most people on a surfboard at one time.As for people who take it more seriously, such as Dale Webster, he consecutively surfed for 14,641 days, making it his main life focus.

When the waves were flat, surfers persevered with sidewalk surfing, which is now called skateboarding. Sidewalk surfing has a similar feel to surfing and requires only a paved road or sidewalk. To create the feel of the wave, surfers even sneak into empty backyard swimming pools to ride in, known as pool skating. Eventually, surfing made its way to the slopes with the invention of the Snurfer, later credited as the first snowboard. Many other board sports have been invented over the years, but all can trace their heritage back to surfing.

Many surfers claim to have a spiritual connection with the ocean, describing surfing, the surfing experience, both in and out of the water, as a type of spiritual experience or a religion.

Standup surfing begins when the surfer paddles toward shore in an attempt to match the speed of the wave (The same applies whether the surfer is standup paddling, bodysurfing, boogie-boarding or using some other type of watercraft, such as a waveski or kayak.). Once the wave begins to carry the surfer forward, the surfer stands up and proceeds to ride the wave. The basic idea is to position the surfboard so it is just ahead of the breaking part (whitewash) of the wave. A common problem for beginners is being able to catch the wave at all.

Surfers' skills are tested by their ability to control their board in difficult conditions, riding challenging waves, and executing maneuvers such as strong turns and cutbacks (turning board back to the breaking wave) and "carving" (a series of strong back-to-back maneuvers). More advanced skills include the "floater" (riding on top of the breaking curl of the wave), and "off the lip" (banking off the breaking wave). A newer addition to surfing is the progression of the "air" whereby a surfer propels off the wave entirely up into the air, and then successfully lands the board back on the wave.
The tube ride is considered to be the ultimate maneuver in surfing. As a wave breaks, if the conditions are ideal, the wave will break in an orderly line from the middle to the shoulder, enabling the experienced surfer to position themselves inside the wave as it is breaking. This is known as a tube ride. Viewed from the shore, the tube rider may disappear from view as the wave breaks over the rider's head. The longer the surfer remains in the tube, the more successful the ride. This is referred to as getting tubed, barreled, shacked or pitted. Some of the world's best known waves for tube riding include Pipeline on the North shore of Oahu, Teahupoo in Tahiti and G-Land in Java. Other names for the tube include "the barrel", and "the pit".

Hanging ten and hanging five are moves usually specific to long boarding. Hanging Ten refers to having both feet on the front end of the board with all of the surfer's toes off the edge, also known as nose-riding. Hanging Five is having just one foot near the front, with five toes off the edge.

Cutback: Generating speed down the line and then turning back to reverse direction.

Floater: Suspending the board atop the wave. Very popular on small waves.

Top-Turn: Turn off the top of the wave. Sometimes used to generate speed and sometimes to shoot spray.

Airs/Aerials: These maneuvers have been becoming more and more prevalent in the sport in both competition and free surfing. An air is when the surfer can achieve enough speed and approach a certain type of section of a wave that is supposed to act as a ramp and launch the surfer above the lip line of the wave, “catching air”, and landing either in the transition of the wave or the whitewash when hitting a close-out section.

Airs can either be straight airs or rotational airs. Straight airs have minimal rotation if any, but definitely no more rotation than 90 degrees. Rotational airs require a rotation of 90 degrees or more depending on the level of the surfer. 

Types of rotations:

180 degrees – called an air reverse, this is when the surfer spins enough to land backwards, then reverts to their original positional with the help of the fins. This rotation can either be done frontside or backside, and can spin right or left.

360 degrees – this is a full rotation air or “full rotor” where the surfer lands where they started or more, as long as they do not land backwards. When this is achieved front side on a wave spinning the opposite of an air reverse is called an alley oop. 

540 – the surfer does a full rotation plus another 180 degrees, and can be inverted or spinning straight, few surfers have been able to land this air. 

Backflip – usually done with a double grab, this hard to land air is made for elite level surfers.

Rodeo flip – usually done backside, it is a backflip with a 180 rotation, and is actually easier than a straight backflip. 

Grabs – a surfer can help land an aerial maneuver by grabbing the surfboard, keeping them attached to the board and keeping the board under their feet. 
Common types of grabs include:

Indy – a grab on the surfers (inside rail going frontside, outside rail going backside) with their back hand

Slob – a grab on the surfers (inside rail going frontside, outside rail going backside) with their front hand. 

Lien – A grab on the surfers (outside rail frontside, inside rail going backside) with their front hand

Stalefish – A grab on the surfers (outside rail frontside, inside rail backside) with their back hand. 

Double grab – A grab on the surfers inside and outside rail, the inside rail with the back hand and the outside rail with the front hand. 

The Glossary of surfing includes some of the extensive vocabulary used to describe various aspects of the sport of surfing as described in literature on the subject. In some cases terms have spread to a wider cultural use. These terms were originally coined by people who were directly involved in the sport of surfing.

Many popular surfing destinations have surf schools and surf camps that offer lessons. Surf camps for beginners and intermediates are multi-day lessons that focus on surfing fundamentals. They are designed to take new surfers and help them become proficient riders. All-inclusive surf camps offer overnight accommodations, meals, lessons and surfboards. Most surf lessons begin with instruction and a safety briefing on land, followed by instructors helping students into waves on longboards or "softboards". The softboard is considered the ideal surfboard for learning, due to the fact it is safer, and has more paddling speed and stability than shorter boards. Funboards are also a popular shape for beginners as they combine the volume and stability of the longboard with the manageable size of a smaller surfboard.
New and inexperienced surfers typically learn to catch waves on softboards around the 7–8 foot funboard size. Due to the softness of the surfboard the chance of getting injured is substantially minimized.

Typical surfing instruction is best performed one-on-one, but can also be done in a group setting. The most popular surf locations offer perfect surfing conditions for beginners, as well as challenging breaks for advanced students. The ideal conditions for learning would be small waves that crumble and break softly, as opposed to the steep, fast-peeling waves desired by more experienced surfers. When available, a sandy seabed is generally safer.

Surfing can be broken into several skills: Paddling strength, Positioning to catch the wave, timing, and balance. Paddling out requires strength, but also the mastery of techniques to break through oncoming waves ("duck diving", "eskimo roll"). Take-off positioning requires experience at predicting the wave set and where they will break. The surfer must pop up quickly as soon as the wave starts pushing the board forward. Preferred positioning on the wave is determined by experience at reading wave features including where the wave is breaking. Balance plays a crucial role in standing on a surfboard. Thus, balance training exercises are a good preparation. Practicing with a Balance board or swing boarding helps novices master the art.

Surfing can be done on various equipment, including surfboards, longboards, Stand Up Paddle boards (SUP's), bodyboards, wave skis, skimboards, kneeboards, surf mats and macca's trays. Surfboards were originally made of solid wood and were large and heavy (often up to long and ). Lighter balsa wood surfboards (first made in the late 1940s and early 1950s) were a significant improvement, not only in portability, but also in increasing maneuverability.

Most modern surfboards are made of fiberglass foam (PU), with one or more wooden strips or "stringers", fiberglass cloth, and polyester resin (PE). An emerging board material is epoxy resin and Expanded Polystyrene foam (EPS) which is stronger and lighter than traditional PU/PE construction. Even newer designs incorporate materials such as carbon fiber and variable-flex composites in conjunction with fiberglass and epoxy or polyester resins. Since epoxy/EPS surfboards are generally lighter, they will float better than a traditional PU/PE board of similar size, shape and thickness. This makes them easier to paddle and faster in the water. However, a common complaint of EPS boards is that they do not provide as much feedback as a traditional PU/PE board. For this reason, many advanced surfers prefer that their surfboards be made from traditional materials.

Other equipment includes a leash (to stop the board from drifting away after a wipeout, and to prevent it from hitting other surfers), surf wax, traction pads (to keep a surfer's feet from slipping off the deck of the board), and fins (also known as "skegs") which can either be permanently attached ("glassed-on") or interchangeable. Sportswear designed or particularly suitable for surfing may be sold as "boardwear" (the term is also used in snowboarding). In warmer climates, swimsuits, surf trunks or boardshorts are worn, and occasionally rash guards; in cold water surfers can opt to wear wetsuits, boots, hoods, and gloves to protect them against lower water temperatures. A newer introduction is a rash vest with a thin layer of titanium to provide maximum warmth without compromising mobility. In recent years, there have been advancements in technology that have allowed surfers to pursue even bigger waves with added elements of safety. Big wave surfers are now experimenting with inflatable vests or colored dye packs to help decrease their odds of drowning.

There are many different surfboard sizes, shapes, and designs in use today. Modern longboards, generally in length, are reminiscent of the earliest surfboards, but now benefit from modern innovations in surfboard shaping and fin design. Competitive longboard surfers need to be competent at traditional "walking" manoeuvres, as well as the short-radius turns normally associated with shortboard surfing. The modern shortboard began life in the late 1960s and has evolved into today's common "thruster" style, defined by its three fins, usually around in length. The thruster was invented by Australian shaper Simon Anderson.

Midsize boards, often called funboards, provide more maneuverability than a longboard, with more flotation than a shortboard. While many surfers find that funboards live up to their name, providing the best of both surfing modes, others are critical.

There are also various niche styles, such as the "Egg", a longboard-style short board targeted for people who want to ride a shortboard but need more paddle power. The "Fish", a board which is typically shorter, flatter, and wider than a normal shortboard, often with a split tail (known as a "swallow tail"). The Fish often has two or four fins and is specifically designed for surfing smaller waves. For big waves there is the "Gun", a long, thick board with a pointed nose and tail (known as a pin tail) specifically designed for big waves.

The physics of surfing involves the physical oceanographic properties of wave creation in the surf zone, the characteristics of the surfboard, and the surfer's interaction with the water and the board.

Ocean waves are defined as a collection of dislocated water parcels that undergo a cycle of being forced past their normal position and being restored back to their normal position. Wind caused ripples and eddies form waves that gradually gain speed and distance (fetch). Waves increase in energy and speed, and then become longer and stronger. The fully developed sea has the strongest wave action that experiences storms lasting 10-hours and creates 15 meter wave heights in the open ocean.

The waves created in the open ocean are classified as deep-water waves. Deep-water waves have no bottom interaction and the orbits of these water molecules are circular; their wavelength is short relative to water depth and the velocity decays before the reaching the bottom of the water basin. Deep waves have depths greater than ½ their wavelengths. Wind forces waves to break in the deep sea.

Deep-water waves travel to shore and become shallow water waves. Shallow water waves have depths less than ½ of their wavelength. Shallow wave's wavelengths are long relative to water depth and have elliptical orbitals. The wave velocity effects the entire water basin. The water interacts with the bottom as it approaches shore and has a drag interaction. The drag interaction pulls on the bottom of the wave, causes refraction, increases the height, decreases the celerity (or the speed of the wave form), and the top (crest) falls over. This phenomenon happens because the velocity of the top of the wave is greater than the velocity of the bottom of the wave.

The surf zone is place of convergence of multiple waves types creating complex wave patterns. A wave suitable for surfing results from maximum speeds of 5 meters per second. This speed is relative because local onshore winds can cause waves to break. In the surf zone, shallow water waves are carried by global winds to the beach and interact with local winds to make surfing waves.

Different onshore and off shore wind patterns in the surf zone create different types of waves. Onshore winds cause random wave breaking patterns and are more suitable for experienced surfers. Light offshore winds create smoother waves, while strong direct offshore winds cause plunging or large barrel waves. Barrel waves are large because the water depth is small when the wave breaks. Thus, the breaker intensity (or force) increases, and the wave speed and height increase. Off shore winds produce non-surfable conditions by flattening a weak swell. Weak swell is made from surface gravity forces and has long wavelengths.

Surfing waves can be analyzed using the following parameters: breaking wave height, wave peel angle (α), wave breaking intensity, and wave section length. The breaking wave height has two measurements, the relative heights estimated by surfers and the exact measurements done by physical oceanographers. Measurements done by surfers were 1.36 to 2.58 times higher than the measurements done by scientists. The scientifically concluded wave heights that are physically possible to surf are 1 to 20 meters.

The wave peel angle is one of the main constituents of a potential surfing wave. Wave peel angle measures the distance between the peel-line and the line tangent to the breaking crest line. This angle controls the speed of the wave crest. The speed of the wave is an addition of the propagation velocity vector (Vw) and peel velocity vector (Vp), which results in the overall velocity of the wave (Vs).

Wave breaking intensity measures the force of the wave as it breaks, spills, or plunges (a plunging wave is termed by surfers as a “barrel wave”). Wave section length is the distance between two breaking crests in a wave set. Wave section length can be hard to measure because local winds, non-linear wave interactions, island sheltering, and swell interactions can cause multifarious wave configurations in the surf zone.

The parameters breaking wave height, wave peel angle (α), and wave breaking intensity, and wave section length are important because they are standardized by past oceanographers who researched surfing; these parameters have been used to create a guide that matches the type of wave formed and the skill level of surfer.

Table 1 shows a relationship of smaller peel angles correlating with a higher skill level of surfer. Smaller wave peel angles increase the velocities of waves. A surfer must know how to react and paddle quickly to match the speed of the wave to catch it. Therefore, more experience is required to catch a low peel angle waves. Also, more experienced surfers can handle longer section lengths, increased velocities, and higher wave heights. Different locations offer different types of surfing conditions for each skill level.

A surf break is an area with an obstruction or an object that causes a wave to break. Surf breaks entail multiple scale phenomena. Wave section creation has microscale factors of peel angle and wave breaking intensity. The microscale components influence wave height and variations on wave crests. The mesoscale components of surf breaks are the ramp, platform, wedge, or ledge that may be present at a surf break. Macroscale processes are the global winds that initially produce offshore waves. Types of surf breaks are headlands (point break), beach break, river/estuary entrance bar, reef breaks, and ledge breaks.

A headland or point break interacts with the water by causing refraction around the point or headland. The point absorbs the high frequency waves and long period waves persist, which are easier to surf. Examples of locations that have headland or point break induced surf breaks are Dunedin (New Zealand), Raglan, Malibu (California), Rincon (California), and Kirra (Australia).

A beach break happens where waves break from offshore waves, and onshore sandbars and rips. Wave breaks happen successively at beach breaks. Example locations are Tairua and Aramoana Beach (New Zealand) and the Gold Coast (Australia).

A river or estuary entrance bar creates waves from the ebb tidal delta, sediment outflow, and tidal currents. An ideal estuary entrance bar exists in Whangamata Bar, New Zealand.

A reef break is conducive to surfing because large waves consistently break over the reef. The reef is usually made of coral, and because of this, many injuries occur while surfing reef breaks. However, the waves that are produced by reef breaks are some of the best in the world. Famous reef breaks are present in Padang Padang (Indonesia), Pipeline (Hawaii), Uluwatu (Bali), and Teahupo'o (Tahiti).

A ledge break is formed by steep rocks ledges that makes intense waves because the waves travel through deeper water then abruptly reach shallower water at the ledge. Shark Island, Australia is a location with a ledge break. Ledge breaks create difficult surfing conditions, sometimes only allowing body surfing as the only feasible way to confront the waves.

Jetties are added to bodies of water to regulate erosion, preserve navigation channels, and make harbors. Jetties are classified into four different types and have two main controlling variables: the type of delta and the size of the jetty.

The first classification is a type 1 jetty. This type of jetty is significantly longer than the surf zone width and the waves break at the shore end of the jetty. The effect of a Type 1 jetty is sediment accumulation in a wedge formation on the jetty. These waves are large and increase in size as they pass over the sediment wedge formation. An example of a Type 1 jetty is Mission Beach, San Diego, California. This 1000-meter jetty was installed in 1950 at the mouth of Mission Bay. The surf waves happen north of the jetty, are longer waves, and are powerful. The bathymetry of the sea bottom in Mission Bay has a wedge shape formation that causes the waves to refract as they become closer to the jetty. The waves converge constructively after they refract and increase the sizes of the waves.

A type 2 jetty occurs in an ebb tidal delta, a delta transitioning between high and low tide. This area has shallow water, refraction, and a distinctive seabed shapes that creates large wave heights.

An example of a type 2 jetty is called "The Poles" in Atlantic Beach, Florida. Atlantic Beach is known to have flat waves, with exceptions during major storms. However, "The Poles" has larger than normal waves due to a 500-meter jetty that was installed on the south side of the St. Johns. This jetty was built to make a deep channel in the river. It formed a delta at "The Poles". This is special area because the jetty increases wave size for surfing, when comparing pre-conditions and post-conditions of the southern St. Johns River mouth area.

The wave size at "The Poles" depends on the direction of the incoming water. When easterly waters (from 55°) interact with the jetty, they create waves larger than southern waters (from 100°). When southern waves (from 100°) move toward "The Poles", one of the waves breaks north of the southern jetty and the other breaks south of the jetty. This does not allow for merging to make larger waves. Easterly waves, from 55°, converge north of the jetty and unite to make bigger waves.

A type 3 jetty is in an ebb tidal area with an unchanging seabed that has naturally created waves. Examples of a Type 3 jetty occurs in “Southside” Tamarack, Carlsbad, California.

A type 4 jetty is one that no longer functions nor traps sediment. The waves are created from reefs in the surf zone. A type 4 jetty can be found in Tamarack, Carlsbad, California.

Rip currents are fast, narrow currents that are caused by onshore transport within the surf zone and the successive return of the water seaward. The wedge bathymetry makes a convenient and consistent rip current of 5–10 meters that brings the surfers to the “take off point” then out to the beach.

Oceanographers have two theories on rip current formation. The wave interaction model assumes that two edges of waves interact, create differing wave heights, and cause longshore transport of nearshore currents. The Boundary Interaction Model assumes that the topography of the sea bottom causes nearshore circulation and longshore transport; the result of both models is a rip current.

Rip currents can be extremely strong and narrow as they extend out of the surf zone into deeper water, reaching speeds of 1–2 feet per second to 8 feet per second. The water in the jet is sediment rich, bubble rich, and moves rapidly. The rip head of the rip current has long shore movement. Rip currents are common on beaches with mild slopes that experience sizable and frequent oceanic swell.

The vorticity and inertia of rip currents have been studied. From a model of the vorticity of a rip current done at Scripps Institute of Oceanography, it was found that a fast rip current extends away from shallow water, the vorticity of the current increases, and the width of the current decreases. This model also acknowledges that friction plays a role and waves are irregular in nature. From data from Sector-Scanning Doppler Sonar at Scripps Institute of Oceanography, it was found that rip currents in La Jolla, CA lasted several minutes, reoccurred one to four times per hour, and created a wedge with a 45° arch and a radius 200–400 meters.

A long surfboard (10 feet) causes more friction with the water; therefore, it will be slower than a smaller lighter board (6 feet). Longer boards are good for beginners who need help balancing. Smaller boards are good for more experienced surfers who want to have more control and maneuverability.

When practicing the sport of surfing, the surfer paddles out past the wave break to wait for a wave. When a surfable wave arrives, the surfer must paddle extremely fast to match the velocity of the wave so the wave can accelerate him or her.

When the surfer is at wave speed, the surfer must quickly pop up, stay low, and stay toward the front of the wave to become stable and prevent falling as the wave steepens. The acceleration is less toward the front than toward the back. The physics behind the surfing of the wave involves the horizontal acceleration force (Fsinθ) and the vertical force (Fcosθ=mg). Therefore, the surfer should lean forward to gain more speed, and lean on back foot to brake. Also, to increase the length of the ride of the wave, the surfer should travel parallel to the wave crest.


Surfing, like all water sports, carries the inherent danger of drowning. Anyone at any age can learn to surf, but should have at least intermediate swimming skills. Although the board assists a surfer in staying buoyant, it can become separated from the user. A leash, attached to the ankle or knee, can keep a board from being swept away, but does not keep a rider on the board or above water. In some cases, possibly including the drowning of professional surfer Mark Foo, a leash can even be a cause of drowning by snagging on a reef or other object and holding the surfer underwater. By keeping the surfboard close to the surfer during a wipeout, a leash also increases the chances that the board may strike the rider, which could knock him or her unconscious and lead to drowning. A fallen rider's board can become trapped in larger waves, and if the rider is attached by a leash, he or she can be dragged for long distances underwater. Surfers should be careful to remain in smaller surf until they have acquired the advanced skills and experience necessary to handle bigger waves and more challenging conditions. However, even world-class surfers have drowned in extremely challenging conditions.

Under the wrong set of conditions, anything that a surfer's body can come in contact with is potentially a danger, including sand bars, rocks, small ice, reefs, surfboards, and other surfers. Collisions with these objects can sometimes cause injuries such as cuts and scrapes and in rare instances, death.

A large number of injuries, up to 66%, are caused by collision with a surfboard (nose or fins). Fins can cause deep lacerations and cuts, as well as bruising. While these injuries can be minor, they can open the skin to infection from the sea; groups like Surfers Against Sewage campaign for cleaner waters to reduce the risk of infections. Local bugs and disease can be a dangerous factor when surfing around the globe.

Falling off a surfboard or colliding with others is commonly referred to as a "wipeout".

Sea life can sometimes cause injuries and even fatalities. Animals such as sharks, stingrays, Weever fish, seals and jellyfish can sometimes present a danger. Warmer-water surfers often do the "stingray shuffle" as they walk out through the shallows, shuffling their feet in the sand to scare away stingrays that may be resting on the bottom.

Rip currents are water channels that flow away from the shore. Under the wrong circumstances these currents can endanger both experienced and inexperienced surfers. Since a rip current appears to be an area of flat water, tired or inexperienced swimmers or surfers may enter one and be carried out beyond the breaking waves. Although many rip currents are much smaller, the largest rip currents have a width of forty or fifty feet. However, by paddling parallel to the shore, a surfer can easily exit a rip current. Alternatively, some surfers actually ride on a rip current because it is a fast and effortless way to get out beyond the zone of breaking waves.

The seabed can pose dangers for surfers. If a surfer falls while riding a wave, the wave tosses and tumbles the surfer around, often in a downwards direction. At reef breaks and beach breaks, surfers have been seriously injured and even killed because of a violent collision with the sea bed, the water above which can sometimes be very shallow, especially at beach breaks or reef breaks during low tide. Cyclops, Western Australia, for example is one of the biggest and thickest reef breaks in the world, with waves measuring up to 10 metres high, but the reef below is only about below the surface of the water.

A January 2018 study by the University of Exeter called the "Beach Bum Survey" found surfers and bodyboarders to be three times as likely as non-surfers to harbor antibiotic-resistant "E. coli" and four times as likely to harbor other bacteria capable of easily becoming antibiotic resistant. The researchers attributed this to the fact that surfers swallow roughly ten times as much seawater as swimmers.

Surfers should use ear protection such as ear plugs to avoid surfer's ear, inflammation of the ear or other damage. Surfer's ear is where the bone near the ear canal grows after repeated exposure to cold water, making the ear canal narrower. The narrowed canal makes it harder for water to drain from the ear. This can result in pain, infection and sometimes ringing of the ear. If surfer's ear develops it does so after repeated surfing sessions. Yet, damage such as inflammation of the ear can occur after only surfing once. This can be caused by repeatedly falling off the surfboard into the water and having the cold water rush into the ears, which can exert a damaging amount of pressure. Therefore, those with sensitive ears should wear ear protection even if they don't plan on surfing much. 


[Category:Sports originating in the United States]

</doc>
<doc id="28202" url="https://en.wikipedia.org/wiki?curid=28202" title="September 24">
September 24





</doc>
<doc id="28203" url="https://en.wikipedia.org/wiki?curid=28203" title="September 25">
September 25





</doc>
<doc id="28204" url="https://en.wikipedia.org/wiki?curid=28204" title="September 29">
September 29





</doc>
<doc id="28207" url="https://en.wikipedia.org/wiki?curid=28207" title="SMS">
SMS

SMS (short message service) is a text messaging service component of most telephone, internet, and mobile-device systems. It uses standardized communication protocols to enable mobile devices to exchange short text messages. An intermediary service can facilitate a text-to-voice conversion to be sent to landlines.
SMS was the most widely used data application, with an estimated 3.5 billion active users, or about 80% of all mobile subscribers, at the end of 2010.

SMS, as used on modern devices, originated from radio telegraphy in radio memo pagers that used standardized phone protocols. These were defined in 1985 as part of the Global System for Mobile Communications (GSM) series of standards.<ref name="GSM 28/85">GSM Doc 28/85 "Services and Facilities to be provided in the GSM System" rev2, June 1985</ref> The protocols allowed users to send and receive messages of up to 160 alpha-numeric characters to and from GSM mobiles. Although most SMS messages are mobile-to-mobile text messages, support for the service has expanded to include other mobile technologies, such as ANSI CDMA networks and Digital AMPS.

SMS is also employed in mobile marketing, a type of direct marketing. According to one market research report, as of 2014, the global SMS messaging business was estimated to be worth over $100 billion, accounting for almost 50 percent of all the revenue generated by mobile messaging.

Adding text messaging functionality to mobile devices began in the early 1980s. The first action plan of the CEPT Group GSM was approved in December 1982, requesting that "The services and facilities offered in the public switched telephone networks and public data networks ... should be available in the mobile system." This plan included the exchange of text messages either directly between mobile stations, or transmitted via message handling systems in use at that time.

The SMS concept was developed in the Franco-German GSM cooperation in 1984 by Friedhelm Hillebrand and Bernard Ghillebaert. The GSM is optimized for telephony, since this was identified as its main application. The key idea for SMS was to use this telephone-optimized system, and to transport messages on the signalling paths needed to control the telephone traffic during periods when no signalling traffic existed. In this way, unused resources in the system could be used to transport messages at minimal cost. However, it was necessary to limit the length of the messages to 128 bytes (later improved to 160 seven-bit characters) so that the messages could fit into the existing signalling formats. Based on his personal observations and on analysis of the typical lengths of postcard and Telex messages, Hillebrand argued that 160 characters was sufficient to express most messages succinctly.

SMS could be implemented in every mobile station by updating its software. Hence, a large base of SMS-capable terminals and networks existed when people began to use SMS. A new network element required was a specialized short message service centre, and enhancements were required to the radio capacity and network transport infrastructure to accommodate growing SMS traffic.

The technical development of SMS was a multinational collaboration supporting the framework of standards bodies. Through these organizations the technology was made freely available to the whole world.

The first proposal which initiated the development of SMS was made by a contribution of Germany and France into the GSM group meeting in February 1985 in Oslo. This proposal was further elaborated in GSM subgroup WP1 Services (Chairman Martine Alvernhe, France Telecom) based on a contribution from Germany. There were also initial discussions in the subgroup WP3 network aspects chaired by Jan Audestad (Telenor). The result was approved by the main GSM group in a June '85 document which was distributed to industry. The input documents on SMS had been prepared by Friedhelm Hillebrand (Deutsche Telekom) with contributions from Bernard Ghillebaert (France Télécom). The definition that Friedhelm Hillebrand and Bernard Ghillebaert brought into GSM called for the provision of a message transmission service of alphanumeric messages to mobile users "with acknowledgement capabilities". The last three words transformed SMS into something much more useful than the prevailing messaging paging that some in GSM might have had in mind.

SMS was considered in the main GSM group as a possible service for the new digital cellular system. In GSM document ""Services and Facilities to be provided in the GSM System,"" both mobile-originated and mobile-terminated short messages appear on the table of GSM teleservices.

The discussions on the GSM services were concluded in the recommendation GSM 02.03 ""TeleServices supported by a GSM PLMN."" Here a rudimentary description of the three services was given:


The material elaborated in GSM and its WP1 subgroup was handed over in Spring 1987 to a new GSM body called IDEG (the Implementation of Data and Telematic Services Experts Group), which had its kickoff in May 1987 under the chairmanship of Friedhelm Hillebrand (German Telecom). The technical standard known today was largely created by IDEG (later WP4) as the two recommendations GSM 03.40 (the two point-to-point services merged) and GSM 03.41 (cell broadcast).

WP4 created a Drafting Group Message Handling (DGMH), which was responsible for the specification of SMS. Finn Trosby of Telenor chaired the draft group through its first 3 years, in which the design of SMS was established. DGMH had five to eight participants, and Finn Trosby mentions as major contributors Kevin Holley, Eija Altonen, Didier Luizard and Alan Cox. The first action plan mentions for the first time the Technical Specification 03.40 "Technical Realisation of the Short Message Service". Responsible editor was Finn Trosby. The first and very rudimentary draft of the technical specification was completed in November 1987. However, drafts useful for the manufacturers followed at a later stage in the period. A comprehensive description of the work in this period is given in.

The work on the draft specification continued in the following few years, where Kevin Holley of Cellnet (now Telefónica O2 UK) played a leading role. Besides the completion of the main specification GSM 03.40, the detailed protocol specifications on the system interfaces also needed to be completed.

The Mobile Application Part (MAP) of the SS7 protocol included support for the transport of Short Messages through the Core Network from its inception. MAP Phase 2 expanded support for SMS by introducing a separate operation code for Mobile Terminated Short Message transport. Since Phase 2, there have been no changes to the Short Message operation packages in MAP, although other operation packages have been enhanced to support CAMEL SMS control.

From 3GPP Releases 99 and 4 onwards, CAMEL Phase 3 introduced the ability for the Intelligent Network (IN) to control aspects of the Mobile Originated Short Message Service, while CAMEL Phase 4, as part of 3GPP Release 5 and onwards, provides the IN with the ability to control the Mobile Terminated service. CAMEL allows the gsmSCP to block the submission (MO) or delivery (MT) of Short Messages, route messages to destinations other than that specified by the user, and perform real-time billing for the use of the service. Prior to standardized CAMEL control of the Short Message Service, IN control relied on switch vendor specific extensions to the Intelligent Network Application Part (INAP) of SS7.

The first SMS message was sent over the Vodafone GSM network in the United Kingdom on 3 December 1992, from Neil Papworth of Sema Group (now Mavenir Systems) using a personal computer to Richard Jarvis of Vodafone using an Orbitel 901 handset. The text of the message was "Merry Christmas."

The first commercial deployment of a short message service center (SMSC) was by Aldiscon part of Logica (now part of Acision) with Telia (now TeliaSonera) in Sweden in 1993, followed by Fleet Call (now Nextel) in the US, Telenor in Norway and BT Cellnet (now O2 UK) later in 1993. All first installations of SMS gateways were for network notifications sent to mobile phones, usually to inform of voice mail messages.

The first commercially sold SMS service was offered to consumers, as a person-to-person text messaging service by Radiolinja (now part of Elisa) in Finland in 1993. Most early GSM mobile phone handsets did not support the ability to send SMS text messages, and Nokia was the only handset manufacturer whose total GSM phone line in 1993 supported user-sending of SMS text messages. According to Matti Makkonen, the inventor of SMS text messages, Nokia 2010, which was released in January 1994, was the first mobile phone to support composing SMSes easily.

Initial growth was slow, with customers in 1995 sending on average only 0.4 messages per GSM customer per month. One factor in the slow takeup of SMS was that operators were slow to set up charging systems, especially for prepaid subscribers, and eliminate billing fraud which was possible by changing SMSC settings on individual handsets to use the SMSCs of other operators. Initially, networks in the UK only allowed customers to send messages to other users on the same network, limiting the usefulness of the service. This restriction was lifted in 1999.

Over time, this issue was eliminated by switch billing instead of billing at the SMSC and by new features within SMSCs to allow blocking of foreign mobile users sending messages through it. By the end of 2000, the average number of messages reached 35 per user per month, and on Christmas Day 2006, over 205 million messages were sent in the UK alone.

SMS was originally designed as part of GSM, but is now available on a wide range of networks, including 3G networks. However, not all text messaging systems use SMS, and some notable alternative implementations of the concept include J-Phone's "SkyMail" and NTT Docomo's "Short Mail", both in Japan. Email messaging from phones, as popularized by NTT Docomo's i-mode and the RIM BlackBerry, also typically uses standard mail protocols such as SMTP over TCP/IP.

, 6.1 trillion (6.1 × 10) SMS text messages were sent. This translates into an average of 193,000 SMS per second. SMS has become a huge commercial industry, earning $114.6 billion globally in 2010. The global average price for an SMS message is US$0.11, while mobile networks charge each other interconnect fees of at least US$0.04 when connecting between different phone networks.

In 2015, the actual cost of sending an SMS in Australia was found to be $0.00016 per SMS.

In 2014, Caktus Group developed the world's first SMS-based voter registration system in Libya. So far, more than 1.5 million people have registered using that system, providing Libyan voters with unprecedented access to the democratic process.

While SMS is still a growing market, traditional SMS is becoming increasingly challenged by Internet Protocol-based messaging services such as Apple Inc.'s iMessage, Facebook Messenger, WhatsApp, Viber, WeChat (in China) and Line (in Japan), available on smart phones with data connections. It has been reported that over 97% of smart phone owners use alternative messaging services at least once a day. However, in the U.S. these Internet-based services have not caught on as much, and SMS continues to be highly popular there. One of the reasons is because the top three American carriers have offered free SMS with almost all phone bundles since 2010, a stark contrast to Europe where SMS costs have been pricey.

Enterprise SMS-messaging, also known as application-to-peer messaging (A2P Messaging) or 2-way SMS, continue to grow steadily at a rate of 4% annually. Enterprise SMS applications are primarily focused on CRM and delivering highly targeted service messages such as parcel-delivery alerts, real-time notification of credit/debit card purchase confirmations to protect against fraud, and appointment confirmations. Another primary source of growing A2P message volumes is two-step verification (alternatively referred to as 2-factor authentication) processes whereby users are delivered a one-time passcode over SMS and then are asked to enter that passcode online in order to verify their identity.

SMS enablement allows individuals to send a SMS to a business phone number (traditional landline) and receive a SMS in return. Providing customers with the ability to text to a phone number allows organizations to offer new services that deliver value. Examples include chat bots, and text enabled customer service and call centers. 

The "Short Message Service—Point to Point (SMS-PP)"—was originally defined in GSM recommendation 03.40, which is now maintained in 3GPP as TS 23.040. GSM 03.41 (now 3GPP TS 23.041) defines the "Short Message Service—Cell Broadcast (SMS-CB)", which allows messages (advertising, public information, etc.) to be broadcast to all mobile users in a specified geographical area.

Messages are sent to a short message service center (SMSC), which provides a "store and forward" mechanism. It attempts to send messages to the SMSC's recipients. If a recipient is not reachable, the SMSC queues the message for later retry. Some SMSCs also provide a "forward and forget" option where transmission is tried only once. Both mobile terminated (MT, for messages sent "to" a mobile handset) and mobile originating (MO, for those sent "from" the mobile handset) operations are supported. Message delivery is "best effort," so there are no guarantees that a message will actually be delivered to its recipient, but delay or complete loss of a message is uncommon, typically affecting less than 5 percent of messages. Some providers allow users to request delivery reports, either via the SMS settings of most modern phones, or by prefixing each message with *0# or *N#. However, the exact meaning of confirmations varies from reaching the network, to being queued for sending, to being sent, to receiving a confirmation of receipt from the target device, and users are often not informed of the specific type of success being reported.

SMS is a stateless communication protocol in which every SMS message is considered entirely independent of other messages. Enterprise applications using SMS as a communication channel for stateful dialogue (where an MO reply message is paired to a specific MT message) requires that session management be maintained external to the protocol.

Transmission of short messages between the SMSC and the handset is done whenever using the Mobile Application Part (MAP) of the SS7 protocol. Messages are sent with the MAP MO- and MT-ForwardSM operations, whose payload length is limited by the constraints of the signaling protocol to precisely 140 bytes (140 bytes * 8 bits / byte = 1120 bits). Short messages can be encoded using a variety of alphabets: the default GSM 7-bit alphabet, the 8-bit data alphabet, and the 16-bit UCS-2 alphabet. Depending on which alphabet the subscriber has configured in the handset, this leads to the maximum individual short message sizes of 160 7-bit characters, 140 8-bit characters, or 70 16-bit characters. GSM 7-bit alphabet support is mandatory for GSM handsets and network elements, but characters in languages such as Hindi, Arabic, Chinese, Korean, Japanese, or Cyrillic alphabet languages (e.g., Russian, Ukrainian, Serbian, Bulgarian, etc.) must be encoded using the 16-bit UCS-2 character encoding (see Unicode). Routing data and other metadata is additional to the payload size.

Larger content (concatenated SMS, multipart or segmented SMS, or "long SMS") can be sent using multiple messages, in which case each message will start with a User Data Header (UDH) containing segmentation information. Since UDH is part of the payload, the number of available characters per segment is lower: 153 for 7-bit encoding, 134 for 8-bit encoding and 67 for 16-bit encoding. The receiving handset is then responsible for reassembling the message and presenting it to the user as one long message. While the standard theoretically permits up to 255 segments, 6 to 8 segment messages are the practical maximum, and long messages are often billed as equivalent to multiple SMS messages. Some providers have offered length-oriented pricing schemes for messages, although that type of pricing structure is rapidly disappearing.

SMS gateway providers facilitate SMS traffic between businesses and mobile subscribers, including SMS for enterprises, content delivery, and entertainment services involving SMS, e.g. TV voting. Considering SMS messaging performance and cost, as well as the level of messaging services, SMS gateway providers can be classified as aggregators or SS7 providers.

The aggregator model is based on multiple agreements with mobile carriers to exchange two-way SMS traffic into and out of the operator's SMSC, also known as "local termination model". Aggregators lack direct access into the SS7 protocol, which is the protocol where the SMS messages are exchanged. SMS messages are delivered to the operator's SMSC, but not the subscriber's handset; the SMSC takes care of further handling of the message through the SS7 network.

Another type of SMS gateway provider is based on SS7 connectivity to route SMS messages, also known as "international termination model". The advantage of this model is the ability to route data directly through SS7, which gives the provider total control and visibility of the complete path during SMS routing. This means SMS messages can be sent directly to and from recipients without having to go through the SMSCs of other mobile operators. Therefore, it is possible to avoid delays and message losses, offering full delivery guarantees of messages and optimized routing. This model is particularly efficient when used in mission-critical messaging and SMS used in corporate communications. Moreover, these SMS gateway providers are providing branded SMS services with masking but after misuse of these gateways most countries's Governments have taken serious steps to block these gateways.

Message Service Centers communicate with the Public Land Mobile Network (PLMN) or PSTN via Interworking and Gateway MSCs.

Subscriber-originated messages are transported from a handset to a service center, and may be destined for mobile users, subscribers on a fixed network, or Value-Added Service Providers (VASPs), also known as application-terminated. Subscriber-terminated messages are transported from the service center to the destination handset, and may originate from mobile users, from fixed network subscribers, or from other sources such as VASPs.

On some carriers nonsubscribers can send messages to a subscriber's phone using an Email-to-SMS gateway. Additionally, many carriers, including AT&T Mobility, T-Mobile USA, Sprint, and Verizon Wireless, offer the ability to do this through their respective web sites.

For example, an AT&T subscriber whose phone number was 555-555-5555 would receive e-mails addressed to 5555555555@txt.att.net as text messages. Subscribers can easily reply to these SMS messages, and the SMS reply is sent back to the original email address. Sending email to SMS is free for the sender, but the recipient is subject to the standard delivery charges. Only the first 160 characters of an email message can be delivered to a phone, and only 160 characters can be sent from a phone. However, longer messages may be broken up into multiple texts, depending upon the telephone service provider.

Text-enabled fixed-line handsets are required to receive messages in text format. However, messages can be delivered to nonenabled phones using text-to-speech conversion.

Short messages can send binary content such as ringtones or logos, as well as Over-the-air programming (OTA) or configuration data. Such uses are a vendor-specific extension of the GSM specification and there are multiple competing standards, although Nokia's Smart Messaging is common. An alternative way for sending such binary content is EMS messaging, which is standardized and not dependent on vendors.

SMS is used for M2M (Machine to Machine) communication. For instance, there is an LED display machine controlled by SMS, and some vehicle tracking companies use SMS for their data transport or telemetry needs. SMS usage for these purposes is slowly being superseded by GPRS services owing to their lower overall cost. GPRS is offered by smaller telco players as a route of sending SMS text to reduce the cost of SMS texting internationally.

Many mobile and satellite transceiver units support the sending and receiving of SMS using an extended version of the Hayes command set, a specific command language originally developed for the Hayes Smartmodem 300-baud modem in 1977.

The connection between the terminal equipment and the transceiver can be realized with a serial cable (e.g., USB), a Bluetooth link, an infrared link, etc. Common AT commands include AT+CMGS (send message), AT+CMSS (send message from storage), AT+CMGL (list messages) and AT+CMGR (read message).

However, not all modern devices support receiving of messages if the message storage (for instance the device's internal memory) is not accessible using AT commands.

Short messages may be used normally to provide premium rate services to subscribers of a telephone network.

Mobile-terminated short messages can be used to deliver digital content such as news alerts, financial information, logos, and ring tones. The first premium-rate media content delivered via the SMS system was the world's first paid downloadable ringing tones, as commercially launched by Saunalahti (later Jippii Group, now part of Elisa Grous), in 1998. Initially, only Nokia branded phones could handle them. By 2002 the ringtone business globally had exceeded $1 billion of service revenues, and nearly US$5 billion by 2008. Today, they are also used to pay smaller payments online—for example, for file-sharing services, in mobile application stores, or VIP section entrance. Outside the online world, one can buy a bus ticket or beverages from ATM, pay a parking ticket, order a store catalog or some goods (e.g., discount movie DVDs), make a donation to charity, and much more.

Premium-rated messages are also used in Donors Message Service to collect money for charities and foundations. DMS was first launched at April 1, 2004, and is very popular in the Czech Republic. For example, the Czech people sent over 1.5 million messages to help South Asia recover from the 2004 Indian Ocean earthquake and tsunami.

The Value-added service provider (VASP) providing the content submits the message to the mobile operator's SMSC(s) using an TCP/IP protocol such as the short message peer-to-peer protocol (SMPP) or the External Machine Interface (EMI). The SMSC delivers the text using the normal Mobile Terminated delivery procedure. The subscribers are charged extra for receiving this premium content; the revenue is typically divided between the mobile network operator and the VASP either through revenue share or a fixed transport fee. Submission to the SMSC is usually handled by a third party.

Mobile-originated short messages may also be used in a premium-rated manner for services such as televoting. In this case, the VASP providing the service obtains a short code from the telephone network operator, and subscribers send texts to that number. The payouts to the carriers vary by carrier; percentages paid are greatest on the lowest-priced premium SMS services. Most information providers should expect to pay about 45 percent of the cost of the premium SMS up front to the carrier. The submission of the text to the SMSC is identical to a standard MO Short Message submission, but once the text is at the SMSC, the Service Center (SC) identifies the Short Code as a premium service. The SC will then direct the content of the text message to the VASP, typically using an IP protocol such as SMPP or EMI. Subscribers are charged a premium for the sending of such messages, with the revenue typically shared between the network operator and the VASP. Short codes only work within one country, they are not international.

An alternative to inbound SMS is based on long numbers (international number format, such as "+44 762 480 5000"), which can be used in place of short codes for SMS reception in several applications, such as TV voting, product promotions and campaigns. Long numbers work internationally, allow businesses to use their own numbers, rather than short codes, which are usually shared across many brands. Additionally, long numbers are nonpremium inbound numbers.

Threaded SMS is a visual styling orientation of SMS message history that arranges messages to and from a contact in chronological order on a single screen. It was first invented by a developer working to implement the SMS client for the BlackBerry, who was looking to make use of the blank screen left below the message on a device with a larger screen capable of displaying far more than the usual 160 characters, and was inspired by threaded Reply conversations in email. Visually, this style of representation provides a back-and-forth chat-like history for each individual contact. Hierarchical-threading at the conversation-level (as typical in blogs and on-line messaging boards)is not widely supported by SMS messaging clients. This limitation is due to the fact that there is no session identifier or subject-line passed back and forth between sent and received messages in the header data (as specified by SMS protocol) from which the client device can properly thread an incoming message to a specific dialogue, or even to a specific message within a dialogue. Most smart phone text-messaging-clients are able to create some contextual threading of "group messages" which narrows the context of the thread around the common interests shared by group members. On the other hand, advanced enterprise messaging applications which push messages from a remote server often display a dynamically changing reply number (multiple numbers used by the same sender), which is used along with the sender's phone number to create session-tracking capabilities analogous to the functionality that cookies provide for web-browsing. As one pervasive example, this technique is used to extend the functionality of many Instant Messenger (IM) applications such that they are able to communicate over two-way dialogues with the much larger SMS user-base. In cases where multiple reply numbers are used by the enterprise server to maintain the dialogue, the visual conversation threading on the client may be separated into multiple threads.

While SMS reached its popularity as a person-to-person messaging, another type of SMS is growing fast: application-to-person (A2P) messaging. A2P is a type of SMS sent from a subscriber to an application or sent from an application to a subscriber. It is commonly used by financial institutions, airlines, hotel booking sites, social networks, and other organizations sending SMS from their systems to their customers.

In the USA, A2P messages must be sent using a short code rather than a standard long code.

All commercial satellite phone networks except ACeS and OptusSat support SMS. While early Iridium handsets only support incoming SMS, later models can also send messages. The price per message varies for different networks. Unlike some mobile phone networks, there is no extra charge for sending international SMS or to send one to a different satellite phone network. SMS can sometimes be sent from areas where the signal is too poor to make a voice call.

Satellite phone networks usually have web-based or email-based SMS portals where one can send free SMS to phones on that particular network.

Unlike dedicated texting systems like the Simple Network Paging Protocol and Motorola's ReFLEX protocol, SMS message delivery is not guaranteed, and many implementations provide no mechanism through which a sender can determine whether an SMS message has been delivered in a timely manner. SMS messages are generally treated as lower-priority traffic than voice, and various studies have shown that around 1% to 5% of messages are lost entirely, even during normal operation conditions, and others may not be delivered until long after their relevance has passed. The use of SMS as an emergency notification service in particular has been questioned.

The Global Service for Mobile communications (GSM), with the greatest worldwide number of users, succumbs to several security vulnerabilities. In the GSM, only the airway traffic between the Mobile Station (MS) and the Base Transceiver Station (BTS) is optionally encrypted with a weak and broken stream cipher (A5/1 or A5/2). The authentication is unilateral and also vulnerable. There are also many other security vulnerabilities and shortcomings. Such vulnerabilities are inherent to SMS as one of the superior and well-tried services with a global availability in the GSM networks. SMS messaging has some extra security vulnerabilities due to its store-and-forward feature, and the problem of fake SMS that can be conducted via the Internet. When a user is roaming, SMS content passes through different networks, perhaps including the Internet, and is exposed to various vulnerabilities and attacks. Another concern arises when an adversary gets access to a phone and reads the previous unprotected messages.

In October 2005, researchers from Pennsylvania State University published an analysis of vulnerabilities in SMS-capable cellular networks. The researchers speculated that attackers might exploit the open functionality of these networks to disrupt them or cause them to fail, possibly on a nationwide scale.

The GSM industry has identified a number of potential fraud attacks on mobile operators that can be delivered via abuse of SMS messaging services. The most serious threat is SMS Spoofing, which occurs when a fraudster manipulates address information in order to impersonate a user that has roamed onto a foreign network and is submitting messages to the home network. Frequently, these messages are addressed to destinations outside the home network—with the home SMSC essentially being "hijacked" to send messages into other networks.

The only sure way of detecting and blocking spoofed messages is to screen incoming mobile-originated messages to verify that the sender is a valid subscriber and that the message is coming from a valid and correct location. This can be implemented by adding an intelligent routing function to the network that can query originating subscriber details from the home location register (HLR) before the message is submitted for delivery. This kind of intelligent routing function is beyond the capabilities of legacy messaging infrastructure.

In an effort to limit telemarketers who had taken to bombarding users with hordes of unsolicited messages, India introduced new regulations in September 2011, including a cap of 3,000 SMS messages per subscriber per month, or an average of 100 per subscriber per day. Due to representations received from some of the service providers and consumers, TRAI (Telecom Regulatory Authority of India) has raised this limit to 200 SMS messages per SIM per day in case of prepaid services, and up to 6,000 SMS messages per SIM per month in case of postpaid services with effect from 1 November 2011. However, it was ruled unconstitutional by the Delhi high court, but there are some limitations.

A Flash SMS is a type of SMS that appears directly on the main screen without user interaction and is not automatically stored in the inbox. It can be useful in emergencies, such as a fire alarm or cases of confidentiality, as in delivering one-time passwords.

In Germany in 2010 almost half a million "silent SMS" messages were sent by the federal police, customs and the secret service "Verfassungsschutz" (offices for protection of the constitution). These silent messages, also known as "silent TMS", "stealth SMS", "stealth ping" or "Short Message Type 0", are used to locate a person and thus to create a complete movement profile. They do not show up on a display, nor trigger any acoustical signal when received. Their primary purpose was to deliver special services of the network operator to any cell phone. The mobile provider, often at the behest of the police, will capture data such as subscriber identification IMSI.



</doc>
