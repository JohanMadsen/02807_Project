<doc id="28208" url="https://en.wikipedia.org/wiki?curid=28208" title="Santa Monica, California">
Santa Monica, California

Santa Monica is a beachfront city in western Los Angeles County, California, United States. Situated on Santa Monica Bay, it is bordered on three sides by the city of Los Angeles – Pacific Palisades to the north, Brentwood on the northeast, West Los Angeles on the east, Mar Vista on the southeast, and Venice on the south. The Census Bureau population for Santa Monica in 2010 was 89,736.

Due in part to an agreeable climate, Santa Monica became a famed resort town by the early 20th century. The city has experienced a boom since the late 1980s through the revitalization of its downtown core, significant job growth and increased tourism. The Santa Monica Pier and Pacific Park remain popular destinations.

Santa Monica was long inhabited by the Tongva people. Santa Monica was called Kecheek in the Tongva language. The first non-indigenous group to set foot in the area was the party of explorer Gaspar de Portolà, who camped near the present-day intersection of Barrington and Ohio Avenues on August 3, 1769. Named after the Christian saint Monica, there are two different accounts of how the city's name came to be. One says it was named in honor of the feast day of Saint Monica (mother of Saint Augustine), but her feast day is May 4. Another version says it was named by Juan Crespí on account of a pair of springs, the Kuruvungna Springs (Serra Springs), that were reminiscent of the tears Saint Monica shed over her son's early impiety.

In Los Angeles, several battles were fought by the Californios. Following the Mexican–American War, Mexico signed the Treaty of Guadalupe Hidalgo, which gave Mexicans and Californios living in state certain unalienable rights. US government sovereignty in California began on February 2, 1848.
In the 1870s the Los Angeles and Independence Railroad, connected Santa Monica with Los Angeles, and a wharf out into the bay. The first town hall was a modest 1873 brick building, later a beer hall, and now part of the Santa Monica Hostel. It is Santa Monica's oldest extant structure. By 1885, the town's first hotel was the Santa Monica Hotel.

Amusement piers became enormously popular in the first decades of the 20th century and the extensive Pacific Electric Railroad brought people to the city's beaches from across the Greater Los Angeles Area.

Around the start of the 20th century, a growing population of Asian Americans lived in and around Santa Monica and Venice. A Japanese fishing village was near the Long Wharf while small numbers of Chinese lived or worked in Santa Monica and Venice. The two ethnic minorities were often viewed differently by White Americans who were often well-disposed towards the Japanese but condescending towards the Chinese. The Japanese village fishermen were an integral economic part of the Santa Monica Bay community.
Donald Wills Douglas, Sr. built a plant in 1922 at Clover Field (Santa Monica Airport) for the Douglas Aircraft Company. In 1924, four Douglas-built planes took off from Clover Field to attempt the first aerial circumnavigation of the world. Two planes returned after covering in 175 days, and were greeted on their return September 23, 1924, by a crowd of 200,000 (generously estimated). The Douglas Company (later McDonnell Douglas) kept facilities in the city until the 1960s.

The Great Depression hit Santa Monica deeply. One report gives citywide employment in 1933 of just 1,000. Hotels and office building owners went bankrupt. In the 1930s, corruption infected Santa Monica (along with neighboring Los Angeles). The federal Works Project Administration helped build several buildings, most notably "City Hall". The main "Post Office" and "Barnum Hall" (Santa Monica High School auditorium) were also among other WPA projects.

Douglas's business grew astronomically with the onset of World War II, employing as many as 44,000 people in 1943. To defend against air attack, set designers from the Warner Brothers Studios prepared elaborate camouflage that disguised the factory and airfield. The RAND Corporation began as a project of the Douglas Company in 1945, and spun off into an independent think tank on May 14, 1948. RAND eventually acquired a 15-acre (61,000 m²) campus between the Civic Center and the pier entrance.

The completion of the Santa Monica Freeway in 1966 brought the promise of new prosperity, though at the cost of decimating the Pico neighborhood that had been a leading African American enclave on the Westside.

Beach volleyball is believed to have been developed by Duke Kahanamoku in Santa Monica during the 1920s.

The Santa Monica Looff Hippodrome (carousel) is a National Historic Landmark. It sits on the Santa Monica Pier, which was built in 1909. The La Monica Ballroom on the pier was once the largest ballroom in the US and the source for many New Year's Eve national network broadcasts.
The Santa Monica Civic Auditorium was an important music venue for several decades and hosted the Academy Awards in the 1960s. McCabe's Guitar Shop is a leading acoustic performance space as well as retail outlet. Bergamot Station is a city-owned art gallery compound that includes the Santa Monica Museum of Art. The city is also home to the California Heritage Museum and the Angels Attic dollhouse and toy museum. The New West Symphony is the resident orchestra of Barnum Hall. They are also resident orchestra of the Oxnard Performing Arts Center and the Thousand Oaks Civic Arts Plaza.

Santa Monica has three main shopping districts, Montana Avenue on the north side, the Downtown District in the city's core, and Main Street on the south end. Each has its own unique feel and personality. Montana Avenue is a stretch of luxury boutique stores, restaurants, and small offices that generally features more upscale shopping. The Main Street district offers an eclectic mix of clothing, restaurants, and other specialty retail.

The Downtown District is the home of the Third Street Promenade, a major outdoor pedestrian-only shopping district that stretches for three blocks between Wilshire Blvd. and Broadway (not the same Broadway in downtown and south Los Angeles).
Third Street is closed to vehicles for those three blocks to allow people to stroll, congregate, shop and enjoy street performers.
Santa Monica Place, featuring Bloomingdale's and Nordstrom in a three-level outdoor environment, is at the Promenade's southern end. After a period of redevelopment, the mall reopened in the fall of 2010 as a modern shopping, entertainment and dining complex with more outdoor space.

Santa Monica hosts the annual Santa Monica Film Festival.

The city's oldest movie theater is the Majestic. Opened in 1912 and also known as the Mayfair Theatre, the theater, it has been closed since the 1994 Northridge earthquake. The Aero Theater (now operated by the American Cinematheque) and Criterion Theater were built in the 1930s and still show movies. The Santa Monica Promenade alone supports more than a dozen movie screens.

Palisades Park stretches out along the crumbling bluffs overlooking the Pacific and is a favorite walking area to view the ocean. It includes a totem pole, camera obscura, artwork, benches, picnic areas, pétanque courts, and restrooms.

Tongva Park occupies 6 acres between Ocean Avenue and Main Street, just south of Colorado Avenue. The park includes an overlook, amphitheater, playground, garden, fountains, picnic areas, and restrooms.

The Santa Monica Stairs, a long, steep staircase that leads from north of San Vicente down into Santa Monica Canyon, is a popular spot for all-natural outdoor workouts. Some area residents have complained that the stairs have become too popular, and attract too many exercisers to the wealthy neighborhood of multimillion-dollar properties.

Natives and tourists alike have enjoyed the Santa Monica Rugby Club since 1972. The club has been very successful since its conception, most recently winning back-to-back national championships in 2005 and 2006. Santa Monica defeated the Boston Irish Wolfhounds 57-19 in the Division 1 final, convincingly claiming its second consecutive American title on June 4, 2006, in San Diego. They offer Men's, Women's and a thriving children's programs. The club recently joined the Rugby Super League.

Santa Monica has two hospitals: Saint John's Health Center and Santa Monica-UCLA Medical Center. Its cemetery is Woodlawn Memorial.

Santa Monica has several newspapers and magazines, including the "Santa Monica Star", "Santa Monica Daily Press", the "Santa Monica Mirror", the "Santa Monica Observer", "Santa Monica Magazine", and the "Santa Monica Sun".

The city of Santa Monica rests on a mostly flat slope that angles down towards Ocean Avenue and towards the south. High bluffs separate the north side of the city from the beaches. Santa Monica borders the L.A. neighborhoods of Pacific Palisades to the north and Venice to the south. To the west, Santa Monica has the 3-mile coastline fronting the Santa Monica Bay, and to the east of the city borders are the Los Angeles communities of West Los Angeles and Brentwood.

Santa Monica has a coastal Mediterranean climate (Köppen "Csb"). Santa Monica enjoys an average of 310 days of sunshine a year. It is in USDA plant hardiness zone 11a. Because of its location, nestled on the vast and open Santa Monica Bay, morning fog is a common phenomenon in May, June and early July (caused by ocean temperature variations and currents). Like other inhabitants of the greater Los Angeles area, residents have a particular terminology for this phenomenon: the "May Gray" and the "June Gloom". Overcast skies are common during June mornings, but usually the strong sun burns the fog off by noon. In the late winter/early summer, daily fog is a phenomenon too. It happens suddenly and it may last some hours or past sunset time. Nonetheless, it will sometimes stay cloudy and cool all day during June, even as other parts of the Los Angeles area enjoy sunny skies and warmer temperatures. At times, the sun can be shining east of 20th Street, while the beach area is overcast. As a general rule, the beach temperature is from 5 to 10 degrees Fahrenheit (3 to 6 degrees Celsius) cooler than it is inland during summer days, and 5–10 degrees warmer during winter nights.

It is also in September highest temperatures tend to be reached. It is winter, however, when the hot, dry winds of the Santa Anas are most common. In contrast, temperatures exceeding 10 degrees below average are rare.

The rainy season is from late October through late March. Winter storms usually approach from the northwest and pass quickly through the Southland. There is very little rain during the rest of the year. Yearly rainfall totals are unpredictable as rainy years are occasionally followed by droughts. There has never been any snow or frost, but there has been hail.

Santa Monica usually enjoys cool breezes blowing in from the ocean, which tend to keep the air fresh and clean. Therefore, smog is less of a problem for Santa Monica than elsewhere around Los Angeles. However, in the autumn months of September through November, the Santa Ana winds will sometimes blow from the east, bringing smoggy and hot inland air to the beaches.

Santa Monica is one of the most environmentally activist municipalities in the nation. The city first proposed its Sustainable City Plan in 1992 and in 1994, was one of the first cities in the nation to formally adopt a comprehensive sustainability plan, setting waste reduction and water conservation policies for both public and private sector through its Office of Sustainability and the Environment. Eighty-two percent of the city's public works vehicles now run on alternative fuels, including nearly 100% of the municipal bus system, making it among the largest such fleets in the country. Santa Monica fleet vehicles and buses now source their natural gas from Redeem, a Southern California-based supplier of renewable and sustainable natural gas obtained from non-fracked methane biogas generated from organic landfill waste.

Santa Monica has adopted a Community Energy Independence Initiative, with a goal of achieving complete energy independence by 2020 (vs. California's already ambitious 33% renewables goal). In the last 15 years, greenhouse gas emissions have been cut citywide by nearly 10% relative to 1990 levels, with further reductions being planned by the Office of Sustainability.

An urban runoff facility (SMURFF), the first of its kind in the US, catches and treats of water each week that would otherwise flow into the bay via storm-drains and sells it back to end-users within the city for reuse as gray-water, while bio-swales throughout the city allow rainwater to percolate into and replenish the groundwater supply. The groundwater supply, in turn, plays an important role in the city's Sustainable Water Master Plan, whereby Santa Monica has set a goal of attaining 100% water independence by 2020. The city has numerous programs designed to promote water conservation among residents, including a rebate of $1.50 per square foot for those who convert water intensive lawns to more local drought-tolerant gardens that require less water.

Santa Monica has also instituted a green building-code whereby merely constructing to code automatically renders a building equivalent to the US Green Building Council's LEED Silver standards. The city's Main Library, for example, is one of many LEED certified or LEED equivalent buildings in the city. It is built over a 200,000 gallon cistern that collects filtered stormwater from the roof. The water is used for landscape irrigation.

Since 2009, Santa Monica has been developing the Zero Waste Strategic Operations Plan by which the city will set a goal of diverting at least 95% of all waste away from landfills, and toward recycling and composting, by 2030. The plan includes a food waste composting program, which diverts 3 million pounds of restaurant food waste away from landfills annually. Currently, 77% of all solid waste produced citywide is diverted from landfills.

The city is also in the process of implementing a 5-year and 20 year Bike Action Plan with a goal of attaining 14 to 35% bicycle transportation mode share by 2030 through the installation of enhanced bicycle infrastructure throughout the city. Other environmentally focused initiatives include curbside recycling, curbside composting bins (in addition to trash, yard-waste, and recycle bins), farmers' markets, community gardens, garden-share, an urban forest initiative, a hazardous materials home-collection service, green business certification, a municipal bus system, and the Metro light rail Expo Line (replacing the former Pacific Electric Santa Monica Air Line last operated in 1953).

Santa Monica's population has grown from 417 in 1880 to 89,736 in 2010.

The 2010 United States Census reported Santa Monica had a population of 89,736. The population density was 10,662.6 people per square mile (4,116.9/km²). The racial makeup of Santa Monica was 69,663 (77.6%) White (70.1% Non-Hispanic White), 3,526 (3.9%) African American, 338 (0.4%) Native American, 8,053 (9.0%) Asian, 124 (0.1%) Pacific Islander, 4,047 (4.5%) from other races, and 3,985 (4.4%) from two or more races. Hispanic or Latino of any race were 11,716 persons (13.1%).

The Census reported 87,610 people (97.6% of the population) lived in households, 1,299 (1.4%) lived in non-institutionalized group quarters, and 827 (0.9%) were institutionalized.

There were 46,917 households, out of which 7,835 (16.7%) had children under the age of 18 living in them, 13,092 (27.9%) were opposite-sex married couples living together, 3,510 (7.5%) had a female householder with no husband present, 1,327 (2.8%) had a male householder with no wife present. There were 2,867 (6.1%) unmarried opposite-sex partnerships, and 416 (0.9%) same-sex married couples or partnerships. 22,716 households (48.4%) were made up of individuals and 5,551 (11.8%) had someone living alone who was 65 years of age or older. The average household size was 1.87. There were 17,929 families (38.2% of all households); the average family size was 2.79.

The population was spread out with 12,580 people (14.0%) under the age of 18, 6,442 people (7.2%) aged 18 to 24, 32,552 people (36.3%) aged 25 to 44, 24,746 people (27.6%) aged 45 to 64, and 13,416 people (15.0%) who were 65 years of age or older. The median age was 40.4 years. For every 100 females, there were 93.2 males. For every 100 females age 18 and over, there were 91.2 males.

There were 50,912 housing units at an average density of 6,049.5 per square mile (2,335.7/km²), of which 13,315 (28.4%) were owner-occupied, and 33,602 (71.6%) were occupied by renters. The homeowner vacancy rate was 1.1%; the rental vacancy rate was 5.1%. 30,067 people (33.5% of the population) lived in owner-occupied housing units and 57,543 people (64.1%) lived in rental housing units.

According to the 2010 United States Census, Santa Monica had a median household income of $73,649, with 11.2% of the population living below the federal poverty line.

As of the census of 2000, there are 84,084 people, 44,497 households, and 16,775 families in the city. The population density is 10,178.7 inhabitants per square mile (3,930.4/km²). There are 47,863 housing units at an average density of 5,794.0 per square mile (2,237.3/km²). The racial makeup of the city is 78.29% White, 7.25% Asian, 3.78% African American, 0.47% Native American, 0.10% Pacific Islander, 5.97% from other races, and 4.13% from two or more races. 13.44% of the population are Hispanic or Latino of any race.
There are 44,497 households, out of which 15.8% have children under the age of 18, 27.5% are married couples living together, 7.5% have a female householder with no husband present, and 62.3% are non-families. 51.2% of all households are made up of individuals and 10.6% have someone living alone who is 65 years of age or older. The average household size is 1.83 and the average family size is 2.80.

The city of Santa Monica is consistently among the most educated cities in the United States, with 23.8 percent of all residents holding graduate degrees.

The population is diverse in age, with 14.6% under 18, 6.1% from 18 to 24, 40.1% from 25 to 44, 24.8% from 45 to 64, and 14.4% 65 years or older. The median age is 39 years. For every 100 females, there are 93.0 males. For every 100 females age 18 and over, there are 91.3 males.

According to a 2009 estimate, the median income for a household in the city is $71,095, and the median income for a family is $109,410. Males have a median income of $55,689 versus $42,948 for females. The per capita income for the city is $42,874. 10.4% of the population and 5.4% of families are below the poverty line. Out of the total population, 9.9% of those under the age of 18 and 10.2% of those 65 and older are living below the poverty line.

In 2006, crime in Santa Monica affected 4.41% of the population, slightly lower than the national average crime rate that year of 4.48%.
The majority of this was property crime,
which affected 3.74% of Santa Monica's population in 2006;
this was higher than
the rates for Los Angeles County (2.76%)
and California (3.17%),
but lower than the national average (3.91%).
These per-capita crime rates are computed based on Santa Monica's full-time population of about 85,000.
However,
the Santa Monica Police Department has suggested the actual per-capita crime rate is much lower,
as tourists, workers, and beachgoers can increase the city's daytime population to between 250,000 and
450,000 people.

Violent crimes affected 0.67% of the population in Santa Monica in 2006,
in line with Los Angeles County (0.65%),
but higher than the averages for California (0.53%)
and the nation (0.55%).

Hate crime has typically been minimal in Santa Monica, with only one reported incident in 2007.
However, the city experienced a spike of anti-Islamic hate crime in 2001, following the attacks of September 11.
Hate crime levels returned to their minimal 2000 levels by 2002.

In 2006, Santa Monica voters passed "Measure Y" with a 65% majority,
which moved the issuance of citations for marijuana smoking to the bottom of the police priority list. A 2009 study by the Santa Monica Daily Press showed since the law took effect in 2007, the Santa Monica Police had
"not issued any citations for offenses involving the adult, personal use of marijuana inside private residences."

In June 2011, the Boston gangster Whitey Bulger was arrested in Santa Monica after being a fugitive for 16 years. He had been living in the area for 15 years.

A shooting in Santa Monica in 2013 left six (including the perpetrator) dead and five more injured.

The Pico neighborhood of Santa Monica (south of the Santa Monica Freeway) experiences some gang activity. The city estimates there are about 50 gang members based in Santa Monica, although some community organizers dispute this claim. Gang activity has been prevalent for decades in the Pico neighborhood.

In October 1998, alleged Culver City 13 gang member Omar Sevilla, 21, of Culver City was killed. A couple of hours after the shooting of Sevilla, German tourist Horst Fietze was killed. Several days later Juan Martin Campos, age 23, a Santa Monica city employee, was shot and killed. Police believe this was a retaliatory killing in response to the death of Omar Sevilla. Less than twenty-four hours later, Javier Cruz was wounded in a drive-by shooting outside his home on 17th and Michigan.

In 1999, there was a double homicide in the Westside Clothing store on Lincoln Boulevard. During the incident, Culver City gang members David "Puppet" Robles and Jesse "Psycho" Garcia entered the store masked and began opening fire, killing Anthony and Michael Juarez. They then ran outside to a getaway vehicle driven by a third Culver City gang member, who is now also in custody. The clothing store was believed to be a local hang out for Santa Monica gang members. The dead included two men from Northern California who had merely been visiting the store's owner, their cousin, to see if they could open a similar store in their area. Police say the incident was in retaliation for a shooting committed by the Santa Monica 13 gang days before the Juarez brothers were gunned down.

Aside from the rivalry with the Culver City gang, gang members also feud with the Venice and West Los Angeles gangs. The main rivals in these regions include Venice 13, Graveyard Gangster Crips, and Venice Shoreline Crips gangs in the Oakwood area of Venice, California.

The Santa Monica-Malibu Unified School District provides public education at the elementary and secondary levels. In addition to the traditional model of early education school houses, SMASH (Santa Monica Alternative School House) is "a K-8 public school of choice with team teachers and multi-aged classrooms".

The district maintains eight public elementary schools in Santa Monica:

The district maintains three public middle schools in Santa Monica: Johna Adams Middle School, Lincoln Middle School and SMASH.

The district maintains three high schools in Santa Monica: Olympic High School, Malibu High School and Santa Monica High School.

Private schools in the city include:

Asahi Gakuen, a weekend Japanese supplementary school system, operates its Santa Monica campus (サンタモニカ校･高等部 "Santamonika-kō kōtōbu") at Webster Middle in the Sawtelle neighborhood of Los Angeles. All high school classes in the Asahi Gakuen system are held at the Santa Monica campus. As of 1986 students take buses from as far away as Orange County to go to the high school classes of the Santa Monica campus.

Santa Monica College is a community college founded in 1929. Many SMC graduates transfer to the University of California system. It occupies 35 acres (14 hectares) and enrolls 30,000 students annually. The Frederick S. Pardee RAND Graduate School, associated with the RAND Corporation, is the U.S.'s largest producer of public policy PhDs. The Art Institute of California – Los Angeles is also in Santa Monica near the Santa Monica Airport.
Universities and colleges within a radius from Santa Monica include Santa Monica College, Antioch University Los Angeles, Loyola Marymount University, Mount St. Mary's University, Pepperdine University, California State University, Northridge, California State University, Los Angeles, UCLA, USC, West Los Angeles College, California Institute of Technology (Caltech), Occidental College (Oxy), Los Angeles City College, Los Angeles Southwest College, Los Angeles Valley College, and Emperor's College of Traditional Oriental Medicine.

The Santa Monica Public Library consists of a Main Library in the downtown area, plus four neighborhood branches: Fairview, Montana Avenue, Ocean Park, and Pico Boulevard.

Santa Monica has a bike action plan and recently launched a bicycle sharing system in November 2015. The city is traversed by the Marvin Braude Bike Trail. Santa Monica has received the Bicycle Friendly Community Award (Bronze in 2009, Silver in 2013) by the League of American Bicyclists. Local bicycle advocacy organizations include Santa Monica Spoke, a local chapter of the Los Angeles County Bicycle Coalition. Santa Monica is thought to be one of the leaders for bicycle infrastructure and programming in Los Angeles County.

In terms of number of bicycle accidents, Santa Monica ranks as one of the worst (#2) out of 102 California cities with population 50,000–100,000, a ranking consistent with the city's composite ranking.
In 2007 and 2008, local police cracked down on Santa Monica Critical Mass rides that had become controversial, putting a damper on the tradition.
The Santa Monica Freeway (Interstate 10) begins in Santa Monica near the Pacific Ocean and heads east. The Santa Monica Freeway between Santa Monica and downtown Los Angeles has the distinction of being one of the busiest highways in all of North America. After traversing Los Angeles County, I-10 crosses seven more states, terminating at Jacksonville, Florida. In Santa Monica, there is a road sign designating this route as the Christopher Columbus Transcontinental Highway. State Route 2 (Santa Monica Boulevard) begins in Santa Monica, barely grazing State Route 1 at Lincoln Boulevard, and continues northeast across Los Angeles County, through the Angeles National Forest, crossing the San Gabriel Mountains as the Angeles Crest Highway, ending in Wrightwood. Santa Monica is also the western (Pacific) terminus of historic U.S. Route 66. Close to the eastern boundary of Santa Monica, Sepulveda Boulevard reaches from Long Beach at the south, to the northern end of the San Fernando Valley. Just east of Santa Monica is Interstate 405, the "San Diego Freeway", a major north-south route in Los Angeles County and Orange County, California.

The City of Santa Monica has purchased the first ZeroTruck all-electric medium-duty truck. The vehicle will be equipped with a Scelzi utility body, it is based on the Isuzu N series chassis, a UQM PowerPhase 100 advanced electric motor and is the only US built electric truck offered for sale in the United States in 2009.

The city of Santa Monica runs its own bus service, the Big Blue Bus, which also serves much of West Los Angeles and the University of California, Los Angeles (UCLA). A Big Blue Bus was featured prominently in the action movie "Speed".

The city of Santa Monica is also served by the Los Angeles County Metropolitan Transportation Authority's (Metro) bus lines. Metro also complements Big Blue service, as when Big Blue routes are not operational overnight, Metro buses make many Big Blue Bus stops, in addition to MTA stops.

Design and construction on the of the Expo Line from Culver City to Santa Monica started in September 2011, with service beginning on May 20, 2016. Santa Monica Metro stations include 26th Street/Bergamot, 17th Street/Santa Monica College, and Downtown Santa Monica. Travel time between the downtown Santa Monica and the downtown Los Angeles termini is approximately 47 minutes.

Historical aspects of the Expo line route are noteworthy. It uses the former Los Angeles region's electric interurban Pacific Electric Railway's right-of-way that ran from the Exposition Park area of Los Angeles to Santa Monica. This route was called the Santa Monica Air Line and provided electric-powered freight and passenger service between Los Angeles and Santa Monica beginning in the 1920s. Passenger service was discontinued in 1953, but diesel-powered freight deliveries to warehouses along the route continued until March 11, 1988. The abandonment of the line spurred future transportation considerations and concerns within the community, and the entire right-of-way was purchased from Southern Pacific by Los Angeles Metropolitan Transportation Authority. The line was built in 1875 as the steam-powered Los Angeles and Independence Railroad to bring mining ore to ships in Santa Monica harbor and as a passenger excursion train to the beach.

Since the mid-1980s, various proposals have been made to extend the Purple Line subway to Santa Monica under Wilshire Boulevard. There are no current plans to complete the "subway to the sea," an estimated $5 billion project.

The city owns and operates a general aviation airport, Santa Monica Airport, which has been the site of several important aviation achievements. Commercial flights are available for residents at Los Angeles International Airport, a few miles south of Santa Monica.

Like other cities in Los Angeles County, Santa Monica is dependent upon the Port of Long Beach and the Port of Los Angeles for international ship cargo. In the 1890s, Santa Monica was once in competition with Wilmington, California, and San Pedro for recognition as the "Port of Los Angeles" (see History of Santa Monica, California).

Two major hospitals are within the Santa Monica city limits, UCLA Santa Monica Hospital and St. John's Hospital. There are four fire stations providing medical and fire response within the city staffed with 6 Paramedic Engines, 1 Truck company, 1 Hazardous Materials team and 1 Urban Search & Rescue team. Santa Monica Fire Department has its own Dispatch Center. Ambulance transportation is provided by AmeriCare Ambulance Services.

Law enforcement services is provided by the Santa Monica Police Department

The Los Angeles County Department of Health Services operates the Simms/Mann Health and Wellness Center in Santa Monica. The Department's West Area Health Office is in the Simms/Mann Center.

Santa Monica has a municipal wireless network which provides several free city Wi-Fi hotspots distributed around the City.

Santa Monica is governed by the Santa Monica City Council, a Council-Manager governing body with seven members elected at-large. Currently, the mayor is Ted Winterer, and the Mayor Pro Tempore is Gleam Davis. The other five current council members are Sue Himmelrich, Kevin McKeown, Tony Vazquez, Pam O'Connor, and Terry O'Day.

In the California State Legislature, Santa Monica is in , and in .

In the United States House of Representatives, Santa Monica is in .

Santa Monica is home to the headquarters of many notable businesses, including Hulu, Universal Music Group, Illumination Entertainment, Saban Capital Group, Miramax, Lionsgate Films, the RAND Corporation, Beachbody, and Macerich. Atlantic Aviation is at the Santa Monica Airport. National Public Radio member station KCRW is at the Santa Monica College campus.

A number of game development studios are based in Santa Monica, making it a major location for the industry. These include:

Fatburger's headquarters are in Santa Monica. TOMS Shoes has its headquarters in Santa Monica.

Former Santa Monica businesses include Douglas Aircraft (now merged with Boeing), MySpace (now headquartered in Beverly Hills), and Metro-Goldwyn-Mayer. In December 1996, GeoCities was headquartered on the third floor of 1918 Main Street in Santa Monica.

Santa Monica has a strong small business community; Fundera ranked the city the 6th best city for small business in a 2016 study.

Recently, Santa Monica has emerged as the center of the Los Angeles region called Silicon Beach, and serves as the home of hundreds of venture-capital funded startup companies.

According to the City's 2012–2013 Comprehensive Annual Financial Report, the top employers in the city are:

The men's and women's marathon ran through parts of Santa Monica during the 1984 Summer Olympics. The Santa Monica Track Club has many prominent track athletes, including many Olympic gold medalists. Santa Monica is the home to Southern California Aquatics, which was founded by Olympic swimmer Clay Evans and Bonnie Adair. Santa Monica is also home to the Santa Monica Rugby Club, a semi-professional team that competes in the Pacific Rugby Premiership, the highest-level rugby union club competition in the United States.

During the 2028 Summer Olympics. Santa Monica will host beach volleyball and surfing.

Hundreds of movies have been shot or set in part within the city of Santa Monica. One of the oldest exterior shots in Santa Monica is Buster Keaton's "Spite Marriage" (1929) which shows much of 2nd Street. The comedy "It's a Mad, Mad, Mad, Mad World" (1963) included several scenes shot in Santa Monica, including those along the California Incline, which led to the movie's treasure spot, "The Big W". The Sylvester Stallone film "Rocky III" (1982) shows Rocky Balboa and Apollo Creed training to fight Clubber Lang by running on the Santa Monica Beach, and Stallone's "Demolition Man" (1993) includes Santa Monica settings. Henry Jaglom's indie "Someone to Love" (1987), the last film in which Orson Welles appeared, takes place in Santa Monica's venerable Mayfair Theatre. "Heathers" (1989) used Santa Monica's John Adams Middle School for many exterior shots. "The Truth About Cats & Dogs" (1996) is set entirely in Santa Monica, particularly the Palisades Park area, and features a radio station that resembles KCRW at Santa Monica College. "17 Again" (2009) was shot at Samohi. Other films that show significant exterior shots of Santa Monica include "Fletch" (1985), "Species" (1995), "Get Shorty" (1995), and "Ocean's Eleven" (2001). Richard Rossi's biopic "Aimee Semple McPherson" opens and closes at the beach in Santa Monica. "Iron Man" features the Santa Monica pier and surrounding communities as Tony Stark tests his experimental flight suit.

The documentary "Dogtown and Z-Boys" (2001) and the related dramatic film "Lords of Dogtown" (2005) are both about the influential skateboarding culture of Santa Monica's Ocean Park neighborhood in the 1970s.

The Santa Monica Pier is shown in many films, including "They Shoot Horses, Don't They?" (1969), "The Sting" (1973), "Ruthless People" (1986), "Beverly Hills Cop III" (1994), "Clean Slate" (1994), "Forrest Gump" (1994), "The Net" (1995), "Love Stinks" (1999), "Cellular" (2004), "" (2006), "Iron Man" (2008) and "" (2009).

A number of television series have been set in Santa Monica, including "Baywatch", "Three's Company", "Pacific Blue", and "Private Practice". The Santa Monica pier is shown in the main theme of CBS series "". In "Buffy the Vampire Slayer", the main exterior set of the town of Sunnydale, including the infamous "sun sign", was in Santa Monica in a lot on Olympic Boulevard.

The films "The Doors" (1991) and "Speed" (1994) featured vehicles from Santa Monica's Big Blue Bus line, relative to the eras depicted in the films.

The city of Santa Monica (and in particular the Santa Monica Airport) was featured in Roland Emmerich's disaster film "2012" (2009). A magnitude 10.9 earthquake destroys the airport and the surrounding area as a group of survivors escape in a personal plane. The Santa Monica Pier and the whole city sinks into the Pacific Ocean after the earthquake.

Raymond Chandler's most famous character, private detective Philip Marlowe, frequently has a portion of his adventures in a place called "Bay City", which is modeled on depression-era Santa Monica. In Marlowe's world, Bay City is "a wide-open town", where gambling and other crimes thrive due to a massively corrupt and ineffective police force.

The main character from Edgar Rice Burroughs' "The Land That Time Forgot" was a shipbuilder from Santa Monica.

In "Al Capone Does My Shirts", the Flanagans move to Alcatraz from Santa Monica.

Tennessee Williams lived (while working at MGM Studios) in a hotel on Ocean Avenue in the 1940s. At that location he wrote The Glass Menagerie. His short story titled "" was set near Santa Monica Beach, and mentions the clock visible in much of the city, high up on The Broadway Building, on Broadway near 2nd Street.


Santa Monica is featured in the video games
"Driver" (1999), "" (2003), "" (2004), "Grand Theft Auto San Andreas" (2004) as a fictional district - Santa Maria Beach, " Destroy All Humans! " (2004), "Tony Hawk's American Wasteland" (2005), "L.A. Rush" (2005), "" (2008), "Cars Race-O-Rama" (2009) as a fictional city - Santa Carburera, "Grand Theft Auto V" (2013) as a fictional district – Del Perro, "" (2013) as a fictional U.S. military base – Fort Santa Monica, "The Crew" (2014) and "Need for Speed" (2015).




</doc>
<doc id="28209" url="https://en.wikipedia.org/wiki?curid=28209" title="Shot put">
Shot put

The shot put is a track and field event involving "throwing"/"putting" (throwing in a pushing motion) a heavy spherical object—the "shot"—as far as possible. The shot put competition for men has been a part of the modern Olympics since their revival in 1896, and women's competition began in 1948.

Homer mentions competitions of rock throwing by soldiers during the Siege of Troy but there is no record of any dead weights being thrown in Greek competitions. The first evidence for stone- or weight-throwing events were in the Scottish Highlands, and date back to approximately the first century. In the 16th century King Henry VIII was noted for his prowess in court competitions of weight and hammer throwing.

The first events resembling the modern shot put likely occurred in the Middle Ages when soldiers held competitions in which they hurled cannonballs. Shot put competitions were first recorded in early 19th century Scotland, and were a part of the British Amateur Championships beginning in 1866.

Competitors take their throw from inside a marked circle in diameter, with a stopboard about high at the front of the circle. The distance thrown is measured from the inside of the circumference of the circle to the nearest mark made in the ground by the falling shot, with distances rounded down to the nearest centimetre under IAAF and WMA rules.

The following rules (indoor and outdoor) are adhered to for a legal throw:

Foul throws occur when an athlete:
At any time if the shot loses contact with the neck then it is technically an illegal put.

The following are either obsolete or non-existent, but commonly believed rules within professional competition:

Shot put competitions have been held at the modern Summer Olympic Games since their inception in 1896, and it is also included as an event in the World Athletics Championships.

Each competition has a set number of rounds of throws. Typically there are three preliminary rounds to determine qualification for the final, and then three more rounds in the final. Each competitor is credited with their longest throw, regardless of whether it was achieved in the preliminary or final rounds. The competitor with the longest legal put is declared the winner.

In open competitions the men's shot weighs , and the women's shot weighs . Junior, school, and masters competitions often use different weights of shots, typically below the weights of those used in open competitions; the individual rules for each competition should be consulted in order to determine the correct weights to be used.

Two putting styles are in current general use by shot put competitors: the "glide" and the "spin". With all putting styles, the goal is to release the shot with maximum forward velocity at an angle of approximately forty degrees.

The origin of this technique glide dates to 1951, when Parry O'Brien from the United States invented a technique that involved the putter facing backwards, rotating 180 degrees across the circle, and then tossing the shot. Unlike spin this technique it is a linear movement.

With this technique, a right-hand thrower would begin facing the rear of the circle, and then kick to the front with the left leg, while pushing off forcefully with the right. As the thrower crosses the circle, the hips twist toward the front, the left arm is swung out then pulled back tight, followed by the shoulders, and they then strike in a putting motion with their right arm. The key is to move quickly across the circle with as little air under the feet as possible, hence the name 'glide'.

Also known as rotational technique. It was first practiced in Europe in the 1950’s but did not receive much attention until the 1970’s. In 1972 Aleksandr Baryshnikov set his first USSR record using a new putting style, the spin ("круговой мах" in Russian), invented by his coach Viktor Alexeyev. The spin involves rotating like a discus thrower and using rotational momentum for power. In 1976 Baryshnikov went on to set a world record of with his spin style, and was the first shot putter to cross the 22-meter mark.

With this technique, a right-hand thrower faces the rear, and begins to spin on the ball of the left foot. The thrower comes around and faces the front of the circle and drives the right foot into the center of the circle. Finally, the thrower reaches for the front of the circle with the left foot, twisting the hips and shoulders like in the glide, and puts the shot.

When the athlete executes the spin, the upper body is twisted hard to the right, so the imaginary lines created by the shoulders and hips are no longer parallel. This action builds up torque, and stretches the muscles, creating an involuntary elasticity in the muscles, providing extra power and momentum. When the athlete prepares to release, the left foot is firmly planted, causing the momentum and energy generated to be conserved, pushing the shot in an upward and outward direction.

Another purpose of the spin is to build up a high rotational speed, by swinging the right leg initially, then to bring all the limbs in tightly, similar to a figure skater bringing in their arms while spinning to increase their speed. Once this fast speed is achieved the shot is released, transferring the energy into the shot put.

Currently, most top male shot putters use the spin. However the glide remains popular since the technique leads to greater consistency compared to the rotational technique. Almost all throwers start by using the glide. Tomasz Majewski notes that although most athletes use the spin, he and some other top shot putters achieved success using this classic method (for example he became first to defend the Olympic title in 56 years).

The world record by a male putter of by Randy Barnes was completed with the spin technique, while the second-best all-time put of by Ulf Timmermann was completed with the glide technique.

The decision to glide or spin may need to be decided on an individual basis, determined by the thrower's size and power. Short throwers may benefit from the spin and taller throwers may benefit from the glide, but many throwers do not follow this guideline.

The shot is made of different kinds of materials depending on its intended use. Materials used include sand, iron, cast iron, solid steel, stainless steel, brass, and synthetic materials like polyvinyl. Some metals are more dense than others making the size of the shot vary. For example, different materials are used to make indoor and outdoor shot - because damage to surroundings must be taken into account - so the latter are smaller. There are various size and weight standards for the implement that depend on the age and gender of the competitors as well as the national customs of the governing body.

The current world record holders are:
The current records held on each continent are:








</doc>
<doc id="28211" url="https://en.wikipedia.org/wiki?curid=28211" title="Stan Kelly-Bootle">
Stan Kelly-Bootle

Stanley Bootle, known as Stan Kelly-Bootle (15 September 1929  – 16 April 2014), was a British author, academic, singer-songwriter and computer scientist. 

He took his stage name Stan Kelly (he was not known as Stan Kelly-Bootle in folk music circles) from the Irish folk song "Kelly, the boy from Killane". His best-known song is the "Liverpool Lullaby" or "The Mucky Kid" which was recorded in 1965 on the "Three City Four" LP and sung by Marian McKenzie. It was also sung by the Ian Campbell Folk Group on the "Contemporary Campbells" LP. It was later a song which Judy Collins recorded in 1966 for her album "In My Life". Cilla Black recorded it three years later as the B-side to her pop hit "Conversations". Kelly-Bootle achieved the first postgraduate degree in computer science in 1954, from the University of Cambridge.

Stan Kelly-Bootle was born Stanley Bootle in Liverpool, Lancashire, on 15 September 1929 and grew up in the Wavertree area of the city. His parents were Arthur Bootle and Ada Gallagher.

Kelly-Bootle was schooled at the Liverpool Institute. He spent 1948–1950 doing his national service in the British Army, achieving the rank of Sgt. Instructor in RADAR. He attended Downing College, Cambridge, graduating with a first class degree in Numerical Analysis and Automatic Computing in 1954, the first postgraduate degree in computer science.

In 1950, Kelly-Bootle helped found the St. Lawrence Folk Song Society at Cambridge University. As a folk singer-songwriter, he performed under the name Stan Kelly. He wrote some of his own tunes and also wrote lyrics set to traditional tunes. In the course of his musical career, he made over 200 radio and television appearances, and released several recordings, as well as having his songs recorded by others.

Solo releases include:

Other audio recordings include:

He started his computing career programming the pioneering EDSAC computer, designed and built at Cambridge University. He worked for IBM in the United States and the UK from 1955 to 1970. From 1970 to 1973, he worked as Manager for University Systems for Sperry-UNIVAC. He also lectured at the University of Warwick.

In 1973, Kelly-Bootle left Sperry-UNIVAC and became a freelance consultant, writer and programmer. He was known in the computer community for "The Devil's DP Dictionary" and its second edition, "The Computer Contradictionary" (1995), which he authored. These works are cynical lexicographies in the vein of Ambrose Bierce's "The Devil's Dictionary". Kelly-Bootle authored or co-authored several serious textbooks and tutorials on subjects such as the Motorola 68000 family of CPUs, programming languages including various C compilers, and the Unix operating system. He authored the "Devil's Advocate" column in "UNIX Review" from 1984 to 2000, and had columns in "Computer Language" ("Bit by Bit", 1989–1994), "OS/2 Magazine" ("End Notes", 1994–97) and "Software Development" ("Seamless Quanta", October 1995 – May 1997). He contributed columns and articles to several other computer industry magazines, as well.
Kelly-Bootle's articles for magazines such as "ACM Queue", "AI/Expert", and "UNIX Review" contain examples of word-play, criticism of silly marketing and usage (he refers often to the computer "laxicon"), and commentary on the industry in general. He wrote an online monthly column posted on the Internet. While most of his writing was oriented towards the computer industry, he wrote a few books relating to his other interests, including 

Stan Kelly-Bootle died on 16 April 2014, aged 84, in hospital in Oswestry, Shropshire.



</doc>
<doc id="28212" url="https://en.wikipedia.org/wiki?curid=28212" title="Skewness">
Skewness

In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive or negative, or undefined.

The qualitative interpretation of the skew is complicated and unintuitive. Skew does not refer to the direction the curve appears to be leaning; in fact, the opposite is true. For a unimodal distribution, negative skew indicates that the "tail" on the left side of the probability density function is longer or fatter than the right side – it does not distinguish these two kinds of shape. Conversely, positive skew indicates that the tail on the right side is longer or fatter than the left side. In cases where one tail is long but the other tail is fat, skewness does not obey a simple rule. For example, a zero value means that the tails on both sides of the mean balance out overall; this is the case for a symmetric distribution, but is also true for an asymmetric distribution where the asymmetries even out, such as one tail being long but thin, and the other being short but fat. Further, in multimodal distributions and discrete distributions, skewness is also difficult to interpret. Importantly, the skewness does not determine the relationship of mean and median. In cases where it is necessary, data might be transformed to have a normal distribution.

Consider the two distributions in the figure just below. Within each graph, the values on the right side of the distribution taper differently from the values on the left side. These tapering sides are called "tails", and they provide a visual means to determine which of the two kinds of skewness a distribution has:


Skewness in a data series may sometimes be observed not only graphically but by simple inspection of the values. For instance, consider the numeric sequence (49, 50, 51), whose values are evenly distributed around a central value of 50. We can transform this sequence into a negatively skewed distribution by adding a value far below the mean, e.g. (40, 49, 50, 51). Similarly, we can make the sequence positively skewed by adding a value far above the mean, e.g. (49, 50, 51, 60).

The skewness is not directly related to the relationship between the mean and median: a distribution with negative skew can have its mean greater than or less than the median, and likewise for positive skew.

In the older notion of nonparametric skew, defined as formula_1 where formula_2 is the mean, formula_3 is the median, and formula_4 is the standard deviation, the skewness is defined in terms of this relationship: positive/right nonparametric skew means the mean is greater than (to the right of) the median, while negative/left nonparametric skew means the mean is less than (to the left of) the median. However, the modern definition of skewness and the traditional nonparametric definition do not in general have the same sign: while they agree for some families of distributions, they differ in general, and conflating them is misleading.

If the distribution is symmetric, then the mean is equal to the median, and the distribution has zero skewness. If the distribution is both symmetric and unimodal, then the mean = median = mode. This is the case of a coin toss or the series 1,2,3,4... Note, however, that the converse is not true in general, i.e. zero skewness does not imply that the mean is equal to the median.

Paul T. von Hippel points out: "Many textbooks, teach a rule of thumb stating that the mean is right of the median under right skew, and left of the median under left skew. This rule fails with surprising frequency. It can fail in multimodal distributions, or in distributions where one tail is long but the other is heavy. Most commonly, though, the rule fails in discrete distributions where the areas to the left and right of the median are not equal. Such distributions not only contradict the textbook relationship between mean, median, and skew, they also contradict the textbook interpretation of the median."

The skewness of a random variable "X" is the third standardized moment "γ", defined as:
where "μ" is the mean, "σ" is the standard deviation, E is the expectation operator, "μ" is the third central moment, and "κ" are the t cumulants. It is sometimes referred to as Pearson's moment coefficient of skewness, or simply the moment coefficient of skewness, but should not be confused with Pearson's other skewness statistics (see below). The last equality expresses skewness in terms of the ratio of the third cumulant "κ" to the 1.5th power of the second cumulant "κ". This is analogous to the definition of kurtosis as the fourth cumulant normalized by the square of the second cumulant. 
The skewness is also sometimes denoted Skew["X"].

Skewness can be expressed in terms of the non-central moment E["X"] by expanding the previous formula,
Skewness can be infinite, as when
where the third cumulants are infinite, or as when
where the third cumulant is undefined.

Starting from a standard cumulant expansion around a normal distribution, one can show that

If "Y" is the sum of "n" independent and identically distributed random variables, all with the distribution of "X", then the third cumulant of "Y" is "n" times that of "X" and the second cumulant of "Y" is "n" times that of "X", so formula_9. This shows that the skewness of the sum is smaller, as it approaches a Gaussian distribution in accordance with the central limit theorem. Note that the assumption that the variables be independent for the above formula is very important because it is possible even for the sum of two Gaussian variables to have a skewed distribution (see this example).

For a sample of "n" values, a natural method of moments estimator of the population skewness is

where formula_11 is the sample mean, "s" is the sample standard deviation, and the numerator "m" is the sample third central moment.

Another common definition of the "sample skewness" is

formula_12

where formula_13 is the unique symmetric unbiased estimator of the third cumulant and formula_14 is the symmetric unbiased estimator of the second cumulant (i.e. the variance).

In general, the ratios formula_15 and formula_16 are both biased estimators of the population skewness formula_17; their expected values can even have the opposite sign from the true skewness. (For instance, a mixed distribution consisting of very thin Gaussians centred at −99, 0.5, and 2 with weights 0.01, 0.66, and 0.33 has a skewness of about −9.77, but in a sample of 3, formula_16 has an expected value of about 0.32, since usually all three samples are in the positive-valued part of the distribution, which is skewed the other way.) Nevertheless, formula_15 and formula_16 each have obviously the correct expected value of zero for any symmetric distribution with a finite third moment, including a normal distribution.

Under the assumption that the underlying random variable formula_21 is normally distributed, it can be shown that formula_22. The variance of the skewness of a random sample of size "n" from a normal distribution is

An approximate alternative is 6/"n", but this is inaccurate for small samples.

In normal samples, formula_15 has the smaller variance of the two estimators, with

where "m" in the denominator is the (biased) sample second central moment.

The adjusted Fisher–Pearson standardized moment coefficient formula_26 is the version found in Excel and several statistical packages including Minitab, SAS and SPSS.

Aside from indicating which direction and a relative magnitude of how far a distribution deviates from normal, skewness itself offers few analytical insights; aside from calculating it for its own sake, it is rarely used as a variable in further calculations. 

Many models assume normal distribution; i.e., data are symmetric about the mean. The normal distribution has a skewness of zero. But in reality, data points may not be perfectly symmetric. So, an understanding of the skewness of the dataset indicates whether deviations from the mean are going to be positive or negative.

D'Agostino's K-squared test is a goodness-of-fit normality test based on sample skewness and sample kurtosis.

Other measures of skewness have been used, including simpler calculations suggested by Karl Pearson (not to be confused with Pearson's moment coefficient of skewness, see above). These other measures are:

The Pearson mode skewness, or first skewness coefficient, is defined as

The Pearson median skewness, or second skewness coefficient, is defined as

The latter is a simple multiple of the nonparametric skew.

Bowley's measure of skewness (from 1901), also called Yule's coefficient (from 1912) is defined as: 
When writing it as formula_28, it is easier to see that the numerator is the average of the upper and lower quartiles (a measure of location) minus the median while the denominator is (Q3-Q1)/2 which (for symmetric distributions) is the MAD measure of dispersion.

Other names for this measure are Galton's measure of skewness, the Yule–Kendall index and the quartile skewness ,

A more general formulation of a skewness function was described by Groeneveld, R. A. and Meeden, G. (1984):
where "F" is the cumulative distribution function. This leads to a corresponding overall measure of skewness defined as the supremum of this over the range 1/2 ≤ "u" < 1. Another measure can be obtained by integrating the numerator and denominator of this expression. The function "γ"("u") satisfies −1 ≤ "γ"("u") ≤ 1 and is well defined without requiring the existence of any moments of the distribution.

Bowley's measure of skewness is γ("u") evaluated at "u" = 3/4. Kelley's measure of skewness uses "u" = 0.1.

Groeneveld & Meeden have suggested, as an alternative measure of skewness,

where "μ" is the mean, "ν" is the median, |…| is the absolute value, and "E"() is the expectation operator. This is closely related in form to Pearson's second skewness coefficient.

Use of L-moments in place of moments provides a measure of skewness known as the L-skewness.

A value of skewness equal to zero does not imply that the probability distribution is symmetric. Thus there is a need for another measure of asymmetry that has this property: such a measure was introduced in 2000. It is called distance skewness and denoted by dSkew. If "X" is a random variable taking values in the "d"-dimensional Euclidean space, "X" has finite expectation, "X" is an independent identically distributed copy of "X", and formula_31 denotes the norm in the Euclidean space, then a simple "measure of asymmetry" with respect to location parameter θ is
and dSkew("X") := 0 for "X" = θ (with probability 1). Distance skewness is always between 0 and 1, equals 0 if and only if "X" is diagonally symmetric with respect to θ ("X" and θ −"X" have the same probability distribution) and equals 1 if and only if X is a constant "c" (formula_33) with probability one. Thus there is a simple consistent statistical test of diagonal symmetry based on the sample distance skewness:

The medcouple is a scale-invariant robust measure of skewness, with a breakdown point of 25%. It is the median of the values of the kernel function
taken over all couples formula_36 such that formula_37, where formula_38 is the median of the sample formula_39. It can be seen as the median of all possible quantile skewness measures.


Adjusting the Tests for Skewness and Kurtosis for Distributional Misspecifications. Working Paper Number 01-0116, University of Illinois. Forthcoming in Comm in Statistics, Simulation and Computation.2016 1-15





</doc>
<doc id="28215" url="https://en.wikipedia.org/wiki?curid=28215" title="Saint Columba (disambiguation)">
Saint Columba (disambiguation)

Saint Columba may refer to:






</doc>
<doc id="28217" url="https://en.wikipedia.org/wiki?curid=28217" title="Serial Experiments Lain">
Serial Experiments Lain

The series focuses on Lain Iwakura, an adolescent middle school girl living in suburban Japan, and her introduction to the Wired, a global communications network which is similar to the Internet. Lain lives with her middle-class family, which consists of her inexpressive older sister Mika, her emotionally distant mother, and her computer-obsessed father; while Lain herself is somewhat awkward, introverted, and socially isolated from most of her school peers. But the status-quo of her life becomes upturned by a series of bizarre incidents that start to take place after she learns that girls from her school have received an e-mail from a dead student, Chisa Yomoda, and she pulls out her old computer in order to check for the same message. Lain finds Chisa telling her that she is not dead, but has merely "abandoned her physical body and flesh" and is alive deep within the virtual reality-world of the Wired itself, where she has found the almighty and divine "God". From this point, Lain is caught up in a series of cryptic and surreal events that see her delving deeper into the mystery of the network in a narrative that explores themes of consciousness, perception, and the nature of reality.

The "Wired" is a virtual reality-world that contains and supports the very sum of "all" human communication and networks, created with the telegraph, televisions, and telephone services, and expanded with the Internet, cyberspace, and subsequent networks. The series assumes that the Wired could be linked to a system that enables unconscious communication between people and machines without physical interface. The storyline introduces such a system with the Schumann resonances, a property of the Earth's magnetic field that theoretically allows for unhindered long distance communications. If such a link were created, the network would become equivalent to Reality as the general consensus of all perceptions and knowledge. The increasingly thin invisible line between what is real and what is virtual/digital begins to slowly shatter.

Masami Eiri is introduced as the project director on Protocol Seven (the next-generation Internet protocol in the series' time-frame) for major computer company Tachibana General Laboratories. He had secretly included code of his very own creation to give himself absolute control of the Wired through the wireless system described above. He then "uploaded" his own brain, conscience, consciousness, memory, feelings, emotions – his very self – into the Wired and "died" a few days after, leaving only his physical, living body behind. These details are unveiled around the middle of the series, but this is the point where the story of "Serial Experiments Lain" begins. Masami later explains that Lain is the artifact by which the wall between the virtual and material worlds is to fall, and that he needs her to get to the Wired and "abandon the flesh", as he did, to achieve his plan. The series sees him trying to convince her through interventions, using the promise of unconditional love, romantic seduction and charm, and even, when all else fails, threats and force.

In the meantime, the anime follows a complex game of hide-and-seek between the "Knights of the Eastern Calculus", hackers whom Masami claims are "believers that enable him to be a God in the Wired", and Tachibana General Laboratories, who try to regain control of Protocol Seven. In the end, the viewer sees Lain realizing, after much introspection, that she has absolute control over everyone's mind and over reality itself. Her dialogue with different versions of herself shows how she feels shunned from the material world, and how she is afraid to live in the Wired, where she has the possibilities and responsibilities of an almighty goddess. The last scenes feature her erasing everything connected to herself from everyone's memories. She is last seen, unchanged, encountering her oldest and closest friend Alice once again, who is now married. Lain promises herself that she and Alice will surely meet again anytime as Lain can literally go and be anywhere she desires between both worlds.

"Serial Experiments Lain" was conceived, as a series, to be original to the point of it being considered "an enormous risk" by its producer Yasuyuki Ueda.

Producer Ueda had to answer repeated queries about a statement made in an "Animerica" interview. The controversial statement said "Lain" was "a sort of cultural war against American culture and the American sense of values we <nowiki>[Japan]</nowiki> adopted after World War II". He later explained in numerous interviews that he created "Lain" with a set of values he took as distinctly Japanese; he hoped Americans would not understand the series as the Japanese would. This would lead to a "war of ideas" over the meaning of the anime, hopefully culminating in new communication between the two cultures. When he discovered that the American audience held the same views on the series as the Japanese, he was disappointed.

The "Lain" franchise was originally conceived to connect across forms of media (anime, video games, manga). Producer Yasuyuki Ueda said in an interview, "the approach I took for this project was to communicate the essence of the work by the total sum of many media products". The scenario for the video game was written first, and the video game was produced at the same time as the anime series, though the series was released first. A dōjinshi titled "The Nightmare of Fabrication" was produced by Yoshitoshi ABe and released in Japanese in the artbook "Omnipresence in the Wired". Ueda and Konaka declared in an interview that the idea of a multimedia project was not unusual in Japan, as opposed to the contents of "Lain", and the way they are exposed.

The authors were asked in interviews if they had been influenced by "Neon Genesis Evangelion", in the themes and graphic design. This was strictly denied by writer Chiaki J. Konaka in an interview, arguing that he had not seen "Evangelion" until he finished the fourth episode of "Lain". Being primarily a horror movies writer, his stated influences are Godard (especially for using typography on screen), "The Exorcist", "Hell House", and Dan Curtis's "House of Dark Shadows". Alice's name, like the names of her two friends Julie and Reika, came from a previous production from Konaka, "Alice in Cyberland", which in turn was largely influenced by "Alice in Wonderland". As the series developed, Konaka was "surprised" by how close Alice's character became to the original "Wonderland" character.

Vannevar Bush (and memex), John C. Lilly, Timothy Leary and his eight-circuit model of consciousness, Ted Nelson and Project Xanadu are cited as precursors to the Wired. Douglas Rushkoff and his book "Cyberia" were originally to be cited as such, and in "Lain" Cyberia became the name of a nightclub populated with hackers and techno-punk teenagers. Likewise, the series' "deus ex machina" lies in the conjunction of the Schumann resonances and Jung's collective unconscious (the authors chose this term over Kabbalah and Akashic Record). Majestic 12 and the Roswell UFO incident are used as examples of how a hoax might still affect history, even after having been exposed as such, by creating sub-cultures. This links again to Vannevar Bush, the alleged "brains" of MJ12. Two of the literary references in "Lain" are quoted through Lain's father: he first logs onto a website with the password "Think Bule Count One Tow" ("Think Blue, Count Two" is an Instrumentality of Man story featuring virtual persons projected as real ones in people's minds); and his saying that "madeleines would be good with the tea" in the last episode makes "Lain" "perhaps the only cartoon to allude to Proust".

Yoshitoshi ABe confesses to have never read manga as a child, as it was "off-limits" in his household. His major influences are "nature and everything around him". Specifically speaking about Lain's character, ABe was inspired by Kenji Tsuruta, Akihiro Yamada, Range Murata and Yukinobu Hoshino. In a broader view, he has been influenced in his style and technique by Japanese artists Chinai-san and Tabuchi-san.

The character design of Lain was not ABe's sole responsibility. Her distinctive left forelock for instance was a demand from Yasuyuki Ueda. The goal was to produce asymmetry to reflect Lain's unstable and disconcerting nature. It was designed as a mystical symbol, as it is supposed to prevent voices and spirits from being heard by the left ear. The bear pajamas she wears were a demand from character animation director Takahiro Kishida. Though bears are a trademark of the Konaka brothers, Chiaki Konaka first opposed the idea. Director Nakamura then explained how the bear motif could be used as a shield for confrontations with her family. It is a key element of the design of the shy "real world" Lain (see "mental illness" under Themes). When she first goes to the Cyberia nightclub, she wears a bear hat for similar reasons. The pajamas were finally considered as possible fan-service by Konaka, in the way they enhance Lain's nymph aspect.

ABe's original design was generally more complicated than what finally appeared on screen. As an example, the X-shaped hairclip was to be an interlocking pattern of gold links. The links would open with a snap, or rotate around an axis until the moment the " X " became a " = ". This was not used as there is no scene where Lain takes her hairclip off.

"Serial Experiments Lain" is not a conventionally linear story, but "an alternative anime, with modern themes and realization". Themes range from theological to psychological and are dealt with in a number of ways: from classical dialogue to image-only introspection, passing by direct interrogation of imaginary characters.

Communication, in its wider sense, is one of the main themes of the series, not only as opposed to loneliness, but also as a subject in itself. Writer Konaka said he wanted to directly "communicate human feelings". Director Nakamura wanted to show the audience — and particularly viewers between 14 and 15—"the multidimensional wavelength of the existential self: the relationship between self and the world".

Loneliness, if only as representing a lack of communication, is recurrent through "Lain". Lain herself (according to Anime Jump) is "almost painfully introverted with no friends to speak of at school, a snotty, condescending sister, a strangely apathetic mother, and a father who seems to want to care but is just too damn busy to give her much of his time". Friendships turn on the first rumor; and the only insert song of the series is named "Kodoku no shigunaru", literally "signal of loneliness".

Mental illness, especially dissociative identity disorder, is a significant theme in "Lain": the main character is constantly confronted with alter-egos, to the point where writer Chiaki Konaka and Lain's voice actress Kaori Shimizu had to agree on subdividing the character's dialogues between three different orthographs. The three names designate distinct "versions" of Lain: the real-world, "childish" Lain has a shy attitude and bear pajamas. The "advanced" Lain, her Wired personality, is bold and questioning. Finally, the "evil" Lain is sly and devious, and does everything she can to harm Lain or the ones close to her. As a writing convention, the authors spelled their respective names in kanji, katakana, and roman characters (see picture).

Reality never has the pretense of objectivity in "Lain". Acceptations of the term are battling throughout the series, such as the "natural" reality, defined through normal dialogue between individuals; the material reality; and the tyrannic reality, enforced by one person onto the minds of others. A key debate to all interpretations of the series is to decide whether matter flows from thought, or the opposite. The production staff carefully avoided "the so-called God's Eye Viewpoint" to make clear the "limited field of vision" of the world of "Lain".

Theology plays its part in the development of the story too. "Lain" has been viewed as a questioning of the possibility of an infinite spirit in a finite body. From self-realization as a goddess to deicide, religion (the title of a layer) is an inherent part of "Lain" background.

"Lain" contains extensive references to Apple computers, as the brand was used at the time by most of the creative staff, such as writers, producers, and the graphical team. As an example, the title at the beginning of each episode is announced by the Apple computer speech synthesis program PlainTalk, using the voice ""Whisper"", e.g. codice_1. Tachibana Industries, the company that creates the NAVI computers, is a reference to Apple computers: "tachibana" means "Mandarin orange" in Japanese. NAVI is the abbreviation of Knowledge Navigator, and the HandiNAVI is based on the Apple Newton, one of the world's first PDAs. The NAVIs are seen to run "Copland OS Enterprise" (this reference to Copland was an initiative of Konaka, a declared Apple fan), and Lain's and Alice's NAVIs closely resembles the Twentieth Anniversary Macintosh and the iMac respectively. The HandiNAVI programming language, as seen on the seventh episode, is a dialect of Lisp. Notice that the Newton also used a Lisp dialect (NewtonScript). The program being typed by Lain can be found in the CMU AI repository; it is a simple implementation of Conway's Game of Life in Common Lisp.

During a series of disconnected images, an iMac and the Think Different advertising slogan appears for a short time, while the "Whisper" voice says it. This was an unsolicited insertion from the graphic team, also Mac-enthusiasts. Other subtle allusions can be found: "Close the world, Open the nExt" is the slogan for the "Serial Experiments Lain" video game. NeXT was the company that produced NeXTSTEP, which later evolved into Mac OS X after Apple bought NeXT. Another example is "To Be Continued." at the end of episodes 1–12, with a blue "B" and a red "e" on "Be": "this" "Be" is the original logo of Be Inc., a company founded by ex-Apple employees and NeXT's main competitor in its time.

"Serial Experiments Lain" was first aired on TV Tokyo on July 6, 1998 and concluded on September 28, 1998 with the thirteenth and final episode. The series consists of 13 episodes (referred to in the series as "Layers") of 24 minutes each, except for the sixth episode, "Kids" (23 minutes 14 seconds). In Japan, the episodes were released in LD, VHS, and DVD with a total of five volumes. A DVD compilation named ""Serial Experiments Lain DVD-BOX Яesurrection"" was released along with a promo DVD called ""LPR-309"" in 2000. As this box set is now discontinued, a rerelease was made in 2005 called ""Serial Experiments Lain TV-BOX"". A 4-volume DVD box set was released in the US by Pioneer/Geneon. A Blu-ray release of the anime was made in December 2009 called ""Serial Experiments Lain Blu-ray Box | RESTORE"". The anime series returned to US television on October 15, 2012 on the Funimation Channel.
The series' opening theme, "Duvet", was written and performed by Jasmine Rodgers and the British band Bôa. The ending theme, , was written and composed by Reichi Nakaido.

The anime series was licensed in North America by Pioneer Entertainment (later Geneon USA) on VHS, DVD and LaserDisc in 1999. However, the company closed its USA division in December 2007 and the series went out-of-print as a result. However, at Anime Expo 2010, North American distributor Funimation announced that it had obtained the license to the series and re-released it in 2012. It was also released in Singapore by Odex.


The first original soundtrack, "Serial Experiments Lain Soundtrack", features music by Reichi Nakaido: the ending theme and part of the television series' score. The series' opening theme, "Duvet", was written and performed in English by the British rock band Bôa. The second, "Serial Experiments Lain Soundtrack: Cyberia Mix", features electronica songs inspired by the television series, including a remix of the opening theme "Duvet". The third, "lain BOOTLEG", consists of two CDs with more than forty-five tracks, containing ambient music from the series. One of the CDs is a mixed-mode data and audio disk, containing a clock program and a game. It was released by Pioneer Records. Because the word "bootleg" appears in its title, it is easily confused with the Sonmay counterfeit edition of itself, which contains one CD of forty-five tracks, some of which are shorter than on the original.

On November 26, 1998, Pioneer LDC released a video game with the same name as the anime for the PlayStation. It was designed by Konaka and Yasuyuki, and made to be a "network simulator" in which the player would navigate to explore Lain's story. The creators themselves did not call it a game, but "Psycho-Stretch-Ware", and it has been described as being a kind of graphic novel: the gameplay is limited to unlocking pieces of information, and then reading/viewing/listening to them, with little or no puzzle needed to unlock. Lain distances itself even more from classical games by the random order in which information is collected. The aim of the authors was to let the player get the feeling that there are myriads of informations that they would have to sort through, and that they would have to do with less than what exists to understand. As with the anime, the creative team's main goal was to let the player "feel" Lain, and "to understand her problems, and to love her". A guidebook to the game called "Serial Experiments Lain Official Guide" () was released the same month by MediaWorks.

"Serial Experiments Lain" was first broadcast in Tokyo at 1:15 a.m. JST. The word "weird" appears almost systematically in English language reviews of the series, or the alternatives "bizarre", and "atypical", due mostly to the freedoms taken with the animation and its unusual science fiction themes, and due to its philosophical and psychological context. Critics responded positively to these thematic and stylistic characteristics, and it was awarded an Excellence Prize by the 1998 Japan Media Arts Festival for "its willingness to question the meaning of contemporary life" and the "extraordinarily philosophical and deep questions" it asks.

According to Christian Nutt from "Newtype USA", the main attraction to the series is its keen view on "the interlocking problems of identity and technology". Nutt saluted Abe's "crisp, clean character design" and the "perfect soundtrack" in his 2005 review of series, saying that ""Serial Experiments Lain" might not yet be considered a true classic, but it's a fascinating evolutionary leap that helped change the future of anime." "Anime Jump" gave it 4.5/5, and Anime on DVD gave it A+ on all criteria for volume 1 and 2, and a mix of A and A+ for volume 3 and 4.
"Lain" was subject to commentary in the literary and academic worlds. The "Asian Horror Encyclopedia" calls it "an outstanding psycho-horror anime about the psychic and spiritual influence of the Internet". It notes that the red spots present in all the shadows look like blood pools (see picture). It notes the death of a girl in a train accident is "a source of much ghost lore in the twentieth century", more so in Tokyo.

The "Anime Essentials" anthology by Gilles Poitras describes it as a "complex and somehow existential" anime that "pushed the envelope" of anime diversity in the 1990s, alongside the much better known "Neon Genesis Evangelion" and "Cowboy Bebop". Professor Susan J. Napier, in her 2003 reading to the American Philosophical Society called "The Problem of Existence in Japanese Animation" (published 2005), compared "Serial Experiments Lain" to "Ghost in the Shell" and Hayao Miyazaki's "Spirited Away". According to her, the main characters of the two other works cross barriers; they can cross back to our world, but Lain cannot. Napier asks whether there is something to which Lain should return, "between an empty 'real' and a dark 'virtual'". Mike Toole of Anime News Network named "Serial Experiments Lain" as one of the most important anime of the 1990s.

Unlike the anime, the video game drew little attention from the public. Criticized for its (lack of) gameplay, as well as for its "clunky interface", interminable dialogues, absence of music and very long loading times, it was nonetheless remarked for its (at the time) remarkable CG graphics, and its beautiful backgrounds.

Despite the positive feedback the television series had received, Anime Academy gave this series a 75%, partly due to the "lifeless" setting it had. Michael Poirier of "EX" magazine stated that the last three episodes fail to resolve the questions in other DVD volumes. Justin Sevakis of Anime News Network noted that the English dub was decent, but that the show relied so little on dialogue that it hardly mattered.





</doc>
<doc id="28219" url="https://en.wikipedia.org/wiki?curid=28219" title="Spontaneous emission">
Spontaneous emission

Spontaneous emission is the process in which a quantum mechanical system (such as an atom, molecule or subatomic particle) transitions from an excited energy state to a lower energy state (e.g., its ground state) and emits a quantum in the form of a photon. Spontaneous emission is ultimately responsible for most of the light we see all around us; it is so ubiquitous that there are many names given to what is essentially the same process. If atoms (or molecules) are excited by some means other than heating, the spontaneous emission is called luminescence. For example, fireflies are luminescent. And there are different forms of luminescence depending on how excited atoms are produced (electroluminescence, chemiluminescence etc.). If the excitation is affected by the absorption of radiation the spontaneous emission is called fluorescence. Sometimes molecules have a metastable level and continue to fluoresce long after the exciting radiation is turned off; this is called phosphorescence. Figurines that glow in the dark are phosphorescent. Lasers start via spontaneous emission, then during continuous operation work by stimulated emission.

Spontaneous emission cannot be explained by classical electromagnetic theory and is fundamentally a quantum process. The first person to derive the rate of spontaneous emission accurately from first principles was Dirac in his quantum theory of radiation, the precursor to the theory which he later coined quantum electrodynamics. Contemporary physicists, when asked to give a physical explanation for spontaneous emission, generally invoke the zero-point energy of the electromagnetic field. In 1963 the Jaynes-Cummings model was developed describing the system of a two-level atom interacting with a quantized field mode (i.e. the vacuum) within an optical cavity. It gave the nonintuitive prediction that the rate of spontaneous emission could be controlled depending on the boundary conditions of the surrounding vacuum field. These experiments gave rise to cavity quantum electrodynamics (CQED), the study of effects of mirrors and cavities on radiative corrections.

If a light source ('the atom') is in an excited state with energy formula_1, it may spontaneously decay to a lower lying level (e.g., the ground state) with energy formula_2, releasing the difference in energy between the two states as a photon. The photon will have angular frequency formula_3 and an energy formula_4:

where formula_6 is the reduced Planck constant. Note: formula_7, where formula_8 is the Planck constant and formula_9 is the linear frequency. The phase of the photon in spontaneous emission is random as is the direction in which the photon propagates. This is not true for stimulated emission. An energy level diagram illustrating the process of spontaneous emission is shown below:

If the number of light sources in the excited state at time formula_10 is given by formula_11, the rate at which formula_12 decays is:

where formula_14 is the rate of spontaneous emission. In the rate-equation formula_14 is a proportionality constant for this particular transition in this particular light source. The constant is referred to as the "Einstein A coefficient", and has units formula_16. 
The above equation can be solved to give:

where formula_18 is the initial number of light sources in the excited state, formula_10 is the time and formula_20 is the radiative decay rate of the transition. The number of excited states formula_12 thus decays exponentially with time, similar to radioactive decay. After one lifetime, the number of excited states decays to 36.8% of its original value (formula_22-time). The radiative decay rate formula_20 is inversely proportional to the lifetime formula_24:

Spontaneous transitions were not explainable within the framework of the Schrödinger equation, in which the electronic energy levels were quantized, but the electromagnetic field was not. Given that the eigenstates of an atom are properly diagonalized, the overlap of the wavefunctions between the excited state and the ground state of the atom is zero. Thus, in the absence of a quantized electromagnetic field, the excited state atom cannot decay to the ground state. In order to explain spontaneous transitions, quantum mechanics must be extended to a quantum field theory, wherein the electromagnetic field is quantized at every point in space. The quantum field theory of electrons and electromagnetic fields is known as quantum electrodynamics.

In quantum electrodynamics (or QED), the electromagnetic field has a ground state, the QED vacuum, which can mix with the excited stationary states of the atom. As a result of this interaction, the "stationary state" of the atom is no longer a true eigenstate of the combined system of the atom plus electromagnetic field. In particular, the electron transition from the excited state to the electronic ground state mixes with the transition of the electromagnetic field from the ground state to an excited state, a field state with one photon in it. Spontaneous emission in free space depends upon vacuum fluctuations to get started.

Although there is only one electronic transition from the excited state to ground state, there are many ways in which the electromagnetic field may go from the ground state to a one-photon state. That is, the electromagnetic field has infinitely more degrees of freedom, corresponding to the different directions in which the photon can be emitted. Equivalently, one might say that the phase space offered by the electromagnetic field is infinitely larger than that offered by the atom. This infinite degree of freedom for the emission of the photon results in the apparent irreversible decay, i.e., spontaneous emission.

In the presence of electromagnetic vacuum modes, the combined atom-vacuum system is explained by the superposition of the wavefunctions of the excited state atom with no photon and the ground state atom with a single emitted photon:

where formula_27 and formula_28 are the atomic excited state-electromagnetic vacuum wavefunction and its probability amplitude, formula_29 and formula_30 are the ground state atom with a single photon (of mode formula_31) wavefunction and its probability amplitude, formula_32 is the atomic transition frequency, and formula_33 is the frequency of the photon. The sum is over formula_34 and formula_35, which are the wavenumber and polarization of the emitted photon, respectively. As mentioned above, the emitted photon has a chance to be emitted with different wavenumbers and polarizations, and the resulting wavefunction is a superposition of these possibilities. To calculate the probability of the atom at the ground state (formula_36), one needs to solve the time evolution of the wavefunction with an appropriate Hamiltonian. To solve for the transition amplitude, one needs to average over (integrate over) all the vacuum modes, since one must consider the probabilities that the emitted photon occupies various parts of phase space equally. The "spontaneously" emitted photon has infinite different modes to propagate into, thus the probability of the atom re-absorbing the photon and returning to the original state is negligible, making the atomic decay practically irreversible. Such irreversible time evolution of the atom-vacuum system is responsible for the apparent spontaneous decay of an excited atom. If one were to keep track of all the vacuum modes, the combined atom-vacuum system would undergo unitary time evolution, making the decay process reversible. Cavity quantum electrodynamics is one such system where the vacuum modes are modified resulting in the reversible decay process, see also Quantum revival. The theory of the spontaneous emission under the QED framework was first calculated by Weisskopf and Wigner.

In spectroscopy one can frequently find that atoms or molecules in the excited states dissipate their energy in the absence of any external source of photons. This is not spontaneous emission, but is actually nonradiative relaxation of the atoms or molecules caused by the fluctuation of the surrounding molecules present inside the bulk.

The rate of spontaneous emission (i.e., the radiative rate) can be described by Fermi's golden rule. The rate of emission depends on two factors: an 'atomic part', which describes
the internal structure of the light source and a 'field part', which describes the density of electromagnetic modes of the environment. The atomic part describes the strength of a transition between two states in terms of transition moments. In a homogeneous medium, such as free space, the rate of spontaneous emission in the dipole approximation is given by:

where formula_3 is the emission frequency, formula_40 is the index of refraction, formula_41 is the transition dipole moment, formula_42 is the vacuum permittivity, formula_6 is the reduced Planck constant, formula_44 is the vacuum speed of light, and formula_45 is the fine structure constant. (This approximation breaks down in the case of inner shell electrons in high-Z atoms.) The above equation clearly shows that the rate of spontaneous emission in free space increases proportionally to formula_46.

In contrast with atoms, which have a discrete emission spectrum, quantum dots can be tuned continuously by changing their size. This property has been used to check the formula_46-frequency dependence of the spontaneous emission rate as described by Fermi's golden rule.

In the rate-equation above, it is assumed that decay of the number of excited states formula_12 only occurs under emission of light. In this case one speaks of full radiative decay and this means that the quantum efficiency is 100%. Besides radiative decay, which occurs under the emission of light, there is a second decay mechanism; nonradiative decay. To determine the total decay rate formula_49, radiative and nonradiative rates should be summed:

where formula_49 is the total decay rate, formula_20 is the radiative decay rate and formula_53 the nonradiative decay rate. The quantum efficiency (QE) is defined as the fraction of emission processes in which emission of light is involved:

In nonradiative relaxation, the energy is released as phonons, more commonly known as heat. Nonradiative relaxation occurs when the energy difference between the levels is very small, and these typically occur on a much faster time scale than radiative transitions. For many materials (for instance, semiconductors), electrons move quickly from a high energy level to a meta-stable level via small nonradiative transitions and then make the final move down to the bottom level via an optical or radiative transition. This final transition is the transition over the bandgap in semiconductors. Large nonradiative transitions do not occur frequently because the crystal structure generally cannot support large vibrations without destroying bonds (which generally doesn't happen for relaxation). Meta-stable states form a very important feature that is exploited in the construction of lasers. Specifically, since electrons decay slowly from them, they can be deliberately piled up in this state without too much loss and then stimulated emission can be used to boost an optical signal.




</doc>
<doc id="28220" url="https://en.wikipedia.org/wiki?curid=28220" title="Nicolas Léonard Sadi Carnot">
Nicolas Léonard Sadi Carnot

Nicolas Léonard Sadi Carnot (; 1 June 1796 – 24 August 1832) was a French military engineer and physicist, often described as the "father of thermodynamics". Like Copernicus, he published only one book, the "Reflections on the Motive Power of Fire" (Paris, 1824), in which he expressed, at the age of 27 years, the first successful theory of the maximum efficiency of heat engines. In this work he laid the foundations of an entirely new discipline, thermodynamics. Carnot's work attracted little attention during his lifetime, but it was later used by Rudolf Clausius and Lord Kelvin to formalize the second law of thermodynamics and define the concept of entropy.

Nicolas Léonard Sadi Carnot was born in Paris into a family that was distinguished in both science and politics. He was the first son of Lazare Carnot, an eminent mathematician, military engineer and leader of the French Revolutionary Army. Lazare chose his son's third given name (by which he would always be known) after the Persian poet Sadi of Shiraz. Sadi was the elder brother of statesman Hippolyte Carnot and the uncle of Marie François Sadi Carnot, who would serve as President of France from 1887 to 1894.

At the age of 16, Sadi Carnot became a cadet in the École Polytechnique in Paris, where his classmates included Michel Chasles and Gaspard-Gustave Coriolis. The École Polytechnique was intended to train engineers for military service, but its professors included such eminent scientists as André-Marie Ampère, François Arago, Joseph Louis Gay-Lussac, Louis Jacques Thénard and Siméon Denis Poisson, and the school had become renowned for its mathematical instruction. After graduating in 1814, Sadi became an officer in the French army's corps of engineers. His father Lazare had served as Napoleon's minister of the interior during the "Hundred Days", and after Napoleon's final defeat in 1815 Lazare was forced into exile. Sadi's position in the army, under the restored Bourbon monarchy of Louis XVIII, became increasingly difficult.

Sadi Carnot was posted to different locations, he inspected fortifications, tracked plans and wrote many reports. It appears his recommendations were ignored and his career was stagnating. On 15 September 1818 he took a six-month leave to prepare for the entrance examination of Royal Corps of Staff and School of Application for the Service of the General Staff.

In 1819, Sadi transferred to the newly formed General Staff, in Paris. He remained on call for military duty, but from then on he dedicated most of his attention to private intellectual pursuits and received only two-thirds pay. Carnot befriended the scientist Nicolas Clément and attended lectures on physics and chemistry. He became interested in understanding the limitation to improving the performance of steam engines, which led him to the investigations that became his "Reflections on the Motive Power of Fire", published in 1824.

Carnot retired from the army in 1828, without a pension. He was interned in a private asylum in 1832 as suffering from "mania" and "general delirum", and he died of cholera shortly thereafter, aged 36, at the hospital in Ivry-sur-Seine.

When Carnot began working on his book, steam engines had achieved widely recognized economic and industrial importance, but there had been no real scientific study of them. Newcomen had invented the first piston-operated steam engine over a century before, in 1712; some 50 years after that, James Watt made his celebrated improvements, which were responsible for greatly increasing the efficiency and practicality of steam engines. Compound engines (engines with more than one stage of expansion) had already been invented, and there was even a crude form of internal-combustion engine, with which Carnot was familiar and which he described in some detail in his book. Although there existed some intuitive understanding of the workings of engines, scientific theory for their operation was almost nonexistent. In 1824 the principle of conservation of energy was still poorly developed and controversial, and an exact formulation of the first law of thermodynamics was still more than a decade away; the mechanical equivalence of heat would not be formulated for another two decades. The prevalent theory of heat was the caloric theory, which regarded heat as a sort of weightless and invisible fluid that flowed when out of equilibrium.

Engineers in Carnot's time had tried, by means such as highly pressurized steam and the use of fluids, to improve the efficiency of engines. In these early stages of engine development, the efficiency of a typical engine — the useful work it was able to do when a given quantity of fuel was burned — was only 3%.

Carnot wanted to answer two questions about the operation of heat engines: "Is the work available from a heat source potentially unbounded?" and "Can heat engines in principle be improved by replacing the steam with some other working fluid or gas?" He attempted to answer these in a memoir, published as a popular work in 1824 when he was only 28 years old. It was entitled "Réflexions sur la Puissance Motrice du Feu" ("Reflections on the Motive Power of Fire"). The book was plainly intended to cover a rather wide range of topics about heat engines in a rather popular fashion; equations were kept to a minimum and called for little more than simple algebra and arithmetic, except occasionally in the footnotes, where he indulged in a few arguments involving some calculus. He discussed the relative merits of air and steam as working fluids, the merits of various aspects of steam engine design, and even included some ideas of his own regarding possible improvements of the practical nature. The most important part of the book was devoted to an abstract presentation of an idealized engine that could be used to understand and clarify the fundamental principles that are generally applied to all heat engines, independent of their design.

Perhaps the most important contribution Carnot made to thermodynamics was his abstraction of the essential features of the steam engine, as they were known in his day, into a more general and idealized heat engine. This resulted in a model thermodynamic system upon which exact calculations could be made, and avoided the complications introduced by many of the crude features of the contemporary steam engine. By idealizing the engine, he could arrive at clear and indisputable answers to his original two questions.

He showed that the efficiency of this idealized engine is a function only of the two temperatures of the reservoirs between which it operates. He did not, however, give the exact form of the function, which was later shown to be (T−T)/T, where T is the absolute temperature of the hotter reservoir. (Note: This equation probably came from Kelvin.) No thermal engine operating any other cycle can be more efficient, given the same operating temperatures.

The Carnot cycle is the most efficient possible engine, not only because of the (trivial) absence of friction and other incidental wasteful processes; the main reason is that it assumes no conduction of heat between parts of the engine at different temperatures. Carnot knew that the conduction of heat between bodies at different temperatures is a wasteful and irreversible process, which must be eliminated if the heat engine is to achieve maximum efficiency.

Regarding the second point, he also was quite certain that the maximum efficiency attainable did not depend upon the exact nature of the working fluid. He stated this for emphasis as a general proposition:

For his "motive power of heat", we would today say "the efficiency of a reversible heat engine", and rather than "transfer of caloric" we would say "the reversible transfer of entropy ∆S" or "the reversible transfer of heat at a given temperature Q/T". He knew intuitively that his engine would have the maximum efficiency, but was unable to state what that efficiency would be.

He concluded:

and
In an idealized model, the caloric transported from a hot to a cold body by a frictionless heat engine that lacks of conductive heat flow, driven by a difference of temperature, yielding work, could also be used to transport the caloric back to the hot body by reversing the motion of the engine consuming the same amount of work, a concept subsequently known as thermodynamic reversibility. Carnot further postulated that no caloric is lost during the operation of his idealized engine. The process being completely reversible, executed by this kind of heat engine is the most efficient possible process. The assumption that heat conduction driven by a temperature difference cannot exist, so that no caloric is lost by the engine, guided him to design the Carnot-cycle to be operated by his idealized engine. The cycle is consequently composed of adiabatic processes where no heat/caloric ∆S = 0 flows and isothermal processes where heat is transferred ∆S > 0 but no temperature difference ∆T = 0 exist. The proof of the existence of a maximum efficiency for heat engines is as follows:

As the cycle named after him doesn't waste caloric, the reversible engine has to use this cycle. Imagine now two large bodies, a hot and a cold one. He postulates now the existence of a heat machine with a greater efficiency. We couple now two idealized machine but of different efficiencies and connect them to the same hot and the same cold body. The first and less efficient one lets a constant amount of entropy ∆S = Q/T flow from hot to cold during each cycle, yielding an amount of work denoted W. If we use now this work to power the other more efficient machine, it would, using the amount of work W gained during each cycle by the first machine, make an amount of entropy ∆S' > ∆S flow from the cold to the hot body. The net effect is a flow of ∆S' − ∆S ≠ 0 of entropy from the cold to the hot body, while no net work is done. Consequently, the cold body is cooled down and the hot body rises in temperature. As the difference of temperature rises now the yielding of work by the first is greater in the successive cycles and due to the second engine difference in temperature of the two bodies stretches by each cycle even more. In the end this set of machines would be a perpetuum mobile that cannot exist. This proves that the assumption of the existence of a more efficient engine was wrong so that an heat engine that operates the Carnot cycle must be the most efficient one. This means that a frictionless heat engine that lacks of conductive heat flow driven by a difference of temperature shows maximum possible efficiency.

He concludes further that the choice of the working fluid, its density or the volume occupied by it cannot change this maximum efficiency. Using the equivalence of any working gas used in heat engines he deduced that the difference in the specific heat of a gas measured at constant pressure and at constant volume must be constant for all gases.
By comparing the operation of his hypothetical heat engines for two different volumes occupied by the same amount of working gas he correctly deduces the relation between entropy and volume for an isothermal process:

formula_1

Carnot's book received very little attention from his contemporaries. The only reference to it within a few years after its publication was in a review in the periodical "Revue Encyclopédique", which was a journal that covered a wide range of topics in literature. The impact of the work had only become apparent once it was modernized by Émile Clapeyron in 1834 and then further elaborated upon by Clausius and Kelvin, who together derived from it the concept of entropy and the second law of thermodynamics.

On Carnot's religious views, he was a Philosophical theist. As a deist, he believed in divine causality, stating that "what to an ignorant man is chance, cannot be chance to one better instructed," but he did not believe in divine punishment. He criticized established religion, though at the same time spoke in favor of "the belief in an all-powerful Being, who loves us and watches over us."

He was a reader of Blaise Pascal, Molière and Jean de La Fontaine.

Carnot died during a cholera epidemic in 1832, at the age of 36. 
Because of the contagious nature of cholera, many of Carnot's belongings and writings were buried together with him after his death. As a consequence, only a handful of his scientific writings survived.

After the publication of "Reflections on the Motive Power of Fire", the book quickly went out of print and for some time was very difficult to obtain. Kelvin, for one, had a difficult time getting a copy of Carnot's book. In 1890 an English translation of the book was published by R. H. Thurston; this version has been reprinted in recent decades by Dover and by Peter Smith, most recently by Dover in 2005. Some of Carnot's posthumous manuscripts have also been translated into English.

Carnot published his book in the heyday of steam engines. His theory explained why steam engines using superheated steam were better because of the higher temperature of the consequent hot reservoir. Carnot's theories and efforts did not immediately help improve the efficiency of steam engines; his theories only helped to explain why one existing practice was superior to others. It was only towards the end of the nineteenth century that Carnot's ideas, namely that a heat engine can be made more efficient if the temperature of its hot reservoir is increased, were put into practice. Carnot's book did, however, eventually have a real impact on the design of practical engines. Rudolf Diesel, for example, used Carnot's theories to design the diesel engine, in which the temperature of the hot reservoir is much higher than that of a steam engine, resulting in an engine which is more efficient.



The text of part of an earlier version of this article was taken from the public domain resource "A Short Account of the History of Mathematics" by W. W. Rouse Ball (4th Edition, 1908)



</doc>
<doc id="28221" url="https://en.wikipedia.org/wiki?curid=28221" title="Suleiman I">
Suleiman I

Suleiman I may refer to:



</doc>
<doc id="28222" url="https://en.wikipedia.org/wiki?curid=28222" title="Sydney Opera House">
Sydney Opera House

The Sydney Opera House is a multi-venue performing arts centre in Sydney, New South Wales, Australia. It is one of the 20th century's most famous and distinctive buildings.

Designed by Danish architect Jørn Utzon, the building was formally opened on 20 October 1973 after a gestation beginning with Utzon's 1957 selection as winner of an international design competition. The Government of New South Wales, led by the premier, Joseph Cahill, authorised work to begin in 1958 with Utzon directing construction. The government's decision to build Utzon's design is often overshadowed by circumstances that followed, including cost and scheduling overruns as well as the architect's ultimate resignation.

The building and its surrounds occupy the whole of Bennelong Point on Sydney Harbour, between Sydney Cove and Farm Cove, adjacent to the Sydney central business district and the Royal Botanic Gardens, and close by the Sydney Harbour Bridge.

Though its name suggests a single venue, the building comprises multiple performance venues which together host well over 1,500 performances annually, attended by more than 1.2 million people. Performances are presented by numerous performing artists, including three resident companies: Opera Australia, the Sydney Theatre Company and the Sydney Symphony Orchestra. As one of the most popular visitor attractions in Australia, more than eight million people visit the site annually, and approximately 350,000 visitors take a guided tour of the building each year. The building is managed by the Sydney Opera House Trust, an agency of the New South Wales State Government.

On 28 June 2007, the Sydney Opera House became a UNESCO World Heritage Site; having been listed on the (now defunct) Register of the National Estate since 1980, the National Trust of Australia register since 1983, the City of Sydney Heritage Inventory since 2000, the New South Wales State Heritage Register since 2003, and the Australian National Heritage List since 2005.

The facility features a modern expressionist design, with a series of large precast concrete "shells", each composed of sections of a sphere of radius, forming the roofs of the structure, set on a monumental podium. The building covers of land and is long and wide at its widest point. It is supported on 588 concrete piers sunk as much as below sea level.

Although the roof structures are commonly referred to as "shells" (as in this article), they are precast concrete panels supported by precast concrete ribs, not shells in a strictly structural sense. Though the shells appear uniformly white from a distance, they actually feature a subtle chevron pattern composed of 1,056,006 tiles in two colours: glossy white and matte cream. The tiles were manufactured by the Swedish company Höganäs AB which generally produced stoneware tiles for the paper-mill industry.

Apart from the tile of the shells and the glass curtain walls of the foyer spaces, the building's exterior is largely clad with aggregate panels composed of pink granite quarried at Tarana. Significant interior surface treatments also include off-form concrete, Australian white birch plywood supplied from Wauchope in northern New South Wales, and brush box glulam.

Of the two larger spaces, the Concert Hall is in the western group of shells, the Joan Sutherland Theatre in the eastern group. The scale of the shells was chosen to reflect the internal height requirements, with low entrance spaces, rising over the seating areas up to the high stage towers. The smaller venues (the Drama Theatre, the Playhouse and the Studio) are within the podium, beneath the Concert Hall. A smaller group of shells set to the western side of the Monumental Steps houses the Bennelong Restaurant. The podium is surrounded by substantial open public spaces, and the large stone-paved forecourt area with the adjacent monumental steps is regularly used as a performance space.

The Sydney Opera House includes a number of performance venues:

Other areas (for example the northern and western foyers) are also used for performances on an occasional basis. Venues are also used for conferences, ceremonies and social functions.

The building also houses a recording studio, cafes, restaurants, bars and retail outlets. Guided tours are available, including a frequent tour of the front-of-house spaces, and a daily backstage tour that takes visitors backstage to see areas normally reserved for performers and crew members.

Planning began in the late 1940s, when Eugene Goossens, the Director of the NSW State Conservatorium of Music, lobbied for a suitable venue for large theatrical productions. The normal venue for such productions, the Sydney Town Hall, was not considered large enough. By 1954, Goossens succeeded in gaining the support of NSW Premier Joseph Cahill, who called for designs for a dedicated opera house. It was also Goossens who insisted that Bennelong Point be the site: Cahill had wanted it to be on or near Wynyard Railway Station in the northwest of the CBD.

An international design competition was launched by Cahill on 13 September 1955 and received 233 entries, representing architects from 32 countries. The criteria specified a large hall seating 3,000 and a small hall for 1,200 people, each to be designed for different uses, including full-scale operas, orchestral and choral concerts, mass meetings, lectures, ballet performances and other presentations.

The winner, announced in 1957, was Jørn Utzon, a Danish architect. According to legend the Utzon design was rescued by noted Finnish-American architect Eero Saarinen from a final cut of 30 "rejects". The runner-up was an entry by firm GBQC of Philadelphia, Pennsylvania. The grand prize was 5,000 Australian pounds. Utzon visited Sydney in 1957 to help supervise the project. His office moved to Palm Beach, Sydney in February 1963.

Utzon received the Pritzker Architecture Prize, architecture's highest honour, in 2003. The Pritzker Prize citation read:
The Fort Macquarie Tram Depot, occupying the site at the time of these plans, was demolished in 1958 and construction began in March 1959. It was built in three stages: stage I (1959–1963) consisted of building the upper podium; stage II (1963–1967) the construction of the outer shells; stage III (1967–1973) interior design and construction.

Stage I commenced on 2 March 1959 with the construction firm Civil & Civic, monitored by the engineers Ove Arup and Partners. The government had pushed for work to begin early, fearing that funding, or public opinion, might turn against them. However, Utzon had still not completed the final designs. Major structural issues still remained unresolved. By 23 January 1961, work was running 47 weeks behind, mainly because of unexpected difficulties (inclement weather, unexpected difficulty diverting stormwater, construction beginning before proper construction drawings had been prepared, changes of original contract documents). Work on the podium was finally completed in February 1963. The forced early start led to significant later problems, not least of which was the fact that the podium columns were not strong enough to support the roof structure, and had to be re-built.

The shells of the competition entry were originally of undefined geometry, but, early in the design process, the "shells" were perceived as a series of parabolas supported by precast concrete ribs. However, engineers Ove Arup and Partners were unable to find an acceptable solution to constructing them. The formwork for using "in-situ" concrete would have been prohibitively expensive, and, because there was no repetition in any of the roof forms, the construction of precast concrete for each individual section would possibly have been even more expensive.

From 1957 to 1963, the design team went through at least 12 iterations of the form of the shells trying to find an economically acceptable form (including schemes with parabolas, circular ribs and ellipsoids) before a workable solution was completed. The design work on the shells involved one of the earliest uses of computers in structural analysis, to understand the complex forces to which the shells would be subjected. The computer system was also used in the assembly of the arches. The pins in the arches were surveyed at the end of each day, and the information was entered into the computer so the next arch could be properly placed the following day. In mid-1961, the design team found a solution to the problem: the shells all being created as sections from a sphere. This solution allows arches of varying length to be cast in a common mould, and a number of arch segments of common length to be placed adjacent to one another, to form a spherical section. With whom exactly this solution originated has been the subject of some controversy. It was originally credited to Utzon. Ove Arup's letter to Ashworth, a member of the Sydney Opera House Executive Committee, states: "Utzon came up with an idea of making all the shells of uniform curvature throughout in both directions." Peter Jones, the author of Ove Arup's biography, states that "the architect and his supporters alike claimed to recall the precise "eureka" moment ... ; the engineers and some of their associates, with equal conviction, recall discussion in both central London and at Ove's house."

He goes on to claim that "the existing evidence shows that Arup's canvassed several possibilities for the geometry of the shells, from parabolas to ellipsoids and spheres." Yuzo Mikami, a member of the design team, presents an opposite view in his book on the project, "Utzon's Sphere". It is unlikely that the truth will ever be categorically known, but there is a clear consensus that the design team worked very well indeed for the first part of the project and that Utzon, Arup, and Ronald Jenkins (partner of Ove Arup and Partners responsible for the Opera House project) all played a very significant part in the design development.

As Peter Murray states in "The Saga of the Sydney Opera House":

The design of the roof was tested on scale models in wind tunnels at University of Southampton and later NPL in order to establish the wind-pressure distribution around the roof shape in very high winds, which helped in the design of the roof tiles and their fixtures.

The shells were constructed by Hornibrook Group Pty Ltd, who were also responsible for construction in Stage III. Hornibrook manufactured the 2400 precast ribs and 4000 roof panels in an on-site factory and also developed the construction processes. The achievement of this solution avoided the need for expensive formwork construction by allowing the use of precast units (it also allowed the roof tiles to be prefabricated in sheets on the ground, instead of being stuck on individually at height). Ove Arup and Partners' site engineer supervised the construction of the shells, which used an innovative adjustable steel-trussed "erection arch" to support the different roofs before completion. On 6 April 1962, it was estimated that the Opera House would be completed between August 1964 and March 1965.

Stage III, the interiors, started with Utzon moving his entire office to Sydney in February 1963. However, there was a change of government in 1965, and the new Robert Askin government declared the project under the jurisdiction of the Ministry of Public Works. Due to the Ministry's criticism of the project's costs and time, along with their impression of Utzon's designs being impractical, this ultimately led to his resignation in 1966 (see below).

The cost of the project so far, even in October 1966, was still only $22.9 million, less than a quarter of the final $102 million cost. However, the projected costs for the design were at this stage much more significant.

The second stage of construction was progressing toward completion when Utzon resigned. His position was principally taken over by Peter Hall, who became largely responsible for the interior design. Other persons appointed that same year to replace Utzon were E. H. Farmer as government architect, D. S. Littlemore and Lionel Todd.

Following Utzon's resignation, the acoustic advisor, Lothar Cremer, confirmed to the Sydney Opera House Executive Committee (SOHEC) that Utzon's original acoustic design allowed for only 2000 seats in the main hall and further stated that increasing the number of seats to 3000 as specified in the brief would be disastrous for the acoustics. According to Peter Jones, the stage designer, Martin Carr, criticised the "shape, height and width of the stage, the physical facilities for artists, the location of the dressing rooms, the widths of doors and lifts, and the location of lighting switchboards."


The Opera House was formally completed in 1973, having cost $102 million. H.R. "Sam" Hoare, the Hornibrook director in charge of the project, provided the following approximations in 1973:
Stage I: podium Civil & Civic Pty Ltd approximately $5.5m.
Stage II: roof shells M.R. Hornibrook (NSW) Pty Ltd approximately $12.5m.
Stage III: completion The Hornibrook Group $56.5m.
Separate contracts: stage equipment, stage lighting and organ $9.0m.
Fees and other costs: $16.5m.

The original cost and scheduling estimates in 1957 projected a cost of £3,500,000 ($7 million) and completion date of 26 January 1963 (Australia Day). In reality, the project was completed ten years late and 1,357% over budget in real terms.

Before the Sydney Opera House competition, Jørn Utzon had won seven of the 18 competitions he had entered but had never seen any of his designs built. Utzon's submitted concept for the Sydney Opera House was almost universally admired and considered groundbreaking. The Assessors Report of January 1957, stated:

For the first stage, Utzon worked successfully with the rest of the design team and the client, but, as the project progressed, the Cahill government insisted on progressive revisions. They also did not fully appreciate the costs or work involved in design and construction. Tensions between the client and the design team grew further when an early start to construction was demanded despite an incomplete design. This resulted in a continuing series of delays and setbacks while various technical engineering issues were being refined. The building was unique, and the problems with the design issues and cost increases were exacerbated by commencement of work before the completion of the final plans.

After the 1965 election of the Liberal Party, with Robert Askin becoming Premier of New South Wales, the relationship of client, architect, engineers and contractors became increasingly tense. Askin had been a "vocal critic of the project prior to gaining office." His new Minister for Public Works, Davis Hughes, was even less sympathetic. Elizabeth Farrelly, an Australian architecture critic, wrote that:
Differences ensued. One of the first was that Utzon believed the clients should receive information on all aspects of the design and construction through his practice, while the clients wanted a system (notably drawn in sketch form by Davis Hughes) where architect, contractors, and engineers each reported to the client directly and separately. This had great implications for procurement methods and cost control, with Utzon wishing to negotiate contracts with chosen suppliers (such as Ralph Symonds for the plywood interiors) and the New South Wales government insisting contracts be put out to tender.

Utzon was highly reluctant to respond to questions or criticism from the client's Sydney Opera House Executive Committee (SOHEC). However, he was greatly supported throughout by a member of the committee and one of the original competition judges, Harry Ingham Ashworth. Utzon was unwilling to compromise on some aspects of his designs that the clients wanted to change.

Utzon's ability was never in doubt, despite questions raised by Davis Hughes, who attempted to portray Utzon as an impractical dreamer. Ove Arup actually stated that Utzon was "probably the best of any I have come across in my long experience of working with architects" and: "The Opera House could become the world's foremost contemporary masterpiece if Utzon is given his head."
In October 1965, Utzon gave Hughes a schedule setting out the completion dates of parts of his work for stage III. Utzon was at this time working closely with Ralph Symonds, a manufacturer of plywood based in Sydney and highly regarded by many, despite an Arup engineer warning that Ralph Symonds's "knowledge of the design stresses of plywood, was extremely sketchy" and that the technical advice was "elementary to say the least and completely useless for our purposes." Australian architecture critic Elizabeth Farrelly has referred to Ove Arup's project engineer Michael Lewis as having "other agendas". In any case, Hughes shortly after withheld permission for the construction of plywood prototypes for the interiors, and the relationship between Utzon and the client never recovered. By February 1966, Utzon was owed more than $100,000 in fees. Hughes then withheld funding so that Utzon could not even pay his own staff. The government minutes record that following several threats of resignation, Utzon finally stated to Davis Hughes: "If you don't do it, I resign." Hughes replied: "I accept your resignation. Thank you very much. Goodbye."

Utzon left the project on 28 February 1966. He said that Hughes's refusal to pay him any fees and the lack of collaboration caused his resignation and later famously described the situation as "Malice in Blunderland". In March 1966, Hughes offered him a subordinate role as "design architect" under a panel of executive architects, without any supervisory powers over the House's construction, but Utzon rejected this. Utzon left the country never to return.

Following the resignation, there was great controversy about who was in the right and who was in the wrong. "The Sydney Morning Herald" initially opined: "No architect in the world has enjoyed greater freedom than Mr Utzon. Few clients have been more patient or more generous than the people and the Government of NSW. One would not like history to record that this partnership was brought to an end by a fit of temper on the one side or by a fit of meanness on the other." On 17 March 1966, the "Herald" offered the view that: "It was not his [Utzon's] fault that a succession of Governments and the Opera House Trust should so signally have failed to impose any control or order on the project ... his concept was so daring that he himself could solve its problems only step by step ... his insistence on perfection led him to alter his design as he went along."

The Sydney Opera House opened the way for the immensely complex geometries of some modern architecture. The design was one of the first examples of the use of computer-aided design to design complex shapes. The design techniques developed by Utzon and Arup for the Sydney Opera House have been further developed and are now used for architecture, such as works of Gehry and blobitecture, as well as most reinforced concrete structures. The design is also one of the first in the world to use araldite to glue the precast structural elements together and proved the concept for future use.

It was also a first in mechanical engineering. Another Danish firm, Steensen Varming, was responsible for designing the new air-conditioning plant, the largest in Australia at the time, supplying over of air per minute, using the innovative idea of harnessing the harbour water to create a water-cooled heat pump system that is still in operation today.

After the resignation of Utzon, the Minister for Public Works, Davis Hughes, and the Government Architect, Ted Farmer, organised a team to bring the Sydney Opera House to completion. The architectural work was divided between three appointees who became the Hall, Todd, Littlemore partnership. David Littlemore would manage construction supervision, Lionel Todd contract documentation, while the crucial role of design became the responsibility of Peter Hall.

Peter Hall (1931–1995) completed a combined arts and architecture degree at Sydney University. Upon graduation a travel scholarship enabled him to spend twelve months in Europe during which time he visited Utzon in Hellebæk. Returning to Sydney, Hall worked for the Government Architect, a branch of the NSW Public Works Department. While there he established himself as a talented design architect with a number of court and university buildings, including the Goldstein Hall at the University of New South Wales, which won the Sir John Sulman Medal in 1964.

Hall resigned from the Government Architects office in early 1966 to pursue his own practice. When approached to take on the design role, (after at least two prominent Sydney architects had declined), Hall spoke with Utzon by phone before accepting the position. Utzon reportedly told Hall: he (Hall) would not be able to finish the job and the Government would have to invite him back. Hall also sought the advice of others, including architect Don Gazzard who warned him acceptance would be a bad career move as the project would "never be his own".

Hall agreed to accept the role on the condition there was no possibility of Utzon returning. Even so, his appointment did not go down well with many of his fellow architects who considered that no one but Utzon should complete the Sydney Opera House. Upon Utzon's dismissal, a rally of protest had marched to Bennelong Point. A petition was also circulated, including in the Government Architects office. Peter Hall was one of the many who had signed the petition that called for Utzon's reinstatement.

When Hall agreed to the design role and was appointed in April 1966, he imagined he would find the design and documentation for the Stage III well advanced. What he found was an enormous amount of work ahead of him with many aspects completely unresolved by Utzon in relation to seating capacity, acoustics and structure. In addition Hall found the project had proceeded for nine years without the development of a concise client brief. To bring himself up to speed, Hall investigated concert and opera venues overseas and engaged stage consultant Ben Schlange and acoustic consultant Wilhelm Jordan, while establishing his team. In consultation with all the potential building users the first Review of Program was completed in January 1967. The most significant conclusion reached by Hall was that concert and opera were incompatible in the same hall. Although Utzon had sketched ideas using plywood for the great enclosing glass walls their structural viability was unresolved when Hall took on the design role. With the ability to delegate tasks and effectively coordinate the work of consultants, Hall guided the project for over five years until the opening day in 1973.

A former Government Architect, Peter Webber, in his book "Peter Hall: the Phantom of the Opera House", concludes: when Utzon resigned no one was better qualified (than Hall) to rise to the challenge of completing the design of the Opera House.

The Sydney Opera House was formally opened by Elizabeth II, Queen of Australia, on 20 October 1973. A large crowd attended. Utzon was not invited to the ceremony, nor was his name mentioned. The opening was televised and included fireworks and a performance of Beethoven's Symphony No. 9.

During the construction phase, lunchtime performances were often arranged for the workers, with American vocalist Paul Robeson the first artist to perform, in 1960.

Various performances were presented prior to the official opening:

After the opening:

In the late 1990s, the Sydney Opera House Trust resumed communication with Utzon in an attempt to effect a reconciliation and to secure his involvement in future changes to the building. In 1999, he was appointed by the Trust as a design consultant for future work.
In 2004, the first interior space rebuilt to an Utzon design was opened, and renamed "The Utzon Room" in his honour. It contains an original Utzon tapestry (14.00 x 3.70 metres) called "Homage to Carl Philipp Emmanuel Bach". In April 2007, he proposed a major reconstruction of the Opera Theatre, as it was then known. Utzon died on 29 November 2008.

A state memorial service, attended by Utzon's son Jan and daughter Lin, celebrating his creative genius, was held in the Concert Hall on 25 March 2009 featuring performances, readings and recollections from prominent figures in the Australian performing arts scene.

Refurbished Western Foyer and Accessibility improvements were commissioned on 17 November 2009, the largest building project completed since Utzon was re-engaged in 1999. Designed by Utzon and his son Jan, the project provided improved ticketing, toilet and cloaking facilities. New escalators and a public lift enabled enhanced access for the disabled and families with prams. The prominent paralympian athlete Louise Sauvage was announced as the building's "accessibility ambassador" to advise on further improvements to aid people with disabilities.

On 29 March 2016, an original 1959 tapestry by Le Corbusier (2.18 x 3.55 metres), commissioned by Utzon to be hung in the Sydney Opera House and called "Les Dés Sont Jetés" (The Dice Are Cast), was finally unveiled "in situ" after being owned by the Utzon family and held at their home in Denmark for over 50 years. The tapestry was bought at auction by the Sydney Opera House in June 2015. It now hangs in the building's Western Foyer and is accessible to the public.

In the second half of 2017, the Joan Sutherland Theatre was closed to replace the stage machinery and for other works. The Concert Hall is scheduled for work in 2020-2021.

In 1993, Constantine Koukias was commissioned by the Sydney Opera House Trust in association with REM Theatre to compose "Icon", a large-scale music theatre piece for the 20th anniversary of the Sydney Opera House 

During the 2000 Summer Olympics, the venue served as the focal point for the triathlon events. The event had a swimming loop at Farm Cove, along with competitions in the neighbouring Royal Botanical Gardens for the cycling and running portions of the event.

Since 2013, a group of residents from the nearby Bennelong Apartments (better known as 'The Toaster'), calling themselves the Sydney Opera House Concerned Citizens Group, have been campaigning against Forecourt Concerts on the grounds that they exceed noise levels outlined in the development approval (DA). In February 2017 the NSW Department of Planning and the Environment handed down a $15,000 fine to the Sydney Opera House for breach of allowed noise levels at a concert held in November 2015. However the DA was amended in 2016 to allow an increase in noise levels in the forecourt by 5 decibels. The residents opposing the concerts contend that a new DA should have been filed rather than an amendment.

The Sydney Opera House sails formed a graphic projection-screen in a lightshow mounted in connection with the International Fleet Review in Sydney Harbour on 5 October 2013.

On 31 December 2013, the venue's 40th anniversary year, a New Year firework display was mounted for the first time in a decade. The Sydney Opera House hosted an event, 'the biggest blind date' on Friday 21 February 2014 that won an historic Guinness World Record. The longest continuous serving employee was commemorated on June 27th 2018, for 50 years of service.




 This Wikipedia article was originally based on the "Sydney Opera House", listed on the "New South Wales State Heritage Register" published by the Government of New South Wales under CC-BY 3.0 AU licence (accessed on 3 September 2017).




</doc>
<doc id="28223" url="https://en.wikipedia.org/wiki?curid=28223" title="Selim II">
Selim II

Selim II (Ottoman Turkish: سليم ثانى "Selīm-i sānī", Turkish: "II.Selim"; 28 May 1524 – 12/15 December 1574), also known as "Selim the Sot (Mest)" or ("Selim the Drunkard") and "Sarı Selim" ("Selim the Blond"), was the Sultan of the Ottoman Empire from 1566 until his death in 1574. He was a son of Suleiman the Magnificent and Haseki Hürrem Sultan. Selim had been an unlikely candidate for the throne until his brother Mehmed died of smallpox, his half-brother Mustafa was strangled to death by the order of his father, and his brother Bayezid was killed in a coordinated effort between him and his father.

Selim was born in Constantinople (Istanbul), on 28 May 1524, during the reign of his father Suleiman the Magnificent. His mother was Hürrem Sultan, a slave and concubine who was born an Orthodox priest's daughter, and later was freed and became Suleiman's legal wife.

In 1545, at Konya, Selim married Nurbanu Sultan, whose background is disputed. It is said that she was originally named Cecelia Venier Baffo, or Rachel, or Kale Katenou. She was the mother of Murad III, Selim's successor. 

Hubbi Hatun, a famous poet of the sixteenth century, was a lady-in-waiting to him.

Selim II gained the throne after palace intrigue and fraternal dispute, succeeding as sultan on 7 September 1566. Selim II became the first sultan devoid of active military interest and willing to abandon power to his ministers, provided he was left free to pursue his orgies and debauches. Therefore, he became known as Selim the Drunkard or Selim the Sot (Turkish: Sarhos Selim). Selim's Grand Vizier, Mehmed Sokollu, a native of what is now Bosnia and Herzegovina, controlled much of state affairs, and two years after Selim's accession succeeded in concluding at Constantinople a treaty (17 February 1568) with the Habsburg Holy Roman Emperor, Maximilian II, whereby the Emperor agreed to pay an annual "present" of 30,000 ducats and granted the Ottomans authority in Moldavia and Walachia.

Against Russia Selim was less fortunate. A plan had been prepared in Istanbul for uniting the Volga and Don by a canal in order to counter Russian expansion toward the Ottomans' northern frontier. In the summer of 1569 a large force of Janissaries and cavalry were sent to lay siege to Astrakhan and begin the canal works, while an Ottoman fleet besieged Azov. However, a sortie from the Astrakhan garrison drove back the besiegers. A Russian relief army of 15,000 attacked and scattered the workmen and the Tatar force sent for their protection. The Ottoman fleet was then destroyed by a storm. Early in 1570 the ambassadors of Ivan IV of Russia concluded at Constantinople a treaty that restored friendly relations between the Sultan and the Tsar.

Expeditions in the Hejaz and Yemen were more successful, but the conquest of Cyprus in 1571, which provided Selim with his favourite vintage, led to the naval defeat against Spain and Italian states in the Battle of Lepanto in the same year.

The Empire's shattered fleets were soon restored (in just six months, it consisted of about 150 galleys and eight galleasses), and the Ottomans maintained control of the eastern Mediterranean (1573). In August 1574, months before Selim's death, the Ottomans regained control of Tunis from Spain, which had captured it in 1572.

During Selim's reign, his elder sister Mihrimah Sultan acted as his Valide Sultan, because his mother Hürrem Sultan died before his reign began.

Selim is known for giving back to Mahidevran Gülbahar her status and her wealth, contrasted with his father's decision. He also built the tomb of his eldest brother, Şehzade Mustafa, who was executed in 1553.

Selim's first and only wife, Nurbanu Sultan, was a Venetian who was the mother of his successor Murad III and three of his daughters. As a Haseki Sultan she received 1,000 aspers a day, while lower-ranking concubines who were the mothers of princes received 40 aspers a day. Selim bestowed upon Nurbanu 110,000 ducats as a dowry, surpassing the 100,000 ducats that his father bestowed upon Hürrem Sultan. According to a privy register cited by Leslie Pierce, Selim had four other concubines, each one the mother of a single prince. Augusta Hamilton records that he had two thousand concubines.


Selim had seven sons:

Selim had at five daughters:




<BR>

[aged 50]


</doc>
<doc id="28224" url="https://en.wikipedia.org/wiki?curid=28224" title="Smith">
Smith

Smith may refer to:












</doc>
<doc id="28226" url="https://en.wikipedia.org/wiki?curid=28226" title="Show business">
Show business

Show business, sometimes shortened to show biz or showbiz (since 1945), is a vernacular term for all aspects of the entertainment industry. From the business side (including managers, agents, producers, and distributors), the term applies to the creative element (including artists, performers, writers, musicians, and technicians) and was in common usage throughout the 20th century, although the first known use in print dates from 1850. At that time and for several decades, it typically included an initial "the". By the latter part of the century, it had acquired a slightly arcane quality associated with the era of variety, but the term is still in active use. In modern entertainment industry, it is also associated with the fashion industry (creating trend and fashion) and acquiring intellectual property rights from the invested research in the entertainment business.

The entertainment sector can be split up into the following subsectors:




</doc>
<doc id="28230" url="https://en.wikipedia.org/wiki?curid=28230" title="Speaker for the Dead">
Speaker for the Dead

Speaker for the Dead is a 1986 science fiction novel by Orson Scott Card and an indirect sequel to the novel "Ender's Game". The book takes place around the year 5270, some 3,000 years after the events in "Ender's Game". However, because of relativistic space travel, Ender himself is only about 35 years older.

This is the first book to discuss the Starways Congress, a high standpoint Legislation for the human colonies. It is also the first to describe the Hundred Worlds, the planets with human colonies that are tightly intertwined by Ansible technology.

Like "Ender's Game", the book won the Nebula Award in 1986 and the Hugo Award in 1987. "Speaker for the Dead" was published in a slightly revised edition in 1991. It was followed by "Xenocide" and "Children of the Mind".

Following the xenocide of the Formic species by his own hand (in "Ender's Game"), Ender Wiggin writes a book under the pseudonym "Speaker for the Dead" called "The Hive Queen", describing the life of the Formics as described to him by the dormant Formic queen which he secretly carries. As humanity uses light-speed travel to establish new colonies, Ender quietly travels with them along with his sister Valentine to find a home for the Formic Queen to restart her species. Ender's older brother, the aged Hegemon Peter Wiggin, recognizes Ender's writings in "The Hive Queen", and requests Ender write for him once he dies. Ender agrees, and authors "The Hegemon". These two books create a new religious movement for Speakers for the Dead who have full authority to investigate a person and their work after their death, and speak without judgement about the essence of them in eulogy.

Some three thousand years after the xenocide, a human colony is established on the planet Lusitania. The planet is home to a sentient species of mammalian forest dwellers that largely resemble pigs. The colonists (who primarily speak Portuguese) dub them "Pequeninos" (Portuguese for little ones) but they are often referred to as "the piggies" due to their physical resemblance to earth swine. The Pequeninos prove to be of great interest to xenobiologists and due to the fact that humans had previously wiped out the only sentient species they'd encountered (the formics), special care is taken to ensure no similar mistakes are made with the Pequeninos. The colony is strictly regulated to allow only limited contact with the Pequeninos to those xenobiologists, and to not share human technology with them. However, shortly after the colony's founding, many of the colonists die from the "Descolada" virus before a cure is found. 

Xenologer Pipo and his son Libo raise Novinha whose parents died from "Descolada". Pipo has developed a friendship with the Pequeninos, finding that it is a matriarchal society with the females segregated from the males, and that their belief system centers around the trees of the forests. Pipo finds one of the males dead, his body eviscerated and a sapling planted within the body, believing this to be their funeral rites. Meanwhile, Novinha discovers that every lifeform on Lusitania carries the "Descolada" virus, and while lethal to humans, appears to serve a beneficial purpose to native lifeforms. When Pipo learns of this, he suddenly has an insight, and before he tells the others, races off to talk to the Pequeninos. Pipo's body is later found cut open, but no sapling planted within the body. As Pipo's death appears unprovoked, the Pequeninos are now considered a threat by the Starways Congress and a fence is erected to protect the colony, with access strictly regulated. Novinha, having fallen in love with Libo but fearing that he will find out from Pipo's files what led to his death, marries another colonist, Marcos Ribeira, so as to lock her files from being opened, under colony law. Emotionally driven, she then makes a call for a Speaker for the Dead for Pipo.

Ender responds to Novinha's call, and as Valentine is pregnant with her child, he decides to travel alone save for an artificial intelligence named Jane that appears to live in the ansible network that enables faster-than-light communications. After relativistic travel, Ender arrives at Lusitania 22 years later, finding that Novinha had canceled her request for a Speaker, but in the intervening time, not only has Libo also died in a similar manner to Pipo, Marcos recently died, and Novinha's children, Ela and Miro, have requested a Speaker for Libo and Marcos. Ender, gaining access to all of the appropriate files, learns of tension since Pipo's death; Novinha has turned away from xenobiology to study crop growth, which created a loveless relationship with Marcos, while Miro has secretly worked with Ouanda to continue to study the Pequeninos, while sharing human technology and knowledge with them. Over the course of time, Miro and Ouanda have fallen in love. With Ender's arrival, Miro tells him that one of the Pequeninos, Human, has taken a great interest in Ender, and Ender becomes aware that Human can hear messages from the Formic Hive Queen. Ender and Jane discover that Marcos was infertile: all six of Novinha's children, including Miro, were fathered by Libo, who is also Ouanda's father. Ender also learns what Pipo had seen in Novinha's data.

As word of Miro's and Ouanda's illegal sharing of human technology with the Pequeninos is reported to Congress, Ender secretly goes to meet with the Pequeninos. They know his true identity, and they implore him to help them be part of humanity, while the Formic Queen tells Ender that Lusitania would be an ideal place to restart the hive, as her race can help guide the Pequeninos. By the time Ender returns to the colony, Congress has ordered Miro and Ouanda to be sent off-planet for penal action and the colony to be disbanded. Ender delivers his eulogy for Marcos, revealing Novinha's infidelity. Miro, distraught at the implications for his relationship with Ouanda, attempts to escape to hide with the Pequeninos, but he suffers neurological damage as he tries to cross the fence. Ender reveals to the colony what he discovered that Pipo had learned: that every life form on Lusitania is paired with another through the "Descolada" virus, so that the death of one births the other, and in the case of the Pequeninos, they become trees when they die. Libo and Pipo learned of this, but their deaths were a respectful misunderstanding by the Pequeninos. The colony leaders recognize Ender's words, and they agree to rebel against Congress, severing their ansible connection and deactivating the fence, allowing Ender, Ouanda, and Ela to go with Human to speak to the Pequenino wives, to help establish a case to present to Congress.

The Pequenino wives help Ender to corroborate the complex life cycle of the Pequeninos, affirming that the death ritual Pipo observed was to help create "fathertrees" who fertilize the Pequenino females to continue their race. The Pequeninos believed they were honoring Pipo, and later Libo, by helping them become fathertrees, but Ender explains that humans lack this "third life", and if the Pequeninos are to cohabitate with humans, they must respect this difference. To affirm their understanding, Human allows Ender to perform the ritual of "killing" him to take him into his "third life" as a fathertree, providing Ouanda with the confirmation needed to present to Congress.

Miro recovers from most of the physical damage from his encounter with the fence, but he is still paralytic. Valentine and her family inform Ender they plan to help Lusitania with the revolt, and they are traveling to help; Ender has Miro meet them halfway. Novinha, having gained understanding into the death of Pipo and Libo, finally absolves herself of her guilt, and she and Ender marry. Ender plants the Hive Queen as per her request, and he writes his third book, a biography of the life of the Pequenino, Human.

At the Los Angeles Times Book Festival (April 20, 2013), Card stated why he does not want "Speaker for the Dead" made into a film: 
""Speaker for the Dead" is unfilmable," Card said in response to a question from the audience. "It consists of talking heads, interrupted by moments of excruciating and unwatchable violence. Now, I admit, there's plenty of unwatchable violence in film, but never attached to my name. "Speaker for the Dead", I don't want it to be filmed. I can't imagine it being filmed."

Card writes in his introduction to the 1991 edition that he has received letters from readers who have conducted "Speakings" at funerals.





</doc>
<doc id="28232" url="https://en.wikipedia.org/wiki?curid=28232" title="Star catalogue">
Star catalogue

A star catalogue (Commonwealth English) or star catalog (American English), is an astronomical catalogue that lists stars. In astronomy, many stars are referred to simply by catalogue numbers. There are a great many different star catalogues which have been produced for different purposes over the years, and this article covers only some of the more frequently quoted ones. Star catalogues were compiled by many different ancient people, including the Babylonians, Greeks, Chinese, Persians, and Arabs. Most modern catalogues are available in electronic format and can be freely downloaded from space agencies data center.

Completeness and accuracy is described by the weakest apparent magnitude V (largest number) and the accuracy of the positions.

From their existing records, it is known that the ancient Egyptians recorded the names of only a few identifiable constellations and a list of thirty-six decans that were used as a star clock. The Egyptians called the circumpolar star 'the star that cannot perish' and, although they made no known formal star catalogues, they nonetheless created extensive star charts of the night sky which adorn the coffins and ceilings of tomb chambers.

Although the ancient Sumerians were the first to record the names of constellations on clay tablets, the earliest known star catalogues were compiled by the ancient Babylonians of Mesopotamia in the late 2nd millennium BC, during the Kassite Period ("ca". 1531 BC to "ca". 1155 BC). They are better known by their Assyrian-era name 'Three Stars Each'. These star catalogues, written on clay tablets, listed thirty-six stars: twelve for 'Anu' along the celestial equator, twelve for 'Ea' south of that, and twelve for 'Enlil' to the north. The Mul.Apin lists, dated to sometime before the Neo-Babylonian Empire (626–539 BC), are direct textual descendants of the 'Three Stars Each' lists and their constellation patterns show similarities to those of later Greek civilization.

In Ancient Greece, the astronomer and mathematician Eudoxus laid down a full set of the classical constellations around 370 BC. His catalogue "Phaenomena", rewritten by Aratus of Soli between 275 and 250 BC as a didactic poem, became one of the most consulted astronomical texts in antiquity and beyond. It contains descriptions of the positions of the stars, the shapes of the constellations and provided information on their relative times of rising and setting.

Approximately in the 3rd century BC, the Greek astronomers Timocharis of Alexandria and Aristillus created another star catalogue. Hipparchus (c. 190 – c. 120 BC) completed his star catalogue in 129 BC, which he compared to Timocharis' and discovered that the longitude of the stars had changed over time. This led him to determine the first value of the precession of the equinoxes. In the 2nd century, Ptolemy (c. 90 – c. 186 AD) of Roman Egypt published a star catalogue as part of his "Almagest", which listed 1,022 stars visible from Alexandria. Ptolemy's catalogue was based almost entirely on an earlier one by Hipparchus. It remained the standard star catalogue in the Western and Arab worlds for over eight centuries. The Islamic astronomer al-Sufi updated it in 964, and the star positions were redetermined by Ulugh Beg in 1437, but it was not fully superseded until the appearance of the thousand-star catalogue of Tycho Brahe in 1598.

Although the ancient Vedas of India specified how the ecliptic was to be divided into twenty-eight "nakshatra", Indian constellation patterns were ultimately borrowed from Greek ones sometime after Alexander's conquests in Asia in the 4th century BC.

The earliest known inscriptions for Chinese star names were written on oracle bones and date to the Shang Dynasty (c. 1600 – c. 1050 BC). Sources dating from the Zhou Dynasty (c. 1050 – 256 BC) which provide star names include the "Zuo Zhuan", the "Shi Jing", and the "Canon of Yao" (堯典) in the "Book of Documents". The "Lüshi Chunqiu" written by the Qin statesman Lü Buwei (d. 235 BC) provides most of the names for the twenty-eight mansions (i.e. asterisms across the ecliptic belt of the celestial sphere used for constructing the calendar). An earlier lacquerware chest found in the Tomb of Marquis Yi of Zeng (interred in 433 BC) contains a complete list of the names of the twenty-eight mansions. Star catalogues are traditionally attributed to Shi Shen and Gan De, two rather obscure Chinese astronomers who may have been active in the 4th century BC of the Warring States period (403–221 BC). The "Shi Shen astronomy" (石申天文, Shi Shen tienwen) is attributed to Shi Shen, and the "Astronomic star observation" (天文星占, Tianwen xingzhan) to Gan De.

It was not until the Han Dynasty (202 BC – 220 AD) that astronomers started to observe and record names for all the stars that were apparent (to the naked eye) in the night sky, not just those around the ecliptic. A star catalogue is featured in one of the chapters of the late 2nd-century-BC history work "Records of the Grand Historian" by Sima Qian (145–86 BC) and contains the "schools" of Shi Shen and Gan De's work (i.e. the different constellations they allegedly focused on for astrological purposes). Sima's catalogue—the "Book of Celestial Offices" (天官書 Tianguan shu)—includes some 90 constellations, the stars therein named after temples, ideas in philosophy, locations such as markets and shops, and different people such as farmers and soldiers. For his "Spiritual Constitution of the Universe" (靈憲, Ling Xian) of 120 AD, the astronomer Zhang Heng (78–139 AD) compiled a star catalogue comprising 124 constellations. Chinese constellation names were later adopted by the Koreans and Japanese.

A large number of star catalogues were published by Muslim astronomers in the medieval Islamic world. These were mainly "Zij" treatises, including Arzachel's "Tables of Toledo" (1087), the Maragheh observatory's "Zij-i Ilkhani" (1272) and Ulugh Beg's "Zij-i-Sultani" (1437). Other famous Arabic star catalogues include Alfraganus' "A compendium of the science of stars" (850) which corrected Ptolemy's "Almagest"; and Azophi's "Book of Fixed Stars" (964) which described observations of the stars, their positions, magnitudes, brightness and colour, drawings for each constellation, and the first descriptions of Andromeda Galaxy and the Large Magellanic Cloud. Many stars are still known by their Arabic names (see List of Arabic star names).

The "Motul Dictionary", compiled in the 16th century by an anonymous author (although attributed to Fray Antonio de Ciudad Real), contains a list of stars originally observed by the ancient Mayas. The Maya Paris Codex also contain symbols for different constellations which were represented by mythological beings.

Two systems introduced in historical catalogues remain in use to the present day. The first system comes from the German astronomer Johann Bayer's "Uranometria", published in 1603 and regarding bright stars. These are given a Greek letter followed by the genitive case of the constellation in which they are located; examples are Alpha Centauri or Gamma Cygni. The major problem with Bayer's naming system was the number of letters in the Greek alphabet (24). It was easy to run out of letters before running out of stars needing names, particularly for large constellations such as Argo Navis. Bayer extended his lists up to 67 stars by using lower-case Roman letters ("a" through "z") then upper-case ones ("A" through "Q"). Few of those designations have survived. It is worth mentioning, however, as it served as the starting point for variable star designations, which start with "R" through "Z", then "RR", "RS", "RT"..."RZ", "SS", "ST"..."ZZ" and beyond.

The second system comes from the English astronomer John Flamsteed's "Historia coelestis Britannica" (1725). It kept the genitive-of-the-constellation rule for the back end of his catalogue names, but used numbers instead of the Greek alphabet for the front half. Examples include 61 Cygni and 47 Ursae Majoris.

Bayer and Flamsteed covered only a few thousand stars between them. In theory, full-sky catalogues try to list every star in the sky. There are, however, billions of stars resolvable by telescopes, so this is an impossible goal; with this kind of catalog, an attempt is generally made to get every star brighter than a given magnitude.

Jérôme Lalande published the "Histoire Céleste Française" in 1801, which contained an extensive star catalog, among other things. The observations made were made from the Paris Observatory and so it describes mostly northern stars. This catalogue contained the positions and magnitudes of 47,390 stars, out to magnitude 9, and was the most complete catalogue up to that time. A significant reworking of this catalogue in 1846 added reference numbers to the stars that are used to refer to some of these stars to this day. The decent accuracy of this catalogue kept it in common use as a reference by observatories around the world throughout the 19th century.

The Henry Draper Catalogue was published in the period 1918–1924. It covers the whole sky down to about ninth or tenth magnitude, and is notable as the first large-scale attempt to catalogue spectral types of stars.
The catalogue was compiled by Annie Jump Cannon and her co-workers at Harvard College Observatory under the supervision of Edward Charles Pickering, and was named in honour of Henry Draper, whose widow donated the money required to finance it.

HD numbers are widely used today for stars which have no Bayer or Flamsteed designation. Stars numbered 1–225300 are from the original catalogue and are numbered in order of right ascension for the 1900.0 epoch. Stars in the range 225301–359083 are from the 1949 extension of the catalogue. The notation HDE can be used for stars in this extension, but they are usually denoted HD as the numbering ensures that there can be no ambiguity.

The Smithsonian Astrophysical Observatory catalogue was compiled in 1966 from various previous astrometric catalogues, and contains only the stars to about ninth magnitude for which accurate proper motions were known. There is considerable overlap with the Henry Draper catalogue, but any star lacking motion data is omitted. The epoch for the position measurements in the latest edition is J2000.0. The SAO catalogue contains this major piece of information not in Draper, the proper motion of the stars, so it is often used when that fact is of importance. The cross-references with the Draper and Durchmusterung catalogue numbers in the latest edition are also useful.

Names in the SAO catalogue start with the letters SAO, followed by a number. The numbers are assigned following 18 ten-degree bands in the sky, with stars sorted by right ascension within each band.

The "Bonner Durchmusterung" ("German": Bonn sampling) and follow-ups were the most complete of the pre-photographic star catalogues.

The "Bonner Durchmusterung" itself was published by Friedrich Wilhelm Argelander, Adalbert Krüger, and Eduard Schönfeld between 1852 and 1859. It covered 320,000 stars in epoch 1855.0.

As it covered only the northern sky and some of the south (being compiled from the Bonn observatory), this was then supplemented by the "Südliche Durchmusterung " (SD), which covers stars between declinations −1 and −23 degrees
(1886, 120,000 stars). It was further supplemented by the "Cordoba Durchmusterung" (580,000 stars), which began to be compiled at Córdoba, Argentina in 1892 under the initiative of John M. Thome and covers declinations −22 to −90. Lastly, the "Cape Photographic Durchmusterung" (450,000 stars, 1896), compiled at the Cape, South Africa, covers declinations −18 to −90.

Astronomers preferentially use the HD designation of a star, as that catalogue also gives spectroscopic information, but as the Durchmusterungs cover more stars they occasionally fall back on the older designations when dealing with one not found in Draper. Unfortunately, a lot of catalogues cross-reference the Durchmusterungs without specifying which one is used in the zones of overlap, so some confusion often remains.

Star names from these catalogues include the initials of which of the four catalogues they are from (though the "Southern" follows the example of the "Bonner" and uses BD; CPD is often shortened to CP), followed by the angle of declination of the star (rounded towards zero, and thus ranging from +00 to +89 and −00 to −89), followed by an arbitrary number as there are always thousands of stars at each angle. Examples include BD+50°1725 or CD−45°13677.

The "Catalogue astrographique" (Astrographic Catalogue) was part of the international "Carte du Ciel" programme designed to photograph and measure the positions of all stars brighter than magnitude 11.0. In total, over 4.6 million stars were observed, many as faint as 13th magnitude. This project was started in the late 19th century. The observations were made between 1891 and 1950. To observe the entire celestial sphere without burdening too many institutions, the sky was divided among 20 observatories, by declination zones. Each observatory exposed and measured the plates of its zone, using a standardized telescope (a "normal astrograph") so each plate photographed had a similar scale of approximately 60 arcsecs/mm. The U.S. Naval Observatory took over custody of the catalogue, now in its 2000.2 edition.

USNO-B1.0 is an all-sky catalogue created by research and operations astrophysicists at the U.S. Naval Observatory (as developed at the United States Naval Observatory Flagstaff Station), that presents positions, proper motions, magnitudes in various optical passbands, and star/galaxy estimators for 1,042,618,261 objects derived from 3,643,201,733 separate observations. The data was obtained from scans of 7,435 Schmidt plates taken for the various sky surveys during the last 50 years. USNO-B1.0 is believed to provide all-sky coverage, completeness down to V = 21, 0.2 arcsecond astrometric accuracy at J2000.0, 0.3 magnitude photometric accuracy in up to five colors, and 85% accuracy for distinguishing stars from non-stellar objects. USNO-B is now followed by NOMAD; both can be found on the Naval Observatory server. The Naval Observatory is currently working on B2 and C variants of the USNO catalogue series.

The "Guide Star Catalog" is an online catalogue of stars produced for the purpose of accurately positioning and identifying stars satisfactory for use as guide stars by the Hubble Space Telescope program. The first version of the catalogue was produced in the late 1980s by digitizing photographic plates and contained about 20 million stars, out to about magnitude 15. The latest version of this catalogue contains information for 945,592,683 stars, out to magnitude 21. The latest version continues to be used to accurately position the Hubble Space Telescope.

Gaia DR1, the first data release of the spacecraft "Gaia" mission, based on 14 months of observations made through September 2015, took place on September 13, 2016. The data release includes positions and magnitudes in a single photometric band for 1.1 billion stars using only "Gaia" data, positions, parallaxes and proper motions for more than 2 million stars based on a combination of "Gaia" and Tycho-2 data for those objects in both catalogues, light curves and characteristics for about 3000 variable stars, and positions and magnitudes for more than 2000 extragalactic sources used to define the celestial reference frame. Data from this DR1 release can be accessed at the "Gaia" archive. The full "Gaia" catalogue will be released in 2022.

Specialized catalogues make no effort to list all the stars in the sky, working instead to highlight a particular type of star, such as variables or nearby stars.

Aitken's double star catalogue (1932) lists 17,180 double stars north of declination −30 degrees.

First published in 1930 as the "Yale Catalog of Bright Stars", this catalogue contained information on all stars brighter than visual magnitude 6.5 in the "Harvard Revised Photometry Catalogue". The list was revised in 1983 with the publication of a supplement that listed additional stars down to magnitude 7.1. The catalogue detailed each star's coordinates, proper motions, photometric data, spectral types, and other useful information.

The last printed version of the Bright Star Catalogue was the 4th revised edition, released in 1982. The 5th edition is in electronic form and is available online.

Stephenson's General Catalogue of galactic Carbon stars is a catalogue of 7000+ carbon stars.

The Gliese (later Gliese-Jahreiß) catalogue attempts to list all star systems within of Earth ordered by right ascension (see the List of nearest stars). Later editions expanded the coverage to . Numbers in the range 1.0–915.0 (Gl numbers) are from the second edition, which was

The integers up to 915 represent systems which were in the first edition. Numbers with a decimal point were used to insert new star systems for the second edition without destroying the desired order (by right ascension). This catalogue is referred to as CNS2, although this name is never used in catalogue numbers.

Numbers in the range 9001–9850 (Wo numbers) are from the supplement

Numbers in the ranges 1000–1294 and 2001–2159 (GJ numbers) are from the supplement

The range 1000–1294 represents nearby stars, while 2001–2159 represents suspected nearby stars. In the literature, the GJ numbers are sometimes retroactively extended to the Gl numbers (since there is no overlap). For example, Gliese 436 can be interchangeably referred to as either Gl 436 or GJ 436.

Numbers in the range 3001–4388 are from

Although this version of the catalogue was termed "preliminary", it is still the current one , and is referred to as CNS3. It lists a total of 3,803 stars. Most of these stars already had GJ numbers, but there were also 1,388 which were not numbered. The need to give these 1,388 "some" name has resulted in them being numbered 3001–4388 (NN numbers, for "no name"), and data files of this catalogue now usually include these numbers. An example of a star which is often referred to by one of these unofficial GJ numbers is GJ 3021.

The General Catalogue of Trigonometric Parallaxes, first published in 1952 and later superseded by the New GCTP (now in its fourth edition), covers nearly 9,000 stars. Unlike the Gliese, it does not cut off at a given distance from the Sun; rather it attempts to catalogue all known measured parallaxes. It gives the co-ordinates in 1900 epoch, the secular variation, the proper motion, the weighted average absolute parallax and its standard error, the number of parallax observations, quality of interagreement of the different values, the visual magnitude and various cross-identifications with other catalogues. Auxiliary information, including UBV photometry, MK spectral types, data on the variability and binary nature of the stars, orbits when available, and miscellaneous information to aid in determining the reliability of the data are also listed.

The Hipparcos catalogue was compiled from the data gathered by the European Space Agency's astrometric satellite "Hipparcos", which was operational from 1989 to 1993. The catalogue was published in June 1997 and contains 118,218 stars; an updated version with re-processed data was published in 2007. It is particularly notable for its parallax measurements, which are considerably more accurate than those produced by ground-based observations.

The PPM Star Catalogue is one of the best, both in the proper motion and star position till 1999. Not as precise as Hipparcos catalogue but with many more stars. The PPM was built from BD, SAO, HD and more, with sophisticated algorithm and is an extension for the Fifth Fundamental Catalogue, "Catalogues of Fundamental Stars".

A common way of detecting nearby stars is to look for relatively high proper motions. Several catalogues exist, of which we'll mention a few. The Ross and Wolf catalogues pioneered the domain:

Willem Jacob Luyten later produced a series of catalogues:

L - Luyten, Proper motion stars and White dwarfs
LFT – Luyten Five-Tenths catalogue

LHS – Luyten Half-Second catalogue

LTT – Luyten Two-Tenths catalogue

NLTT – New Luyten Two-Tenths catalogue

LPM – Luyten Proper-Motion catalogue

Around the same time period, Henry Lee Giclas worked on a similar series of catalogues:

The "ubvyβ Photoelectric Photometric Catalogue" is a compilation of previously published photometric data. Published in 1998, the catalogue includes 63,316 stars surveyed through 1996.

Stars evolve and move over time, making catalogues evolving, impermanent databases at even the most rigorous levels of production. The USNO catalogues are the most current and widely used astrometric catalogues available at present, and include USNO products such as USNO-B (the successor to USNO-A), NOMAD, UCAC and others in production or narrowly released. Some users may see specialized catalogues (more recent versions of the above), tailored catalogues, interferometrically-produced cataloges, dynamic catalogues, and those with updated positions, motions, colors, and improved errors. Catalogue data is continually collected at the Naval Observatory dark-sky facility, NOFS; and the latest refined, updated catalogues are reduced and produced by NOFS and the USNO. See the USNO Catalog and Image Servers for more information and access.






</doc>
<doc id="28233" url="https://en.wikipedia.org/wiki?curid=28233" title="Stellar designations and names">
Stellar designations and names

Designations and names of stars (and other celestial bodies) are currently primarily mediated in the scientific community by the International Astronomical Union (IAU), a de facto authority. The IAU states that it is keen to make a distinction between the terms "name" and "designation". To the IAU, "name" refers to the (usually colloquial) term used for a star in everyday speech, while "designation" is solely alphanumerical and used almost exclusively in official catalogues and for professional astronomy. Many of the names and some of the designations in use today were inherited from the time before the IAU existed. Other designations are being added all the time.

The Bright Star Catalogue, which is a star catalogue listing all stars of apparent magnitude 6.5 or brighter, or roughly every star visible to the naked eye from Earth, contains 9,096 stars. Pre-modern catalogues listed only the brightest of these. Hipparchus in the 2nd century BC enumerated about 850 stars. Johann Bayer in 1603 listed about twice this number. Only a minority of these have proper names; all others are designated by numbers from various catalogues. Only in the 19th century did star catalogues list the naked-eye stars exhaustively. The most voluminous modern catalogues list on the order of a billion stars, out of an estimated total of 200 to 400 billion in the Milky Way.

Several hundred of the brightest stars had traditional names, most of which derived from Arabic, but a few from Latin. There were a number of problems with these names, however:

In 2016, the IAU organized a Working Group on Star Names (WGSN) to catalog and standardize proper names for stars. The WGSN's first bulletin dated July 2016 included a table of 125 stars comprising the first two batches of names approved by the WGSN (on 30 June and 20 July 2016) together with names of stars (including four traditional star names: Ain, Edasich, Errai and Fomalhaut) reviewed and adopted by the IAU Executive Committee Working Group on Public Naming of Planets and Planetary Satellites during the 2015 NameExoWorlds campaign and recognized by the WGSN. Further batches of names were approved on 21 August, 12 September, 5 October and 6 November 2016. These were listed in a table of 102 stars included in the WGSN's second bulletin dated November 2016. The next additions were done on 1 February 2017 (13 new star names), 30 June 2017 (29), 5 September 2017 (41), 17 November 2017 (3) and 1 June 2018 (17). All 330 names are included in the current List of IAU-approved Star Names, last updated on 1 June 2018 (with a minor correction posted on 11 June 2018).

In practice, names are only universally used for the very brightest stars (Sirius, Arcturus, Vega, etc.) and for a small number of slightly less bright but "interesting" stars (Algol, Polaris, Mira, etc.). For other naked eye stars, the Bayer or Flamsteed designation is often preferred.

In addition to the traditional names, a small number of stars that are "interesting" can have modern English names. For instance, two second-magnitude stars, Alpha Pavonis and Epsilon Carinae, were assigned the proper names Peacock and Avior respectively in 1937 by Her Majesty's Nautical Almanac Office during the creation of "The Air Almanac", a navigational almanac for the Royal Air Force. Of the fifty-seven stars included in the new almanac, these two had no traditional names. The RAF insisted that all of the stars must have names, so new names were invented for them. These names have been approved by the IAU WGSN.

The book "" by R. H. Allen (1899) has had effects on star names:

A few stars are named for individuals. These are mostly names in common use that were taken up by the scientific community at some juncture. The first such case (discounting characters from mythology) was Cor Caroli (α CVn), named in the 17th century for Charles I of England. The remaining examples are mostly named after astronomers, the best known are probably Barnard's Star (which has the highest known proper motion of any star and is thus notable even though it is far too faint to be seen with the naked eye), Kapteyn's Star and recently Tabby's Star.

In July 2014 the IAU launched a process for giving proper names to exoplanets and their host stars. As a result, the IAU approved the names Cervantes for Mu Arae and Copernicus for 55 Cancri A.

In the absence of any better means of designating a star, catalogue numbers are generally used. Many star catalogues are used for this purpose; see star catalogues.

The first modern schemes for designating stars systematically labelled them within their constellation.


Full-sky star catalogues detach the star designation from the star's constellation and aim at enumerating all stars with apparent magnitude greater than a given cut-off value.


Variable stars that do not have Bayer designations are assigned designations in a variable star scheme that superficially extends the Bayer scheme with uppercase Latin letters followed by constellation names, starting with single letters R to Z, and proceeding to pairs of letters. Such designations mark them as variable stars. Examples include R Cygni, RR Lyrae, and GN Andromedae. (Many variable stars also have designations in other catalogues.)

When a planet is detected around a star, the star is often given a name and number based on the name of the telescope or survey mission that discovered it and based on how many planets have already been discovered by that mission e.g. HAT-P-9, WASP-1, COROT-1, Kepler-4, TRAPPIST-1.

Star naming rights are not available for sale via the IAU. Rather, star names are selected on a non-commercial basis by a small number of international organizations of astronomers, scientists, and registration bodies, who assign names consisting usually of a Greek letter followed by the star's constellation name, or less frequently based on their ancient traditional name.

However, there are a number of non-scientific "star-naming" companies that offer to assign personalized names to stars within their own private catalogs. These names are used only within that company (and usually available for viewing on their web site), and are not recognized by the astronomical community, or by competing star-naming companies. A survey conducted by amateur astronomers discovered that 54% of consumers would still want to "name a star" with a non-scientific star-naming company even though they have been warned or informed such naming is not recognized by the astronomical community.




</doc>
<doc id="28235" url="https://en.wikipedia.org/wiki?curid=28235" title="Space Shuttle Challenger">
Space Shuttle Challenger

Space Shuttle "Challenger" (Orbiter Vehicle Designation: OV-099) was the second orbiter of NASA's space shuttle program to be put into service, after "Columbia". "Challenger" was built by Rockwell International's Space Transportation Systems Division, in Downey, California. Its maiden flight, STS-6, began on April 4, 1983. The orbiter was launched and landed nine times before breaking apart 73 seconds into its tenth mission, STS-51-L, on January 28, 1986, resulting in the death of all seven crew members, including a civilian school teacher. It was the first of two shuttles to be destroyed in flight, the other being "Columbia", in 2003. The accident led to a two-and-a-half-year grounding of the shuttle fleet; flights resumed in 1988, with STS-26 flown by "Discovery". "Challenger" was replaced by "Endeavour", which was built from structural spares ordered by NASA in the construction contracts for "Discovery" and "Atlantis".

"Challenger" was named after HMS "Challenger", a British corvette that was the command ship for the "Challenger" Expedition, a pioneering global marine research expedition undertaken from 1872 through 1876. The Apollo 17 lunar module, which landed on the Moon in 1972, also was named "Challenger".

Because of the low production volume of orbiters, the Space Shuttle program decided to build a vehicle as a Structural Test Article, STA-099, that could later be converted to a flight vehicle. The contract for STA-099 was awarded to North American Rockwell on July 26, 1972, and construction was completed in February 1978. After STA-099's rollout, it was sent to a Lockheed test site in Palmdale, where it spent over 11 months in vibration tests designed to simulate entire shuttle flights, from launch to landing. To prevent damage during structural testing, qualification tests were performed to a safety factor of 1.2 times the design limit loads. The qualification tests were used to validate computational models, and compliance with the required 1.4 factor of safety was shown by analysis. STA-099 was essentially a complete airframe of a Space Shuttle orbiter, with only a mockup crew module installed and thermal insulation placed on its forward fuselage.

NASA planned to refit the prototype orbiter "Enterprise" (OV-101), used for flight testing, as the second operational orbiter; but "Enterprise" lacked most of the systems needed for flight, including a functional propulsion system, thermal insulation, a life support system, and most of the cockpit instrumentation. Modifying it for spaceflight would have been far too difficult, expensive, and time-consuming. Since STA-099 was not as far along in the construction of its airframe, it would be easier to upgrade to a flight article. Because STA-099's qualification testing prevented damage, NASA found that rebuilding STA-099 as OV-099 would be less expensive than refitting "Enterprise". Work on converting STA-099 into "Challenger" began in January 1979, starting with the crew module (the pressurized portion of the vehicle) because the rest of the orbiter was still used by Lockheed. STA-099 returned to the Rockwell plant in November 1979, and the original, unfinished crew module was replaced with the newly constructed model. Major parts of STA-099, including the payload bay doors, body flap, wings, and vertical stabilizer, also had to be returned to their individual subcontractors for rework. By early 1981, most of these components had returned to Palmdale and were reinstalled on the orbiter. Work continued on the conversion until July 1982.

"Challenger", as did the orbiters built after it, had fewer tiles in its Thermal Protection System than "Columbia", though it still made heavier use of the white LRSI tiles on the cabin and main fuselage than did the later orbiters. Most of the tiles on the payload bay doors, upper wing surfaces, and rear fuselage surfaces were replaced with DuPont white Nomex felt insulation. These modifications and an overall lighter structure allowed "Challenger" to carry 2,500 lb (1,100 kg) more payload than "Columbia." "Challenger's" fuselage and wings were also stronger than "Columbia's" despite being lighter. The hatch and vertical-stabilizer tile patterns were also different from those of the other orbiters. "Challenger" was also the first orbiter to have a head-up display system for use in the descent phase of a mission, and the first to feature Phase I main engines rated for 104% maximum thrust.

After its first flight in April 1983, "Challenger" flew on 85% of all Space Shuttle missions. Even when the orbiters "Discovery" and "Atlantis" joined the fleet, "Challenger" flew three missions a year from 1983 to 1985. "Challenger", along with "Discovery", was modified at Kennedy Space Center to be able to carry the Centaur-G upper stage in its payload bay. If flight STS-51-L had been successful, "Challenger"'s next mission would have been the deployment of the "Ulysses" probe with the Centaur to study the polar regions of the Sun.

"Challenger" flew the first American woman, African-American, Dutchman and Canadian into space; carried three Spacelab missions; and performed the first night launch and night landing of a Space Shuttle. "Challenger" was also the first space shuttle to be destroyed in an accident during a mission. The collected debris of the vessel is currently buried in decommissioned missile silos at Launch Complex 31, Cape Canaveral Air Force Station. A section of the fuselage recovered from Space Shuttle "Challenger" can also be found at the “Forever Remembered” memorial at the Kennedy Space Center Visitor Complex in Florida. From time to time, further pieces of debris from the orbiter wash up on the Florida coast. When this happens, they are collected and transported to the silos for storage. Because of its early loss, "Challenger" was the only space shuttle that never wore the NASA "meatball" logo, and was never modified with the MEDS "glass cockpit". The tail was never fitted with a drag chute – it was fitted to the remaining orbiters in 1992. Also because of its early demise "Challenger" was also one of only two shuttles that never visited the Mir Space Station or the International Space Station – the other one being its sister ship "Columbia".





</doc>
<doc id="28236" url="https://en.wikipedia.org/wiki?curid=28236" title="Space Shuttle Enterprise">
Space Shuttle Enterprise

Space Shuttle "Enterprise" (Orbiter Vehicle Designation: OV-101) was the first orbiter of the Space Shuttle system. Rolled out on September 17, 1976, it was built for NASA as part of the Space Shuttle program to perform atmospheric test flights after being launched from a modified Boeing 747. It was constructed without engines or a functional heat shield, and was therefore not capable of spaceflight.

Originally, "Enterprise" had been intended to be refitted for orbital flight to become the second space-rated orbiter in service. However, during the construction of , details of the final design changed, making it simpler and less costly to build around a body frame that had been built as a test article. Similarly, "Enterprise" was considered for refit to replace "Challenger" after the latter was destroyed, but was built from structural spares instead.

"Enterprise" was restored and placed on display in 2003 at the Smithsonian's new Steven F. Udvar-Hazy Center in Virginia. Following the retirement of the Space Shuttle fleet, replaced "Enterprise" at the Udvar-Hazy Center, and "Enterprise" was transferred to the Intrepid Sea, Air & Space Museum in New York City, where it has been on display since July 2012.

The design of "Enterprise" was not the same as that planned for , the first flight model; the aft fuselage was constructed differently, and it did not have the interfaces to mount OMS pods. A large number of subsystems—ranging from main engines to radar equipment—were not installed on "Enterprise", but the capacity to add them in the future was retained, as NASA originally intended to refit the orbiter for spaceflight at the conclusion of its testing. Instead of a thermal protection system, its surface was primarily covered with simulated tiles made from polyurethane foam. Fiberglass was used for the leading edge panels in place of the reinforced carbon–carbon ones of spaceflight-worthy orbiters. Only a few sample thermal tiles and some Nomex blankets were real. "Enterprise" used fuel cells to generate its electrical power, but these were not sufficient to power the orbiter for spaceflight.

"Enterprise" also lacked RCS thrusters (which were useless in atmospheric flight) and hydraulic mechanisms for the landing gear; the landing gear doors were simply opened through the use of explosive bolts and the gear dropped down solely by gravity. As it was only used for atmospheric testing, "Enterprise" featured a large nose probe mounted on its nose cap, common on test aircraft because the location provides the most accurate readings for the test instruments, being mounted out in front of the disturbed airflow.

"Enterprise" was equipped with Lockheed-manufactured zero-zero ejection seats like those its sister carried on its first four missions.

Construction began on "Enterprise" on June 4, 1974. Designated OV-101, it was originally planned to be named "Constitution" and unveiled on Constitution Day, September 17, 1976. Fans of asked US President Gerald Ford, through a letter-writing campaign, to name the orbiter after the television show's fictional starship, USS "Enterprise". White House advisors cited "hundreds of thousands of letters" from Trekkies, "one of the most dedicated constituencies in the country", as a reason for giving the shuttle the name. Although Ford did not publicly mention the campaign, the president said that he was "partial to the name" "Enterprise", and directed NASA officials to change the name.

In mid-1976 the orbiter was used for ground vibration tests, allowing engineers to compare data from an actual flight vehicle with theoretical models.

On September 17, 1976, "Enterprise" was rolled out of Rockwell's plant at Palmdale, California. In recognition of its fictional namesake, "Star Trek" creator Gene Roddenberry and most of the principal cast of the original series of "Star Trek" were on hand at the dedication ceremony.

On January 31, 1977, "Enterprise" was taken by road to Dryden Flight Research Center at Edwards Air Force Base to begin operational testing.

While at NASA Dryden "Enterprise" was used by NASA for a variety of ground and flight tests intended to validate aspects of the shuttle program. The initial nine-month testing period was referred to by the acronym ALT, for "Approach and Landing Test". These tests included a maiden "flight" on February 18, 1977, atop a Boeing 747 Shuttle Carrier Aircraft (SCA) to measure structural loads and ground handling and braking characteristics of the mated system. Ground tests of all orbiter subsystems were carried out to verify functionality prior to atmospheric flight.

The mated "Enterprise"/SCA combination was then subjected to five test flights with "Enterprise" unmanned and unactivated. The purpose of these test flights was to measure the flight characteristics of the mated combination. These tests were followed with three test flights with "Enterprise" manned to test the shuttle flight control systems.

On August 12, 1977, "Enterprise" flew on its own for the first time. "Enterprise" underwent four more free flights where the craft separated from the SCA and was landed under astronaut control. These tests verified the flight characteristics of the orbiter design and were carried out under several aerodynamic and weight configurations. The first three flights were flown with a tailcone placed at the end of "Enterprise" aft fuselage, which reduced drag and turbulence when mated to the SCA. The final two flights saw the tailcone removed and mockup main engines installed. On the fifth and final glider flight, pilot-induced oscillation problems were revealed, which had to be addressed before the first orbital launch occurred.

Following the conclusion of the ALT test flight program, on March 13, 1978, "Enterprise" was flown once again, but this time halfway across the country to NASA's Marshall Space Flight Center (MSFC) in Alabama for the Mated Vertical Ground Vibration Testing (MGVT). The orbiter was lifted up on a sling very similar to the one used at Kennedy Space Center and placed inside the Dynamic Test Stand building, and there mated to the Vertical Mate Ground Vibration Test tank (VMGVT-ET), which in turn was attached to a set of inert Solid Rocket Boosters (SRB) to form a complete shuttle launch stack, and marked the first time in the program's history that all Space Shuttle elements, an Orbiter, an External Tank (ET), and two SRBs, were mated together. During the course of the program, "Enterprise" and the rest of the launch stack would be exposed to a punishing series of vibration tests simulating as closely as possible those expected during various phases of launch, some tests with and others without the SRBs in place.

At the conclusion of this testing, "Enterprise" was due to be taken back to Palmdale for retrofitting as a fully spaceflight capable vehicle. Under this arrangement, "Enterprise" would be launched on its maiden spaceflight in July 1981 to launch a communications satellite and retrieve the Long Duration Exposure Facility, then planned for a 1980 release on the first operational orbiter, "Columbia". Afterwards, "Enterprise" would conduct two Spacelab missions. However, in the period between the rollout of "Enterprise" and the rollout of "Columbia", a number of significant design changes had taken place, particularly with regard to the weight of the fuselage and wings, which meant retrofitting the prototype would have been a much more expensive process than previously realized, requiring the dismantling of the orbiter and the return of various sections to subcontractors across the country. As a consequence, NASA took the decision to convert an incomplete Structural Test Article, numbered STA-099, which had been built to undergo a variety of stress tests, into a fully flight-worthy orbiter, which became .

Following the MGVT program and with the decision to not use "Enterprise" for orbital missions, it was ferried on April 10, 1979, to KSC. By June 1979, it was again mated with an external tank and solid rocket boosters (known as a boilerplate configuration) and tested in a launch configuration at Kennedy Space Center Launch Pad 39A for a series of fit checks of the facilities there.

With the completion of critical testing, "Enterprise" was returned to Rockwell's plant in Palmdale in October 1979 and was partially disassembled to allow certain components to be reused in other shuttles. After this period, "Enterprise" was returned to NASA's Dryden Flight Research Facility in September 1981. During 1983 and 1984, "Enterprise" underwent an international tour visiting France, Germany, Italy, the UK, Canada, and the American states of California, Alabama, and Louisiana (during the 1984 Louisiana World Exposition). It was also used to fit-check the never-used shuttle launch pad at Vandenberg AFB, California. On November 18, 1985, "Enterprise" was ferried to Washington, D.C., where it became property of the Smithsonian Institution, and in December 2011 was moved and became the property of the Intrepid Sea, Air & Space Museum in New York City.

After the "Challenger" disaster, NASA considered using "Enterprise" as a replacement. Refitting the shuttle with all of the necessary equipment for it to be used in space was considered, but NASA decided to use spares constructed at the same time as and to build .

In 2003 after the breakup of during re-entry, the "Columbia" Accident Investigation Board conducted tests at Southwest Research Institute, which used an air gun to shoot foam blocks of similar size, mass and speed to that which struck "Columbia" at a test structure which mechanically replicated the orbiter wing leading edge. They removed a section of fiberglass leading edge from "Enterprise" wing to perform analysis of the material and attached it to the test structure, then shot a foam block at it. While the leading edge was not broken as a result of the test, which took place on May 29, 2003, the impact was enough to permanently deform a seal and leave a thin gap long. Since the strength of the reinforced carbon–carbon (RCC) on "Columbia" is "substantially weaker and less flexible" than the test section from "Enterprise", this result suggested that the RCC would have been shattered. A section of RCC leading edge from "Discovery" was tested on June 6, to determine the effects of the foam on a similarly aged leading edge, resulting in a crack on panel 6 and cracking on a "T"-shaped seal between panels 6 and 7. On July 7, using a leading edge from "Atlantis" and focused on panel 8 with refined parameters stemming from the "Columbia" accident investigation, a second test created a ragged hole approximately in the RCC structure. The tests clearly demonstrated that a foam impact of the type "Columbia" sustained could seriously breach the protective RCC panels on the wing leading edge.

The board determined that the probable cause of the accident was that the foam impact caused a breach of a reinforced carbon-carbon panel along the leading edge of "Columbia" left wing, allowing hot gases generated during re-entry to enter the wing and cause structural collapse. This caused "Columbia" to tumble out of control, breaking up with the loss of the entire crew.

From 1985 to 2003, "Enterprise" was stored at the Smithsonian's hangar at Washington Dulles International Airport before it was restored and moved to the Smithsonian's newly built National Air and Space Museum Steven F. Udvar-Hazy Center at Washington Dulles, where it was the centerpiece of the space collection. On April 12, 2011, NASA announced that , the most traveled orbiter in the fleet, would be added to the collection once the Shuttle fleet was retired. On April 17, 2012, "Discovery" was transported by Shuttle Carrier Aircraft to Dulles from Kennedy Space Center, where it made several passes over the Washington D.C. metro area.

On December 12, 2011, ownership of "Enterprise" was officially transferred to the Intrepid Sea, Air & Space Museum in New York City. In preparation for the anticipated relocation, engineers evaluated the vehicle in early 2010 and determined that it was safe to fly on the Shuttle Carrier Aircraft once again. At approximately 13:40 UTC on April 27, 2012, "Enterprise" took off from Dulles International Airport en route to a fly-by over the Hudson River, New York's JFK International Airport, the Statue of Liberty, the George Washington and Verrazano-Narrows Bridges, and several other landmarks in the city, in an approximately 45-minute "final tour". At 15:23 UTC, "Enterprise" touched down at JFK International Airport.

The mobile Mate-Demate Device and cranes were transported from Dulles to the ramp at JFK and the shuttle was removed from the SCA overnight on May 12, 2012, placed on a specially designed flat bed trailer and returned to Hangar 12. On June 3 a Weeks Marine barge took "Enterprise" to Jersey City. The Shuttle sustained cosmetic damage to a wingtip when a gust of wind blew the barge towards a piling. It was hoisted June 6 onto the Intrepid Museum in Manhattan.

"Enterprise" went on public display on July 19, 2012, at the Intrepid Museum's new Space Shuttle Pavilion, a temporary shelter consisting of a pressurized, air-supported fabric bubble constructed on the aft end of the carrier's flight deck.

On October 29, 2012, storm surges from Hurricane Sandy caused Pier 86, including the Intrepid Museum's visitor center, to flood, and knocked out the museum's electrical power and both backup generators. The loss of power caused the Space Shuttle Pavilion to deflate, and high winds from the hurricane caused the fabric of the Pavilion to tear and collapse around the orbiter. Minor damage was spotted on the vertical stabilizer of the orbiter, as a portion of the tail fin above the rudder/speedbrake had broken off. The broken section was recovered by museum staff. While the pavilion itself could not be replaced for some time in 2013, the museum erected scaffolding and sheeting around "Enterprise" to protect it from the environment.

By April 2013, the damage sustained to "Enterprise" vertical stabilizer had been fully repaired, and construction work on the structure for a new pavilion was under way. The pavilion and exhibit reopened on July 10, 2013.

"Enterprise" was listed on the National Register of Historic Places on March 13, 2013, reference number 13000071, in recognition of its role in the development of the Space Shuttle Program. The historic significance criteria are in space exploration, transportation, and engineering.





</doc>
<doc id="28237" url="https://en.wikipedia.org/wiki?curid=28237" title="Space Shuttle Columbia">
Space Shuttle Columbia

Space Shuttle "Columbia" (Orbiter Vehicle Designation: OV-102) was the first space-rated orbiter in NASA's Space Shuttle fleet. It launched for the first time on mission STS-1 on April 12, 1981, the first flight of the Space Shuttle program. Over 22 years of service, it completed 27 missions before disintegrating during re-entry near the end of its 28th mission, STS-107 on February 1, 2003, resulting in the deaths of all seven crew members.

Construction began on "Columbia" in 1975 at Rockwell International's (formerly North American Aviation/North American Rockwell) principal assembly facility in Palmdale, California, a suburb of Los Angeles. "Columbia" was named after the American sloop "Columbia Rediviva" which, from 1787 to 1793, under the command of Captain Robert Gray, explored the US Pacific Northwest and became the first American vessel to circumnavigate the globe. It is also named after the Command Module of Apollo 11, the first manned landing on another celestial body. "Columbia" was also the female symbol of the United States. After construction, the orbiter arrived at Kennedy Space Center on March 25, 1979, to prepare for its first launch. "Columbia" was originally scheduled to lift off in late 1979, however the launch date was delayed by problems with both the Space Shuttle main engine (SSME), as well as the thermal protection system (TPS). On March 19, 1981, during preparations for a ground test, workers were asphyxiated while working in Columbia's nitrogen-purged aft engine compartment, resulting in (variously reported) two or three fatalities.
The first flight of "Columbia" (STS-1) was commanded by John Young, a veteran from the Gemini and Apollo programs who was the ninth person to walk on the Moon in 1972, and piloted by Robert Crippen, a rookie astronaut originally selected to fly on the military's Manned Orbital Laboratory (MOL) spacecraft, but transferred to NASA after its cancellation, and served as a support crew member for the Skylab and Apollo-Soyuz missions.

"Columbia" spent 610 days in the Orbiter Processing Facility (OPF), another 35 days in the Vehicle Assembly Building (VAB), and 105 days on Pad 39A before finally lifting off. "Columbia" was successfully launched on April 12, 1981, the 20th anniversary of the first human spaceflight (Vostok 1), and returned on April 14, 1981, after orbiting the Earth 36 times, landing on the dry lakebed runway at Edwards Air Force Base in California. "Columbia" then undertook three further research missions to test its technical characteristics and performance. Its first operational mission, with a four-man crew, was STS-5, which launched on November 11, 1982. At this point "Columbia" was joined by "Challenger", which flew the next three shuttle missions, while "Columbia" underwent modifications for the first Spacelab mission.

In 1983, "Columbia", under the command of John Young on what was his sixth spaceflight, undertook its second operational mission (STS-9), in which the Spacelab science laboratory and a six-person crew was carried, including the first non-American astronaut on a space shuttle, Ulf Merbold. After the flight, "Columbia" spent 18 months at the Rockwell Palmdale facility beginning in January 1984, undergoing modifications that removed the Orbiter Flight Test hardware and bringing it up to similar specifications as those of its sister orbiters. At that time the shuttle fleet was expanded to include "Discovery" and "Atlantis".

"Columbia" returned to space on January 12, 1986, with the launch of STS-61-C. The mission's crew included Dr. Franklin Chang-Diaz, as well as the first sitting member of the House of Representatives to venture into space, Bill Nelson.

The next shuttle mission, STS-51-L, was undertaken by "Challenger". It was launched on January 28, 1986, ten days after STS-61-C had landed, and ended in disaster 73 seconds after launch. In the aftermath NASA's shuttle timetable was disrupted, and "Columbia" was not flown again until 1989 (on STS-28), after which it resumed normal service as part of the shuttle fleet.

STS-93, launched on July 23, 1999, was the first U.S. space mission with a female commander, Lt. Col. Eileen Collins. This mission deployed the Chandra X-ray Observatory.

"Columbia"'s final successful mission was STS-109, the fourth servicing mission for the Hubble Space Telescope. Its next mission, STS-107, culminated in the orbiter's loss when it disintegrated during reentry, killing all seven of its crew.

Consequently, President Bush decided to retire the Shuttle orbiter fleet by 2010 in favor of the Constellation program and its manned Orion spacecraft. The Constellation program was later cancelled with the NASA Authorization Act of 2010 signed by President Obama on October 11.

As the second orbiter to be constructed, and the first able to fly into space, "Columbia" was roughly heavier than subsequent orbiters such as "Endeavour", which were of a slightly different design, and had benefited from advances in materials technology. In part, this was due to heavier wing and fuselage spars, the weight of early test instrumentation that remained fitted to the avionics suite, and an internal airlock that, originally fitted into the other orbiters, was later removed in favor of an external airlock to facilitate Shuttle/Mir and Shuttle/International Space Station dockings. Due to its weight, "Columbia" could not have used the planned Centaur-G booster (cancelled after the loss of "Challenger"). The retention of the internal airlock allowed NASA to use "Columbia" for the STS-109 Hubble Space Telescope servicing mission, along with the Spacehab double module used on STS-107. Due to "Columbia's" heavier weight, it was less ideal for NASA to use it for missions to the International Space Station, though modifications were made to the Shuttle during its last refit in case the spacecraft was needed for such tasks.

Externally, "Columbia" was the first orbiter in the fleet whose surface was mostly covered with High & Low Temperature Reusable Surface Insulation (HRSI/LRSI) tiles as its main thermal protection system (TPS), with white silicone rubber-painted Nomex – known as Felt Reusable Surface Insulation (FRSI) blankets – in some areas on the wings, fuselage and payload bay doors. FRSI once covered almost 25% of the orbiter; the first upgrade resulted in its removal from many areas, and in later flights it was only used on the upper section of the payload bay doors and inboard sections of the upper wing surfaces. The upgrade also involved replacing many of the white LRSI tiles on the upper surfaces with Advanced Flexible Reusable Surface Insulation (AFRSI) blankets (also known as Fibrous Insulation Blankets, or FIBs) that had been used on "Discovery" and "Atlantis". Originally, "Columbia" had 32,000 tiles – the upgrade reduced this to 24,300. The AFRSI blankets consisted of layers of pure silica felt sandwiched between a layer of silica fabric on the outside and S-Glass fabric on the inside, stitched together using pure silica thread in a 1-inch grid, then coated with a high-purity silica coating. The blankets were semi-rigid and could be made as large as 30" by 30". Each blanket replaced as many as 25 tiles and was bonded directly to the orbiter. The direct application of the blankets to the orbiter resulted in weight reduction, improved durability, reduced fabrication and installation cost, and reduced installation schedule time. All of this work was performed during "Columbia's" first retrofitting and the post-"Challenger" stand-down.

Despite refinements to the orbiter's thermal protection system and other enhancements, "Columbia" would never weigh as little unloaded as the other orbiters in the fleet. The next-oldest shuttle, "Challenger", was also relatively heavy, although lighter than "Columbia".

Until its last refit, "Columbia" was the only operational orbiter with wing markings consisting of an American flag on the port (left) wing and the letters "USA" on the starboard (right) wing. "Challenger", "Discovery", "Atlantis" and "Endeavour" all, until 1998, bore markings consisting of the letters "USA" above an American flag on the left wing, and the pre-1998 NASA "worm" logo afore the respective orbiter's name on the right wing. ("Enterprise", the test vehicle which was the prototype for "Columbia", originally had the same wing markings as "Columbia" but with the letters "USA" on the right wing spaced closer together; "Enterprise"'s markings were modified to match "Challenger" in 1983.) The name of the orbiter was originally placed on the payload bay doors much like "Enterprise" but was placed on the crew cabin after the "Challenger" disaster so that the orbiter could be easily identified while in orbit. From its last refit to its destruction, "Columbia" bore markings identical to those of its operational sister orbiters – the NASA "meatball" logo on the left wing and the American flag afore the orbiter's name on the right; only "Columbia's" distinctive wing "chines" remained. These black areas on the upper surfaces of the shuttle's forward wing were added because, at first, shuttle designers did not know how reentry heating would affect the craft's upper wing surfaces. The "chines" allowed "Columbia" to be easily recognized at a distance, as opposed to the subsequent orbiters. The "chines" were added after "Columbia" arrived at KSC in 1979.

Another unique external feature, termed the "SILTS" pod (Shuttle Infrared Leeside Temperature Sensing), was located on the top of "Columbia's" vertical stabilizer, and was installed after STS-9 to acquire infrared and other thermal data. Though the pod's equipment was removed after initial tests, NASA decided to leave it in place, mainly to save costs, along with the agency's plans to use it for future experiments. The vertical stabilizer was later modified to incorporate the drag chute first used on "Endeavour" in 1992.

"Columbia" was also originally fitted with Lockheed-built ejection seats identical to those found on the SR-71 Blackbird. These were active for the four orbital test flights, but deactivated after STS-4, and removed entirely after STS-9. "Columbia" was also the only spaceworthy orbiter not delivered with head-up displays for the Commander and Pilot, although these were incorporated after STS-9. Like its sister ships, "Columbia" was eventually retrofitted with the new MEDS "glass cockpit" display and lightweight seats.

Had "Columbia" not been destroyed, it would have been fitted with the external airlock/docking adapter for STS-118, an International Space Station assembly mission, originally planned for November 2003. "Columbia" was scheduled for this mission due to "Discovery" being out of service for its Orbital Maintenance Down Period, and because the ISS assembly schedule could not be adhered to with only "Endeavour" and "Atlantis".

"Columbia"’s 'career' would have started to wind down after STS-118. It was to service the Hubble Space Telescope two more times between 2004 and 2005, but no more missions were planned for it again except for a mission designated STS-144 where it would retrieve the Hubble Space Telescope from orbit and bring it back to Earth. Following the "Columbia" accident, NASA flew the STS-125 mission using "Atlantis", combining the planned fourth and fifth servicing missions into one final mission to Hubble. Because of the retirement of the Space Shuttle fleet, the batteries and gyroscopes that keep the telescope pointed will eventually fail also because of the magnifier screen, which would result in its reentry and break-up in Earth's atmosphere. A "Soft Capture Docking Mechanism", based on the docking adapter that was to be used on the Orion spacecraft, was installed during the last servicing mission in anticipation of this event.

"Columbia" was also scheduled to launch the X-38 V-201 Crew Return Vehicle prototype as the next mission after STS-118, until the cancellation of the project in 2002.

"Columbia" flew 28 missions, gathering 300.74 days spent in space with 4,808 orbits and a total distance of up until STS-107.

Despite being in service during the Shuttle-Mir and International Space Station programs, "Columbia" did not fly any missions that visited a space station. The other three active orbiters at the time had visited both "Mir" and the ISS at least once. "Columbia" was not suited for high-inclination missions.


"Columbia" was destroyed at about 09:00 EST on February 1, 2003 while re-entering the atmosphere after a 16-day scientific mission. The Columbia Accident Investigation Board determined that a hole was punctured in the leading edge on one of "Columbia's" wings, which was made of a carbon composite. The hole had formed when a piece of insulating foam from the external fuel tank peeled off during the launch 16 days earlier and struck the shuttle's left wing. During the intense heat of re-entry, hot gases penetrated the interior of the wing, likely compromising the hydraulic system and leading to control failure of the control surfaces. The resulting loss of control exposed minimally protected areas of the orbiter to full-entry heating and dynamic pressures that eventually led to vehicle break up.

The report delved deeply into the underlying organizational and cultural issues that the Board believed contributed to the accident. The report was highly critical of NASA's decision-making and risk-assessment processes. Further, the board determined that, unlike NASA's early claims, a rescue mission would have been possible using the Shuttle Atlantis, which was essentially ready for launch, and might have saved the Columbia crewmembers. The nearly 84,000 pieces of collected debris of the vessel are stored in a 16th-floor office suite in the Vehicle Assembly Building at the Kennedy Space Center. The collection was opened to the media once and has since been open only to researchers. Unlike "Challenger", which had a replacement orbiter built, "Columbia" did not.

The seven crew members who died aboard this final mission were: Rick Husband, Commander; William C. McCool, Pilot; Michael P. Anderson, Payload Commander/Mission Specialist 3; David M. Brown, Mission Specialist 1; Kalpana Chawla, Mission Specialist 2; Laurel Clark, Mission Specialist 4; and Ilan Ramon, Payload Specialist 1.

The debris field encompassed hundreds of miles across Northeast Texas and into Louisiana. The nose cap and remains of all seven crew members were found in Sabine County, East Texas.

To honor those who lost their lives aboard the shuttle and during the recovery efforts, the Patricia Huffman Smith NASA Museum "Remembering Columbia" was opened in Hemphill, Sabine County, Texas. The museum tells the story of Space Shuttle Columbia explorations throughout all its missions, including the final STS-107. Its exhibits also show the efforts of local citizens during the recovery period of the "Columbia" shuttle debris and its crew's remains. An area is dedicated to each STS-107 crew member, and also to the Texas Forest Service helicopter pilot who died in the recovery effort. The museum houses many objects and artifacts from: NASA and its contractors; the families of the STS-107 crew; and other individuals. The crew's families contributed personal items of the crew members to be on permanent display. The museum features two interactive simulator displays that emulate activities of the shuttle and orbiter. The digital learning center and its classroom provide educational opportunities for all ages.

The Columbia Memorial Space Center is the U.S. national memorial for the Space Shuttle Columbia’s seven crew members. It is located in Downey on the site of the Space Shuttle's origins and production, the former North American Aviation plant in Los Angeles County, southern California. The facility is also a hands-on learning center with interactive exhibits, workshops, and classes about space science, astronautics, and the Space Shuttle program's legacy — providing educational opportunities for all ages.

The shuttle's final crew was honored in 2003 when the United States Board on Geographic Names approved the name Columbia Point for a mountain in Colorado's Sangre de Cristo Mountains, less than a half-mile from Challenger Point, a peak named after America's other lost shuttle. The Columbia Hills on Mars were also named in honor of the crew, and a host of other memorials were dedicated in various forms.

The Columbia supercomputer at the NASA Advanced Supercomputing (NAS) Division located at Ames Research Center in California was named in honor of the crew lost in the 2003 disaster. Built as a joint effort between NASA and technical partners SGI and Intel in 2004, the supercomputer was used in scientific research of space, the Earth's climate, and aerodynamic design of space launch vehicles and aircraft. The first part of the system, built in 2003, was dedicated to STS-107 astronaut and engineer Kalpana Chawla, who prior to joining the Space Shuttle program worked at Ames Research Center.

Guitarist Steve Morse of the rock band Deep Purple wrote the instrumental "Contact Lost" in response to the news of the tragedy, recorded by Deep Purple and featured as the closing track on their 2003 album "Bananas". It was dedicated to the astronauts whose lives were lost in the disaster. Morse donated songwriting royalties to the families of lost astronauts. Astronaut and mission specialist engineer Kalpana Chawla, one of the victims of the accident, was a fan of Deep Purple and had exchanged e-mails with the band during the flight, making the tragedy even more personal for the group. She took three CDs into space with her, two of which were Deep Purple albums ("Machine Head" and "Purpendicular"). Both CDs survived the destruction of the shuttle and the 39-mile plunge.

The musical group Echo's Children included singer-songwriter Cat Faber's "Columbia" on their final album "From the Hazel Tree".

The Long Winters band's 2005 album "Ultimatum" features the song "The Commander Thinks Aloud", a tribute to the final "Columbia" crew.

The Eric Johnson instrumental "Columbia" from his 2005 album "Bloom" was written as a commemoration and tribute to the lives that were lost. Johnson said "I wanted to make it more of a positive message, a salute, a celebration rather than just concentrating on a few moments of tragedy, but instead the bigger picture of these brave people’s lives."

The graphic novel "Orbiter" by Warren Ellis and Colleen Doran was dedicated to the "lives, memories and legacies of the seven astronauts lost on space shuttle "Columbia" during mission STS-107."

Laurel Clark’s wake up call on STS – 107 was Runrig’s "Running to the Light". Laurel took The Stamping Ground CD into space with her and when the Shuttle exploded the CD was found back on Earth, and presented to the band. The bands last Studio Album, "The Story", final track, "Somewhere", ends with a recording of her voice introducing the song.





</doc>
<doc id="28238" url="https://en.wikipedia.org/wiki?curid=28238" title="Space Shuttle Discovery">
Space Shuttle Discovery

Space Shuttle "Discovery" (Orbiter Vehicle Designation: OV-103) is one of the orbiters from NASA's Space Shuttle program and the third of five fully operational orbiters to be built. Its first mission, STS-41-D, flew from August 30 to September 5, 1984. Over 27 years of service it launched and landed 39 times, gathering more spaceflights than any other spacecraft to date.

"Discovery" became the third operational orbiter to enter service, preceded by "Columbia" and "Challenger". It embarked on its last mission, STS-133, on February 24, 2011 and touched down for the final time at Kennedy Space Center on March 9, having spent a cumulative total of almost a full year in space. "Discovery" performed both research and International Space Station (ISS) assembly missions. It also carried the Hubble Space Telescope into orbit. "Discovery" was the first operational shuttle to be retired, followed by "Endeavour" and then "Atlantis".

The name "Discovery" was chosen to carry on a tradition based on ships of exploration, primarily , one of the ships commanded by Captain James Cook during his third and final major voyage from 1776 to 1779, and Henry Hudson's , which was used in 1610–1611 to explore Hudson Bay and search for a Northwest Passage. Other ships bearing the name have included of the 1875–1876 British Arctic Expedition to the North Pole and , which led the 1901–1904 "Discovery Expedition" to Antarctica.

"Discovery" launched the Hubble Space Telescope and conducted the second and third Hubble service missions. It also launched the Ulysses probe and three TDRS satellites. Twice "Discovery" was chosen as the "Return To Flight" Orbiter, first in 1988 after the loss of "Challenger" in 1986, and then again for the twin "Return To Flight" missions in July 2005 and July 2006 after the "Columbia" disaster in 2003. Project Mercury astronaut John Glenn, who was 77 at the time, flew with "Discovery" on STS-95 in 1998, making him the oldest person to go into space.

Had plans to launch United States Department of Defense payloads from Vandenberg Air Force Base gone ahead, "Discovery" would have become the dedicated US Air Force shuttle. Its first West Coast mission, STS-62-A, was scheduled for 1986, but canceled in the aftermath of "Challenger".

"Discovery" was retired after completing its final mission, STS 133 on March 9, 2011. The spacecraft is now on display in Virginia at the Steven F. Udvar-Hazy Center, an annex of the Smithsonian Institution's National Air and Space Museum.

"Discovery" weighed roughly 3600 kg (3.6t)less than "Columbia" when it was brought into service due to optimizations determined during the construction and testing of "Enterprise", "Columbia" and "Challenger". "Discovery" weighs heavier than "Atlantis" and heavier than "Endeavour".

Part of the Discovery weight optimizations included the greater use of quilted AFRSI blankets rather than the white LRSI tiles on the fuselage, and the use of graphite epoxy instead of aluminum for the payload bay doors and some of the wing spars and beams.

Upon its delivery to the Kennedy Space Center in 1983, "Discovery" was modified alongside "Challenger" to accommodate the liquid-fueled Centaur-G booster, which had been planned for use beginning in 1986 but was cancelled in the wake of the "Challenger" disaster.

Beginning in late 1995, the orbiter underwent a nine-month Orbiter Maintenance Down Period (OMDP) in Palmdale, California. This included outfitting the vehicle with a 5th set of cryogenic tanks and an external airlock to support missions to the International Space Station. As with all the orbiters, it could be attached to the top of specialized aircraft and did so in June 1996 when it returned to the Kennedy Space Center, and later in April 2012 when sent to the Udvar-Hazy Center, riding piggy-back on a modified Boeing 747.

After STS-105, "Discovery" became the first of the orbiter fleet to undergo Orbiter Major Modification (OMM) period at the Kennedy Space Center. Work began in September 2002 to prepare the vehicle for Return to Flight. The work included scheduled upgrades and additional safety modifications.

"Discovery" was decommissioned on March 9, 2011.

NASA offered "Discovery" to the Smithsonian Institution's National Air and Space Museum for public display and preservation, after a month-long decontamination process, as part of the national collection. "Discovery" replaced in the Smithsonian's display at the Steven F. Udvar-Hazy Center in Virginia. "Discovery" was transported to Washington Dulles International Airport on April 17, 2012, and was transferred to the Udvar-Hazy on April 19 where a welcome ceremony was held. Afterwards, at around 5: 30 pm, "Discovery" was rolled to its "final wheels stop" in the Udvar Hazy Center.

By its last mission, "Discovery" had flown 149 million miles (238 million km) in 39 missions, completed 5,830 orbits, and spent 365 days in orbit over 27 years. "Discovery" flew more flights than any other Orbiter Shuttle, including four in 1985 alone. "Discovery" flew all three "return to flight" missions after the "Challenger" and "Columbia" disasters: STS-26 in 1988, STS-114 in 2005, and STS-121 in 2006. "Discovery" flew the ante-penultimate mission of the Space Shuttle program, STS-133, having launched on February 24, 2011. "Endeavour" flew STS-134 and "Atlantis" performed STS-135, NASA's last Space Shuttle mission. On February 24, 2011, Space Shuttle "Discovery" launched from Kennedy Space Center's Launch Complex 39-A to begin its final orbital flight.

The Flow Director was responsible for the overall preparation of the shuttle for launch and processing it after landing, and remained permanently assigned to head the spacecraft's ground crew while the astronaut flight crews changed for every mission. Each shuttle's Flow Director was supported by a Vehicle Manager for the same spacecraft. Space shuttle "Discovery"'s Flow Directors were:




</doc>
<doc id="28239" url="https://en.wikipedia.org/wiki?curid=28239" title="Space Shuttle Atlantis">
Space Shuttle Atlantis

Space Shuttle "Atlantis" (Orbiter Vehicle Designation: OV‑104) is a Space Shuttle orbiter vehicle belonging to the National Aeronautics and Space Administration (NASA), the spaceflight and space exploration agency of the United States. Constructed by the Rockwell International company in Southern California and delivered to the Kennedy Space Center in Eastern Florida in April 1985, "Atlantis" is the fourth operational and the second-to-last Space Shuttle built. Its maiden flight was STS-51-J from 3 to 7 October 1985.

"Atlantis" embarked on its 33rd and final mission, also the final mission of a space shuttle, STS-135, on 8 July 2011. STS-134 by "Endeavour was expected to be the final flight before STS-135 was authorized in October 2010. STS-135 took advantage of the processing for the STS-335 Launch On Need mission that would have been necessary if STS-134's crew became stranded in orbit.
"Atlantis" landed for the final time at the Kennedy Space Center on 21 July 2011.

By the end of its final mission, "Atlantis" had orbited the Earth a total of 4,848 times, traveling nearly or more than 525 times the distance from the Earth to the Moon.

"Atlantis" is named after RV "Atlantis", a two-masted sailing ship that operated as the primary research vessel for the Woods Hole Oceanographic Institution from 1930 to 1966.


Space Shuttle "Atlantis" lifted off on its maiden voyage on 3 October 1985, on mission STS-51-J, the second dedicated Department of Defense flight. It flew one other mission, STS-61-B, the second night launch in the shuttle program, before the Space Shuttle "Challenger" disaster temporarily grounded the shuttle fleet in 1986. Among the five Space Shuttles flown into space, "Atlantis" conducted a subsequent mission in the shortest time after the previous mission (turnaround time) when it launched in November 1985 on STS-61-B, only 50 days after its previous mission, STS-51-J in October 1985. "Atlantis" was then used for ten flights between 1988 and 1992. Two of these, both flown in 1989, deployed the planetary probes "Magellan" to Venus (on STS-30) and "Galileo" to Jupiter (on STS-34). With STS-30 "Atlantis" became the first shuttle to launch an interplanetary probe.

During another mission, STS-37 flown in 1991, "Atlantis" deployed the Compton Gamma Ray Observatory. Beginning in 1995 with STS-71, "Atlantis" made seven straight flights to the former Russian space station Mir as part of the Shuttle-Mir Program. STS-71 marked a number of firsts in human spaceflight: 100th U.S. manned space flight; first U.S. shuttle-Russian Space Station Mir docking and joint on-orbit operations; and first on-orbit changeout of shuttle crew. When linked, "Atlantis" and "Mir" together formed the largest spacecraft in orbit at the time.

Shuttle "Atlantis" also delivered several vital components for the construction of the International Space Station (ISS). During the February 2001 mission STS-98 to the ISS, "Atlantis" delivered the Destiny Module, the primary operating facility for U.S. research payloads aboard the ISS. The five hour 25 minute third spacewalk performed by astronauts Robert Curbeam and Thomas Jones during STS-98 marked NASA's 100th extra vehicular activity in space. The Quest Joint Airlock, was flown and installed to the ISS by "Atlantis" during the mission STS-104 in July 2001. The successful installation of the airlock gave on-board space station crews the ability to stage repair and maintenance spacewalks outside the ISS using U.S. EMU or Russian Orlan space suits. The first mission flown by "Atlantis" after the Space Shuttle Columbia disaster was STS-115, conducted during September 2006. The mission carried the P3/P4 truss segments and solar arrays to the ISS. On ISS assembly flight STS-122 in February 2008, "Atlantis" delivered the Columbus laboratory to the ISS. Columbus laboratory is the largest single contribution to the ISS made by the European Space Agency (ESA).

In May 2009 "Atlantis" flew a seven-member crew to the Hubble Space Telescope for its Servicing Mission 4, STS-125. The mission was a success, with the crew completing five spacewalks totalling 37 hours to install new cameras, batteries, a gyroscope and other components to the telescope.
This was the final mission not to the ISS.

The longest mission flown using "Atlantis" was STS-117 which lasted almost 14 days in June 2007. During STS-117, Atlantis' crew added a new starboard truss segment and solar array pair (the S3/S4 truss), folded the P6 array in preparation for its relocation and performed four spacewalks. "Atlantis" was not equipped to take advantage of the Station-to-Shuttle Power Transfer System so missions could not be extended by making use of power provided by ISS.

During the STS-129 post-flight interview on 16 November 2009, shuttle launch director Mike Leinbach said that "Atlantis" officially beat shuttle "Discovery" for the record low amount of Interim Problem Reports, with a total of just 54 listed since returning from STS-125. He continued to add "It is due to the team and the hardware processing. They just did a great job. The record will probably never be broken again in the history of the Space Shuttle Program, so congratulations to them".

During the STS-132 post-launch interview on 14 May 2010, shuttle launch director Mike Leinbach said that "Atlantis" beat its own previous record low amount of Interim Problem Reports, with a total of 46 listed between STS-129 and STS-132.

"Atlantis" went through two overhauls of scheduled Orbiter Maintenance Down Periods (OMDPs) during its operational history.

"Atlantis" arrived at Palmdale, California in October 1992 for OMDP-1. During that visit 165 modifications were made over the next 20 months. These included the installation of a drag chute, new plumbing lines to configure the orbiter for extended duration, improved nose wheel steering, more than 800 new heat tiles and blankets and new insulation for main landing gear and structural modifications to the airframe.

On 5 November 1997, "Atlantis" again arrived at Palmdale for OMDP-2 which was completed on 24 September 1998. The 130 modifications carried out during OMDP-2 included glass cockpit displays, replacement of TACAN navigation with GPS and ISS airlock and docking installation. Several weight reduction modifications were also performed on the orbiter including replacement of Advanced Flexible Reusable Surface Insulation (AFRSI) insulation blankets on upper surfaces with FRSI. Lightweight crew seats were installed and the Extended Duration Orbiter (EDO) package installed on OMDP-1 was removed to lighten "Atlantis" to better serve its prime mission of servicing the ISS.

During the stand down period post "Columbia" accident, "Atlantis" went through over 75 modifications to the orbiter ranging from very minor bolt change-outs to window change-outs and different fluid systems.

Atlantis was known among the shuttle workforce as being more prone than the others in the fleet to problems that needed to be addressed while readying the vehicle for launch leading to some nicknaming it "Britney".

NASA initially planned to withdraw "Atlantis" from service in 2008, as the orbiter would have been due to undergo its third scheduled OMDP. However, because of the timescale of the final retirement of the shuttle fleet, this was deemed uneconomical. It was planned that "Atlantis" would be kept in near-flight condition to be used as a spares source for "Discovery" and "Endeavour". However, with the significant planned flight schedule up to 2010, the decision was taken to extend the time between OMDPs, allowing "Atlantis" to be retained for operations. "Atlantis" was subsequently swapped for one flight of each "Discovery" and "Endeavour" in the flight manifest. "Atlantis" had completed what was meant to be its last flight, STS-132, prior to the end of the shuttle program, but the extension of the Shuttle program into 2011 led to "Atlantis" being selected for STS-135, the final Space Shuttle mission in July 2011.

"Atlantis" is currently displayed at the Kennedy Space Center Visitor Complex. NASA Administrator Charles Bolden announced the decision at an employee event held on 12 April 2011 to commemorate the 30th anniversary of the first shuttle flight: "First, here at the Kennedy Space Center where every shuttle mission and so many other historic human space flights have originated, we'll showcase my old friend, "Atlantis"."

The Visitor Complex displays "Atlantis" with payload bay doors opened mounted at an angle to give the appearance of being in orbit around the Earth. The 43.21 degree mount angle also pays tribute to the countdown that preceded every shuttle launch at KSC. A multi-story digital projection of Earth rotates behind the orbiter in a indoor facility. Ground breaking of the facility occurred in 2012.
The exhibit opened on 29 June 2013.

A total of 156 individuals flew with Space Shuttle "Atlantis" over the course of its 33 missions. Because the shuttle sometimes flew crew members arriving and departing Mir and the ISS, not all of them launched and landed on "Atlantis".

Astronaut Clayton Anderson, ESA astronaut Leopold Eyharts and Russian cosmonauts Nikolai Budarin and Anatoly Solovyev only launched on "Atlantis". Similarly, astronauts Daniel Tani and Sunita Williams, as well as cosmonauts Vladimir Dezhurov and Gennady Strekalov only landed with "Atlantis". Only 146 men and women both launched and landed aboard "Atlantis".

Some of those people flew with "Atlantis" more than once. Taking them into account, 203 total seats were filled over "Atlantis" 33 missions. Astronaut Jerry Ross holds the record for the most flights aboard "Atlantis" at five.

Astronaut Rodolfo Neri Vela who flew aboard Atlantis on STS-61-B mission in 1985 became the first and so far only Mexican to have traveled to space. ESA astronaut Dirk Frimout who flew on STS-45 as a payload specialist was the first Belgian in space. STS-46 mission specialist Claude Nicollier was the first astronaut from Switzerland. On the same flight, astronaut Franco Malerba became the first citizen of Italy to travel to space.

Astronaut Michael Massimino who flew on STS-125 mission became the first person to use Twitter in space in May 2009.

Having flown aboard "Atlantis" as part of the STS-132 crew in May 2010 and "Discovery" as part of the STS-133 crew in February/March 2011, Stephen Bowen became the first NASA astronaut to be launched on consecutive missions.

NASA announced in 2007 that 24 helium and nitrogen gas tanks in "Atlantis" were older than their designed lifetime. These composite overwrapped pressure vessels (COPV) were designed for a 10-year life and later cleared for an additional 10 years; they exceeded this life in 2005. NASA said it could not guarantee any longer that the vessels on "Atlantis" would not burst or explode under full pressure. Failure of these tanks could have damaged parts of the orbiter and even wound or kill ground personnel. An in-flight failure of a pressure vessel could have even resulted in the loss of the orbiter and its crew. NASA analyses originally assumed that the vessels would leak before they burst, but new tests showed that they could in fact burst before leaking.

Because the original vendor was no longer in business, and a new manufacturer could not be qualified before 2010, when the shuttles were scheduled to be retired, NASA decided to continue operations with the existing tanks. Therefore, to reduce the risk of failure and the cumulative effects of load, the vessels were maintained at 80 percent of the operating pressure as late in the launch countdown as possible, and the launch pad was cleared of all but essential personnel when pressure was increased to 100 percent. The new launch procedure was employed during some the remaining launches of "Atlantis", but was resolved when the two COPVs deemed to have the highest risk of failure were replaced.

After the STS-125 mission, a work light knob was discovered jammed in the space between one of "Atlantis"s front interior windows and the Orbiter dashboard structure. The knob was believed to have entered the space during flight, when the pressurized Orbiter was expanded to its maximum size. Then, once back on Earth, the Orbiter contracted, jamming the knob in place. Leaving "as-is" was considered unsafe for flight, and some options for removal (including window replacement) would have included a 6-month delay of "Atlantis"s next mission (planned to be STS-129). Had the removal of the knob been unsuccessful, the worst-case scenario was that "Atlantis" could have been retired from the fleet, leaving "Discovery" and "Endeavour" to complete the manifest alone. On 29 June 2009, "Atlantis" was pressurized to (3 psi above ambient), which forced the Orbiter to expand slightly. The knob was then frozen with dry ice, and successfully removed. Small areas of damage to the window were discovered where the edges of the knob had been embedded into the pane. Subsequent investigation of the window damage discovered a maximum defect depth of approximately , less than the reportable depth threshold of and not serious enough to warrant the pane’s replacement.





</doc>
<doc id="28240" url="https://en.wikipedia.org/wiki?curid=28240" title="Space Shuttle Endeavour">
Space Shuttle Endeavour

Space Shuttle "Endeavour" (Orbiter Vehicle Designation: OV-105) is a retired orbiter from NASA's Space Shuttle program and the fifth and final operational shuttle built. It embarked on its first mission, STS-49, in May 1992 and its 25th and final mission, STS-134, in May 2011. STS-134 was expected to be the final mission of the Space Shuttle program, but with the authorization of STS-135, "Atlantis" became the last shuttle to fly.

The United States Congress approved the construction of "Endeavour" in 1987 to replace "Challenger", which was lost in 1986.

Structural spares built during the construction of "Discovery" and "Atlantis" were used in its assembly. NASA chose, on cost grounds, to build "Endeavour" from spares rather than refitting "Enterprise" or 

The orbiter is named after the British HMS "Endeavour", the ship which took Captain James Cook on his first voyage of discovery (1768–1771). This is why the name is spelled in the British English manner, rather than the American English ("Endeavor"). This has caused confusion, including when NASA itself misspelled a sign on the launch pad in 2007. The Space Shuttle carried a piece of the original wood from Cook’s ship inside the cockpit. The name also honored "Endeavour", the Command Module of Apollo 15, which was also named after Cook's ship.

"Endeavour" was named through a national competition involving students in elementary and secondary schools. Entries included an essay about the name, the story behind it and why it was appropriate for a NASA shuttle, and the project that supported the name. "Endeavour" was the most popular entry, accounting for almost one-third of the state-level winners. The national winners were Senatobia Middle School in Senatobia, Mississippi, in the elementary division and Tallulah Falls School in Tallulah Falls, Georgia, in the upper school division. They were honored at several ceremonies in Washington, D.C., including a White House ceremony where then-President George H. W. Bush presented awards to each school.

"Endeavour" was delivered by Rockwell International Space Transportation Systems Division in May 1991 and first launched a year later, in May 1992, on STS-49. Rockwell International claimed that it had made no profit on Space Shuttle "Endeavour", despite construction costing US$2.2 billion.

On its first mission, it captured and redeployed the stranded "INTELSAT VI" communications satellite. The first African-American woman astronaut, Mae Jemison, was launched into space on the mission STS-47 on September 12, 1992.

"Endeavour" flew the first servicing mission STS-61 for the Hubble Space Telescope in 1993. In 1997 it was withdrawn from service for eight months for a retrofit, including installation of a new airlock. In December 1998, it delivered the Unity Module to the International Space Station.

"Endeavour"'s last Orbiter Major Modification period began in December 2003 and ended on October 6, 2005. During this time, "Endeavour" received major hardware upgrades, including a new, multi-functional, electronic display system, often referred to as a glass cockpit, and an advanced GPS receiver, along with safety upgrades recommended by the Columbia Accident Investigation Board (CAIB) for the shuttle's return to flight following the loss of "Columbia" during reentry on 1 February 2003.

The STS-118 mission, "Endeavour"'s first since the refit, included astronaut Barbara Morgan, formerly assigned to the Teacher in Space project, and later a member of the Astronaut Corps from 1998 to 2008, as part of the crew. Morgan was the backup for Christa McAuliffe who was on the ill-fated mission STS-51-L in 1986.

As it was constructed later, "Endeavour" was built with new hardware designed to improve and expand orbiter capabilities. Most of this equipment was later incorporated into the other three orbiters during out-of-service major inspection and modification programs. "Endeavour"’s upgrades include:

Modifications resulting from a 2005–2006 refit of "Endeavour" included:

"Endeavour" flew its final mission, STS-134, to the International Space Station (ISS) in May 2011. After the conclusion of STS-134, "Endeavour" was formally decommissioned.

STS-134 was intended to launch in late 2010, but on July 1 NASA released a statement saying the "Endeavour" mission was rescheduled for February 27, 2011.

"The target dates were adjusted because critical payload hardware for STS-133 will not be ready in time to support the previously planned 16 September launch," NASA said in a statement. With the "Discovery" launch moving to November, "Endeavour" mission "cannot fly as planned, so the next available launch window is in February 2011," NASA said, adding that the launch dates were subject to change.

The launch was further postponed until April to avoid a scheduling conflict with a Russian supply vehicle heading for the International Space Station. STS-134 did not launch until 16 May at 08:56 EDT.

"Endeavour" landed at the Kennedy Space Center at 06:34 UTC on June 1, 2011, completing its final mission. It was the 25th night landing of a shuttle. Over its flight career, "Endeavour" flew 122,883,151 miles and spent 299 days in space. During "Endeavour's" last mission, the Russian spacecraft Soyuz TMA-20 departed from the ISS and paused at a distance of 200 meters. Italian astronaut Paolo Nespoli took a series of photographs and videos of the ISS with "Endeavour" docked. This was the second time a shuttle was photographed docked and the first time since 1996. Commander Mark Kelly was the last astronaut off "Endeavour" after the landing, and the crew stayed on the landing strip to sign autographs and pose for pictures.

STS-134 was the penultimate space shuttle mission; STS-135 was added to the schedule in January 2011, and in July "Atlantis" flew for the final time.

After more than twenty organizations submitted proposals to NASA for the display of an orbiter, NASA announced that "Endeavour" would go to the California Science Center in Los Angeles.

After low level flyovers above NASA and civic landmarks across the country and in California, it was delivered to Los Angeles International Airport (LAX) on September 21, 2012. The orbiter was slowly and carefully transported through the streets of Los Angeles and Inglewood three weeks later, from October 11–14 along La Tijera, Manchester, Crenshaw, and Martin Luther King, Jr. Boulevards to its final destination at the California Science Center in Exposition Park.

"Endeavour's" route on the city streets between LAX and Exposition Park was meticulously measured and each move was carefully choreographed. In multiple locations, there were only inches of clearance for the shuttle's wide wings between telephone poles, apartment buildings and other structures. Many street light standards and traffic signals were temporarily removed as the shuttle passed through. It was necessary to remove over 400 street trees as well, some of which were fairly old, creating a small controversy. However, the removed trees were replaced two-for-one by the Science Center, using part of the $200 million funding for the move.

The power had to be turned off and power carrying poles had to be removed temporarily as the orbiter crept along Manchester, to Prairie Avenue, then Crenshaw Boulevard. News crews lined the streets along the path with visible news personalities in the news trucks. Police escorts and other security personnel controlled the large crowds gathered. "Endeavour" was parked for a few hours at the Great Western Forum where it was available for viewing. The journey was famous for an unmodified Toyota Tundra pickup truck pulling the space shuttle across the Manchester Boulevard Bridge. The space shuttle was mainly carried by four self-propelled robotic dollies throughout the 12 mile journey. However, due to bridge weight restrictions, the space shuttle was moved onto the dolly towed by the Tundra. After it had completely crossed the bridge, the Space Shuttle was returned to the robotic dollies. The footage was later used in a commercial for the 2013 Super Bowl. Having taken longer than expected, "Endeavour" finally reached the Science Center on October 14.

The exhibit was opened to the public on October 30, 2012 at the temporary Samuel Oschin Space Shuttle "Endeavour" Display Pavilion of the museum. A new addition to the Science Center, called the Samuel Oschin Air and Space Center, is under construction as "Endeavour's" permanent home. Planned for a 2017 opening, "Endeavour" will be mounted vertically with an external tank and a pair of solid rocket boosters in the shuttle stack configuration. One payload door will be open to reveal a demonstration payload inside.

After its decommissioning, "Endeavour's" Canadarm (formally the 'Shuttle Remote Manipulator System') was removed in order to be sent to the Canadian Space Agency’s John H. Chapman Space Centre in Longueuil, Quebec, a suburb of Montreal, where it was to be placed on display. In a Canadian poll on which science or aerospace museum should be selected to display the Canadarm, originally built by SPAR Aerospace, the Canadian Space Agency’s headquarters placed third to last with only 35 out of 638 votes. "Endeavour's" Canadarm has since gone on permanent display at the Canada Aviation and Space Museum in Ottawa.

In August 2015 NASA engineers went to work on removing a few of the tanks from "Endeavour" so that they may be used as storage containers for potable water on the International Space Station.

The Flow Director was responsible for the overall preparation of the shuttle for launch and processing it after landing, and remained permanently assigned to head the spacecraft's ground crew while the astronaut flight crews changed for every mission. Each shuttle's Flow Director was supported by a Vehicle Manager for the same spacecraft. Space shuttle "Endeavour"'s Flow Directors were:

"Endeavour" is currently housed in the Samuel Oschin Pavilion at the California Science Center in Exposition Park in South Los Angeles about two miles south of Downtown Los Angeles. A companion exhibit, ""Endeavour": The California Story", features images and artifacts that relate the shuttle program to California, where the orbiters were originally constructed. It has been planned for a new facility to be built with "Endeavour" attached to an external fuel tank (the last mission-ready one in existence as all others were destroyed during launch) and the two solid rocket boosters and raised in an upright position, as if "Endeavour" were to make one more flight.





</doc>
<doc id="28242" url="https://en.wikipedia.org/wiki?curid=28242" title="Sports Car Club of America">
Sports Car Club of America

The Sports Car Club of America (SCCA) is an American automobile club and sanctioning body supporting road racing, rallying, and autocross in the United States. Formed in 1944, it runs many programs for both amateur and professional racers.

The SCCA traces its roots to the Automobile Racing Club of America (not to be confused with the current stock car series of the same name). ARCA was founded in 1933 by brothers Miles and Sam Collier, and dissolved in 1941 at the outbreak of World War II. The SCCA was formed in 1944 as an enthusiast group. The SCCA began sanctioning road racing in 1948 with the inaugural Watkins Glen Grand Prix. Cameron Argetsinger, an SCCA member and local enthusiast who would later become Director of Pro Racing and Executive Director of the SCCA, helped organize the event for the SCCA.

In 1951, the SCCA National Sports Car Championship was formed from existing marquee events around the nation, including Watkins Glen, Pebble Beach, and Elkhart Lake. Many early SCCA events were held on disused air force bases, organized with the help of Air Force General Curtis LeMay, a renowned enthusiast of sports car racing. LeMay loaned out facilities of Strategic Air Command bases for the SCCA's use; the SCCA relied heavily on these venues during the early and mid-1950s during the transition from street racing to permanent circuits.

By 1962, the SCCA was tasked with managing the U.S. World Sportscar Championship rounds at Daytona, Sebring, Bridgehampton and Watkins Glen. The club was also involved in the Formula 1 U.S. Grand Prix. SCCA Executive Director John Bishop helped to create the United States Road Racing Championship series for Group 7 sports cars to recover races that had been taken by rival USAC Road Racing Championship. Bishop was also instrumental in founding the SCCA Trans-Am Series and the SCCA/CASC Can-Am series. In 1969, tension and infighting over Pro Racing's autonomy caused Bishop to resign and help form the International Motor Sports Association.

The SCCA dropped its amateur policy in 1962 and began sanctioning professional racing. In 1963, the United States Road Racing Championship was formed. In 1966 the Canadian-American Challenge Cup (Can-Am) was created for Group 7 open-top sportscars. The Trans-Am Series for pony cars also began in 1966. Today, Trans-Am uses GT-1 class regulations, giving amateur drivers a chance to race professionally. A professional series for open-wheel racing cars was introduced in 1967 as the SCCA Grand Prix Championship. This series was then held under various names through to the 1976 SCCA/USAC Formula 5000 Championship.

Current SCCA-sanctioned series include Trans Am, the Pirelli World Challenge for GT and touring cars, the Global MX-5 Cup, F2000 Championship Series, F1600 Championship Series and the Atlantic Championship Series. SCCA Pro Racing has also sanctioned professional series for some amateur classes such as Spec Racer Ford Pro and Formula Enterprises Pro. SCCA Pro Racing also sanctioned the Volkswagen Jetta TDI Cup during its time.

The Club Racing program is a road racing division where drivers race on either dedicated race tracks or on temporary street circuits. Competitors require either a regional or a national racing license. Both modified production cars (ranging from lightly modified cars with only extra safety equipment to heavily modified cars that retain only the basic shape of the original vehicle) and designed-from-scratch "formula" and "sports racer" cars can be used in Club Racing. Most of the participants in the Club Racing program are unpaid amateurs, but some go on to professional racing careers. The club is also the source for race workers in all specialties.

The annual national championship for Club Racing is called the SCCA National Championship Runoffs and has been held at Riverside International Raceway (1964, 1966, 1968), Daytona International Speedway (1965, 1967, 1969, 2015), Road Atlanta (1970–1993), Mid-Ohio Sports Car Course (1994–2005, 2016), Heartland Park Topeka (2006–2008), Road America (2009-2013, 2020), Mazda Raceway Laguna Seca (2014), and Indianapolis Motor Speedway (2017). In 2018, the Runoffs will go back west to Sonoma Raceway. In 2019, the race will be held at Virginia International Raceway a track where the race has never been held. It was announced on June 15 2018 that the Runoffs would go back to Road America in the year 2020. The current SCCA record holder is Jerry Hansen, (former owner of Brainerd International Raceway), with twenty-seven national championships.

The eight classes of the formula group are Formula Atlantic (FA), Formula 1000 (FB), Formula SCCA (FE), Formula Continental (FC), Formula Mazda (FM), Formula F (FF), Formula 500 (F500) and Formula Vee (FV)

The autocross program is branded as "Solo". Up to four cars at a time run on a course laid out with traffic cones on a large paved surface, such as a parking lot or airport runway, without interfering with one another.

Competitions are held at the regional, divisional, and national levels. Each division typically crowns a divisional champion in each class, determined at a single event. Similarly, a national champion in each class is determined at the national championship (usually referred to as "Nationals") held in September. In 2009, Solo Nationals moved to the Lincoln Airpark in Lincoln, Nebraska. Individual national-level events called "Championship Tours" and "Match Tours" are held throughout the racing season. The SCCA also holds national-level events in an alternate format called "ProSolo". In ProSolo, two cars compete at the same time on mirror-image courses with drag racing-style starts, complete with reaction and 60-foot times. Class winners and other qualifiers (based on time differential against the class winner) then compete in a handicapped elimination round called the "Challenge". Points are awarded in both class and Challenge competition, and an annual champion is crowned each September at the ProSolo Finale event in Lincoln, Nebraska.

The SCCA sanctions "RallyCross" events, similar to autocross, but on a non-paved course. SCCA ProRally was a national performance rally series similar to the World Rally Championship. At the end of the 2004 season SCCA dropped ProRally and ClubRally. A new organization, Rally America, picked up both series starting in 2005.

Road rallies are run on open, public roads. These are not races in the sense of speed, but of precision and navigation. The object is to drive on time, arriving at checkpoints with the proper amount of elapsed time from the previous checkpoint. Competitors do not know where the checkpoints are.

In recent years, the SCCA has expanded and re-organized some of the higher-speed events under the Time Trials banner. These include Performance Driving Experience ("PDX"), Club Trials, Track Trials, and Hill Climb events. PDX events are non-competition HPDE-type events and consist of driver-education and car control classroom learning combined with on-track instruction.

The SCCA is organized into nine divisions and 115 regions, each organizing events in that area to make the events more accessible to people throughout the country. The number of divisions has increased since the SCCA's foundation. Northern Pacific and Southern Pacific started as a single Pacific Coast Division until dividing in 1966. Rocky Mountain Division is a relatively recent split. The Great Lakes Division was split from the Central Division at the end of 2006.




</doc>
<doc id="28244" url="https://en.wikipedia.org/wiki?curid=28244" title="Star network">
Star network

A Star network is one of the most common computer network topologies. In its simplest form, a star network consists of one central hub which acts as a conduit to transmit messages. In star topology, every host is connected to a central hub. A star network is an implementation of a spoke–hub distribution paradigm in computer networks. 

The hub and hosts, and the transmission lines between them, form a graph with the topology of a star. Data on a star network passes through the hub before continuing to its destination. The hub manages and controls all functions of the network. It also acts as a repeater for the data flow. 

The star topology reduces the impact of a transmission line failure by independently connecting each host to the hub. Each host may thus communicate with all others by transmitting to, and receiving from, the hub. The failure of a transmission line linking any host to the hub will result in the isolation of that host from all others, but the rest of the network will be unaffected.

The star configuration is commonly used with twisted pair cable and optical fibre cable. However, it can also be used with coaxial cable.




</doc>
<doc id="28245" url="https://en.wikipedia.org/wiki?curid=28245" title="SQL Server">
SQL Server

SQL Server may refer to:


</doc>
<doc id="28246" url="https://en.wikipedia.org/wiki?curid=28246" title="Sufism">
Sufism

Sufism, or Taṣawwuf (; personal noun: "ṣūfiyy" / "ṣūfī", "mutaṣawwuf"), variously defined as "Islamic mysticism", "the inward dimension of Islam" or "the phenomenon of mysticism within Islam", is a mystical trend in Islam "characterized ... [by particular] values, ritual practices, doctrines and institutions" which began very early in Islamic history and represents "the main manifestation and the most important and central crystallization of" mystical practice in Islam. Practitioners of Sufism have been referred to as "Sufis" (Arabic plurals: "ṣūfiyyah"; "ṣūfiyyūn"; "mutaṣawwufah"; "mutaṣawwufūn").

Idries Shah, in his book The Sufis, wrote that "There were Sufis at all times and in all countries, says the tradition. Sufis existed as such and under this name before Islam. But, if there was a name for the practitioner, there was no name for the practice ...".

Historically, Sufis have often belonged to different "ṭuruq", or "orders" – congregations formed around a grand master referred to as a "wali" who traces a direct chain of successive teachers back to the Islamic prophet, Muhammad. These orders meet for spiritual sessions ("majalis") in meeting places known as "zawiyas", "khanqahs" or "tekke". They strive for "ihsan" (perfection of worship), as detailed in a "hadith": "Ihsan is to worship Allah as if you see Him; if you can't see Him, surely He sees you." Rumi stated: "The Sufi is hanging on to Muhammad, like Abu Bakr." Sufis regard Muhammad as "al-Insān al-Kāmil", the primary perfect man who exemplifies the morality of God, and see him as their leader and prime spiritual guide.

All Sufi orders trace many of their original precepts from Muhammad through his son-in-law Ali, with the notable exception of the Naqshbandi, who claim to trace their origins from Muhammad through the first Rashid Caliph, Abu Bakr. Although the overwhelming majority of Sufis, both pre-modern and modern, were and are adherents of Sunni Islam, there also developed certain strands of Sufi practice within the ambit of Shia Islam during the late medieval period. Although Sufis were opposed to dry legalism, they strictly observed Islamic law and belonged to various schools of Islamic jurisprudence and theology.

Sufis have been characterized by their asceticism, especially by their attachment to "dhikr", the practice of remembrance of God, often performed after prayers. They gained adherents among a number of Muslims as a reaction against the worldliness of the early Umayyad Caliphate (661–750)
and have spanned several continents and cultures over a millennium, initially expressing their beliefs in Arabic and later expanding into Persian, Turkish, and Urdu, among others. Sufis played an important role in the formation of Muslim societies through their missionary and educational activities. According to William Chittick, "In a broad sense, Sufism can be described as the interiorization, and intensification of Islamic faith and practice."

The term Sufism could be a be neologism of German origin coined by August Tholuck in his first book "Sufismus, sive theosophia Persarum pantheistica", published in Latin in Berlin in 1821.


</doc>
<doc id="28249" url="https://en.wikipedia.org/wiki?curid=28249" title="Search algorithm">
Search algorithm

In computer science, a search algorithm is any algorithm which solves the search problem, namely, to retrieve information stored within some data structure, or calculated in the search space of a problem domain. Examples of such structures include but are not limited to a linked list, an array data structure, or a search tree. The appropriate search algorithm often depends on the data structure being searched, and may also include prior knowledge about the data. Searching also encompasses algorithms that query the data structure, such as the SQL SELECT command.""

Search algorithms can be classified based on their mechanism of searching. Linear search algorithms check every record for the one associated with a target key in a linear fashion. Binary, or half interval searches, repeatedly target the center of the search structure and divide the search space in half. Comparison search algorithms improve on linear searching by successively eliminating records based on comparisons of the keys until the target record is found, and can be applied on data structures with a defined order. Digital search algorithms work based on the properties of digits in data structures that use numerical keys. Finally, hashing directly maps keys to records based on a hash function. Searches outside a linear search require that the data be sorted in some way.

Search functions are also evaluated on the basis of their complexity, or maximum theoretical run time. Binary search functions, for example, have a maximum complexity of , or logarithmic time. This means that the maximum number of operations needed to find the search target is a logarithmic function of the size of the search space.

Algorithms for searching virtual spaces are used in the constraint satisfaction problem, where the goal is to find a set of value assignments to certain variables that will satisfy specific mathematical equations and inequations / equalities. They are also used when the goal is to find a variable assignment that will maximize or minimize a certain function of those variables. Algorithms for these problems include the basic brute-force search (also called "naïve" or "uninformed" search), and a variety of heuristics that try to exploit partial knowledge about the structure of this space, such as linear relaxation, constraint generation, and constraint propagation.

An important subclass are the local search methods, that view the elements of the search space as the vertices of a graph, with edges defined by a set of heuristics applicable to the case; and scan the space by moving from item to item along the edges, for example according to the steepest descent or best-first criterion, or in a stochastic search. This category includes a great variety of general metaheuristic methods, such as simulated annealing, tabu search, A-teams, and genetic programming, that combine arbitrary heuristics in specific ways.

This class also includes various tree search algorithms, that view the elements as vertices of a tree, and traverse that tree in some special order. Examples of the latter include the exhaustive methods such as depth-first search and breadth-first search, as well as various heuristic-based search tree pruning methods such as backtracking and branch and bound. Unlike general metaheuristics, which at best work only in a probabilistic sense, many of these tree-search methods are guaranteed to find the exact or optimal solution, if given enough time. This is called "completeness".

Another important sub-class consists of algorithms for exploring the game tree of multiple-player games, such as chess or backgammon, whose nodes consist of all possible game situations that could result from the current situation. The goal in these problems is to find the move that provides the best chance of a win, taking into account all possible moves of the opponent(s). Similar problems occur when humans or machines have to make successive decisions whose outcomes are not entirely under one's control, such as in robot guidance or in marketing, financial, or military strategy planning. This kind of problem — combinatorial search — has been extensively studied in the context of artificial intelligence. Examples of algorithms for this class are the minimax algorithm, alpha–beta pruning, * Informational search and the A* algorithm.

The name "combinatorial search" is generally used for algorithms that look for a specific sub-structure of a given discrete structure, such as a graph, a string, a finite group, and so on. The term combinatorial optimization is typically used when the goal is to find a sub-structure with a maximum (or minimum) value of some parameter. (Since the sub-structure is usually represented in the computer by a set of integer variables with constraints, these problems can be viewed as special cases of constraint satisfaction or discrete optimization; but they are usually formulated and solved in a more abstract setting where the internal representation is not explicitly mentioned.)

An important and extensively studied subclass are the graph algorithms, in particular graph traversal algorithms, for finding specific sub-structures in a given graph — such as subgraphs, paths, circuits, and so on. Examples include Dijkstra's algorithm, Kruskal's algorithm, the nearest neighbour algorithm, and Prim's algorithm.

Another important subclass of this category are the string searching algorithms, that search for patterns within strings. Two famous examples are the Boyer–Moore and Knuth–Morris–Pratt algorithms, and several algorithms based on the suffix tree data structure.

In 1953, American statistician Jack Kiefer devised Fibonacci search which can be used to find the maximum of a unimodal function and has many other applications in computer science.

There are also search methods designed for quantum computers, like Grover's algorithm, that are theoretically faster than linear or brute-force search even without the help of data structures or heuristics.




</doc>
<doc id="28250" url="https://en.wikipedia.org/wiki?curid=28250" title="Sorcerer (Linux distribution)">
Sorcerer (Linux distribution)

Sorcerer was a source-based Linux distribution. The distribution downloads and compiles source code to install and update installed software.

Instead of using abbreviations such as rpm (Red Hat) and dpkg (Debian), Sorcerer's tool terminology is based upon magic words. For example, a recipe for downloading, compiling, and installing software is called a "spell". Software to install is "cast" onto the computer. Installed software can be removed by "dispelling". Consequently, the command line tools for casting and dispelling software are called cast and dispel, respectively.

In 2000, Kyle Sallee created a source-based Linux distribution called "Sorcerer GNU/Linux" and released it under the GNU GPL. During this time Sorcerer was a technology demonstration rather than a stable distribution. Eventually the distribution tools, called sorcery, and the software catalog, called grimoire, were redesigned and rewritten to be stable and usable on production machines. Due to the effort involved in single handedly creating and maintaining a distribution Sallee ceased "Sorcerer GNU/Linux" during the redesign and rewrite of sorcery and grimoire. A month or two before the rewrite happened, in early 2002, Chuck S. Mead, who had previously created a fork of RedHat, created a fork of Sorcerer GNU/Linux. The first fork of Sorcerer GNU/Linux was called by the same name as Mead's fork of RedHat. It was called either "Lunar Penguin" or Lunar Linux. This fork's timing was fortunate for system administrators, because it granted them an opportunity to continue deployment of the distribution that Sallee was about to discontinue.

The earliest versions of Sorcerer were named "Sorcerer GNU/Linux", with key components licensed under the GNU General Public License (GPL). However, from around 2002 and forward current versions of Sorcerer release some key components under the "Sorcerer Public License" and not the GPL, and the distribution has dropped the term "GNU/Linux". Sorcerer has two ancient forks: Lunar Linux and Source Mage which are not based on nor compatible with the current sorcery code nor compatible with current grimoire. Their terminology also deviates from Sorcerer terminology.

Sorcerer is based almost exclusively on source code. While many other operating systems generally make use of a package that contains pre-compiled (executable) programs, Sorcerer compiles source code on the machine prior to installation.

A new "grimoire", which is a catalog of software supported for immediate installation, is made available daily. When new sources are available, the spells in the grimoire are updated. A user's desktop is updated by first installing a current grimoire. If necessary, the installed Sorcery is updated. Finally, installed software can be updated according to the user's decision. Users can also add new spells to the grimoire on their local machine and submit the new spell for inclusion in the general distribution.

Sorcery automatically recompiles installed software as necessary to ensure continued compatibility and usability when installed libraries are updated. Unlike a pre-compiled binary-based distribution that must always download new packages, Sorcery most often recompiles installed software from previously downloaded sources. When a new source is required and an older source was previously downloaded, then Sorcerer will download a tiny patch that transforms the old source tarball into a current source tarball. The practice of keeping old source tarballs and downloading patches for updates allows Sorcerer systems to be updated using less bandwidth than distributions that provide pre-compiled packages.

The minimum system requirements are given as 1 GB of RAM and 20 GB of hard disk space. This is suggested because compiling some sources will consume large amounts of resources. Sorcerer has recently started using cgroups to limit the impact of software compilation on the system performance. Therefore, Sorcerer installations are normally updated while still in multi-user mode while causing no interruption to services or downtime.

Other source-based Linux distributions:



</doc>
<doc id="28251" url="https://en.wikipedia.org/wiki?curid=28251" title="Software package">
Software package

Software package may refer to:




</doc>
<doc id="28254" url="https://en.wikipedia.org/wiki?curid=28254" title="Safe semantics">
Safe semantics

Safe semantics is a consistency model that describes one type of guarantee a data register provides when it is shared by several processors in a parallel machine or in a network of computers working together.
This notion was first defined by Lamport in 1985. Later on, it was formally defined in Leslie Lamport's "On Interprocess Communication", which was published in "Distributed Computing" in 1986.

Safe semantics are defined for a variable with a single writer but multiple readers (SWMR). A SWMR register is safe if each read operation satisfies the two following properties:



In particular, if there is concurrency between a read and a write operation, the read operation can return a value which has never been written by any write operation. The return value only belongs to the register domain.

We can see a binary safe register as modeling a bit flickering. Whatever the previous value of the register is, the register's value could flicker until the write operation finishes. Therefore, the read operation which overlaps with a write operation could return 0 or 1.

There have been many implementations of safe register in distributed systems. Baldoni et al. show that there is no way to implement a register having the stronger property of regular semantics in a synchronous system under continuous churn. On the other hand, it has been demonstrated in that a safe register can be implemented under continuous churn in a non-synchronous system. Here, churn refers to the
leaving and joining of servers from/into a distributed system. Modeling and Implementing a type of storage memory (Safe Register) under non-quiescent churn in required some system models such as client and server systems.Client systems contains finite arbitrary number of processes and they are responsible for reading and writing into the server system.On the other hand,the server system just make sure that read and write operations happen properly.Safe register implementation was as follow:

-Safe register was maintained by the set of active servers.

-Clients do not maintain any register information (trigger operation, and interact with servers)

-Eventually synchronous system

-Quorums(set of server or client systems)

-Size of the Read() and Write() operation executed on quorums = n – f – J (n is the number of servers, J is the number of servers that leave and join,and f is the number of Byzantine failures.

Before implementing the safe register,in some algorithms were introduced such as join,read, and write operation.

Join Operation():A server(si) which wants to get entered into a server system will broadcast an inquiry message to other servers to inform other servers of its arrival into the distributed system,si also wants to find a current value of the register.Once other server received this inquiry they will send a reply message to si.After si receives enough reply from other servers,it will collect all the replies and saves them into a reply set.Si waits until it gets enough reply(n-f-j) from other servers then it will pick up the most frequent value among other values.Si will also do the following :

-Updates its local copy of the register

-It becomes active

-Sends reply to the processes in the set reply

-If si its active it will sends reply message to the other servers immediately.

-Otherwise,if Si is not active, it will store the inquiries somewhere to reply them by the time it become active.

-When si gets reply from other servers it will eventually add the new reply to the reply set and throw the old value from the reply set.

-If the value of the respond server is bigger that si value, then si will update its information with the new value.

Read operation(): the read operation algorithm is a basic version of the join operation.The only difference between these two algorithms is the broadcast mechanism used by the read operation.A client (cw)will broadcast a message to the system and once a server receives the inquiry,it will send a reply message to the client.Once the client receives enough replies (n-f-j) it will stop sending an inquiry.

Write operation:Client(cw) sends an inquiry into the system in different rounds and it will wait until it receives two acknowledgment.(sn =sequence number)

The reason for receiving two acknowledgment is because there could be a danger in a system. When a process Sends an acks, it may die after one millisecond.Therefore,there will be no confirmation received by the client.

In the validity of the safe register(If a read() not concurrent with any write(), returns the last value written before its invocation) was proved based on the quorum system.Assume that there are two quorum system(Qw,Qr).Qw indicates the Servers that know about the latest value,and Qr indicates Values of Read’s responses.Based on the assumption in the size of each quorum is equal to n-f-j.To prove the safe register's validity we need to prove the following equation:
(Qw∩Qr)\B >(Qr∩B) :Note that B is the number of Byzantine failures.
Proof : Red region indicates (Qw∩Qr)\B and the blue region indicates Qr∩B.Based on the assumption,we know that the size of each quorum is n-f-j,so the red region will have n-3f-2j active servers.Therefore,n-3f-2J > f --> n > 4f+2J --> n is strictly greater than f. 


</doc>
<doc id="28258" url="https://en.wikipedia.org/wiki?curid=28258" title="Sarawak">
Sarawak

Sarawak (; ) is a state of Malaysia. Being the largest among 13 other states with the size almost equal to West Malaysia, Sarawak is located in northwest Borneo Island, Sarawak is bordered by the Malaysian state of Sabah to the northeast, Kalimantan, the Indonesian portion of Borneo, to the south, and the independent country of Brunei in the north. The capital city, Kuching, is the economic centre of the state and seat of the Sarawak state government. Other cities and towns in Sarawak include Miri, Sibu, and Bintulu. As of the 2015 census, the population of Sarawak was 2,636,000. Sarawak has an equatorial climate with tropical rainforests and abundant animal and plant species. It has several prominent cave systems at Gunung Mulu National Park. Rajang River is the longest river in Malaysia; Bakun Dam, one of the largest dams in Southeast Asia, is located on one of its tributaries, the Balui River. Mount Murud is the highest point in Sarawak.

The earliest known human settlement in Sarawak at the Niah Caves dates back 40,000 years. A series of Chinese ceramics dated from the 8th to 13th century AD was uncovered at the archaeological site of Santubong. The coastal regions of Sarawak came under the influence of the Bruneian Empire in the 16th century. In 1839, James Brooke, a British explorer, arrived in Sarawak. He, and his descendants, governed the state from 1841 to 1946. During World War II, it was occupied by the Japanese for three years. After the war, the last White Rajah, Charles Vyner Brooke, ceded Sarawak to Britain, and in 1946 it became a British Crown Colony. On 22 July 1963, Sarawak was granted self-government by the British and subsequently became one of the founding members of the Federation of Malaysia, established on 16 September 1963. However, the federation was opposed by Indonesia leading to a three-year confrontation. The creation of the Federation also resulted in a communist insurgency that lasted until 1990.

The head of state is the Governor, also known as the Yang di-Pertua Negeri, while the head of government is the Chief Minister. Sarawak is divided into administrative divisions, and districts, governed by a system that is closely modelled on the Westminster parliamentary system and was the earliest state legislature system in Malaysia.

Because of its natural resources, Sarawak specialises in the export of oil and gas, timber and oil palms, but also possesses strong manufacturing, energy and tourism sectors. It is ethnically, culturally, and linguistically diverse; major ethnic groups including Iban, Malay, Chinese, Melanau, Bidayuh and Orang Ulu. English and Malay are the two official languages of the state; there is no official religion.

The generally-accepted explanation of the state's name is that it is derived from the Sarawak Malay word "serawak", which means antimony. A popular alternative explanation is that it is a contraction of the four Malay words purportedly uttered by Pangeran Muda Hashim (uncle to the Sultan of Brunei), ""Saya serah pada awak"" (I surrender it to you), when he gave Sarawak to James Brooke, an English explorer in 1841. However, the latter explanation is incorrect: the territory had been named Sarawak before the arrival of James Brooke, and the word "awak" was not in the vocabulary of Sarawak Malay before the formation of Malaysia.

Sarawak is nicknamed "Land of the Hornbills" ("Bumi Kenyalang"). These birds are important cultural symbols for the Dayak people, representing the spirit of God. It is also believed that if a hornbill is seen flying over residences, it will bring good luck to the local community. Sarawak has eight of the world's fifty-four species of hornbills, and the Rhinoceros hornbill is the state bird of Sarawak.

Foragers are known to have lived around the west mouth of the Niah Caves (located southwest of Miri) 40,000 years ago. A modern human skull found near the Niah Caves is the oldest human remain found in Malaysia and the oldest modern human skull from Southeast Asia. Chinese ceramics dating to the Tang and Song dynasties (8th to 13th century AD, respectively) found at Santubong (near Kuching) hint at its significance as a seaport.

The Bruneian Empire was established in the coastal regions of Sarawak by the mid-15th century, and the Kuching area was known to Portuguese cartographers during the 16th century as "Cerava", one of the five great seaports of Borneo. It was also during this time that witnessed the birth of the Sultanate of Sarawak, a local kingdom that lasted for almost half a century before being reunited with Brunei in 1641. By the early 19th century, the Bruneian Empire was in decline, retaining only a tenuous hold along the coastal regions of Sarawak which were otherwise controlled by semi-independent Malay leaders. Away from the coast, territorial wars were fought between the Iban and a Kenyah-Kayan alliance.

The discovery of antimony ore in the Kuching region led Pangeran Indera Mahkota, a representative of the Sultan of Brunei, to increase development in the territory between 1824 and 1830. Increasing antimony production in the region led the Brunei Sultanate to demand higher taxes, which ultimately led to civil unrest. In 1839, Sultan Omar Ali Saifuddin II (1827–1852) assigned his uncle Pangeran Muda Hashim the task of restoring order but his inability to do so caused him to request the aid of British sailor James Brooke. Brooke's success in quelling the revolt was rewarded with antimony, property and the governorship of Sarawak, which at that time consisted only of a small area centred on Kuching.
The Brooke family, later called the White Rajahs, set about expanding the territory they had been ceded. 
With expansion came the need for efficient governance and thus, beginning in 1841, Sarawak was separated into the first of its administrative divisions with currency, the Sarawak dollar, beginning circulation in 1858. By 1912, a total of five divisions had been established in Sarawak, each headed by a Resident. The Brooke family generally practised a paternalistic form of government with minimal bureaucracy, but were pressured to establish some form of legal framework. Since they were unfamiliar with local customs, the Brooke government created an advisory Supreme Council, mostly consisting of Malay chiefs, to provide guidance. This council is the oldest state legislative assembly in Malaysia, with the first General Council meeting taking place at Bintulu in 1867. In 1928, a Judicial Commissioner, Thomas Stirling Boyd, was appointed as the first legally trained judge. A similar system relating to matters concerning various Chinese communities was also formed. Members of the local community were encouraged by the Brooke regime to focus on particular functions within the territory: the Ibans and other Dayak people were hired as militia while Malays were primarily administrators. Chinese, both local and immigrant, were mostly employed in plantations, mines and as bureaucrats. Expanding trade led to the formation of the Borneo Company Limited in 1856. The company was, and still is, involved in a wide range of businesses in Sarawak including trade, banking, agriculture, mineral exploration, and development.
Between 1853 and 1862, there were a number of uprisings against the Brooke government but all were successfully contained with the aid of local tribes. To guard against future uprisings, a series of forts were constructed to protect Kuching, including Fort Margherita, completed in 1871. By that time Brooke's control of Sarawak was such that defences were largely unnecessary.

Charles Anthoni Brooke succeeded his uncle in 1868 as the next White Rajah. Under his rule, Sarawak gained Limbang and the Baram and Trusan valleys from the Sultan of Brunei, later becoming a protectorate in 1888 with Britain handling foreign affairs but the Brooke government retaining administrative powers. Domestically, Brooke established the Sarawak Museum – the oldest museum in Borneo – in 1891, and brokered a peace in Marudi by ending intertribal wars there. Economic development continued, with oil wells drilling from 1910 and the Brooke Dockyard opening two years later. Anthony Brooke, who would become Rajah Muda (heir apparent) in 1939, was born in 1912.

A centenary celebration of Brooke rule in Sarawak was held in 1941. During the celebration, a new constitution was introduced that would limit the power of the Rajah and grant the Sarawak people a greater role in the functioning of government. However, this constitution was never fully implemented due to the Japanese occupation. That same year saw the British withdrawing its air and marine forces defending Sarawak to Singapore. With Sarawak now unguarded, the Brooke regime adopted a scorched earth policy where oil installations in Miri were to be destroyed and the Kuching airfield held as long as possible before being destroyed. Nevertheless, a Japanese invasion force led by Kiyotake Kawaguchi landed in Miri on 16 December 1941 and conquered Kuching on 24 December 1941, with British ground forces retreating to Singkawang in neighbouring Dutch Borneo. After ten weeks of fighting there, the Allied forces surrendered on 1 April 1942. Charles Vyner Brooke, the last Rajah of Sarawak, had already left for Sydney, Australia; his officers were captured by the Japanese and interned at the Batu Lintang camp.

Sarawak remained part of the Empire of Japan for three years and eight months. During this time it was divided into three provinces – Kuching-shu, Sibu-shu, and Miri-shu – each under their respective Provincial Governor. The Japanese otherwise preserved the Brooke administrative structure and appointed the Japanese to important government positions. Allied forces later carried out Operation Semut to sabotage Japanese operations in Sarawak. During the battle of North Borneo, the Australian forces landed at Lutong-Miri area on 20 June 1945 and had penetrated as far as Marudi and Limbang before halting their operations in Sarawak. After the surrender of Japan, the Japanese surrendered to the Australian forces at Labuan on 10 September 1945. The following day, the Japanese forces at Kuching surrendered, and the Batu Lintang camp was liberated. Sarawak was immediately placed under British Military Administration and managed by Australian Imperial Forces (AIF) until April 1946.
Lacking the resources to rebuild Sarawak after the war, Charles Vyner Brooke decided to cede Sarawak as British Crown Colony and a Cession Bill was put forth in the Council Negri (now Sarawak State Legislative Assembly), which was debated for three days. The bill was passed on 17 May 1946 with a narrow majority (19 versus 16 votes). This caused hundreds of Malay civil servants to resign in protest, sparking an anti-cession movement and the assassination of the second colonial governor of Sarawak Sir Duncan Stewart. Despite the resistance, Sarawak became a British Crown colony on 1 July 1946. Anthony Brooke opposed the cession of Sarawak to the British Crown, for which he was banished from Sarawak by the colonial government. He was only allowed to return 17 years later after Sarawak had become part of Malaysia. In 1950 all anti-cession movements in Sarawak ceased after a clamp-down by the colonial government.
On 27 May 1961, Tunku Abdul Rahman, the prime minister of the Federation of Malaya, announced a plan to form a greater federation together with Singapore, Sarawak, Sabah and Brunei, to be called Malaysia. On 17 January 1962, the Cobbold Commission was formed to gauge the support of Sarawak and Sabah for the plan; the Commission reported 80 percent support for federation. On 23 October 1962, five political parties in Sarawak formed a united front that supported the formation of Malaysia. Sarawak was officially granted self-government on 22 July 1963, and became federated with Malaya, North Borneo (now Sabah), and Singapore to form the federation of Malaysia on 16 September 1963. The governments of the Philippines and Indonesia opposed the federation, as did the Brunei People's Party and Sarawak-based communist groups, and in 1962, the Brunei Revolt broke out. Indonesian President Sukarno responded by deploying armed volunteers and, later, military forces into Sarawak. Thousands of Sarawak communist members went into Kalimantan, Indonesian Borneo, and underwent training with the Communist Party of Indonesia. The most significant engagement of the confrontation was fought at Plaman Mapu in April 1965. The defeat at Plaman Mapu ultimately resulted in the fall of Sukarno and he was replaced by Suharto as president of Indonesia. Negotiations were restarted between Malaysia and Indonesia and led to the end of the confrontation on 11 August 1966.

A number of communist groups existed in Sarawak, the first of which, the Sarawak Overseas Chinese Democratic Youth League, formed in 1951. Another group, the North Kalimantan Communist Party (NKCP) (also known as Clandestine Communist Organisation (CCO) by government sources) was formally set up in 1970. Weng Min Chyuan and Bong Kee Chok were two of the more notable communist leaders involved in the insurgency. As the political scene changed, it grew progressively more difficult for the communists to operate. This led to Bong opening talks with chief minister Abdul Rahman Ya'kub in 1973 and eventually signing an agreement with the government. Weng, who had moved to China in the mid-1960s but nonetheless retained control of the CCO, pushed for a continued armed insurrection against the government in spite of this agreement. The conflict continued mostly in the Rajang Delta region but eventually ended when, on 17 October 1990, the NKCP signed a peace agreement with the Sarawak government.

The head of the Sarawak state is the Yang di-Pertua Negeri (also known as TYT or Governor), a largely symbolic position appointed by the Yang di-Pertuan Agong (King of Malaysia) on the advice of the Malaysian federal government. Since 2014 this position has been held by Abdul Taib Mahmud. The TYT appoints the chief minister, currently held by Abang Johari Openg (BN), as the head of government. Generally, the leader of the party that commands the majority of the state Legislative Assembly is appointed as the chief minister; democratically elected representatives are known as state assemblymen. The state assembly passes laws on subjects that are not under the jurisdiction of the Parliament of Malaysia such as land administration, employment, forests, immigration, merchant shipping and fisheries. The state government is constituted by the chief minister, the cabinet ministers and their assistant ministers.

To protect the interests of the Sarawakians in the Malaysian federation, special safeguards have been included in the Constitution of Malaysia. These include: control over immigration in and out of the state as well as the residence status of non-Sarawakians and non-Sabahans, limitations on the practice of law to resident lawyers, independence of the Sarawak High Court from the High Court Peninsular Malaysia, a requirement that the Sarawak Chief Minister be consulted prior to the appointment of the chief judge of the Sarawak High Court, the existence of Native Courts in Sarawak and the power to levy sales tax. Natives in Sarawak enjoy special privileges such as quotas and employment in public service, scholarships, university placements, and business permits. Local governments in Sarawak are exempt from local council laws enacted by the Malaysian parliament.
Major political parties in Sarawak can be divided into three categories: native non-Muslim, native Muslim, and non-native; parties, however, may also include members from more than one group. The first political party, the Sarawak United Peoples' Party (SUPP), was established in 1959, followed by the Parti Negara Sarawak (PANAS) in 1960 and the Sarawak National Party (SNAP) in 1961. Other major political parties such as Parti Pesaka Sarawak (PESAKA) appeared by 1962. These parties later joined the national coalition of the Alliance Party. The Alliance Party (later regrouped into Barisan Nasional) has ruled Sarawak since the formation of Malaysia. The opposition in Sarawak has consistently alleged that the ruling coalition uses various types of vote-buying tactics in order to win elections. Stephen Kalong Ningkan was the first Chief Minister of Sarawak from 1963 to 1966 following his landslide victory in local council elections. However, he was ousted in 1966 by Tawi Sli with the help of the Malaysian federal government, causing the 1966 Sarawak constitutional crisis.
In 1969, the first Sarawak state election was held, with members of the Council Negri being directly elected by the voters. This election marked the beginning of ethnic Melanau domination in Sarawak politics by Abdul Rahman Ya'kub and Abdul Taib Mahmud. In the same year, the North Kalimantan Communist Party (NKCP) which subsequently waged a guerilla war against the newly elected Sarawak state government, was formed. The party was dissolved after the signing of a peace agreement in 1990. 1973 saw the birth of Parti Pesaka Bumiputera Bersatu (PBB) following a merger of several parties. This party would later become the backbone of the Sarawak BN coalition. In 1978, the Democratic Action Party (DAP) was the first West Malaysia-based party to open its branches in Sarawak. Sarawak originally held state elections together with national parliamentary elections. However, the then chief minister Abdul Rahman Ya'kub delayed the dissolution of the state assembly by a year to prepare for the challenges posed by opposition parties. This made Sarawak the only state in Malaysia to hold state elections separate from the national parliamentary elections since 1979. In 1983, SNAP started to fragment into several splinter parties due to recurrent leadership crises. The political climate in the state was stable until the 1987 Ming Court Affair, a political coup initiated by Abdul Taib Mahmud's uncle to topple the Taib-led BN coalition. However, the coup was unsuccessful and Taib retained his position as chief minister.

Since the 2006 state election, the Democractic Action Party (DAP) has derived the majority of its support from urban centres and became the largest opposition party in Sarawak. In 2010, it formed the Pakatan Rakyat coalition with Parti Keadilan Rakyat (PKR) and Parti Islam Se-Malaysia (PAS); the latter two parties had become active in Sarawak between 1996 and 2001. Sarawak is the only state in Malaysia where West Malaysia-based component parties in the BN coalition, especially the UMNO, have not been active in state politics.

On 12 June 2018, the Sarawak Parties Alliance was formed by the BN parties in the state in the aftermath of an historic meeting of party leaders in Kuching, where they decided that in light of the BN defeat in the 2018 Malaysian general election and the changing national situation and a new government, the parties will leave the BN altogether. 

Unlike states in Peninsular Malaysia, Sarawak is divided into divisions, 12 in all, as well as districts, each headed by an appointed resident.

A division is divided into districts, each headed by a district officer, which are in turn divided into sub-districts, each headed by a Sarawak Administrative Officer (SAO). There is also one development officer for each division and district to implement development projects. The state government appoints a headman (known as "ketua kampung" or "penghulu") for each village. There are a total of 26 sub-districts in Sarawak all under the jurisdiction of the Sarawak Ministry of Local Government and Community Development. The list of divisions, districts, and subdistricts is shown in the table below:

The first paramilitary armed forces in Sarawak, a regiment formed by the Brooke regime in 1862, were known as the Sarawak Rangers. The regiment, renowned for its jungle tracking skills, served in the campaign to end the intertribal wars in Sarawak. It also engaged in guerrilla warfare against the Japanese, in the Malayan Emergency (in West Malaysia) and the Sarawak Communist Insurgency against the communists. Following the formation of Malaysia, the regiment was absorbed into the Malaysian military forces and is now known as the Royal Ranger Regiment.

In 1888, Sarawak, together with neighbouring North Borneo, and Brunei, became British protectorates, and the responsibility for foreign policy was handed over to the British in exchange for military protection. Since the formation of Malaysia, the Malaysian federal government has been solely responsible for foreign policy and military forces in the country.

The Malaysian government has a number of border disputes with neighbouring countries, of which several concern Sarawak. This includes land and maritime disputes with neighbouring Brunei. In 2009, Malaysian prime minister Abdullah Ahmad Badawi claimed that in a meeting with Sultan of Brunei, Brunei agreed to drop its claim over Limbang. This was however denied by the second Foreign Minister of Brunei Lim Jock Seng, stating the issue was never discussed during the meeting. James Shoal (Betting Serupai) and the Luconia Shoals (Betting Raja Jarum/Patinggi Ali), islands in the South China Sea, fall within Sarawak's exclusive economic zone, but concerns have been raised about Chinese incursions. There are also several Sarawak–Kalimantan border issues with Indonesia.

The total land area of Sarawak is nearly , making up 37.5 percent of the total area of Malaysia, and lies between the northern latitudes 0° 50′ and 5° and eastern longitudes 109° 36′ and 115° 40′ E. Its of coastline is interrupted in the north by about of Bruneian coast. Sarawak is separated from Kalimantan Borneo by ranges of high hills and mountains that are part of the central mountain range of Borneo. These become loftier to the north, and are highest near the source of the Baram River at the steep Mount Batu Lawi and Mount Mulu. Mount Murud is the highest point in Sarawak.

Sarawak has a tropical geography with an equatorial climate and experiences two monsoon seasons: a northeast monsoon and a southwest monsoon. The northeast monsoon occurs between November and February, bringing heavy rainfall while the southwest monsoon, which occurs between March and October, brings somewhat less rainfall. The climate is stable throughout the year except for the two monsoons, with average daily temperature varying between in the morning to in the afternoon at coastal areas. Miri has the lowest average temperatures in comparison to other major towns in Sarawak and has the longest daylight hours (more than six hours a day), while other areas receive sunshine for five to six hours a day. Humidity is usually high, exceeding 68 percent, with annual rainfall varying between and for up to 220 days a year. At highland areas, the temperature can vary from to during the day and as low as during the night.

Sarawak is divided into three ecoregions. The coastal region is rather low-lying and flat with large areas of swamp and other wet environments. Beaches in Sarawak include Pasir Panjang and Damai beaches in Kuching, Tanjung Batu beach in Bintulu, and Tanjung Lobang and Hawaii beaches in Miri. Hilly terrain accounts for much of the inhabited land and is where most of the cities and towns are found. The ports of Kuching and Sibu are built some distance from the coast on rivers while Bintulu and Miri are close to the coastline where the hills stretch right to the South China Sea. The third region is the mountainous region along the SarawakKalimantan border, where a number of villages such as Bario, Ba'kelalan, and Usun Apau Plieran are located. A number of rivers flow through Sarawak, with the Sarawak River being the main river flowing through Kuching. The Rajang River is the longest river in Malaysia, measuring including its tributary, Balleh River. To the north, the Baram, Limbang and Trusan Rivers drain into the Brunei Bay.
Sarawak can be divided into two geological zones: the Sunda Shield, which extends southwest from the Batang Lupar River (near Sri Aman) and forms the southern tip of Sarawak, and the geosyncline region, which extends northeast to the Batang Lupar River, forming the central and northern regions of Sarawak. The oldest rock type in southern Sarawak is schist formed during the Carboniferous and Lower Permian times, while the youngest igneous rock in this region, andesite, can be found at Sematan. Geological formation of the central and northern regions started during the late Cretaceous period. Other types of stone that can be found in central and northern Sarawak are shale, sandstone, and chert. The Miri Division in eastern Sarawak is the region of Neogene strata containing organic rich rock formations which are the prolific oil and gas reserves. The rocks enriched in organic components are mudstones in Lambir, Miri and Tukau Formations of Middle Miocene-Lower Pliocene age. Significant quantities of Sarawak soil are lithosols, up to 60 percent, and podsols, around 12 percent, while abundant alluvial soil is found in coastal and riverine regions. 12 percent of Sarawak is covered with peat swamp forest.

There are thirty national parks, among which are Niah with its eponymous caves, the highly developed ecosystem around Lambir Hills, and the World Heritage Site of Gunung Mulu. The last contains Sarawak Chamber, one of the world's largest underground chambers, Deer Cave, the largest cave passage in the world, and Clearwater Cave, the longest cave system in Southeast Asia.

Sarawak contains large tracts of tropical rainforest with diverse plant species, which has led to a number of them being studied for medicinal properties. Mangrove and nipah forests lining its estuaries comprise 2% of its forested area, peat swamp forests along other parts of its coastline cover 16%, Kerangas forest covers 5% and Dipterocarpaceae forests cover most mountainous areas. The major trees found in estuary forests include "bako" and "nibong", while those in the peat swamp forests include "ramin" ("Gonystylus bancanus"), "meranti" ("Shorea"), and "medang jongkong" ("Dactylocladus stenostachys").
Animal species are also highly varied, with 185 species of mammals, 530 species of birds, 166 species of snakes, 104 species of lizards, and 113 species of amphibians, of which 19 percent of the mammals, 6 percent of the birds, 20 percent of the snakes and 32 percent of the lizards are endemic. These species are largely found in Totally Protected Areas. There are over 2,000 tree species in Sarawak. Other plants includes 1,000 species of orchids, 757 species of ferns, and 260 species of palm. The state is the habitat of endangered animals, including the borneo pygmy elephant, proboscis monkey, orangutans and Sumatran rhinoceroses. Matang Wildlife Centre, Semenggoh Nature Reserve, and Lanjak Entimau Wildlife Sanctuary are noted for their orangutan protection programmes. TalangSatang National Park is notable for its turtle conservation initiatives. Birdwatching is a common activity in various national parks such as Gunung Mulu National Park, Lambir Hills National Park, and Similajau National Park. MiriSibuti National Park is known for its coral reefs and Gunung Gading National Park for its "Rafflesia" flowers. Bako National Park, the oldest national park in Sarawak, is known for its 275 proboscis monkeys, and Padawan Pitcher Garden for its various carnivorous pitcher plants. In 1854, Alfred Russel Wallace visited Sarawak. A year later, he formulated the "Sarawak Law" which foreshadowed the formulation of his (and Darwin's) theory of evolution by natural selection three years later.

The Sarawak state government has enacted several laws to protect its forests and endangered wildlife species. Some of the protected species are the orangutan, green sea turtle, flying lemur, and piping hornbill. Under the Wild Life Protection Ordinance 1998, Sarawak natives are given permissions to hunt for a restricted range of wild animals in the jungles but should not possess more than of meat. The Sarawak Forest Department was established in 1919 to conserve forest resources in the state. Following international criticism of the logging industry in Sarawak, the state government decided to downsize the Sarawak Forest Department and created the Sarawak Forestry Corporation in 1995. The Sarawak Biodiversity Centre was set up in 1997 for the conservation, protection, and sustainable development of biodiversity in the state.

Sarawak's rain forests are primarily threatened by the logging industry and palm oil plantations. The issue of human rights of the Penan and deforestation in Sarawak became an international environmental issue when Swiss activist Bruno Manser visited Sarawak regularly between 1984 and 2000. Deforestation has affected the life of indigenous tribes, especially the Penan, whose livelihood is heavily dependent on forest produce. This led to several blockades by indigenous tribes during the 1980s and 1990s against logging companies encroaching on their lands. There have also been cases where Native Customary Rights (NCR) lands have been given to timber and plantation companies without the permission of the locals. The indigenous people have resorted to legal means to reinstate their NCR. In 2001 the High Court of Sarawak fully reinstated the NCR land claimed by the Rumah Nor people, but this was overturned partially in 2005. However, this case has served as a precedent, leading to more NCR being upheld by the high court in the following years. Sarawak's mega-dam policies, such as the Bakun Dam and Murum Dam projects, have submerged thousands of hectares of forest and displaced thousands of indigenous people. Since 2013, the proposed Baram Dam project has been delayed due to ongoing protests from local indigenous tribes. Since 2014, the Sarawak government under chief minister Adenan Satem started to take action against illegal logging in the state and to diversify the economy of the state. Through the course of 2016 over 2 million acres of forest, much of it in orangutan habitats, were declared protected areas.

Sources vary as to Sarawak's remaining forest cover: former chief minister Abdul Taib Mahmud declared that it fell from 70% to 48% between 2011 and 2012, the Sarawak Forest Department and the Ministry of Resource Planning and Environment both held that it remained at 80% in 2012, and Wetlands International reported that it fell by 10% between 2005 and 2010, 3.5 times faster than the rest of Asia combined.

Historically, Sarawak's economy was stagnant during the rule of previous three white Rajahs. After the formation of Malaysia, Sarawak GDP growth rate has risen due to increase in petroleum output and the rise in global petroleum prices. However, the state economy is less diversified and still heavily dependent upon the export of primary commodities when compared to Malaysia overall. The per capita GDP in Sarawak was lower than the national average from 1970 to 1990. As of 2016, GDP per capita for Sarawak stands at RM 44,333 - the fifth highest in Malaysia. However, the urban-rural income gap remained a major problem in Sarawak.

Sarawak is abundant in natural resources, and primary industries such as mining, agriculture, and forestry accounted for 32.8% of its economy in 2013. It also specialises in the manufacture of food and beverages, wood-based and rattan products, basic metal products, and petrochemicals, as well as cargo and air services and tourism. 
The state's gross domestic product (GDP) grew by 5.0% per year on average from 2000 to 2009, but became more volatile later on, ranging from −2.0% in 2009 to 7.0% in 2010. Sarawak contributed 10.1% of Malaysia's GDP in the nine years leading up to 2013, making it the third largest contributor after Selangor and Kuala Lumpur. From 2006 to 2013, the oil and gas industry accounted for 34.8% of the Sarawak government's revenue. It attracted RM 9.6 billion (US$2.88 billion) in foreign investments, with 90% going to the Sarawak Corridor of Renewable Energy (SCORE), the second largest economic corridor in Malaysia.

The export-oriented economy is dominated by liquefied natural gas (LNG), which accounts for more than half of total exports. Crude petroleum accounts for 20.8%, while palm oil, sawlogs, and sawn timber account for 9.0% collectively. The state receives a 5% royalty from Petronas over oil explorations in its territorial waters. Most of the oil and gas deposits are located offshore next to Bintulu and Miri at Balingian basin, Baram basin, and around Luconia Shoals.

Sarawak is one of the world's largest exporters of tropical hardwood timber, constituting 65% of the total Malaysian log exports in 2000. The last United Nations statistics in 2001 estimated Sarawak's sawlog exports at an average of per year between 1996 and 2000.

In 1955, OCBC became the first foreign bank to operate in Sarawak, with other overseas banks following suit. Other notable Sarawak-based companies include Cahya Mata Sarawak Berhad, Naim Holdings, and Rimbunan Hijau.

Electricity in Sarawak, supplied by the state-owned Sarawak Energy Berhad (SEB), is primarily sourced from traditional coal fired power plants and thermal power stations using LNG, but diesel based sources and hydroelectricity are also utilised. There are 3 hydroelectric dams at Batang Ai, Bakun, and Murum, with several others under consideration. In early 2016, SEB signed Malaysia's first energy export deal to supply electricity to neighbouring West Kalimantan in Indonesia.

In 2008, SCORE was established as a framework to develop the energy sector in the state, specifically the Murum, Baram, and Baleh Dams as well as potential coal-based power plants, and 10high priority industries out to 2030. The Regional Corridor Development Authority is the government agency responsible for managing SCORE. The entire central region of Sarawak is covered under SCORE, including areas such as Samalaju (near Bintulu), Tanjung Manis, and Mukah. Samalaju will be developed as an industrial park, with Tanjung Manis as a halal food hub, and Mukah as the administrative centre for SCORE with a focus on resource-based research and development.

Tourism plays a major role in the economy of the state, contributing 7.89% of the state's GDP in 2016. 
Foreign visitors to Sarawak are predominantly from Brunei, Indonesia, the Philippines, Singapore, China and the United Kingdom. A number of different organisations, both state and private, are involved in the promotion of tourism in Sarawak: the Sarawak Tourism Board is the state body responsible for tourism promotion in the state, various private tourism groups are united under the Sarawak Tourism Federation, and the Sarawak Convention Bureau is responsible for attracting conventions, conferences, and corporate events which are held in the Borneo Convention Centre in Kuching. The public and private bodies in Sarawak hold a biannual event to award the Sarawak Hornbill Tourism Award, an award for achievements within various categories, to recognise businesses and individuals for their efforts in the development of tourism within the state.

The Rainforest World Music Festival is the region's primary musical event, attracting more than 20,000 people annually. Other events that are held regularly in Sarawak are the ASEAN International Film Festival, Asia Music Festival, Borneo Jazz Festival, Borneo Cultural Festival, and Borneo International Kite Festival. Major shopping complexes in Sarawak include The Spring, Boulevard, Hock Lee Centre, City One shopping malls in Kuching, and Bintang Megamall, Boulevard, Imperial Mall, and Miri Plaza shopping malls in Miri.

Infrastructure development in Sarawak is overseen by the Ministry of Infrastructure Development and Transportation, successor to the Ministry of Infrastructure Development and Communications (MIDCom) after it was renamed in 2016. Despite this ministerial oversight, infrastructure in Sarawak remains relatively underdeveloped compared to Peninsular Malaysia.

In 2009, 94% of urban Sarawak was supplied with electricity, but only 67% of rural areas had electricity. However, this had increased to 91% by 2014. According to a 2015 article, household internet penetration in Sarawak was lower than Malaysian national average, 41.2% versus 58.6%, with 58.5% of internet use being in urban areas and 29.9% in rural areas. In comparison, mobile telecommunication uptake in Sarawak was comparable to the national average, 93.3% against a national average of 94.2%, and on par with neighbouring Sabah. Mobile telecommunication infrastructure, specifically broadcast towers, are built and managed by SacofaSdnBhd (Sacofa Private Limited), which enjoys a monopoly in Sarawak after the company was granted a 20-year exclusivity deal on the provision, maintenance and leasing of towers in the state.

A number of different bodies manage the supply of water depending on their region of responsibility, including the Kuching Water Board (KWB), Sibu Water Board (SWB), and LAKU Management SdnBhd, which handle water supply in Miri, Bintulu, and Limbang respectively, and the Rural Water Supply Department managing the water supply for the remaining areas. , 82% of the rural areas have a fresh water supply.

Much like many former British territories, Sarawak uses a dual carriageway with the left-hand traffic rule. As of 2013, Sarawak had a total of of connected roadways, with being paved state routes, of dirt tracks, of gravel roads, and of paved federal highway. The primary route in Sarawak is the Pan Borneo Highway, which runs from Sematan, Sarawak, through Brunei to Tawau, Sabah. Despite being a major highway, the condition of the road is poor leading to numerous accidents and fatalities. 16 billion ringgit worth of contracts were awarded to a number of local companies in December 2016 to add new vehicle and pedestrian bridges, interchanges and bus shelters to the highway as part of a multi-phase project.

A railway line existed before the war, but the last remnants of the line were dismantled in 1959. A rail project was announced in 2008 to be in line with the transport needs of SCORE, but as yet no construction work has begun despite an anticipated completion date in 2015. In 2017, the Sarawak government proposed a light rail system (Kuching Line) connecting Kuching, Samarahan and Serian divisions with anticipated completion in 2020. Currently, buses are the primary mode of public transportation in Sarawak with interstate services connecting the state to Sabah, Brunei, and Pontianak (Indonesia).

Sarawak is served by a number of airports with Kuching International Airport, located south west of Kuching, being the largest. Flights from Kuching are mainly to Kuala Lumpur but also to Johor Bahru, Penang, Sabah, Kelantan, Singapore and Pontianak, Indonesia. A second airport at Miri serves flights primarily to other Malaysian states as well as services to Singapore. Other smaller airports such as Sibu Airport, Bintulu Airport, Mukah Airport, Marudi Airport, Mulu Airport, and Limbang Airport provide domestic services within Malaysia. There are also a number of remote airstrips serving rural communities in the state. Three airlines serve flights in Sarawak, Malaysia Airlines, Air Asia, and MASwings all of which use Kuching Airport as their main hub. The state owned Hornbill Skyways is an aviation company that largely provides private chartered flights and flight services for public servants.
Sarawak has four primary ports located at Kuching, Sibu, Bintulu, and Miri. The busiest seaport at Bintulu is under the jurisdiction of the Malaysian federal government and mainly handles LNG products and regular cargo. The remaining ports are under the respective state port authorities. The combined throughput of the four primary ports was 61.04million freight weight tonnes (FWT) in 2013. Sarawak has 55 navigable river networks with a combined length of . For centuries, the rivers of Sarawak have been a primary means of transport as well as a route for timber and other agricultural goods moving downriver for export at the country's major ports. Sibu port, located from the river's mouth, is the main hub along the Rajang River mainly handling timber products. However, the throughput of Sibu port has declined over the years after Tanjung Manis Industrial Port (TIMP) began operating further downriver.

Health care in Sarawak is provided by three major government hospitals, Sarawak General Hospital, Sibu Hospital, and Miri Hospital, as well as numerous district hospitals, public health clinics, 1Malaysia clinics, and rural clinics. Besides government-owned hospitals and clinics, there are several private hospitals in Sarawak such as the Normah Medical Specialists Centre, Timberland Medical Specialists Centre, and Sibu Specialist Medical Centre. Hospitals in Sarawak typically provide the full gamut of health care options, from triage to palliative care for the terminally ill. In 1994, Sarawak General Hospital Department of Radiotherapy, Oncology & Palliative Care instituted an at-home care, or hospice care, program for cancer patients. The non profit Sarawak Hospice Society was established in 1998 to promote this program. 
In comparison to the number of other medical facilities, mental health is only serviced by a single facility, Hospital Sentosa. This abundance of medical services has made Sarawak a medical tourism destination for visitors from neighbouring Brunei and Indonesia.

In comparison to the prevalence of health services in urban regions, much of rural Sarawak is only accessible by river transport, which limits access. Remote rural areas that are beyond the operating areas of health clinics, about , and inaccessible by land or river are serviced by a monthly flying doctor service, which was established in 1973.
A village health promoter program, where volunteers are provided with basic medical training, was established in 1981 but difficulty in providing medical supplies to remote villages, as well as a lack of incentive, resulted in a decline of the program. A variety of traditional medicine practices are still being used by the various communities in Sarawak to supplement modern medical practices but this practice is also declining. However, since 2004, there has been a resurgence in traditional medicine in Malaysia resulting in the establishment of a traditional medicine division within the Ministry of Health. A 2006 government program to have integrated hospitals led to numerous universities starting programs to teach traditional medicine and major hospitals, including Sarawak General Hospital, providing traditional therapies.

Education in Malaysia falls under the remit of two federal ministries; the Malaysian Ministry of Education is responsible for primary and secondary education, while the Ministry of Higher Education has oversight over public universities, polytechnic and community colleges. Early childhood education is not directly controlled by the Ministry of Education as it does with primary and secondary education. However, the ministry does oversee the licensing of private kindergartens, the main form of early childhood education, in accordance with the National Pre-School Quality Standard, which was launched in 2013.

Around the time of Federation, overall literacy in Sarawak was quite low. In 1960, the overall literacy rate was 25%, with a heavy slant in the literacy rate towards the Chinese population, 53%, compared with that of indigenous peoples which was substantially lower, only 17%. By 2007, overall literacy in adults aged 15 and over had significantly increased to 92.3% and in 2012, this had climbed to 96%.

There were 1480 schools in Sarawak in 2014, of which 1271 were primary, 202 were secondary and 7 were vocational/technical secondary schools. Among these are a number of schools that date from the Brooke era, including St. Thomas's School Kuching (1848), St Mary's School Kuching (1848), and St Joseph's School Kuching (1882). As well as government schools, there are four international schools: Tunku Putra School, a primary and secondary school offering national and Cambridge curricula, Lodge International School, which is also open to local students and uses both the British National and Cambridge systems, Kidurong International School, which is owned by Shell and offers primary education mainly to children of employees but local children may enter depending on space availability, and Tenby International School, which opened in 2014 and is open to both local and expatriate children. There are also 14 Chinese independent secondary schools in Sarawak that teach in Chinese rather than English or Malay. Previously, only Chinese students were enrolled in these schools, but mobility of the workforce has led to increasing turnover of students as parents move to other areas for employment. This has led to an increasing number of bumiputera students being enrolled in Chinese schools.

Sarawak is home to three public universities – Universiti Malaysia Sarawak, Universiti Teknologi Mara at Kota Samarahan, and Universiti Putra Malaysia – as well as the private Curtin University, Malaysia and Swinburne University of Technology Sarawak Campus. The latter two are satellite campuses of Curtin University in Perth and Swinburne University of Technology in Melbourne, Australia.

With the establishment of SCORE and the associated potential of 1.6 million more jobs by 2030, the state government allocated RM1 billion from 2016 to 2020 to a Skills Development Fund for vocational education. In 2015, Petronas provided vocational scholarships to 150 underprivileged Sarawak students as part of its Vocational Institution Sponsorship and Training Assistance program, although it had been criticised for under-representing local students in its previous allocations; the company also provided support to other Sarawak vocational education centres.

The 2015 census of Malaysia reported a population of 2,636,000 in Sarawak, making it the fourth most populous state. However, this population is distributed over a large area resulting in Sarawak having the lowest population density in the country with only 20people per km. Although it has a low population density, the average population growth rate of 1.8%, from 2000 to 2010, is very close to the national average of 2.0%. In 2014, 58% of the population resided in urban areas with the remainder in rural areas, but over the next 10 years it is predicted that the urban population would rise to 65%. , the crude birth rate in Sarawak was 16.3 per 1000 individuals, the crude death rate was 4.3 per 1000 population, and the infant mortality rate was 6.5 per 1000 live births.

Urban populations consist predominantly of Malays, Melanaus, Chinese, and a small population of urban Ibans and Bidayuhs who migrated from their home villages seeking employment. The latter two are among the more than 40 sub-ethnic groups of Sarawak, many of whom still inhabit remote areas and are referred to as Orang Asal. The Orang Asal, and Malays, of Peninsular Malaysia, Sarawak and Sabah are referred to collectively as Bumiputera (son of the soil). This classification grants them special privileges in education, jobs, finance, and political positions.

The registration for, and issuing of, National identity cards, a legally required document for accessing various services, to these remote tribes has been problematic for many years, and in the past had even resulted in a large number of people from the Penan ethnic group being rendered effectively stateless. In recent years, this issue has seen progressive improvement with the implementation of systems such as mobile registration units.

Sarawak has a large immigrant work force with as many as 150,000 registered foreign migrant workers working as domestic workers or in plantation, manufacturing, construction, services and agriculture. However, this population of legally registered workers is overshadowed by a large population of between 320,000 and 350,000 illegal workers.

Sarawak has six major ethnic groups, Iban, Chinese, Malay, Bidayuh, Melanau, and Orang Ulu, as well as a number of ethnic groups with smaller but still substantial populations, such as the Kedayan, Javanese, Bugis, Murut, and Indian. In 2015, the Bidayuh and Iban, both indigenous ethnic groups of Sarawak, were officially recognised by the government of Malaysia as comprising the Dayak people.

The population of 745,400 of the Iban people in Sarawak, based on 2014 statistics, makes it the largest ethnic group in the state. The Iban were, in the past, a society that paid particular attention to social status, especially to those who displayed martial prowess as well as to those who demonstrated expertise in various fields such as farming and oratory. Specific terms were used to refer to those who belonged to particular social strata, such as the "raja berani" (rich and the brave), "orang mayuh" (ordinary people), and "ulun" (slaves). Despite modern influences, Iban still observe many of their traditional rituals such as Gawai Antu (festival of the dead) and Gawai Dayak (Harvest Festival).

Although the presence of Chinese in Sarawak dates back to the 6th century AD when traders first came to the state, the Chinese population today largely consists of communities originating from immigrants during the Brooke era. This migration was driven by the employment opportunities at gold mines in Bau. 
Sarawak Chinese are primarily Buddhist and Christian, and speak a multitude of dialects: Cantonese, Foochow, Hakka, Hokkien, Teochew, and Henghua (Putian people). They celebrate major cultural festivals such as Hungry Ghost Festival and the Chinese New Year much as their ancestors did. Chinese settlers in Sarawak were not limited to any one area. Those who settled in Kuching did so near the Sarawak River in an area that is now referred to as Chinatown. Immigrants from Fujian, led by Wong Nai Siong in 1901, settled along the Rajang River in what is now Sibu, while those who arrived in Miri sought work in the coal mines and oilfields.

During the Brooke era, Sarawak Malays were predominantly fishermen, leading to their villages being concentrated along river banks. However, with the advent of urban development, many Malays have migrated to seek employment in public and private sectors. Traditionally, they are known for their silver and brass crafts, wood carvings, and textiles.

The Melanau are a native people of Sarawak that lived in areas primarily around the modern city of Mukah, where they worked as fishermen and craftsmen as well renowned boat-builders. Historically the Melanau practised Animism, a belief that spirits inhabited objects in their environment, and while this is still practised today, most Melanau have since been converted to Christianity and Islam.

The Bidayuh are a southern Sarawak people, that were referred to by early European settlers as Land Dayaks because they traditionally live on steep limestone mountains. They account for 8.4 percent of the population of Sarawak and are the second most numerous of the indigenous Dayak people, after the Iban. The Bidayuh are indigenous to the areas that comprise the modern day divisions of Kuching and Samarahan. Although considered one people, their language is regionally distinct resulting in dialects that are unintelligible to Bidayuh from outside the immediate locale, resulting in English and Malay being the lingua franca. Like many other indigenous peoples, the majority of the Bidayuh have been converted to Christianity,
but still live in villages consisting of longhouses, with the addition of the distinctive round "baruk" where communal gatherings were held.

The numerous tribes who reside in Sarawak's interior such as the Kenyah, Kayan, Lun Bawang, Kelabit, Penan, Bisaya, and Berawan are collectively referred to as Orang Ulu. In the Iban language this name means "Upriver People," reflecting the location these tribes settled in; most of them reside near the drainage basin of the Baram River. Both woodworking and artistry are highly visible aspects of Orang Ulu culture exemplified by mural covered longhouses, carved wooden boats, and tattooing. Well-known musical instruments from the Orang Ulu are the Kayans' sapeh and Kenyah's sampe' and Lun Bawang's bamboo band. The Kelabit and Lun Bawang people are known for their production of fragrant rice. As with the many other indigenous peoples of Sarawak, the majority of Orang Ulu are Christians.

Although Islam is the official religion of the federation, Sarawak has no official state religion. However, during the chieftainship of Abdul Rahman Ya'kub, the Constitution of Sarawak was amended to make the Yang di-Pertuan Agong as the head of Islam in Sarawak and empower the state assembly to pass laws regarding Islamic affairs. With such provisions, Islamic policies can be formulated in Sarawak and the establishment of Islamic state agencies is possible. The 1978 Majlis Islam Bill enabled the setting up of Syariah Courts in Sarawak with jurisdictions over matrimonial, child custody, betrothal, inheritance, and criminal cases in the state. An appeals court and Courts of Kadi were also formed.

Sarawak is the only state in Malaysia where Christians outnumber Muslims. The earliest Christian missionaries in Sarawak were Church of England (Anglicans) in 1848, followed by Roman Catholics a few years later, and Methodists in 1903. Evangelizing first took place among the Chinese immigrants before spreading to indigenous animists. Other Christian denominations in Sarawak are Borneo Evangelical Mission (or Sidang Injil Borneo), and Baptists. Indigenous people such as the Iban, Bidayuh, and Orang Ulu have adopted Christianity although they do retain some of their traditional religious rites. Many Muslims come from the Malay, Melanau, and Kayan ethnic groups. Buddhism, Taoism, and Chinese folk religion are predominantly practised by Chinese Malaysians. Other minor religions in Sarawak are Baha'i, Hinduism, Sikhism, and animism.

English was the official language of Sarawak from 1963 to 1974 due to opposition from First Chief Minister of Sarawak Stephen Kalong Ningkan to the use of the Malay language in Sarawak. In 1974 the new Chief Minister Abdul Rahman Ya'kub recognised Malay alongside English as an official language of Sarawak. This new status given to the Malay language was further reinforced by new education standards transitioning curriculum to Malay. In 1985 English lost the status of an official language, leaving only Malay. Despite official policy, Sarawak opposition members argue that English remained the "de facto" official language of Sarawak. English is still spoken in the legal courts, and state legislative assembly. In 2015, Chief Minister Adenan Satem announced that English will be reinstated as an official language.

Although the official form of Malay, Bahasa Malaysia, is spoken by the government administration, it is used infrequently in colloquial conversation. The local dialect of "Bahasa Sarawak" (Sarawak Malay) dominates the vernacular. Bahasa Sarawak is the most common language of Sarawak Malays and other indigenous tribes. The Iban language, which has minor regional variations, is the most widely spoken native language, with 34 percent of the Sarawak population speaking it as a first language. The Bidayuh language, with six major dialects, is spoken by 10 percent of the population. The Orang Ulu have about 30 different language dialects. While the ethnic Chinese originate from a variety of backgrounds and speak many different dialects such as Cantonese, Hokkien, Hakka, Fuzhou, and Teochew, they also converse in Malaysian Mandarin.

The location and history of Sarawak has resulted in a broad diversity of ethnicity, culture and languages. Among the indigenous peoples of Sarawak, outside influences have led to many changes over time. The Iban tribal culture in Sarawak centred on the concept of the warrior and the ability to take heads from other tribes in battle. This practice, central as it was to the Iban people, was made illegal under James Brooke's rule and ultimately faded away although reminders of the practice are still seen in some long houses. Two other tribal peoples of the Sarawak Highlands, the Kelabit and Lun Bawang, have seen fundamental changes to their ethnic identities as a direct result of their conversion to Christianity. One major change was the shift in the focal point of their social interactions from the traditional long house to the local church. Their religious devotion has also helped shape their worldview outside of their village, particularly in response to change. For the Penan people, one of the last tribes to still be practising a nomadic lifestyle within the jungle, outside influence, particularly education, has resulted in a significant decline in the population that practice the nomadic lifestyle. Others settle down after intermixing with members of different tribes, such as the Orang Ulu. One direct result of this diversity in cultures, engendered by a policy of tolerance to all races, is the increasing numbers of tribal peoples marrying not only other Sarawakian tribes, but also to Chinese, Malays as well as citizens of European or American descent.

The indigenous tribes of Sarawak traditionally used oratory to pass on their culture from one generation to the next; examples of these traditional practices include the Iban's Ngajat dances, "Renong" (Iban vocal repertory), "Ensera" (Iban oral narratives), and epic storytelling by the Kayan and Kenyah.

In the years before federation, the colonial government recognised that British education and indigenous culture was influencing a new generation of Iban teachers. Thus, on 15 September 1958, the Borneo Literature Bureau was inaugurated with a charter to nurture and encourage local literature while also supporting the government in its release of documentation, particularly in technical and instructional manuscripts that were to be distributed to the indigenous peoples of Sarawak and Sabah. As well as indigenous languages, documents would also be published in English, Chinese and Malay. In 1977, the bureau came under the authority of the federal government language planning and development agency, the Dewan Bahasa dan Pustaka (DBP), which advocated publication only in Malay ultimately causing the demise of fledgling indigenous literature.

It was a number of decades before print media began to appear in Sarawak. The "Sarawak Gazette", published by the Brooke government, recorded a variety of news relating to economics, agriculture, anthropology, archaeology, began circulation in 1870 and continues in modern times.
However, in the decades following federation, restrictive laws and connections to businesses have meant that the media is a largely state-owned enterprise. One of the earliest known text publications in Borneo, "Hikayat Panglima Nikosa" (Story of Nikosa the Warrior), was first printed in Kuching, 1876.

There are a number of museums in Sarawak that preserve and maintain artefacts of Sarawak's culture. At the foot of Mount Santubong, Kuching, is Sarawak Cultural Village, a "living museum" that showcases the various ethnic groups carrying out traditional activities in their respective traditional houses. The Sarawak State Museum houses a collection of artefacts such as pottery, textiles, and woodcarving tools from various ethnic tribes in Sarawak, as well as ethnographic materials of local cultures. Orang Ulu's Sapeh (a dug-out guitar) is the best known traditional musical instrument in Sarawak and was played for Queen Elizabeth II during her official visit to Sarawak in 1972.

Sarawakians observe a number of holidays and festivals throughout the year. Apart from national Hari Merdeka and Malaysia Day celebrations, the state also celebrates Sarawak self-government Day on 22 July and the State Governor's birthday. Ethnic groups also celebrate their own festivals. The open house tradition allows other ethnic groups to join in the celebrations. Sarawak is the only state in Malaysia to declare the Gawai Dayak celebration a public holiday.

Sarawak being home to diverse communities, Sarawakian cuisine has a variety of ethnically influenced cuisines and cooking styles rarely found elsewhere in Malaysia. Notable dishes in the state include Sarawak laksa, kolo mee, and ayam pansuh. The state is also known for its Sarawak layer cake dessert.

Sarawak sent its own teams to participate in the 1958 and 1962 British Empire and Commonwealth Games, and 1962 Asian Games; after 1963, Sarawakians competed as part of the Malaysian team. Sarawak hosted the Malaysian SUKMA Games in 1990 and 2016, and was overall champion in the 1990, 1992, and 1994 SUKMA games. Sarawak has been overall champion for 11 consecutive years at the Malaysia Para Games since 1994.




</doc>
<doc id="28259" url="https://en.wikipedia.org/wiki?curid=28259" title="Seneca">
Seneca

Seneca may refer to:











</doc>
<doc id="28260" url="https://en.wikipedia.org/wiki?curid=28260" title="Sonnet">
Sonnet

A sonnet is a poem in a specific form which originated in Italy; Giacomo da Lentini is credited with its invention.

The term "sonnet" is derived from the Italian word "sonetto" (from Old Provençal "sonet" a little poem, from "son" song, from Latin "sonus" a sound). By the thirteenth century it signified a poem of fourteen lines that follows a strict rhyme scheme and specific structure. Conventions associated with the sonnet have evolved over its history. Writers of sonnets are sometimes called "sonneteers", although the term can be used derisively.

The sonnet was created by Giacomo da Lentini, head of the Sicilian School under Emperor Frederick II. Guittone d'Arezzo rediscovered it and brought it to Tuscany where he adapted it to his language when he founded the Siculo-Tuscan School, or Guittonian school of poetry (1235–1294). He wrote almost 250 sonnets. Other Italian poets of the time, including Dante Alighieri (1265–1321) and Guido Cavalcanti (c. 1250–1300), wrote sonnets, but the most famous early sonneteer was Petrarch. Other fine examples were written by Michelangelo.

The structure of a typical Italian sonnet of the time included two parts that together formed a compact form of "argument". First, the octave, forms the "proposition", which describes a "problem", or "question", followed by a sestet (two tercets), which proposes a "resolution". Typically, the ninth line initiates what is called the "turn", or "volta", which signals the move from proposition to resolution. Even in sonnets that don't strictly follow the problem/resolution structure, the ninth line still often marks a "turn" by signaling a change in the tone, mood, or stance of the poem.

Later, the "abba, abba" pattern became the standard for Italian sonnets. For the sestet there were two different possibilities: "cde, cde" and "cdc, cdc". In time, other variants on this rhyming scheme were introduced, such as "cdcdcd". Petrarch typically used an "abba, abba" pattern for the octave, followed by either "cde, cde" or "cdc, cdc" rhymes in the sestet. (The symmetries ("abba" vs. "cdc") of these rhyme schemes have also been rendered in musical structure in the late 20th century composition "Scrivo in Vento" by Elliott Carter, inspired by Petrarch's Sonnet 212, "Beato in Sogno".)

In English, both the English or Shakespearean sonnet, and the Italian Petrarchan sonnet are traditionally written in iambic pentameter.

The first known sonnets in English, written by Sir Thomas Wyatt and Henry Howard, Earl of Surrey, used the Italian, Petrarchan form, as did sonnets by later English poets, including John Milton, Thomas Gray, William Wordsworth and Elizabeth Barrett Browning. Early twentieth-century American poet Edna St. Vincent Millay also wrote mostly Petrarchan sonnets.

"On His Blindness" by Milton, gives a sense of the Petrarchan rhyme scheme:
<poem style="margin-left: 2em">
When I consider how my light is spent (a)
To serve therewith my Maker, and present (a)
That murmur, soon replies, "God doth not need (c)
Is Kingly. Thousands at his bidding speed (c)
</poem>

Most Sonnets in Dante's "La Vita Nuova" are Petrarchan. Chapter VII gives sonnet "O voi che per la via", with two sestets (AABAAB AABAAB) and two quatrains (CDDC CDDC), and Ch. VIII, "Morte villana", with two sestets (AABBBA AABBBA) and two quatrains (CDDC CDDC).

The sole confirmed surviving sonnet in the Occitan language is confidently dated to 1284, and is conserved only in troubadour manuscript "P", an Italian chansonnier of 1310, now XLI.42 in the Biblioteca Laurenziana in Florence. It was written by Paolo Lanfranchi da Pistoia and is addressed to Peter III of Aragon. It employs the rhyme scheme "a-b-a-b, a-b-a-b, c-d-c-d-c-d". This poem is historically interesting for its information on north Italian perspectives concerning the War of the Sicilian Vespers, the conflict between the Angevins and Aragonese for Sicily. Peter III and the Aragonese cause was popular in northern Italy at the time and Paolo's sonnet is a celebration of his victory over the Angevins and Capetians in the Aragonese Crusade:
An Occitan sonnet, dated to 1321 and assigned to one "William of Almarichi", is found in Jean de Nostredame and cited in Giovanni Mario Crescimbeni's, "Istoria della volgar poesia". It congratulates Robert of Naples on his recent victory. Its authenticity is dubious. There are also two poorly regarded sonnets by the Italian Dante de Maiano.

In the 16th century, around Ronsard (1524–1585)), Joachim du Bellay (1522–1560) and Jean Antoine de Baïf (1532–1589), there formed a group of radical young noble poets of the court (generally known today as La Pléiade, although use of this term is debated), who began writing in, amongst other forms of poetry, the Petrarchan sonnet cycle (developed around an amorous encounter or an idealized woman). The character of "La Pléiade" literary program was given in Du Bellay's manifesto, the "Defense and Illustration of the French Language" (1549), which maintained that French (like the Tuscan of Petrarch and Dante) was a worthy language for literary expression and which promulgated a program of linguistic and literary production (including the imitation of Latin and Greek genres) and purification.

By the late 17th century poets on increasingly relied on stanza forms incorporating rhymed couplets, and by the 18th century fixed-form poems – and, in particular, the sonnet – were largely avoided. The resulting versification – less constrained by meter and rhyme patterns than Renaissance poetry – more closely mirrored prose.

The Romantics were responsible for a return to (and sometimes a modification of) many of the fixed-form poems used during the 15th and 16th centuries, as well as for the creation of new forms. The sonnet however was little used until the Parnassians brought it back into favor, and the sonnet would subsequently find its most significant practitioner in Charles Baudelaire (1821–1867) . The traditional French sonnet form was however significantly modified by Baudelaire, who used 32 different forms of sonnet with non-traditional rhyme patterns to great effect in his "Les Fleurs du mal".

When English sonnets were introduced by Thomas Wyatt (1503–1542) in the early 16th century, his sonnets and those of his contemporary the Earl of Surrey were chiefly translations from the Italian of Petrarch and the French of Ronsard and others. While Wyatt introduced the sonnet into English, it was Surrey who developed the rhyme scheme – "abab cdcd efef gg" – which now characterizes the English sonnet. Having previously circulated in manuscripts only, both poets' sonnets were first published in Richard Tottel's "Songes and Sonnetts," better known as "Tottel's Miscellany" (1557).

It was, however, Sir Philip Sidney's sequence "Astrophel and Stella" (1591) that started the English vogue for sonnet sequences. The next two decades saw sonnet sequences by William Shakespeare, Edmund Spenser, Michael Drayton, Samuel Daniel, Fulke Greville, William Drummond of Hawthornden, and many others. These sonnets were all essentially inspired by the Petrarchan tradition, and generally treat of the poet's love for some woman, with the exception of Shakespeare's sequence of 154 sonnets. The form is often named after Shakespeare, not because he was the first to write in this form but because he became its most famous practitioner. The form consists of fourteen lines structured as three quatrains and a couplet. The third quatrain generally introduces an unexpected sharp thematic or imagistic "turn", the volta. In Shakespeare's sonnets, however, the volta usually comes in the couplet, and usually summarizes the theme of the poem or introduces a fresh new look at the theme. With only a rare exception, the meter is iambic pentameter.

This example, Shakespeare's "Sonnet 116", illustrates the form (with some typical variances one may expect when reading an Elizabethan-age sonnet with modern eyes):

<poem style="margin-left: 2em">
Let me not to the marriage of true minds (a)
Admit impediments, love is not love (b)*
Which alters when it alteration finds, (a)
Or bends with the remover to remove. (b)*
O no, it is an ever fixèd mark (c)**
That looks on tempests and is never shaken; (d)***
It is the star to every wand'ring bark, (c)**
Whose worth's unknown although his height be taken. (d)***
Love's not time's fool, though rosy lips and cheeks (e)
Within his bending sickle's compass come, (f)*
Love alters not with his brief hours and weeks, (e)
But bears it out even to the edge of doom: (f)*
</poem>
"* "<br>
"** <br>
"*** "<br>

The Prologue to "Romeo and Juliet" is also a sonnet, as is Romeo and Juliet's first exchange in Act One, Scene Five, lines 104–117, beginning with "If I profane with my unworthiest hand" (104) and ending with "Then move not while my prayer's effect I take" (117). The Epilogue to "Henry V" is also in the form of a sonnet.

A variant on the English form is the Spenserian sonnet, named after Edmund Spenser (c.1552–1599), in which the rhyme scheme is "abab, bcbc, cdcd, ee". The linked rhymes of his quatrains suggest the linked rhymes of such Italian forms as terza rima. This example is taken from "Amoretti":

<poem style="margin-left: 2em">
"Happy ye leaves! whenas those lily hands"

Happy ye leaves. whenas those lily hands, (a)
Which hold my life in their dead doing might, (b)
Shall handle you, and hold in love's soft bands, (a)
Like captives trembling at the victor's sight. (b)
And happy lines on which, with starry light, (b)
Those lamping eyes will deign sometimes to look,(c)
And read the sorrows of my dying sprite, (b)
Written with tears in heart's close bleeding book. (c)
And happy rhymes! bathed in the sacred brook (c)
Of Helicon, whence she derived is, (d)
When ye behold that angel's blessed look, (c)
My soul's long lacked food, my heaven's bliss. (d)
Leaves, lines, and rhymes seek her to please alone, (e)
Whom if ye please, I care for other none. (e)
</poem>

In the 17th century, the sonnet was adapted to other purposes, with John Donne and George Herbert writing religious sonnets (see John Donne's "Holy Sonnets"), and John Milton using the sonnet as a general meditative poem. Probably Milton's most famous sonnet is "When I Consider How My Light is Spent", titled by a later editor "On His Blindness". Both the Shakespearean and Petrarchan rhyme schemes were popular throughout this period, as well as many variants.

The fashion for the sonnet went out with the Restoration, and hardly any sonnets were written between 1670 and Wordsworth's time. However, sonnets came back strongly with the French Revolution. Wordsworth himself wrote hundreds of sonnets, of which amongst the best-known are "Upon Westminster Bridge", "The world is too much with us" and "London, 1802" addressed to Milton; his sonnets were essentially modelled on Milton's. Keats and Shelley also wrote major sonnets; Keats's sonnets used formal and rhetorical patterns inspired partly by Shakespeare, and Shelley innovated radically, creating his own rhyme scheme for the sonnet "Ozymandias". Sonnets were written throughout the 19th century, but, apart from Elizabeth Barrett Browning's "Sonnets from the Portuguese" and the sonnets of Dante Gabriel Rossetti, there were few very successful traditional sonnets. "Modern Love" (1862) by George Meredith is a collection of fifty 16-line sonnets about the failure of his first marriage.

Gerard Manley Hopkins wrote several major sonnets, often in sprung rhythm, such as "The Windhover", and also several sonnet variants such as the 10-line curtal sonnet "Pied Beauty" and the 24-line caudate sonnet "That Nature is a Heraclitean Fire". Hopkin's poetry was, however, not published until 1918. By the end of the 19th century, the sonnet had been adapted into a general-purpose form of great flexibility.

In the United States, Henry Wadsworth Longfellow wrote many sonnets, among others the cycle "Divina Commedia" ("Divine Comedy"). He used the Petrarchan rhyme scheme. Emma Lazarus also published many sonnets. She is the author of perhaps the best-known American sonnet, "The New Colossus".

In Canada during the last decades of the century, the Confederation Poets and especially Archibald Lampman were known for their sonnets, which were mainly on pastoral themes.

This flexibility was extended even further in the 20th century. Among the major poets of the early Modernist period, Robert Frost, Edna St. Vincent Millay and E. E. Cummings all used the sonnet regularly. William Butler Yeats wrote the major sonnet "Leda and the Swan", which uses half rhymes. Wilfred Owen's sonnet "Anthem for Doomed Youth" is another sonnet of the early 20th century. Spaniard Federico García Lorca also wrote sonnets. W. H. Auden wrote two sonnet sequences and several other sonnets throughout his career, and widened the range of rhyme-schemes used considerably. Auden also wrote one of the first unrhymed sonnets in English, "The Secret Agent" (1928). Robert Lowell wrote five books of unrhymed "American sonnets", including his Pulitzer Prize-winning volume "The Dolphin" (1973). Half-rhymed, unrhymed, and even unmetrical sonnets have been very popular since 1950; perhaps the best works in the genre are Seamus Heaney's "Glanmore Sonnets" and "Clearances," both of which use half rhymes, and Geoffrey Hill's mid-period sequence "An Apology for the Revival of Christian Architecture in England". The 1990s saw something of a formalist revival, however, and several traditional sonnets have been written in the past decade.

Other modern poets, including Don Paterson, Joan Brossa, Paul Muldoon used the form. Wendy Cope's poem "Stress" is a sonnet. Elizabeth Bishop's inverted "Sonnet" was one of her last poems. Ted Berrigan's book, "The Sonnets", "is conventional almost exclusively in [the] line count)". Paul Muldoon often experiments with 14 lines and sonnet rhymes, though without regular sonnet meter. The advent of the New Formalism movement in the United States has also contributed to contemporary interest in the sonnet. This includes the invention of the "word sonnet", which are fourteen line poems, with one word per line. Frequently allusive and imagistic, they can also be irreverent and playful. The Canadian poet Seymour Mayne published a few collections of word sonnets, and is one of the chief innovators of the form. Contemporary word sonnets combine a variation of styles often considered to be mutually exclusive to separate genres, as demonstrated in works such as "An Ode to Mary". The Greek poet Yannis Livadas in 1993 invented the so-called "fusion sonnet", which first appeared in a poetry collection entitled "The Hanging Verses Of Babylon"/"Οι Κρεμαστοί Στίχοι Της Βαβυλώνας" (Melani Books, Athens 2007), .

Paulus Melissus (1539–1602) was the first to use the sonnet and the "terza rima" in German lyric. In his lifetime he was recognized as an author fully versed in Latin love poetry.

The "Sonnets to Orpheus" are a cycle of 55 sonnets written in 1922 by the Bohemian-Austrian poet Rainer Maria Rilke (1875–1926). It was first published the following year. Rilke, who is "widely recognized as one of the most lyrically intense German-language poets", wrote the cycle in a period of three weeks experiencing what he described a "savage creative storm". Inspired by the news of the death of Wera Ouckama Knoop (1900–1919), a playmate of Rilke's daughter Ruth, he dedicated them as a memorial, or "" (literally "grave-marker"), to her memory.

In the Netherlands Pieter Corneliszoon Hooft wrote sonnets. A famous example is "Mijn lief, mijn lief, mijn lief". Some of his poems were translated by Edmund Gosse. More recent examples include Martinus Nijhoff and Jan Kal.

In the Indian subcontinent, sonnets have been written in the Assamese, Bengali, Dogri, English, Gujarati, Hindi, Kashmiri, Malayalam, Manipuri, Marathi, Nepali, Oriya, Sindhi and Urdu languages. Urdu poets, also influenced by English and other European poets, took to writing sonnets in the Urdu language rather late. Azmatullah Khan (1887–1923) is believed to have introduced this format to Urdu literature in the very early part of the 20th century. The other renowned Urdu poets who wrote sonnets were Akhtar Junagarhi, Akhtar Sheerani, Noon Meem Rashid, Mehr Lal Soni Zia Fatehabadi, Salaam Machhalishahari and Wazir Agha. This example, a sonnet by Zia Fatehabadi taken from his collection "Meri Tasveer", is in the usual English (Shakespearean) sonnet rhyme-scheme.

Alexander Pushkin's novel in verse "Eugene Onegin" consists almost entirely of 389 stanzas of iambic tetrameter with the unusual rhyme scheme "AbAbCCddEffEgg", where the uppercase letters represent feminine rhymes while the lowercase letters represent masculine rhymes. This form has come to be known as the "Onegin stanza" or the "Pushkin sonnet."

Unlike other traditional forms, such as the Petrarchan sonnet or Shakespearean sonnet, the Onegin stanza does not divide into smaller stanzas of four lines or two in an obvious way. There are many different ways this sonnet can be divided.

In post-Pushkin Russian poetry, the form has been utilized by authors as diverse as Mikhail Lermontov, Vyacheslav Ivanov, Jurgis Baltrušaitis and , in genres ranging from one-stanza lyrical piece to voluminous autobiography. Nevertheless, the Onegin stanza, being easily recognisable, is strongly identified as belonging to its creator.

John Fuller's 1980 "The Illusionists" and Jon Stallworthy's 1987 "The Nutcracker" used this stanza form, and Vikram Seth's 1986 novel "The Golden Gate" is written wholly in Onegin stanzas.

The sonnet was introduced into Polish literature in the 16th century by Jan Kochanowski, Mikołaj Sęp-Szarzyński and Sebastian Grabowiecki. Later in 1826 Adam Mickiewicz wrote a series known as "Crimean Sonnets", which was translated into English by Edna Worthley Underwood. Sonnets were also written by Adam Asnyk, Jan Kasprowicz and Leopold Staff. Polish poets usually shape their sonnets according to Italian or French practice. The English sonnet is not common. Kasprowicz used a Shelleyan rhyme scheme: aba bcb cdc ded ee. Polish sonnets are typically written in either hendecasyllables (5+6 syllables) or Polish alexandrines (7+6 syllables).

The sonnet was introduced into Czech literature at the beginning of the 19th century. The first great Czech sonneteer was Ján Kollár, who wrote a cycle of sonnets named "Slávy Dcera" ("The daughter of Sláva" / "The daughter of fame"). Kollár was Slovak and a supporter of Pan-Slavism, but wrote in Czech, as he disagreed that Slovak should be a separate language. Kollár's magnum opus was planned as a Slavic epic poem as great as Dante's Divine Comedy. It consists of "The Prelude" written in quantitative hexameters, and sonnets. The number of poems increased in subsequent editions and came up to 645. The greatest Czech romantic poet, Karel Hynek Mácha also wrote many sonnets. In the second half of the 19th century Jaroslav Vrchlický published "Sonety samotáře" ("Sonnets of a Solitudinarian"). Another poet, who wrote many sonnets was Josef Svatopluk Machar. He published "Čtyři knihy sonetů" ("The Four Books of Sonnets"). In the 20th century Vítězslav Nezval wrote the cycle "100 sonetů zachránkyni věčného studenta Roberta Davida" ("One Hundred Sonnets for the Woman who Rescued Perpetual Student Robert David"). After the Second World War the sonnet was the favourite form of Oldřich Vyhlídal. Czech poets use different metres for sonnets, Kollár and Mácha used decasyllables, Vrchlický iambic pentameter, Antonín Sova free verse, and Jiří Orten the Czech alexandrine. Ondřej Hanus wrote a monograph about Czech Sonnets in the first half of the twentieth century.

In Slovenia the sonnet became a national verse form. The greatest Slovenian poet, France Prešeren, wrote many sonnets. His best known work worldwide is "Sonetni venec" ("A Wreath of Sonnets"), which is an example of crown of sonnets. Another work of his is the sequence "Sonetje nesreče" ("Sonnets of Misfortune"). In writing sonnets Prešeren was followed by many later poets. After the Second World War sonnets remained very popular. Slovenian poets write both traditional rhymed sonnets and modern ones, unrhymed, in free verse. Among them are Milan Jesih and Aleš Debeljak. The metre for sonnets in Slovenian poetry is iambic pentameter with feminine rhymes, based both on the Italian endecasillabo and German iambic pentameter.





</doc>
<doc id="28261" url="https://en.wikipedia.org/wiki?curid=28261" title="Samba">
Samba

Samba () is a Brazilian musical genre and dance style, with its roots in Africa via the West African slave trade and African religious traditions, particularly of Angola and the Congo, through the samba de roda genre of the northeastern Brazilian state of Bahia, from which it derived. Although there were various forms of samba in Brazil with popular rhythms originated from drumming, samba as a music genre has its origins in Rio de Janeiro, the former capital of Brazil.

Samba is recognized around the world as a symbol of Brazil and the Brazilian Carnival. Considered one of the most popular Brazilian cultural expressions, samba has become an icon of Brazilian national identity.
The Bahian Samba de Roda (dance circle), which became a UNESCO Heritage of Humanity in 2005, is the main root of the "samba carioca", the samba that is played and danced in Rio de Janeiro.

The modern samba that emerged at the beginning of the 20th century is predominantly in a 2/4 time signature varied with the conscious use of a sung chorus to a batucada rhythm, with various stanzas of declaratory verses. Traditionally, the samba is played by strings (cavaquinho and various types of guitar) and various percussion instruments such as tamborim. Influenced by American orchestras in vogue since the Second World War and the cultural impact of US music post-war, samba began to use trombones, trumpets, choros, flutes, and clarinets.

In addition to distinct rhythms and meters, samba brings a whole historical culture of food, varied dances (miudinho, coco, samba de roda, and pernada), parties, clothes such as linen shirts, and the Naif painting of established names such as Nelson Sargento, Guilherme de Brito, and Heitor dos Prazeres. Anonymous community artists, including painters, sculptors, designers, and stylists, make the clothes, costumes, carnival floats, and cars, opening the doors of schools of samba. There is also a great tradition of ballroom samba in Brazil, with many styles. Samba de Gafieira is the style more famous in Rio de Janeiro, where common people used to go to the gafieira parties since the 1930s, and where the moves and identity of this dance have emerged, getting more and more different from its African, European and Cuban origins and influences.

The Samba National Day is celebrated on December 2. The date was established at the initiative of Luis Monteiro da Costa, an Alderman of Salvador, in honor of Ary Barroso. He composed ""Na Baixa do Sapateiro"" even though he had never been in Bahia. Thus 2 December marked the first visit of Ary Barroso to Salvador. Initially, this day was celebrated only in Salvador, but eventually it turned into a national holiday.

Samba is a local style in Southeastern Brazil and Northeast Brazil, especially in Rio de Janeiro, São Paulo, Salvador and Recife. Its importance as Brazil's national music transcends region, however; samba schools, samba musicians and carnival organizations centered on the performance of samba exist in every region of the country, even though other musical styles prevail in various regions (for instance, in Southern Brazil, Center-West Brazil, and all of the Brazilian countryside, Sertanejo, or Brazilian country music, is the most popular style).

The etymology of samba is uncertain. Possibilites include:

One of the oldest records of the word samba appeared in Pernambuco magazine's "O Carapuceiro", dated February 1838, when Father Miguel Lopes Gama of Sacramento wrote against what he called "the samba d'almocreve" – not referring to the future musical genre, but a kind of merriment (dance drama) popular for black people of that time. According to Hiram Araújo da Costa, over the centuries, the festival of dances of slaves in Bahia were called samba.

In the middle of the 19th century, the word samba defined different types of music made by African slaves when conducted by different types of Batuque, but it assumed its own characteristics in each Brazilian state, not only by the diversity of tribes for slaves, but also the peculiarity of each region in which they were settlers. Some of these popular dances were known as Baião, Bochinche, Candombe (Candomblé), Catêrêtê, Caxambú, Choradinho, Côco-inchádo, Cocumbí, Córta-jáca, Cururú, Furrundú, Jongo, Lundú, Maracatú, Maxíxe, Quimbête, São-Gonçalo, Saramba; not to mention the many varieties of the Portuguese Fandango, and the Indio dance Puracé.

In Argentina, there is a dance called "Zamba", a name which seems to share etymological origins with the Samba, though the dance itself is quite different.

Samba-enredo or samba de enredo is a subgenre of Samba in which songs are performed by a samba school (or "escola de samba") for the festivities of Brazilian Carnival. "Samba-enredo" translates literally in Portuguese to "samba in song", or "song samba". Each samba school creates a new samba-enredo in advance of the next year's Carnaval, which is selected by competition, to be performed in the final Carnaval parades and events leading up to Carnaval.

For each samba school, choosing the following year's samba-enredo is a long process. Well in advance of the Carnaval parade, each samba school holds contests for writing the song. The song is written by samba composers from within the school itself, ("Ala dos Compositores") or sometimes from outside composers, normally in ""parcerias"(partnerships). Each school receives many—sometimes hundreds—songs, hoping to be the next samba-enredo for that year. The samba-enredo is written by these numerous composers mentioned above only after the Carnival Art Director, or "Carnavalesco" officially publishes the samba school's parade theme synopsis for the year. After a careful explanation of the parade-theme, many times done by the Carnival Art Director himself, composers may ask questions in order to clarify the synopsis, so they could start writing the samba-enredos.
The schools select the song by process of elimination and usually end up somewhere between five and ten songs. Around this time, the finalist samba-enredos are played with music and are voted on by the leaders of the samba school and the "carnavalesco"—the director of the school for Carnaval. After months of deliberation, the new samba-enredo is chosen and becomes the voice of the samba school for the next year's Carnaval. The most important night in this process, is called the "final de samba", or samba final, when the samba school decides between two or three samba-enredos. At the end of the process, the winning samba-enredo is selected, and it is this song that is sung during the school's parade in the sambadrome. This process normally happens in Brazil from August until November, and today is highly professionalized, with samba-composers hiring fans, producing CDs, banners, and throwing parties to promote their samba-enredo.

It is important to note that the samba-enredo is one of the criteria used by the Judging committee to decide who is the winner of the Carnaval parade competition. The samba-enredo must be well sung by the samba school's "puxador" (or singer) or the school will lose points. While the puxador sings, everyone marching in the Carnaval parade sings the samba-enredo along with him, and harmony is another judging criterion.

Although samba exists throughout Brazil – especially in the states of Bahia, Maranhão, Minas Gerais, and São Paulo – in the form of various popular rhythms and dances that originated from the regional batuque of the eastern Brazilian state of Bahia, a music form from Cape Verde, samba is frequently identified as a musical expression of urban Rio de Janeiro, where it developed during the first years of the 20th century. Early styles of samba - and specifically samba de roda - are traced back to the Recôncavo region of Bahia during the 17th century, and the informal dancing following a candomblé ceremony. It was in Rio de Janeiro that the dance practiced by former slaves who migrated from Bahia came into contact with and incorporated other genres played in the city (such as the polka, the maxixe, the lundu, and the xote), acquiring a completely unique character and creating the "samba carioca urbana" (samba school) and "carnavalesco" (Carnaval school director). Samba schools are large organizations of up to 5,000 people which compete annually in the Carnival with thematic floats, elaborate costumes, and original music.

During the first decade of the 20th century, some songs under the name of samba were recorded, but these recordings did not achieve great popularity. However, in 1917, ""Pelo Telefone"" ("Through the Telephone") was recorded, and it is considered the first true samba. The song was claimed to be authored by Ernesto dos Santos, best known as , with co-composition attributed to Mauro de Almeida, a well-known Carnival columnist. Actually, "Pelo Telefone" was created by a collective of musicians who participated in celebrations at the house of Tia Ciata (Aunt Ciata). It was eventually registered by Donga and the Almeida National Library.

""Pelo Telefone"" was the first composition to achieve great success with the style of samba and to contribute to the dissemination and popularization of the genre. From that moment on, samba started to spread across the country, initially associated with Carnival and then developing its own place in the music market. There were many composers, including Heitor dos Prazeres, João da Bahiana, Pixinguinha, and Sinhô, but the sambas of these composers were "amaxixados" (a mix of maxixe), known as sambas-maxixes.

The contours of the modern samba came only at the end of the 1920s, from the innovations of a group of composers of carnival blocks in the neighborhoods of Estácio de Sá and Osvaldo Cruz, and the hills of Mangueira, Salgueiro, and São Carlos. Since then, there have been many great names in samba, such as Ismael Silva, Cartola, Ary Barroso, Noel Rosa, Ataulfo Alves, Wilson Batista, Geraldo Pereira, Zé Kéti, Candeia, Ciro Monteiro, Nelson Cavaquinho, Elton Medeiros, Paulinho da Viola, Martinho da Vila, and many others.

As the samba consolidated as an urban and modern expression, it began to be played on radio stations, spreading across the hills and neighborhoods to the affluent southern areas of Rio de Janeiro. Initially viewed with prejudice and discrimination because it had black roots, the samba, because of its hypnotic rhythms and melodic intonations in addition to its playful lyrics, eventually conquered the white middle class as well. Other musical genres derived from samba, such as samba-canção, partido alto, samba-enredo, samba de gafieira, samba de breque, bossa nova, samba-rock, and pagode, have all earned names for themselves.

The samba is frequently associated abroad with football and Carnival. This history began with the international success of Aquarela do Brasil, by Ary Barroso, followed by Carmen Miranda (supported by Getúlio Vargas government and the US Good Neighbor policy), which led samba to the United States. Bossa nova finally entered the country into the world of samba music. Brazilian percussionist and studio musician Paulinho Da Costa, currently based in Los Angeles, incorporates the rhythms and instrumentation of the samba into the albums of hundreds of American, European and Japanese artists — including producer Quincy Jones, jazz performer Dizzy Gillespie, pop singer Michael Jackson and vocalist Barbra Streisand.

The success of the samba in Europe and Japan only confirms its ability to win fans, regardless of their language. Currently, there are hundreds of samba schools held on European soil and scattered among countries like Germany, Belgium, Netherlands, France, Sweden, and Switzerland. Already in Japan, the records invest heavily in the launch of former Sambista's set of discs, which eventually created a market composed solely of catalogs of Japanese record labels.

From the second half of the 19th century onward, as blacks, mestizas, and ex-soldiers of the War of Canudos in Rio de Janeiro came from various parts of Brazil (mainly Bahia) and settled in the vicinity of Morro da Conceição, Pedra do Sal, Praça Mauá, Praça Onze, Cidade Nova, Saúde, and Zona Portuária. These stands form poor communities that these people called the favelas (later the term became synonymous with the irregular buildings of the poor).

These communities would be the scene of a significant part of Brazilian black culture, particularly with respect to Candomblé and "samba amaxixado" at that time. Among the early highlights were the musician and dancer Hilário Jovino Ferreira—responsible for the founding of several blocks of afoxé and Carnival's ranchos—and "Tias Baianas", a term given to the female descendants of Bahian slaves.

Thus, the samba and musical genre was born in the houses of "Tias Baianas" (Bahian aunts) in the beginning of the 20th century, as a descendant of the style lundu of the candomblé de terreiro parties between umbigada (Samba) and capoeira's pernadas, marked in pandeiro, prato-e-faca (plate-and-knife) and in the "palmas", hand claps. There are some controversies about the word "samba-raiado", one of the first appointments to the samba. It is known that the "samba-raiado" is marked by the sound and accent sertanejos / rural brought by ""Tias Baianas"" to Rio de Janeiro. According to João da Baiana, the "samba-raiado" was the same as "chula raiada" or samba de partido-alto. For the sambist Caninha, this was the first name would have heard at the home of Tia Dadá. At the same time, there were the "samba-corrido", a line that had more work together with the rural Bahian accent, and the samba-chulado, a more rhyming and melodic style that characterized the urban samba carioca.

By the 1870s, Republican propagandists were attempting to prohibit samba on the pretext that folklorist dances shamed Brazil's national image. It would take the edict of a federal administration to halt the persecution of neighborhood samba groups and to recognize officially their parades. Later, the views of anthropologist Gilberto Freyre, and Getrllio Vargas, who became Brazil's new populist president in 1930, provided the country with fresh perspectives on racial mixing. Under Vargas, samba schools and carnaval parades were supported by the state and quickly established themselves all over Brazil. Samba significantly benefited from these political efforts to create a homogeneous national culture. While certain types of music suggested different racial or class origins, samba dissipated social antagonisms and helped unify a society that varied in its origins, appearance, and ways of living and thinking. Samba's triumph over the airwaves allowed it to penetrate all sectors of Brazilian society.

According to anthropologist Hermano Vianna, configuring Samba as a symbol of Brazilianness was possible thanks to the cultural exchange between the working classes and intellectual elite. He cites a guitar meeting between anthropologist Gilberto Freyre, the historian Sérgio Buarque de Holanda, promoter and journalist Prudente de Moraes Neto, the classical composer Villa Lobos and pianist Lucio Gallet, all representative of the intellectual and cultural elite of white origin on the one hand; and Pixinguinha musician and composers / samba Donga and Patrick Teixeira, from the popular and crossbred layers on the other, saying how the occasion marked the meeting of two different or even opposing groups of Brazilian society.

The urban carioca samba is the anchor of 20th century ""Brazilian samba"" par excellence. However, before this type of samba was to consolidate as the ""national samba"" in Brazil, there were traditional forms of sambas in Bahia and São Paulo.

The rural Bahia samba acquired additional names as choreographic variations – for example, the ""samba-de-chave"", where the soloist dancer faking looking "roda" in the middle of a key, and when found, was replaced. The poetic structure of Bahian samba followed the way call-and-response—composed of a single verse, a solo, followed by another, and repeated by the chorus of dancers as the falderal. With no chorus, the samba is called "samba-corrido", which is an uncommon variant. The chants were taken by one singer, one of the musicians, or soloist dancer. Another peculiarity of Bahian samba was a form of competition that dances sometimes presented: it was a dispute between participants to see who performed better. Besides the umbigada, common to all the bahianian samba, the Bahia presented three basic steps: "corta-a-jaca", "separa-o-visgo", and "apanha-o-bag". There is also another choreographic element danced by women: the "miudinho" (this also appeared in São Paulo, as dance solo in the center of the "roda"). The instruments of the Bahian samba were pandeiros, shakers, guitars, and sometimes the castanets and berimbaus.

In São Paulo state, samba became the domain of blacks and caboclos. In rural areas, samba can occur without the traditional umbigada. There are also other choreographic variations—the dancers may be placed in rows with men on one side and women on another. The instruments of the samba paulista were violas and pandeiros. It is possible that the early provision of the "roda", in Goiás, has been modified by the influence of quadrilha or cateretê. According to historian Luís da Câmara Cascudo, it is possible to observe the influence of city in the samba, by the fact that it is also danced by pair connections.

One of the most noticeable groups of São Paulo's samba, Demônios da Garoa (Drizzle's Demons), had a strong link with Adoniran Barbosa, who composed their songs. Songs like "Samba do Arnesto" and "Saudosa Maloca" became legendary, recognized as "the real Samba Paulistano". The group is still active, but with a different lineup. In 2000, one of their most famous songs, "Trem das Onze", was elected as an official symbol of the city of São Paulo.

Tia Ciata, grandmother of the composer Bucy Moreira, was responsible for the sedimentation of samba carioca. According to the folklore of that time, for a samba musician to achieve success, he would have to pass the house of Tia Ciata and be approved on the "rodas de samba". Many compositions were created and sung in improvisation, where the samba "Pelo Telefone" (from Donga and Mauro de Almeida), samba for which there were also many other versions, but to come to the history of samba, Pelo Telefone was the first recorded Samba, in 1917

Meanwhile, other recordings have been done as samba before "Pelo Telefone", as this composition was done by double Donga / Mauro de Almeida, who is regarded as a founder of the genre. Still, the song is written and discussed, and its proximity to the maxixe made it finally be designated samba-maxixe. This section was influenced by maxixe dance and basically played the piano—unlike the Rio samba played in the Morros hills—and the composer has musician Sinhô, self-titled "o rei do samba" ("the king of Samba") which with other pioneers such as Heitor dos Prazeres and Caninha, lay the first foundations of the musical genre.

The growing shantytowns in the hills of suburban Rio would become the home of new musical talents. Almost simultaneously, the "samba carioca", which was born in the city center, would climb the slopes of the hills and spread outside the periphery, to the point that, over time, it came to be identified as "samba de morro" (samba from the hills).

At the end of the 1920s, the carnival samba of blocks of the districts Estácio de Sá and Osvaldo Cruz was born, and in the hills of Mangueira, Salgueiro, and São Carlos, there were innovations in rhythmic samba that persist until the present day. This group, the "Turma do Estácio", from which would arise "Deixa Falar", was the first samba school in Brazil. Formed by some composers in the neighborhood of Estácio, including Alcebíades Barcellos (aka Bide) Armando Marçal, Ismael Silva, Nilton Bastos and the more "malandros" such as Baiaco, Brancura, Mano Edgar, Mano Rubem, the ""Turma do Estácio"" marked the history of the Brazilian samba by injecting more pace to the genre one performed, which has the endorsement of the youth's middle class, as the ex-student of law Ary Barroso and former student of medicine Noel Rosa.

Initially a "rancho carnavalesco", then a Carnival's Block, and finally a samba school, the "Deixa Falar" was the first to Rio Carnival parade in the sound of an orchestra made up of percussion surdos, tambourines, and cuícas, who joined pandeiro and shakers. This group was instrumental and is called "bateria", and it lends itself to the monitoring of a type of samba that was quite different from those of Donga, Sinhô, and Pixinguinha. The samba of Estácio de Sá signed up quickly as the samba carioca par excellence.

The "Turma do Estácio" has made the appropriate rhythmic samba were so it could be accompanied in the carnival's parade, thus distancing the progress "samba-amaxixado" of composers such as Sinhô. Moreover, its "rodas" of samba were attended by composers from other Rio hills, as Cartola, Carlos Cachaça, and then Nelson Cavaquinho, e Geraldo Pereira, Paulo da Portela, Alcides Malandro Histórico, Manacéia, Chico Santana, and others. Accompanied by a pandeiro, a tambourine, a cuíca and a surdo, they created and spread the samba-de-morro.

After the founding of ""Deixa Falar"", the phenomenon of the samba schools took over the scene and helped boost Rio's samba subgenera of Partido Alto, singing and challenging in "candomblé terreiros" the samba-enredo.

From the 1930s, the popularization of radio in Brazil helped to spread the samba across the country, mainly the subgenres "samba-canção" and "samba-exaltação". The "samba-canção" was released in 1928 with the recording "Ai, yo-yo" by Aracy Cortes. Also known as "samba half of the year", the "samba-canção" became established in the next decade. It was a slow and rhythmic samba music and had an emphasis on melody and generally easy acceptance. This aspect was later influenced by the rhythms of foreigners, first by foxtrot in the 1940s and then bolero the 1950s. The most famous composers were Noel Rosa, Ary Barroso, Lamartine Babo, Braguinha (also known as João de Barro), and Ataulfo Alves. Other practitioners of this style were Antonio Maria, Custódio Mesquita, Dolores Duran, Fernando Lobo, Ismael Neto, Lupicínio Rodrigues, Batatinha, and Adoniran Barbosa (this latter by sharply satirical doses).

The ideology of Getúlio Vargas's Estado Novo changed the scene of the samba. With "Aquarela do Brasil", composed by Ary Barroso and recorded by Francisco Alves in 1939, the "samba-exaltação" become the first success abroad. This kind of samba was characterized by extensive compositions of melody and patriotic verses. Carmen Miranda popularized samba internationally through her Hollywood films.

With the support of the Brazilian president Getúlio Vargas, the samba won status as the "official music" of Brazil. With this status of national identity came the recognition of the intellectual and classical composer Heitor Villa-Lobos, who arranged a recording with the maestro Leopold Stokowski in 1940, which involved Cartola, Donga, João da Baiana, Pixinguinha, and Zé da Zilda.

Also in the 1940s, there arose a new crop of artists: Francisco Alves, Mário Reis, Orlando Silva, Silvio Caldas, Aracy de Almeida, Dalva de Oliveira, and Elizeth Cardoso, among others. Others such as Assis Valente, Ataulfo Alves, Dorival Caymmi, Herivelto Martins, Pedro Caetano, and Synval Silva led the samba to the music industry.

A movement was born in the southern area of Rio de Janeiro, strongly influenced by jazz, marking the history of samba and Brazilian popular music in the 1950s. The bossa nova emerged at the end of that decade, with an original rhythmic accent which divided the phrasing of the samba and added influences of impressionist music and jazz and a different style of singing which was both intimate and gentle. After precursors such as Johnny Alf, João Donato, and musicians like Luis Bonfá and Garoto, this subgenre was inaugurated by João Gilberto, Tom Jobim, and Vinicius de Moraes. It then had a generation of disciples and followers including Carlos Lyra, Roberto Menescal, Durval Ferreira, and groups like Tamba Trio, Bossa 3, Zimbo Trio, and The Cariocas.

The sambalanço also began at the end of the 1950s. It was a branch of the popular bossa nova (most appreciated by the middle class) which also mingled samba rhythms and American jazz. Sambalanço was often found at suburban dances of the 1960s, 1970s, and 1980s. This style was developed by artists such as Bebeto, Bedeu, Scotland 7, Djalma Ferreira, the Daydreams, Dhema, Ed Lincoln, Elza Soares, and Miltinho, among others. In the 21st century, groups like Funk Como Le Gusta and Clube do Balanço continue to keep this subgenre alive.

In the 1960s, Brazil became politically divided with the arrival of a military dictatorship, and the leftist musicians of bossa nova started to gather attention to the music made in the "favelas". Many popular artists were discovered at this time. Musicians like Cartola, Nelson Cavaquinho, Guilherme de Brito, Velha Guarda da Portela, Zé Keti, and Clementina de Jesus recorded their first albums during this time.

In the 1970s, samba returned strongly to the air waves with composers and singers like Paulinho da Viola, Martinho da Vila, Clara Nunes, and Beth Carvalho dominating the hit parade. Great samba lyricists like Paulo César Pinheiro (especially in the praised partnership with João Nogueira) and Aldir Blanc started to appear around that time.

With bossa nova, samba is further away from its popular roots. The influence of jazz deepened, and techniques have been incorporated from classical music. From a festival in Carnegie Hall of New York, in 1962, the bossa nova reached worldwide success. But over the 1960s and 1970s, many artists who emerged—like Chico Buarque, Billy Blanco, Martinho da Vila, and Paulinho da Viola—advocated the return of the samba beat in its traditional form. They also wanted veterans like Candeia, Cartola, Nelson Cavaquinho, and Zé Kéti to return. In the early 1960s, the "Movement for Revitalization of Traditional Samba", promoted by Center for Popular Culture, started in partnership with the Brazilian National Union of Students. During the 1960s, some samba groups appeared and were formed by previous experiences with the world of samba and songs recorded by great names of Brazilian music. Among them were The Cinco Crioulos, The Voz do Morro, Mensageiros do Samba, and The Cinco Só.

Outside the main scene of the Brazilian Popular Music festivals, the sambists founded the Bienal do Samba in the late 1960s, and it became the space for the big names of the genre and followers. Even in the final decade, the "samba-empolgação" (samba-excitement) of carnival blocks "Bafo da Onça", "Cacique de Ramos," and "Boêmios de Irajá" came into being.

Also in the 1960s came the samba funk. The samba-funk emerged at the end of the 1960s with pianist Dom Salvador and his group, which merged the samba with American funk, which was then newly arrived in the Brazil. With the departure of Dom Salvador to the United States, the band broke up, but at the beginning of the 1970s, some ex-members, including Luiz Carlos, José Carlos Barroso, and Oberdan joined Christovao Bastos, Jamil Joanes, Cláudio Stevenson and Lúcio da Silva to form Banda Black Rio. The new group has deepened the work of Don Salvador in the double mixture of the bar with the Brazilian samba funk of the American Quaternary, based on the dynamics of implementation, driven by drums and bass. Even after the Banda Black Rio in the 1980s, British disc jockeys began to play the group's work. It was rediscovered throughout Europe, but mainly in UK and Germany.

At the turn of the 1960s to the 1970s, the young Martinho da Vila would give a new face to the traditional sambas-enredo established by authors such as Silas de Oliveira and Mano Decio da Viola, compressing them and expanding its potential in the music market. Martin popularized the style of the Partido alto with songs like "Casa de Bamba" and "Pequeno Burguês" and launched his first album in 1969.

Although the term "partido alto" originally arose at the beginning of the 1900s to describe instrumental music, the term came to be used to signify a type of samba which is characterized by a highly percussive beat of pandeiro, using the palm of the hand in the center of the instrument in place. The harmony of Partido alto is always higher in pitch, usually played by a set of percussion instruments (usually surdo, pandeiro, and tambourine) and accompanied by a cavaquinho and/or a guitar.

Also in that decade, some popular singers and composers appeared in the samba, including Alcione, Beth Carvalho, and Clara Nunes. As highlighted in city of São Paulo, Geraldo Filme was one of the leading names in samba paulistano, next to Germano Mathias, Osvaldinho of Cuíca, Tobias da Vai-Vai, Aldo Bueno, and Adoniran Barbosa.

In the early 1980s, after having been eclipsed by the popularity of disco and Brazilian rock, Samba reappeared in the media with a musical movement created in the suburbs of Rio de Janeiro. It was the "pagode", a renewed samba, with new instruments like the banjo and the tan-tan. It also had a new language that reflected the way that many people actually spoke by including heavy "gíria", or slang. The most popular artists were Zeca Pagodinho, Almir Guineto, Grupo Fundo de Quintal, Jorge Aragão, and Jovelina Pérola Negra.<ref name="cliquemusic/pagode"> (</ref>

In 1995, the world saw one of the most popular Pagode groups, the Gera Samba, later renamed to "É o Tchan", come out from Savador. This group created the most sexual dance of the Pagode during the 1990s, mixing a lot of Axé music in it. Some groups like Patrulha do Samba and Harmonia do Samba, also mixed in a bit of Axé. Samba, as a result, morphed during this period, embracing types of music that were growing popular in the Caribbean such as rap, reggae, and rock. Examples of Samba fusions with popular Caribbean music is samba-rap, samba-rock, and samba-reggae, all of which were efforts to not only entertain, but also to unify all Blacks throughout the Americas culturally and politically via song. In other words, samba-rap and the like often carried lyrics that encouraged Black pride, and spoke out against social injustice. Samba, however, is not accepted by all as the national music of Brazil, or as a valuable art form. Light-skinned "upper-class" Brazilians often associated Samba with dark-skinned blacks because of its arrival from West Africa. As a result, there are some light-skinned Brazilians who claim that samba is the music of low-class, dark-skinned Brazilians and, therefore, is a "thing of bums and bandits".

Samba continued to act as a unifying agent during the 1990s, when Rio stood as a national Brazilian symbol. Even though it was not the capital city, Rio acted as a Brazilian unifier, and the fact that samba originated in Rio helped the unification process. In 1994, the FIFA World Cup had its own samba composed for the occasion, the "Copa 94". The 1994 FIFA World Cup, in which samba played a major cultural role, holds the record for highest attendance in World Cup history. Samba is thought to be able to unify because individuals participate in it regardless of social or ethnic group. Today, samba is viewed as perhaps the only uniting factor in a country fragmented by political division.

The Afro-Brazilians played a significant role in the development of the samba over time. This change in the samba was an integral part of Brazilian nationalism, which was referred to as "Brazilianism".

"What appears to be new is the local response to that flow, in that instead of simply assimilating outside influences into a local genre or movement, the presence of foreign genres is acknowledged
as part of the local scene: samba-rock, samba-reggae, samba-rap.
But this acknowledgment does not imply mere imitation of the foreign
models or, for that matter, passive consumption by national audiences." – Gerard Béhague, "Selected Reports in Ethnomusicology." Pg. 84

From the year 2000 onwards, there were some artists who were looking to reconnect the most popular traditions of samba. The cases of Marquinhos of Oswaldo Cruz and Teresa Cristina, were, among others, the ones that contributed to the revitalization of the region of Lapa in Rio de Janeiro. In São Paulo, samba resumed the tradition with concerts in Sesc Pompéia Club and with the work of several groups, including the group Quinteto em Branco e Preto which developed the event "Pagode da Vela" ("Pagoda of Sail"). These all helped to attract many artists from Rio de Janeiro, which has established residence in neighborhoods of the capital paulistana.

Samba was also mixed with drum and bass leading to the foundation of Sambass. Despite the evolution during the decades, Samba still remains a traditional dance, and cannot be considered a sport.

In 2004, the minister of culture Gilberto Gil submitted to Unesco an application for declaring samba as a Cultural Heritage of Humanity in the category "Intangible Goods" by the Institute of National Historical and Artistic Heritage. In 2005 the samba-de-roda of Baiano Recôncavo was proclaimed part of the Heritage of Humanity by Unesco, in the category of "Oral and intangible expressions". The Samba is often performed on different dance shows, such as Strictly Come Dancing.





</doc>
<doc id="28262" url="https://en.wikipedia.org/wiki?curid=28262" title="Snowboard">
Snowboard

Snowboards are boards where both feet are secured to the same board, which are wider than skis, with the ability to glide on snow. Snowboards widths are between 6 and 12 inches or 15 to 30 centimeters. Snowboards are differentiated from monoskis by the stance of the user. In monoskiing, the user stands with feet inline with direction of travel (facing tip of monoski/downhill) (parallel to long axis of board), whereas in snowboarding, users stand with feet transverse (more or less) to the longitude of the board. Users of such equipment may be referred to as "snowboarder"s. "Commercial snowboards" generally require extra equipment such as bindings and special boots which help secure both feet of a snowboarder, who generally rides in an upright position. These types of boards are commonly used by people at ski hills or resorts for leisure, entertainment, and competitive purposes in the activity called snowboarding.

In 1939, Vern Wicklund, at the age of 13, fashioned a shred deck in Cloquet, Minnesota. This modified sled was dubbed a “bunker" by Vern and his friends. He, along with relatives Harvey and Gunnar Burgeson, patented the very first snowboard twenty two years later.

However, a man by the name of Sherman Poppen, from Muskegon, MI, came up with what most consider the first "snowboard" in 1965 and was called the Snurfer (a blend of "snow" and "surfer") who sold his first 4 "snurfers" to Randall Baldwin Lee of Muskegon, MI who worked at Outdoorsman Sports Center 605 Ottawa Street in Muskegon, MI (owned by Justin and Richard Frey or Muskegon). Randy believes that Sherman took an old water ski and made it into the snurfer for his children who were bored in the winter. He added bindings to keep their boots secure. (Randy Lee, October 14, 2014) Commercially available Snurfers in the late 1960s and early 1970s had no bindings. The snowboarder held onto a looped nylon lanyard attached to the front of the Snurfer, and stood upon several rows of square U-shaped staples that were partially driven into the board but protruded about 1 cm above the board's surface to provide traction even when packed with snow. Later Snurfer models replaced the staples with ridged rubber grips running longitudinally along the length of the board (originally) or, subsequently, as subrectangular pads upon which the snowboarder would stand. It is widely accepted that Jake Burton Carpenter (founder of Burton Snowboards) and/or Tom Sims (founder of Sims Snowboards) invented modern snowboarding by introducing bindings and steel edges to snowboards.

In 1981, a couple of Winterstick team riders went to France at the invitation of Alain Gaimard, marketing director at Les Arcs. After seeing an early film of this event, French skiers/surfers Augustin Coppey, Olivier Lehaneur, Olivier Roland and Antoine Yarmola made their first successful attempts during the winter of 1983 in France (Val Thorens), using primitive, home-made clones of the Winterstick. Starting with pure powder, skateboard-shaped wooden-boards equipped with aluminium fins, foot-straps and leashes, their technology evolved within a few years to pressed wood/fiber composite boards fitted with polyethylene soles, steel edges and modified ski boot shells. These were more suitable for the mixed conditions encountered while snowboarding mainly off-piste, but having to get back to ski lifts on packed snow.
In 1985, James Bond popularized snowboarding in the movie "A View to a Kill". In the scene, he escapes Soviet agents who are on skis. The snowboard he used was from the debris of a snowmobile that exploded.

At the same time the Snurfer was turning into a snowboard on the other side of the iron curtain.
In 1980, Aleksey Ostatnigrosh and Alexei Melnikov - two members of the only Snurfer club in the Soviet Union started changing the Snurfer design to allow jumping and to improve control on hard packed snow. Being completely unaware of the developments in the Snurfer/snowboard world, they attached a bungee cord to the Snurfer tail which the rider could grab before jumping. Later, in 1982, they attached a foot binding to the Snurfer. The binding was only for the back foot, and had a release capability.
In 1985, after several iterations of the Snurfer binding system, Aleksey Ostatnigrosh made the first Russian snowboard. The board was cut out of a single vinyl plastic sheet and had no metal edges. The bindings were attached by a central bolt and could rotate while on the move or be fixed at any angle.
In 1988, OstatniGROsh and MELnikov started the first Russian snowboard manufacturing company, GROMEL.

By 1986, although still very much a minority sport, commercial snowboards started appearing in leading French ski resorts.

In 2008, selling snowboarding equipment was a $487 million industry. In 2008, average equipment ran about $540 including board, boots, and bindings.

The bottom or 'base' of the snowboard is generally made of UHMW and is surrounded by a thin strip of steel, known as the 'edge'. Artwork was primarily printed on PBT using a sublimation process in the 1990s, but poor color retention and fade after moderate use moved high-end producers to longer-lasting materials.

Snowboards come in several different styles, depending on the type of riding intended:

Snowboards are generally constructed of a hardwood core which is sandwiched between multiple layers of fibreglass. Some snowboards incorporate the use of more exotic materials such as carbon fiber, Kevlar, aluminium (as a honeycomb core structure), and have incorporated piezo dampers. The front (or "nose") of the board is upturned to help the board glide over uneven snow. The back (or "tail") of the board is also upturned to enable backwards (or "switch") riding. The base (the side of the board which contacts the ground) is made of Polyethylene plastic. The two major types of base construction are extruded and sintered. An extruded base is a basic, low-maintenance design which basically consists of the plastic base material melted into its form. A sintered base uses the same material as an extruded base, but first grinds the material into a powder, then, using heat and pressure, molds the material into its desired form. A sintered base is generally softer than its extruded counterpart, but has a porous structure which enables it to absorb wax. This wax absorption (along with a properly done 'hot wax'), greatly reduces surface friction between the base and the snow, allowing the snowboard to travel on a thin layer of water. Snowboards with sintered bases are much faster, but require semi-regular maintenance and are easier to damage. The bottom edges of the snowboard are fitted with a thin strip of steel, just a couple of millimeters wide. This steel edge allows the board to grab or 'dig into' hard snow and ice (like the blade of an ice skate), and also protects the boards internal structure. The top of the board is typically a layer of acrylic with some form of graphic designed to attract attention, showcase artwork, or serve the purpose similar to that of any other form of printed media. Flite Snowboards, an early designer, pressed the first closed-molded boards from a garage in Newport, Rhode Island, in the mid-1980s. Snowboard topsheet graphics can be a highly personal statement and many riders spend many hours customizing the look of their boards. The top of some boards may even include thin inlays with other materials, and some are made entirely of epoxy-impregnated wood. The base of the board may also feature graphics, often designed in a manner to make the board's manufacturer recognizable in photos.

Snowboard designs differ primarily in:

The various components of a snowboard are:




Amongst Climate Change, the winter sports community is a growing environmentalist group, whom depend on snowy winters for the survival of their culture. This movement is, in part, being energized by a nonprofit named "Protect Our Winters" and the legendary rider Jeremy Jones. The organization provides education initiatives, support for community based projects, and is active in climate discussions with the government. Alongside this organization, there are many other winter sports companies who see the ensuing calamity and are striving to produce products that are less damaging to the environment. Snowboard manufacturers are adapting to decreasing supplies of petroleum and timber with ingenious designs.


When it comes down to it "the least of our worries will be that skiers and snowboarders don't get to go play," says Jeremy Jones.

Snowboard boots are mostly considered soft boots, though alpine snowboarding uses a harder boot similar to a ski boot. A boot's primary function is to transfer the rider's energy into the board, protect the rider with support, and keep the rider's feet warm. A snowboarder shopping for boots is usually looking for a good fit, flex, and looks. Boots can have different features such as lacing styles, heat molding liners, and gel padding that the snowboarder also might be looking for. Tradeoffs include rigidity versus comfort, and built in forward lean, versus comfort.

There are three incompatible types:

Bindings are separate components from the snowboard deck and are very important parts of the total snowboard interface. The bindings' main function is to hold the rider's boot in place tightly to transfer their energy to the board. Most bindings are attached to the board with three or four screws that are placed in the center of the binding. Although a rather new technology from Burton called Infinite channel system uses two screws, both on the outsides of the binding.

There are several types of bindings. Strap-in, step-in, and hybrid bindings are used by most recreational riders and all freestyle riders.

These are the most popular bindings in snowboarding. Before snowboard specific boots existed, snowboarders used any means necessary to attach their feet to their snowboards and gain the leverage needed for turning. Typical boots used in these early days of snowboarding were Sorels or snowmobile boots. These boots were not designed for snowboarding and did not provide the support desired for doing turns on the heel edge of a snowboard. As a result, early innovators such as Louis Fournier conceived the "high-back" binding design which was later commercialized and patented by Jeff Grell. The highback binding is the technology produced by most binding equipment manufacturers in the snowboard industry. The leverage provided by highbacks greatly improved board control. Snowboarders such as Craig Kelly adapted plastic "tongues" to their boots to provide the same support for toe-side turns that the highback provided for heel-side turns. In response, companies such as Burton and Gnu began to offer "tongues".

With modern strap bindings, the rider wears a boot which has a thick but flexible sole, and padded uppers. The foot is held onto the board with two buckle straps – one strapped across the top of the toe area, and one across the ankle area. They can be tightly ratcheted closed for a tight fit and good rider control of the board. Straps are typically padded to more evenly distribute pressure across the foot. While nowhere near as popular as two-strap bindings, some people prefer three-strap bindings for more specialized riding such as carving. The third strap tends to provide additional stiffness to the binding.

Cap-strap bindings are a recent modification that provide a very tight fit to the toe of the boot, and seats the boot more securely in the binding. Numerous companies have adopted various versions of the cap strap.

Innovators of step-in systems produced prototypes and designed proprietary step-in boot and binding systems with the goal of improving the performance of snowboard boots and bindings, and as a result, the mid-90s saw an explosion of step-in binding and boot development. New companies, Switch and Device, were built on new step-in binding technology. Existing companies Shimano, K2 and Emery were also quick to market with new step-in technology. Meanwhile, early market leaders Burton and Sims were noticeably absent from the step-in market. Sims was the first established industry leader to market with a step-in binding. Sims licensed a step-in system called DNR which was produced by the established ski-binding company Marker. Marker never improved the product which was eventually discontinued. Sims never re-entered the step-in market.

The risk of commercial failure from a poorly performing Step-in binding presented serious risk to established market leaders. This was evidenced by Airwalk who enjoyed 30% market share in snowboard boot sales when they began development of their step-in binding system. The Airwalk step-in System experienced serious product failure at the first dealer demonstrations, seriously damaging the company's credibility and heralded a decline in the company's former position as the market leader in Snowboard boots. Established snowboarding brands seeking to gain market share while reducing risk, purchased proven step-in innovators. For example, snowboard boot company Vans purchased the Switch step-in company, while Device step-in company was purchased by Ride Snowboards.

Although initially refusing to expose themselves to the risk and expense associated with bringing a step-in system to market, Burton chose to focus primarily on improvements to existing strap-in technology. However, Burton eventually released 2 models of step-in systems, the SI and the PSI, Burton's SI system enjoyed moderate success, yet never matched the performance of the company's strap-in products and was never improved upon. Burton never marketed any improvements to either of their step-in binding systems and eventually discontinued the products.

Most Popular (and incompatible) step-in systems used unique and proprietary mechanisms, such as the step-ins produced by Burton, Rossignol and Switch. Shimano and K2 used a technology similar to clipless bicycle pedals. Burton and K2 Clicker step-in binding systems are no longer in production as both companies have opted to focus on the strap-in binding system. Rossignol remains as the sole provider of step-in binding systems and offers them primarily to the rental market as most consumers and retailers alike have been discouraged by lack of adequate development and industry support for step-in technology.

There are also proprietary systems that seek to combine the convenience of step-in systems with the control levels attainable with strap-ins. An example is the Flow binding system, which is similar to a strap-in binding, except that the foot enters the binding through the back. The back flips down and allows the boot to slide in; it's then flipped up and locked into place with a clamp, eliminating the need to loosen and then re-tighten straps every time the rider frees and then re-secures their rear foot. The rider's boot is held down by an adjustable webbing that covers most of the foot. Newer Flow models have connected straps in place of the webbing found on older models; these straps are also micro adjustable. In 2004, K2 released the Cinch series, a similar rear-entry binding; riders slip their foot in as they would a Flow binding, however rather than webbing, the foot is held down by straps.

A stiff molded support behind the heel and up the calf area. The HyBak was originally designed by inventor Jeff Grell and built by Flite Snowboards. This allows the rider to apply pressure and effect a "heelside" turn. Some high backs are stiff vertically but provide some flex for twisting of the riders legs.

Plate bindings are used with hardboots on Alpine or racing snowboards. Extreme carvers and some Boarder Cross racers also use plate bindings. The stiff bindings and boots give much more control over the board and allow the board to be carved much more easily than with softer bindings. Alpine snowboards tend to be longer and thinner with a much stiffer flex for greater edge hold and better carving performance.

Snowboard bindings, unlike ski bindings, do not automatically release upon impact or after falling over. With skis, this mechanism is designed to protect from injuries (particularly to the knee) caused by skis torn in different directions. Automatic release is not required in snowboarding, as the rider's legs are fixed in a static position and twisting of the knee joint cannot occur to the same extent. Furthermore, it reduces the dangerous prospect of a board hurtling downhill riderless, and the rider slipping downhill on his back with no means to maintain grip on a steep slope. Nevertheless, some ski areas require the use of a "leash" that connects the snowboard to the rider's leg or boot, in case the snowboard manages to get away from its rider. This is most likely to happen when the rider removes the board at the top or the bottom of a run (or while on a chairlift, which could be dangerous).

A Noboard is a snowboard binding alternative with only peel and stick pads applied directly to any snowboard deck and no attachment.

Stomp pads, which are placed between the bindings closer to the rear binding, allow the rider to better control the board with only one boot strapped in, such as when maneuvering onto a chair lift, riding a ski tow or performing a one footed trick. Whereas the upper surface of the board is smooth, the stomp pad has a textured pattern which provides grip to the underside of the boot. Stomp pads can be decorative and vary in their size, shape and the kind and number of small spikes or friction points they provide.

There are two types of stance-direction used by snowboarders. A "regular" stance places the rider's left foot at the front of the snowboard. "Goofy", the opposite stance direction, places the rider's right foot at the front, as in skateboarding. Regular is the most common. There are different ways to determine whether a rider is "regular" or "goofy". One method used for first time riders is to observe the first step forward when walking or climbing up stairs. The first foot forward would be the foot set up at the front of the snowboard. Another method used for first time riders is to use the same foot that you kick a football with as your back foot (though this can be an inaccurate sign for some, as there are people who prefer goofy though are right handed, and therefore naturally kick a football with their right foot). This is a good method for setting up the snowboard stance for a new snowboarder. However having a surfing or skateboarding background will also help a person determine their preferred stance, although not all riders will have the same stance skateboarding and snowboarding. Another way to determine a rider's stance is to get the rider to run and slide on a tiled or wooden floor, wearing only socks, and observe which foot the person puts forward during the slide. This simulates the motion of riding a snowboard and exposes that persons natural tendency to put a particular foot forward. Another method is to stand behind the first-timer and give them a shove, enough for them to put one foot forward to stop themselves from falling. Other good ways of determining which way you ride are rushing a door (leading shoulder equals leading foot) or going into a defensive boxing stance (see which foot goes forward).

Most experienced riders are able to ride in the opposite direction to their usual stance (i.e. a "regular" rider would lead with their right foot instead of their left foot). This is called riding "fakie" or "switch".

Stance width helps determine the rider's balance on the board. The size of the rider is an important factor as well as the style of their riding when determining a proper stance width. A common measurement used for new riders is to position the bindings so that the feet are placed a little wider than shoulder width apart. Another, less orthodox form of measurement may be taken by putting your feet together and place your hands, palm down, on the ground in a straight line with your body by squatting down. This generally gives a good natural measurement for how wide of a base your body uses to properly balance itself when knees are bent. However, personal preference and comfort are important and most experienced riders will adjust the stance width to personal preference. Skateboarders should find that their snowboarding and skateboarding stance widths are relatively similar.

A wider stance, common for freestyle riders, gives more stability when landing a jump or jibbing a rail. Control in a wider stance is reduced when turning on the piste. Conversely a narrow stance will give the rider more control when turning on the piste but less stability when freestyling. A narrow stance is more common for riders looking for quicker turn edge-hold (i.e. small radius turns). The narrow stance will give the rider a concentrated stability between the bindings allowing the board to dig into the snow quicker than a wider stance so the rider is less prone to wash out.

Binding angle is defined by the degrees off of perpendicular from the length of the snowboard. A binding angle of 0° is when the foot is perpendicular to the length of the snowboard. Positive angles are pointed towards the front of the board, whereas negative angles are pointed towards the back of the board. The question of "how much" the bindings are angled depends on the rider's purpose and preference. Different binding angles can be used for different types of snowboarding. Someone who participates in freestyle competition would have a much different "stance" than someone who explores backcountry and powder. The recent advancement and boom of snowboard culture and technology has made binding angle adjustments relatively easy. Binding companies design their bindings with similar baseplates that can easily mount onto any type of snowboard regardless of the brand. With the exception of Burton, and their newly released "channel system", adjusting bindings is something that remains constant among all snowboarders. Done with a small screw-driver or a snowboard tool, the base plates on bindings can be easily rotated to whatever preferred stance. One must un-screw the baseplate, pick their degree angles, and then re-screw the baseplates. Bindings should also regularly be checked to ensure that the screws don't come undone from the movements of snowboarding. 







</doc>
<doc id="28263" url="https://en.wikipedia.org/wiki?curid=28263" title="Stanza">
Stanza

In poetry, a stanza (; from Italian "stanza" , "room") is a grouped set of lines within a poem, usually set off from other stanzas by a blank line or indentation. Stanzas can have regular rhyme and metrical schemes, though stanzas are not strictly required to have either. Even though the term "stanza" is taken from Italian, in the Italian language the word "strofa" is more commonly used. There are many unique . Some stanzaic forms are simple, such as four-line quatrains. Other forms are more complex, such as the Spenserian stanza. Fixed verse poems, such as sestinas, can be defined by the number and form of their stanzas. The term "stanza" is similar to "strophe", though strophe sometimes refers to irregular set of lines, as opposed to regular, rhymed stanzas.

The stanza in poetry is analogous with the paragraph that is seen in prose; related thoughts are grouped into units. In music, groups of lines are typically referred to as "verses". The stanza has also been known by terms such as "batch", "fit", and "stave".

This short poem by Emily Dickinson has two stanzas of four lines each.
<poem>I had no time to hate, because
The grave would hinder me,
And life was not so ample I
Could finish enmity.

Nor had I time to love; but since
Some industry must be, 
The little toil of love, I thought,
Was large enough for me.</poem>

This poem by Andrew John Young has three stanzas of six lines each
<poem>Frost called to the water Halt 
And crusted the moist snow with sparkling salt;
Brooks, their one bridges, stop, 
And icicles in long stalactites drop. 
And tench in water-holes 
Lurk under gluey glass like fish in bowls.

In the hard-rutted lane 
At every footstep breaks a brittle pane, 
And tinkling trees ice-bound, 
Changed into weeping willows, sweep the ground; 
Dead boughs take root in ponds 
And ferns on windows shoot their ghostly fronds.

But vainly the fierce frost 
Interns poor fish, ranks trees in an armed host, 
Hangs daggers from house-eaves 
And on the windows ferny ambush weaves; 
In the long war grown warmer 
The sun will strike him dead and strip his armour.</poem>


</doc>
<doc id="28265" url="https://en.wikipedia.org/wiki?curid=28265" title="Spanish–American War">
Spanish–American War

The Spanish–American War ( or ""; ) was fought between the United States and Spain in 1898. Hostilities began in the aftermath of the internal explosion of the USS "Maine" in Havana Harbor in Cuba, leading to US intervention in the Cuban War of Independence. American acquisition of Spain's Pacific possessions led to its involvement in the Philippine Revolution and ultimately in the Philippine–American War.

The main issue was Cuban independence. Revolts had been occurring for some years in Cuba against Spanish rule. The US later backed these revolts upon entering the Spanish–American War. There had been war scares before, as in the Virginius Affair in 1873, but in the late 1890s, US public opinion was agitated by anti-Spanish propaganda led by newspaper publishers such as Joseph Pulitzer and William Randolph Hearst which used yellow journalism to call for war. The business community across the United States had just recovered from a deep depression and feared that a war would reverse the gains. It lobbied vigorously against going to war.

The United States Navy armoured cruiser had mysteriously sunk in Havana Harbor; political pressures from the Democratic Party pushed the administration of Republican President William McKinley into a war that he had wished to avoid.

President McKinley signed a joint Congressional resolution demanding Spanish withdrawal and authorizing the President to use military force to help Cuba gain independence on April 20, 1898. In response, Spain severed diplomatic relations with the United States on April 21. On the same day, the US Navy began a blockade of Cuba. On April 23, Spain stated that it would declare war if the US forces invaded its territory. On April 25, Congress declared that a state of war between the US and Spain and had de facto existed since April 21, the day the blockade of Cuba had begun. The United States sent an ultimatum to Spain demanding that it surrender control of Cuba, but due to Spain not replying soon enough, the United States assumed Spain had ignored the ultimatum and continued to occupy Cuba.

The ten-week war was fought in both the Caribbean and the Pacific. As the American agitators for war well knew, US naval power proved decisive, allowing expeditionary forces to disembark in Cuba against a Spanish garrison already facing nationwide Cuban insurgent attacks and further wasted by yellow fever. American, Cuban, and Philippine forces obtained the surrender of Santiago de Cuba and Manila despite the good performance of some Spanish infantry units and fierce fighting for positions such as San Juan Hill. Madrid sued for peace after two obsolete Spanish squadrons sunk in Santiago de Cuba and Manila Bay and a third, more modern fleet was recalled home to protect the Spanish coasts.

The result was the 1898 Treaty of Paris, negotiated on terms favorable to the US which allowed it temporary control of Cuba and ceded ownership of Puerto Rico, Guam, and the Philippine islands. The cession of the Philippines involved payment of $20 million ($ today) to Spain by the US to cover infrastructure owned by Spain.

The defeat and loss of the last remnants of the Spanish Empire was a profound shock to Spain's national psyche and provoked a thorough philosophical and artistic reevaluation of Spanish society known as the Generation of '98. The United States gained several island possessions spanning the globe and a rancorous new debate over the wisdom of expansionism. It was one of only five US wars (against a total of eleven sovereign states) to have been formally declared by the US Congress.

The combined problems arising from the Peninsular War (1807–1814), the loss of most of its colonies in the Americas in the early 19th-century Spanish American wars of independence, and three Carlist Wars (1832–1876) marked the low point of Spanish colonialism. Liberal Spanish elites like Antonio Cánovas del Castillo and Emilio Castelar offered new interpretations of the concept of "empire" to dovetail with Spain's emerging nationalism. Cánovas made clear in an address to the University of Madrid in 1882 his view of the Spanish nation as based on shared cultural and linguistic elements – on both sides of the Atlantic – that tied Spain's territories together.

Cánovas saw Spanish imperialism as markedly different in its methods and purposes of colonization from those of rival empires like the British or French. Spaniards regarded the spreading of civilization and Christianity as Spain's major objective and contribution to the New World. The concept of cultural unity bestowed special significance on Cuba, which had been Spanish for almost four hundred years, and was viewed as an integral part of the Spanish nation. The focus on preserving the empire would have negative consequences for Spain's national pride in the aftermath of the Spanish–American War.

In 1823, the fifth American President James Monroe (1758–1831, served 1817–1825) enunciated the Monroe Doctrine, which stated that the United States would not tolerate further efforts by European governments to retake, expand their colonial holdings in the Americas or to interfere with the newly independent states in the hemisphere; at the same time, the doctrine stated that the US would respect the status of the existing European colonies. Before the American Civil War (1861–1865), Southern interests attempted to have the United States purchase Cuba and convert it into a new slave territory. The pro-slavery element proposed the Ostend Manifesto proposal of 1854. It was rejected by anti-slavery forces.

After the American Civil War and Cuba's Ten Years' War, US businessmen began monopolizing the devalued sugar markets in Cuba. In 1894, 90% of Cuba's total exports went to the United States, which also provided 40% of Cuba's imports. Cuba's total exports to the US were almost twelve times larger than the export to her mother country, Spain. US business interests indicated that while Spain still held political authority over Cuba, economic authority in Cuba, acting-authority, was shifting to the US.

The US became interested in a trans-isthmus canal either in Nicaragua, or in Panama, where the Panama Canal would later be built (1903–1914), and realized the need for naval protection. Captain Alfred Thayer Mahan was an especially influential theorist; his ideas were much admired by future 26th President Theodore Roosevelt, as the US rapidly built a powerful naval fleet of steel warships in the 1880s and 1890s. Roosevelt served as Assistant Secretary of the Navy in 1897–1898 and was an aggressive supporter of an American war with Spain over Cuban interests.

Meanwhile, the "Cuba Libre" movement, led by Cuban intellectual José Martí until his death in 1895, had established offices in Florida. The face of the Cuban revolution in the US was the Cuban "Junta", under the leadership of Tomás Estrada Palma, who in 1902 became Cuba's first president. The Junta dealt with leading newspapers and Washington officials and held fund-raising events across the US. It funded and smuggled weapons. It mounted a large propaganda campaign that generated enormous popular support in the US in favor of the Cubans. Protestant churches and most Democrats were supportive, but business interests called on Washington to negotiate a settlement and avoid war.

Cuba attracted enormous American attention, but almost no discussion involved the other Spanish colonies of Philippines, Guam, or Puerto Rico. Historians note that there was no popular demand in the United States for an overseas colonial empire—Americans did not admire the British Empire or the others.

The first serious bid for Cuban independence, the Ten Years' War, erupted in 1868 and was subdued by the authorities a decade later. Neither the fighting nor the reforms in the Pact of Zanjón (February 1878) quelled the desire of some revolutionaries for wider autonomy and ultimately independence. One such revolutionary, José Martí, continued to promote Cuban financial and political autonomy in exile. In early 1895, after years of organizing, Martí launched a three-pronged invasion of the island.

The plan called for one group from Santo Domingo led by Máximo Gómez, one group from Costa Rica led by Antonio Maceo Grajales, and another from the United States (preemptively thwarted by US officials in Florida) to land in different places on the island and provoke an uprising. While their call for revolution, the "grito de Baíre", was successful, the result was not the grand show of force Martí had expected. With a quick victory effectively lost, the revolutionaries settled in to fight a protracted guerrilla campaign.

Antonio Cánovas del Castillo, the architect of Spain's Restoration constitution and the prime minister at the time, ordered General Arsenio Martínez-Campos, a distinguished veteran of the war against the previous uprising in Cuba, to quell the revolt. Campos's reluctance to accept his new assignment and his method of containing the revolt to the province of Oriente earned him criticism in the Spanish press.

The mounting pressure forced Cánovas to replace General Campos with General Valeriano Weyler, a soldier who had experience in quelling rebellions in overseas provinces and the Spanish metropole. Weyler deprived the insurgency of weaponry, supplies, and assistance by ordering the residents of some Cuban districts to move to reconcentration areas near the military headquarters. This strategy was effective in slowing the spread of rebellion. In the United States, this fueled the fire of anti-Spanish propaganda. In a political speech President William McKinley used this to ram Spanish actions against armed rebels. He even said this "was not civilized warfare" but "extermination".

The Spanish Government regarded Cuba as a province of Spain rather than a colony, and depended on it for prestige and trade, and as a training ground for the army. Spanish Prime Minister Antonio Cánovas del Castillo announced that "the Spanish nation is disposed to sacrifice to the last peseta of its treasure and to the last drop of blood of the last Spaniard before consenting that anyone snatch from it even one piece of its territory". He had long dominated and stabilized Spanish politics. He was assassinated in 1897 by Italian anarchist Michele Angiolillo, leaving a Spanish political system that was not stable and could not risk a blow to its prestige.

The eruption of the Cuban revolt, Weyler's measures, and the popular fury these events whipped up proved to be a boon to the newspaper industry in New York City, where Joseph Pulitzer of the "New York World" and William Randolph Hearst of the "New York Journal" recognized the potential for great headlines and stories that would sell copies. Both papers denounced Spain, but had little influence outside New York. American opinion generally saw Spain as a hopelessly backward power that was unable to deal fairly with Cuba. American Catholics were divided before the war began, but supported it enthusiastically once it started.

The US had important economic interests that were being harmed by the prolonged conflict and deepening uncertainty about the future of Cuba. Shipping firms that had relied heavily on trade with Cuba now suffered losses as the conflict continued unresolved. These firms pressed Congress and McKinley to seek an end to the revolt. Other American business concerns, specifically those who had invested in Cuban sugar, looked to the Spanish to restore order. Stability, not war, was the goal of both interests. How stability would be achieved would depend largely on the ability of Spain and the US to work out their issues diplomatically.

While tension increased among the Cubans and Spanish Government, popular support of intervention began to spring up in the United States, due to the emergence of the "Cuba Libre" movement and the fact that many Americans had drawn parallels between the American Revolution and the Cuban revolt, seeing the Spanish Government as the tyrannical colonial oppressor. Historian Louis Pérez notes that "The proposition of war in behalf of Cuban independence took hold immediately and held on thereafter. Such was the sense of the public mood." At the time many poems and songs were written in the United States to express support of the "Cuba Libre" movement. At the same time, many African Americans, facing growing racial discrimination and increasing retardation of their civil rights, wanted to take part in the war because they saw it as a way to advance the cause of equality, service to country hopefully helping to gain political and public respect amongst the wider population.

President McKinley, well aware of the political complexity surrounding the conflict, wanted to end the revolt peacefully. In accordance with this policy, McKinley began to negotiate with the Spanish government, hoping that the negotiations would be able to end the yellow journalism in the United States, and therefore, end the loudest calls to go to war with Spain. An attempt was made to negotiate a peace before McKinley took office, however, the Spanish refused to take part in the negotiations. In 1897 McKinley appointed Stewart L. Woodford as the new minister to Spain, who again offered to negotiate a peace. In October 1897, the Spanish government still refused the United States offer to negotiate between the Spanish and the Cubans, but promised the US it would give the Cubans more autonomy. However, with the election of a more liberal Spanish government in November, Spain began to change their policies in Cuba. First, the new Spanish government told the United States that it was willing to offer a change in the Reconcentration policies (the main set of policies that was feeding yellow journalism in the United States) if the Cuban rebels agreed to a cessation of hostilities. This time the rebels refused the terms in hopes that continued conflict would lead to US intervention and the creation of an independent Cuba. The liberal Spanish government also recalled the Spanish Governor General Valeriano Weyler from Cuba. This action alarmed many Cubans loyal to Spain.

The Cubans loyal to Weyler began planning large demonstrations to take place when the next Governor General, Ramon Blanco, arrived in Cuba. US consul Fitzhugh Lee learned of these plans and sent a request to the US State Department to send a US warship to Cuba. This request lead to the USS. "Maine" being sent to Cuba. While the "Maine" was docked in Havana, an explosion sank the ship. The sinking of the "Maine" was blamed on the Spanish and made the possibility of a negotiated peace very slim. Throughout the negotiation process, the major European powers, especially Britain, France, and Russia, generally supported the American position and urged Spain to give in. Spain repeatedly promised specific reforms that would pacify Cuba but failed to deliver; American patience ran out.

McKinley sent the USS "Maine" to Havana to ensure the safety of American citizens and interests, and to underscore the urgent need for reform. Naval forces were moved in position to attack simultaneously on several fronts if the war was not avoided. As "Maine" left Florida, a large part of the North Atlantic Squadron was moved to Key West and the Gulf of Mexico. Others were also moved just off the shore of Lisbon, and still others were moved to Hong Kong.

At 9:40 on the evening of February 15, 1898, "Maine" sank in Havana Harbor after suffering a massive explosion. While McKinley urged patience and did not declare that Spain had caused the explosion, the deaths of 250 out of 355 sailors on board focused American attention. McKinley asked Congress to appropriate $50 million for defense, and Congress unanimously obliged. Most American leaders took the position that the cause of the explosion was unknown, but public attention was now riveted on the situation and Spain could not find a diplomatic solution to avoid war. Spain appealed to the European powers, most of whom advised it to accept US conditions for Cuba in order to avoid war. Germany urged a united European stand against the United States but took no action.

The US Navy's investigation, made public on March 28, concluded that the ship's powder magazines were ignited when an external explosion was set off under the ship's hull. This report poured fuel on popular indignation in the US, making the war inevitable. Spain's investigation came to the opposite conclusion: the explosion originated within the ship. Other investigations in later years came to various contradictory conclusions, but had no bearing on the coming of the war. In 1974, Admiral Hyman George Rickover had his staff look at the documents and decided there was an internal explosion. A study commissioned by "National Geographic" magazine in 1999, using AME computer modelling, stated that the explosion could have been caused by a mine, but no definitive evidence was found.

After the "Maine" was destroyed, New York City newspaper publishers Hearst and Pulitzer decided that the Spanish were to blame, and they publicized this theory as fact in their papers. They both used sensationalistic and astonishing accounts of "atrocities" committed by the Spanish in Cuba by using headlines in their newspapers, such as "Spanish Murderers" and "Remember The Maine". Their press exaggerated what was happening and how the Spanish were treating the Cuban prisoners. The stories were based on factual accounts, but most of the time, the articles that were published were embellished and written with incendiary language causing emotional and often heated responses among readers. A common myth falsely states that when illustrator Frederic Remington said there was no war brewing in Cuba, Hearst responded: "You furnish the pictures and I'll furnish the war."

This new "yellow journalism" was, however, uncommon outside New York City, and historians no longer consider it the major force shaping the national mood. Public opinion nationwide did demand immediate action, overwhelming the efforts of President McKinley, Speaker of the House Thomas Brackett Reed, and the business community to find a negotiated solution. Wall Street, big business, high finance and Main Street businesses across the country were vocally opposed to war and demanded peace. After years of severe depression, the economic outlook for the domestic economy was suddenly bright again in 1897. However, the uncertainties of warfare posed a serious threat to full economic recovery. "War would impede the march of prosperity and put the country back many years," warned the "New Jersey Trade Review." The leading railroad magazine editorialized, "From a commercial and mercenary standpoint it seems peculiarly bitter that this war should come when the country had already suffered so much and so needed rest and peace." McKinley paid close attention to the strong anti-war consensus of the business community, and strengthened his resolve to use diplomacy and negotiation rather than brute force to end the Spanish tyranny in Cuba.

A speech delivered by Republican Senator Redfield Proctor of Vermont on March 17, 1898, thoroughly analyzed the situation and greatly strengthened the pro-war cause. Proctor concluded that war was the only answer. Many in the business and religious communities which had until then opposed war, switched sides, leaving McKinley and Speaker Reed almost alone in their resistance to a war. On April 11, McKinley ended his resistance and asked Congress for authority to send American troops to Cuba to end the civil war there, knowing that Congress would force a war.

On April 19, while Congress was considering joint resolutions supporting Cuban independence, Republican Senator Henry M. Teller of Colorado proposed the Teller Amendment to ensure that the US would not establish permanent control over Cuba after the war. The amendment, disclaiming any intention to annex Cuba, passed the Senate 42 to 35; the House concurred the same day, 311 to 6. The amended resolution demanded Spanish withdrawal and authorized the President to use as much military force as he thought necessary to help Cuba gain independence from Spain. President McKinley signed the joint resolution on April 20, 1898, and the ultimatum was sent to Spain. In response, Spain severed diplomatic relations with the United States on April 21. On the same day, the US Navy began a blockade of Cuba. Spain stated, it would declare war if the US forces invaded its territory, on April 23. On April 25, the US Congress declared that a state of war between the US and Spain had de facto existed since April 21, the day the blockade of Cuba had begun.

The Navy was ready, but the Army was not well-prepared for the war and made radical changes in plans and quickly purchased supplies. In the spring of 1898, the strength of the Regular US Army was just 25,000 men. The Army wanted 50,000 new men but received over 220,000 through volunteers and the mobilization of state National Guard units, even gaining nearly 100,000 men on the first night after the explosion of the USS "Maine".

The overwhelming consensus of observers in the 1890s, and historians ever since, is that an upsurge of humanitarian concern with the plight of the Cubans was the main motivating force that caused the war with Spain in 1898. McKinley put it succinctly in late 1897 that if Spain failed to resolve its crisis, the United States would see “a duty imposed by our obligations to ourselves, to civilization and humanity to intervene with force." Intervention in terms of negotiating a settlement proved impossible—neither Spain nor the insurgents would agree. Louis Perez states, "Certainly the moralistic determinants of war in 1898 has been accorded preponderant explanatory weight in the historiography." By the 1950s, however, American political scientists began attacking the war as a mistake based on idealism, arguing that a better policy would be realism. They discredited the idealism by suggesting the people were deliberately misled by propaganda and sensationalist yellow journalism. Political scientist Robert Osgood, writing in 1953, led the attack on the American decision process as a confused mix of "self-righteousness and genuine moral fervor," in the form of a "crusade" and a combination of "knight-errantry and national self- assertiveness." Osgood argued: 

In his "War and Empire", Prof. Paul Atwood of the University of Massachusetts (Boston) writes:

The Spanish–American War was fomented on outright lies and trumped up accusations against the intended enemy. ... War fever in the general population never reached a critical temperature until the accidental sinking of the "USS Maine" was deliberately, and falsely, attributed to Spanish villainy. ... In a cryptic message ... Senator Lodge wrote that 'There may be an explosion any day in Cuba which would settle a great many things. We have got a battleship in the harbor of Havana, and our fleet, which overmatches anything the Spanish have, is masked at the Dry Tortugas.

In his autobiography, Theodore Roosevelt gave his views of the origins of the war:

Our own direct interests were great, because of the Cuban tobacco and sugar, and especially because of Cuba's relation to the projected Isthmian [Panama] Canal. But even greater were our interests from the standpoint of humanity. ... It was our duty, even more from the standpoint of National honor than from the standpoint of National interest, to stop the devastation and destruction. Because of these considerations I favored war.

In the 333 years of Spanish rule, the Philippines developed from a small overseas colony governed from the Viceroyalty of New Spain to a land with modern elements in the cities. The Spanish-speaking middle classes of the 19th century were mostly educated in the liberal ideas coming from Europe. Among these Ilustrados was the Filipino national hero José Rizal, who demanded larger reforms from the Spanish authorities. This movement eventually led to the Philippine Revolution against Spanish colonial rule. The revolution had been in a state of truce since the signing of the Pact of Biak-na-Bato in 1897, with revolutionary leaders having accepted exile outside of the country.

On April 23, 1898, a document appeared in the "Manila Gazette" newspaper warning of the impeding war and calling for Filipinos to participate on the side of Spain.

The first battle between American and Spanish forces was at Manila Bay where, on May 1, Commodore George Dewey, commanding the US Navy's Asiatic Squadron aboard , in a matter of hours defeated a Spanish squadron under Admiral Patricio Montojo. Dewey managed this with only nine wounded. With the German seizure of Tsingtao in 1897, Dewey's squadron had become the only naval force in the Far East without a local base of its own, and was beset with coal and ammunition problems. Despite these problems, the Asiatic Squadron not only destroyed the Spanish fleet but also captured the harbor of Manila.

Following Dewey's victory, Manila Bay was filled with the warships of Britain, Germany, France, and Japan. The German fleet of eight ships, ostensibly in Philippine waters to protect German interests, acted provocatively – cutting in front of American ships, refusing to salute the United States flag (according to customs of naval courtesy), taking soundings of the harbor, and landing supplies for the besieged Spanish.

The Germans, with interests of their own, were eager to take advantage of whatever opportunities the conflict in the islands might afford. There was a fear at the time that the islands would become a German possession. The Americans called the bluff of the Germans, threatening conflict if the aggression continued, and the Germans backed down. At the time, the Germans expected the confrontation in the Philippines to end in an American defeat, with the revolutionaries capturing Manila and leaving the Philippines ripe for German picking.

Commodore Dewey transported Emilio Aguinaldo, a Filipino leader who had led rebellion against Spanish rule in the Philippines in 1896, from exile in Hong Kong to the Philippines to rally more Filipinos against the Spanish colonial government. By June 9, Aguinaldo's forces controlled the provinces of Bulacan, Cavite, Laguna, Batangas, Bataan, Zambales, Pampanga, Pangasinan, and Mindoro, and had laid siege to Manila. On June 12, Aguinaldo proclaimed the independence of the Philippines.

On August 5, on instructions from Spain, Governor General Basilo Augistin turned over command of the Philippines to his deputy, Fermin Jaudenes. On August 13, with American commanders unaware that a cease-fire had been signed between Spain and the US on the previous day in Washington D.C., American forces captured the city of Manila from the Spanish in the Battle of Manila. This battle marked the end of Filipino–American collaboration, as the American action of preventing Filipino forces from entering the captured city of Manila was deeply resented by the Filipinos. This later led to the Philippine–American War, which would prove to be more deadly and costly than the Spanish–American War.

The US had sent a force of some 11,000 ground troops to the Philippines. On August 14, 1899, Spanish Captain-General Jaudenes formally capitulated and US Generally Merritt formally accepted the surrender and declared the establishment of a US military government in occupation. That same day, the Schurman Commission recommended that the US retain control of the Philippines, possibly granting independence in the future. On December 10, 1898, the Spanish government ceded the Philippines to the United States in the Treaty of Paris. Armed conflict broke out between US forces and the Filipinos when US troops began to take the place of the Spanish in control of the country after the end of the war, resulting in the Philippine–American War.

On June 20, a US fleet commanded by Captain Henry Glass, consisting of the protected cruiser and three transports carrying troops to the Philippines, entered Guam's Apra Harbor, Captain Glass having opened sealed orders instructing him to proceed to Guam and capture it. "Charleston" fired a few cannon rounds at Fort Santa Cruz without receiving return fire. Two local officials, not knowing that war had been declared and believing the firing had been a salute, came out to "Charleston" to apologize for their inability to return the salute as they were out of gunpowder. Glass informed them that the US and Spain were at war.

The following day, Glass sent Lt. William Braunersruehter to meet the Spanish Governor to arrange the surrender of the island and the Spanish garrison there. Some 54 Spanish infantry were captured and transported to the Philippines as prisoners of war. No US forces were left on Guam, but the only US citizen on the island, Frank Portusach, told Captain Glass that he would look after things until US forces returned.

Theodore Roosevelt advocated intervention in Cuba, both for the Cuban people and to promote the Monroe Doctrine. While Assistant Secretary of the Navy, he placed the Navy on a war-time footing and prepared Dewey's Asiatic Squadron for battle. He also worked with Leonard Wood in convincing the Army to raise an all-volunteer regiment, the 1st US Volunteer Cavalry. Wood was given command of the regiment that quickly became known as the "Rough Riders".

The Americans planned to capture the city of Santiago de Cuba to destroy Linares' army and Cervera's fleet. To reach Santiago they had to pass through concentrated Spanish defenses in the San Juan Hills and a small town in El Caney. The American forces were aided in Cuba by the pro-independence rebels led by General Calixto García.

For quite some time the Cuban public believed the United States government to possibly hold the key to its independence, and even annexation was considered for a time, which historian Louis Pérez explored in his book "Cuba and the United States: Ties of Singular Intimacy". The Cubans harbored a great deal of discontent towards the Spanish Government, due to years of manipulation on the part of the Spanish. The prospect of getting the United States involved in the fight was considered by many Cubans as a step in the right direction. While the Cubans were wary of the United States' intentions, the overwhelming support from the American public provided the Cubans with some peace of mind, because they believed that the United States was committed to helping them achieve their independence. However, with the imposition of the Platt Amendment of 1903 after the war, as well as economic and military manipulation on the part of the United States, Cuban sentiment towards the United States became polarized, with many Cubans disappointed with continuing American interference.

From June 22 to 24, the Fifth Army Corps under General William R. Shafter landed at Daiquirí and Siboney, east of Santiago, and established an American base of operations. A contingent of Spanish troops, having fought a skirmish with the Americans near Siboney on June 23, had retired to their lightly entrenched positions at Las Guasimas. An advance guard of US forces under former Confederate General Joseph Wheeler ignored Cuban scouting parties and orders to proceed with caution. They caught up with and engaged the Spanish rearguard of about 2,000 soldiers led by General Antero Rubín who effectively ambushed them, in the Battle of Las Guasimas on June 24. The battle ended indecisively in favor of Spain and the Spanish left Las Guasimas on their planned retreat to Santiago.
The US Army employed Civil War-era skirmishers at the head of the advancing columns. Three of four of the US soldiers who had volunteered to act as skirmishers walking point at the head of the American column were killed, including Hamilton Fish II (grandson of Hamilton Fish, the Secretary of State under Ulysses S. Grant), and Captain Allyn K. Capron, Jr., whom Theodore Roosevelt would describe as one of the finest natural leaders and soldiers he ever met. Only Oklahoma Territory Pawnee Indian, Tom Isbell, wounded seven times, survived.

The Battle of Las Guasimas showed the US that quick-thinking American soldiers would not stick to the linear tactics which did not work effectively against Spanish troops who had learned the art of cover and concealment from their own struggle with Cuban insurgents, and never made the error of revealing their positions while on the defense. Americans advanced by rushes and stayed in the weeds so that they, too, were largely invisible to the Spaniards who used un-targeted volley fire to try to mass fires against the advancing Americans. While some troops were hit, this technique was mostly a waste of bullets as the Americans learned to duck as soon as they heard the Spanish word Fire, "Fuego" yelled by the Spanish officers. Spanish troops were equipped with smokeless powder arms that also helped them to hide their positions while firing.

Regular Spanish troops were mostly armed with modern charger-loaded, 7 mm 1893 Spanish Mauser rifles and using smokeless powder. The high-speed 7×57mm Mauser round was termed the "Spanish Hornet" by the Americans because of the supersonic crack as it passed overhead. Other irregular troops were armed with Remington Rolling Block rifles in .43 Spanish using smokeless powder and brass-jacketed bullets. US regular infantry were armed with the .30–40 Krag–Jørgensen, a bolt-action rifle with a complex rotating magazine. Both the US regular cavalry and the volunteer cavalry used smokeless ammunition. In later battles, state volunteers used the .45–70 Springfield a single-shot black powder rifle.

On July 1, a combined force of about 15,000 American troops in regular infantry and cavalry regiments, including all four of the army's "Colored" regiments, and volunteer regiments, among them Roosevelt and his "Rough Riders", the 71st New York, the 2nd Massachusetts Infantry, and 1st North Carolina, and rebel Cuban forces attacked 1,270 entrenched Spaniards in dangerous Civil War-style frontal assaults at the Battle of El Caney and Battle of San Juan Hill outside of Santiago. More than 200 US soldiers were killed and close to 1,200 wounded in the fighting, thanks to the high rate of fire the Spanish put down range at the Americans. Supporting fire by Gatling guns was critical to the success of the assault. Cervera decided to escape Santiago two days later. First Lieutenant John J. Pershing, nicknamed "Black Jack", oversaw the 10th Cavalry Unit during the war. Pershing and his unit fought in the Battle of San Juan Hill. Pershing was cited for his gallantry during the battle.

The Spanish forces at Guantánamo were so isolated by Marines and Cuban forces that they did not know that Santiago was under siege, and their forces in the northern part of the province could not break through Cuban lines. This was not true of the Escario relief column from Manzanillo, which fought its way past determined Cuban resistance but arrived too late to participate in the siege.

After the battles of San Juan Hill and El Caney, the American advance halted. Spanish troops successfully defended Fort Canosa, allowing them to stabilize their line and bar the entry to Santiago. The Americans and Cubans forcibly began a bloody, strangling siege of the city. During the nights, Cuban troops dug successive series of "trenches" (raised parapets), toward the Spanish positions. Once completed, these parapets were occupied by US soldiers and a new set of excavations went forward. American troops, while suffering daily losses from Spanish fire, suffered far more casualties from heat exhaustion and mosquito-borne disease. At the western approaches to the city, Cuban general Calixto Garcia began to encroach on the city, causing much panic and fear of reprisals among the Spanish forces.

The major port of Santiago de Cuba was the main target of naval operations during the war. The US fleet attacking Santiago needed shelter from the summer hurricane season; Guantánamo Bay, with its excellent harbor, was chosen. The 1898 invasion of Guantánamo Bay happened between June 6 and 10, with the first US naval attack and subsequent successful landing of US Marines with naval support.

On April 23, a council of senior admirals of the Spanish Navy had decided to order Admiral Pascual Cervera y Topete's squadron of four armored cruisers and three torpedo boat destroyers to proceed from their present location in Cape Verde (having left from Cadiz, Spain) to the West Indies.

The Battle of Santiago de Cuba on July 3, was the largest naval engagement of the Spanish–American War and resulted in the destruction of the Spanish Caribbean Squadron (also known as the "Flota de Ultramar"). In May, the fleet of Spanish Admiral Pascual Cervera y Topete had been spotted by American forces in Santiago harbor, where they had taken shelter for protection from sea attack. A two-month stand-off between Spanish and American naval forces followed.

When the Spanish squadron finally attempted to leave the harbor on July 3, the American forces destroyed or grounded five of the six ships. Only one Spanish vessel, the new armored cruiser , survived, but her captain hauled down her flag and scuttled her when the Americans finally caught up with her. The 1,612 Spanish sailors who were captured, including Admiral Cervera, were sent to Seavey's Island at the Portsmouth Naval Shipyard in Kittery, Maine, where they were confined at Camp Long as prisoners of war from July 11 until mid-September.

During the stand-off, US Assistant Naval Constructor, Lieutenant Richmond Pearson Hobson had been ordered by Rear Admiral William T. Sampson to sink the collier in the harbor to bottle up the Spanish fleet. The mission was a failure, and Hobson and his crew were captured. They were exchanged on July 6, and Hobson became a national hero; he received the Medal of Honor in 1933, retired as a Rear Admiral and became a Congressman.

Yellow fever had quickly spread amongst the American occupation force, crippling it. A group of concerned officers of the American army chose Theodore Roosevelt to draft a request to Washington that it withdraw the Army, a request that paralleled a similar one from General Shafter, who described his force as an "army of convalescents". By the time of his letter, 75% of the force in Cuba was unfit for service.

On August 7, the American invasion force started to leave Cuba. The evacuation was not total. The US Army kept the black Ninth US Cavalry Regiment in Cuba to support the occupation. The logic was that their race and the fact that many black volunteers came from southern states would protect them from disease; this logic led to these soldiers being nicknamed "Immunes". Still, when the Ninth left, 73 of its 984 soldiers had contracted the disease.

In May 1898, Lt. Henry H. Whitney of the United States Fourth Artillery was sent to Puerto Rico on a reconnaissance mission, sponsored by the Army's Bureau of Military Intelligence. He provided maps and information on the Spanish military forces to the US government before the invasion.

The American offensive began on May 12, 1898, when a squadron of 12 US ships commanded by Rear Adm. William T. Sampson of the United States Navy attacked the archipelago's capital, San Juan. Though the damage inflicted on the city was minimal, the Americans established a blockade in the city's harbor, San Juan Bay. On June 22, the cruiser "Isabel II" and the destroyer "Terror" delivered a Spanish counterattack, but were unable to break the blockade and the "Terror" was damaged.

The land offensive began on July 25, when 1,300 infantry soldiers led by Nelson A. Miles disembarked off the coast of Guánica. The first organized armed opposition occurred in Yauco in what became known as the Battle of Yauco.

This encounter was followed by the Battle of Fajardo. The United States seized control of Fajardo on August 1, but were forced to withdraw on August 5 after a group of 200 Puerto Rican-Spanish soldiers led by Pedro del Pino gained control of the city, while most civilian inhabitants fled to a nearby lighthouse. The Americans encountered larger opposition during the Battle of Guayama and as they advanced towards the main island's interior. They engaged in crossfire at Guamaní River Bridge, Coamo and Silva Heights and finally at the Battle of Asomante. The battles were inconclusive as the allied soldiers retreated.

A battle in San Germán concluded in a similar fashion with the Spanish retreating to Lares. On August 9, 1898, American troops that were pursuing units retreating from Coamo encountered heavy resistance in Aibonito in a mountain known as "Cerro Gervasio del Asomante" and retreated after six of their soldiers were injured. They returned three days later, reinforced with artillery units and attempted a surprise attack. In the subsequent crossfire, confused soldiers reported seeing Spanish reinforcements nearby and five American officers were gravely injured, which prompted a retreat order. All military actions in Puerto Rico were suspended on August 13, after US President William McKinley and French Ambassador Jules Cambon, acting on behalf of the Spanish Government, signed an armistice whereby Spain relinquished its sovereignty over Puerto Rico.

With defeats in Cuba and the Philippines, and both of its fleets destroyed, Spain sued for peace and negotiations were opened between the two parties. After the sickness and death of British consul Edward Henry Rawson-Walker, American admiral George Dewey requested the Belgian consul to Manila, Édouard André, to take Rawson-Walker's place as intermediary with the Spanish Government.

Hostilities were halted on August 12, 1898, with the signing in Washington of a Protocol of Peace between the United States and Spain. After over two months of difficult negotiations, the formal peace treaty, the Treaty of Paris, was signed in Paris on December 10, 1898, and was ratified by the United States Senate on February 6, 1899.

The United States gained Spain's colonies of the Philippines, Guam and Puerto Rico in the treaty, and Cuba became a US protectorate. The treaty came into force in Cuba April 11, 1899, with Cubans participating only as observers. Having been occupied since July 17, 1898, and thus under the jurisdiction of the United States Military Government (USMG), Cuba formed its own civil government and gained independence on May 20, 1902, with the announced end of USMG jurisdiction over the island. However, the US imposed various restrictions on the new government, including prohibiting alliances with other countries, and reserved the right to intervene. The US also established a perpetual lease of Guantánamo Bay.

The war lasted ten weeks. John Hay (the United States Ambassador to the United Kingdom), writing from London to his friend Theodore Roosevelt, declared that it had been "a splendid little war". The press showed Northerners and Southerners, blacks and whites fighting against a common foe, helping to ease the scars left from the American Civil War. Exemplary of this was the fact that four former Confederate States Army generals had served in the war, now in the US Army and all of them again carrying similar ranks. These officers included Matthew Butler, Fitzhugh Lee, Thomas L. Rosser and Joseph Wheeler, though only the latter had seen action. Still, in an exciting moment during the Battle of Las Guasimas, Wheeler apparently forgot for a moment which war he was fighting, having supposedly called out "Let's go, boys! We've got the damn Yankees on the run again!"

The war marked American entry into world affairs. Since then, the US has had a significant hand in various conflicts around the world, and entered many treaties and agreements. The Panic of 1893 was over by this point, and the US entered a long and prosperous period of economic and population growth, and technological innovation that lasted through the 1920s.

The war redefined national identity, served as a solution of sorts to the social divisions plaguing the American mind, and provided a model for all future news reporting.

The idea of American imperialism changed in the public's mind after the short and successful Spanish–American War. Due to the United States' powerful influence diplomatically and militarily, Cuba's status after the war relied heavily upon American actions. Two major developments emerged from the Spanish–American War: one, it greatly enforced the United States' vision of itself as a "defender of democracy" and as a major world power, and two, it had severe implications for Cuban–American relations in the future. As historian Louis Pérez argued in his book "Cuba in the American Imagination: Metaphor and the Imperial Ethos", the Spanish–American War of 1898 "fixed permanently how Americans came to think of themselves: a righteous people given to the service of righteous purpose".

The war greatly reduced the Spanish Empire. Spain had been declining as an imperial power since the early 19th century as a result of Napoleon's invasion. The loss of Cuba caused a national trauma because of the affinity of peninsular Spaniards with Cuba, which was seen as another province of Spain rather than as a colony. Spain retained only a handful of overseas holdings: Spanish West Africa (Spanish Sahara), Spanish Guinea, Spanish Morocco, and the Canary Islands.

The Spanish soldier Julio Cervera Baviera, who served in the Puerto Rican Campaign, published a pamphlet in which he blamed the natives of that colony for its occupation by the Americans, saying, "I have never seen such a servile, ungrateful country [i.e., Puerto Rico]... In twenty-four hours, the people of Puerto Rico went from being fervently Spanish to enthusiastically American... They humiliated themselves, giving in to the invader as the slave bows to the powerful lord." He was challenged to a duel by a group of young Puerto Ricans for writing this pamphlet.

Culturally, a new wave called the Generation of '98 originated as a response to this trauma, marking a renaissance in Spanish culture. Economically, the war benefited Spain, because after the war large sums of capital held by Spaniards in Cuba and the United States were returned to the peninsula and invested in Spain. This massive flow of capital (equivalent to 25% of the gross domestic product of one year) helped to develop the large modern firms in Spain in the steel, chemical, financial, mechanical, textile, shipyard, and electrical power industries. However, the political consequences were serious. The defeat in the war began the weakening of the fragile political stability that had been established earlier by the rule of Alfonso XII.

The Teller Amendment, which was enacted on April 20, 1898, was a promise from the United States to the Cuban people that it was not declaring war to annex Cuba, but to help it gain its independence from Spain. The Platt Amendment was a move by the United States' government to shape Cuban affairs without violating the Teller Amendment.

The US Congress had passed the Teller Amendment before the war, promising Cuban independence. However, the Senate passed the Platt Amendment as a rider to an Army appropriations bill, forcing a peace treaty on Cuba which prohibited it from signing treaties with other nations or contracting a public debt. The Platt Amendment was pushed by imperialists who wanted to project US power abroad (in contrast to the Teller Amendment which was pushed by anti-imperialists who called for a restraint on US rule). The amendment granted the United States the right to stabilize Cuba militarily as needed. In addition, the Platt Amendment permitted the United States to deploy Marines to Cuba if its freedom and independence was ever threatened or jeopardized by an external or internal force. The Platt Amendment also provided for a permanent American naval base in Cuba. Guantánamo Bay was established after the signing of the Cuban–American Treaty of Relations in 1903. Thus, despite that Cuba technically gained its independence after the war ended, the United States government ensured that it had some form of power and control over Cuban affairs.

The US annexed the former Spanish colonies of Puerto Rico, the Philippines and Guam. The notion of the United States as an imperial power, with colonies, was hotly debated domestically with President McKinley and the Pro-Imperialists winning their way over vocal opposition led by Democrat William Jennings Bryan, who had supported the war. The American public largely supported the possession of colonies, but there were many outspoken critics such as Mark Twain, who wrote "The War Prayer" in protest.

Roosevelt returned to the United States a war hero, and he was soon elected governor of New York and then became the vice president. At the age of 42 he became the youngest man to become President after the assassination of President William McKinley.
The war served to further repair relations between the American North and South. The war gave both sides a common enemy for the first time since the end of the Civil War in 1865, and many friendships were formed between soldiers of northern and southern states during their tours of duty. This was an important development, since many soldiers in this war were the children of Civil War veterans on both sides.
The African-American community strongly supported the rebels in Cuba, supported entry into the war, and gained prestige from their wartime performance in the Army. Spokesmen noted that 33 African-American seamen had died in the "Maine" explosion. The most influential Black leader, Booker T. Washington, argued that his race was ready to fight. War offered them a chance "to render service to our country that no other race can", because, unlike Whites, they were "accustomed" to the "peculiar and dangerous climate" of Cuba. One of the Black units that served in the war was the 9th Cavalry Regiment. In March 1898, Washington promised the Secretary of the Navy that war would be answered by "at least ten thousand loyal, brave, strong black men in the south who crave an opportunity to show their loyalty to our land, and would gladly take this method of showing their gratitude for the lives laid down, and the sacrifices made, that Blacks might have their freedom and rights."

In 1904, the United Spanish War Veterans was created from smaller groups of the veterans of the Spanish–American War. Today, that organization is defunct, but it left an heir in the Sons of Spanish–American War Veterans, created in 1937 at the 39th National Encampment of the United Spanish War Veterans. According to data from the United States Department of Veterans Affairs, the last surviving US veteran of the conflict, Nathan E. Cook, died on September 10, 1992, at age 106. (If the data is to be believed, Cook, born October 10, 1885, would have been only 12 years old when he served in the war.)

The Veterans of Foreign Wars of the United States (VFW) was formed in 1914 from the merger of two veterans organizations which both arose in 1899: the American Veterans of Foreign Service and the National Society of the Army of the Philippines. The former was formed for veterans of the Spanish–American War, while the latter was formed for veterans of the Philippine–American War. Both organizations were formed in response to the general neglect veterans returning from the war experienced at the hands of the government.

To pay the costs of the war, Congress passed an excise tax on long-distance phone service. At the time, it affected only wealthy Americans who owned telephones. However, the Congress neglected to repeal the tax after the war ended four months later, and the tax remained in place for over 100 years until, on August 1, 2006, it was announced that the US Department of the Treasury and the IRS would no longer collect the tax.

The change in sovereignty of Puerto Rico, like the occupation of Cuba, brought about major changes in both the insular and US economies. Before 1898 the sugar industry in Puerto Rico was in decline for nearly half a century. In the second half of the nineteenth century, technological advances increased the capital requirements to remain competitive in the sugar industry. Agriculture began to shift toward coffee production, which required less capital and land accumulation. However, these trends were reversed with US hegemony. Early US monetary and legal policies made it both harder for local farmers to continue operations and easier for American businesses to accumulate land. This, along with the large capital reserves of American businesses, led to a resurgence in the Puerto Rican nuts and sugar industry in the form of large American owned agro-industrial complexes.

At the same time, the inclusion of Puerto Rico into the US tariff system as a customs area, effectively treating Puerto Rico as a state with respect to internal or external trade, increased the codependence of the insular and mainland economies and benefitted sugar exports with tariff protection. In 1897 the United States purchased 19.6 percent of Puerto Rico's exports while supplying 18.5 percent of its imports. By 1905 these figures jumped to 84 percent and 85 percent, respectively. However, coffee was not protected, as it was not a product of the mainland. At the same time, Cuba and Spain, traditionally the largest importers of Puerto Rican coffee, now subjected Puerto Rico to previously nonexistent import tariffs. These two effects led to a decline in the coffee industry. From 1897 to 1901 coffee went from 65.8 percent of exports to 19.6 percent while sugar went from 21.6 percent to 55 percent. The tariff system also provided a protected market place for Puerto Rican tobacco exports. The tobacco industry went from nearly nonexistent in Puerto Rico to a major part of the country's agricultural sector.

The Spanish–American War was the first US war in which the motion picture camera played a role. The Library of Congress archives contain many films and film clips from the war. In addition, a few feature films have been made about the war. These include

The United States awards and decorations of the Spanish–American War were as follows:



The governments of Spain and Cuba also issued a wide variety of military awards to honor Spanish, Cuban, and Philippine soldiers who had served in the conflict.







</doc>
<doc id="28266" url="https://en.wikipedia.org/wiki?curid=28266" title="Scurvy">
Scurvy

Scurvy is a disease resulting from a lack of vitamin C (ascorbic acid). Early symptoms include weakness, feeling tired, and sore arms and legs. Without treatment, decreased red blood cells, gum disease, changes to hair, and bleeding from the skin may occur. As scurvy worsens there can be poor wound healing, personality changes, and finally death from infection or bleeding.
Typically, scurvy is caused by a lack of vitamin C in the diet. It takes at least a month of little to no vitamin C before symptoms occur. In modern times, it occurs most commonly in people with mental disorders, unusual eating habits, alcoholism, and old people who live alone. Other risk factors include intestinal malabsorption and dialysis. Humans and certain other animals require vitamin C in their diets to make the building blocks for collagen. Diagnosis typically is based on physical signs, X-rays, and improvement after treatment.
Treatment is with vitamin C supplements taken by mouth. Improvement often begins in a few days with complete recovery in a few weeks. Sources of vitamin C in the diet include citrus fruit and a number of vegetables such as tomatoes and potatoes. Cooking often decreases vitamin C in foods.
Scurvy currently is rare. It occurs more often in the developing world in association with malnutrition. Rates among refugees are reported at 5 to 45 percent. Scurvy was described as early as the time of ancient Egypt. It was a limiting factor in long distance sea travel, often killing large numbers of people. During the Age of Sail, it was assumed that 50 percent of the sailors would die of scurvy on a given trip. A Scottish surgeon in the Royal Navy, James Lind, is generally credited with proving that scurvy can be successfully treated with citrus fruit in 1753. Nonetheless, it would be 1795 before health reformers such as Gilbert Blane convinced the British Royal Navy to routinely give lemon juice to its sailors.

Early symptoms are malaise and lethargy. Even earlier might be a pain in a section of the gums which interferes with digestion. After one to three months, patients develop shortness of breath and bone pain. Myalgias may occur because of reduced carnitine production. Other symptoms include skin changes with roughness, easy bruising and petechiae, gum disease, loosening of teeth, poor wound healing, and emotional changes (which may appear before any physical changes). Dry mouth and dry eyes similar to Sjögren's syndrome may occur. In the late stages, jaundice, generalized edema, oliguria, neuropathy, fever, convulsions, and eventual death are frequently seen.
Scurvy, including subclinical scurvy, is caused by a deficiency of dietary vitamin C since humans are unable to metabolically make this chemical. Provided diet contains sufficient vitamin C, the lack of working GULO enzyme has no significance, and in modern Western societies, scurvy is rarely present in adults, although infants and elderly people are affected. Virtually all commercially available baby formulas contain added vitamin C, preventing infantile scurvy. Human breast milk contains sufficient vitamin C, if the mother has an adequate intake. Commercial milk is pasteurized, a heating process that destroys the natural vitamin C content of the milk.

Scurvy is one of the accompanying diseases of malnutrition (other such micronutrient deficiencies are beriberi or pellagra) and thus is still widespread in areas of the world depending on external food aid. 
Although rare, there are also documented cases of scurvy due to poor dietary choices by people living in industrialized nations.

Vitamins are essential to the production and use of enzymes that are involved in ongoing processes throughout the human body. Ascorbic acid is needed for a variety of biosynthetic pathways, by accelerating hydroxylation and amidation reactions. In the synthesis of collagen, ascorbic acid is required as a cofactor for prolyl hydroxylase and lysyl hydroxylase. These two enzymes are responsible for the hydroxylation of the proline and lysine amino acids in collagen. Hydroxyproline and hydroxylysine are important for stabilizing collagen by cross-linking the propeptides in collagen.

Collagen is a primary structural protein in the human body, necessary for healthy blood vessels, muscle, skin, bone, cartilage, and other connective tissues.
Defective connective tissue leads to fragile capillaries, resulting in abnormal bleeding, bruising, and internal hemorrhaging.
Collagen is an important part of bone, so bone formation is also affected. Teeth loosen, bones break more easily, and once-healed breaks may recur. 
Defective collagen fibrillogenesis impairs wound healing.
Untreated scurvy is invariably fatal.

Diagnosis typically is based on physical signs, X-rays, and improvement after treatment.

Scurvy can be prevented by a diet that includes vitamin C-rich foods such as bell peppers (sweet peppers), blackcurrants, broccoli, chili peppers, guava, kiwifruit, and parsley. Other sources rich in vitamin C are fruits such as lemons, limes, oranges, papaya, and strawberries. It is also found in vegetables, such as brussels sprouts, cabbage, potatoes, and spinach. Some fruits and vegetables not high in vitamin C may be pickled in lemon juice, which is high in vitamin C. Though redundant in the presence of a balanced diet, various nutritional supplements are available that provide ascorbic acid well in excess of that required to prevent scurvy.

Some animal products, including liver, Muktuk (whale skin), oysters, and parts of the central nervous system, including the adrenal medulla, brain, and spinal cord, contain large amounts of vitamin C, and can even be used to treat scurvy. Fresh meat from animals which make their own vitamin C (which most animals do) contains enough vitamin C to prevent scurvy, and even partly treat it. In some cases (notably French soldiers eating fresh horse meat), it was discovered that meat alone, even partly cooked meat, could alleviate scurvy. Conversely, in other cases, a meat-only diet could cause scurvy.

Scott's 1902 Antarctic expedition used lightly fried seal meat and liver, whereby complete recovery from incipient scurvy was reported to have taken less than two weeks.

Hippocrates documented scurvy as a disease, and Egyptians have recorded its symptoms as early as 1550 BCE. The knowledge that consuming foods containing vitamin C is a cure for scurvy has been repeatedly rediscovered and forgotten into the early 20th century.

In the 13th century, the Crusaders frequently suffered from scurvy. In the 1497 expedition of Vasco de Gama, the curative effects of citrus fruit were already known and confirmed by Pedro Álvares Cabral and his crew in 1507.

The Portuguese planted fruit trees and vegetables in Saint Helena, a stopping point for homebound voyages from Asia, and left their sick, suffering from scurvy and other ailments, to be taken home, if they recovered, by the next ship.

In 1500, one of the pilots of Cabral's fleet bound for India noted that in Malindi, its king offered the expedition fresh supplies such as lambs, chickens, and ducks, along with lemons and oranges, due to which "some of our ill were cured of scurvy".

Unfortunately, these travel accounts did not stop further maritime tragedies caused by scurvy, first because of the lack of communication between travelers and those responsible for their health, and because fruits and vegetables could not be kept for long on ships.

In 1536, the French explorer Jacques Cartier, exploring the St. Lawrence River, used the local natives' knowledge to save his men who were dying of scurvy. He boiled the needles of the arbor vitae tree (Eastern White Cedar) to make a tea that was later shown to contain 50 mg of vitamin C per 100 grams. Such treatments were not available aboard ship, where the disease was most common.
In February 1601, Captain James Lancaster, while sailing to Sumatra, landed on the northern coast to specifically obtain lemons and oranges for his crew to stop scurvy. Captain Lancaster conducted an experiment using four ships under his command. One ship's crew received routine doses of lemon juice while the other three ships did not receive any such treatment. As a result, members of the non-treated ships started to become ill, contracting scurvy with many dying as a result.

During the Age of Exploration (between 1500 and 1800), it has been estimated that scurvy killed at least two million sailors. Jonathan Lamb wrote: "In 1499, Vasco da Gama lost 116 of his crew of 170; In 1520, Magellan lost 208 out of 230;...all mainly to scurvy."

In 1579, the Spanish friar and physician Agustin Farfán published a book in which he recommended oranges and lemons for scurvy, a remedy that was already known in the Spanish Navy.

In 1593, Admiral Sir Richard Hawkins advocated drinking orange and lemon juice as a means of preventing scurvy.

In 1614, John Woodall, Surgeon General of the East India Company, published "The Surgion's Mate" as a handbook for apprentice surgeons aboard the company's ships. He repeated the experience of mariners that the cure for scurvy was fresh food or, if not available, oranges, lemons, limes, and tamarinds. He was, however, unable to explain the reason why, and his assertion had no impact on the opinions of the influential physicians who ran the medical establishment that scurvy was a digestive complaint.

A 1707 handwritten book by Mrs. Ebot Mitchell, discovered in a house in Hasfield, Gloucestershire, contains a "Recp.t for the Scurvy" that consisted of extracts from various plants mixed with a plentiful supply of orange juice, white wine or beer.

In 1734, the Leiden-based physician Johann Bachstrom published a book on scurvy in which he stated, "scurvy is solely owing to a total abstinence from fresh vegetable food, and greens; which is alone the primary cause of the disease", and urged the use of fresh fruit and vegetables as a cure.

However, it was not until 1747 that James Lind formally demonstrated that scurvy could be treated by supplementing the diet with citrus fruit, in one of the first controlled clinical experiments reported in the history of medicine. 
As a naval surgeon on HMS "Salisbury", Lind had compared several suggested scurvy cures: hard cider, vitriol, vinegar, seawater, oranges, lemons, and a mixture of balsam of Peru, garlic, myrrh, mustard seed and radish root. In "A Treatise on the Scurvy" (1753) 
Lind explained the details of his clinical trial and concluded "the results of all my experiments was, that oranges and lemons were the most effectual remedies for this distemper at sea.”

Unfortunately, the experiment and its results occupied only a few paragraphs in a work that was long and complex and had little impact. Lind himself never actively promoted lemon juice as a single ‘cure’. He shared medical opinion at the time that scurvy had multiple causes – notably hard work, bad water, and the consumption of salt meat in a damp atmosphere which inhibited healthful perspiration and normal excretion - and therefore required multiple solutions.
Lind was also sidetracked by the possibilities of producing a concentrated ‘rob’ of lemon juice by boiling it. Unfortunately this process destroyed the vitamin C and was therefore unsuccessful.

During the 18th century, disease killed more British sailors than enemy action. It was mainly by scurvy that George Anson, in his celebrated voyage of 1740–1744, lost nearly two-thirds of his crew (1,300 out of 2,000) within the first 10 months of the voyage. 
The Royal Navy enlisted 184,899 sailors during the Seven Years' War; 133,708 of these were "missing" or died from disease, and scurvy was the leading cause.

Although throughout this period sailors and naval surgeons were increasingly convinced that citrus fruits could cure scurvy, the classically trained physicians who ran the medical establishment dismissed this evidence as mere anecdote which did not conform to current theories of disease. Literature championing the cause of citrus juice, therefore, had no practical impact. Medical theory was based on the assumption that scurvy was a disease of internal putrefaction brought on by faulty digestion caused by the hardships of life at sea and the naval diet. Although this basic idea was given different emphases by successive theorists, the remedies they advocated (and which the navy accepted) amounted to little more than the consumption of ‘fizzy drinks’ to activate the digestive system, the most extreme of which was the regular consumption of ‘elixir of vitriol’ – sulphuric acid taken with spirits and barley water, and laced with spices.

In 1764, a new variant appeared. Advocated by Dr David MacBride and Sir John Pringle, Surgeon General of the Army and later President of the Royal Society, this idea was that scurvy was the result of a lack of ‘fixed air’ in the tissues which could be prevented by drinking infusions of malt and wort whose fermentation within the body would stimulate digestion and restore the missing gases. These ideas received wide and influential backing, when James Cook set off to circumnavigate the world (1768–1771) in , malt and wort were top of the list of the remedies he was ordered to investigate. The others were beer, sour crout and Lind's ‘rob’. The list did not include lemons.

Cook did not lose a single man to scurvy, and his report came down in favour of malt and wort, although it is now clear that the reason for the health of his crews on this and other voyages was Cook's regime of shipboard cleanliness, enforced by strict discipline, as well as frequent replenishment of fresh food and green stuffs. Another rule implemented by Cook was his prohibition of the consumption of salt fat skimmed from the ship's copper boiling pans, then a common practice in the Navy. In contact with air the copper formed compounds that prevented the absorption of vitamins by the intestines.

The first major long distance expedition that experienced virtually no scurvy was that of the Spanish naval officer Alessandro Malaspina, 1789–1794. Malaspina's medical officer, Pedro González, was convinced that fresh oranges and lemons were essential for preventing scurvy. Only one outbreak occurred, during a 56-day trip across the open sea. Five sailors came down with symptoms, one seriously. After three days at Guam all five were healthy again. Spain's large empire and many ports of call made it easier to acquire fresh fruit.

Although towards the end of the century MacBride's theories were being challenged, the medical establishment in Britain remained wedded to the notion that scurvy was a disease of internal ‘putrefaction’ and the Sick and Hurt Board, run by administrators, felt obliged to follow its advice. Within the Royal Navy, however, opinion – strengthened by first-hand experience of the use of lemon juice at the siege of Gibraltar and during Admiral Rodney's expedition to the Caribbean – had become increasingly convinced of its efficacy. This was reinforced by the writings of experts like Gilbert Blane and Thomas Trotter and by the reports of up-and-coming naval commanders.

With the coming of war in 1793, the need to eliminate scurvy acquired a new urgency. But the first initiative came not from the medical establishment but from the admirals. Ordered to lead an expedition against Mauritius, Rear Admiral Gardner was uninterested in the wort, malt and elixir of vitriol which were still being issued to ships of the Royal Navy, and demanded that he be supplied with lemons, to counteract scurvy on the voyage. Members of the Sick and Hurt Board, recently augmented by two practical naval surgeons, supported the request, and the Admiralty ordered that it be done. There was, however, a last minute change of plan. The expedition against Mauritius was cancelled. On 2 May 1794, only and two sloops under Commodore Peter Rainier sailed for the east with an outward bound convoy, but the warships were fully supplied with lemon juice and the sugar with which it had to be mixed. Then in March 1795, came astonishing news. "Suffolk" had arrived in India after a four-month voyage without a trace of scurvy and with a crew that was healthier than when it set out.

The effect was immediate. Fleet commanders clamoured also to be supplied with lemon juice, and by June the Admiralty acknowledged the groundswell of demand in the navy had agreed to a proposal from the Sick and Hurt Board that lemon juice and sugar should in future be issued as a daily ration to the crews of all warships.

It took a few years before the method of distribution to all ships in the fleet had been perfected and the supply of the huge quantities of lemon juice required to be secured, but by 1800, the system was in place and functioning. This led to a remarkable health improvement among the sailors and consequently played a critical role in gaining the advantage in naval battles against enemies who had yet to introduce the measures.

The surgeon-in-chief of Napoleon's army at the Siege of Alexandria (1801), Baron Dominique-Jean Larrey, wrote in his memoirs that the consumption of horse meat helped the French to curb an epidemic of scurvy. The meat was cooked but was freshly obtained from young horses bought from Arabs, and was nevertheless effective. This helped to start the 19th-century tradition of horse meat consumption in France.

The last slaveship that traveled from West Africa to America in 1860, the "Clotilda", may have added vinegar to the water that was given to the captives. The last African survivor of that journey, Oluale Kossola (Cudjo Lewis), recalled in 1927: "Dey give us a little bit of water twice a day. Oh Lor’, Lor’, we so thirst! De water taste sour."

Lauchlin Rose patented a method used to preserve citrus juice without alcohol in 1867, creating a concentrated drink known as Rose's lime juice. The Merchant Shipping Act of 1867 required all ships of the Royal Navy and Merchant Navy to provide a daily lime ration to sailors to prevent scurvy. The product became nearly ubiquitous, hence the term "limey", first for British sailors, then for English immigrants within the former British colonies (particularly America, New Zealand and South Africa), and finally, in old American slang, all British people.

The plant "Cochlearia officinalis", also known as "Common Scurvygrass", acquired its common name from the observation that it cured scurvy, and it was taken on board ships in dried bundles or distilled extracts. Its very bitter taste was usually disguised with herbs and spices; however, this did not prevent scurvygrass drinks and sandwiches from becoming a popular fad in the UK until the middle of the nineteenth century, when citrus fruits became more readily available.

West Indian limes began to supplement lemons, when Spain's alliance with France against Britain in the Napoleonic Wars made the supply of Mediterranean lemons problematic, and because they were more easily obtained from Britain's Caribbean colonies and were believed to be more effective because they were more acidic. It was the acid, not the (then-unknown) Vitamin C that was believed to cure scurvy. In fact, the West Indian limes were significantly lower in Vitamin C than the previous lemons and further were not served fresh but rather as lime juice, which had been exposed to light and air, and piped through copper tubing, all of which significantly reduced the Vitamin C. Indeed, a 1918 animal experiment using representative samples of the Navy and Merchant Marine's lime juice showed that it had virtually no antiscorbutic power at all.

The belief that scurvy was fundamentally a nutritional deficiency, best treated by consumption of fresh food, particularly fresh citrus or fresh meat, was not universal in the 19th and early 20th centuries, and thus sailors and explorers continued to suffer from scurvy into the 20th century. For example, the Belgian Antarctic Expedition of 1897–1899 became seriously affected by scurvy when its leader, Adrien de Gerlache, initially discouraged his men from eating penguin and seal meat.

In the Royal Navy's Arctic expeditions in the 19th century it was widely believed that scurvy was prevented by good hygiene on board ship, regular exercise, and maintaining the morale of the crew, rather than by a diet of fresh food. Navy expeditions continued to be plagued by scurvy even while fresh (not jerked or tinned) meat was well known as a practical antiscorbutic among civilian whalers and explorers in the Arctic. Even cooking fresh meat did not entirely destroy its antiscorbutic properties, especially as many cooking methods failed to bring all the meat to high temperature.

The confusion is attributed to a number of factors:
In the resulting confusion, a new hypothesis was proposed, following the new germ theory of disease – that scurvy was caused by ptomaine, a waste product of bacteria, particularly in tainted tinned meat.

Infantile scurvy emerged in the late 19th century because children were being fed pasteurized cow's milk, particularly in the urban upper class. While pasteurization killed bacteria, it also destroyed vitamin C. This was eventually resolved by supplementing with onion juice or cooked potatoes.

By the early 20th century, when Robert Falcon Scott made his first expedition to the Antarctic (1901–1904), the prevailing theory was that scurvy was caused by "ptomaine poisoning", particularly in tinned meat. However, Scott discovered that a diet of fresh meat from Antarctic seals cured scurvy before any fatalities occurred.

In 1907, an animal model which would eventually help to isolate and identify the "antiscorbutic factor" was discovered. Axel Holst and Theodor Frølich, two Norwegian physicians studying shipboard beriberi contracted by ship's crews in the Norwegian Fishing Fleet, wanted a small test mammal to substitute for the pigeons then used in beriberi research. They fed guinea pigs their test diet of grains and flour, which had earlier produced beriberi in their pigeons, and were surprised when classic scurvy resulted instead. This was a serendipitous choice of animal. Until that time, scurvy had not been observed in any organism apart from humans and had been considered an exclusively human disease. Certain birds, mammals, and fish are susceptible to scurvy, but pigeons are unaffected, since they can synthesize ascorbic acid internally. Holst and Frølich found they could cure scurvy in guinea pigs with the addition of various fresh foods and extracts. This discovery of an animal experimental model for scurvy, which was made even before the essential idea of "vitamins" in foods had been put forward, has been called the single most important piece of vitamin C research.

In 1915, New Zealand troops in the Gallipoli Campaign had a lack of vitamin C in their diet which caused many of the soldiers to contract scurvy. It is thought that scurvy is one of many reasons that the Allied attack on Gallipoli failed and had the soldiers had a proper diet, the outcome of WWI and the future of the Middle East might have looked very different.

Vilhjalmur Stefansson, an arctic explorer who had lived among the Inuit, proved that the all-meat diet they consumed did not lead to vitamin deficiencies. He participated in a study in New York's Bellevue Hospital in February 1928, where he and a companion ate only meat for a year while under close medical observation, yet remained in good health.

In 1927, Hungarian biochemist Szent-Györgyi isolated a compound he called "hexuronic acid". Szent-Györgyi suspected hexuronic acid, which he had isolated from adrenal glands, to be the antiscorbutic agent, but he could not prove it without an animal-deficiency model. In 1932, the connection between hexuronic acid and scurvy was finally proven by American researcher Charles Glen King of the University of Pittsburgh. King's laboratory was given some hexuronic acid by Szent-Györgyi and soon established that it was the sought-after anti-scorbutic agent. Because of this, hexuronic acid was subsequently renamed "ascorbic acid."

Rates of scurvy in most of the world are low. Those most commonly affected are malnourished people in the developing world and the homeless. There have been outbreaks of the condition in refugee camps. Case reports in the developing world of those with poorly healing wounds have occurred.

Notable human dietary studies of experimentally induced scurvy have been conducted on conscientious objectors during World War II in Britain and on Iowa state prisoner volunteers in the late 1960s. These studies both found that all obvious symptoms of scurvy previously induced by an experimental scorbutic diet with extremely low vitamin C content could be completely reversed by additional vitamin C supplementation of only 10 mg per day. In these experiments, no clinical difference was noted between men given 70 mg vitamin C per day (which produced blood levels of vitamin C of about 0.55 mg/dl, about of tissue saturation levels), and those given 10 mg per day (which produced lower blood levels). Men in the prison study developed the first signs of scurvy about 4 weeks after starting the vitamin C-free diet, whereas in the British study, six to eight months were required, possibly because the subjects were pre-loaded with a 70 mg/day supplement for six weeks before the scorbutic diet was fed.

Men in both studies, on a diet devoid or nearly devoid of vitamin C, had blood levels of vitamin C too low to be accurately measured when they developed signs of scurvy, and in the Iowa study, at this time were estimated (by labeled vitamin C dilution) to have a body pool of less than 300 mg, with daily turnover of only 2.5 mg/day.

The vast majority of animals and plants are able to synthesize vitamin C, through a sequence of enzyme-driven steps, which convert monosaccharides to vitamin C. However, some mammals have lost the ability to synthesize vitamin C, notably simians and tarsiers. These make up one of two major primate suborders, Haplorrhini and this group includes humans. The Strepsirrhini (non-tarsier prosimians) can make their own vitamin C, and these include lemurs, lorises, pottos, and galagos. Ascorbic acid is also not synthesized by at least two species of Caviidae, the capybara and the guinea pig. There are known species of birds and fish that do not synthesize their own Vitamin C. All species that do not synthesize ascorbate require it in the diet. Deficiency causes scurvy in humans, and somewhat similar symptoms in other animals.

Animals that can contract scurvy all lack the -gulonolactone oxidase (GULO) enzyme, which is required in the last step of vitamin C synthesis. The genomes of these species contain GULO as pseudogenes, which serve as insight into the evolutionary past of the species.

In babies, scurvy is sometimes referred to as Barlow's disease, named after Thomas Barlow, a British physician who described it in 1883. However, Barlow's disease or Barlow's syndrome may also refer to mitral valve prolapse, first described by John Brereton Barlow in 1966.



</doc>
<doc id="28267" url="https://en.wikipedia.org/wiki?curid=28267" title="Sydney Harbour Bridge">
Sydney Harbour Bridge

The Sydney Harbour Bridge is a steel through arch bridge across Sydney Harbour that carries rail, vehicular, bicycle, and pedestrian traffic between the Sydney central business district (CBD) and the North Shore. The dramatic view of the bridge, the harbour, and the nearby Sydney Opera House is an iconic image of Sydney, and Australia itself. The bridge is nicknamed "The Coathanger" because of its arch-based design.

Under the direction of Dr John Bradfield of the NSW Department of Public Works, the bridge was designed and built by British firm Dorman Long and Co Ltd of Middlesbrough and opened in 1932. The bridge's design was influenced by the Hell Gate Bridge in New York City. It is the sixth longest spanning-arch bridge in the world and the tallest steel arch bridge, measuring from top to water level. It was also the world's widest long-span bridge, at wide, until construction of the new Port Mann Bridge in Vancouver was completed in 2012.

The southern end of the bridge is located at Dawes Point in The Rocks area, and the northern end at Milsons Point in the lower North Shore area. There are six original lanes of road traffic through the main roadway, plus an additional two lanes of road traffic on its eastern side, using lanes that were formerly tram tracks. Adjacent to the road traffic, a path for pedestrian use runs along the eastern side of the bridge, whilst a dedicated path for bicycle use only runs along the western side; between the main roadway and the western bicycle path are two lanes used for railway tracks, servicing the T1 North Shore Line for Sydney Trains.

The main roadway across the bridge is known as the Bradfield Highway and is about long, making it one of the shortest highways in Australia.

The arch is composed of two 28-panel arch trusses; their heights vary from at the centre of the arch to at the ends next to the pylons.

The arch has a span of and its summit is above mean sea level; expansion of the steel structure on hot days can increase the height of the arch by .

The total weight of the steelwork of the bridge, including the arch and approach spans, is , with the arch itself weighing . About 79% of the steel was imported from England, with the rest being sourced from . On site, the contractors (Dorman Long and Co.) set up two workshops at Milsons Point, at the site of the present day Luna Park, and fabricated the steel into the girders and other required parts.

The bridge is held together by six million Australian-made hand-driven rivets supplied by the McPherson company of Melbourne, the last being driven through the deck on 21 January 1932. The rivets were heated red-hot and inserted into the plates; the headless end was immediately rounded over with a large pneumatic rivet gun. The largest of the rivets used weighed and was long. The practice of riveting large steel structures, rather than welding, was, at the time, a proven and understood construction technique, whilst structural welding had not at that stage been adequately developed for use on the bridge.

At each end of the arch stands a pair of high concrete pylons, faced with granite. The pylons were designed by the Scottish architect Thomas S. Tait, a partner in the architectural firm John Burnet & Partners.

Some 250 Australian, Scottish, and Italian stonemasons and their families relocated to a temporary settlement at Moruya, NSW, south of Sydney, where they quarried around of granite for the bridge pylons. The stonemasons cut, dressed, and numbered the blocks, which were then transported to Sydney on three ships built specifically for this purpose. The Moruya quarry was managed by John Gilmore, a Scottish stonemason who emigrated, with his young family to Australia in 1924, at the request of the project managers. The concrete used was also Australian-made and supplied from Railton, Tasmania.

Abutments at the base of the pylons are essential to support the loads from the arch and hold its span firmly in place, but the pylons themselves have no structural purpose. They were included to provide a frame for the arch panels and to give better visual balance to the bridge. The pylons were not part of the original design, and were only added to allay public concern about the structural integrity of the bridge.

Although originally added to the bridge solely for their aesthetic value, all four pylons have now been put to use. The south-eastern pylon contains a museum and tourist centre, with a 360° lookout at the top providing views across the harbour and city. The south-western pylon is used by the New South Wales Roads and Traffic Authority (RTA) to support its CCTV cameras overlooking the bridge and the roads around that area. The two pylons on the north shore include venting chimneys for fumes from the Sydney Harbour Tunnel, with the base of the southern pylon containing the RMS maintenance shed for the bridge, and the base of the northern pylon containing the traffic management shed for tow trucks and safety vehicles used on the bridge.

In 1942 the pylons were modified to include parapets and anti-aircraft guns designed to assist in both Australia's defence and general war effort.The top level of stonework was never removed.

There had been plans to build a bridge as early as 1815, when convict and noted architect Francis Greenway reputedly proposed to Governor Lachlan Macquarie that a bridge be built from the northern to the southern shore of the harbour. In 1825, Greenway wrote a letter to the then "The Australian" newspaper stating that such a bridge would "give an idea of strength and magnificence that would reflect credit and glory on the colony and the Mother Country".

Nothing came of Greenway's suggestions, but the idea remained alive, and many further suggestions were made during the nineteenth century. In 1840, naval architect Robert Brindley proposed that a floating bridge be built. Engineer Peter Henderson produced one of the earliest known drawings of a bridge across the harbour around 1857. A suggestion for a truss bridge was made in 1879, and in 1880 a high-level bridge estimated at $850,000 was proposed.

In 1900, the Lyne government committed to building a new Central railway station and organised a worldwide competition for the design and construction of a harbour bridge. Local engineer Norman Selfe submitted a design for a suspension bridge and won the second prize of £500. In 1902, when the outcome of the first competition became mired in controversy, Selfe won a second competition outright, with a design for a steel cantilever bridge. The selection board were unanimous, commenting that, "The structural lines are correct and in true proportion, and... the outline is graceful". However due to an economic downturn and a change of government at the 1904 NSW State election construction never began.
A unique three-span bridge was proposed in 1922 by Ernest Stowe with connections at Balls Head, Millers Point, and Balmain with a memorial tower and hub on Goat Island.

In 1914 John Bradfield was appointed "Chief Engineer of Sydney Harbour Bridge and Metropolitan Railway Construction", and his work on the project over many years earned him the legacy as the "father" of the bridge. Bradfield's preference at the time was for a cantilever bridge without piers, and in 1916 the NSW Legislative Assembly passed a bill for such a construction, however it did not proceed as the Legislative Council rejected the legislation on the basis that the money would be better spent on the war effort.

Following World War I, plans to build the bridge again built momentum. Bradfield persevered with the project, fleshing out the details of the specifications and financing for his cantilever bridge proposal, and in 1921 he travelled overseas to investigate tenders. On return from his travels Bradfield decided that an arch design would also be suitable and he and officers of the NSW Department of Public Works prepared a general design for a single-arch bridge based upon New York City's Hell Gate Bridge. In 1922 the government passed the Sydney Harbour Bridge Act No. 28, specifying the construction of a high-level cantilever or arch bridge across the harbour between Dawes Point and Milsons Point, along with construction of necessary approaches and electric railway lines, and worldwide tenders were invited for the project.

As a result of the tendering process, the government received twenty proposals from six companies; on 24 March 1924 the contract was awarded to British firm Dorman Long and Co Ltd, of Middlesbrough well known as the contractors who built the similar Tyne Bridge of Newcastle Upon Tyne, for an arch bridge at a quoted price of AU£4,217,721 11s 10d. The arch design was cheaper than alternative cantilever and suspension bridge proposals, and also provided greater rigidity making it better suited for the heavy loads expected.

Bradfield and his staff were ultimately to oversee the entire bridge design and building process, while Dorman Long and Co's Consulting Engineer, Sir Ralph Freeman of Sir Douglas Fox and Partners, and his associate Mr. G.C. Imbault, carried out the detailed design and erection process of the bridge. Architects for the contractors were from the British firm John Burnet & Partners of Glasgow, Scotland.

The building of the bridge coincided with the construction of a system of underground railways in Sydney's CBD, known today as the City Circle, and the bridge was designed with this in mind. The bridge was designed to carry six lanes of road traffic, flanked on each side by two railway tracks and a footpath. Both sets of rail tracks were linked into the underground Wynyard railway station on the south (city) side of the bridge by symmetrical ramps and tunnels. The eastern-side railway tracks were intended for use by a planned rail link to the Northern Beaches; in the interim they were used to carry trams from the North Shore into a terminal within Wynyard station, and when tram services were discontinued in 1958, they were converted into extra traffic lanes. The Bradfield Highway, which is the main roadway section of the bridge and its approaches, is named in honour of Bradfield's contribution to the bridge.

The building of the bridge was under the management of Bradfield. Three other people heavily involved in the bridge's design and construction were Lawrence Ennis, Edward Judge, and Sir Ralph Freeman. Ennis was the engineer-in-charge at Dorman Long and Co and the main on-site supervisor (Bradfield visited occasionally throughout the project and, in particular, at many key stages of the project, to inspect progress and make managerial decisions), Judge was chief technical engineer of Dorman Long, and Freeman was hired by the company to design the accepted model in further detail. Later a bitter disagreement broke out between Bradfield and Freeman as to who actually designed the bridge.

The official ceremony to mark the "turning of the first sod" occurred on 28 July 1923, on the spot at Milsons Point on the north shore where two workshops to assist in building the bridge were to be constructed.

An estimated 469 buildings on the north shore, both private homes and commercial operations, were demolished to allow construction to proceed, with little or no compensation being paid. Work on the bridge itself commenced with the construction of approaches and approach spans, and by September 1926 concrete piers to support the approach spans were in place on each side of the harbour.

As construction of the approaches took place, work was also started on preparing the foundations required to support the enormous weight of the arch and loadings. Concrete and granite faced abutment towers were constructed, with the angled foundations built into their sides.

Once work had progressed sufficiently on the support structures, a giant "creeper crane" was erected on each side of the harbour. These cranes were fitted with a cradle, and then used to hoist men and materials into position to allow for erection of the steelwork. To stabilise works while building the arches, tunnels were excavated on each shore with steel cables passed through them and then fixed to the upper sections of each half-arch to stop them collapsing as they extended outwards.

Arch construction itself began on 26 October 1928. The southern end of the bridge was worked on ahead of the northern end, to detect any errors and to help with alignment. The cranes would "creep" along the arches as they were constructed, eventually meeting up in the middle. In less than two years, on Tuesday, 19 August 1930, the two halves of the arch touched for the first time. Workers riveted both top and bottom sections of the arch together, and the arch became self-supporting, allowing the support cables to be removed. On 20 August 1930 the joining of the arches was celebrated by flying the flags of Australia and the United Kingdom from the jibs of the creeper cranes.

Once the arch was completed, the creeper cranes were then worked back down the arches, allowing the roadway and other parts of the bridge to be constructed from the centre out. The vertical hangers were attached to the arch, and these were then joined with horizontal crossbeams. The deck for the roadway and railway were built on top of the crossbeams, with the deck itself being completed by June 1931, and the creeper cranes were dismantled. Rails for trains and trams were laid, and road was surfaced using concrete topped with asphalt. Power and telephone lines, and water, gas, and drainage pipes were also all added to the bridge in 1931.

The pylons were built atop the abutment towers, with construction advancing rapidly from July 1931. Carpenters built wooden scaffolding, with concreters and masons then setting the masonry and pouring the concrete behind it. Gangers built the steelwork in the towers, while day labourers manually cleaned the granite with wire brushes. The last stone of the north-west pylon was set in place on 15 January 1932, and the timber towers used to support the cranes were removed.

On 19 January 1932, the first test train, a steam locomotive, safely crossed the bridge. Load testing of the bridge took place in February 1932, with the four rail tracks being loaded with as many as 96 steam locomotives positioned end-to-end. The bridge underwent testing for three weeks, after which it was declared safe and ready to be opened. The construction worksheds were demolished after the bridge was completed, and the land that they were on is now occupied by Luna Park.

The standards of industrial safety during construction were poor by today's standards. Sixteen workers died during construction, but surprisingly only two from falling off the bridge. Several more were injured from unsafe working practices undertaken whilst heating and inserting its rivets, and the deafness experienced by many of the workers in later years was blamed on the project. Henri Mallard between 1930 and 1932 produced hundreds of stills and film footage which reveal at close quarters the bravery of the workers in tough Depression-era conditions.

Interviews were conducted between 1982-1989 with a variety of tradesmen who worked on the building of the bridge. Among the tradesmen interviewed were drillers, riveters, concrete packers, boilermakers, riggers, ironworkers, plasterers, stonemasons, an official photographer, sleepcutters, engineers and draughtsmen.

The total financial cost of the bridge was AU£6.25 million, which was not paid off in full until 1988.

The bridge was formally opened on Saturday, 19 March 1932. Amongst those who attended and gave speeches were the Governor of New South Wales, Sir Philip Game, and the Minister for Public Works, Lawrence Ennis. The Premier of New South Wales, Jack Lang, was to open the bridge by cutting a ribbon at its southern end.
However, just as Lang was about to cut the ribbon, a man in military uniform rode up on a horse, slashing the ribbon with his sword and opening the Sydney Harbour Bridge in the name of the people of New South Wales before the official ceremony began. He was promptly arrested. The ribbon was hurriedly retied and Lang performed the official opening ceremony and Game thereafter inaugurated the name of the bridge as 'Sydney Harbour Bridge' and the associated roadway as the 'Bradfield Highway'. After they did so, there was a 21-gun salute and an RAAF flypast. The intruder was identified as Francis de Groot. He was convicted of offensive behaviour and fined £5 after a psychiatric test proved he was sane, but this verdict was reversed on appeal. De Groot then successfully sued the Commissioner of Police for wrongful arrest, and was awarded an undisclosed out of court settlement. De Groot was a member of a right-wing paramilitary group called the New Guard, opposed to Lang's leftist policies and resentful of the fact that a member of the Royal Family had not been asked to open the bridge. De Groot was not a member of the regular army but his uniform allowed him to blend in with the real cavalry. This incident was one of several involving Lang and the New Guard during that year.

A similar ribbon-cutting ceremony on the bridge's northern side by North Sydney's mayor, Alderman Primrose, was carried out without incident. It was later discovered that Primrose was also a New Guard member but his role in and knowledge of the de Groot incident, if any, are unclear. The pair of golden scissors used in the ribbon cutting ceremonies on both sides of the bridge was also used to cut the ribbon at the dedication of the Bayonne Bridge, which had opened in Bayonne, New Jersey, close to New York City, the year before.

Despite the bridge opening in the midst of the Great Depression, opening celebrations were organised by the Citizens of Sydney Organising Committee, an influential body of prominent men and politicians that formed in 1931 under the chairmanship of the Lord Mayor to oversee the festivities. The celebrations included an array of decorated floats, a procession of passenger ships sailing below the bridge, and a Venetian Carnival. A message from a primary school in Tottenham, away in rural New South Wales, arrived at the bridge on the day and was presented at the opening ceremony. It had been carried all the way from Tottenham to the bridge by relays of school children, with the final relay being run by two children from the nearby Fort Street Boys' and Girls' schools. 
After the official ceremonies, the public was allowed to walk across the bridge on the deck, something that would not be repeated until the 50th anniversary celebrations. Estimates suggest that between 300,000 and one million people took part in the opening festivities, a phenomenal number given that the entire population of Sydney at the time was estimated to be 1,256,000.

There had also been numerous preparatory arrangements. On 14 March 1932, three postage stamps were issued to commemorate the imminent opening of the bridge. Several songs were composed for the occasion.

The bridge itself was regarded as a triumph over Depression times, earning the nickname "the Iron Lung", as it kept many Depression-era workers employed.

In 2010, the average daily traffic included 204 trains, 160,435 vehicles and 1650 bicycles.

From the Sydney CBD side, motor vehicle access to the bridge is normally via Grosvenor Street, Clarence Street, Kent Street, the Cahill Expressway, or the Western Distributor. Drivers on the northern side will find themselves on the Warringah Freeway, though it is easy to turn off the freeway to drive westwards into North Sydney or eastwards to Neutral Bay and beyond upon arrival on the northern side.

The bridge originally only had four wider traffic lanes occupying the central space which now has six, as photos taken soon after the opening clearly show. In 1958 tram services across the bridge were withdrawn and the tracks replaced by two extra road lanes; these lanes are now the leftmost southbound lanes on the bridge and are still clearly distinguishable from the other six road lanes. Lanes 7 and 8 now connect the bridge to the elevated Cahill Expressway that carries traffic to the Eastern Distributor.

In 1988, work began to build a tunnel to complement the bridge. It was determined that the bridge could no longer support the increased traffic flow of the 1980s. The Sydney Harbour Tunnel was completed in August 1992 and carries only motor vehicles.

The Bradfield Highway is designated as a Travelling Stock Route which means that it is permissible to herd livestock across the bridge, but only between midnight and dawn, and after giving notice of intention to do so. In practice, owing to the high-density urban nature of modern Sydney, and the relocation of abattoirs and markets, this has not taken place for approximately half a century.

The bridge is equipped for tidal flow operation, permitting the direction of traffic flow on the bridge to be altered to better suit the morning and evening rush hours' traffic patterns.

The bridge has eight lanes in total, numbered one to eight from west to east. Lanes three, four and five are reversible. One and two always flow north. Six, seven, and eight always flow south. The default is four each way. For the morning rush hour, the lane changes on the bridge also require changes to the Warringah Freeway, with its inner western reversible carriageway directing traffic to the bridge lane numbers three and four southbound.

The bridge has a series of overhead gantries which indicate the direction of flow for each traffic lane. A green arrow pointing down to a traffic lane means the lane is open. A flashing red "X" indicates the lane is closing, but is not yet in use for traffic travelling in the other direction. A static red "X" means the lane is in use for oncoming traffic. This arrangement was introduced in the 1990s, replacing a slow operation where lane markers were manually moved to mark the centre median.

It is possible to see odd arrangements of flow during night periods when maintenance occurs, which may involve completely closing some lanes. Normally this is done between midnight and dawn, because of the enormous traffic demands placed on the bridge outside these hours.

The vehicular traffic lanes on the bridge are operated as a toll road. As of 27 January 2009 there is a variable tolling system for all vehicles headed into the CBD (southbound). The toll paid is dependent on the time of day in which the vehicle passes through the toll plaza. The toll varies from a minimum value of $2.50 to a maximum value of $4. There is no toll for northbound traffic (though taxis travelling north may charge passengers the toll in anticipation of the toll the taxi must pay on the return journey). There are toll plazas at the northern and southern ends. The two eastern lanes (which continue over the Cahill Expressway at the southern end of the bridge) have their tollbooths at the northern end, while the other southbound lanes (for CBD traffic) are serviced by tollbooths at the southern end of the bridge. There is a bridge-long median strip between lanes 6 and 7 to separate traffic which has already paid the toll (at the northern end) from other southbound traffic (which must pay the toll at the southern end).

The toll was originally placed on travel across the bridge, in both directions, to recoup the cost of its construction. This cost was recovered in the 1980s, but the toll has been kept (indeed increased) by the state government's Roads and Traffic Authority to recoup the costs of the Sydney Harbour Tunnel.

After the decision to build the Sydney Harbour Tunnel was made in the early 1980s, the toll was increased (from 20 cents to $1, then to $1.50, and finally to $2 by the time the tunnel opened) to pay for its construction. The tunnel also had an initial toll of $2 southbound. After the increase to $1, the concrete barrier on the bridge separating the Bradfield Highway from the Cahill Expressway was increased in height, because of the large numbers of drivers crossing it illegally from lane 6 to 7, to avoid the toll. The toll for all southbound vehicles was increased to $3 in March 2004.

Originally it cost a car or motorcycle six pence to cross, a horse and rider being three pence. Use of the bridge by bicycle riders (provided that they use the cycleway) and by pedestrians is free. Later governments capped the fee for motorcycles at one-quarter of the passenger-vehicle cost, but now it is again the same as the cost for a passenger vehicle, although quarterly flat-fee passes are available which are much cheaper for frequent users.

In July 2008 a new electronic tolling system called e-TAG was introduced. The Sydney Harbour Tunnel was converted to this new tolling system while the Sydney Harbour Bridge itself had several cash lanes. The electronic system as of 12 January 2009 has now replaced all booths with E-tag lanes. In January 2017 work commenced to remove the southern toll booths.

The pedestrian-only footway is located on the east side of the bridge. Access from the northern side involves climbing an easily spotted flight of stairs, located on the east side of the bridge at Broughton St, Kirribilli. Pedestrian access on the southern side is more complicated, but signposts in the Rocks area now direct pedestrians to the long and sheltered flight of stairs that leads to the bridge's southern end. These stairs are located near Gloucester Street and Cumberland Street.

The bridge can also be approached from the south by accessing Cahill Walk, which runs along the Cahill Expressway. Pedestrians can access this walkway from the east end of Circular Quay by a flight of stairs or a lift. Alternatively it can be accessed from the Botanic Gardens.

The bike-only cycleway is located on the western side of the bridge. Access from the northern side involves carrying or pushing a bicycle up a staircase, consisting of 55 steps, located on the western side of the bridge at Burton St, Milsons Point. A wide smooth concrete strip in the centre of the stairs permits cycles to be wheeled up and down from the bridge deck whilst the rider is dismounted. A campaign to eliminate the steps on this popular cycling route to the CBD has been running since at least 2008. On 7 December 2016 the NSW Roads Minister Duncan Gay confirmed that the northern stairway would be replaced with a 20 million ramp alleviating the needs for cyclists to dismount. At the same time the NSW Government announced plans to upgrade the southern ramp at a projected cost of 20 million. Both projects are expected to completed by late 2020. Access to the cycleway on the southern side is via the northern end of the Kent Street cycleway and/or Upper Fort Street in The Rocks.

The bridge lies between Milsons Point and Wynyard railway stations, located on the north and south shores respectively, with two tracks running along the western side of the bridge. These tracks are part of the North Shore railway line.

In 1958 tram services across the bridge were withdrawn and the tracks they had used were removed and replaced by two extra road lanes; these lanes are now the leftmost southbound lanes on the bridge and are still clearly distinguishable from the other six road lanes. The original ramp that took the trams into their terminus at the underground Wynyard railway station is still visible at the southern end of the main walkway under lanes 7 and 8, although the tunnels have been converted into a car park and firing range.

The Sydney Harbour Bridge requires constant inspections and other maintenance work to keep it safe for the public, and to protect from corrosion. Among the trades employed on the bridge are painters, ironworkers, boilermakers, fitters, electricians, plasterers, carpenters, plumbers, and riggers.

The most noticeable maintenance work on the bridge involves painting. The steelwork of the bridge that needs to be painted is a combined , the equivalent of sixty football fields. Each coat on the bridge requires some of paint. A special fast-drying paint is used, so that any paint drops have dried before reaching the vehicles or bridge surface. One notable identity from previous bridge-painting crews is Australian comedian and actor Paul Hogan, who worked as a bridge painter before rising to media fame in the 1970s.

In 2003 the Roads & Traffic Authority began completely repainting the southern approach spans of the bridge. This involved removing the old lead-based paint, and repainting the of steel below the deck. Workers operated from self-contained platforms below the deck, with each platform having an air extraction system to filter airborne particles. An abrasive blasting was used, with the lead waste collected and safely removed from the site for disposal.
Between December 2006 and March 2010 the bridge was subject to works designed to ensure its longevity. The work included some strengthening.

Since 2013, two grit-blasting robots specially developed with the University of Technology, Sydney have been employed to help with the paint stripping operation on the bridge. The robots, nicknamed Rosie and Sandy, are intended to reduce workers' potential exposure to dangerous lead paint and asbestos and the blasting equipment which has enough force to cut through clothes and skin.

Even during its construction, the bridge was such a prominent feature of Sydney that it would attract tourist interest. One of the ongoing tourist attractions of the bridge has been the south-east pylon, which is accessed via the pedestrian walkway across the bridge, and then a climb to the top of the pylon of about 200 steps.

Not long after the bridge's opening, commencing in 1934, Archer Whitford first converted this pylon into a tourist destination. He installed a number of attractions, including a café, a camera obscura, an Aboriginal museum, a "Mother's Nook" where visitors could write letters, and a "pashometer". The main attraction was the viewing platform, where "charming attendants" assisted visitors to use the telescopes available, and a copper cladding (still present) over the granite guard rails identified the suburbs and landmarks of Sydney at the time.

The outbreak of World War II in 1939 saw tourist activities on the bridge cease, as the military took over the four pylons and modified them to include parapets and anti-aircraft guns.

In 1948 Yvonne Rentoul opened the "All Australian Exhibition" in the pylon. This contained dioramas, and displays about Australian perspectives on subjects such as farming, sport, transport, mining, and the armed forces. An orientation table was installed at the viewing platform, along with a wall guide and binoculars. The owner kept several white cats in a rooftop cattery, which also served as an attraction, and there was a souvenir shop and postal outlet. Rentoul's lease expired in 1971, and the pylon and its lookout remained closed to the public for over a decade.

The pylon was reopened in 1982, with a new exhibition celebrating the bridge's 50th anniversary. In 1987 a "Bicentennial Exhibition" was opened to mark the 200th anniversary of European settlement in Australia in 1988.

The pylon was closed from April to November 2000 for the Roads & Traffic Authority and BridgeClimb to create a new exhibition called "Proud Arch". The exhibition focussed on Bradfield, and included a glass direction finder on the observation level, and various important heritage items.

The pylon again closed for four weeks in 2003 for the installation of an exhibit called "Dangerous Works", highlighting the dangerous conditions experienced by the original construction workers on the bridge, and two stained glass feature windows in memory of the workers.

In 1950s and 1960s there were occasional newspaper reports of climbers who had made illegal arch traversals of the bridge, invariably by night. In 1973 Philippe Petit walked across a wire between the two pylons at the southern end of the Sydney Harbour Bridge. Since 1998, BridgeClimb has made it possible for tourists to legally climb the southern half of the bridge. Tours run throughout the day, from dawn to night, and are only cancelled for electrical storms or high wind.

Groups of climbers are provided with protective clothing appropriate to the prevailing weather conditions, and are given an orientation briefing before climbing. During the climb, attendees are secured to the bridge by a wire lifeline. Each climb begins on the eastern side of the bridge and ascends to the top. At the summit, the group crosses to the western side of the arch for the descent. Each climb takes three-and-a-half-hours, including the preparations.

In December 2006, BridgeClimb launched an alternative to climbing the upper arches of the bridge. The Discovery Climb allows climbers to ascend the lower chord of the bridge and view its internal structure. From the apex of the lower chord, climbers ascend a staircase to a platform at the summit.

Since the opening, the bridge has been the focal point of much tourism and national pride.

In 1982 the bridge celebrated the 50th anniversary of its opening. For the first time since its opening in 1932, the bridge was closed to most vehicles with the exception of vintage vehicles, and pedestrians were allowed full access for the day. The celebrations were attended by Edward Judge, who represented Dorman Long. 
Australia's bicentennial celebrations on 26 January 1988 attracted large crowds in the bridge's vicinity as merrymakers flocked to the foreshores to view the events on the harbour. The highlight was the biggest parade of sail ever held in Sydney, square-riggers from all over the world, surrounded by hundreds of smaller craft of every description, passing majestically under the Sydney Harbour Bridge. The day's festivities culminated in a fireworks display in which the bridge was the focal point of the finale, with fireworks streaming from the arch and roadway. This was to become the pattern for later firework displays.

The Harbour Bridge is an integral part of the Sydney New Year's Eve celebrations, generally being used in spectacular ways during the fireworks displays at 21:00 and midnight. In recent times, the bridge has included a ropelight display on a framework in the centre of the eastern arch, which is used to complement the fireworks. As the scaffolding and framework are clearly visible for some weeks before the event, revealing the outline of the design, there is much speculation as to how the effect is to be realised.

During the millennium celebrations in 2000, the Sydney Harbour Bridge was lit up with the word "Eternity", as a tribute to the legacy of Arthur Stace a Sydney artist who for many years inscribed that word on pavements in chalk in beautiful copperplate writing despite the fact that he was illiterate.

The effects have been as follows:

The numbers for the New Year's Eve countdown also appear on the eastern side of the Bridge pylons.

In May 2000 the bridge was closed to vehicular access for a day to allow a special reconciliation march—the "Walk for Reconciliation"—to take place. This was part of a response to an Aboriginal Stolen Generations inquiry, which found widespread suffering had taken place amongst Australian Aboriginal children forcibly placed into the care of white parents in a little-publicised state government scheme. Between 200,000 and 300,000 people were estimated to have walked the bridge in a symbolic gesture of crossing a divide.

During the Sydney 2000 Olympics in September and October 2000, the bridge was adorned with the Olympic Rings. It was included in the Olympic torch's route to the Olympic stadium. The men's and women's Olympic marathon events likewise included the bridge as part of their route to the Olympic stadium. A fireworks display at the end of the closing ceremony ended at the bridge. The east-facing side of the bridge has been used several times since as a framework from which to hang static fireworks, especially during the elaborate New Year's Eve displays.

In 2005 Mark Webber drove a Williams-BMW Formula One car across the bridge.

In 2007, the 75th anniversary of its opening was commemorated with an exhibition at the Museum of Sydney, called "Bridging Sydney". An initiative of the Historic Houses Trust, the exhibition featured dramatic photographs and paintings with rare and previously unseen alternative bridge and tunnel proposals, plans and sketches.

On 18 March 2007, the Sydney Harbour Bridge celebrated its 75th anniversary. The occasion was marked with a ribbon-cutting ceremony by the governor, Marie Bashir and the premier of New South Wales, Morris Iemma. The bridge was subsequently open to the public to walk southward from Milsons Point or North Sydney. Several major roads, mainly in the CBD, were closed for the day. An Aboriginal smoking ceremony was held at 19:00.

Approximately 250,000 people (50,000 more than were registered) took part in the event. Bright yellow souvenir caps were distributed to walkers. A series of speakers placed at intervals along the bridge formed a sound installation. Each group of speakers broadcast sound and music from a particular era (e.g. King Edward VIII's abdication speech; Gough Whitlam's speech at Parliament House in 1975), the overall effect being that the soundscape would "flow" through history as walkers proceeded along the bridge. A light-show began after sunset and continued late into the night, the bridge being bathed in constantly changing, multi-coloured lighting, designed to highlight structural features of the bridge. In the evening the bright yellow caps were replaced by orange caps with a small, bright LED attached. The bridge was closed to walkers at about 20:30.

On 25 October 2009 turf was laid across the eight lanes of bitumen, and 6,000 people celebrated a picnic on the bridge accompanied by live music. The event was repeated in 2010. Although originally scheduled again in 2011, this event was moved to Bondi Beach due to traffic concerns about the prolonged closing of the bridge.

On 19 March 2012 the 80th anniversary of the Sydney Harbour Bridge was celebrated with a picnic dedicated to the stories of people with personal connections to the bridge. In addition, Google dedicated its Google Doodle on the 19th to the event.
The proposal to upgrade the bridge tolling equipment was announced by the NSW Roads Minister Duncan Gay.


 
Webcams:

Images:


</doc>
<doc id="28269" url="https://en.wikipedia.org/wiki?curid=28269" title="Saving Private Ryan">
Saving Private Ryan

Saving Private Ryan is a 1998 American epic war film directed by Steven Spielberg and written by Robert Rodat. Set during the Invasion of Normandy in World War II, the film is notable for its graphic portrayal of war, and for the intensity of its opening 27 minutes, which includes a depiction of the Omaha Beach assault during the Normandy landings. It follows United States Army Rangers Captain John H. Miller (Tom Hanks) and a squad (Tom Sizemore, Edward Burns, Barry Pepper, Giovanni Ribisi, Vin Diesel, Adam Goldberg, and Jeremy Davies) as they search for a paratrooper, Private First Class James Francis Ryan (Matt Damon), who is the last-surviving brother of four servicemen.

The film was a significant critical and commercial success, grossing $216.8 million domestically, making it the highest-grossing film of 1998 in the United States, and $481.8 million worldwide, making it the second-highest-grossing film of 1998 worldwide. The film was nominated for 11 Academy Awards, including Best Picture, and won five, including Spielberg's second win for Best Director. "Saving Private Ryan" was released on home video in May 1999, earning another $44 million from sales.

Since its release, the film has been frequently lauded as one of the best war films ever made. In 2014, the film was selected for preservation in the National Film Registry by the Library of Congress, being deemed "culturally, historically, or aesthetically significant."

An elderly veteran visits the Normandy American Cemetery and Memorial with his family. At a tombstone, he falls to his knees with emotion. The scene then shifts to the morning of June 6, 1944, as American soldiers land on Omaha Beach as part of the Normandy Invasion. They suffer heavy losses in assaulting fortified German defensive positions. Captain Miller of the 2nd Ranger Battalion leads a breakout from the beach. Elsewhere on the beach, a dead soldier lies face-down in the bloody surf; his pack is stenciled "Ryan,S".

In Washington, D.C., at the U.S. War Department, General George Marshall learns that three of the four sons of the Ryan family were killed in action and that the fourth son, James, is with the 101st Airborne Division somewhere in Normandy. After reading Abraham Lincoln's Bixby letter aloud, Marshall orders Ryan brought home.

Three days after D-Day, Miller receives orders to find Ryan and bring him back. He chooses six men from his company—T/Sgt. Horvath, Privates First Class Reiben and Caparzo, Privates Mellish and Jackson, medic Wade—plus T/5 Upham, an interpreter from headquarters. They move out to Neuville, where they meet a squad of the 101st engaged against the enemy. Caparzo is killed by a German sniper who is then killed by Jackson. They locate a Private James Ryan but he is not the right one. From passing soldiers, Miller learns that Ryan is defending an important bridge in Ramelle.

Near Ramelle, Miller decides to neutralize a German machine gun position at a derelict radar station, despite his men's misgivings. Wade is killed in the skirmish. At Upham's urging, Miller declines to execute a surviving German soldier, and sets him free. Losing confidence in Miller's leadership, Reiben declares his intention to desert, prompting a confrontation with Horvath, which Miller defuses by disclosing his civilian career as a high school English teacher, about which his men had set up a betting pool. Reiben stays.

At Ramelle, Ryan is among a small group of paratroopers preparing to defend the key bridge. Miller tells Ryan that his brothers are dead, and that he was ordered to bring him home. Ryan is distressed about his brothers, but he will not leave the fight. Miller combines his unit with the paratroopers in defense of the bridge against the imminent German attack. Miller prepares to ambush the enemy with various .30 caliber guns, Molotov cocktails, detonation cords, anti-tank mines and impromptu satchel charges made from socks.

Elements of the 2nd SS Panzer Division arrive with two Tiger tanks and two Marder assault guns, all protected by foot soldiers. Although the Americans inflict heavy casualties on the Germans, including destroying one Tiger tank, both Marders and a 20 mm gun, most of the paratroopers, along with Jackson, Mellish and Horvath are killed, while Upham is immobilized by fear. Miller attempts to blow up the bridge, but is shot and mortally wounded by the freed German prisoner from the radar station, who had somehow rejoined a fighting unit. Miller crawls to retrieve the bridge detonator, and fires ineffectually with his pistol at the oncoming tank. As the tank reaches the bridge, an American P-51 Mustang flies overhead and destroys the tank, after which American armored units arrive to rout the remaining Germans. Having seen Miller get shot, Upham leaps out from hiding and confronts the German prisoner, shooting and killing him.

Reiben and Ryan are with Miller as he utters his last words, "James... earn this. Earn it."

The elderly veteran is revealed to be Ryan and the grave he is standing at is Miller’s. Ryan asks if he was worthy of such sacrifice. He salutes Miller's grave.

In 1994, Robert Rodat wrote the script for the film. Rodat's script was submitted to producer Mark Gordon, who liked it and in turn passed it along to Spielberg to direct. The film is loosely based on the World War II life stories of the Niland brothers. A shooting date was set for June 27, 1997.

In casting the film Spielberg sought to create a cast that "looked" the part, stating in an interview, "You know, the people in World War II actually looked different than people look today", adding to this end that he cast partly based on wanting the cast "to match the faces I saw on the newsreels." 

Before filming began, several of the film's stars, including Edward Burns, Barry Pepper, Vin Diesel, Adam Goldberg, Giovanni Ribisi, and Tom Hanks, endured ten days of "boot camp" training led by Marine veteran Dale Dye and Warriors, Inc., a California-based company that specializes in training actors for realistic military portrayals. Matt Damon was intentionally not brought into the camp, to make the rest of the group feel resentment towards the character. Spielberg had stated that his main intention in forcing the actors to go through the boot camp was not to learn the proper techniques but rather "because I wanted them to respect what it was like to be a soldier."

The film's second scene is a 20+ minute sequence recounting the landing on the beaches of Normandy. Spielberg chose to include this particularly violent sequence in order "to bring the audience onto the stage with me" specifically noting that he did not want the "audience to be spectators" but rather he wanted to "demand them to be participants with those kids who had never seen combat before in real life, and get to the top of Omaha Beach together."

Spielberg had already demonstrated his interest in World War II themes with the films "1941", "Empire of the Sun", "Schindler's List", and the "Indiana Jones" series. Spielberg later co-produced the World War II themed television miniseries "Band of Brothers" and its counterpart "The Pacific" with Tom Hanks. When asked about this by "American Cinematographer", Spielberg said, "I think that World War II is the most significant event of the last 100 years; the fate of the baby boomers and even Generation X was linked to the outcome. Beyond that, I've just always been interested in World War II. My earliest films, which I made when I was about 14 years old, were combat pictures that were set both on the ground and in the air. For years now, I've been looking for the right World War II story to shoot, and when Robert Rodat wrote "Saving Private Ryan", I found it."

Spielberg wanted an almost exact replica of the Omaha Beach landscape for the movie, including similar sand and a bluff similar to the one where German forces were stationed and a near match was found in Ireland. The D-Day scenes were shot in Ballinesker Beach, Curracloe Strand, Ballinesker, just east of Curracloe, County Wexford, Ireland. Hanks recalled to Roger Ebert that although he realized it was a movie, the experience still hit him hard, stating, "The first day of shooting the D-Day sequences, I was in the back of the landing craft, and that ramp went down and I saw the first 1-2-3-4 rows of guys just getting blown to bits. In my head, of course, I knew it was special effects, but I still wasn't prepared for how tactile it was." Filming began June 27, 1997, and lasted for two months. Some shooting was done in Normandy, for the Normandy American Cemetery and Memorial in Colleville-sur-Mer and Calvados. Other scenes were filmed in England, such as a former British Aerospace factory in Hatfield, Hertfordshire, Thame Park, Oxfordshire and Wiltshire. Production was due to also take place in Seaham, County Durham, but government restrictions disallowed this.

"Saving Private Ryan" has received critical acclaim for its realistic portrayal of World War II combat. In particular, the sequence depicting the Omaha Beach landings was named the "best battle scene of all time" by "Empire" magazine and was ranked number one on "TV Guide's" list of the "50 Greatest Movie Moments". The scene cost US$12 million and involved up to 1,500 extras, some of whom were members of the Irish Reserve Defence Forces. Members of local reenactment groups such as the Second Battle Group were cast as extras to play German soldiers. In addition, twenty to thirty actual amputees were used to portray American soldiers maimed during the landing. Spielberg did not storyboard the sequence, as he wanted spontaneous reactions and for "the action to inspire me as to where to put the camera".

The historical representation of Charlie Company's actions, led by its commander, Captain Ralph E. Goranson, was well maintained in the opening sequence. The sequence and details of the events are very close to the historical record, including the sea sickness experienced by many of the soldiers as the landing craft moved toward the shoreline, significant casualties among the men as they disembarked from the boats, and difficulty linking up with adjacent units on the shore.

The distinctive signature "ping" of the US soldiers' M1 Garand rifles ejecting their ammunition clips is heard throughout the battle sequence. The contextual details of the Company's actions were well maintained, for instance, the correct code names for the sector Charlie Company assaulted, and adjacent sectors, were used. Included in the cinematic depiction of the landing was a follow-on mission of clearing a bunker and trench system at the top of the cliffs which was not part of the original mission objectives for Charlie Company, but which they did undertake after the assault on the beach.

The landing craft used included twelve actual World War II examples, 10 LCVPs and 2 LCMs, standing in for the British LCAs that the Ranger Companies rode in to the beach during Operation Overlord. The filmmakers used underwater cameras to better depict soldiers being hit by bullets in the water. Forty barrels of fake blood were used to simulate the effect of blood in the seawater. This degree of realism was more difficult to achieve when depicting World War II German armored vehicles, as few examples survive in operating condition. The Tiger I tanks in the film were copies built on the chassis of old, but functional, Soviet T-34 tanks. The two vehicles described in the film as Panzers were meant to portray Marder III tank destroyers. One was created for the film using the chassis of a Czech-built Panzer 38(t) tank similar to the construction of the original Marder III; the other was a cosmetically modified Swedish SAV m/43 assault gun, which also used the 38(t) chassis.

There are, however, historical inaccuracies in the film's depiction of the Normandy campaign. At the time of the mission, American forces from the two American beach areas, Utah and Omaha, had not yet linked up. In reality, a Ranger team operating out of the Omaha beach area would have had to move through the heavily enemy-occupied city of Carentan, or swim or boat across the estuary linking Carentan to the channel, or transfer by boat to the Utah landing area. On the other hand, US forces moving out of Utah would have had direct and much shorter routes, relatively unencumbered by enemy positions, and were already in contact with some teams from both US airborne divisions landed in the area.

The Utah beach landings, however, were relatively uncontested, with assault units landing on largely unoccupied beaches and experiencing far less action than the landings at Omaha. The filmmakers chose to begin the narrative with a depiction of the more dramatic story of Omaha, despite the strategic inaccuracy of an impossible mission that could easily have been pursued from the other beach area. In addition, one of the most notable of the operational flaws is the depiction of the 2nd SS Panzer Division Das Reich as the adversary during the fictional Battle of Ramelle. The 2nd SS was not engaged in Normandy until July, and then at Caen against the British and Canadians, 100 miles east. Furthermore, the Merderet River bridges were not an objective of the 101st Airborne Division but of the 82nd Airborne Division, part of Mission Boston.

Much has also been said about various "tactical errors" made by both the German and American forces in the film's climactic battle. Spielberg responded, saying that in many scenes he opted to replace sound military tactics and strict historical accuracy for dramatic effect. Some other technical errors were also made, often censored, including the mistaken reversed orientation of the beach barriers; the tripod obstructions with a mine at the apex.

To achieve a tone and quality that was true to the story as well as reflected the period in which it is set, Spielberg once again collaborated with cinematographer Janusz Kamiński, saying, "Early on, we both knew that we did not want this to look like a Technicolor extravaganza about World War II, but more like color newsreel footage from the 1940s, which is very desaturated and low-tech." Kamiński had the protective coating stripped from the camera lenses, making them closer to those used in the 1940s. He explains that "without the protective coating, the light goes in and starts bouncing around, which makes it slightly more diffused and a bit softer without being out of focus." The cinematographer completed the overall effect by putting the negative through bleach bypass, a process that reduces brightness and color saturation. The shutter timing was set to 90 or 45 degrees for many of the battle sequences, as opposed to the standard of 180-degree timing. Kamiński clarifies, "In this way, we attained a certain staccato in the actors' movements and a certain crispness in the explosions, which makes them slightly more realistic."

"Saving Private Ryan" was a critical and commercial success and is credited with contributing to a resurgence in America's interest in World War II. Old and new films, video games, and novels about the war enjoyed renewed popularity after its release. The film's use of desaturated colors, hand-held cameras, and tight angles has profoundly influenced subsequent films and video games. "Saving Private Ryan" was released in 2,463 theaters on July 24, 1998, and grossed $30.5 million on its opening weekend. The film grossed $216.5 million in the US and Canada. and $265.3 million in other territories, bringing its worldwide total to $481.8 million and making it the highest-grossing US film of the year. Box Office Mojo estimates that the film sold over 45.74 million tickets in the United States and Canada.

The film received critical acclaim and has a 'certified fresh' rating of 92% on Rotten Tomatoes based on 132 reviews with an average score of 8.6/10. The consensus states "Anchored by another winning performance from Hanks, Spielberg's unflinchingly realistic war film virtually redefines the genre." The film also has a score of 90 out of 100 on Metacritic based on 35 critic reviews indicating "universal acclaim".

Much of the praise went for the realistic battle scenes and the actors' performances. However, it did earn some criticism for ignoring the contributions of several other countries to the D-Day landings in general and at Omaha Beach specifically. The most direct example of the latter is that during the actual landing the 2nd Rangers disembarked from British ships and were taken to Omaha Beach by Royal Navy landing craft (LCAs). The film depicts them as being United States Coast Guard-crewed craft (LCVPs and LCMs) from an American ship, the . This criticism was far from universal with other critics recognizing the director's intent to make an "American" film. The film was not released in Malaysia after Spielberg refused to cut the violent scenes; however, the film was finally released there on DVD with an 18SG certificate much later in 2005. Many critics associations, such as New York Film Critics Circle and Los Angeles Film Critics Association, chose "Saving Private Ryan" as Film of the Year. Roger Ebert gave it four stars out of four and called it "a powerful experience". Janet Maslin of "The New York Times" called it "the finest war movie of our time."

Filmmaker Robert Altman wrote a letter to Spielberg stating, ""Private Ryan" was awesome — best I've seen." Filmmaker Quentin Tarantino has expressed admiration for the film and has cited it as an influence on his 2009 film, "Inglourious Basterds".

Many World War II veterans stated that the film was the most realistic depiction of combat they had ever seen. The film was so realistic that combat veterans of D-Day and Vietnam left theaters rather than finish watching the opening scene depicting the Normandy invasion. Their visits to posttraumatic stress disorder counselors rose in number after the film's release, and many counselors advised "'more psychologically vulnerable'" veterans to avoid watching it. The Department of Veterans Affairs set up a nationwide hotline for veterans who were affected by the film, and less than two weeks after the film was released it had already received over 170 calls.

The film has gained criticism and negative reviews from some war veterans and film critics. Film director and military veteran Oliver Stone has accused the film of promoting "the worship of World War II as the good war," and has placed it alongside films such as "Gladiator" and "Black Hawk Down" that he believes were well-made, but may have inadvertently contributed to Americans' readiness for the 2003 invasion of Iraq. In defense of the film's portrait of warfare, Brian De Palma commented, "The level of violence in something like "Saving Private Ryan" makes sense because Spielberg is trying to show something about the brutality of what happened." Actor Richard Todd, who performed in "The Longest Day" and was amongst the first of the Allied soldiers to land in Normandy (Operation Tonga), said the film was "Rubbish. Overdone." American academic Paul Fussell, who saw combat in France during World War II, objected to what he described as, "the way Spielberg's "Saving Private Ryan", after an honest, harrowing, 15-minute opening visualizing details of the unbearable bloody mess at Omaha Beach, degenerated into a harmless, uncritical patriotic performance apparently designed to thrill 12-year-old boys during the summer bad-film season. Its genre was pure cowboys and Indians, with the virtuous cowboys of course victorious." Historian James DiEugenio has noted that the film is actually "90 percent fiction" and that Tom Hanks knew this, with his goal being to "...commemorate World War II as the Good War and to depict the American role in it as crucial."

The film was nominated for eleven Academy Awards, and won five including Best Cinematography, Best Sound Mixing, Best Sound Effects Editing, Best Film Editing, and Best Director for Spielberg, but lost the Best Picture award to "Shakespeare in Love", being one of a few that have won the Best Director award without also winning Best Picture. The Academy's decision to not award the film with the Best Picture Oscar has resulted in much criticism in recent years, with many considering it as one of the biggest snubs in the ceremony's history.

The film also won the Golden Globes for Best Motion Picture – Drama and Director, the BAFTA Award for Special Effects and Sound, the Directors Guild of America Award, a Grammy Award for Best Film Soundtrack, the Producers Guild of America Golden Laurel Award, and the Saturn Award for Best Action, Adventure, or Thriller Film. The American Film Institute has included "Saving Private Ryan" in many of its lists, ranking it as the 71st greatest American movie in AFI's 100 Years...100 Movies (10th Anniversary Edition), as well as the 45th most thrilling film in AFI's 100 Years...100 Thrills, the 10th most inspiring in AFI's 100 Years...100 Cheers, and the eighth best epic film in "AFI's 10 Top 10".

On Veterans Day from 2001–2004, the American Broadcasting Company aired the film uncut and with limited commercial interruption. The network airings were given a TV-MA rating, as the violent battle scenes and the profanity were left intact. The 2004 airing was marred by pre-emptions in many markets because of the language, in the backlash of Super Bowl XXXVIII's halftime show controversy. However, critics and veterans' groups such as the American Legion and the Veterans of Foreign Wars assailed those stations and their owners, including Hearst-Argyle Television (owner of 12 ABC affiliates); Scripps Howard Broadcasting (owner of six); and Belo (owner of four) for putting profits ahead of programming and honoring those who gave their lives at wartime, saying the stations made more money running their own programming instead of being paid by the network to carry the film, especially during a sweeps period.

A total of 65 ABC affiliates—28% of the network—did not clear the available timeslot for the film, even with the offer of The Walt Disney Company, ABC's parent, to pay all fines for language to the Federal Communications Commission. In the end, however, no complaints were lodged against ABC affiliates who showed "Ryan", perhaps because even conservative watchdogs like the Parents Television Council supported the unedited rebroadcast of the film. Additionally, some ABC affiliates in other markets that were near affected markets, such as Youngstown, Ohio, ABC affiliate WYTV (which is viewable in parts of the Columbus, Cleveland, and Pittsburgh markets, none of which aired the film) and Gainesville, Florida, ABC affiliate WCJB-TV (which is viewable in parts of the Orlando and Tampa markets), still aired the film and gave those nearby markets the option of viewing the film. TNT and Turner Classic Movies have also broadcast the film.

The film was released on home video in May 1999 with a VHS release that earned over $44 million. The DVD release became available in November of the same year, and was one of the best-selling titles of the year, with over 1.5 million units sold. The DVD was released in two separate versions: one with Dolby Digital and the other with DTS 5.1 surround sound. Besides the different 5.1 tracks, the two DVDs are identical. The film was also issued in a limited 2-disc LaserDisc in November 1999, making it one of the last feature films to be issued in this format, as LaserDiscs ceased manufacturing and distribution by year's end.

In 2004, a "Saving Private Ryan" special edition DVD was released to commemorate the 60th anniversary of D-Day. This two-disc edition was also included in a box set titled "World War II Collection", along with two documentaries produced by Spielberg, "Price For Peace" (about the Pacific War) and "Shooting War" (about war photographers, narrated by Tom Hanks). The film was released on Blu-ray Disc on April 26, 2010 in the UK and on May 4, 2010 in the US, as part of Paramount Home Video's premium Sapphire Series. However, only weeks after its release, Paramount issued a recall due to audio synchronization problems. The studio issued an official statement acknowledging the problem, which they attributed to an authoring error by Technicolor that escaped the quality control process, and that they had already begun the process of replacing the defective discs.

On May 8, 2018, Paramount Home Media Distribution released Saving Private Ryan on Ultra HD Blu-ray to celebrate the 20th anniversary of the release of the film.





</doc>
<doc id="28270" url="https://en.wikipedia.org/wiki?curid=28270" title="Shaggy dog story">
Shaggy dog story

In its original sense, a shaggy dog story or yarn is an extremely long-winded anecdote characterized by extensive narration of typically irrelevant incidents and terminated by an anticlimax or a pointless punchline.

Shaggy dog stories play upon the audience's preconceptions of joke-telling. The audience listens to the story with certain expectations, which are either simply not met or met in some entirely unexpected manner. A lengthy shaggy dog story derives its humour from the fact that the joke-teller held the attention of the listeners for a long time (such jokes can take five minutes or more to tell) for no reason at all, as the end resolution is essentially meaningless. The nature of their delivery is reflected in the English idiom "spin a yarn", by way of analogy with the production of yarn. 

The commonly believed archetype of the shaggy dog story is a story that concerns a shaggy dog. The story builds up, repeatedly emphasizing how shaggy the dog is. At the climax of the story, someone in the story reacts with, "That dog's not so shaggy." The expectations of the audience that have been built up by the presentation of the story, that the story will end with a punchline, are thus disappointed. Ted Cohen gives the following example of this story:

However, authorities disagree as to whether this particular story is the archetype after which the category is named. Eric Partridge, for example, provides a very different story, as do William and Mary Morris in "The Morris Dictionary of Word and Phrase Origins".

According to Partridge and the Morrises, the archetypical shaggy dog story involves an advertisement placed in the "Times" announcing a search for a shaggy dog. In the Partridge story, an aristocratic family living in Park Lane is searching for a lost dog, and an American answers the advertisement with a shaggy dog that he has found and personally brought across the Atlantic, only to be received by the butler at the end of the story who takes one look at the dog and shuts the door in his face, saying, "But not so shaggy as "that", sir!" In the Morris story, the advertiser is organizing a competition to find the shaggiest dog in the world, and after a lengthy exposition of the search for such a dog, a winner is presented to the aristocratic instigator of the competition, who says, "I don't think he's so shaggy."

A typical shaggy dog story occurs in Mark Twain's book about his travels west, "Roughing It". Twain's friends encourage him to go find a man called Jim Blaine when he is properly drunk, and ask him to tell "the stirring story about his grandfather's old ram." Twain, encouraged by his friends who have already heard the story, finally finds Blaine, an old silver miner, who sets out to tell Twain and his friends the tale. Blaine starts out with the ram ("There never was a bullier old ram than what he was"), and goes on for four more mostly dull but occasionally hilarious unparagraphed pages. Along the way, Blaine tells many stories, each of which connects back to the one before by some tenuous thread, and none of which has to do with the old ram. Among these stories are: a tale of boiled missionaries; of a lady who borrows a false eye, a peg leg, and the wig of a coffin-salesman's wife; and a final tale of a man who gets caught in machinery at a carpet factory and whose "widder bought the piece of carpet that had his remains wove in..." As Blaine tells the story of the carpet man's funeral, he begins to fall asleep, and Twain, looking around, sees his friends "suffocating with suppressed laughter." They now inform him that "at a certain stage of intoxication, no human power could keep [Blaine] from setting out, with impressive unction, to tell about a wonderful adventure which he had once had with his grandfather's old ram — and the mention of the ram in the first sentence was as far as any man had heard him get, concerning it."

"Buy Jupiter and Other Stories", a collection of stories by Isaac Asimov, contains a tale whose title is "Shah Guido G." In his background notes, Asimov defines the tale as a shaggy dog story, and explains that the title is a play on "shaggy dog".




The actor Norm MacDonald is famous for telling shaggy dog stories.




</doc>
<doc id="28271" url="https://en.wikipedia.org/wiki?curid=28271" title="Sushi">
Sushi

Sushi is traditionally made with medium-grain white rice, though it can be prepared with brown rice. It is often prepared with seafood, such as calamari, eel, or imitation crab meat. Many others are vegetarian. Sushi is often served with pickled ginger (gari), wasabi, and soy sauce. Daikon radish is popular as a garnish.

Sushi is sometimes confused with "sashimi", a related Japanese dish consisting of thinly sliced raw fish, or occasionally meat, and an optional serving of rice.

Sushi originates in a Southeast Asian dish, known today as "narezushi" ( – "salted fish"), stored in fermented rice for possibly months at a time. The lacto-fermentation of the rice prevented the fish from spoiling; the rice would be discarded before consumption of the fish. This early type of sushi became an important source of protein for its Japanese consumers. The term "sushi" comes from an antiquated grammatical form no longer used in other contexts, and literally means "sour-tasting"; the overall dish has a sour and umami or savoury taste. Narezushi still exists as a regional specialty, notably as "funa-zushi" from Shiga Prefecture.

Vinegar began to be added to the preparation of narezushi in the Muromachi period (1336–1573) for the sake of enhancing both taste and preservation. In addition to increasing the sourness of the rice, the vinegar significantly increased the dish's longevity, causing the fermentation process to be shortened and eventually abandoned. The primitive sushi would be further developed in Osaka, where over several centuries it became "oshi-zushi" or "hako-zushi"; in this preparation, the seafood and rice were pressed into shape with wooden (typically bamboo) molds.

It was not until the Edo period (1603-1868) that fresh fish was served over vinegared rice. The particular style of today's "nigirizushi" became popular in Edo (contemporary Tokyo) in the 1820s or 1830s. One common story of "nigirizushi"'s origins is of the chef Hanaya Yohei (1799-1858), who invented or perfected the technique in 1824 at his shop in Ryōgoku. The dish was originally termed "Edomae zushi" as it used freshly caught fish from the "Edo-mae" (Edo or Tokyo Bay); the term "Edomae nigirizushi" is still used today as a by-word for quality sushi, regardless of its ingredients' origins.

The "Oxford English Dictionary" mistakenly notes the earliest written mention of sushi in English in an 1893 book, "A Japanese Interior", where it mentions sushi as "a roll of cold rice with fish, sea-weed, or some other flavoring". However, there is an earlier mention of sushi in James Hepburn's Japanese-English dictionary from 1873, and an 1879 article on Japanese cookery in the journal "Notes and Queries".

The common ingredient in all types of sushi is vinegared "sushi rice". Fillings, toppings, condiments, and preparation vary widely.

Note that due to rendaku consonant mutation, sushi is spelled with "zu" instead of "su" whenever a prefix is attached, such as in nigirizushi for instance.

"Chirashizushi" (, "scattered sushi", also referred to as "barazushi") serves the rice in a bowl and tops it with a variety of raw fish and vegetable garnishes. It is commonly eaten because it is filling, fast and easy to make. It is eaten annually on Hinamatsuri in March.

Inarizushi () is a pouch of fried tofu typically filled with sushi rice alone.

Tales tell that inarizushi is named after the Shinto god "Inari". Foxes, messengers of Inari are believed to have a fondness for fried tofu, and an Inari-zushi roll has pointed corners that resemble fox ears.

Regional variations include pouches made of a thin omelette (, "fukusa-zushi", or , "chakin-zushi") instead of tofu. It should not be confused with "inari maki", which is a roll filled with flavored fried tofu.

Cone sushi is a variant of "inarizushi" originating in Hawaii that may include green beans, carrots, or gobo along with rice, wrapped in a triangular aburage piece. It is often sold in "okazu-ya" (Japanese delis) and as a component of bento boxes.

Makizushi (, "rolled sushi"), norimaki (, "Nori roll") or makimono (, "variety of rolls") is a cylindrical piece, formed with the help of a bamboo mat known as a "makisu" (). "Makizushi" is generally wrapped in nori (seaweed), but is occasionally wrapped in a thin omelette, soy paper, cucumber, or shiso (perilla) leaves. "Makizushi" is usually cut into six or eight pieces, which constitutes a single roll order. Below are some common types of "makizushi", but many other kinds exist.
Futomaki (, "thick, large or fat rolls") is a large cylindrical piece, usually with "nori" on the outside. A typical "futomaki" is five to six centimeters (2–2.5 in) in diameter. They are often made with two, three, or more fillings that are chosen for their complementary tastes and colors. During the evening of the Setsubun festival, it is traditional in the Kansai region to eat uncut futomaki in its cylindrical form, where it is called "ehō-maki" (, lit. happy direction rolls). By 2000 the custom had spread to all of Japan. Futomaki are often vegetarian, and may utilize strips of cucumber, "kampyō" gourd, "takenoko" bamboo shoots, or lotus root. Strips of "tamagoyaki" omelette, tiny fish roe, chopped tuna, and "oboro (food)" whitefish flakes are typical non-vegetarian fillings. Traditionally, the rice is lightly seasoned with salt and sesame oil/perilla oil. Popular protein ingredients are fish cakes, imitation crab meat, eggs, or seasoned beef rib-eye. Vegetables usually include cucumbers, spinach, carrot and (pickled radish). After the makizushi has been rolled and sliced, it is typically served with takuan.

Short grain white rice is usually used, although short-grain brown rice, like olive oil on nori, is now becoming more widespread among the health-conscious. Rarely, sweet rice is mixed in makizushi rice.
Nowadays, the rice in makizushi can be many kinds of black rice, boiled rice and cereals etc.Besides the common ingredients listed above, some varieties may include cheese, spicy cooked squid, yakiniku, kamaboko, lunch meat, sausage, bacon or spicy tuna. The nori may be brushed with sesame oil or sprinkled with sesame seeds. In a variation, sliced pieces of makizushi may be lightly fried with egg coating.
Tamago makizushi (玉子巻き寿司) is makizushi is rolled out by a thin egg. Tempura Makizushi (天ぷら 巻き寿司) or Agezushi (揚げ寿司ロール) is a fried version of the dish.

Hosomaki (, "thin rolls") is a small cylindrical piece, with "nori" on the outside. A typical "hosomaki" has a diameter of about two and a half centimeters (1 in). They generally contain only one filling, often tuna, cucumber, "kanpyō", thinly sliced carrots, or, more recently, avocado. "Kappamaki", () a kind of "Hosomaki" filled with cucumber, is named after the Japanese legendary water imp fond of cucumbers called the kappa. Traditionally, "kappamaki" is consumed to clear the palate between eating raw fish and other kinds of food, so that the flavors of the fish are distinct from the tastes of other foods. "Tekkamaki" () is a kind of "hosomaki" filled with raw tuna. Although it is believed that the word "tekka", meaning "red hot iron", alludes to the color of the tuna flesh or salmon flesh, it actually originated as a quick snack to eat in gambling dens called "tekkaba" (), much like the sandwich. "Negitoromaki" () is a kind of "hosomaki" filled with scallion ("negi") and chopped tuna ("toro"). Fatty tuna is often used in this style. "Tsunamayomaki" () is a kind of "hosomaki" filled with canned tuna tossed with mayonnaise.

Temaki (, "hand roll") is a large cone-shaped piece of "nori" on the outside and the ingredients spilling out the wide end. A typical "temaki" is about ten centimeters (4 in) long, and is eaten with fingers because it is too awkward to pick it up with chopsticks. For optimal taste and texture, "temaki" must be eaten quickly after being made because the "nori" cone soon absorbs moisture from the filling and loses its crispness, making it somewhat difficult to bite through. For this reason, the "nori" in pre-made or take-out temaki is sealed in plastic film which is removed immediately before eating.

"Narezushi" (, "matured sushi") is a traditional form of fermented sushi. Skinned and gutted fish are stuffed with salt, placed in a wooden barrel, doused with salt again, then weighed down with a heavy tsukemonoishi (pickling stone). As days pass, water seeps out and is removed. After six months, this "sushi" can be eaten, remaining edible for another six months or more.

The most famous variety of "narezushi" are the ones offered as a specialty dish of Shiga Prefecture, particularly the "funa-zushi" made from fish of the crucian carp genus, the authentic version of which calls for the use "nigorobuna", a particular locally differentiated variety of wild goldfish endemic to Lake Biwa).

"Nigirizushi" (, "hand-pressed sushi") consists of an oblong mound of "sushi rice" that the chef presses between the palms of the hands to form an oval-shaped ball, and a topping (the "neta") draped over the ball. It is usually served with a bit of "wasabi"; "neta" are typically fish such as salmon, tuna or other seafood. Certain toppings are typically bound to the rice with a thin strip of "nori", most commonly octopus ("tako"), freshwater eel ("unagi"), sea eel ("anago"), squid ("ika"), and sweet egg ("tamago"). One order of a given type of fish typically results in two pieces, while a sushi set (sampler dish) may contain only one piece of each topping.

"Gunkanmaki" (, "warship roll") is a special type of "nigirizushi": an oval, hand-formed clump of sushi rice that has a strip of "nori" wrapped around its perimeter to form a vessel that is filled with some soft, loose or fine-chopped ingredient that requires the confinement of "nori" such as roe, "nattō", oysters, "uni" (sea urchin roe), corn with mayonnaise, scallops, and quail eggs. "Gunkan-maki" was invented at the "Ginza Kyubey" restaurant in 1941; its invention significantly expanded the repertoire of soft toppings used in sushi.

"Temarizushi" (, "ball sushi") is a sushi made by pressing rice and fish into a ball-shaped form by hand using a plastic wrap.

, also known as , is a pressed sushi from the Kansai region, a favorite and specialty of Osaka. A block-shaped piece is formed using a wooden mold, called an "oshibako". The chef lines the bottom of the "oshibako" with the toppings, covers them with sushi rice, and then presses the lid of the mold down to create a compact, rectilinear block. The block is removed from the mold and then cut into bite-sized pieces. Particularly famous is ("battera", pressed mackerel sushi) or ("saba zushi"). In "oshizushi", all the ingredients are either cooked or cured and raw fish is never used.

The increasing popularity of "sushi" around the world has resulted in variations typically found in the Western world, but rarely in Japan (a notable exception to this is the use of salmon, which was introduced by Bjørn Eirik Olsen, a Norwegian businessman tasked with helping the Norwegian salmon industry sell more fish in the early 1980s). Such creations to suit the Western palate were initially fueled by the invention of the California roll (a "norimaki" with crab (later, imitation crab), cucumber, and avocado). A wide variety of popular rolls ("norimaki" and "uramaki") has evolved since. Norway roll is another variant of "uramakizushi" filled with "tamago" (omelette), imitation crab and cucumber, rolled with "shiso" leaf and "nori", topped with slices of Norwegian salmon, garnished with lemon and mayonnaise.

Uramaki (, "inside-out roll") is a medium-sized cylindrical piece with two or more fillings, and was developed as a result of the creation of the California roll, as a method originally meant to hide the nori. "Uramaki" differs from other "makimono" because the rice is on the outside and the "nori" inside. The filling is in the center surrounded by "nori", then a layer of rice, and optionally an outer coating of some other ingredients such as roe or toasted sesame seeds. It can be made with different fillings, such as tuna, crab meat, avocado, mayonnaise, cucumber or carrots.

Examples of variations include the rainbow roll (an inside-out topped with thinly sliced "maguro, hamachi, ebi, sake" and avocado) and the caterpillar roll (an inside-out topped with thinly sliced avocado). Also commonly found is the "rock and roll" (an inside-out roll with barbecued freshwater eel and avocado with toasted sesame seeds on the outside).

In Japan, "uramaki" is an uncommon type of "makimono"; because sushi is traditionally eaten by hand in Japan, the outer layer of rice can be quite difficult to handle with fingers.

"Futomaki" is a more popular variation of sushi within the United States, and comes in variations that take their names from their place of origin. Other rolls may include a variety of ingredients, including chopped scallops, spicy tuna, beef or chicken teriyaki roll, okra, and assorted vegetables such as cucumber and avocado, and the "tempura roll", where shrimp tempura is inside the roll or the entire roll is battered and fried tempura-style. In the Southern United States, many sushi restaurants prepare rolls using crawfish. Sometimes, rolls are made with brown rice or black rice, which appear in Japanese cuisine as well.

Per Food and Drug Administration regulations, raw fish served in the United States must be frozen prior to serving in order to kill parasites. Because of this and the relative difficulty of acquiring fresh seafood compared to Japan, raw seafood (e.g., sashimi) is not as prevalent of a component in American-style sushi.

Since rolls are usually made to-order it is not unusual for the customer to specify the exact ingredients desired (e.g. salmon roll, cucumber roll, avocado roll, Shrimp/tuna tempura roll, tuna roll, etc.). Though the menu names of dishes often vary by restaurant, some examples include:

All sushi has a base of specially prepared rice, complemented with other ingredients.

"Sushi-meshi" (also known as "Su-meshi" , "shari" , or "gohan" ) is a preparation of white, short-grained, Japanese rice mixed with a dressing consisting of rice vinegar, sugar, salt, and occasionally kombu and "sake". It has to be cooled to room temperature before being used for a filling in a "sushi" or else it will get too sticky while being seasoned. Traditionally, the mixing is done with a hangiri, which is a round, flat-bottom wooden tub or barrel, and a wooden paddle (shamoji).

Sushi rice is prepared with short-grain Japanese rice, which has a consistency that differs from long-grain strains such as those from India, Sri Lanka, Bangladesh, Thailand, and Vietnam. The essential quality is its stickiness or glutinousness, although the type of rice used for sushi is different from glutinous rice. Freshly harvested rice ("shinmai") typically contains too much water, and requires extra time to drain the rice cooker after washing. In some fusion cuisine restaurants, short-grain brown rice and wild rice are also used.

There are regional variations in sushi rice and individual chefs have their individual methods. Most of the variations are in the rice vinegar dressing: the Kantō region (or East Japan) version of the dressing commonly uses more salt; in Kansai region (or West Japan), the dressing has more sugar.

The black seaweed wrappers used in "makimono" are called "nori" (海苔). "Nori" is a type of algae, traditionally cultivated in the harbors of Japan. Originally, algae was scraped from dock pilings, rolled out into thin, edible sheets, and dried in the sun, in a process similar to making rice paper. Today, the commercial product is farmed, processed, toasted, packaged, and sold in sheets.

The size of a "nori" sheet influences the size of "makimono". A full-size sheet produces "futomaki", and a half produces "hosomaki" and "temaki". To produce "gunkan" and some other "makimono", an appropriately-sized piece of "nori" is cut from a whole sheet.

"Nori" by itself is an edible snack and is available with salt or flavored with teriyaki sauce. The flavored variety, however, tends to be of lesser quality and is not suitable for sushi.

When making "fukusazushi", a paper-thin omelette may replace a sheet of "nori" as the wrapping. The omelette is traditionally made on a rectangular omelette pan (makiyakinabe), and used to form the pouch for the rice and fillings.

For culinary, sanitary, and aesthetic reasons, the minimum quality and freshness of fish to be eaten raw must be superior to that of fish which is to be cooked. Sushi chefs are trained to recognize important attributes, including smell, color, firmness, and freedom from parasites that may go undetected in commercial inspection. Commonly used fish are tuna ("maguro, shiro-maguro"), Japanese amberjack, yellowtail ("hamachi"), snapper ("kurodai"), mackerel ("saba"), and salmon ("sake"). The most valued sushi ingredient is "toro," the fatty cut of the fish. This comes in a variety of "ōtoro" (often from the bluefin species of tuna) and "chūtoro", meaning "middle toro", implying that it is halfway into the fattiness between "toro" and the regular cut. "Aburi" style refers to "nigiri" sushi where the fish is partially grilled (topside) and partially raw. Most nigiri sushi will have completely raw "neta".

Other seafoods such as squid ("ika"), eel ("anago" and "unagi"), pike conger ("hamo"), octopus ("tako"), shrimp ("ebi" and "amaebi"), clam ("mirugai", "aoyagi" and "akagai"), fish roe ("ikura", "masago", "kazunoko" and "tobiko"), sea urchin ("uni"), crab ("kani"), and various kinds of shellfish (abalone, prawn, scallop) are the most popular seafoods in sushi. Oysters, however, are less common, as the taste is not thought to go well with the rice. "Kani kama", or imitation crab stick, is commonly substituted for real crab, most notably in California rolls.

Pickled daikon radish ("takuan") in "shinko maki", pickled vegetables ("tsukemono"), fermented soybeans ("nattō") in "nattō maki", avocado, cucumber in "kappa maki", asparagus, yam, pickled ume ("umeboshi"), gourd ("kanpyō"), burdock ("gobo"), and sweet corn (possibly mixed with mayonnaise) are also used in sushi.

Tofu and eggs (in the form of slightly sweet, layered omelette called "tamagoyaki" and raw quail eggs ride as a "gunkan-maki" topping) are common.

Sushi is commonly eaten with condiments. Sushi may be dipped in shōyu (soy sauce), and is usually flavored with wasabi, a piquant paste made from the grated stem of the "Wasabia japonica" plant. Japanese-style mayonnaise is a common condiment in Japan on salmon, pork and other sushi cuts.

True wasabi has anti-microbial properties and may reduce the risk of food poisoning. The traditional grating tool for wasabi is a sharkskin grater or "samegawa oroshi". An imitation wasabi ("seiyo-wasabi"), made from horseradish, mustard powder and green dye is common. It is found at lower-end "kaiten-zushi" restaurants, in bento box sushi and at most restaurants outside Japan. If manufactured in Japan, it may be labelled "Japanese Horseradish".

Gari (sweet, pickled ginger) is eaten in between sushi courses to both cleanse the palate and aid in digestion. In Japan, green tea ("ocha") is invariably served together with sushi. Better sushi restaurants often use a distinctive premium tea known as "mecha". In sushi vocabulary, green tea is known as "agari".

Sushi may be garnished with gobo, grated daikon, thinly sliced vegetables, carrots/radishes/cucumbers that have been shaped to look like flowers, real flowers, or seaweed salad.

When closely arranged on a tray, different pieces are often separated by green strips called "baran" or "kiri-zasa" (切り笹). These dividers prevent the flavors of neighboring pieces of sushi from mixing and help to achieve an attractive presentation. Originally, these were cut leaves from the "Aspidistra elatior" (葉蘭 "haran") and "Sasa veitchii" (熊笹 "kuma-zasa") plants, respectively. Using actual leaves had the added benefit of releasing antimicrobial phytoncides when cut thereby extending the limited shelf life of the sushi. Sushi bento boxes are a staple of Japanese supermarkets and convenience stores. As these stores began rising in prominence in the 1960s, the labor-intensive cut leaves were increasingly replaced with green plastic in order to lower costs. This coincided with the increased prevalence of refrigeration which acted to extend the shelf life of sushi without the need for the cut leaves. Today the plastic strips are commonly used in sushi bento boxes and to a lesser degree in sushi presentations found in sushi bars and restaurants. In store-sold or to-go packages of sushi, the plastic leaf strips are often used to prevent the rolls from coming into early or unwanted contact with the ginger and wasabi included with the dish.

The main ingredients of traditional Japanese sushi, raw fish and rice, are naturally low in fat, high in protein, carbohydrates (the rice only), vitamins, and minerals, as are "gari" and "nori". Other vegetables wrapped within the sushi also offer various vitamins and minerals. Many of the seafood ingredients also contain omega-3 fatty acids, which have a variety of health benefits. The omega-3 fatty acids found in fish has certain beneficial property, especially on cardiovascular health, natural anti-inflammatory compounds, and play a role in brain function. 

Generally sushi is not a particularly fattening food. However, rice in sushi contains a fair amount of carbohydrates, plus the addition of other ingredients such as mayonnaise added into sushi rolls might increase the caloric content. Sushi also has a relatively high sodium content, especially contributed from "shoyu" soy sauce seasoning.

Some of the ingredients in sushi can present health risks. Large marine apex predators such as tuna (especially bluefin) can harbor high levels of methylmercury, which can lead to mercury poisoning when consumed in large quantity or when consumed by certain higher-risk groups, including women who are pregnant or may become pregnant, nursing mothers and young children.

According to recent studies, there have been about 18 million infections worldwide from eating raw fish. This serves as a great risk to expecting mothers due to the health risks that medical interventions or treatment measures may pose on the developing fetus. Parasitic infections can have a wide range of health impacts, including bowel obstruction, anemia, liver disease, and more. The impact of these illnesses alone can pose some health concerns on the expecting mother and baby, but the curative measures that may need to take place to recover, are also a concern as well.

Sashimi or other types of sushi containing raw fish present a risk of infection by three main types of parasites:

For the above reasons, EU regulations forbid the use of fresh raw fish. It must be frozen at temperatures below in all parts of the product for no less than 24 hours. As such, a number of fishing boats, suppliers and end users "super-freeze" fish for sushi to temperatures as low as −60 °C. As well as parasite destruction, super-freezing also prevents oxidation of the blood in tuna flesh, thus preventing the discoloration that happens at temperatures above −20 °C.

Some forms of sushi, notably those containing pufferfish fugu and some kinds of shellfish, can cause severe poisoning if not prepared properly. Particularly, fugu consumption can be fatal. Fugu fish has a lethal dose of tetrodotoxin in its internal organs and, by law in many countries, must be prepared by a licensed fugu chef who has passed the prefectural examination in Japan. The licensing examination process consists of a written test, a fish-identification test, and a practical test that involves preparing the fugu and separating out the poisonous organs. Only about 35 percent of the applicants pass.

Sustainable sushi is sushi made from fished or farmed sources that can be maintained or whose future production does not significantly jeopardize the ecosystems from which it is acquired. Concerns over the sustainability of sushi ingredients arise from greater concerns over environmental, economic and social stability and human health.

Traditionally, sushi is served on minimalist Japanese-style, geometric, mono- or duo-tone wood or lacquer plates, in keeping with the aesthetic qualities of this cuisine.

Many sushi restaurants offer fixed-price sets, selected by the chef from the catch of the day. These are often graded as "shō-chiku-bai" (), "shō/matsu" (, pine), "chiku/take" (, bamboo) and ("bai/ume"), with "matsu" the most expensive and " ume" the cheapest. Sushi restaurants will often have private booth dining, where guests are asked to remove their shoes, leaving them outside the room; However, most sushi bars offer diners a casual experience with an open dining room concept.

Sushi may be served "kaiten zushi" (sushi train) style. Color-coded plates of sushi are placed on a conveyor belt; as the belt passes, customers choose as they please. After finishing, the bill is tallied by counting how many plates of each color have been taken. Newer "kaiten zushi" restaurants use barcodes or RFID tags embedded in the dishes to manage elapsed time after the item was prepared.

Some specialized or slang terms are used in the sushi culture. Most of these terms are used only in sushi bars.


Unlike sashimi, which is almost always eaten with chopsticks, "nigirizushi" is traditionally eaten with the fingers, even in formal settings. Although it is commonly served on a small platter with a side dish for dipping, sushi can also be served in a "bento", a box with small compartments that hold the various dishes of the meal.

Soy sauce is the usual condiment, and sushi is normally served with a small sauce dish, or a compartment in the bento. Traditional etiquette suggests that the sushi is turned over so that only the topping is dipped; this is because the soy sauce is for flavoring the topping, not the rice, and because the rice would absorb too much soy sauce and would fall apart. If it is difficult to turn the sushi upside-down, one can baste the sushi in soy sauce using "gari" (sliced ginger) as a brush. Toppings that have their own sauce (such as eel) should not be eaten with soy sauce.

Traditionally, the sushi chef will add an appropriate amount of wasabi to the sushi while preparing it, and etiquette suggests eating the sushi as is, since the chef is supposed to know the proper amount of wasabi to use. However, today wasabi is more a matter of personal taste, and even restaurants in Japan may serve wasabi on the side for customers to use at their discretion, even when there is wasabi already in the dish.




</doc>
<doc id="28272" url="https://en.wikipedia.org/wiki?curid=28272" title="Shinto">
Shinto

Shinto practices were first recorded and codified in the written historical records of the "Kojiki" and "Nihon Shoki" in the 8th century. Still, these earliest Japanese writings do not refer to a unified religion, but rather to a collection of native beliefs and mythology. Shinto today is the religion of public shrines devoted to the worship of a multitude of 'spirits', 'essences' ("kami"), suited to various purposes such as war memorials and harvest festivals, and applies as well to various sectarian organizations. Practitioners express their diverse beliefs through a standard language and practice, adopting a similar style in dress and ritual, dating from around the time of the Nara and Heian periods (8th–12th century).

The word "Shinto" ("Way of the Gods") was adopted, originally as "Jindō" or "Shindō", from the written Chinese "Shendao" (神道, ), combining two "kanji": , meaning 'spirit' or "kami"; and , 'path', meaning a philosophical path or study (from the Chinese word "dào"). The oldest recorded usage of the word "Shindo" is from the second half of the 6th century. "Kami" is rendered in English as 'spirits', 'essences', or 'gods', and refers to the energy generating the phenomena. Since the Japanese language does not distinguish between singular and plural, "kami" also refers to the singular divinity, or sacred essence, that manifests in multiple forms: rocks, trees, rivers, animals, places, and even people can be said to possess the nature of "kami". Kami and people are not separate; they exist within the same world and share its interrelated complexity.

As much as nearly 80% of the population in Japan participates in Shinto practices or rituals, but only a small percentage of these identify themselves as "Shintoists" in surveys. This is because "Shinto" has different meanings in Japan. Most of the Japanese attend Shinto shrines and beseech "kami" without belonging to an institutional Shinto religion. There are no formal rituals to become a practitioner of "folk Shinto". Thus, "Shinto membership" is often estimated counting only those who do join organised Shinto sects. Shinto has 81,000 shrines and 85,000 priests in the country. According to surveys carried out in 2006 and 2008, less than 40% of the population of Japan identifies with an organised religion: around 35% are Buddhists, 3% to 4% are members of Shinto sects and derived religions. In 2008, 26% of the participants reported often visiting Shinto shrines, while only 16.2% expressed belief in the existence of a god or gods (神) in general.

According to Inoue (2003): "In modern scholarship, the term is often used with reference to kami worship and related theologies, rituals and practices. In these contexts, 'Shinto' takes on the meaning of 'Japan's traditional religion', as opposed to foreign religions such as Christianity, Buddhism, Islam and so forth."

Shinto religious expressions have been distinguished by scholars into a series of categories:

Many other sects and schools can be distinguished. is a grouping of Japanese new religions developed since the second half of the 20th century that have significantly departed from traditional Shinto and are not always regarded as part of it.

"Kami", "shin", or, archaically, "jin" (神) is defined in English as "god", "spirit", or "spiritual essence", all these terms meaning "the energy generating a thing". Since the Japanese language does not distinguish between singular and plural, "kami" refers to the divinity, or sacred essence, that manifests in multiple forms. Rocks, trees, rivers, animals, places, and even people can be said to possess the nature of "kami". Kami and people exist within the same world and share its interrelated complexity.

Early anthropologists called Shinto "animistic" in which animate and inanimate things have spirits or souls that are worshipped. The concept of animism in Shinto is no longer current, however.
Shinto gods are collectively called , an expression literally meaning "eight million kami", but interpreted as meaning "myriad", although it can be translated as "many kami". There is a phonetic variation, "kamu", and a similar word in the Ainu language, "kamui". An analogous word is "mi-koto".

"Kami" refers particularly to the power of phenomena that inspire a sense of wonder and awe in the beholder (the sacred), testifying to the divinity of such a phenomenon. It is comparable to what Rudolf Otto described as the "mysterium tremendum et fascinans", which translates as "fearful and fascinating mystery".

The "kami" reside in all things, but certain objects and places are designated for the interface of people and kami: "yorishiro", "shintai", shrines, and "kamidana". There are natural places considered to have an unusually sacred spirit about them and are objects of worship. They are frequently mountains, trees, unusual rocks, rivers, waterfalls, and other natural things. In most cases they are on or near a shrine grounds. The shrine is a building in which the kami is enshrined (housed). It is a sacred space, creating a separation from the "ordinary" world. The "kamidana" is a household shrine that acts as a substitute for a large shrine on a daily basis. In each case the object of worship is considered a sacred space inside which the kami spirit actually dwells, being treated with the utmost respect.

In Shinto "kannagara" (惟神 or 随神), meaning "way [path] of [expression] of the "kami"", refers to the law of the natural order. It is the sense of the terms "michi" or "to", "way", in the terms "kami-no-michi" or "Shinto". Those who understand "kannagara" know the divine, the human, and how people should live. From this knowledge stems the ethical dimension of Shinto, focusing on sincerity ("makoto"), honesty ("tadashii") and purity.

According to the "Kojiki", "Amenominakanushi" (天御中主 "All-Father of the Originating Hub", or 天之御中主神 "Heavenly Ancestral God of the Originating Heart of the Universe") is the first "kami", and the concept of the source of the universe according to theologies. In mythology he is described as a "god who came into being alone" ("hitorigami"), the first of the "zōka sanshin"("three "kami" of creation"), and one of the five "kotoamatsukami" ("distinguished heavenly gods").

Amenominakanushi had been considered a concept developed under the influence of Chinese thought, but now most scholars believe otherwise. With the flourishing of "kokugaku" the concept was studied by scholars. The theologian Hirata Atsutane identified Amenominakanushi as the spirit of the North Star, master of the seven stars of the Big Dipper. The god was emphasised by the "Daikyōin" in the Meiji period, and worshiped by some Shinto sects.

The god manifests in a duality, a male and a female function, respectively "Takamimusubi" (高御産巣日神) and "Kamimusubi" (神産巣日神). In other mythical accounts the originating "kami" is called "Umashiashikabihikoji" (宇摩志阿斯訶備比古遅神 "God of the "Ashi" [Reed]") or "Kuninotokotachi" (国之常立神 in Kojiki, 国常立尊 in Nihonshoki; "Kunitokotachi-no-Kami" or "Kuninotokotachi-no-Kami"; the "God Founder of the Nation"), the latter used in the "Nihon Shoki".

The generation of the Japanese archipelago is expressed mythologically as the action of two gods: Izanagi ("He-who-invites") and Izanami ("She-who-is-invited"). The interaction of these two principles begets the islands of Japan and a further group of "kami".

The events are described in the "Kojiki" as follows:

In the myth, the birth of the god of fire ("Kagu-Tsuchi") causes the death of Izanami, who descends into "Yomi-no-kuni", the netherworld. Izanagi chases her there, but runs away when he finds the dead figure of his spouse. As he returns to the land of the living, Amaterasu (the sun goddess) is born from his left eye, Tsukiyomi (the moon deity) from his right eye, and Susanoo (the storm deity) is born from Izanagi's nose.

Shinto teaches that certain deeds create a kind of ritual impurity that one should want cleansed for one's own peace of mind and good fortune rather than because impurity is wrong. Wrong deeds are called , which is opposed to . Normal days are called "day" ("ke"), and festive days are called "sunny" or, simply, "good" ("hare").

Those who are killed without being shown gratitude for their sacrifice will hold a (grudge) and become powerful and evil "kami" who seek revenge ("aragami"). Additionally, if anyone is injured on the grounds of a shrine, the area must be ritually purified.

Purification rites called Harae are a vital part of Shinto. They are done on a daily, weekly, seasonal, lunar, and annual basis. These rituals are the lifeblood of the practice of Shinto. Such ceremonies have also been adapted to modern life. New buildings made in Japan are frequently blessed by a Shinto priest called during the groundbreaking ceremony (Jichinsai 地鎮祭), and many cars made in Japan have been blessed as part of the assembly process. Moreover, many Japanese businesses built outside Japan have a Shinto priest perform ceremonies. On occasion priests visit annually to re-purify.

It is common for families to participate in ceremonies for children at a shrine, yet have a Buddhist funeral at the time of death. In old Japanese legends, it is often claimed that the dead go to a place called "yomi" (黄泉), a gloomy underground realm with a river separating the living from the dead mentioned in the legend of Izanami and Izanagi. This "yomi" very closely resembles the Greek Hades; however, later myths include notions of resurrection and even Elysium-like descriptions such as in the legend of Okuninushi and Susanoo. Shinto tends to hold negative views on death and corpses as a source of pollution called "kegare". However, death is also viewed as a path towards apotheosis in Shintoism as can be evidenced by how legendary individuals become enshrined after death. Perhaps the most famous would be Emperor Ojin who was enshrined as Hachiman the God of War after his death.

Unlike many religions, one does not need to publicly profess belief in Shinto to be a believer. Whenever a child is born in Japan, a local Shinto shrine adds the child's name to a list kept at the shrine and declares him or her a . After death an "ujiko" becomes a . One may choose to have one's name added to another list when moving and then be listed at both places. Names can be added to the list without consent and regardless of the beliefs of the person added to the list. This is not considered an imposition of belief, but a sign of being welcomed by the local "kami", with the promise of addition to the pantheon of "kami" after death.

Shinto Funeral were established during the Tokugawa period and focused on two themes: concern for the fate of the corpse, and maintenance of the relationship between the living and the dead. There are at least twenty steps involved in burying the dead. Mourners wear solid black in a day of mourning called Kichu-fuda and a Shinto priest will perform various rituals. People will give monetary gifts to the deceased's family called Koden, and Kotsuge is the gathering of the deceased's ashes. Some of the ashes are taken by family members to put in their home shrines at the step known as Bunkotsu.

The principal worship of "kami" is done at public shrines or worship at small home shrines called "kamidana" (神棚, lit. "god-shelf"). The public shrine is a building or place that functions as a conduit for "kami". A fewer number of shrines are also natural places called "mori". The most common of the "mori" are sacred groves of trees, or mountains, or waterfalls. All shrines are open to the public at some times or throughout the year.

While many of the public shrines are elaborate structures, all are characteristic Japanese architectural styles of different periods depending on their age. Shrines are fronted by a distinctive Japanese gate (鳥居, "torii") made of two uprights and two crossbars denoting the separation between common space and sacred space. The "torii" have 20 styles and matching buildings based on the enshrined kami and lineage.

There are a number of symbolic and real barriers that exist between the normal world and the shrine grounds including: statues of protection, gates, fences, ropes, and other delineations of ordinary to sacred space. Usually there will be only one or sometimes two approaches to the Shrine for the public and all will have the "torii" over the way. In shrine compounds, there are a "haiden" (拝殿) or public hall of worship, "heiden" (幣殿) or hall of offerings and the "honden" (本殿) or the main hall. The innermost precinct of the grounds is the "honden" or worship hall, which is entered only by the high priest, or worshippers on certain occasions. The "honden" houses the symbol of the enshrined "kami".

The heart of the shrine is periodic rituals, spiritual events in parishioners' lives, and festivals. All of this is organized by priests who are both spiritual conduits and administrators. Shrines are private institutions, and are supported financially by the congregation and visitors. Some shrines may have festivals that attract hundreds of thousands, especially in the New Year season.

Of the 80,000 Shinto shrines:

Any person may visit a shrine and one need not be Shinto to do this. Doing so is called "Omairi".
Typically there are a few basic steps to visiting a shrine.

The rite of ritual purification, usually done daily at a shrine, is a ceremony of offerings and prayers of several forms. Shinsen (food offerings of fruit, fish, and vegetables), tamagushi (sakaki tree branches), shio (salt), gohan (rice), mochi (rice cake), and sake (rice wine) are all typical offerings. On holidays and other special occasions the inner shrine doors may be opened and special offerings made.

Misogi means purification. Misogi harai or Misogi Shūhō (禊修法) is the term for water purification.

The practice of purification by ritual use of water while reciting prayers is typically done daily by regular practitioners, and when possible by lay practitioners. There is a defined set of prayers and physical activities that precede and occur during the ritual. This will usually be performed at a shrine, in a natural setting, but can be done anywhere there is clean running water.

The basic performance of this is the hand and mouth washing (Temizu 手水) done at the entrance to a shrine. The more dedicated believer may perform misogi by standing beneath a waterfall or performing the ritual ablutions in a river. This practice comes from Shinto history, when the kami Izanagi-no-Mikoto first performed misogi after returning from the land of Yomi, where he was made impure by Izanami-no-Mikoto after her death.

Another form of ritual cleanliness is avoidance, which means that a taboo is placed upon certain persons or acts. To illustrate, one would not visit a shrine if a close relative in the household had died recently. Killing is generally unclean and is to be avoided. When one is performing acts that harm the land or other living things, prayers and rituals are performed to placate the Kami of the area. This type of cleanliness is usually performed to prevent ill outcomes.

Ema are small wooden plaques that wishes or desires are written upon and left at a place in the shrine grounds so that one may get a wish or desire fulfilled. They have a picture on them and are frequently associated with the larger Shrines.

"Ofuda" are talismans—made of paper, wood, or metal—that are issued at shrines. They are inscribed with the names of kami and are used for protection in the home. They are typically placed in the home at a kamidana. Ofuda may be kept anywhere, as long as they are in their protective pouches, but there are several rules about the proper placement of kamidana. They are also renewed annually.

"Omamori" are personal-protection amulets that are sold by shrines. They are frequently used to ward off bad luck and to gain better health. More recently, there are also amulets to promote good driving, good business, and success at school. Their history lies with Buddhist practice of selling amulets. They are generally replaced once a year, and old omamori are brought to a shrine so they can be properly disposed of through burning by a priest.

"Omikuji" are paper lots upon which personal fortunes are written. The fortunes can range from "daikichi" (大吉), meaning "great good luck," to "daikyou" (大凶), meaning "great bad luck."

A "daruma" is a round, paper doll of the Indian monk, Bodhidharma. The recipient makes a wish and paints one eye; when the goal is accomplished, the recipient paints the other eye. While this is a Buddhist practice, darumas can be found at shrines, as well. These dolls are very common.

Other protective items include "dorei", which are earthenware bells that are used to pray for good fortune. These bells are usually in the shapes of the zodiacal animals: "hamaya", which are symbolic arrows for the fight against evil and bad luck; and "Inuhariko", which are paper dogs that are used to induce and to bless good births.

"Kagura" is the ancient Shinto ritual dance of shamanic origin. The word "kagura" is thought to be a contracted form of "kami no kura" or "seat of the kami" or the "site where the kami is received." There is a mythological tale of how "kagura" dance came into existence. The sun goddess Amaterasu became very upset at her brother so she hid in a cave. All of the other gods and goddesses were concerned and wanted her to come outside. Ame-no-uzeme began to dance and create a noisy commotion in order to entice Amaterasu to come out. The kami (gods) tricked Amaterasu by telling her there was a better sun goddess in the heavens. Amaterasu came out and light returned to the universe.

Music plays a very important role in the "kagura" performance. Everything from the setup of the instruments to the most subtle sounds and the arrangement of the music is crucial to encouraging the kami to come down and dance. The songs are used as magical devices to summon the gods and as prayers for blessings. Rhythm patterns of five and seven are common, possibly relating to the Shinto belief of the twelve generations of heavenly and earthly deities. There is also vocal accompaniment called "kami uta" in which the drummer sings sacred songs to the gods. Often the vocal accompaniment is overshadowed by the drumming and instruments, reinforcing that the vocal aspect of the music is more for incantation rather than aesthetics.

In both ancient Japanese collections, the Nihongi and Kojiki, Ame-no-uzeme’s dance is described as "asobi", which in old Japanese language means a ceremony that is designed to appease the spirits of the departed, and which was conducted at funeral ceremonies. Therefore, "kagura" is a rite of "tama shizume", of pacifying the spirits of the departed. In the Heian period (8th–12th centuries) this was one of the important rites at the Imperial Court and had found its fixed place in the "tama shizume" festival in the eleventh month. At this festival people sing as accompaniment to the dance: "Depart! Depart! Be cleansed and go! Be purified and leave!"
This rite of purification is also known as "chinkon". It was used for securing and strengthening the soul of a dying person. It was closely related to the ritual of "tama furi" (shaking the spirit), to call back the departed soul of the dead or to energize a weakened spirit. Spirit pacification and rejuvenation were usually achieved by songs and dances, also called "asobi". The ritual of "chinkon" continued to be performed on the emperors of Japan, thought to be descendents of Amaterasu. It is possible that this ritual is connected with the ritual to revive the sun goddess during the low point of the winter solstice.

There is a division between the "kagura" that is performed at the Imperial palace and the shrines related to it, and the "kagura" that is performed in the countryside. Folk "kagura", or "kagura" from the countryside is divided according to region. The following descriptions relate to "sato kagura", "kagura" that is from the countryside. The main types are: "miko kagura", "Ise kagura", "Izumo kagura", and "shishi kagura".
"Miko kagura" is the oldest type of "kagura" and is danced by women in Shinto shrines and during folk festivals. The ancient miko were shamanesses, but are now considered priestesses in the service of the Shinto Shrines. "Miko kagura" originally was a shamanic trance dance, but later, it became an art and was interpreted as a prayer dance. It is performed in many of the larger Shinto shrines and is characterized by slow, elegant, circular movements, by emphasis on the four directions and by the central use of torimono (objects dancers carry in their hands), especially the fan and bells.

"Ise kagura" is a collective name for rituals that are based upon the "yudate" (boiling water rites of Shugendō origin) ritual. It includes "miko" dances as well as dancing of the "torimono" type. The "kami" are believed to be present in the pot of boiling water, so the dancers dip their "torimono" in the water and sprinkle it in the four directions and on the observers for purification and blessing.

"Izumo kagura" is centered in the Sada shrine of Izumo, Shimane prefecture. It has two types: "torimono ma", unmasked dances that include held objects, and "shinno" (sacred No), dramatic masked dances based on myths. "Izumo kagura" appears to be the most popular type of "kagura".

"Shishi kagura" also known as the Shugen-No tradition, uses the dance of a "shishi" (lion or mountain animal) mask as the image and presence of the deity. It includes the "Ise daikagura" group and the "yamabushi kagura" and "bangaku" groups of the Tohoku area (Northeastern Japan). "Ise daikagura" employs a large red Chinese type of lion head which can move its ears. The lion head of the "yamabushi kagura" schools is black and can click its teeth. Unlike other "kagura" types in which the "kami" appear only temporarily, during the "shishi kagura" the "kami" is constantly present in the shishi head mask. During the Edo period, the lion dances became showy and acrobatic losing its touch with spirituality. However, the "yamabushi kagura" tradition has retained its ritualistic and religious nature.

Originally, the practice of "kagura" involved authentic possession by the "kami" invoked. In modern-day Japan it appears to be difficult to find authentic ritual possession, called "kamigakari", in "kagura" dance. However, it is common to see choreographed possession in the dances. Actual possession is not taking place but elements of possession such as losing control and high jumps are applied in the dance.

There is no core sacred text in Shinto, as the Bible is in Christianity or Qur'an is in Islam. Instead there are books of lore and history which provide stories and background to many Shinto beliefs.

Shinto has very ancient roots in the Japanese islands. The recorded history dates to the Kojiki (712) and Nihon Shoki (720), but archeological records date back significantly further. Both are compilations of prior oral traditions. The Kojiki establishes the Japanese imperial family as the foundation of Japanese culture, being the descendants of Amaterasu Omikami. There is also a creation myth and a genealogy of the gods. The Nihonshoki was more interested in creating a structural system of government, foreign policy, religious hierarchy, and domestic social order.

There is an internal system of historical Shinto development that configures the relationships between Shinto and other religious practices over its long history; the inside and outside Kami (spirits). The inside or "ujigami" ("uji" meaning clan) Kami roles that supports cohesion and continuation of established roles and patterns; and the "hitogami" or outside Kami, bringing innovation, new beliefs, new messages, and some instability.

Jomon peoples of Japan used natural housing, predated rice farming, and frequently were hunter-gatherers; the physical evidence for ritual practices are difficult to document. There are many locations of stone ritual structures, refined burial practices and early Torii that lend to the continuity of primal Shinto. The Jomon had a clan-based tribal system developed similar to much of the world's indigenous people. In the context of this clan based system, local beliefs developed naturally and when assimilation between clans occurred, they also took on some beliefs of the neighboring tribes. At some point there was a recognition that the ancestors created the current generations and the reverence of ancestors ("tama") took shape. There was some trade amongst the indigenous peoples within Japanese islands and the mainland, as well as some varying migrations. The trade and interchange of people helped the growth and complexity of the peoples spirituality by exposure to new beliefs. The natural spirituality of the people appeared to be based on the worship of nature forces or "mono", and the natural elements to which they all depended.

The gradual introduction of methodical religious and government organizations from mainland Asia starting around 300 BCE seeded the reactive changes in primal Shinto over the next 700 years to a more formalized system. These changes were directed internally by the various clans frequently as a syncratic cultural event to outside influences. Eventually as the Yamato gained power a formalization process began. The genesis of the Imperial household and subsequent creation of the Kojiki helped facilitate the continuity needed for this long term development through modern history. There is today a balance between outside influences of Buddhist, Confucian, Taoist, Abrahamic, Hindu and secular beliefs. In more modern times Shinto has developed new branches and forms on a regular basis, including leaving Japan.

By the end of the Jōmon period, a dramatic shift had taken place according to archaeological studies. New arrivals from the continent seem to have invaded Japan from the West, bringing with them new technologies such as rice farming and metallurgy. The settlements of the new arrivals seem to have coexisted with those of the Jōmon for some time. Under these influences, the incipient cultivation of the Jōmon evolved into sophisticated rice-paddy farming and government control. Many other elements of Japanese culture also may date from this period and reflect a mingled migration from the northern Asian continent and the southern Pacific areas. Among these elements are Shinto mythology, marriage customs, architectural styles, and technological developments such as lacquerware, textiles, laminated bows, metalworking, and glass making. The Jōmon is succeeded by the Yayoi period.

Japanese culture begins to develop in no small part due to influences from mainland trade and immigration from China. During this time in the pre-writing historical period, objects from the mainland start appearing in large amounts, specifically mirrors, swords, and jewels. All three of these have a direct connection to the imperial divine status as they are the symbols of imperial divinity and are Shinto honorary objects. Also the rice culture begins to blossom throughout Japan and this leads to the settlement of society, and seasonal reliance of crops. Both of these changes are highly influential on the Japanese people's relationship to the natural world, and likely development of a more complex system of religion. This is also the period that is referenced as the beginning of the divine imperial family. The Yayoi culture was a clan based culture that lived in compounds with a defined leader who was the chief and head priest. They were responsible for the relationship with their "gods" Kami and if one clan conquered another, their "god" would be assimilated. The earliest records of Japanese culture were written by Chinese traders who described this land as "Wa". This time period led to the creation of the Yamato culture and development of formal Shinto practices.

The development of "niiname" or the (now) Shinto harvest festival is attributed to this period as offerings for good harvests of similar format (typically rice) become common.

The great bells and drums, Kofun burial mounds, and the founding of the imperial family are important to this period. This is the period of the development of the feudal state, and the Yamato and Izumo cultures. Both of these dominant cultures have a large and central shrine which still exists today, Ise Shrine in the North East and Izumo Taisha in the South West. This time period is defined by the increase of central power in Naniwa, now Osaka, of the feudal lord system. Also there was an increasing influence of Chinese culture which profoundly changed the practices of government structure, social structure, burial practices, and warfare. The Japanese also held close alliance and trade with the Gaya confederacy which was in the south of the peninsula. The Paekche in the Three Kingdoms of Korea had political alliances with Yamato, and in the 5th century imported the Chinese writing system to record Japanese names and events for trade and political records. In 513 they sent a Confucian scholar to the court to assist in the teachings of Confucian thought. In 552 or 538 a Buddha image was given to the Yamato leader which profoundly changed the course of Japanese religious history, especially in relation to the undeveloped native religious conglomeration that was Shinto. In the latter 6th century, there was a breakdown of the alliances between Japan and Paekche but the influence led to the codification of Shinto as the native religion in opposition to the extreme outside influences of the mainland. Up to this time Shinto had been largely a clan ('uji') based religious practice, exclusive to each clan.

The Theory of Five Elements in Yin and Yang philosophy of Taoism and the esoteric Buddhism had a profound impact on the development of a unified system of Shinto beliefs. In the early Nara period, the "Kojiki" and the "Nihon Shoki" were written by compiling existing myths and legends into a unified account of Japanese mythology. These accounts were written with two purposes in mind: the introduction of Taoist, Confucian, and Buddhist themes into Japanese religion; and garnering support for the legitimacy of the Imperial house, based on its lineage from the sun goddess, Amaterasu. Much of modern Japan was under only fragmentary control by the Imperial family, and rival ethnic groups. The mythological anthologies, along with other poetry anthologies like the "Collection of Ten Thousand Leaves" ("Man'yōshū") and others, were intended to impress others with the worthiness of the Imperial family and their divine mandate to rule.

In particular the Asuka rulers of 552–645 saw disputes between the more major families of the clan Shinto families. There were disputes about who would ascend to power and support the imperial family between the Soga and Mononobe/Nakatomi Shinto families. The Soga family eventually prevailed and supported Empress Suiko and Prince Shotoku, who helped impress Buddhist faith into Japan. However, it was not until the Hakuho ruling period of 645–710 was Shinto installed as the imperial faith along with the Fujiwara Clan and reforms that followed.

Beginning with Emperor Tenmu (672–686), continuing through Empress Jitō (686–697) and Emperor Monmu (697–707) Court Shinto rites are strengthened and made parallel to Buddhist beliefs in court life. Prior to this time clan Shinto had dominated and a codification of "Imperial Shinto" did not exist as such. The Nakatomi family are made the chief court Shinto chaplains and chief priests at Ise Daijingū which held until 1892. Also the practice of sending imperial princesses to the Ise shrine begins. This marks the rise of Ise Daijingū as the main imperial shrine historically. Due to increasing influence from Buddhism and mainland Asian thought, codification of the "Japanese" way of religion and laws begins in earnest. This culminates in three major outcomes: Taiho Code (701 but started earlier), the "Kojiki" (712), and the "Nihon Shoki" (720).

The Taiho Code also called Ritsuryō (律令) was an attempt to create a bulwark to dynamic external influences and stabilize the society through imperial power. It was a liturgy of rules and codifications, primarily focused on regulation of religion, government structure, land codes, criminal and civil law. All priests, monks, and nuns were required to be registered, as were temples. The Shinto rites of the imperial line were codified, especially seasonal cycles, lunar calendar rituals, harvest festivals, and purification rites. The creation of the imperial Jingi-kan or Shinto Shrine office was completed.

This period hosted many changes to the country, government, and religion. The capital is moved again to Heijō-kyō, or Nara, in AD 710 by Empress Genmei due to the death of the Emperor. This practice was necessary due to the Shinto belief in the impurity of death and the need to avoid this pollution. However, this practice of moving the capital due to "death impurity" is then abolished by the Taihō Code and rise in Buddhist influence. The establishment of the imperial city in partnership with Taihō Code is important to Shinto as the office of the Shinto rites becomes more powerful in assimilating local clan shrines into the imperial fold. New shrines are built and assimilated each time the city is moved. All of the grand shrines are regulated under Taihō and are required to account for incomes, priests, and practices due to their national contributions.

During this time, Buddhism becomes structurally established within Japan by Emperor Shōmu (reign 724–749), and several large building projects are undertaken. The Emperor lays out plans for the Buddha Dainichi (Great Sun Buddha), at Tōdai-ji assisted by the Priest Gyogi (or Gyoki) Bosatsu. The priest Gyogi went to Ise Daijingu Shrine for blessings to build the Buddha Dainichi. They identified the statue of Viarocana with Amaterasu (the sun goddess) as the manifestation of the supreme expression of universality.

The priest Gyogi is known for his belief in assimilation of Shinto Kami and Buddhas. Shinto kami are commonly being seen by Buddhist clergy as guardians of manifestation, guardians, or pupils of Buddhas and bodhisattvas. The priest Gyogi conferred boddhisattva precepts on the Emperor in 749 effectively making the Imperial line the head of state and divine to Shinto while beholden to Buddhism.

With the introduction of Buddhism and its rapid adoption by the court in the 6th century, it was necessary to explain the apparent differences between native Japanese beliefs and Buddhist teachings. One Buddhist explanation saw the "kami" as supernatural beings still caught in the cycle of birth and rebirth (reincarnation). The "kami" are born, live, die, and are reborn like all other beings in the karmic cycle. However, the "kami" played a special role in protecting Buddhism and allowing its teachings of compassion to flourish.

This explanation was later challenged by Kūkai (空海, 774–835), who saw the "kami" as different embodiments of the Buddhas themselves ("honji suijaku" theory). For example, he linked Amaterasu (the sun goddess and ancestor of the Imperial family) with Dainichi Nyorai, a central manifestation of the Buddhists, whose name means literally "Great Sun Buddha". In his view, the "kami" were just Buddhas by another name.

Buddhism and Shinto coexisted and were amalgamated in the "shinbutsu shūgō" and Kūkai's syncretic view held wide sway up until the end of the Edo period. There was no theological study that could be called "Shinto" during medieval and early modern Japanese history, and a mixture of Buddhist and popular beliefs proliferated. At that time, there was a renewed interest in "Japanese studies" ("kokugaku"), perhaps as a result of the closed country policy.

In the 18th century, various Japanese scholars, in particular Motoori Norinaga (本居 宣長, 1730–1801), tried to tear apart the "real" Shinto from various foreign influences. The attempt was largely unsuccessful, since as early as the "Nihon Shoki" parts of the mythology were explicitly borrowed from Taoism doctrines. For example, the co-creator deities Izanami and Izanagi are explicitly compared to yin and yang. However, the attempt did set the stage for the arrival of state Shinto, following the Meiji Restoration (c.1868), when Shinto and Buddhism were separated ("shinbutsu bunri").

Fridell argues that scholars call the period 1868–1945 the "State Shinto period" because, "during these decades, Shinto elements came under a great deal of overt state influence and control as the Japanese government systematically utilized shrine worship as a major force for mobilizing imperial loyalties on behalf of modern nation-building." However, the government had already been treating shrines as an extension of government before Meiji; see for example the Tenpō Reforms. Moreover, according to the scholar Jason Ānanda Josephson, It is inaccurate to describe shrines as constituting a "state religion" or a "theocracy" during this period since they had neither organization, nor doctrine, and were uninterested in conversion.

The Meiji Restoration reasserted the importance of the emperor and the ancient chronicles to establish the Empire of Japan, and in 1868 the government attempted to recreate the ancient imperial Shinto by separating shrines from the temples that housed them. During this period, numerous scholars of "kokugaku" believed that this national Shinto could be the unifying agent of the country around the Emperor while the process of modernization was undertaken with all possible speed. The psychological shock of the Western "Black Ships" and the subsequent collapse of the shogunate convinced many that the nation needed to unify in order to resist being colonized by outside forces.

In 1871, a Ministry of Rites ("jingi-kan)" was formed and Shinto shrines were divided into twelve levels with the Ise Shrine (dedicated to Amaterasu, and thus symbolic of the legitimacy of the Imperial family) at the peak and small sanctuaries of humble towns at the base. The following year, the ministry was replaced with a new Ministry of Religion, charged with leading instruction in ""shushin"" (moral courses). Priests were officially nominated and organized by the state, and they instructed the youth in a form of Shinto theology based on the official dogma of the divinity of Japan's national origins and its Emperor. However, this propaganda did not take, and the unpopular Ministry of Rites was dissolved in the mid-1870s.

Although the government sponsorship of shrines declined, Japanese nationalism remained closely linked to the legends of foundation and emperors, as developed by the "kokugaku" scholars. In 1890, the Imperial Rescript on Education was issued, and students were required to ritually recite its oath to "offer yourselves courageously to the State" as well as to protect the Imperial family. Such processes continued to deepen throughout the early Shōwa period, coming to an abrupt end in August 1945 when Japan lost the war in the Pacific. On 1 January 1946, Emperor Shōwa issued the Ningen-sengen, in which he quoted the Five Charter Oath of Emperor Meiji and declared that he was not an "akitsumikami" (a deity in human form).

The imperial era came to an abrupt close with the end of World War II, when Americans declared that Japanese nationalism had been informed by something called "State Shinto", which they attempted to define with the Shinto Directive. The meaning of "State Shinto" has been a matter of debate ever since.

In the post-war period, numerous "New Religions" cropped up, many of them ostensibly based on Shinto, but on the whole, Japanese religiosity may have decreased. However, the concept of religion in Japan is a complex one. A survey conducted in the mid-1970s indicated that of those participants who claimed not to believe in religion, one-third had a Buddhist or Shinto altar in their home, and about one quarter carried an "omamori" (an amulet to gain protection by "kami") on their person. Following the war, Shinto shrines tended to focus on helping ordinary people gain better fortunes for themselves through maintaining good relations with their ancestors and other "kami". The number of Japanese citizens identifying their religious beliefs as Shinto has declined a great deal, yet the general practice of Shinto rituals has not decreased in proportion, and many practices have persisted as general cultural beliefs (such as ancestor worship), and community festivals ("matsuri")—focusing more on religious practices. The explanation generally given for this anomaly is that, following the demise of State Shinto, modern Shinto has reverted to its more traditional position as a traditional religion which is culturally ingrained, rather than enforced. In any case, Shinto and its values continue to be a fundamental component of the Japanese cultural mindset.

Shinto has also spread abroad to a limited extent, and a few non-Japanese Shinto priests have been ordained. A relatively small number of people practice Shinto in America. There are several Shinto shrines in America. Shrines were also established in Taiwan and Korea during the period of Japanese imperial rule, but following the war, they were either destroyed or converted into some other use.

Within Shinto, there are a variety of sects which are not a part of Shrine Shinto and the officially defunct State Shinto. Sect Shinto, like Izumo Taishakyo Mission of Hawaii and Konkokyo, have unique practices which originated alongside older Shinto practices before the classification and separation of Shinto practices of the Meiji era in 1868.



</doc>
<doc id="28273" url="https://en.wikipedia.org/wiki?curid=28273" title="Shell">
Shell

Shell may refer to:














</doc>
<doc id="28278" url="https://en.wikipedia.org/wiki?curid=28278" title="Scottish Rite">
Scottish Rite

The Ancient and Accepted Scottish Rite of Freemasonry (the Northern Masonic Jurisdiction in the United States often omits the "and", while the English Constitution in the United Kingdom omits the "Scottish"), commonly known as simply the Scottish Rite (or, in England and Australia, as the Rose Croix although this is only one of its degrees), is one of several Rites of Freemasonry. A Rite is a progressive series of degrees conferred by various Masonic organizations or bodies, each of which operates under the control of its own central authority. In the Scottish Rite the central authority is called a Supreme Council.

The Scottish Rite is one of the appendant bodies of Freemasonry that a Master Mason may join for further exposure to the principles of Freemasonry. It is also concordant, in that some of its degrees relate to the degrees of Symbolic (Craft) Freemasonry. In England and some other countries, while the Scottish Rite is not accorded official recognition by the Grand Lodge, there is no prohibition against a Freemason electing to join it. In the United States, however, the Scottish Rite is officially recognized by Grand Lodges as an extension of the degrees of Freemasonry. The Scottish Rite builds upon the ethical teachings and philosophy offered in the Craft (or Blue) Lodge, through dramatic presentation of the individual degrees.

There are records of lodges conferring the degree of "Scots Master" or "Scotch Master" as early as 1733. A lodge at Temple Bar in London is the earliest such lodge on record. Other lodges include a lodge at Bath in 1735, and the French lodge, St. George de l'Observance No. 49 at Covent Garden in 1736. The references to these few occasions indicate that these were special meetings held for the purpose of performing unusual ceremonies, probably by visiting Freemasons. The Copiale cipher, dating from the 1740s says, "The rank of a Scottish master is an entirely new invention..."

The seed of the myth of Stuart Jacobite influence on the higher degrees may have been a careless and unsubstantiated remark made by John Noorthouk in the 1784 Book of Constitutions of the Premier Grand Lodge of London. It was stated, without support, that King Charles II (older brother and predecessor to James II) was made a Freemason in the Netherlands during the years of his exile (1649–60). However, there were no documented lodges of Freemasons on the continent during those years. The statement may have been made to flatter the fraternity by claiming membership for a previous monarch. This folly was then embellished by John Robison (1739–1805), a professor of Natural Philosophy at the University of Edinburgh, in an anti-Masonic work published in 1797. The lack of scholarship exhibited by Robison in that work caused the "Encyclopædia Britannica" to denounce it.

A German bookseller and Freemason, living in Paris, working under the assumed name of C. Lenning, embellished the story further in a manuscript titled "Encyclopedia of Freemasonry" probably written between 1822 and 1828 at Leipzig. This manuscript was later revised and published by another German Freemason named Friedrich Mossdorf (1757–1830). Lenning stated that King James II of England, after his flight to France in 1688, resided at the Jesuit College of Clermont, where his followers fabricated certain degrees for the purpose of carrying out their political ends.

By the mid-19th century, the story had gained currency. The well-known English Masonic writer, Dr. George Oliver (1782–1867), in his "Historical Landmarks", 1846, carried the story forward and even claimed that King Charles II was active in his attendance at meetings—an obvious invention, for if it had been true, it would not have escaped the notice of the historians of the time. The story was then repeated by the French writers Jean-Baptiste Ragon (1771–1862) and Emmanuel Rebold, in their Masonic histories. Rebold's claim that the high degrees were created and practiced in Lodge Canongate Kilwinning at Edinburgh are entirely false.

James II died in 1701at the Palace of St. Germain en Laye, and was succeeded in his claims to the British throne by his son, James Francis Edward Stuart (1699–1766), the Chevalier St. George, better known as "the Old Pretender", but recognized as James III by the French King Louis XIV. He was succeeded in his claim by Charles Edward Stuart ("Bonnie Prince Charles"), also known as "the Young Pretender", whose ultimate defeat at the Battle of Culloden in 1746 effectively put an end to any serious hopes of the Stuarts regaining the British crowns.

The natural confusion between the names of the Jesuit College of Clermont, and the short-lived Masonic Chapter of Clermont, a Masonic body that controlled a few high degrees during its brief existence, only served to add fuel to the myth of Stuart Jacobite influence in Freemasonry's high degrees. However, the College and the Chapter had nothing to do with each other. The Jesuit College was located at Clermont, whereas the Masonic Chapter was not. Rather, it was named "Clermont" in honor of the French Grand Master, the Comte de Clermont (Louis de Bourbon, Comte de Clermont) (1709–1771), and not because of any connection with the Jesuit College of Clermont.

A French trader, by the name of Estienne Morin, had been involved in high-degree Masonry in Bordeaux since 1744 and, in 1747, founded an ""Écossais"" lodge (Scots Masters Lodge) in the city of Le Cap Français, on the north coast of the French colony of Saint-Domingue (now Haiti). Over the next decade, high-degree Freemasonry was carried by French men to other cities in the Western hemisphere. The high-degree lodge at Bordeaux warranted or recognized seven Écossais lodges there. In Paris in the year 1761, a patent was issued to Estienne Morin, dated 27 August, creating him "Grand Inspector for all parts of the New World". This Patent was signed by officials of the Grand Lodge at Paris and appears to have originally granted him power over the craft lodges only, and not over the high, or "Écossais", degree lodges. Later copies of this Patent appear to have been embellished, probably by Morin, to improve his position over the high-degree lodges in the West Indies.

Early writers long believed that a "Rite of Perfection" consisting of 25 degrees, the highest being the "Sublime Prince of the Royal Secret", and being the predecessor of the Scottish Rite, had been formed in Paris by a high-degree council calling itself "The Council of Emperors of the East and West". The title "Rite of Perfection" first appeared in the Preface to the "Grand Constitutions of 1786", the authority for which is now known to be faulty. It is now generally accepted that this Rite of twenty-five degrees was compiled by Estienne Morin and is more properly called "The Rite of the Royal Secret", or "Morin's Rite". However, it was known as "The Order of Prince of the Royal Secret" by the founders of the Scottish Rite, who mentioned it in their "Circular throughout the two Hemispheres" or "Manifesto", issued on December 4, 1802.

Morin returned to the West Indies in 1762 or 1763, to Saint-Domingue. Based on his new Patent, he assumed powers to constitute lodges of all degrees, spreading the high degrees throughout the West Indies and North America. Morin stayed in Saint-Domingue until 1766, when he moved to Jamaica. At Kingston, Jamaica, in 1770, Morin created a "Grand Chapter" of his new Rite (the Grand Council of Jamaica). Morin died in 1771 and was buried in Kingston.

Henry Andrew Francken, a naturalized French subject of Dutch origin, was most important in assisting Morin in spreading the degrees in the New World. Morin appointed him Deputy Grand Inspector General (DGIG) as one of his first acts after returning to the West Indies. Francken worked closely with Morin and, in 1771, produced a manuscript book giving the rituals for the 15th through the 25th degrees. Francken produced at least four such manuscripts. In addition to the 1771 manuscript, there is a second which can be dated to 1783; a third manuscript, of uncertain date, written in Francken’s handwriting, with the rituals 4–25°, which was found in the archives of the Provincial Grand Lodge of Lancashire in Liverpool in approximately 1984; and a fourth, again of uncertain date, with rituals 4–24°, which was known to have been given by H. J. Whymper to the District Grand Lodge of the Punjab and rediscovered about 2010. Additionally, there is a French manuscript dating from 1790-1800 which contains the 25 degrees of the Order of the Royal Secret with additional detail, as well as three other "Hauts Grades" rituals; its literary structure suggests it is derived from a common source as the Francken Manuscripts.

A Loge de Parfaits d' Écosse was formed on 12 April 1764 at New Orleans, becoming the first high-degree lodge on the North American continent. Its life, however, was short, as the Treaty of Paris (1763) ceded New Orleans to Spain, and the Catholic Spanish crown had been historically hostile to Freemasonry. Documented Masonic activity ceased for a time. It did not return to New Orleans until the late 1790s, when French refugees from the revolution in Saint-Domingue settled in the city.

Francken traveled to New York in 1767 where he granted a Patent, dated 26 December 1767, for the formation of a Lodge of Perfection at Albany, which was called "Ineffable Lodge of Perfection". This marked the first time the Degrees of Perfection (the 4th through the 14th) were conferred in one of the Thirteen British colonies in North America. This Patent, and the early minutes of the Lodge, are still extant and are in the archives of Supreme Council, Northern Jurisdiction. (The minutes of Ineffable Lodge of Perfection reveal that it ceased activity on December 5, 1774. It was revived by Giles Fonda Yates about 1820 or 1821, and came under authority of the Supreme Council, Southern Jurisdiction until 1827. That year it was transferred to the Supreme Council, Northern Jurisdiction.)

While in New York City, Francken also communicated the degrees to Moses Michael Hays, a Jewish businessman, and appointed him as a Deputy Inspector General. In 1781, Hays made eight Deputy Inspectors General, four of whom were later important in the establishment of Scottish Rite Freemasonry in South Carolina:
<br>◊_ Isaac Da Costa, Sr., D.I.G. for South Carolina;
<br>◊_ Abraham Forst, D.I.G. for Virginia;
<br>◊_ Joseph M. Myers, D.I.G. for Maryland;
<br>◊_ and Barend M. Spitzer, D.I.G. for Georgia.
Da Costa returned to Charleston, South Carolina, where he established the "Sublime Grand Lodge of Perfection" in February 1783. After Da Costa's death in November 1783, Hays appointed Myers as Da Costa's successor. Joined by Forst and Spitzer, Myers created additional high-degree bodies in Charleston.

Physician Hyman Isaac Long from the island of Jamaica, who settled in New York City, went to Charleston in 1796 to appoint eight French men; he had received his authority through Spitzer. These men had arrived as refugees from Saint-Domingue, where the slave revolution was underway that would establish Haiti as an independent republic in 1804. They organized a Consistory of the 25th Degree, or "Princes of the Royal Secret," which Masonic historian Brigadier ACF Jackson says became the first Supreme Council of the Scottish Rite. According to Fox, by 1801, the Charleston bodies were the only extant bodies of the Rite in North America.

Although most of the thirty-three degrees of the Scottish Rite existed in parts of previous degree systems, the Scottish Rite did not come into being until the formation of the Mother Supreme Council at Charleston, South Carolina, in May 1801. The Founding Fathers of the Scottish Rite who attended became known as "The Eleven Gentlemen of Charleston".

Subsequently, other Supreme Councils were formed in Saint-Domingue (now Haiti) in 1802, in France in 1804, in Italy in 1805, and in Spain in 1811.

On May 1, 1813, an officer from the Supreme Council at Charleston initiated several New York Masons into the Thirty-third Degree and organized a Supreme Council for the "Northern Masonic District and Jurisdiction". On May 21, 1814 this Supreme Council reopened and proceeded to "nominate, elect, appoint, install and proclaim in due, legal and ample form" the elected officers "as forming the "second" Grand and Supreme Council...". Finally, the charter of this organization (written January 7, 1815) added, “We think the "Ratification" ought to be dated 21st day May 5815."

Officially, the Supreme Council, 33°, N.M.J. dates itself from May 15, 1867. This was the date of the "Union of 1867", when it merged with the competing Cerneau "Supreme Council" in New York. The current Ancient and Accepted Scottish Rite, Northern Masonic Jurisdiction of the United States, was thus formed.

Born in Boston, Massachusetts on December 29, 1809, Albert Pike is asserted within the Southern Jurisdiction as the man most responsible for the growth and success of the Scottish Rite from an obscure Masonic Rite in the mid-19th century to the international fraternity that it became. Pike received the 4th through the 32nd Degrees in March 1853 from Albert Mackey, in Charleston, South Carolina, and was appointed Deputy Inspector for Arkansas that same year.

At this point, the degrees were in a rudimentary form, and often included only a brief history and legend of each degree, as well as other brief details which usually lacked a workable ritual for their conferral. In 1855, the Supreme Council appointed a committee to prepare and compile rituals for the 4th through the 32nd Degrees. That committee was composed of Albert G. Mackey, John H. Honour, William S. Rockwell, Claude P. Samory, and Albert Pike. Of these five committee members, Pike did all the work of the committee.

In 1857 Pike completed his first revision of the 4°-32° ritual, and printed 100 copies. This revision, which Mackey dubbed the "Magnum Opus", was never adopted by the Supreme Council. According to Arturo de Hoyos, 33°, the Scottish Rite's Grand Historian, the Magnum Opus became the basis for future ritual revisions.

In March 1858, Pike was elected a member of the Supreme Council for the Southern Jurisdiction of the United States, and in January 1859 he became its Grand Commander. The American Civil War interrupted his work on the Scottish Rite rituals. About 1870 he, and the Supreme Council, moved to Washington, DC. In 1884 his revision of the rituals was complete.

Scottish Rite Grand Archivist and Grand Historian de Hoyos created the following chart of Pike's ritual revisions:

Pike also wrote lectures about all the degrees, which were published in 1871 under the title "Morals and Dogma of the Ancient and Accepted Scottish Rite of Freemasonry".
In 2000 the Southern Jurisdiction revised its ritual. The current ritual is based upon Pike's, but with some significant differences.

The thirty-three degrees of the Scottish Rite are conferred by several controlling bodies. The first of these is the Craft Lodge, which confers the Entered Apprentice, Fellowcraft, and Master Mason degrees. Craft lodges operate under the authority of national Grand Lodges, not the Scottish Rite. Attainment of the third Masonic degree, that of a Master Mason, represents the attainment of the highest rank in all of Masonry. Additional degrees such as those of the AASR are sometimes referred to as "appendant degrees", even where the degree numbering might imply a hierarchy. They represent a lateral movement in Masonic education rather than an upward movement, and are degrees of instruction rather than rank.

The AASR does have its own distinctive versions of the Craft rituals, but most lodges throughout the English-speaking world do not confer them. However, there are a handful of lodges in New Orleans and several other major U.S. cities that have traditionally conferred the Scottish Rite version of these degrees, as do most Lodges under the jurisdiction of the Grande Loge Nationale Française

According to Masonic historian Alain Bernheim, Belgian Masonic scholar Pierre Noël demonstrated in a 2002 paper that the AASR Craft degrees derived from the French translation of the Masonic exposé "Three Distinct Knocks", issued in London in 1760.

In 2000, the Southern Jurisdiction in the United States completed a revision of its ritual scripts. In 2004, the Northern Jurisdiction in the United States rewrote and reorganized its degrees. Further changes have occurred in 2006. The current titles of the degrees and their arrangement in the Southern Jurisdiction remains substantially unchanged from the beginning.

The list of degrees for the Supreme Councils of Australia, England and Wales, and most other jurisdictions largely agrees with that of the Southern Jurisdiction of the U.S. However, the list of degrees for the Northern Jurisdiction of the United States is now somewhat different and is given in the table below. The list of degrees of the Supreme Council of Canada reflects a mixture of the two, with some unique titles as well:

The Ancient and Accepted Scottish Rite in each country is governed by a Supreme Council. There is no international governing body; each Supreme Council in each country is sovereign unto itself in its own jurisdiction.

In Canada, whose Supreme Council was warranted in 1874 by that of England and Wales, the Rite is known as Ancient and Accepted Scottish Rite. The council is called "Supreme Council 33° Ancient and Accepted Scottish Rite of Freemasonry of Canada". Canada's Supreme Council office is located at 4 Queen Street South in Hamilton, Ontario. There are 45 local units or "Valleys" across Canada.

When Comte de Grasse-Tilly returned to France in 1804, he worked to establish the Ancient and Accepted Scottish Rite there. He founded the first Supreme Council in France that same year.

The Grand Orient of France signed a treaty of union in December 1804 with the Supreme Council of the 33rd Degree in France; the treaty declared that "the Grand Orient united to itself" the Supreme Council in France. This accord was applied until 1814. Thanks to this treaty, the Grand Orient of France took ownership, as it were, of the Scottish Rite.

From 1805 to 1814, the Grand Orient of France administered the first 18 degrees of the Rite, leaving the Supreme Council of France to administer the last 15. In 1815, five of the leaders of the Supreme Council founded the "Suprême Conseil des Rites" within the Grand Orient of France. The original Supreme Council of France fell dormant from 1815 to 1821.

The "Suprême Conseil des Isles d'Amérique" (founded in 1802 by Grasse-Tilly and revived around 1810 by his father-in-law Delahogue, who had also returned from the United States) breathed new life into the Supreme Council for the 33rd Degree in France. They merged into a single organization: the Supreme Council of France. This developed as an independent and sovereign Masonic power. It created symbolic lodges (those composed of the first three degrees, which otherwise would be federated around a Grand Lodge or a Grand Orient).

In 1894, the Supreme Council of France created the Grand Lodge of France. It became fully independent in 1904, when the Supreme Council of France ceased chartering new lodges. The Supreme Council of France still considers itself the overseer of all 33 degrees of the Rite. Relations between the two structures remain close, as shown by their organizing two joint meetings a year. 

In 1964, the Sovereign Grand Commander Charles Riandey, along with 400 to 500 members, left the jurisdiction of the Supreme Council of France and joined the Grande Loge Nationale Française. Because of his resignation and withdrawal of hundreds of members, there was no longer a Supreme Council of France. Riandey then reinitiated the 33 degrees of the rite in Amsterdam. With the support of the Supreme Council of the Southern Jurisdiction of the United States, he founded a new Supreme Council in France, called the "Suprême Conseil pour la France". This was the only one to be recognized by the Supreme Councils of the United States after it was designated in 1970 as the sole authority of the Scottish Rite for France by the Supreme Council of the Southern Jurisdiction (the oldest Supreme Council in the world) at the Barranquilla conference.

France has three different and arguably legitimate Supreme Councils:

The Ancient and Accepted Scottish Rite was established in Romania in 1881, a year after the National Grand Lodge of Romania was founded. On 27 December 1922, the Supreme Council of Scottish Rite of Romania, received the recognition of the Supreme Council of France in 1922, and recognition from the Supreme Council, Southern Jurisdiction of the United States in 1925.

Between 1948 – 1989 all of Romanian Freemasonry, including the Ancient and Accepted Scottish Rite of Romania, was banned by the Communist regime.

The Supreme Council of the Ancient and Accepted Scottish Rite of Romania was reconsecrated in 1993.

In England and Wales, whose Supreme Council was warranted by that of the Northern Jurisdiction of the USA (in 1845), the Rite is known colloquially as the "Rose Croix" or more formally as "The Ancient and Accepted Rite for England and Wales and its Districts and Chapters Overseas" (continental European jurisdictions retain the "Écossais"). England and Wales are divided into Districts, which administer the Rose Croix Chapters within their District; many degrees are conferred in name only, and degrees beyond the 18° are conferred only by the Supreme Council itself.

All candidates for membership must profess the Trinitarian Christian faith and have been Master masons for at least one year.

In England and Wales, the candidate is perfected in the 18th degree with the preceding degrees awarded in name only. Continuing to the 30th degree is restricted to those who have served in the chair of the Chapter. Elevation beyond the 30th degree is as in Scotland.

In Scotland, candidates are perfected in the 18th degree, with the preceding degrees awarded in name only. A minimum of a two-year interval is required before continuing to the 30th degree, again with the intervening degrees awarded by name only. Elevation beyond that is by invitation only, and numbers are severely restricted.

In the United States of America there are two Supreme Councils: one in Washington, D.C. (which controls the Southern Jurisdiction), and one in Lexington, Massachusetts (which controls the Northern Masonic Jurisdiction). They each have particular characteristics that make them different.
In the United States, members of the Scottish Rite can be elected to receive the 33° by the Supreme Council. It is conferred on members who have made major contributions to society or to Masonry in general.

Based in Washington, D.C., the Southern Jurisdiction (often referred to as the "Mother Supreme Council of the World") was founded in Charleston, South Carolina, in 1801. It oversees the Scottish Rite in 35 states, which are referred to as "Orients", and local bodies, which are called "Valleys".

In the Southern Jurisdiction of the United States, the Supreme Council consists of no more than 33 members and is presided over by a Grand Commander. Other members of the Supreme Council are called "Sovereign Grand Inspectors General" (S.G.I.G.), and each is the head of the Rite in his respective Orient (or state). Other heads of the various Orients who are not members of the Supreme Council are called "Deputies of the Supreme Council". The Supreme Council of the Southern Jurisdiction meets every odd year during the month of August at the House of the Temple, Scottish Rite of Freemasonry Southern Jurisdiction Headquarters, in Washington, D.C. During this conference, closed meetings between the Grand Commander and the S.G.I.G.'s are held, and many members of the fraternity from all over the world attend the open ceremony on the 5th of 6 council meeting days.

In the Southern Jurisdiction, a member who has been a 32° Scottish Rite Mason for 46 months or more is eligible to be elected to receive the "rank and decoration" of Knight Commander of the Court of Honour (K.C.C.H.) in recognition of outstanding service. After 46 months as a K.C.C.H. he is then eligible to be elected to the 33rd degree, upon approval of the Supreme Council and Grand Commander.
The Lexington, Massachusetts-based Northern Masonic Jurisdiction, formed in 1813, oversees the bodies in fifteen states: Connecticut, Delaware, Illinois, Indiana, Maine, Massachusetts, Michigan, New Jersey, New Hampshire, New York, Ohio, Pennsylvania, Rhode Island, Wisconsin and Vermont. The Northern Jurisdiction is only divided into "Valleys", not Orients. Each Valley has up to four Scottish Rite bodies, and each body confers a set of degrees.

In the Northern Jurisdiction, the Supreme Council consists of no more than 66 members. Those who are elected to membership on the Supreme Council are then designated "Active." In the Northern Jurisdiction
all recipients of the 33rd Degree are honorary members of the Supreme Council, and all members are referred to as a "Sovereign Grand Inspectors General." The head of the Rite in each State of the Northern Jurisdiction is called a "Deputy of the Supreme Council." Thus the highest ranking Scottish Rite officer in Ohio, is titled, "Deputy for Ohio", and so forth for each state. Additionally, each Deputy has one or more "Actives" to assist him in the administration of the state. Active members of the Supreme Council who have served faithfully for ten years, or reach the age of 75, may be designated "Active, Emeritus". The Northern Jurisdiction Supreme Council meets yearly, in the even years by an executive session, and in the odd years, with the full membership invited. The 33rd Degree is conferred on the odd years at the Annual Meeting.

In the Northern Jurisdiction, there is a 46-month requirement for eligibility to receive the 33rd degree, and while there is a Meritorious Service Award (as well as a Distinguished Service Award), they are not required intermediate steps towards the 33°.





</doc>
<doc id="28279" url="https://en.wikipedia.org/wiki?curid=28279" title="Sandman (disambiguation)">
Sandman (disambiguation)

The Sandman is a figure in folklore who brings good sleep and dreams.

Sandman may also refer to:













</doc>
<doc id="28284" url="https://en.wikipedia.org/wiki?curid=28284" title="Switch">
Switch

In electrical engineering, a switch is an electrical component that can "make" or "break" an electrical circuit, interrupting the current or diverting it from one conductor to another.
The mechanism of a switch removes or restores the conducting path in a circuit when it is operated. It may be operated manually, for example, a light switch or a keyboard button, may be operated by a moving object such as a door, or may be operated by some sensing element for pressure, temperature or flow. A switch will have one or more sets of contacts, which may operate simultaneously, sequentially, or alternately. Switches in high-powered circuits must operate rapidly to prevent destructive arcing, and may include special features to assist in rapidly interrupting a heavy current. Multiple forms of actuators are used for operation by hand or to sense position, level, temperature or flow. Special types are used, for example, for control of machinery, to reverse electric motors, or to sense liquid level. Many specialized forms exist. A common use is control of lighting, where multiple switches may be wired into one circuit to allow convenient control of light fixtures.

By analogy with the devices that select one or more possible paths for electric currents, devices that route information in a computer network are also called "switches" - these are usually more complicated than simple electromechanical toggles or pushbutton devices, and operate without direct human interaction.

The most familiar form of switch is a manually operated electromechanical device with one or more sets of electrical contacts, which are connected to external circuits. Each set of contacts can be in one of two states: either "closed" meaning the contacts are touching and electricity can flow between them, or "open", meaning the contacts are separated and the switch is nonconducting. The mechanism actuating the transition between these two states (open or closed) are usually (there are other types of actions) either an ""alternate action"" (flip the switch for continuous "on" or "off") or ""momentary"" (push for "on" and release for "off") type.

A switch may be directly manipulated by a human as a control signal to a system, such as a computer keyboard button, or to control power flow in a circuit, such as a light switch. Automatically operated switches can be used to control the motions of machines, for example, to indicate that a garage door has reached its full open position or that a machine tool is in a position to accept another workpiece. Switches may be operated by process variables such as pressure, temperature, flow, current, voltage, and force, acting as sensors in a process and used to automatically control a system. For example, a thermostat is a temperature-operated switch used to control a heating process. A switch that is operated by another electrical circuit is called a relay. Large switches may be remotely operated by a motor drive mechanism. Some switches are used to isolate electric power from a system, providing a visible point of isolation that can be padlocked if necessary to prevent accidental operation of a machine during maintenance, or to prevent electric shock.

An ideal switch would have no voltage drop when closed, and would have no limits on voltage or current rating. It would have zero rise time and fall time during state changes, and would change state without "bouncing" between on and off positions.

Practical switches fall short of this ideal; they have resistance, limits on the current and voltage they can handle, finite switching time, etc. The ideal switch is often used in circuit analysis as it greatly simplifies the system of equations to be solved, but this can lead to a less accurate solution. Theoretical treatment of the effects of non-ideal properties is required in the design of large networks of switches, as for example used in telephone exchanges.

In the simplest case, a switch has two conductive pieces, often metal, called "contacts", connected to an external circuit, that touch to complete (make) the circuit, and separate to open (break) the circuit. The contact material is chosen for its resistance to corrosion, because most metals form insulating oxides that would prevent the switch from working. Contact materials are also chosen on the basis of electrical conductivity, hardness (resistance to abrasive wear), mechanical strength, low cost and low toxicity.

Sometimes the contacts are plated with noble metals. They may be designed to wipe against each other to clean off any contamination. Nonmetallic conductors, such as conductive plastic, are sometimes used. To prevent the formation of insulating oxides, a minimum wetting current may be specified for a given switch design.

In electronics, switches are classified according to the arrangement of their contacts. A pair of contacts is said to be ""closed"" when current can flow from one to the other. When the contacts are separated by an insulating air gap, they are said to be ""open"", and no current can flow between them at normal voltages. The terms ""make"" for closure of contacts and ""break"" for opening of contacts are also widely used.

The terms pole and throw are also used to describe switch contact variations. The number of ""poles"" is the number of electrically separate switches which are controlled by a single physical actuator. For example, a ""2-pole"" switch has two separate, parallel sets of contacts that open and close in unison via the same mechanism. The number of ""throws"" is the number of separate wiring path choices other than "open" that the switch can adopt for each pole. A single-throw switch has one pair of contacts that can either be closed or open. A double-throw switch has a contact that can be connected to either of two other contacts, a triple-throw has a contact which can be connected to one of three other contacts, etc.
In a switch where the contacts remain in one state unless actuated, such as a push-button switch, the contacts can either be normally open (abbreviated "n.o." or "no") until closed by operation of the switch, or normally closed ("n.c." or "nc") and opened by the switch action. A switch with both types of contact is called a "changeover switch" or "double-throw switch". These may be "make-before-break" ("MBB" or shorting) which momentarily connects both circuits, or may be "break-before-make" ("BBM" or non-shorting) which interrupts one circuit before closing the other.

These terms have given rise to abbreviations for the types of switch which are used in the electronics industry such as ""single-pole, single-throw"" (SPST) (the simplest type, "on or off") or ""single-pole, double-throw"" (SPDT), connecting either of two terminals to the common terminal. In electrical power wiring (i.e., house and building wiring by electricians), names generally involve the suffix ""-way""; however, these terms differ between British English and American English (i.e., the terms "two way" and "three  way" are used with different meanings).

Switches with larger numbers of poles or throws can be described by replacing the "S" or "D" with a number (e.g. 3PST, SP4T, etc.) or in some cases the letter "T" (for "triple") or "Q" (for "quadruple"). In the rest of this article the terms "SPST", "SPDT" and "intermediate" will be used to avoid the ambiguity.

Contact bounce (also called "chatter") is a common problem with mechanical switches and relays. Switch and relay contacts are usually made of springy metals. When the contacts strike together, their momentum and elasticity act together to cause them to bounce apart one or more times before making steady contact. The result is a rapidly pulsed electric current instead of a clean transition from zero to full current. The effect is usually unimportant in power circuits, but causes problems in some analogue and logic circuits that respond fast enough to misinterpret the on‑off pulses as a data stream.

The effects of contact bounce can be eliminated by use of mercury-wetted contacts, but these are now infrequently used because of the hazard of mercury release. Alternatively, contact circuit voltages can be low-pass filtered to reduce or eliminate multiple pulses from appearing. In digital systems, multiple samples of the contact state can be taken at a low rate and examined for a steady sequence, so that contacts can settle before the contact level is considered reliable and acted upon. Bounce in SPDT switch contacts signals can be filtered out using a SR flip-flop (latch) or Schmitt trigger. All of these methods are referred to as 'debouncing'.

By analogy, the term "debounce" has arisen in the software development industry to describe rate-limiting or throttling the frequency of a method's execution.

When the power being switched is sufficiently large, the electron flow across opening switch contacts is sufficient to ionize the air molecules across the tiny gap between the contacts as the switch is opened, forming a gas plasma, also known as an electric arc. The plasma is of low resistance and is able to sustain power flow, even with the separation distance between the switch contacts steadily increasing. The plasma is also very hot and is capable of eroding the metal surfaces of the switch contacts. Electric current arcing causes significant degradation of the contacts and also significant electromagnetic interference (EMI), requiring the use of arc suppression methods.

Where the voltage is sufficiently high, an arc can also form as the switch is closed and the contacts approach. If the voltage potential is sufficient to exceed the breakdown voltage of the air separating the contacts, an arc forms which is sustained until the switch closes completely and the switch surfaces make contact.

In either case, the standard method for minimizing arc formation and preventing contact damage is to use a fast-moving switch mechanism, typically using a spring-operated tipping-point mechanism to assure quick motion of switch contacts, regardless of the speed at which the switch control is operated by the user. Movement of the switch control lever applies tension to a spring until a tipping point is reached, and the contacts suddenly snap open or closed as the spring tension is released.

As the power being switched increases, other methods are used to minimize or prevent arc formation. A plasma is hot and will rise due to convection air currents. The arc can be quenched with a series of non-conductive blades spanning the distance between switch contacts, and as the arc rises, its length increases as it forms ridges rising into the spaces between the blades, until the arc is too long to stay sustained and is extinguished. A "puffer" may be used to blow a sudden high velocity burst of gas across the switch contacts, which rapidly extends the length of the arc to extinguish it quickly.

Extremely large switches in excess of 100,000‑watt capacity often have switch contacts surrounded by something other than air to more rapidly extinguish the arc. For example, the switch contacts may operate in a vacuum, immersed in mineral oil, or in sulfur hexafluoride.

In AC power service, the current periodically passes through zero; this effect makes it harder to sustain an arc on opening. Manufacturers may rate switches with lower voltage or current rating when used in DC circuits.

When a switch is designed to switch significant power, the transitional state of the switch as well as the ability to withstand continuous operating currents must be considered. When a switch is in the on state, its resistance is near zero and very little power is dropped in the contacts; when a switch is in the off state, its resistance is extremely high and even less power is dropped in the contacts. However, when the switch is flicked, the resistance must pass through a state where a quarter of the load's rated power (or worse if the load is not purely resistive) is briefly dropped in the switch.

For this reason, power switches intended to interrupt a load current have spring mechanisms to make sure the transition between on and off is as short as possible regardless of the speed at which the user moves the rocker.

Power switches usually come in two types. A momentary on‑off switch (such as on a laser pointer) usually takes the form of a button and only closes the circuit when the button is depressed. A regular on‑off switch (such as on a flashlight) has a constant on-off feature. Dual-action switches incorporate both of these features.

When a strongly inductive load such as an electric motor is switched off, the current cannot drop instantaneously to zero; a spark will jump across the opening contacts. Switches for inductive loads must be rated to handle these cases. The spark will cause electromagnetic interference if not suppressed; a snubber network of a resistor and capacitor in series will quell the spark.

When turned on, an incandescent lamp draws a large inrush current of about ten times the steady-state current; as the filament heats up, its resistance rises and the current decreases to a steady-state value. A switch designed for an incandescent lamp load can withstand this inrush current.

"Wetting current" is the minimum current needing to flow through a mechanical switch while it is operated to break through any film of oxidation that may have been deposited on the switch contacts. The film of oxidation occurs often in areas with high humidity. Providing a sufficient amount of wetting current is a crucial step in designing systems that use delicate switches with small contact pressure as sensor inputs. Failing to do this might result in switches remaining electrically "open" due to contact oxidation.

The moving part that applies the operating force to the contacts is called the "actuator", and may be a toggle or "dolly", a rocker, a push-button or any type of mechanical linkage "(see photo)."

A switch normally maintains its set position once operated. A biased switch contains a mechanism that springs it into another position when released by an operator. The momentary push-button switch is a type of biased switch. The most common type is a "push-to-make" (or normally-open or NO) switch, which makes contact when the button is pressed and breaks when the button is released. Each key of a computer keyboard, for example, is a normally-open "push-to-make" switch. A "push-to-break" (or normally-closed or NC) switch, on the other hand, breaks contact when the button is pressed and makes contact when it is released. An example of a push-to-break switch is a button used to release a door held closed by an electromagnet. The interior lamp of a household refrigerator is controlled by a switch that is held open when the door is closed.

A rotary switch operates with a twisting motion of the operating handle with at least two positions. One or more positions of the switch may be momentary (biased with a spring), requiring the operator to hold the switch in the position. Other positions may have a detent to hold the position when released. A rotary switch may have multiple levels or "decks" in order to allow it to control multiple circuits.

One form of rotary switch consists of a spindle or "rotor" that has a contact arm or "spoke" which projects from its surface like a cam. It has an array of terminals, arranged in a circle around the rotor, each of which serves as a contact for the "spoke" through which any one of a number of different electrical circuits can be connected to the rotor. The switch is layered to allow the use of multiple poles, each layer is equivalent to one pole. Usually such a switch has a detent mechanism so it "clicks" from one active position to another rather than stalls in an intermediate position. Thus a rotary switch provides greater pole and throw capabilities than simpler switches do.

Other types use a cam mechanism to operate multiple independent sets of contacts.

Rotary switches were used as channel selectors on television receivers until the early 1970s, as range selectors on electrical metering equipment, as band selectors on multi-band radios and other similar purposes. In industry, rotary switches are used for control of measuring instruments, switchgear, or in control circuits. For example, a radio controlled overhead crane may have a large multi-circuit rotary switch to transfer hard-wired control signals from the local manual controls in the cab to the outputs of the remote control receiver.

A toggle switch is a class of electrical switches that are manually actuated by a mechanical lever, handle, or rocking mechanism.

Toggle switches are available in many different styles and sizes, and are used in numerous applications. Many are designed to provide the simultaneous actuation of multiple sets of electrical contacts, or the control of large amounts of electric current or mains voltages.

The word "toggle" is a reference to a kind of mechanism or joint consisting of two arms, which are almost in line with each other, connected with an elbow-like pivot. However, the phrase "toggle switch" is applied to a switch with a short handle and a positive snap-action, whether it actually contains a toggle mechanism or not. Similarly, a switch where a definitive click is heard, is called a "positive on-off switch". A very common use of this type of switch is to switch lights or other electrical equipment on or off. Multiple toggle switches may be mechanically interlocked to prevent forbidden combinations.

In some contexts, particularly computing, a toggle switch, or the action of toggling, is understood in the different sense of a mechanical or software switch that alternates between two states each time it is activated, regardless of mechanical construction. For example, the caps lock key on a computer causes all letters to be generated in capitals after it is pressed once; pressing it again reverts to lower-case letters.

Switches can be designed to respond to any type of mechanical stimulus: for example, vibration (the trembler switch), tilt, air pressure, fluid level (a float switch), the turning of a key (key switch), linear or rotary movement (a limit switch or microswitch), or presence of a magnetic field (the reed switch). Many switches are operated automatically by changes in some environmental condition or by motion of machinery. A limit switch is used, for example, in machine tools to interlock operation with the proper position of tools. In heating or cooling systems a sail switch ensures that air flow is adequate in a duct. Pressure switches respond to fluid pressure.

The mercury switch consists of a drop of mercury inside a glass bulb with two or more contacts. The two contacts pass through the glass, and are connected by the mercury when the bulb is tilted to make the mercury roll on to them.

This type of switch performs much better than the ball tilt switch, as the liquid metal connection is unaffected by dirt, debris and oxidation, it wets the contacts ensuring a very low resistance bounce-free connection, and movement and vibration do not produce a poor contact. These types can be used for precision works.

It can also be used where arcing is dangerous (such as in the presence of explosive vapour) as the entire unit is sealed.

Knife switches consist of a flat metal blade, hinged at one end, with an insulating handle for operation, and a fixed contact. When the switch is closed, current flows through the hinged pivot and blade and through the fixed contact. Such switches are usually not enclosed. The knife and contacts are typically formed of copper, steel, or brass, depending on the application. Fixed contacts may be backed up with a spring. Several parallel blades can be operated at the same time by one handle. The parts may be mounted on an insulating base with terminals for wiring, or may be directly bolted to an insulated switch board in a large assembly. Since the electrical contacts are exposed, the switch is used only where people cannot accidentally come in contact with the switch or where the voltage is so low as to not present a hazard.

Knife switches are made in many sizes from miniature switches to large devices used to carry thousands of amperes. In electrical transmission and distribution, gang-operated switches are used in circuits up to the highest voltages.

The disadvantages of the knife switch are the slow opening speed and the proximity of the operator to exposed live parts. Metal-enclosed safety disconnect switches are used for isolation of circuits in industrial power distribution. Sometimes spring-loaded auxiliary blades are fitted which momentarily carry the full current during opening, then quickly part to rapidly extinguish the arc.

A footswitch is a rugged switch which is operated by foot pressure. An example of use is in the control of a machine tool, allowing the operator to have both hands free to manipulate the workpiece. The foot control of an electric guitar is also a footswitch.

A DPDT switch has six connections, but since polarity reversal is a very common usage of DPDT switches, some variations of the DPDT switch are internally wired specifically for polarity reversal. These crossover switches only have four terminals rather than six. Two of the terminals are inputs and two are outputs. When connected to a battery or other DC source, the 4-way switch selects from either normal or reversed polarity. Such switches can also be used as intermediate switches in a multiway switching system for control of lamps by more than two switches.

In building wiring, light switches are installed at convenient locations to control lighting and occasionally other circuits. By use of multiple-pole switches, multiway switching control of a lamp can be obtained from two or more places, such as the ends of a corridor or stairwell. A wireless light switch allows remote control of lamps for convenience; some lamps include a touch switch which electronically controls the lamp if touched anywhere. In public buildings several types of vandal resistant switches are used to prevent unauthorized use.

A relay is an electrically operated switch. Many relays use an electromagnet to operate a switching mechanism mechanically, but other operating principles are also used. Solid-state relays control power circuits with no moving parts, instead using a semiconductor device to perform switching—often a silicon-controlled rectifier or triac.

The analogue switch uses two MOSFET transistors in a transmission gate arrangement as a switch that works much like a relay, with some advantages and several limitations compared to an electromechanical relay.

The power transistor(s) in a switching voltage regulator, such as a power supply unit, are used like a switch to alternately let power flow and block power from flowing.

Many people use metonymy to call a variety of devices "switches" that conceptually connect or disconnect signals and communication paths between electrical devices, analogous to the way mechanical switches connect and disconnect paths for electrons to flow between two conductors. Early telephone systems used an automatically operated Strowger switch to connect telephone callers; telephone exchanges contain one or more crossbar switches today.

Since the advent of digital logic in the 1950s, the term "switch" has spread to a variety of digital active devices such as transistors and logic gates whose function is to change their output state between two logic levels or connect different signal lines, and even computers, network switches, whose function is to provide connections between different ports in a computer network. The term 'switched' is also applied to telecommunications networks, and signifies a network that is circuit switched, providing dedicated circuits for communication between end nodes, such as the public switched telephone network. The common feature of all these usages is they refer to devices that control a binary state: they are either "on" or "off", "closed" or "open", "connected" or "not connected".



</doc>
<doc id="28287" url="https://en.wikipedia.org/wiki?curid=28287" title="Sutra">
Sutra

A sutra (; Sanskrit: IAST: "sūtra"; Pali: "sutta") is a religious discourse (teaching) in text form originating from the spiritual traditions of India, particularly Hinduism, Buddhism, and Jainism.

The term "sutra" may refer to a single aphorism, a collection of aphorisms or a more lengthy prose teaching. Sutras are considered a genre of ancient and medieval Indian texts.

In Hinduism, sutras are a distinct type of literary composition, a compilation of short aphoristic statements. Each sutra is any short rule, like a theorem distilled into few words or syllables, around which teachings of ritual, philosophy, grammar, or any field of knowledge can be woven. The oldest sutras of Hinduism are found in the Brahmana and Aranyaka layers of the Vedas. Every school of Hindu philosophy, Vedic guides for rites of passage, various fields of arts, law, and social ethics developed respective sutras, which helped teach and transmit ideas from one generation to the next.

In Buddhism, sutras, also known as "suttas", are canonical scriptures, many of which are regarded as records of the oral teachings of Gautama Buddha. The Pali form of the word, "sutta", is used exclusively to refer to the scriptures of the early Pali Canon, the only texts recognized by Theravada Buddhism as canonical.

In Jainism, sutras also known as "suyas" are canonical sermons of Mahavira contained in the Jain Agamas as well as some later (post-canonical) normative texts.

The Sanskrit word "Sūtra" (Sanskrit: सूत्र, Pali: "sūtta", Ardha Magadhi: "sūya") means "string, thread". The root of the word is "siv", that which sews and holds things together. The word is related to "sūci" (Sanskrit: सूचि) meaning "needle, list", and "sūnā" (Sanskrit: सूना) meaning "woven".

In the context of literature, "sūtra" means a distilled collection of syllables and words, any form or manual of "aphorism, rule, direction" hanging together like threads with which the teachings of ritual, philosophy, grammar, or any field of knowledge can be woven.

A "sūtra" is any short rule, states Moriz Winternitz, in Indian literature; it is "a theorem condensed in few words". A collection of "sūtras" becomes a text, and this is also called "sūtra" (often capitalized in Western literature).

A "sūtra" is different from other components such as "Shlokas", "Anuvyakhayas" and "Vyakhyas" found in ancient Indian literature. A "sūtra" is a condensed rule which succinctly states the message, while a "Shloka" is a verse that conveys the complete message and is structured to certain rules of musical meter, a "Anuvyakhaya" is an explanation of the reviewed text, while a "Vyakhya" is a comment by the reviewer.

Sutras first appear in the Brahmana and Aranyaka layer of Vedic literature. They grow in the Vedangas, such as the Shrauta Sutras and Kalpa Sutras. These were designed so that they can be easily communicated from a teacher to student, memorized by the recipient for discussion or self-study or as reference.

A sutra by itself is condensed shorthand, and the threads of syllable are difficult to decipher or understand, without associated scholarly Bhasya or deciphering commentary that fills in the "woof".

The oldest manuscripts that have survived into the modern era, that contain extensive sutras, are part of the Vedas dated to be from the late 2nd millennium BCE through mid 1st-millennium BCE. The Aitareya Aranyaka for example, states Winternitz, is primarily a collection of "sutras". Their use and ancient roots are attested by sutras being mentioned in larger genre of ancient non-Vedic Hindu literature called "Gatha", "Narashansi", "Itihasa", and "Akhyana" (songs, legends, epics, and stories).

In the history of Indian literature, large compilations of sutras, in diverse fields of knowledge, have been traced to the period from 600 BCE to 200 BCE (mostly after Buddha and Mahavira), and this has been called the "sutras period". This period followed the more ancient "Chhandas period", "Mantra period" and "Brahmana period".

Some of the earliest surviving specimen of "sutras" of Hinduism are found in the "Anupada Sutras" and "Nidana Sutras". The former distills the epistemic debate whether Sruti or Smriti or neither must be considered the more reliable source of knowledge, while the latter distills the rules of musical meters for Samaveda chants and songs.

A larger collection of ancient sutra literature in Hinduism corresponds to the six Vedangas, or six limbs of the Vedas. These are six subjects that were called in the Vedas as necessary for complete mastery of the Vedas. The six subjects with their own "sutras" were "pronunciation (Shiksha), meter (Chandas), grammar (Vyakarana), explanation of words (Nirukta), time keeping through astronomy (Jyotisha), and ceremonial rituals (Kalpa). The first two, states Max Muller, were considered in the Vedic era to be necessary for reading the Veda, the second two for understanding it, and the last two for deploying the Vedic knowledge at yajnas (fire rituals). The "sutras" corresponding to these are embedded inside the Brahmana and Aranyaka layers of the Vedas. Taittiriya Aranyaka, for example in Book 7, embeds sutras for accurate pronunciation after the terse phrases "On Letters", "On Accents", "On Quantity", "On Delivery", and "On Euphonic Laws".

The fourth and often the last layer of philosophical, speculative text in the Vedas, the Upanishads, too have embedded sutras such as those found in the Taittiriya Upanishad.

The compendium of ancient Vedic sutra literature that has survived, in full or fragments, includes the Kalpa Sutras, Smarta Sutras, Srauta Sutras, Dharma Sutras, Grhya Sutras, and Sulba Sutras. Other fields for which ancient sutras are known include etymology, phonetics, and grammar.

Some examples of sutra texts in various schools of Hindu philosophy include:



Some scholars consider that the Buddhist use of "sutra" is a faulty Sanskritization of the Prakrit or Pali word "sutta" and that the latter actually represented Sanskrit "sūkta", "well spoken, good news". The early Buddhist sutras do not present the aphoristic, nearly cryptic nature of the Hindu sutras even though they also have been designed for mnemonic purposes in an oral tradition. On the contrary, they are most often lengthy, with many repetitions which serve the mnemonic purpose of the audience. They share the character of sermons of "good news" with the Jaina sutras, whose original name of "sūya" in Ardha Magadhi can derive from Sanskrit "sūkta", but hardly from "sutra".

In Buddhism, sutra or sutta refers mostly to canonical scriptures.

In Chinese, these are known as 經 (pinyin: "jīng"). These teachings are assembled in part of the Tripiṭaka which is called the "Sutta Pitaka". There are many important or influential Mahayana texts, such as the "Platform Sutra" and the "Lotus Sutra", that are called sutras despite being attributed to much later authors.

In the Jain tradition, sutras are an important genre of "fixed text", which used to be memorized.

The Kalpa Sūtra is, for example, a Jain text with scripture of monastic rules, as well as the biographies of the Jain Tirthankaras. Many sutras discuss all aspects of ascetic and lay life in Jainism. Various ancient sutras particularly from the early 1st millennium CE, for example, states M. Whitney Kelting, recommend "bhakti as devotionalism is a central part of a Jain practice".

The surviving scriptures of Jaina tradition, such as the Acaranga Sutra (Agamas) exist in sutra format, as is the Tattvartha Sutra – a Sanskrit text accepted by all four Jainism sects as the most authoritative philosophical text that completely summarizes the foundations of Jainism.




</doc>
<doc id="28288" url="https://en.wikipedia.org/wiki?curid=28288" title="Samurai">
Samurai

In Japanese, they are usually referred to as or . According to translator William Scott Wilson: "In Chinese, the character 侍 was originally a verb meaning 'to wait upon', 'accompany persons' in the upper ranks of society, and this is also true of the original term in Japanese, "saburau". In both countries the terms were nominalized to mean 'those who serve in close attendance to the nobility', the Japanese term "saburai" being the nominal form of the verb." According to Wilson, an early reference to the word "samurai" appears in the "Kokin Wakashū" (905–914), the first imperial anthology of poems, completed in the first part of the 10th century.

By the end of the 12th century, "samurai" became almost entirely synonymous with "bushi", and the word was closely associated with the middle and upper echelons of the warrior class. The samurai were usually associated with a clan and their lord, and were trained as officers in military tactics and grand strategy. While the samurai numbered less than 10% of then Japan's population, their teachings can still be found today in both everyday life and in modern Japanese martial arts.

Following the Battle of Hakusukinoe against Tang China and Silla in 663 AD, which led to a retreat from Korean affairs, Japan underwent widespread reform. One of the most important was that of the Taika Reform, issued by Prince Naka-no-Ōe (Emperor Tenji) in 646 AD. This edict allowed the Japanese aristocracy to adopt the Tang dynasty political structure, bureaucracy, culture, religion, and philosophy. As part of the Taihō Code of 702 AD, and the later Yōrō Code, the population was required to report regularly for the census, a precursor for national conscription. With an understanding of how the population was distributed, Emperor Monmu introduced a law whereby 1 in 3–4 adult males were drafted into the national military. These soldiers were required to supply their own weapons, and in return were exempted from duties and taxes. This was one of the first attempts by the Imperial government to form an organized army modeled after the Chinese system. It was called "Gundan-Sei" () by later historians and is believed to have been short-lived.
The Taihō Code classified most of the Imperial bureaucrats into 12 ranks, each divided into two sub-ranks, 1st rank being the highest adviser to the Emperor. Those of 6th rank and below were referred to as "samurai" and dealt with day-to-day affairs. Although these "samurai" were civilian public servants, the modern word is believed to have derived from this term. Military men, however, would not be referred to as "samurai" for many more centuries.

In the early Heian period, during the late 8th and early 9th centuries, Emperor Kanmu sought to consolidate and expand his rule in northern Honshū, and sent military campaigns against the Emishi, who resisted the governance of the Kyoto-based imperial court. Emperor Kanmu introduced the title of "sei'i-taishōgun" (), or "shōgun", and began to rely on the powerful regional clans to conquer the Emishi. Skilled in mounted combat and archery (kyūdō), these clan warriors became the Emperor's preferred tool for putting down rebellions; the most well-known of which was Sakanoue no Tamuramaro. Though this is the first known use of the title "shōgun", it was a temporary title and was not imbued with political power until the 13th century. At this time (the 7th to 9th centuries), the Imperial Court officials considered them to be merely a military section under the control of the Imperial Court.

Ultimately, Emperor Kanmu disbanded his army. From this time, the emperor's power gradually declined. While the emperor was still the ruler, powerful clans around Kyoto assumed positions as ministers, and their relatives bought positions as magistrates. To amass wealth and repay their debts, magistrates often imposed heavy taxes, resulting in many farmers becoming landless. Through protective agreements and political marriages, they accumulated political power, eventually surpassing the traditional aristocracy.

Some clans were originally formed by farmers who had taken up arms to protect themselves from the Imperial magistrates sent to govern their lands and collect taxes. These clans formed alliances to protect themselves against more powerful clans, and by the mid-Heian period, they had adopted characteristic Japanese armor and weapons.

Originally, the Emperor and non-warrior nobility employed these warrior nobles. In time they amassed enough manpower, resources and political backing, in the form of alliances with one another, to establish the first samurai-dominated government. As the power of these regional clans grew, their chief was typically a distant relative of the Emperor and a lesser member of either the Fujiwara, Minamoto, or Taira clans. Though originally sent to provincial areas for fixed four-year terms as magistrates, the "toryo" declined to return to the capital when their terms ended, and their sons inherited their positions and continued to lead the clans in putting down rebellions throughout Japan during the middle- and later-Heian period. Because of their rising military and economic power, the warriors ultimately became a new force in the politics of the Imperial court. Their involvement in the Hōgen Rebellion in the late Heian period consolidated their power, which later pitted the rivalry of Minamoto and Taira clans against each other in the Heiji Rebellion of 1160.

The victor, Taira no Kiyomori, became an imperial advisor and was the first warrior to attain such a position. He eventually seized control of the central government, establishing the first samurai-dominated government and relegating the Emperor to figurehead status. However, the Taira clan was still very conservative when compared to its eventual successor, the Minamoto, and instead of expanding or strengthening its military might, the clan had its women marry Emperors and exercise control through the Emperor.

The Taira and the Minamoto clashed again in 1180, beginning the Genpei War, which ended in 1185. Samurai fought at the naval battle of Dan-no-ura, at the Shimonoseki Strait which separates Honshu and Kyūshū in 1185. The victorious Minamoto no Yoritomo established the superiority of the samurai over the aristocracy. In 1190 he visited Kyoto and in 1192 became "Sei'i Taishōgun", establishing the Kamakura shogunate, or "Kamakura bakufu". Instead of ruling from Kyoto, he set up the shogunate in Kamakura, near his base of power. "Bakufu" means "tent government", taken from the encampments the soldiers would live in, in accordance with the Bakufu's status as a military government.

After the Genpei war, Yoritomo obtained the right to appoint "shugo" and "jitō", and was allowed to organize soldiers and police, and to collect a certain amount of tax. Initially, their responsibility was restricted to arresting rebels and collecting needed army provisions and they were forbidden from interfering with "Kokushi" officials, but their responsibility gradually expanded. Thus, the samurai-class appeared as the political ruling power in Japan.

Various samurai clans struggled for power during the Kamakura and Ashikaga shogunates. Zen Buddhism spread among the samurai in the 13th century and helped to shape their standards of conduct, particularly overcoming the fear of death and killing, but among the general populace Pure Land Buddhism was favored.

In 1274, the Mongol-founded Yuan dynasty in China sent a force of some 40,000 men and 900 ships to invade Japan in northern Kyūshū. Japan mustered a mere 10,000 samurai to meet this threat. The invading army was harassed by major thunderstorms throughout the invasion, which aided the defenders by inflicting heavy casualties. The Yuan army was eventually recalled and the invasion was called off. The Mongol invaders used small bombs, which was likely the first appearance of bombs and gunpowder in Japan.

The Japanese defenders recognized the possibility of a renewed invasion and began construction of a great stone barrier around Hakata Bay in 1276. Completed in 1277, this wall stretched for 20 kilometers around the border of the bay. It would later serve as a strong defensive point against the Mongols. The Mongols attempted to settle matters in a diplomatic way from 1275 to 1279, but every envoy sent to Japan was executed. This set the stage for one of the most famous engagements in Japanese history.

In 1281, a Yuan army of 140,000 men with 5,000 ships was mustered for another invasion of Japan. Northern Kyūshū was defended by a Japanese army of 40,000 men. The Mongol army was still on its ships preparing for the landing operation when a typhoon hit north Kyūshū island. The casualties and damage inflicted by the typhoon, followed by the Japanese defense of the Hakata Bay barrier, resulted in the Mongols again recalling their armies.

The thunderstorms of 1274 and the typhoon of 1281 helped the samurai defenders of Japan repel the Mongol invaders despite being vastly outnumbered. These winds became known as "kami-no-Kaze", which literally translates as "wind of the gods". This is often given a simplified translation as "divine wind". The "kami-no-Kaze" lent credence to the Japanese belief that their lands were indeed divine and under supernatural protection.

During this period, the tradition of Japananese swordsmithing developed using laminated or piled steel, a technique dating back over 2,000 years in the Mediterranean and Europe of combining layers of soft and hard steel to produce a blade with a very hard (but brittle) edge, capable of being highly sharpened, supported by a softer, tougher, more flexible spine. The Japanese swordsmiths refined this technique by using multiple layers of steel of varying composition, together with differential heat treatment, or tempering, of the finished blade, achieved by protecting part of it with a layer of clay while quenching (as explained in the article on Japanese swordsmithing). The craft was perfected in the 14th century by the great swordsmith Masamune. The Japanese sword (katana) became renowned around the world for its sharpness and resistance to breaking. Many swords made using these techniques were exported across the East China Sea, a few making their way as far as India.

Issues of inheritance caused family strife as primogeniture became common, in contrast to the division of succession designated by law before the 14th century. Invasions of neighboring samurai territories became common to avoid infighting, and bickering among samurai was a constant problem for the Kamakura and Ashikaga shogunates.

The "Sengoku jidai" ("warring states period") was marked by the loosening of samurai culture, with people born into other social strata sometimes making a name for themselves as warriors and thus becoming "de facto" samurai.

Japanese war tactics and technologies improved rapidly in the 15th and 16th centuries. Use of large numbers of infantry called ashigaru ("light-foot", due to their light armor), formed of humble warriors or ordinary people with "naga yari" (a long lance) or "naginata", was introduced and combined with cavalry in maneuvers. The number of people mobilized in warfare ranged from thousands to hundreds of thousands.

The arquebus, a matchlock gun, was introduced by the Portuguese via a Chinese pirate ship in 1543 and the Japanese succeeded in assimilating it within a decade. Groups of mercenaries with mass-produced arquebuses began playing a critical role. By the end of the Sengoku period, several hundred thousand firearms existed in Japan and massive armies numbering over 100,000 clashed in battles.

Oda Nobunaga was the well-known lord of the Nagoya area (once called Owari Province) and an exceptional example of a samurai of the Sengoku period. He came within a few years of, and laid down the path for his successors to follow, the reunification of Japan under a new "bakufu" (shogunate).

Oda Nobunaga made innovations in the fields of organization and war tactics, made heavy use of arquebuses, developed commerce and industry, and treasured innovation. Consecutive victories enabled him to realize the termination of the Ashikaga Bakufu and the disarmament of the military powers of the Buddhist monks, which had inflamed futile struggles among the populace for centuries. Attacking from the "sanctuary" of Buddhist temples, they were constant headaches to any warlord and even the Emperor who tried to control their actions. He died in 1582 when one of his generals, Akechi Mitsuhide, turned upon him with his army.

Importantly, Toyotomi Hideyoshi (see below) and Tokugawa Ieyasu, who founded the Tokugawa shogunate, were loyal followers of Nobunaga. Hideyoshi began as a peasant and became one of Nobunaga's top generals, and Ieyasu had shared his childhood with Nobunaga. Hideyoshi defeated Mitsuhide within a month, and was regarded as the rightful successor of Nobunaga by avenging the treachery of Mitsuhide.

These two were able to use Nobunaga's previous achievements on which build a unified Japan and there was a saying: "The reunification is a rice cake; Oda made it. Hashiba shaped it. In the end, only Ieyasu tastes it." (Hashiba is the family name that Toyotomi Hideyoshi used while he was a follower of Nobunaga.)

Toyotomi Hideyoshi, who became a grand minister in 1586, himself the son of a poor peasant family, created a law that the samurai caste became codified as permanent and hereditary, and that non-samurai were forbidden to carry weapons, thereby ending the social mobility of Japan up until that point, which lasted until the dissolution of the Edo shogunate by the Meiji revolutionaries.

It is important to note that the distinction between samurai and non-samurai was so obscure that during the 16th century, most male adults in any social class (even small farmers) belonged to at least one military organization of their own and served in wars before and during Hideyoshi's rule. It can be said that an "all against all" situation continued for a century.

The authorized samurai families after the 17th century were those that chose to follow Nobunaga, Hideyoshi and Ieyasu. Large battles occurred during the change between regimes, and a number of defeated samurai were destroyed, went "rōnin" or were absorbed into the general populace.

In 1592, and again in 1597, Toyotomi Hideyoshi, aiming to invade China () through Korea, mobilized an army of 160,000 peasants and samurai and deployed them to Korea. (See Hideyoshi's invasions of Korea, ). Taking advantage of arquebus mastery and extensive wartime experience from the Sengoku period, Japanese samurai armies made major gains in most of Korea. A few of the more famous samurai generals of this war were Katō Kiyomasa, Konishi Yukinaga, and Shimazu Yoshihiro. Katō Kiyomasa advanced to Orangkai territory (present-day Manchuria) bordering Korea to the northeast and crossed the border into Manchuria, but withdrew after retaliatory attacks from the Jurchens there, as it was clear he had outpaced the rest of the Japanese invasion force. Shimazu Yoshihiro led some 7,000 samurai and, despite being heavily outnumbered, defeated a host of allied Ming and Korean forces at the Battle of Sacheon in 1598, near the conclusion of the campaigns. Yoshihiro was feared as "Oni-Shimazu" ("Shimazu ogre") and his nickname spread across not only Korea but to Ming Dynasty China.

In spite of the superiority of Japanese land forces, ultimately the two expeditions failed, though they did devastate the Korean peninsula. The causes of the failure included Korean naval superiority (which, led by Admiral Yi Sun-sin, harassed Japanese supply lines continuously throughout the wars, resulting in supply shortages on land), the commitment of sizeable Ming forces to Korea, Korean guerrilla actions, wavering Japanese commitment to the campaigns as the wars dragged on, and the underestimation of resistance by Japanese commanders. In the first campaign of 1592, Korean defenses on land were caught unprepared, under-trained, and under-armed; they were rapidly overrun, with only a limited number of successfully resistant engagements against the more experienced and battle-hardened Japanese forces. During the second campaign, in 1597, however, Korean and Ming forces proved far more resilient and, with the support of continued Korean naval superiority, managed to limit Japanese gains to parts of southeastern Korea. The final death blow to the Japanese campaigns in Korea came with Hideyoshi's death in late 1598 and the recall of all Japanese forces in Korea by the Council of Five Elders (established by Hideyoshi to oversee the transition from his regency to that of his son Hideyori).

Many samurai forces that were active throughout this period were not deployed to Korea; most importantly, the "daimyōs" Tokugawa Ieyasu carefully kept forces under his command out of the Korean campaigns, and other samurai commanders who were opposed to Hideyoshi's domination of Japan either mulled Hideyoshi's call to invade Korea or contributed a small token force. Most commanders who opposed or otherwise resisted or resented Hideyoshi ended up as part of the so-called Eastern Army, while commanders loyal to Hideyoshi and his son (a notable exception to this trend was Katō Kiyomasa, who deployed with Tokugawa and the Eastern Army) were largely committed to the Western Army; the two opposing sides (so named for the relative geographical locations of their respective commanders' domains) would later clash, most notably at the Battle of Sekigahara, which was won by Tokugawa Ieyasu and the Eastern Forces, paving the way for the establishment of the Tokugawa shogunate.

Social mobility was high, as the ancient regime collapsed and emerging samurai needed to maintain a large military and administrative organizations in their areas of influence. Most of the samurai families that survived to the 19th century originated in this era, declaring themselves to be the blood of one of the four ancient noble clans: Minamoto, Taira, Fujiwara and Tachibana. In most cases, however, it is hard to prove these claims.

During the Tokugawa shogunate, samurai increasingly became courtiers, bureaucrats, and administrators rather than warriors. With no warfare since the early 17th century, samurai gradually lost their military function during the Tokugawa era (also called the Edo period). By the end of the Tokugawa era, samurai were aristocratic bureaucrats for the "daimyōs", with their "daishō", the paired long and short swords of the samurai (cf. katana and wakizashi) becoming more of a symbolic emblem of power rather than a weapon used in daily life. They still had the legal right to cut down any commoner who did not show proper respect , but to what extent this right was used is unknown. When the central government forced "daimyōs" to cut the size of their armies, unemployed rōnin became a social problem.

Theoretical obligations between a samurai and his lord (usually a "daimyō") increased from the Genpei era to the Edo era. They were strongly emphasized by the teachings of Confucius (551–479 BC) and Mencius (372–289 BC), which were required reading for the educated samurai class. The leading figures who introduced confucianism in Japan in the early Tokugawa period were Fujiwara Seika (1561–1619), Hayashi Razan (1583–1657) and Matsunaga Sekigo (1592–1657).

The conduct of samurai served as role model behavior for the other social classes. With time on their hands, samurai spent more time in pursuit of other interests such as becoming scholars.

The relative peace of the Tokugawa era was shattered with the arrival of Commodore Matthew Perry's massive U.S. Navy steamships in 1853. Perry used his superior firepower to force Japan to open its borders to trade. Prior to that only a few harbor towns, under strict control from the shogunate, were allowed to participate in Western trade, and even then, it was based largely on the idea of playing the Franciscans and Dominicans off against one another (in exchange for the crucial arquebus technology, which in turn was a major contributor to the downfall of the classical samurai).

From 1854, the samurai army and the navy were modernized. A naval training school was established in Nagasaki in 1855. Naval students were sent to study in Western naval schools for several years, starting a tradition of foreign-educated future leaders, such as Admiral Enomoto. French naval engineers were hired to build naval arsenals, such as Yokosuka and Nagasaki. By the end of the Tokugawa shogunate in 1867, the Japanese navy of the "shōgun" already possessed eight western-style steam warships around the flagship "Kaiyō Maru", which were used against pro-imperial forces during the Boshin War, under the command of Admiral Enomoto. A French Military Mission to Japan (1867) was established to help modernize the armies of the Bakufu.

The last showing of the original samurai was in 1867 when samurai from Chōshū and Satsuma provinces defeated the Shogunate forces in favor of the rule of the Emperor in the Boshin War (1868–1869). The two provinces were the lands of the "daimyōs" that submitted to Ieyasu after the Battle of Sekigahara (1600).

Emperor Meiji abolished the samurai's right to be the only armed force in favor of a more modern, western-style, conscripted army in 1873. Samurai became "Shizoku" () who retained some of their salaries, but the right to wear a katana in public was eventually abolished along with the right to execute commoners who paid them disrespect. The samurai finally came to an end after hundreds of years of enjoyment of their status, their powers, and their ability to shape the government of Japan. However, the rule of the state by the military class was not yet over. In defining how a modern Japan should be, members of the Meiji government decided to follow the footsteps of the United Kingdom and Germany, basing the country on the concept of "noblesse oblige". Samurai were not a political force under the new order. With the Meiji reforms in the late 19th century, the samurai class was abolished, and a western-style national army was established. The Imperial Japanese Armies were conscripted, but many samurai volunteered as soldiers, and many advanced to be trained as officers. Much of the Imperial Army officer class was of samurai origin, and were highly motivated, disciplined, and exceptionally trained.

The last samurai conflict was arguably in 1877, during the Satsuma Rebellion in the Battle of Shiroyama. This conflict had its genesis in the previous uprising to defeat the Tokugawa shogunate, leading to the Meiji Restoration. The newly formed government instituted radical changes, aimed at reducing the power of the feudal domains, including Satsuma, and the dissolution of samurai status. This led to the ultimately premature uprising, led by Saigō Takamori.

Samurai were many of the early exchange students, not directly because they were samurai, but because many samurai were literate and well-educated scholars. Some of these exchange students started private schools for higher educations, while many samurai took pens instead of guns and became reporters and writers, setting up newspaper companies, and others entered governmental service. Some samurai became businessmen. For example, Iwasaki Yatarō, who was the great-grandson of a samurai, established Mitsubishi.

Only the name Shizoku existed after that. After Japan lost World War II, the name Shizoku disappeared under the law on 1 January 1947.

The philosophies of Buddhism and Zen, and to a lesser extent Confucianism and Shinto, influenced the samurai culture. Zen meditation became an important teaching, because it offered a process to calm one's mind. The Buddhist concept of reincarnation and rebirth led samurai to abandon torture and needless killing, while some samurai even gave up violence altogether and became Buddhist monks after coming to believe that their killings were fruitless. Some were killed as they came to terms with these conclusions in the battlefield. The most defining role that Confucianism played in samurai philosophy was to stress the importance of the lord-retainer relationship—the loyalty that a samurai was required to show his lord.

Literature on the subject of "bushido" such as "Hagakure" ("Hidden in Leaves") by Yamamoto Tsunetomo and " Gorin no Sho" ("Book of the Five Rings") by Miyamoto Musashi, both written in the Edo period (1603–1868), contributed to the development of "bushidō" and Zen philosophy.

The philosophies of Buddhism and Zen, and to a lesser extent Confucianism and Shinto, are attributed to the development of the samurai culture. According to Robert Sharf, "The notion that Zen is somehow related to Japanese culture in general, and bushidō in particular, is familiar to Western students of Zen through the writings of D. T. Suzuki, no doubt the single most important figure in the spread of Zen in the West."

In an account of Japan sent to Father Ignatius Loyola at Rome, drawn from the statements of Anger (Han-Siro's western name), Xavier describes the importance of honor to the Japanese (Letter preserved at College of Coimbra):

In the first place, the nation with which we have had to do here surpasses in goodness any of the nations lately discovered. I really think that among barbarous nations there can be none that has more natural goodness than the Japanese. They are of a kindly disposition, not at all given to cheating, wonderfully desirous of honour and rank. Honour with them is placed above everything else. There are a great many poor among them, but poverty is not a disgrace to any one. There is one thing among them of which I hardly know whether it is practised anywhere among Christians. The nobles, however poor they may be, receive the same honour from the rest as if they were rich.

In the 13th century, Hōjō Shigetoki (1198–1261 AD) wrote: "When one is serving officially or in the master's court, he should not think of a hundred or a thousand people, but should consider only the importance of the master." Carl Steenstrup noted that 13th and 14th century warrior writings ("gunki") "portrayed the bushi in their natural element, war, eulogizing such virtues as reckless bravery, fierce family pride, and selfless, at times senseless devotion of master and man". Feudal lords such as Shiba Yoshimasa (1350–1410) stated that a warrior looked forward to a glorious death in the service of a military leader or the Emperor: "It is a matter of regret to let the moment when one should die pass by ... First, a man whose profession is the use of arms should think and then act upon not only his own fame, but also that of his descendants. He should not scandalize his name forever by holding his one and only life too dear ... One's main purpose in throwing away his life is to do so either for the sake of the Emperor or in some great undertaking of a military general. It is that exactly that will be the great fame of one's descendants."

In 1412 AD, Imagawa Sadayo wrote a letter of admonishment to his brother stressing the importance of duty to one's master. Imagawa was admired for his balance of military and administrative skills during his lifetime, and his writings became widespread. The letters became central to Tokugawa-era laws and became required study material for traditional Japanese until World War II:

"First of all, a samurai who dislikes battle and has not put his heart in the right place even though he has been born in the house of the warrior, should not be reckoned among one's retainers ... It is forbidden to forget the great debt of kindness one owes to his master and ancestors and thereby make light of the virtues of loyalty and filial piety ... It is forbidden that one should ... attach little importance to his duties to his master ... There is a primary need to distinguish loyalty from disloyalty and to establish rewards and punishments."

Similarly, the feudal lord Takeda Nobushige (1525–1561) stated: "In matters both great and small, one should not turn his back on his master's commands ... One should not ask for gifts or enfiefments from the master ... No matter how unreasonably the master may treat a man, he should not feel disgruntled ... An underling does not pass judgments on a superior."

Nobushige's brother Takeda Shingen (1521–1573) also made similar observations: "One who was born in the house of a warrior, regardless of his rank or class, first acquaints himself with a man of military feats and achievements in loyalty ... Everyone knows that if a man doesn't hold filial piety toward his own parents he would also neglect his duties toward his lord. Such a neglect means a disloyalty toward humanity. Therefore such a man doesn't deserve to be called 'samurai'."

The feudal lord Asakura Yoshikage (1428–1481) wrote: "In the fief of the Asakura, one should not determine hereditary chief retainers. A man should be assigned according to his ability and loyalty." Asakura also observed that the successes of his father were obtained by the kind treatment of the warriors and common people living in domain. By his civility, "all were willing to sacrifice their lives for him and become his allies."

Katō Kiyomasa was one of the most powerful and well-known lords of the Sengoku period. He commanded most of Japan's major clans during the invasion of Korea (1592–1598). In a handbook he addressed to "all samurai, regardless of rank", he told his followers that a warrior's only duty in life was to "grasp the long and the short swords and to die". He also ordered his followers to put forth great effort in studying the military classics, especially those related to loyalty and filial piety. He is best known for his quote: "If a man does not investigate into the matter of Bushido daily, it will be difficult for him to die a brave and manly death. Thus it is essential to engrave this business of the warrior into one's mind well."

Nabeshima Naoshige (1538–1618 AD) was another Sengoku "daimyō" who fought alongside Kato Kiyomasa in Korea. He stated that it was shameful for any man to have not risked his life at least once in the line of duty, regardless of his rank. Nabeshima's sayings would be passed down to his son and grandson and would become the basis for Tsunetomo Yamamoto's "Hagakure". He is best known for his saying "The way of the Samurai is in desperateness. Ten men or more cannot kill such a man."

Torii Mototada (1539–1600) was a feudal lord in the service of Tokugawa Ieyasu. On the eve of the battle of Sekigahara, he volunteered to remain behind in the doomed Fushimi Castle while his lord advanced to the east. Torii and Tokugawa both agreed that the castle was indefensible. In an act of loyalty to his lord, Torii chose to remain behind, pledging that he and his men would fight to the finish. As was custom, Torii vowed that he would not be taken alive. In a dramatic last stand, the garrison of 2,000 men held out against overwhelming odds for ten days against the massive army of Ishida Mitsunari's 40,000 warriors. In a moving last statement to his son Tadamasa, he wrote:

"It is not the Way of the Warrior [i.e., "bushidō"] to be shamed and avoid death even under circumstances that are not particularly important. It goes without saying that to sacrifice one's life for the sake of his master is an unchanging principle. That I should be able to go ahead of all the other warriors of this country and lay down my life for the sake of my master's benevolence is an honor to my family and has been my most fervent desire for many years."

It is said that both men cried when they parted ways, because they knew they would never see each other again. Torii's father and grandfather had served the Tokugawa before him and his own brother had already been killed in battle. Torii's actions changed the course of Japanese history. Ieyasu Tokugawa would successfully raise an army and win at Sekigahara.

The translator of "Hagakure", William Scott Wilson observed examples of warrior emphasis on death in clans other than Yamamoto's: "he (Takeda Shingen) was a strict disciplinarian as a warrior, and there is an exemplary story in the "Hagakure" relating his execution of two brawlers, not because they had fought, but because they had not fought to the death".

The rival of Takeda Shingen (1521–1573) was Uesugi Kenshin (1530–1578), a legendary Sengoku warlord well-versed in the Chinese military classics and who advocated the "way of the warrior as death". Japanese historian Daisetz Teitaro Suzuki describes Uesugi's beliefs as: "Those who are reluctant to give up their lives and embrace death are not true warriors ... Go to the battlefield firmly confident of victory, and you will come home with no wounds whatever. Engage in combat fully determined to die and you will be alive; wish to survive in the battle and you will surely meet death. When you leave the house determined not to see it again you will come home safely; when you have any thought of returning you will not return. You may not be in the wrong to think that the world is always subject to change, but the warrior must not entertain this way of thinking, for his fate is always determined."

Families such as the Imagawa were influential in the development of warrior ethics and were widely quoted by other lords during their lifetime. The writings of Imagawa Sadayo were highly respected and sought out by Tokugawa Ieyasu as the source of Japanese Feudal Law. These writings were a required study among traditional Japanese until World War II.

Historian H. Paul Varley notes the description of Japan given by Jesuit leader St. Francis Xavier (1506–1552): "There is no nation in the world which fears death less." Xavier further describes the honour and manners of the people: "I fancy that there are no people in the world more punctilious about their honour than the Japanese, for they will not put up with a single insult or even a word spoken in anger." Xavier spent the years 1549–1551 converting Japanese to Christianity. He also observed: "The Japanese are much braver and more warlike than the people of China, Korea, Ternate and all of the other nations around the Philippines."

In December 1547, Francis was in Malacca (Malaysia) waiting to return to Goa (India) when he met a low-ranked samurai named Anjiro (possibly spelled "Yajiro"). Anjiro was not an intellectual, but he impressed Xavier because he took careful notes of everything he said in church. Xavier made the decision to go to Japan in part because this low-ranking samurai convinced him in Portuguese that the Japanese people were highly educated and eager to learn. They were hard workers and respectful of authority. In their laws and customs they were led by reason, and, should the Christian faith convince them of its truth, they would accept it en masse.

By the 12th century, upper-class samurai were highly literate due to the general introduction of Confucianism from China during the 7th to 9th centuries and in response to their perceived need to deal with the imperial court, who had a monopoly on culture and literacy for most of the Heian period. As a result, they aspired to the more cultured abilities of the nobility.

Examples such as Taira Tadanori (a samurai who appears in the "Heike Monogatari") demonstrate that warriors idealized the arts and aspired to become skilled in them.

Tadanori was famous for his skill with the pen and the sword or the "bun and the bu", the harmony of fighting and learning.
Samurai were expected to be cultured and literate, and admired the ancient saying "bunbu-ryōdō" (文武両道, lit., literary arts, military arts, both ways) or "The pen and the sword in accord". By the time of the Edo period, Japan had a higher literacy comparable to that in central Europe.

The number of men who actually achieved the ideal and lived their lives by it was high. An early term for warrior, "uruwashii", was written with a kanji that combined the characters for literary study ("bun" 文) and military arts ("bu" 武), and is mentioned in the Heike Monogatari (late 12th century). The Heike Monogatari makes reference to the educated poet-swordsman ideal in its mention of Taira no Tadanori's death:

In his book "Ideals of the Samurai" translator William Scott Wilson states: "The warriors in the "Heike Monogatari" served as models for the educated warriors of later generations, and the ideals depicted by them were not assumed to be beyond reach. Rather, these ideals were vigorously pursued in the upper echelons of warrior society and recommended as the proper form of the Japanese man of arms. With the Heike Monogatari, the image of the Japanese warrior in literature came to its full maturity." Wilson then translates the writings of several warriors who mention the Heike Monogatari as an example for their men to follow.

Plenty of warrior writings document this ideal from the 13th century onward. Most warriors aspired to or followed this ideal otherwise there would have been no cohesion in the samurai armies.

As aristocrats for centuries, samurai developed their own cultures that influenced Japanese culture as a whole. The culture associated with the samurai such as the tea ceremony, monochrome ink painting, rock gardens and poetry was adopted by warrior patrons throughout the centuries 1200–1600. These practices were adapted from the Chinese arts. Zen monks introduced them to Japan and they were allowed to flourish due to the interest of powerful warrior elites. Musō Soseki (1275–1351) was a Zen monk who was advisor to both Emperor Go-Daigo and General Ashikaga Takauji (1304–58). Musō, as well as other monks, served as a political and cultural diplomat between Japan and China. Musō was particularly well known for his garden design. Another Ashikaga patron of the arts was Yoshimasa. His cultural advisor, the Zen monk Zeami, introduced the tea ceremony to him. Previously, tea had been used primarily for Buddhist monks to stay awake during meditation.

In general, samurai, aristocrats, and priests had a very high literacy rate in kanji. Recent studies have shown that literacy in kanji among other groups in society was somewhat higher than previously understood. For example, court documents, birth and death records and marriage records from the Kamakura period, submitted by farmers, were prepared in Kanji. Both the kanji literacy rate and skills in math improved toward the end of Kamakura period.

Some samurai had "buke bunko", or "warrior library", a personal library that held texts on strategy, the science of warfare, and other documents that would have proved useful during the warring era of feudal Japan. One such library held 20,000 volumes. The upper class had "Kuge bunko", or "family libraries", that held classics, Buddhist sacred texts, and family histories, as well as genealogical records.

Literacy was generally high among the warriors and the common classes as well. The feudal lord Asakura Norikage (1474–1555 AD) noted the great loyalty given to his father, due to his polite letters, not just to fellow samurai, but also to the farmers and townspeople:

There were to Lord Eirin's character many high points difficult to measure, but according to the elders the foremost of these was the way he governed the province by his civility. It goes without saying that he acted this way toward those in the samurai class, but he was also polite in writing letters to the farmers and townspeople, and even in addressing these letters he was gracious beyond normal practice. In this way, all were willing to sacrifice their lives for him and become his allies.

In a letter dated 29 January 1552, St Francis Xavier observed the ease of which the Japanese understood prayers due to the high level of literacy in Japan at that time:

There are two kinds of writing in Japan, one used by men and the other by women; and for the most part both men and women, especially of the nobility and the commercial class, have a literary education. The bonzes, or bonzesses, in their monasteries teach letters to the girls and boys, though rich and noble persons entrust the education of their children to private tutors.<br>
Most of them can read, and this is a great help to them for the easy understanding of our usual prayers and the chief points of our holy religion.

In a letter to Father Ignatius Loyola at Rome, Xavier further noted the education of the upper classes:

The Nobles send their sons to monasteries to be educated as soon as they are 8 years old, and they remain there until they are 19 or 20, learning reading, writing and religion; as soon as they come out, they marry and apply themselves to politics.

They are discreet, magnanimous and lovers of virtue and letters, honouring learned men very much.

In a letter dated 11 November 1549, Xavier described a multi-tiered educational system in Japan consisting of "universities", "colleges", "academies" and hundreds of monasteries that served as a principal center for learning by the populace:

But now we must give you an account of our stay at Cagoxima. We put into that port because the wind was adverse to our sailing to Meaco, which is the largest city in Japan, and most famous as the residence of the King and the Princes. It is said that after four months are passed the favourable season for a voyage to Meaco will return, and then with the good help of God we shall sail thither. The distance from Cagoxima is three hundred leagues. We hear wonderful stories about the size of Meaco: they say that it consists of more than ninety thousand dwellings. There is a very famous University there, as well as five chief colleges of students, and more than two hundred monasteries of bonzes, and of others who are like coenobites, called Legioxi, as well as of women of the same kind, who are called Hamacutis. Besides this of Meaco, there are in Japan five other principal academies, at Coya, at Negu, at Fisso, and at Homia. These are situated round Meaco, with short distances between them, and each is frequented by about three thousand five hundred scholars. Besides these there is the Academy at Bandou, much the largest and most famous in all Japan, and at a great distance from Meaco. Bandou is a large territory, ruled by six minor princes, one of whom is more powerful than the others and is obeyed by them, being himself subject to the King of Japan, who is called the Great King of Meaco. The things that are given out as to the greatness and celebrity of these universities and cities are so wonderful as to make us think of seeing them first with our own eyes and ascertaining the truth, and then when we have discovered and know how things really are, of writing an account of them to you. They say that there are several lesser academies besides those which we have mentioned.

A samurai was usually named by combining one kanji from his father or grandfather and one new kanji. Samurai normally used only a small part of their total name.

For example, the full name of Oda Nobunaga would be "Oda Kazusanosuke Saburo Nobunaga" (), in which "Oda" is a clan or family name, "Kazusanosuke" is a title of vice-governor of Kazusa province, "Saburo" is a formal nickname ("yobina"), and "Nobunaga" is an adult name ("nanori") given at genpuku, the coming of age ceremony. A man was addressed by his family name and his title, or by his "yobina" if he did not have a title. However, the "nanori" was a private name that could be used by only a very few, including the Emperor.

Samurai could choose their own "nanori", and frequently changed their names to reflect their allegiances.

Samurai had arranged marriages, which were arranged by a go-between of the same or higher rank. While for those samurai in the upper ranks this was a necessity (as most had few opportunities to meet women), this was a formality for lower-ranked samurai. Most samurai married women from a samurai family, but for lower-ranked samurai, marriages with commoners were permitted. In these marriages a dowry was brought by the woman and was used to set up the couple's new household.

A samurai could take concubines but their backgrounds were checked by higher-ranked samurai. In many cases, taking a concubine was akin to a marriage. Kidnapping a concubine, although common in fiction, would have been shameful, if not criminal. If the concubine was a commoner, a messenger was sent with betrothal money or a note for exemption of tax to ask for her parents' acceptance. Even though the woman would not be a legal wife, a situation normally considered a demotion, many wealthy merchants believed that being the concubine of a samurai was superior to being the legal wife of a commoner. When a merchant's daughter married a samurai, her family's money erased the samurai's debts, and the samurai's social status improved the standing of the merchant family. If a samurai's commoner concubine gave birth to a son, the son could inherit his father's social status.

A samurai could divorce his wife for a variety of reasons with approval from a superior, but divorce was, while not entirely nonexistent, a rare event. A wife's failure to produce a son was cause for divorce, but adoption of a male heir was considered an acceptable alternative to divorce. A samurai could divorce for personal reasons, even if he simply did not like his wife, but this was generally avoided as it would embarrass the person who had arranged the marriage. A woman could also arrange a divorce, although it would generally take the form of the samurai divorcing her. After a divorce samurai had to return the betrothal money, which often prevented divorces.

Maintaining the household was the main duty of women of the samurai class. This was especially crucial during early feudal Japan, when warrior husbands were often traveling abroad or engaged in clan battles. The wife, or "okugatasama" (meaning: one who remains in the home), was left to manage all household affairs, care for the children, and perhaps even defend the home forcibly. For this reason, many women of the samurai class were trained in wielding a polearm called a naginata or a special knife called the "kaiken" in an art called "tantojutsu" (lit. the skill of the knife), which they could use to protect their household, family, and honor if the need arose.

Traits valued in women of the samurai class were humility, obedience, self-control, strength, and loyalty. Ideally, a samurai wife would be skilled at managing property, keeping records, dealing with financial matters, educating the children (and perhaps servants, too), and caring for elderly parents or in-laws that may be living under her roof. Confucian law, which helped define personal relationships and the code of ethics of the warrior class required that a woman show subservience to her husband, filial piety to her parents, and care to the children. Too much love and affection was also said to indulge and spoil the youngsters. Thus, a woman was also to exercise discipline.

Though women of wealthier samurai families enjoyed perks of their elevated position in society, such as avoiding the physical labor that those of lower classes often engaged in, they were still viewed as far beneath men. Women were prohibited from engaging in any political affairs and were usually not the heads of their household.

This does not mean that women in the samurai class were always powerless. Powerful women both wisely and unwisely wielded power at various occasions. After Ashikaga Yoshimasa, 8th "shōgun" of the Muromachi shogunate, lost interest in politics, his wife Hino Tomiko largely ruled in his place. Nene, wife of Toyotomi Hideyoshi, was known to overrule her husband's decisions at times and Yodo-dono, his concubine, became the "de facto" master of Osaka castle and the Toyotomi clan after Hideyoshi's death. Tachibana Ginchiyo was chosen to lead the Tachibana clan after her father's death. Chiyo, wife of Yamauchi Kazutoyo, has long been considered the ideal samurai wife. According to legend, she made her kimono out of a quilted patchwork of bits of old cloth and saved pennies to buy her husband a magnificent horse, on which he rode to many victories. The fact that Chiyo (though she is better known as "Wife of Yamauchi Kazutoyo") is held in such high esteem for her economic sense is illuminating in the light of the fact that she never produced an heir and the Yamauchi clan was succeeded by Kazutoyo's younger brother. The source of power for women may have been that samurai left their finances to their wives.

Though many women engaged in battle commonly alongside samurai men in japan, most of female warriors (Onna-bugeisha) were not formal samurai. They usually were not allowed to wear two swords and did not form master-servant relationships with lords, nevertheless there are some exceptions. 

As the Tokugawa period progressed more value became placed on education, and the education of females beginning at a young age became important to families and society as a whole. Marriage criteria began to weigh intelligence and education as desirable attributes in a wife, right along with physical attractiveness. Though many of the texts written for women during the Tokugawa period only pertained to how a woman could become a successful wife and household manager, there were those that undertook the challenge of learning to read, and also tackled philosophical and literary classics. Nearly all women of the samurai class were literate by the end of the Tokugawa period.

Several people born in foreign countries were granted the title of samurai.

Yasuke was a retainer of black African origin who served under the Japanese hegemon and warlord Oda Nobunaga in 1581 and 1582. He arrived in Japan in 1579 in the service of the Italian Jesuit Alessandro Valignano. Fascinated by his strength and intelligence, Nobunaga made Yasuke a close retainer and gave him a katana, his own residence, and a salary. He fought alongside Nobunaga and his son Nobutada in the Honnō-ji incident. He was one of the first foreign samurai and the only African samurai recorded in contemporary accounts.

After Bunroku and Keichō no eki, many people born in the Joseon dynasty were brought to Japan as prisoners or cooperators. Some of them served "daimyōs" as retainers. One of the most prominent figures among them was Kim Yeocheol, who was granted the Japanese name Wakita Naokata and promoted to Commissioner of Kanazawa city. 

The English sailor and adventurer William Adams (1564–1620) was, along with Joosten, among the first Westerners to receive the dignity of samurai. The "shōgun" Tokugawa Ieyasu presented him with two swords representing the authority of a samurai, and decreed that William Adams the sailor was dead and that Anjin Miura (), a samurai, was born. Adams also received the title of "hatamoto" (bannerman), a high-prestige position as a direct retainer in the "shōgun"s court. He was provided with generous revenues: "For the services that I have done and do daily, being employed in the Emperor's service, the Emperor has given me a living". (Letters) He was granted a fief in Hemi () within the boundaries of present-day Yokosuka City, "with eighty or ninety husbandmen, that be my slaves or servants". (Letters) His estate was valued at 250 "koku". He finally wrote "God hath provided for me after my great misery", (Letters) by which he meant the disaster-ridden voyage that initially brought him to Japan.

Jan Joosten van Lodensteijn ( – ), a Dutch colleague of Adams' on their ill-fated voyage to Japan in the ship De Liefde, was also given similar privileges by Tokugawa Ieyasu. Joosten likewise became a hatamoto samurai and was given a residence within Ieyasu's castle at Edo. Today, this area at the east exit of Tokyo Station is known as Yaesu (八重洲). Yaesu is a corruption of the Dutchman's Japanese name, Yayousu (耶楊子). Also in common with Adams, Joostens was given a Red Seal Ship (朱印船) allowing him to trade between Japan and Indo-China. On a return journey from Batavia Joosten drowned after his ship ran aground.

During the Boshin War (1868–1869), French soldiers joined the forces of the "shōgun" against the southern "daimyōs" favorable to the restoration of the Meiji Emperor. It is recorded that the French Navy officer Eugène Collache fought in samurai attire with his Japanese brothers-in-arms.

In the same war, the Prussian Edward Schnell served the Aizu domain as a military instructor and procurer of weapons. He was granted the Japanese name Hiramatsu Buhei (平松武兵衛), which inverted the characters of the "daimyō"s name Matsudaira. Hiramatsu (Schnell) was given the right to wear swords, as well as a residence in the castle town of Wakamatsu, a Japanese wife, and retainers. In many contemporary references, he is portrayed wearing a Japanese kimono, overcoat, and swords, with Western riding trousers and boots.


As far back as the seventh century Japanese warriors wore a form of "lamellar armor", this armor eventually evolved into the armor worn by the samurai. The first types of Japanese armors identified as samurai armor were known as "yoroi". These early samurai armors were made from small individual scales known as "kozane". The kozane were made from either iron or leather and were bound together into small strips, the strips were coated with lacquer to protect the kozane from water. A series of strips of kozane were then laced together with silk or leather lace and formed into a complete chest armor ("dou or dō"). A complete set of the yoroi weighed 66 lbs.

In the 1500s a new type of armor started to become popular due to the advent of firearms, new fighting tactics and the need for additional protection. The "kozane dou" made from individual scales was replaced by "plate armor". This new armor, which used iron plated "dou (dō)", was referred to as "Tosei-gusoku", or modern armor. The newer armor added features and pieces of armor for the face, thigh, and back. The back piece had multiple uses, such as for a flag bearing. Various other components of armor protected the samurai's body. The helmet "kabuto" was an important part of the samurai's armor. It was paired with a shikoro and fukigaeshi for protection of the head and neck. The garment worn under all of the armor and clothing was called the Fundoshi, also known as a loincloth. Samurai armor changed and developed as the methods of samurai warfare changed over the centuries. The known last use of samurai armor occurring in 1877 during the Satsuma Rebellion. As the last samurai rebellion was crushed, Japan modernized its defenses and turned to a national conscription army that used uniforms.

Most samurai were bound by a code of honor and were expected to set an example for those below them. A notable part of their code is or "hara kiri", which allowed a disgraced samurai to regain his honor by passing into death, where samurai were still beholden to social rules. Whilst there are many romanticized characterizations of samurai behavior such as the writing of in 1905, studies of Kobudō and traditional Budō indicate that the samurai were as practical on the battlefield as were any other warriors.

Despite the rampant romanticism of the 20th century, samurai could be disloyal and treacherous (e.g., Akechi Mitsuhide), cowardly, brave, or overly loyal (e.g., Kusunoki Masashige). Samurai were usually loyal to their immediate superiors, who in turn allied themselves with higher lords. These loyalties to the higher lords often shifted; for example, the high lords allied under Toyotomi Hideyoshi () were served by loyal samurai, but the feudal lords under them could shift their support to Tokugawa, taking their samurai with them. There were, however, also notable instances where samurai would be disloyal to their lord ("daimyō"), when loyalty to the Emperor was seen to have supremacy.

Jidaigeki (literally historical drama) has always been a staple program on Japanese movies and television. The programs typically feature a samurai. Samurai films and westerns share a number of similarities and the two have influenced each other over the years. One of Japan's most renowned directors, Akira Kurosawa, greatly influenced western film-making. George Lucas's "Star Wars" series incorporated many stylistic traits pioneered by Kurosawa and "Star Wars: A New Hope" takes the core story of a rescued princess being transported to a secret base from Kurosawa's "The Hidden Fortress". Kurosawa was inspired by the works of director John Ford and in turn Kurosawa's works have been remade into westerns such as "Seven Samurai" into "The Magnificent Seven" and "Yojimbo" into "A Fistful of Dollars". There is also a 26 episode anime adaptation ("Samurai 7") of "Seven Samurai". Along with film, literature containing samurai influences are seen as well.

As well as influence from American Westerns Kurosawa's also adapted two of Shakespeare's plays as sources for samurai movies; "Throne of Blood" was based on "Macbeth" and "Ran" was based on "King Lear".

Most common are historical works where the protagonist is either a samurai or former samurai (or another rank or position) who possesses considerable martial skill. Eiji Yoshikawa is one of the most famous Japanese historical novelists. His retellings of popular works, including Taiko, Musashi and "The Tale of the Heike", are popular among readers for their epic narratives and rich realism in depicting samurai and warrior culture. The samurai have also appeared frequently in Japanese comics (manga) and animation (anime). Examples are "Samurai Champloo", "Shigurui", "Requiem from the Darkness", "", and "Afro Samurai". Samurai-like characters are not just restricted to historical settings and a number of works set in the modern age, and even the future, include characters who live, train and fight like samurai. Some of these works have made their way to the west, where it has been increasing in popularity with America.

Just in the last two decades, samurai have become more popular in America. "Hyperbolizing the samurai in such a way that they appear as a whole to be a loyal body of master warriors provides international interest in certain characters due to admirable traits." (Moscardi, N. D.) Through various media, producers and writers have been capitalizing on the notion that Americans admire the samurai lifestyle. The animated series, "Afro Samurai", became well-liked in American popular culture due to its blend of hack-and-slash animation and gritty urban music.

Created by Takashi Okazaki, "Afro Samurai" was initially a dōjinshi, or manga series, which was then made into an animated series by Studio Gonzo. In 2007 the animated series debuted on American cable television on the Spike TV channel. (Denison, 2010) The series was produced for American viewers which “embodies the trend... comparing hip-hop artists to samurai warriors, an image some rappers claim for themselves". (Solomon, 2009) The storyline keeps in tone with the perception of a samurais finding vengeance against someone who has wronged him. Starring the voice of well known American actor Samuel L. Jackson, "Afro is the second-strongest fighter in a futuristic, yet, still feudal Japan and seeks revenge upon the gunman who killed his father." (King 2008) Due to its popularity, "Afro Samurai" was adopted into a full feature animated film and also became titles on gaming consoles such as the PlayStation 3 and Xbox. Not only has the samurai culture been adopted into animation and video games, it can also be seen in comic books.

American comic books have adopted the character type for stories of their own like the mutant-villain Silver Samurai of Marvel Comics. The design of this character preserves the samurai appearance; the villain is "Clad in traditional gleaming samurai armor and wielding an energy charged katana". (Buxton, 2013) Not only does the Silver Samurai make over 350 comic book appearances, the character is playable in several video games, such as "Marvel vs. Capcom" 1 and 2. In 2013, the samurai villain was depicted in James Mangold's film "The Wolverine". Ten years before the Wolverine debuted, another film helped pave the way to ensure the samurai were made known to American cinema:
A film released in 2003 titled "The Last Samurai", starring Tom Cruise, is inspired by the samurai way of life. In the film, Cruise's character finds himself deeply immersed in samurai culture. The character in the film, "Nathan Algren, is a fictional contrivance to make nineteenth-century Japanese history less foreign to American viewers". (Ravina, 2010) After being captured by a group of samurai rebels, he becomes empathetic towards the cause they fight for. Taking place during the Meiji Period, Tom Cruise plays the role of US Army Captain Nathan Algren, who travels to Japan to train a rookie army in fighting off samurai rebel groups. Becoming a product of his environment, Algren joins the samurai clan in an attempt to rescue a captured samurai leader. "By the end of the film, he has clearly taken on many of the samurai traits, such as zen-like mastery of the sword, and a budding understanding of spirituality". (Manion, 2006)

The television series "Power Rangers Samurai" (adapted from "Samurai Sentai Shinkenger") is also inspired by the way of the Samurai.






</doc>
<doc id="28290" url="https://en.wikipedia.org/wiki?curid=28290" title="Slackware">
Slackware

Slackware is a Linux distribution created by Patrick Volkerding in 1993. Originally based on Softlanding Linux System, Slackware has been the basis for many other Linux distributions, most notably the first versions of SUSE Linux distributions, and is the oldest distribution that is still maintained.

Slackware aims for design stability and simplicity and to be the most "Unix-like" Linux distribution. It makes as few modifications as possible to software packages from upstream and tries not to anticipate use cases or preclude user decisions. In contrast to most modern Linux distributions, Slackware provides no graphical installation procedure and no automatic dependency resolution of software packages. It uses plain text files and only a small set of shell scripts for configuration and administration. Without further modification it boots into a command-line interface environment. Because of its many conservative and simplistic features, Slackware is often considered to be most suitable for advanced and technically inclined Linux users.

Slackware is available for the IA-32 and x86_64 architectures, with a port to the ARM architecture. While Slackware is mostly free and open source software, it does not have a formal bug tracking facility or public code repository, with releases periodically announced by Volkerding. There is no formal membership procedure for developers and Volkerding is the primary contributor to releases.

The name "Slackware" stems from the fact that the distribution started as a private side project with no intended commitment. To prevent it from being taken too seriously at first, Volkerding gave it a humorous name, which stuck even after Slackware became a serious project.

Slackware refers to the "pursuit of Slack", a tenet of the Church of the SubGenius, a parody religion. Certain aspects of Slackware graphics reflect this—the pipe which Tux is smoking, as influenced by the image of J. R. "Bob" Dobbs' head.

A humorous reference to the Church of the SubGenius can be found in many versions of the "install.end" text files, which indicate the end of a software series to the setup program. In recent versions, including Slackware release 14.1, the text is ROT13 obfuscated.

Slackware was originally derived from the Softlanding Linux System (SLS), the most popular of the original Linux distributions and the first to offer a comprehensive software collection that comprised more than just the kernel and basic utilities, including X11 graphical interface, TCP/IP and UUCP networking and GNU Emacs.

Patrick Volkerding started with SLS after needing a LISP interpreter for a school project at the then named Moorhead State University (MSU). He found CLISP was available for Linux and downloaded SLS to run it. A few weeks later, Volkerding was asked by his artificial intelligence professor at MSU to show him how to install Linux at home and on some of the computers at school. Volkerding had made notes describing fixes to issues he found after installing SLS and he and his professor went through and applied those changes to a new installation. However, this took almost as long as it took to just install SLS, so the professor asked if the install disks could be adjusted so the fixes could be applied during installation. This was the start of Slackware. Volkerding continued making improvements to SLS: fixing bugs, upgrading software, automatic installation of shared libraries and the kernel image, fixing file permissions, and more. In a short time, Volkerding had upgraded around half the packages beyond what SLS had available.

Volkerding had no intentions to provide his modified SLS version for the public. His friends at MSU urged him to put his SLS modifications onto an FTP server, but Volkerding assumed that "SLS would be putting out a new version that included these things soon enough", so he held off for a few weeks. During that time, many SLS users on the internet were asking SLS for a new release, so eventually Volkerding made a post titled "Anyone want an SLS-like 0.99pl11A system?", to which he received many positive responses. After a discussion with the local sysadmin at MSU, Volkerding obtained permission to upload Slackware to the university's FTP server. This first Slackware release, version 1.00, was distributed on 17 July 1993 at 00:16:36 (UTC), and was supplied as 24 3½" floppy disk images. After the announcement was made, Volkerding watched as the flood of FTP connections continually crashed the server. Soon afterwards, Walnut Creek CDROM offered additional archive space on their FTP servers.

The size of Slackware quickly increased with the addition of included software, and by version 2.1, released October 1994, it had more than tripled to comprise 73 1.44M floppy disk images.

In 1999, Slackware saw its version jump from 4 to 7. Slackware version numbers were lagging behind other distributions, and this led many users to believe it was out of date even though the bundled software versions were similar. Volkerding made the decision to bump the version as a marketing effort to show that Slackware was as up-to-date as other Linux distributions, many of which had release numbers of 6 at the time. He chose 7 estimating that most other distributions would soon be at this release number.

In April 2004, Patrick Volkerding added X.Org Server packages into the testing/ directory of -current as a replacement for the XFree86 packages currently being used, with a request for comments on what the future of the X Window System in Slackware should be. A month later, he switched from XFree86 to X.Org Server after stating that the opinions were more than 4 to 1 in favor of using the X.org release as the default version of X. He stated the decision was primarily a technical one, as XFree86 was proving to cause compatibility problems. Slackware 10.0 was the first release with X.Org Server.

In March 2005, Patrick Volkerding announced the removal of the GNOME desktop environment in the development ChangeLog. He stated this had been in consideration for more than 4 years and that there were already projects that provided a more complete version of GNOME for Slackware than what Slackware provided itself. Volkerding stated future GNOME support would rely on the community. The community responded and as of October 2016, there are several active GNOME projects for Slackware. These include: Cinnamon, Dlackware, Dropline GNOME, MATE, and SlackMATE. The removal was deemed significant by some in the Linux community due to the prevalence of GNOME in many distributions.

In May 2009, Patrick Volkerding announced the public (development) release of an official x86_64 variant, called Slackware64, maintained in parallel with the IA-32 distribution. Slackware64 is a pure 64-bit distribution in that it does not support running or compiling 32-bit programs, however, it was designed as "multilib-ready". Eric Hameleers, one of the core Slackware team members, maintains a multilib repository that contains the necessary packages to convert Slackware64 to multilib to enable running of 32-bit software. Hameleers started the 64-bit port as a diversion from the pain of recovering from surgery in September 2008. Volkerding tested the port in December 2008, and was impressed when he saw speed increases between 20 and 40 percent for some benchmarks compared to the 32-bit version. To minimize the extra effort of maintaining both versions in parallel, Slackware's build scripts, called SlackBuilds, were slowly transitioned to supporting either architecture, allowing for one set of sources for both versions. Slackware64 saw its first stable release with version 13.0.

Between the November 2013 release of 14.1 and June 2016, Slackware saw a 31-month gap between releases, marking the longest span in release history. During this time the development branch went without updates for 47 days. However, on 21 April 2015, Patrick Volkerding apologized on the ChangeLog for the absence of updates and stated that the development team used the time to get "some good work done." There were over 700 program changes listed on that ChangeLog entry, including many major library upgrades. In January 2016, Volkerding announced the reluctant addition of PulseAudio, primarily due to BlueZ dropping direct ALSA support in v5.x. while various other projects were in turn dropping support for BlueZ v4.x. Knowing some users would not be happy with the change, he stated that "Bug reports, complaints, and threats can go to me." These changes culminated in the release of Slackware 14.2 in June 2016.

The design philosophy of Slackware is oriented toward simplicity, software purity, and a core design that emphasizes lack of change to upstream sources. Many design choices in Slackware can be seen as a heritage of the simplicity of traditional Unix systems and as examples of the KISS principle. In this context, "simple" refers to the simplicity in system design, rather than system usage. Thus, ease of use may vary between users: those lacking knowledge of command line interfaces and classic Unix tools may experience a steep learning curve using Slackware, whereas users with a Unix background may benefit from a less abstract system environment. In keeping with Slackware's design philosophy, and its spirit of purity, most software in Slackware uses the original configuration mechanisms supplied by the software's authors; however, for some administrative tasks, distribution-specific configuration tools are delivered.

There is no formal issue tracking system and no official procedure to become a code contributor or developer. The project does not maintain a public code repository. Bug reports and contributions, while being essential to the project, are managed in an informal way. All the final decisions about what is going to be included in a Slackware release strictly remain with Slackware's benevolent dictator for life, Patrick Volkerding.

The first versions of Slackware were developed by Patrick Volkerding alone. Beginning with version 4.0, the official Slackware announce files list David Cantrell and Logan Johnson as part of the "Slackware team". Later announce statements, up to release version 8.1, include Chris Lumens. Lumens, Johnson and Cantrell are also the authors of the first edition of "Slackware Linux Essentials", the official guide to Slackware Linux. The Slackware website mentions Chris Lumens and David Cantrell as being "Slackware Alumni", who "worked full-time on the Slackware project for several years." In his release notes for Slackware 10.0 and 10.1 Volkerding thanks Eric Hameleers for "his work on supporting USB, PCI, and Cardbus wireless cards". Starting with version 12.0 there is, for a second time, a team building around Volkerding. According to the release notes of 12.2, the development team consists of seven people. Future versions added people. Since version 13.0, the Slackware team seems to have core members. Eric Hameleers gives an insight into the core team with his essay on the "History of Slackware Development", written on 3–4 October 2009 (shortly after the release of version 13.0).

Slackware's package management system, collectively known as pkgtools, can administer (pkgtool), install (installpkg), upgrade (upgradepkg), and remove (removepkg) packages from local sources. It can also uncompress (explodepkg) and create (makepkg) packages. The official tool to update Slackware over a network or the internet is slackpkg. It was originally developed by Piter Punk as an unofficial way to keep Slackware up-to-date. It was officially included in the main tree in Slackware 12.2, having been included in extras/ since Slackware 9.1. When a package is upgraded, it will install the new package over the old one and then remove any files that no longer exist in the new package. When running upgradepkg, it only confirms that the version numbers are "different", thus allowing downgrading the package if desired.

Slackware packages are tarballs compressed using various methods. Starting with 13.0, most packages are compressed using xz (based on the LZMA compression algorithm), utilizing the .txz filename extension. Prior to 13.0, packages were compressed using gzip (based on the DEFLATE compression algorithm), using the .tgz extension. Support for bzip2 and lzma compression was also added, using the filename extensions .tbz and .tlz respectively, although these are not commonly used.

Packages contain all the files for that program, as well as additional metadata files used by the package manager. The package tarball contains the full directory structure of the files and is meant to be extracted in the system's root directory during installation. The additional metadata files, located under the special install/ directory within the tarball, usually include a slack-desc file, which is a specifically formatted text file that is read by the package manager to provide users with a description of the packaged software, as well as a doinst.sh file, which is a post-unpacking shell script allowing creation of symbolic links, preserving permissions on startup files, proper handling of new configuration files, and any other aspects of installation that can't be implemented via the package's directory structure.

The package manager maintains a directory, /var/log/packages, where each package installed will have a corresponding install log file that lists the package size, both compressed and uncompressed, the software description, and the full path of all files that were installed. It also maintains the directory /var/log/scripts containing all doinst.sh files to allow proper removal of installed symlinks. When a package is removed or upgraded, the old install logs and doinst.sh files are moved to /var/log/removed_package and /var/log/removed_scripts respectively, making it possible to review any previous packages and see when they were removed.

The package management system does not track or manage "dependencies"; however, when performing the recommended full install, all dependencies of the stock packages are met. For custom installations or 3rd-party packages, Slackware relies on the user to ensure that the system has all the supporting system libraries and programs required by the program. Since no official lists of dependencies for stock packages are provided, if users decide to install a custom installation or install 3rd-party software, they will need to work through any possible missing dependencies themselves. Since the package manager doesn't manage dependencies, it will install any and all packages, whether or not dependencies are met. A user may find out that dependencies are missing only when attempting to use the software.

While Slackware itself does not incorporate official tools to resolve dependencies, some unofficial, community-supported software tools do provide this function, similar to the way APT does for Debian-based distributions and yum does for Red Hat-based distributions. They include:


There are no official repositories for Slackware. The only official packages Slackware provides are available on the installation media. However, there are many third-party repositories for Slackware; some are standalone repositories and others are for distributions that are Slackware-based but retain package compatibility with Slackware. Many of these can be searched at once using pkgs.org, which is a Linux package search engine. However, mixing and matching dependencies from multiple repositories can lead to two or more packages that require different versions of the same dependency, which is a form of dependency hell. Slackware itself won't provide any dependency resolution for these packages, however some projects will provide a list of dependencies that are not included with Slackware with the files for the package, commonly with a .dep extension.

Due to the possibility of dependency issues, many users choose to compile their own programs using community-provided SlackBuilds. SlackBuilds are shell scripts that will create an installable Slackware package from a provided software tarball. Since SlackBuilds are scripts, they aren't limited to just compiling a program's source; they can also be used to repackage pre-compiled binaries provided by projects or other distributions' repositories into proper Slackware packages. SlackBuilds that compile sources have several advantages over pre-built packages: since they build from the original author's source code, the user does not have to trust a third-party packager; furthermore the local compilation process allows for machine-specific optimization. In comparison to manual compilation and installation of software, SlackBuilds provide cleaner integration to the system by utilizing Slackware's package manager. Some SlackBuilds will come with an additional file with metadata that allows automated tools to download the source, verify the source is not corrupted, and calculate additional dependencies that are not part of Slackware. Some repositories will include both SlackBuilds and the resulting Slackware packages, allowing users to either build their own or install a pre-built package.

The only officially endorsed SlackBuilds repository is SlackBuilds.org, commonly referred to as SBo. This is a community-supported project offering SlackBuilds for building software not included with Slackware. Users are able to submit new SlackBuilds for software to the site and, once approved, they become the "package maintainer". They are then responsible for providing updates to the SlackBuild, either to fix issues or to build newer versions provided by upstream. To ensure all programs can be compiled and used, any required dependencies of the software not included with Slackware are required to be documented and be available on the site. All submissions are tested by the site's administrators before being added to the repository. The administrators intend for the build process to be nearly identical to the way Slackware's official packages are built, mainly to ensure Volkerding was "sympathetic of our cause". This allows SlackBuilds that Volkerding deems worthy to be pulled into regular Slackware with minimal changes to the script. It also prevent users from suggesting Volkerding to change his scripts to match SBo's. SBo provides templates for SlackBuilds and the additional metadata files and they encourage package maintainers to not deviate unless necessary.

Two Slackware team members, Eric Hameleers and Robby Workman each have their own repository of pre-compiled packages along with the SlackBuilds and source files used to create the packages. While most packages are just additional software not included in Slackware that they felt was worth their time to maintain, some packages are used as a testbed for future upgrades to Slackware, most notably, Hameleers provides "Ktown" packages for newer versions of KDE. He also maintains Slackware's "multilib" repository, enabling Slackware64 to run and compile 32bit packages.

Slackware's release policy follows a feature and stability based release cycle, in contrast to the time-bound ("e.g.", Ubuntu) or rolling release ("e.g.", Gentoo Linux) schemes of other Linux distributions. This means there's no set time on when to expect a release. Volkerding will release the next version after he feels a suitable number of changes from the previous version have been made and those changes lead to a stable environment. As stated by Patrick Volkerding, "It's usually our policy not to speculate on release dates, since that's what it is — pure speculation. It's not always possible to know how long it will take to make the upgrades needed and tie up all the related loose ends. As things are built for the upcoming release, they'll be uploaded into the -current tree."

Throughout Slackware's history, they generally try to deliver up-to-date software on at least an annual basis. However, between Slackware 14.1 and 14.2, there was more than a 2-year gap between releases. From its inception, other than 2014 and 2015, Slackware had at least one release per year. Release activity peaked in 1994, 1995, 1997 and 1999, with three releases each year. Starting with version 7.1 (22 June 2000) the release progression became more stable and tended to occur once a year. Since then, the only years with two releases were 2003, 2005 and 2008.

Slackware's latest 32bit x86 and 64bit x86_64 stable releases are at version 14.2 (released on 30 June 2016), which include support for Linux 4.4.14.

Volkerding also maintains a testing/developmental version of Slackware called "-current" that can be used for a more bleeding edge configuration. This version will eventually become the next stable release, at which point Volkerding will start a new -current to start developing for the next release of Slackware. While this version is generally known to be stable, it is possible for things to break, so -current tends to not be recommended for production systems.

Currently, Slackware has no officially stated support term policy. However, on 14 June 2012, notices appeared in the changelogs for versions 8.1, 9.0, 9.1, 10.0, 10.1, 10.2, 11.0, and 12.0 stating that, effective 1 August 2012, security patches would no longer be provided for these versions. The oldest release, version 8.1, was released on 18 June 2002 and had over 10 years of support before reaching EOL. Later, on 30 August 2013, announcements were made on the changelogs of 12.1 and 12.2 stating their EOL on 9 December 2013. It was stated in the changelog entries that they had at least 5 years of support. On 6 April 2018, versions of 13.0, 13.1 and 13.37 were declared reaching their EOL on 5 July 2018. It was stated in the changelog entries that they had at least 7 years of support (13.0 had been supported almost 9 years). , there have been no announcements from the Slackware team on when any versions of Slackware from 14.0 and up will be EOL.

While there have been no official announcements for versions prior to 8.1, they are no longer maintained and are effectively EOL.

Historically, Slackware concentrated solely on the IA-32 architecture and releases were available as 32-bit only. However, starting with Slackware 13.0, a 64-bit x86_64 variant is available and officially supported in symmetrical development with the 32-bit platform. Prior to the release of Slackware64 users wanting 64-bit were required to use unofficial ports such as slamd64.

Slackware is also available for the IBM S/390 architecture in the form of Slack/390 and for the ARM architecture under Slackware ARM (originally known as 'ARMedslack'). Both ports have been declared "official" by Patrick Volkerding. However, the S/390 port is still at version 10.0 for the stable version and 11.0 for the testing/developmental version, and has had no updates since 2009. Also, on 7 May 2016, the developer of Slackware ARM announced 14.1 will be EOL on 1 September 2016 and development of -current will cease with the release of 14.2, however support for 14.2 will be maintained for the foreseeable future. The EOL announcement for 14.1 was added to the changelog on 25 June 2016.

In July 2016, the developer of Slackware ARM announced that the development and build tools had been enhanced to reduce the manual effort involved in maintaining the ARM port, and proceeded to announce that a 32-bit hardware floating port was in development. The port was released in August 2016 in "current" form.

Slackintosh was an unofficial port of Slackware to the "New World" Macintosh's PowerPC architecture. Slackintosh's final release was 12.1 and is no longer being maintained.

The latest stable version of Slackware can be ordered from the official Slackware store as a 6-CD set or as a single Dual-DVD. The CD set is targeted at the 32bit IA-32 platform but also runs on 64bit x86_64 processors in 32-bit mode. The DVD contains both the 32bit and 64bit versions.

Slackware ISO images for the CD set and the DVD can also be downloaded for free via BitTorrent or from various FTP and HTTP mirrors.

The distributions of the ports for the ARM architecture and for IBM S/390 are neither available as CD/DVDs nor as ISO images, but can be downloaded. Slackware S/390 installs from a DOS Partition or from floppy disk. Slackware ARM does not distribute ISO files because most ARM devices can not boot from a CD or DVD. Instead, it is installed off a network, using Das U-Boot and a TFTP boot server or from a mini-root filesystem. Slackware ARM can also be installed on a PC running QEMU using the same technique.

DistroWatch shows a decreasing but still substantial interest regarding Slackware. In 2002 the Slackware page was ranked as number 7, but dropped to number 10 by 2005. In 2006 it reached number 9, whereas since then being constantly below the ten most popular pages. In 2010 it had been listed as number 11, in the years 2011 and 2012 as number 12, and in 2015 as number 33.

However, since DistroWatch only tracks visitors to the various distributions' pages, they state their ranking doesn't always correlate with the usage of a distribution, only the popularity of that distribution on their site. Because of this, their rankings "should not be used to measure the market share of distributions." Many people who are already familiar with a distribution may have no need to visit DistroWatch, so their trends could be applied more toward either new or potentially new users who are curious about a distribution.

Currently, there is no official method to track the usage or number of installs of Slackware.



</doc>
<doc id="28291" url="https://en.wikipedia.org/wiki?curid=28291" title="Sequencer">
Sequencer

Sequencer may refer to:





</doc>
<doc id="28292" url="https://en.wikipedia.org/wiki?curid=28292" title="Stalingrad (disambiguation)">
Stalingrad (disambiguation)

Stalingrad is the former name of Volgograd, a city in Russia.

Stalingrad may also refer to:







</doc>
<doc id="28296" url="https://en.wikipedia.org/wiki?curid=28296" title="Short story">
Short story

A short story is a piece of prose fiction that typically can be read in one sitting and focuses on a self-contained incident or series of linked incidents, with the intent of evoking a "single effect" or mood, however there are many exceptions to this. 

A dictionary definition is "an invented prose narrative shorter than a novel usually dealing with a few characters and aiming at unity of effect and often concentrating on the creation of mood rather than plot." 

The short story is a crafted form in its own right. Short stories make use of plot, resonance, and other dynamic components as in a novel, but typically to a lesser degree. While the short story is largely distinct from the novel or novella (a shorter novel), authors generally draw from a common pool of literary techniques.

Short story writers may define their works as part of the artistic and personal expression of the form. They may also attempt to resist categorization by genre and fixed formation.

Short stories have deep roots and the power of short fiction has been recognised in modern society for hundreds of years. The short form is, conceivably, more natural to us than longer forms. We are drawn to short stories as the well-told story, and as William Boyd, the award-winning British author and short story writer has said:"[short stories] seem to answer something very deep in our nature as if, for the duration of its telling, something special has been created, some essence of our experience extrapolated, some temporary sense has been made of our common, turbulent journey towards the grave and oblivion".In terms of length, word count is typically anywhere from 1,000 to 4,000 for short stories, however some have 20,000 words and are still classed as short stories. Stories of fewer than 1,000 words are sometimes referred to as "short short stories", or "flash fiction".

 run into the thousands. 

Global sales of short story fiction are very strong. In the UK sales jumped 45 per cent in 2017, driven by collections from international names such as Alice Munro, new writers to the genre such as Tom Hanks, and the revival of short story salons, such as those held by short fiction company, Pin Drop Studio. 

More than 690,000 short stories and anthologies were sold in the UK in 2017, generating £5.88 million, the genre’s highest sales since 2010.

In 2012 Pin Drop Studio launched a short story salon held regularly in London and other major cities. Short story writers who have appeared at the salon to read their short stories to a live audience include Ben Okri, Lionel Shriver, Elizabeth Day, A.L.Kennedy, Will Self, William Boyd, Graham Swift, David Nicholls, Will Self, Sebastian Faulks, Julian Barnes, Evie Wylde and Claire Fuller.

Prominent short story awards such as The Sunday Times Short Story Award and the Pin Drop Studio Short Story Award, attract hundreds of entries each year. Published and non-published writers take part, sending their stories from all corners of the world.

Alice Munro, "master of the contemporary short story" according to her citation for the 2013 Nobel Prize in Literature, said she hopes the award would bring readership for the short story in general.

Short stories are sometimes adapted for radio, TV and film:


Determining what exactly separates a short story from longer fictional formats is problematic. A classic definition of a short story is that one should be able to read it in one sitting, a point most notably made in Edgar Allan Poe's essay "The Philosophy of Composition" (1846). 

Interpreting this standard nowadays is problematic, because the expected length of "one sitting" may now be briefer than it was in Poe's era. 

Short stories have no set length. In terms of word count there is no official demarcation between an anecdote, a short story, and a novel. Rather, the form's parameters are given by the rhetorical and practical context in which a given story is produced and considered, so that what constitutes a short story may differ between genres, countries, eras, and commentators. Like the novel, the short story's predominant shape reflects the demands of the available markets for publication, and the evolution of the form seems closely tied to the evolution of the publishing industry and the submission guidelines of its constituent houses.

As a point of reference for the genre writer, the Science Fiction and Fantasy Writers of America define short story length in the Nebula Awards for science fiction submission guidelines as having a word count of fewer than 7,500 words.

Longer stories that cannot be called novels are sometimes considered "novellas" or novelettes and, like short stories, may be collected into the more marketable form of "collections", often containing previously unpublished stories. Sometimes, authors who do not have the time or money to write a novella or novel decide to write short stories instead, working out a deal with a popular website or magazine to publish them for profit.

Emerging from earlier oral storytelling traditions in the 17th century, the short story has grown to encompass a body of work so diverse as to defy easy characterization.

With the rise of the realistic novel, the short story evolved in a parallel tradition, with some of its first distinctive examples in the tales of E. T. A. Hoffmann. The character of the form developed particularly with authors known for their short fiction, either by choice (they wrote nothing else) or by critical regard, which acknowledged the focus and craft required in the short form. An example is Jorge Luis Borges, who won American fame with "The Garden of Forking Paths", published in the August 1948 "Ellery Queen's Mystery Magazine". Another example is O. Henry (author of "Gift of the Magi"), for whom the O. Henry Award is named. Other of his most popular, inventive and most often reprinted stories (among over 600) include: "A Municipal Report", "An Unfinished Story", "A Blackjack Barginer", "A Lickpenny Lover", "Mammon and the Archer", "Two Thanksgiving Day Gentlemen", "The Last Leaf". American examples include: Jack London, Ambrose Bierce, F. Scott Fitzgerald, Ernest Hemingway, William Faulkner, Flannery O'Connor, John Cheever, and Raymond Carver. Science fiction short story with a special poetic touch was a genre developed with great popular success by Ray Bradbury.
The genre of the short story was often neglected until the second half of the 19th century.

The evolution of printing technologies and periodical editions were among the factors contributing to the increasing importance of short story publications. Pioneering role in founding the rules of the genre in the Western canon include, among others, Rudyard Kipling (United Kingdom), Anton Chekhov (Russia), Guy de Maupassant (France), Manuel Gutiérrez Nájera (Mexico) and Rubén Darío (Nicaragua).

An important theoretical example for storytelling analysis is provided by Walter Benjamin in his illuminated essay "The Storyteller" where he argues about the decline of storytelling art and the incommunicability of experiences in the modern world. Oscar Wilde's essay "The Decay of Lying" and Henry James's "The Art of Fiction" are also partly related with this subject.

Short stories date back to oral storytelling traditions which originally produced epics such as Homer's "Iliad" and "Odyssey". Oral narratives were often told in the form of rhyming or rhythmic verse, often including recurring sections or, in the case of Homer, "Homeric epithets". Such stylistic devices often acted as mnemonics for easier recall, rendition and adaptation of the story. Short sections of verse might focus on individual narratives that could be told at one sitting. The overall arc of the tale would emerge only through the telling of multiple such sections.

The other ancient form of short story, the anecdote, was popular under the Roman Empire. Anecdotes functioned as a sort of parable, a brief realistic narrative that embodies a point. Many surviving Roman anecdotes were collected in the 13th or 14th century as the "Gesta Romanorum". Anecdotes remained popular in Europe well into the 18th century, when the fictional anecdotal letters of Sir Roger de Coverley were published.

In Europe, the oral story-telling tradition began to develop into written stories in the early 14th century, most notably with Geoffrey Chaucer's "Canterbury Tales" and Giovanni Boccaccio's "Decameron". Both of these books are composed of individual short stories (which range from farce or humorous anecdotes to well-crafted literary fictions) set within a larger narrative story (a frame story), although the frame-tale device was not adopted by all writers. At the end of the 16th century, some of the most popular short stories in Europe were the darkly tragic "novella" of Matteo Bandello (especially in their French translation).

The mid 17th century in France saw the development of a refined short novel, the "nouvelle", by such authors as Madame de Lafayette. In the 1690s, traditional fairy tales began to be published (one of the most famous collections was by Charles Perrault). The appearance of Antoine Galland's first modern translation of the "Thousand and One Nights" (or "Arabian Nights") (from 1704; another translation appeared in 1710–12) would have an enormous influence on the 18th-century European short stories of Voltaire, Diderot and others.

There are early examples of short stories published separately between 1790 and 1810, but the first true collections of short stories appeared between 1810 and 1830 in several countries around the same period.

The first short stories in the United Kingdom were gothic tales like Richard Cumberland's "remarkable narrative" "The Poisoner of Montremos" (1791). Great novelists like Sir Walter Scott and Charles Dickens also wrote some short stories.

One of the earliest short stories in the United States was Charles Brockden Brown's "Somnambulism" from 1805. Washington Irving wrote mysterious tales including "Rip van Winkle" (1819) and "The Legend of Sleepy Hollow" (1820). Nathaniel Hawthorne published the first part of his "Twice-Told Tales" in 1837. Edgar Allan Poe wrote his tales of mystery
and imagination between 1832 and 1849. Classic stories are "The Fall of the House of Usher", "The Tell-Tale Heart", "The Cask of Amontillado", "The Pit and the Pendulum", and the first detective story, "The Murders in the Rue Morgue". In "The Philosophy of Composition" (1846) Poe argued that a literary work should be short enough for a reader to finish in one sitting.

In Germany, the first collection of short stories was by Heinrich von Kleist in 1810 and 1811. The Brothers Grimm published their first volume of collected fairy tales in 1812. E. T. A. Hoffmann followed with his own original fantasy tales, of which "The Nutcracker and the Mouse King" (1816) is the most famous.

In France, Prosper Mérimée wrote "Mateo Falcone" in 1829.

In the latter half of the 19th century, the growth of print magazines and journals created a strong demand for short fiction of between 3,000 and 15,000 words.

In the United Kingdom, Thomas Hardy wrote dozens of short stories, including "The Three Strangers" (1883), "A Mere Interlude" (1885) and "Barbara of the House of Grebe" (1890). Rudyard Kipling published short story collections for grown-ups, e.g. "Plain Tales from the Hills" (1888), as well as for children, e.g. "The Jungle Book" (1894). In 1892 Arthur Conan Doyle brought the detective story to a new height with "The Adventures of Sherlock Holmes". H. G. Wells wrote his first science fiction stories in the 1880s. One of his best known is "The Country of the Blind" (1904).

In the United States, Herman Melville published his story collection "The Piazza Tales" in 1856. "The Celebrated Jumping Frog of Calaveras County" was the title story of Mark Twain's first book one year later. In 1884, Brander Matthews, the first American professor of dramatic literature, published "The Philosophy of the Short-Story". At that same year, Matthews was the first one to name the emerging genre "short story". Another theorist of narrative fiction was Henry James. James wrote a lot of short stories himself, including "The Real Thing" (1892), "Maud-Evelyn" and "The Beast in the Jungle" (1903). In the 1890s Kate Chopin published short stories in several magazines.

The most prolific French author of short stories was Guy de Maupassant. Stories like ""Boule de Suif"" ("Ball of Fat", 1880) and ""L'Inutile Beauté"" ("The Useless Beauty", 1890) are good examples of French realism.

In Russia, Ivan Turgenev gained recognition with his story collection "A Sportsman's Sketches". Nikolai Leskov created his first short stories in the 1860s. Late in his life Fyodor Dostoyevski wrote "The Meek One" (1876) and "The Dream of a Ridiculous Man" (1877), two stories with great psychological and philosophical depth. Leo Tolstoy handled ethical questions in his short stories, for example in "Ivan the Fool" (1885), "How Much Land Does a Man Need?" (1886) and "Alyosha the Pot" (1905). The greatest specialist of the Russian short story, however, was Anton Chekhov. Classic examples of his realistic prose are "The Bet" (1889), "Ward No. 6" (1892), and "The Lady with the Dog" (1899). Maxim Gorky's best known short story is "Twenty-six Men and a Girl" (1899).

The prolific Indian author of short stories Munshi Premchand, pioneered the genre in the Hindustani language, writing a substantial body of short stories and novels in a style characterized by realism and an unsentimental and authentic introspection into the complexities of Indian society. Premchand's work, including his over 200 short stories (such as the story "Lottery") and his novel Godaan remain substantial works.

A master of the short story, the Urdu language writer Saadat Hasan Manto, is revered for his exceptional depth, irony and sardonic humour. The author of some 250 short stories, radio plays, essays, reminiscences and a novel, Manto is widely admired for his analyses of violence, bigotry, prejudice and the relationships between reason and unreason. Combining realism with surrealism and irony, Manto's works such as the celebrated short story Toba Tek Singh are aesthetic masterpieces which continue to give profound insight into the nature of human loss, violence and devastation.

In India, Rabindranath Tagore published short stories, on the lives of the poor and oppressed such as peasants, women and villagers under colonial misrule and exploitation.

In Poland, Bolesław Prus was the most important author of short stories. In 1888 he wrote "A Legend of Old Egypt".

Machado de Assis, one of the majors novelist from Brazil was the most important short story writer from his country at the time, under influences (among others) of Xavier de Maistre, Lawrence Sterne, Guy de Maupassant. In the end of the 19th century the writer João do Rio became popular by short stories about the bohemianism. Writing about the former slaves, and very ironical about nationalism, Lima Barreto died almost forgotten, but became very popular in the 20th century.

In Portuguese literature, the major names of the time are Almeida Garrett and the historian and novelist Alexandre Herculano. Still influential, Eça de Queiroz produced some short stories with a style influenced by Émile Zola, Balzac and Dickens.

In the United Kingdom, periodicals like "The Strand Magazine" and "Story-Teller" contributed to the popularity of the short story. Hector Hugh Munro (1870–1916), also known by his pen name of Saki, wrote satirical short stories about Edwardian England. W. Somerset Maugham, who wrote over a hundred short stories, was one of the most popular authors of his time. P. G. Wodehouse published his first collection of comical stories about valet Jeeves in 1917. Many detective stories were written by G. K. Chesterton, Agatha Christie and Dorothy L. Sayers. Short stories by Virginia Woolf are "Kew Gardens" (1919) and "Solid Objects," about a politician with mental problems. Graham Greene wrote his Twenty-One Stories between 1929 and 1954. A specialist of the short story was V. S. Pritchett, whose first collection appeared in 1932. Arthur C. Clarke published his first science fiction story, "Travel by Wire!" in 1937. Evelyn Waugh, Muriel Spark and L. P. Hartley were other popular British storytellers whose career started in this period.

In Ireland, James Joyce published his short story collection "Dubliners" in 1914. These stories, written in a more accessible style than his later novels, are based on careful observation of the inhabitants of his birth city.

In the first half of the 20th century, a number of high-profile American magazines such as "The Atlantic Monthly", "Harper's Magazine", "The New Yorker", "Scribner's", "The Saturday Evening Post", "Esquire", and "The Bookman" published short stories in each issue. The demand for quality short stories was so great and the money paid for such so well that F. Scott Fitzgerald repeatedly turned to short-story (as Matthews preferred to write it) writing to pay his numerous debts. His first collection "Flappers and Philosophers" appeared in book form in 1920. William Faulkner wrote over one hundred short stories. "Go Down, Moses", a collection of seven stories, appeared in 1941. Ernest Hemingway's concise writing style was perfectly fit for shorter fiction. Stories like "A Clean, Well-Lighted Place" (1926), "Hills Like White Elephants" (1927) and "The Snows of Kilimanjaro" (1936) are only a few pages long, but carefully crafted. Dorothy Parker's bittersweet story "Big Blonde" debuted in 1929. A popular science fiction story is "Nightfall" by Isaac Asimov.

Katherine Mansfield from New Zealand wrote many short stories between 1912 and her death in 1923. "The Doll's House" (1922) treats the topic of social inequity.

Two important authors of short stories in the German language were Thomas Mann and Franz Kafka. In 1922 the latter wrote "A Hunger Artist", about a man who fasts for several days.

Ryūnosuke Akutagawa (1892–1927) is called the Father of the Japanese short story.

In Brazil, the most famous modern short story writer is Mário de Andrade. At the time, Paulistan writer António de Alcantâra Machado became very popular from his collection of short stories titled, "Brás, Bexiga e Barra Funda" (1928), about several Italian neighborhoods, but now he is mostly read in just São Paulo. Also, novelist Graciliano Ramos and poet Carlos Drummond de Andrade have significant short story works.

Portuguese writers like Mário de Sá-Carneiro, Florbela Espanca and Fernando Pessoa wrote well-known short stories, although their major genre was poetry.

The period following World War II saw a great flowering of literary short fiction in the United States. "The New Yorker" continued to publish the works of the form's leading mid-century practitioners, including Shirley Jackson, whose story, "The Lottery", published in 1948, elicited the strongest response in the magazine's history to that time. Other frequent contributors during the last 1940s included John Cheever, John Steinbeck, Jean Stafford, and Eudora Welty. J. D. Salinger's "Nine Stories" (1953) experimented with point of view and voice, while Flannery O'Connor's story "A Good Man is Hard to Find" (1955) reinvigorated the Southern Gothic style. Cultural and social identity played a considerable role in much of the short fiction of the 1960s. Philip Roth and Grace Paley cultivated distinctive Jewish-American voices. Tillie Olsen's "I Stand Here Ironing" (1961) adopted a consciously feminist perspective. James Baldwin's collection "Going to Meet the Man" (1965) told stories of African-American life. Frank O'Connor's "The Lonely Voice", an exploration of the short story, appeared in 1963. Wallace Stegner's short stories are primarily set in the American West. Stephen King published many short stories in men's magazines in the 1960s and after. The 1970s saw the rise of the postmodern short story in the works of Donald Barthelme and John Barth. Traditionalists including John Updike and Joyce Carol Oates maintained significant influence on the form. Minimalism gained widespread influence in the 1980s, most notably in the work of Raymond Carver and Ann Beattie.

Canadian short story writers include Alice Munro, Mavis Gallant, and Lynn Coady.

In the United Kingdom, Daphne du Maurier wrote suspense stories like "The Birds" (1952) and "Don't Look Now" (1971). Roald Dahl was the master of the twist-in-the-tale. Short story collections like "Lamb to the Slaughter" (1953) and "Kiss Kiss" (1960) illustrate his dark humour.

In Italy, Italo Calvino published the short story collection "Marcovaldo", about a poor man in a city, in 1963.

In Brazil, the short story became popular among female writers like Clarice Lispector, Lygia Fagundes Telles, Adélia Prado, who wrote about their society from a feminine viewpoint, although the genre has great male writers like Dalton Trevisan, Autran Dourado Moacyr Scliar and Carlos Heitor Cony too. Also, writing about poverty and the favelas, João Antonio became a well known writer. Other post-modern short fiction authors include writers Hilda Hilst and Caio Fernando Abreu. Detective literature was led by Rubem Fonseca. It is also necessary to mention João Guimarães Rosa, wrote short stories in the book "Sagarana" using a complex, experimental language based on tales of oral traditional.

Portuguese writers like Virgílo Ferreira, Fernando Goncalves Namora and Sophia de Mello Breyner Andresen are among the most influential short story writers from 20th-century Portuguese language literature. Manuel da Silva Ramos is one of the most well-known names of postmodernism in the country. Nobel Prize-winner José Saramago published few short stories, but became popular from his novels.

The Angolan writer José Luandino Vieira is one of the most well-known writers from his country and has several short stories. José Eduardo Agualusa is also increasingly read in Portuguese-speaking countries.

Mozambican Mia Couto is a widely known writer of post modern prose, and he is read even in non-Portuguese speaking countries. Other Mozambican writers such as Suleiman Cassamo, Paulina Chiziane and Eduardo White are gaining popularity with Portuguese-speakers too.

The Argentine writer Jorge Luis Borges is one of the most famous writers of short stories in the Spanish language. "The Library of Babel" (1941) and "The Aleph" (1945) handle difficult subjects like infinity. Two of the most representative writers of the Magical realism genre are also widely known Argentinian short story writers: Adolfo Bioy Casares and Julio Cortázar.

The Uruguayan writer Juan Carlos Onetti is known as one of the most important magical realist writer from Latin America.

In Colombia, the Nobel prize laureate author Gabriel Garcia Marquez is the main novelist and short story writer, known by his magical realist stories and his defense of the Communist Party in his country.

The Peruvian writer Mario Vargas Llosa, also a Nobel prize winner, has significant short story works.

The Egyptian Nobel Prize-winner Naguib Mafouz is the most well-known author from his country, but has only a few short stories.

Japanese world-known short story writers include Kenzaburō Ōe (Nobel prize winner of 1994), Yukio Mishima and Haruki Murakami.

Multi-awarded Philippine writer Peter Solis Nery is one of the most famous writers of short stories in Hiligaynon language. His stories "Lirio" (1998), "Candido" (2007), "Donato Bugtot" (2011), and "Si Padre Olan kag ang Dios" (2013) are all gold prize winners at the Palanca Awards of Philippine Literature.

As a concentrated form of narrative prose fiction, the short story has been theorised through the traditional elements of dramatic structure: exposition (the introduction of setting, situation and main characters), complication (the event that introduces the conflict), rising action, crisis (the decisive moment for the protagonist and his commitment to a course of action), climax (the point of highest interest in terms of the conflict and the point with the most action) and resolution (the point when the conflict is resolved). Because of their length, short stories may or may not follow this pattern. For example, modern short stories only occasionally have an exposition, more typically beginning in the middle of the action ("in medias res"). As with longer stories, plots of short stories also have a climax, crisis, or turning point. However, the endings of many short stories are abrupt and open and may or may not have a moral or practical lesson. As with any art form, the exact characteristics of a short story will vary by creator.






</doc>
<doc id="28297" url="https://en.wikipedia.org/wiki?curid=28297" title="Soul">
Soul

In many religious, philosophical, and mythological traditions, there is a belief in the incorporeal essence of a living being called the soul.

Soul or psyche (Greek: "psychē", of "psychein", "to breathe") are the mental abilities of a living being: reason, character, feeling, consciousness, memory, perception, thinking, etc.

Depending on the philosophical system, a soul can either be mortal or immortal. In Judeo-Christianity, only human beings have immortal souls (although immortality is disputed within Judaism and may have been influenced by Plato). For example, the Catholic theologian Thomas Aquinas attributed "soul" ("anima") to all organisms but argued that only human souls are immortal. Other religions (most notably Hinduism, Buddhism, and Jainism) hold that all the living things whether it is smallest as a bacteria to very large being such as humans and animals are the souls themselves (Atman, jiva) and have physical representative (the body) in the world. The actual self is the soul, while the body is only a mechanism to experience the karma of that life i.e if we see a tiger then there is a self conscious identity residing in it (the soul), and a physical representative (the whole body of tiger which is observable) in the world.

Some teach that even non-biological entities (such as rivers and mountains) possess souls. This belief is called animism.

Greek philosophers, such as Socrates, Plato, and Aristotle, understood that the soul (ψυχή "psūchê") must have a logical faculty, the exercise of which was the most divine of human actions. At his defense trial, Socrates even summarized his teaching as nothing other than an exhortation for his fellow Athenians to excel in matters of the psyche since all bodily goods are dependent on such excellence ("Apology" 30a–b).

"Anima mundi" is the concept of a "world soul" connecting all living organisms on planet Earth.

The Modern English word "soul", derived from Old English "sáwol, sáwel", was first attested in the 8th-century poem "Beowulf" v. 2820 and in the Vespasian Psalter 77.50. It is cognate with other German and Baltic terms for the same idea, including Gothic "saiwala", Old High German "sêula, sêla", Old Saxon "sêola", Old Low Franconian "sêla, sîla", Old Norse "sála" and Lithuanian "siela". Further etymology of the Germanic word is uncertain. The original concept is meant to be 'coming from or belonging to the sea/lake', because of the German belief in souls being born out of and returning to sacred lakes, Old Saxon "sêola" (soul) compared to Old Saxon "sêo" (sea).

The Koine Greek Septuagint uses ("psyche") to translate Hebrew ("nephesh"), meaning "life, vital breath", and specifically refers to a mortal, physical life, but in English it is variously translated as "soul, self, life, creature, person, appetite, mind, living being, desire, emotion, passion"; an example can be found in :

The Koine Greek word ("psychē"), "life, spirit, consciousness", is derived from a verb meaning "to cool, to blow", and hence refers to the breath, as opposed to ("soma"), meaning "body". "Psychē" occurs juxtaposed to , as seen in :

Paul the Apostle used ψυχή ("psychē") and ("pneuma") specifically to distinguish between the Jewish notions of ("nephesh") and "ruah" (spirit) (also in the Septuagint, e.g. = = "" = "the Spirit of God").

In the ancient Egyptian religion, an individual was believed to be made up of various elements, some physical and some spiritual. Similar ideas are found in ancient Assyrian and Babylonian religion. Kuttamuwa, an 8th-century BC royal official from Sam'al, ordered an inscribed stele erected upon his death. The inscription requested that his mourners commemorate his life and his afterlife with feasts "for my soul that is in this stele". It is one of the earliest references to a soul as a separate entity from the body. The basalt stele is tall and wide. It was uncovered in the third season of excavations by the Neubauer Expedition of the Oriental Institute in Chicago, Illinois.

The Bahá'í Faith affirms that "the soul is a sign of God, a heavenly gem whose reality the most learned of men hath failed to grasp, and whose mystery no mind, however acute, can ever hope to unravel". Bahá'u'lláh stated that the soul not only continues to live after the physical death of the human body, but is, in fact, immortal. Heaven can be seen partly as the soul's state of nearness to God; and hell as a state of remoteness from God. Each state follows as a natural consequence of individual efforts, or the lack thereof, to develop spiritually. Bahá'u'lláh taught that individuals have no existence prior to their life here on earth and the soul's evolution is always towards God and away from the material world.

Buddhism teaches that all things are in a constant state of flux: all is changing, and no permanent state exists by itself. This applies to human beings as much as to anything else in the cosmos. Thus, a human being has no permanent self. According to this doctrine of "anatta" (Pāli; Sanskrit: "anātman") – "no-self" or "no soul" – the words "I" or "me" do not refer to any fixed thing. They are simply convenient terms that allow us to refer to an ever-changing entity.

The "anatta" doctrine is not a kind of materialism. Buddhism does not deny the existence of "immaterial" entities, and it (at least traditionally) distinguishes bodily states from mental states. Thus, the conventional translation of "anatta" as "no-soul" can be confusing. If the word "soul" simply refers to an incorporeal component in living things that can continue after death, then Buddhism does not deny the existence of the soul. Instead, Buddhism denies the existence of a permanent entity that remains constant behind the changing corporeal and incorporeal components of a living being. Just as the body changes from moment to moment, so thoughts come and go, and there is no permanent state underlying the mind that experiences these thoughts, as in Cartesianism. Conscious mental states simply arise and perish with no "thinker" behind them. When the body dies, Buddhists believe the incorporeal mental processes continue and are reborn in a new body. Because the mental processes are constantly changing, the being that is reborn is neither entirely different from, nor exactly the same as, the being that died. However, the new being is "continuous" with the being that died – in the same way that the "you" of this moment is continuous with the "you" of a moment before, despite the fact that you are constantly changing.

Buddhist teaching holds that a notion of a permanent, abiding self is a delusion that is one of the causes of human conflict on the emotional, social, and political levels. They add that an understanding of "anatta" provides an accurate description of the human condition, and that this understanding allows us to pacify our mundane desires.

Various schools of Buddhism have differing ideas about what continues after death. The Yogacara school in Mahayana Buddhism said there are Store consciousness which continue to exist after death. In some schools, particularly Tibetan Buddhism, the view is that there are three minds: "very subtle mind", which does not disintegrate in death; "subtle mind", which disintegrates in death and which is "dreaming mind" or "unconscious mind"; and "gross mind", which does not exist when one is "sleeping". Therefore, "gross mind" is less permanent than subtle mind, which does not exist in death. "Very subtle mind", however, does continue, and when it "catches on", or coincides with phenomena, again, a new "subtle mind" emerges, with its own personality/assumptions/habits, and "that" entity experiences karma in the current continuum.

Plants were said to be non-sentient (無情), but Buddhist monks are required to not cut or burn trees, because some sentient beings rely on them. Some Mahayana monks said non-sentient beings such as plants and stones have Buddha-nature.

Certain modern Buddhists, particularly in Western countries, reject—or at least take an agnostic stance toward—the concept of rebirth or reincarnation. Stephen Batchelor discusses this in his book "Buddhism Without Beliefs". Others point to research that has been conducted at the University of Virginia as proof that some people are reborn.

Most Christians understand the soul as an ontological reality distinct from, yet integrally connected with, the body. Its characteristics are described in moral, spiritual, and philosophical terms. Richard Swinburne, a Christian philosopher of religion at Oxford University, wrote that "it is a frequent criticism of substance dualism that dualists cannot say what souls are. Souls are immaterial subjects of mental properties. They have sensations and thoughts, desires and beliefs, and perform intentional actions. Souls are essential parts of human beings". According to a common Christian eschatology, when people die, their souls will be judged by God and determined to go to Heaven or to Hell. Though all branches of Christianity – Catholics, Eastern Orthodox, Oriental Orthodox, Church of the East, Evangelical, and mainline Protestants – teach that Jesus Christ plays a decisive role in the Christian salvation process, the specifics of that role and the part played by individual persons or ecclesiastical rituals and relationships, is a matter of wide diversity in official church teaching, theological speculation and popular practice. Some Christians believe that if one has not repented of one's sins and has not trusted in Jesus Christ as Lord and Savior, he/she will go to Hell and suffer eternal damnation or eternal separation from God. Some hold a belief that babies (including the unborn) and those with cognitive or mental impairments who have died will be received into Heaven on the basis of God's grace through the sacrifice of Jesus.

Other Christians understand the soul as the life, and believe that the dead are sleeping (Christian conditionalism). This belief is traditionally accompanied by the belief that the unrighteous soul will cease to exist instead of suffering eternally (annihilationism). Believers will inherit eternal life either in Heaven, or in a Kingdom of God on earth, and enjoy eternal fellowship with God.

There are also beliefs in universal salvation.
Augustine, one of western Christianity's most influential early Christian thinkers, described the soul as "a special substance, endowed with reason, adapted to rule the body". Some Christians espouse a trichotomic view of humans, which characterizes humans as consisting of a body ("soma"), soul ("psyche"), and spirit ("pneuma"). However, the majority of modern Bible scholars point out how spirit and soul are used interchangeably in many biblical passages, and so hold to dichotomy: the view that each of us is body and soul. Paul said that the "body wars against" the soul, "For the word of God is living and active and sharper than any two-edged sword, and piercing as far as the division of soul and spirit" (Heb 4:12 NASB), and that "I buffet my body", to keep it under control.
Trichotomy was changed to dichotomy as tenet of Christian faith at the Council of Constantinople in 869 regarded as the 8th Ecumenical Council by Roman Catholics.

The 'origin of the soul' has provided a vexing question in Christianity. The major theories put forward include soul creationism, traducianism, and pre-existence. According to creationism, each individual soul is created directly by God, either at the moment of conception or some later time. According to traducianism, the soul comes from the parents by natural generation. According to the preexistence theory, the soul exists before the moment of conception. There have been differing thoughts regarding whether human embryos have souls from conception, or there is a point between conception and birth where the fetus acquires a soul, consciousness, and/or personhood. Stances in this question might more or less influence judgements on the morality of abortion.

The present Catechism of the Catholic Church defines the soul as "the innermost aspect of humans, that which is of greatest value in them, that by which they are in God's image described as 'soul' signifies the "spiritual principle" in man". All souls living and dead will be judged by Jesus Christ when he comes back to earth. The Catholic Church teaches that the existence of each individual soul is dependent wholly upon God: "The doctrine of the faith affirms that the spiritual and immortal soul is created immediately by God."

Protestants generally believe in the soul's existence, but fall into two major camps about what this means in terms of an afterlife. Some, following Calvin, believe in the immortality of the soul and conscious existence after death, while others, following Luther, believe in the mortality of the soul and unconscious "sleep" until the resurrection of the dead. Various new religious movements derived from Adventism—including Christadelphians, Seventh-day Adventists and Jehovah's Witnesses—similarly believe that the dead do not possess a soul separate from the body and are unconscious until the resurrection.

The Church of Jesus Christ of Latter-day Saints teaches that the spirit and body together constitute the Soul of Man (Mankind). "The spirit and the body are the soul of man." Latter-day Saints believe that the soul is the union of a pre-existing, God-made spirit and a temporal body, which is formed by physical conception on earth. After death, the spirit continues to live and progress in the Spirit world until the resurrection, when it is reunited with the body that once housed it. This reuniting of body and spirit results in a perfect soul that is immortal and eternal and capable of receiving a fulness of joy. Latter-day Saint cosmology also describes "intelligences" as the essence of consciousness or agency. These are co-eternal with God, and animate the spirits. The union of a newly created spirit body with an eternally-existing intelligence constitutes a "spirit birth" and justifies God's title "Father of our spirits".

"Ātman" is a Sanskrit word that means inner self or soul. In Hindu philosophy, especially in the Vedanta school of Hinduism, Ātman is the first principle, the "true" self of an individual beyond identification with phenomena, the essence of an individual. In order to attain liberation (moksha), a human being must acquire self-knowledge (atma jnana), which is to realize that one's true self (Ātman) is identical with the transcendent self Brahman. 

The six orthodox schools of Hinduism believe that there is Ātman (self, essence) in every being, a major point of difference with Buddhism, which does not believe that there is either soul or self.

In Hinduism and Jainism, a "jiva" (, , alternative spelling "jiwa"; , , alternative spelling "jeev") is a living being, or any entity imbued with a life force. 

In Jainism, "jiva" is the immortal essence or soul of a living organism (human, animal, fish or plant etc.) which survives physical death. The concept of "Ajiva" in Jainism means "not soul", and represents matter (including body), time, space, non-motion and motion. In Jainism, a "Jiva" is either "samsari" (mundane, caught in cycle of rebirths) or "mukta" (liberated).

The concept of "jiva" in Jainism is similar to "atman" in Hinduism. However, some Hindu traditions differentiate between the two concepts, with "jiva" considered as individual self, while atman as that which is universal unchanging self that is present in all living beings and everything else as the metaphysical Brahman. The latter is sometimes referred to as "jiva-atman" (a soul in a living body).
According to Brahma Kumaris, the soul is an eternal point of light, resides between forehead.

The Quran, the holy book of Islam, distinguishes between the immortal "rūḥ" (spirit or "soul") and the mortal "nafs" (psyche). The immortal rūḥ "drives" the mortal nafs, which comprises temporal desires and perceptions necessary for living.

One of the passages in the Quran that mention "rûh" occur in chapters 17 ("The Night Journey") and 39 ("The Throngs"):
In Jainism, every living being, from plant or bacterium to human, has a soul and the concept forms the very basis of Jainism. According to Jainism, there is no beginning or end to the existence of soul. It is eternal in nature and changes its form until it attains liberation.

The soul "(Jīva)" is basically categorized in one of two ways based on its present state.

Until the time the soul is liberated from the "saṃsāra" (cycle of repeated birth and death), it gets attached to one of these bodies based on the karma (actions) of the individual soul. Irrespective of which state the soul is in, it has got the same attributes and qualities. The difference between the liberated and non-liberated souls is that the qualities and attributes are manifested completely in case of "siddha" (liberated soul) as they have overcome all the karmic bondages whereas in case of non-liberated souls they are partially exhibited.

Concerning the Jain view of the soul, Virchand Gandhi said

The Hebrew terms "nefesh" (literally "living being"), "ruach" (literally "wind"), "neshamah" (literally "breath"), "chayah" (literally "life") and "yechidah" (literally "singularity") are used to describe the soul or spirit.

In Judaism the soul was believed to be given by God to Adam as mentioned in Genesis, 

Judaism relates the quality of one's soul to one's performance of the commandments ("mitzvot)" and reaching higher levels of understanding, and thus closeness to God. A person with such closeness is called a "tzadik". Therefore, Judaism embraces the commemoration of the day of one's death, "nahala"/"Yahrtzeit" and not the birthday as a festivity of remembrance, for only toward the end of life's struggles, tests and challenges could human souls be judged and credited for righteousness. Judaism places great importance on the study of the souls.

Kabbalah and other mystic traditions go into greater detail into the nature of the soul. Kabbalah separates the soul into five elements, corresponding to the five worlds:


Kabbalah also proposed a concept of reincarnation, the "gilgul". (See also "nefesh habehamit" the "animal soul".)

The Scientology view is that a person does not have a soul, it is a soul. A person is immortal, and may be reincarnated if they wish. The Scientology term for the soul is "thetan", derived from the Greek word "theta", symbolizing thought. Scientology counselling (called auditing) addresses the soul to improve abilities, both worldly and spiritual.

According to Nadya Yuguseva, a shaman from the Altai, "A woman has 40 souls; men have just one".

Sikhism considers soul ("atma") to be part of God (Waheguru). Various hymns are cited from the holy book Guru Granth Sahib (SGGS) that suggests this belief. "God is in the Soul and the Soul is in the God." The same concept is repeated at various pages of the SGGS. For example: "The soul is divine; divine is the soul. Worship Him with love." and "The soul is the Lord, and the Lord is the soul; contemplating the Shabad, the Lord is found."

The "atma" or soul according to Sikhism is an entity or "spiritual spark" or "light" in our body because of which the body can sustain life. On the departure of this entity from the body, the body becomes lifeless – No amount of manipulations to the body can make the person make any physical actions. The soul is the ‘driver’ in the body. It is the "roohu" or spirit or "atma", the presence of which makes the physical body alive.

Many religious and philosophical traditions support the view that the soul is the ethereal substance – a spirit; a non material spark – particular to a unique living being. Such traditions often consider the soul both immortal and innately aware of its immortal nature, as well as the true basis for sentience in each living being. The concept of the soul has strong links with notions of an afterlife, but opinions may vary wildly even within a given religion as to what happens to the soul after death. Many within these religions and philosophies see the soul as immaterial, while others consider it possibly material.

According to Chinese traditions, every person has two types of soul called hun and po (魂 and 魄), which are respectively yang and yin. Taoism believes in ten souls, "sanhunqipo" () "three "hun" and seven "po"". The pò is linked to the dead body and the grave, whereas the hún is linked to the ancestral tablet. A living being that loses any of them is said to have mental illness or unconsciousness, while a dead soul may reincarnate to a disability, lower desire realms, or may even be unable to reincarnate.

In theological reference to the soul, the terms "life" and "death" are viewed as emphatically more definitive than the common concepts of "biological life" and "biological death". Because the soul is said to be transcendent of the "material existence," and is said to have (potentially) eternal life, the death of the soul is likewise said to be an "eternal death". Thus, in the concept of divine judgment, God is commonly said to have options with regard to the dispensation of souls, ranging from Heaven (i.e., angels) to hell (i.e., demons), with various concepts in between. Typically both Heaven and hell are said to be eternal, or at least far beyond a typical human concept of lifespan and time.

According to Louis Ginzberg, soul of Adam is the image of God. Every soul of human also escapes from the body every night, rises up to heaven, and fetches new life thence for the body of man.

In Brahma Kumaris, human souls are believed to be incorporeal and eternal. God is considered to be the Supreme Soul, with maximum degrees of spiritual qualities, such as peace, love and purity.

In Helena Blavatsky's Theosophy, the soul is the field of our psychological activity (thinking, emotions, memory, desires, will, and so on) as well as of the so-called paranormal or psychic phenomena (extrasensory perception, out-of-body experiences, etc.). However, the soul is not the highest, but a middle dimension of human beings. Higher than the soul is the spirit, which is considered to be the real self; the source of everything we call "good"—happiness, wisdom, love, compassion, harmony, peace, etc. While the spirit is eternal and incorruptible, the soul is not. The soul acts as a link between the material body and the spiritual self, and therefore shares some characteristics of both. The soul can be attracted either towards the spiritual or towards the material realm, being thus the "battlefield" of good and evil. It is only when the soul is attracted towards the spiritual and merges with the Self that it becomes eternal and divine.

Rudolf Steiner differentiated three stages of soul development, which interpenetrate one another in consciousness:

In Surat Shabda Yoga, the soul is considered to be an exact replica and spark of the Divine. The purpose of Surat Shabd Yoga is to realize one's True Self as soul (Self-Realisation), True Essence (Spirit-Realisation) and True Divinity (God-Realisation) while living in the physical body.

Similarly, the spiritual teacher Meher Baba held that "Atma, or the soul, is in reality identical with Paramatma the Oversoul — which is one, infinite, and eternal...[and] [t]he sole purpose of creation is for the soul to enjoy the infinite state of the Oversoul consciously."

Eckankar, founded by Paul Twitchell in 1965, defines Soul as the true self; the inner, most sacred part of each person.

The ancient Greeks used the word "ensouled" to represent the concept of being "alive", indicating that the earliest surviving western philosophical view believed that the soul was that which gave the body life. The soul was considered the incorporeal or spiritual "breath" that animates (from the Latin, "anima", cf. "animal") the living organism.

Francis M. Cornford quotes Pindar by saying that the soul sleeps while the limbs are active, but when one is sleeping, the soul is active and reveals "an award of joy or sorrow drawing near" in dreams.

Erwin Rohde writes that an early pre-Pythagorean belief presented the soul as lifeless when it departed the body, and that it retired into Hades with no hope of returning to a body.

Drawing on the words of his teacher Socrates, Plato considered the psyche to be the essence of a person, being that which decides how we behave. He considered this essence to be an incorporeal, eternal occupant of our being. Plato says that even after death, the soul exists and is able to think. He believed that as bodies die, the soul is continually reborn in subsequent bodies. However, Aristotle believed that only one part of the soul was immortal namely the intellect ("logos"). The Platonic soul consists of three parts:

The parts are located in different regions of the body:

Plato also compares the three parts of the soul or psyche to a societal caste system. According to Plato's theory, the three-part soul is essentially the same thing as a state's class system because, to function well, each part must contribute so that the whole functions well. Logos keeps the other functions of the soul regulated.

Aristotle (384 BC – 322 BC) defined the soul, or "Psūchê" (ψυχή), as the "first actuality" of a naturally organized body, and argued against its separate existence from the physical body. In Aristotle's view, the primary activity, or full actualization, of a living thing constitutes its soul. For example, the full actualization of an eye, as an independent organism, is to see (its purpose or final cause). Another example is that the full actualization of a human being would be living a fully functional human life in accordance with reason (which he considered to be a faculty unique to humanity). For Aristotle, the soul is the organization of the form and matter of a natural being which allows it to strive for its full actualization. This organization between form and matter is necessary for any activity, or functionality, to be possible in a natural being. Using an artifact (non-natural being) as an example, a house is a building for human habituation, but for a house to be actualized requires the material (wood, nails, bricks, etc.) necessary for its actuality (i.e. being a fully functional house). However, this does not imply that a house has a soul. In regards to artifacts, the source of motion that is required for their full actualization is outside of themselves (for example, a builder builds a house). In natural beings, this source of motion is contained within the being itself. Aristotle elaborates on this point when he addresses the faculties of the soul.

The various faculties of the soul, such as nutrition, movement (peculiar to animals), reason (peculiar to humans), sensation (special, common, and incidental) and so forth, when exercised, constitute the "second" actuality, or fulfillment, of the capacity to be alive. For example, someone who falls asleep, as opposed to someone who falls dead, can wake up and live their life, while the latter can no longer do so.

Aristotle identified three hierarchical levels of natural beings: plants, animals, and people. For these groups, he identified three corresponding levels of soul, or biological activity: the nutritive activity of growth, sustenance and reproduction which all life shares; the self-willed motive activity and sensory faculties, which only animals and people have in common; and finally "reason", of which people alone are capable.

Aristotle's discussion of the soul is in his work, "De Anima" ("On the Soul"). Although mostly seen as opposing Plato in regard to the immortality of the soul, a controversy can be found in relation to the fifth chapter of the third book. In this text both interpretations can be argued for, soul as a whole can be deemed mortal and a part called "active intellect" or "active mind" is immortal and eternal. Advocates exist for both sides of the controversy, but it has been understood that there will be permanent disagreement about its final conclusions, as no other Aristotelian text contains this specific point, and this part of "De Anima" is obscure. Further, Aristotle states that the soul helps humans find the truth and understanding the true purpose or role of the soul is extremely difficult. 

Following Aristotle, Avicenna (Ibn Sina) and Ibn al-Nafis, a Persian philosopher, further elaborated upon the Aristotelian understanding of the soul and developed their own theories on the soul. They both made a distinction between the soul and the spirit, and the Avicennian doctrine on the nature of the soul was influential among the Scholastics. Some of Avicenna's views on the soul include the idea that the immortality of the soul is a consequence of its nature, and not a purpose for it to fulfill. In his theory of "The Ten Intellects", he viewed the human soul as the tenth and final intellect.

While he was imprisoned, Avicenna wrote his famous "Floating Man" thought experiment to demonstrate human self-awareness and the substantial nature of the soul. He told his readers to imagine themselves suspended in the air, isolated from all sensations, which includes no sensory contact with even their own bodies. He argues that in this scenario one would still have self-consciousness. He thus concludes that the idea of the self is not logically dependent on any physical thing, and that the soul should not be seen in relative terms, but as a primary given, a substance. This argument was later refined and simplified by René Descartes in epistemic terms, when he stated: "I can abstract from the supposition of all external things, but not from the supposition of my own consciousness."

Avicenna generally supported Aristotle's idea of the soul originating from the heart, whereas Ibn al-Nafis rejected this idea and instead argued that the soul "is related to the entirety and not to one or a few organs". He further criticized Aristotle's idea whereby every unique soul requires the existence of a unique source, in this case the heart. al-Nafis concluded that "the soul is related primarily neither to the spirit nor to any organ, but rather to the entire matter whose temperament is prepared to receive that soul," and he defined the soul as nothing other than "what a human indicates by saying "I".

Following Aristotle and Avicenna, Thomas Aquinas (1225–74) understood the soul to be the first actuality of the living body. Consequent to this, he distinguished three orders of life: plants, which feed and grow; animals, which add sensation to the operations of plants; and humans, which add intellect to the operations of animals.

Concerning the human soul, his epistemological theory required that, since the knower becomes what he knows, the soul is definitely not corporeal—if it is corporeal when it knows what some corporeal thing is, that thing would come to be within it. Therefore, the soul has an operation which does not rely on a body organ, and therefore the soul can exist without a body. Furthermore, since the rational soul of human beings is a subsistent form and not something made of matter and form, it cannot be destroyed in any natural process. The full argument for the immortality of the soul and Aquinas' elaboration of Aristotelian theory is found in Question 75 of the First Part of the Summa Theologica.

In his discussions of rational psychology, Immanuel Kant (1724–1804) identified the soul as the "I" in the strictest sense, and argued that the existence of inner experience can neither be proved nor disproved.

Gilbert Ryle's ghost in the machine argument, which is a rejection of Descartes' mind–body dualism, can provide a contemporary understanding of the soul/mind, and the problem concerning its connection to the brain/body.

Psychologist James Hillman's archetypal psychology is an attempt to restore the concept of the soul, which Hillman viewed as the "self-sustaining and imagining substrate" upon which consciousness rests. Hillman described the soul as that "which makes meaning possible, [deepens] events into experiences, is communicated in love, and has a religious concern", as well as "a special relation with death". Departing from the Cartesian dualism "between outer tangible reality and inner states of mind", Hillman takes the Neoplatonic stance that there is a "third, middle position" in which soul resides. Archetypal psychology acknowledges this third position by attuning to, and often accepting, the archetypes, dreams, myths, and even psychopathologies through which, in Hillman's view, soul expresses itself.

Neuroscience as an interdisciplinary field, and its branch of cognitive neuroscience particularly, operates under the ontological assumption of physicalism. In other words, it assumes—in order to perform its science—that only the fundamental phenomena studied by physics exist. Thus, neuroscience seeks to understand mental phenomena within the framework according to which human thought and behavior are caused solely by physical processes taking place inside the brain, and it operates by the way of reductionism by seeking an explanation for the mind in terms of brain activity.

To study the mind in terms of the brain several methods of functional neuroimaging are used to study the neuroanatomical correlates of various cognitive processes that constitute the mind. The evidence from brain imaging indicates that all processes of the mind have physical correlates in brain function. However, such correlational studies cannot determine whether neural activity plays a causal role in the occurrence of these cognitive processes (correlation does not imply causation) and they cannot determine if the neural activity is either necessary or sufficient for such processes to occur. Identification of causation, and of necessary and sufficient conditions requires explicit experimental manipulation of that activity. If manipulation of brain activity changes consciousness, then a causal role for that brain activity can be inferred. Two of the most common types of manipulation experiments are loss-of-function and gain-of-function experiments. In a loss-of-function (also called "necessity") experiment, a part of the nervous system is diminished or removed in an attempt to determine if it is necessary for a certain process to occur, and in a gain-of-function (also called "sufficiency") experiment, an aspect of the nervous system is increased relative to normal. Manipulations of brain activity can be performed with direct electrical brain stimulation, magnetic brain stimulation using transcranial magnetic stimulation, psychopharmacological manipulation, optogenetic manipulation, and by studying the symptoms of brain damage (case studies) and lesions. In addition, neuroscientists are also investigating how the mind develops with the development of the brain.

Physicist Sean M. Carroll has written that the idea of a soul is in opposition to quantum field theory (QFT). He writes that for a soul to exist: "Not only is new physics required, but dramatically new physics. Within QFT, there can't be a new collection of 'spirit particles' and 'spirit forces' that interact with our regular atoms, because we would have detected them in existing experiments."

Quantum indeterminism has been invoked by some theorists as a solution to the problem of how a soul might interact with the brain but neuroscientist Peter Clarke found errors with this viewpoint, noting there is no evidence that such processes play a role in brain function; and concluded that a Cartesian soul has no basis from quantum physics.

Some parapsychologists have attempted to establish, by scientific experiment, whether a soul separate from the brain exists, as is more commonly defined in religion rather than as a synonym of psyche or mind. Milbourne Christopher (1979) and Mary Roach (2010) have argued that none of the attempts by parapsychologists have yet succeeded.

In 1901 Duncan MacDougall conducted an experiment in which he made weight measurements of patients as they died. He claimed that there was weight loss of varying amounts at the time of death; he concluded the soul weighed 21 grams. The physicist Robert L. Park has written that MacDougall's experiments "are not regarded today as having any scientific merit" and the psychologist Bruce Hood wrote that "because the weight loss was not reliable or replicable, his findings were unscientific."





</doc>
<doc id="28299" url="https://en.wikipedia.org/wiki?curid=28299" title="Steradian">
Steradian

The steradian (symbol: sr) or square radian is the SI unit of solid angle. It is used in three-dimensional geometry, and is analogous to the radian, which quantifies planar angles. Whereas an angle in radians, projected onto a circle, gives a "length" on the circumference, a solid angle in steradians, projected onto a sphere, gives an "area" on the surface. The name is derived from the Greek στερεός "stereos" 'solid' + radian.

The steradian, like the radian, is a dimensionless unit, essentially because a solid angle is the ratio between the area subtended and the square of its distance from the center: both the numerator and denominator of this ratio have dimension length squared (i.e. , dimensionless). It is useful, however, to distinguish between dimensionless quantities of a different nature, so the symbol "sr" is used to indicate a solid angle. For example, radiant intensity can be measured in watts per steradian (W⋅sr). The steradian was formerly an SI supplementary unit, but this category was abolished in 1995 and the steradian is now considered an SI derived unit.

A steradian can be defined as the solid angle subtended at the center of a unit sphere by a unit area on its surface. For a general sphere of radius "r", any portion of its surface with area subtends one steradian at its center.

The solid angle is related to the area it cuts out of a sphere:

Because the surface area "A" of a sphere is 4"r", the definition implies that a sphere subtends 4 steradians (≈ 12.56637 sr) at its center. By the same argument, the maximum solid angle that can be subtended at any point is 4 sr.

Since , it corresponds to the area of a spherical cap () (where "h" stands for the "height" of the cap), and the relationship holds. Therefore, one steradian corresponds to the plane (i.e. radian) angle of the cross-section of a simple cone subtending the plane angle 2"θ", with "θ" given by:

This angle corresponds to the plane aperture angle of 2"θ" ≈ 1.144 rad or 65.54°.

A steradian is also equal to the spherical area of a polygon having an angle excess of 1 radian, to of a complete sphere, or to ≈ 3282.80635 square degrees.

The solid angle of a cone whose cross-section subtends the angle 2"θ" is:

Millisteradians (msr) and microsteradians (μsr) are occasionally used to describe light and particle beams. Other multiples are rarely used.

Solid angles over 4 steradians—the solid angle of a full Euclidean sphere—are rarely encountered.



</doc>
<doc id="28305" url="https://en.wikipedia.org/wiki?curid=28305" title="String theory">
String theory

In physics, string theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. It describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale, a string looks just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In string theory, one of the many vibrational states of the string corresponds to the graviton, a quantum mechanical particle that carries gravitational force. Thus string theory is a theory of quantum gravity.

String theory is a broad and varied subject that attempts to address a number of deep questions of fundamental physics. String theory has been applied to a variety of problems in black hole physics, early universe cosmology, nuclear physics, and condensed matter physics, and it has stimulated a number of major developments in pure mathematics. Because string theory potentially provides a unified description of gravity and particle physics, it is a candidate for a theory of everything, a self-contained mathematical model that describes all fundamental forces and forms of matter. Despite much work on these problems, it is not known to what extent string theory describes the real world or how much freedom the theory allows in the choice of its details.

String theory was first studied in the late 1960s as a theory of the strong nuclear force, before being abandoned in favor of quantum chromodynamics. Subsequently, it was realized that the very properties that made string theory unsuitable as a theory of nuclear physics made it a promising candidate for a quantum theory of gravity. The earliest version of string theory, bosonic string theory, incorporated only the class of particles known as bosons. It later developed into superstring theory, which posits a connection called supersymmetry between bosons and the class of particles called fermions. Five consistent versions of superstring theory were developed before it was conjectured in the mid-1990s that they were all different limiting cases of a single theory in eleven dimensions known as M-theory. In late 1997, theorists discovered an important relationship called the AdS/CFT correspondence, which relates string theory to another type of physical theory called a quantum field theory.

One of the challenges of string theory is that the full theory does not have a satisfactory definition in all circumstances. Another issue is that the theory is thought to describe an enormous landscape of possible universes, and this has complicated efforts to develop theories of particle physics based on string theory. These issues have led some in the community to criticize these approaches to physics and question the value of continued research on string theory unification.

In the twentieth century, two theoretical frameworks emerged for formulating the laws of physics. The first is Albert Einstein's general theory of relativity, a theory that explains the force of gravity and the structure of space and time. The other is quantum mechanics which is a completely different formulation to describe physical phenomena using the known probability principles. By the late 1970s, these two frameworks had proven to be sufficient to explain most of the observed features of the universe, from elementary particles to atoms to the evolution of stars and the universe as a whole.

In spite of these successes, there are still many problems that remain to be solved. One of the deepest problems in modern physics is the problem of quantum gravity. The general theory of relativity is formulated within the framework of classical physics, whereas the other fundamental forces are described within the framework of quantum mechanics. A quantum theory of gravity is needed in order to reconcile general relativity with the principles of quantum mechanics, but difficulties arise when one attempts to apply the usual prescriptions of quantum theory to the force of gravity. In addition to the problem of developing a consistent theory of quantum gravity, there are many other fundamental problems in the physics of atomic nuclei, black holes, and the early universe.

String theory is a theoretical framework that attempts to address these questions and many others. The starting point for string theory is the idea that the point-like particles of particle physics can also be modeled as one-dimensional objects called strings. String theory describes how strings propagate through space and interact with each other. In a given version of string theory, there is only one kind of string, which may look like a small loop or segment of ordinary string, and it can vibrate in different ways. On distance scales larger than the string scale, a string will look just like an ordinary particle, with its mass, charge, and other properties determined by the vibrational state of the string. In this way, all of the different elementary particles may be viewed as vibrating strings. In string theory, one of the vibrational states of the string gives rise to the graviton, a quantum mechanical particle that carries gravitational force. Thus string theory is a theory of quantum gravity.

One of the main developments of the past several decades in string theory was the discovery of certain "dualities", mathematical transformations that identify one physical theory with another. Physicists studying string theory have discovered a number of these dualities between different versions of string theory, and this has led to the conjecture that all consistent versions of string theory are subsumed in a single framework known as M-theory.

Studies of string theory have also yielded a number of results on the nature of black holes and the gravitational interaction. There are certain paradoxes that arise when one attempts to understand the quantum aspects of black holes, and work on string theory has attempted to clarify these issues. In late 1997 this line of work culminated in the discovery of the anti-de Sitter/conformal field theory correspondence or AdS/CFT. This is a theoretical result which relates string theory to other physical theories which are better understood theoretically. The AdS/CFT correspondence has implications for the study of black holes and quantum gravity, and it has been applied to other subjects, including nuclear and condensed matter physics.

Since string theory incorporates all of the fundamental interactions, including gravity, many physicists hope that it fully describes our universe, making it a theory of everything. One of the goals of current research in string theory is to find a solution of the theory that reproduces the observed spectrum of elementary particles, with a small cosmological constant, containing dark matter and a plausible mechanism for cosmic inflation. While there has been progress toward these goals, it is not known to what extent string theory describes the real world or how much freedom the theory allows in the choice of details.

One of the challenges of string theory is that the full theory does not have a satisfactory definition in all circumstances. The scattering of strings is most straightforwardly defined using the techniques of perturbation theory, but it is not known in general how to define string theory nonperturbatively. It is also not clear whether there is any principle by which string theory selects its vacuum state, the physical state that determines the properties of our universe. These problems have led some in the community to criticize these approaches to the unification of physics and question the value of continued research on these problems.

The application of quantum mechanics to physical objects such as the electromagnetic field, which are extended in space and time, is known as quantum field theory. In particle physics, quantum field theories form the basis for our understanding of elementary particles, which are modeled as excitations in the fundamental fields.

In quantum field theory, one typically computes the probabilities of various physical events using the techniques of perturbation theory. Developed by Richard Feynman and others in the first half of the twentieth century, perturbative quantum field theory uses special diagrams called Feynman diagrams to organize computations. One imagines that these diagrams depict the paths of point-like particles and their interactions.

The starting point for string theory is the idea that the point-like particles of quantum field theory can also be modeled as one-dimensional objects called strings. The interaction of strings is most straightforwardly defined by generalizing the perturbation theory used in ordinary quantum field theory. At the level of Feynman diagrams, this means replacing the one-dimensional diagram representing the path of a point particle by a two-dimensional surface representing the motion of a string. Unlike in quantum field theory, string theory does not have a full non-perturbative definition, so many of the theoretical questions that physicists would like to answer remain out of reach.

In theories of particle physics based on string theory, the characteristic length scale of strings is assumed to be on the order of the Planck length, or meters, the scale at which the effects of quantum gravity are believed to become significant. On much larger length scales, such as the scales visible in physics laboratories, such objects would be indistinguishable from zero-dimensional point particles, and the vibrational state of the string would determine the type of particle. One of the vibrational states of a string corresponds to the graviton, a quantum mechanical particle that carries the gravitational force.

The original version of string theory was bosonic string theory, but this version described only bosons, a class of particles which transmit forces between the matter particles, or fermions. Bosonic string theory was eventually superseded by theories called superstring theories. These theories describe both bosons and fermions, and they incorporate a theoretical idea called supersymmetry. This is a mathematical relation that exists in certain physical theories between the bosons and fermions. In theories with supersymmetry, each boson has a counterpart which is a fermion, and vice versa.

There are several versions of superstring theory: type I, type IIA, type IIB, and two flavors of heterotic string theory ( and ). The different theories allow different types of strings, and the particles that arise at low energies exhibit different symmetries. For example, the type I theory includes both open strings (which are segments with endpoints) and closed strings (which form closed loops), while types IIA, IIB and heterotic include only closed strings.

In everyday life, there are three familiar dimensions of space: height, width and length. Einstein's general theory of relativity treats time as a dimension on par with the three spatial dimensions; in general relativity, space and time are not modeled as separate entities but are instead unified to a four-dimensional spacetime. In this framework, the phenomenon of gravity is viewed as a consequence of the geometry of spacetime.

In spite of the fact that the universe is well described by four-dimensional spacetime, there are several reasons why physicists consider theories in other dimensions. In some cases, by modeling spacetime in a different number of dimensions, a theory becomes more mathematically tractable, and one can perform calculations and gain general insights more easily. There are also situations where theories in two or three spacetime dimensions are useful for describing phenomena in condensed matter physics. Finally, there exist scenarios in which there could actually be more than four dimensions of spacetime which have nonetheless managed to escape detection.

One notable feature of string theories is that these theories require extra dimensions of spacetime for their mathematical consistency. In bosonic string theory, spacetime is 26-dimensional, while in superstring theory it is 10-dimensional, and in M-theory it is 11-dimensional. In order to describe real physical phenomena using string theory, one must therefore imagine scenarios in which these extra dimensions would not be observed in experiments.

Compactification is one way of modifying the number of dimensions in a physical theory. In compactification, some of the extra dimensions are assumed to "close up" on themselves to form circles. In the limit where these curled up dimensions become very small, one obtains a theory in which spacetime has effectively a lower number of dimensions. A standard analogy for this is to consider a multidimensional object such as a garden hose. If the hose is viewed from a sufficient distance, it appears to have only one dimension, its length. However, as one approaches the hose, one discovers that it contains a second dimension, its circumference. Thus, an ant crawling on the surface of the hose would move in two dimensions.

Compactification can be used to construct models in which spacetime is effectively four-dimensional. However, not every way of compactifying the extra dimensions produces a model with the right properties to describe nature. In a viable model of particle physics, the compact extra dimensions must be shaped like a Calabi–Yau manifold. A Calabi–Yau manifold is a special space which is typically taken to be six-dimensional in applications to string theory. It is named after mathematicians Eugenio Calabi and Shing-Tung Yau.

Another approach to reducing the number of dimensions is the so-called brane-world scenario. In this approach, physicists assume that the observable universe is a four-dimensional subspace of a higher dimensional space. In such models, the force-carrying bosons of particle physics arise from open strings with endpoints attached to the four-dimensional subspace, while gravity arises from closed strings propagating through the larger ambient space. This idea plays an important role in attempts to develop models of real world physics based on string theory, and it provides a natural explanation for the weakness of gravity compared to the other fundamental forces.

One notable fact about string theory is that the different versions of the theory all turn out to be related in highly nontrivial ways. One of the relationships that can exist between different string theories is called S-duality. This is a relationship which says that a collection of strongly interacting particles in one theory can, in some cases, be viewed as a collection of weakly interacting particles in a completely different theory. Roughly speaking, a collection of particles is said to be strongly interacting if they combine and decay often and weakly interacting if they do so infrequently. Type I string theory turns out to be equivalent by S-duality to the heterotic string theory. Similarly, type IIB string theory is related to itself in a nontrivial way by S-duality.

Another relationship between different string theories is T-duality. Here one considers strings propagating around a circular extra dimension. T-duality states that a string propagating around a circle of radius is equivalent to a string propagating around a circle of radius in the sense that all observable quantities in one description are identified with quantities in the dual description. For example, a string has momentum as it propagates around a circle, and it can also wind around the circle one or more times. The number of times the string winds around a circle is called the winding number. If a string has momentum and winding number in one description, it will have momentum and winding number in the dual description. For example, type IIA string theory is equivalent to type IIB string theory via T-duality, and the two versions of heterotic string theory are also related by T-duality.

In general, the term "duality" refers to a situation where two seemingly different physical systems turn out to be equivalent in a nontrivial way. Two theories related by a duality need not be string theories. For example, Montonen–Olive duality is example of an S-duality relationship between quantum field theories. The AdS/CFT correspondence is example of a duality which relates string theory to a quantum field theory. If two theories are related by a duality, it means that one theory can be transformed in some way so that it ends up looking just like the other theory. The two theories are then said to be "dual" to one another under the transformation. Put differently, the two theories are mathematically different descriptions of the same phenomena.

In string theory and other related theories, a brane is a physical object that generalizes the notion of a point particle to higher dimensions. For instance, a point particle can be viewed as a brane of dimension zero, while a string can be viewed as a brane of dimension one. It is also possible to consider higher-dimensional branes. In dimension "p", these are called "p"-branes. The word brane comes from the word "membrane" which refers to a two-dimensional brane.

Branes are dynamical objects which can propagate through spacetime according to the rules of quantum mechanics. They have mass and can have other attributes such as charge. A "p"-brane sweeps out a ("p"+1)-dimensional volume in spacetime called its "worldvolume". Physicists often study fields analogous to the electromagnetic field which live on the worldvolume of a brane.

In string theory, D-branes are an important class of branes that arise when one considers open strings. As an open string propagates through spacetime, its endpoints are required to lie on a D-brane. The letter "D" in D-brane refers to a certain mathematical condition on the system known as the Dirichlet boundary condition. The study of D-branes in string theory has led to important results such as the AdS/CFT correspondence, which has shed light on many problems in quantum field theory.

Branes are frequently studied from a purely mathematical point of view, and they are described as objects of certain categories, such as the derived category of coherent sheaves on a complex algebraic variety, or the Fukaya category of a symplectic manifold. The connection between the physical notion of a brane and the mathematical notion of a category has led to important mathematical insights in the fields of algebraic and symplectic geometry and representation theory.

Prior to 1995, theorists believed that there were five consistent versions of superstring theory (type I, type IIA, type IIB, and two versions of heterotic string theory). This understanding changed in 1995 when Edward Witten suggested that the five theories were just special limiting cases of an eleven-dimensional theory called M-theory. Witten's conjecture was based on the work of a number of other physicists, including Ashoke Sen, Chris Hull, Paul Townsend, and Michael Duff. His announcement led to a flurry of research activity now known as the second superstring revolution.

In the 1970s, many physicists became interested in supergravity theories, which combine general relativity with supersymmetry. Whereas general relativity makes sense in any number of dimensions, supergravity places an upper limit on the number of dimensions. In 1978, work by Werner Nahm showed that the maximum spacetime dimension in which one can formulate a consistent supersymmetric theory is eleven. In the same year, Eugene Cremmer, Bernard Julia, and Joel Scherk of the École Normale Supérieure showed that supergravity not only permits up to eleven dimensions but is in fact most elegant in this maximal number of dimensions.

Initially, many physicists hoped that by compactifying eleven-dimensional supergravity, it might be possible to construct realistic models of our four-dimensional world. The hope was that such models would provide a unified description of the four fundamental forces of nature: electromagnetism, the strong and weak nuclear forces, and gravity. Interest in eleven-dimensional supergravity soon waned as various flaws in this scheme were discovered. One of the problems was that the laws of physics appear to distinguish between clockwise and counterclockwise, a phenomenon known as chirality. Edward Witten and others observed this chirality property cannot be readily derived by compactifying from eleven dimensions.

In the first superstring revolution in 1984, many physicists turned to string theory as a unified theory of particle physics and quantum gravity. Unlike supergravity theory, string theory was able to accommodate the chirality of the standard model, and it provided a theory of gravity consistent with quantum effects. Another feature of string theory that many physicists were drawn to in the 1980s and 1990s was its high degree of uniqueness. In ordinary particle theories, one can consider any collection of elementary particles whose classical behavior is described by an arbitrary Lagrangian. In string theory, the possibilities are much more constrained: by the 1990s, physicists had argued that there were only five consistent supersymmetric versions of the theory.

Although there were only a handful of consistent superstring theories, it remained a mystery why there was not just one consistent formulation. However, as physicists began to examine string theory more closely, they realized that these theories are related in intricate and nontrivial ways. They found that a system of strongly interacting strings can, in some cases, be viewed as a system of weakly interacting strings. This phenomenon is known as S-duality. It was studied by Ashoke Sen in the context of heterotic strings in four dimensions and by Chris Hull and Paul Townsend in the context of the type IIB theory. Theorists also found that different string theories may be related by T-duality. This duality implies that strings propagating on completely different spacetime geometries may be physically equivalent.

At around the same time, as many physicists were studying the properties of strings, a small group of physicists was examining the possible applications of higher dimensional objects. In 1987, Eric Bergshoeff, Ergin Sezgin, and Paul Townsend showed that eleven-dimensional supergravity includes two-dimensional branes. Intuitively, these objects look like sheets or membranes propagating through the eleven-dimensional spacetime. Shortly after this discovery, Michael Duff, Paul Howe, Takeo Inami, and Kellogg Stelle considered a particular compactification of eleven-dimensional supergravity with one of the dimensions curled up into a circle. In this setting, one can imagine the membrane wrapping around the circular dimension. If the radius of the circle is sufficiently small, then this membrane looks just like a string in ten-dimensional spacetime. In fact, Duff and his collaborators showed that this construction reproduces exactly the strings appearing in type IIA superstring theory.

Speaking at a string theory conference in 1995, Edward Witten made the surprising suggestion that all five superstring theories were in fact just different limiting cases of a single theory in eleven spacetime dimensions. Witten's announcement drew together all of the previous results on S- and T-duality and the appearance of higher dimensional branes in string theory. In the months following Witten's announcement, hundreds of new papers appeared on the Internet confirming different parts of his proposal. Today this flurry of work is known as the second superstring revolution.

Initially, some physicists suggested that the new theory was a fundamental theory of membranes, but Witten was skeptical of the role of membranes in the theory. In a paper from 1996, Hořava and Witten wrote "As it has been proposed that the eleven-dimensional theory is a supermembrane theory but there are some reasons to doubt that interpretation, we will non-committally call it the M-theory, leaving to the future the relation of M to membranes." In the absence of an understanding of the true meaning and structure of M-theory, Witten has suggested that the "M" should stand for "magic", "mystery", or "membrane" according to taste, and the true meaning of the title should be decided when a more fundamental formulation of the theory is known.

In mathematics, a matrix is a rectangular array of numbers or other data. In physics, a matrix model is a particular kind of physical theory whose mathematical formulation involves the notion of a matrix in an important way. A matrix model describes the behavior of a set of matrices within the framework of quantum mechanics.

One important example of a matrix model is the BFSS matrix model proposed by Tom Banks, Willy Fischler, Stephen Shenker, and Leonard Susskind in 1997. This theory describes the behavior of a set of nine large matrices. In their original paper, these authors showed, among other things, that the low energy limit of this matrix model is described by eleven-dimensional supergravity. These calculations led them to propose that the BFSS matrix model is exactly equivalent to M-theory. The BFSS matrix model can therefore be used as a prototype for a correct formulation of M-theory and a tool for investigating the properties of M-theory in a relatively simple setting.

The development of the matrix model formulation of M-theory has led physicists to consider various connections between string theory and a branch of mathematics called noncommutative geometry. This subject is a generalization of ordinary geometry in which mathematicians define new geometric notions using tools from noncommutative algebra. In a paper from 1998, Alain Connes, Michael R. Douglas, and Albert Schwarz showed that some aspects of matrix models and M-theory are described by a noncommutative quantum field theory, a special kind of physical theory in which spacetime is described mathematically using noncommutative geometry. This established a link between matrix models and M-theory on the one hand, and noncommutative geometry on the other hand. It quickly led to the discovery of other important links between noncommutative geometry and various physical theories.

In general relativity, a black hole is defined as a region of spacetime in which the gravitational field is so strong that no particle or radiation can escape. In the currently accepted models of stellar evolution, black holes are thought to arise when massive stars undergo gravitational collapse, and many galaxies are thought to contain supermassive black holes at their centers. Black holes are also important for theoretical reasons, as they present profound challenges for theorists attempting to understand the quantum aspects of gravity. String theory has proved to be an important tool for investigating the theoretical properties of black holes because it provides a framework in which theorists can study their thermodynamics.

In the branch of physics called statistical mechanics, entropy is a measure of the randomness or disorder of a physical system. This concept was studied in the 1870s by the Austrian physicist Ludwig Boltzmann, who showed that the thermodynamic properties of a gas could be derived from the combined properties of its many constituent molecules. Boltzmann argued that by averaging the behaviors of all the different molecules in a gas, one can understand macroscopic properties such as volume, temperature, and pressure. In addition, this perspective led him to give a precise definition of entropy as the natural logarithm of the number of different states of the molecules (also called "microstates") that give rise to the same macroscopic features.

In the twentieth century, physicists began to apply the same concepts to black holes. In most systems such as gases, the entropy scales with the volume. In the 1970s, the physicist Jacob Bekenstein suggested that the entropy of a black hole is instead proportional to the "surface area" of its event horizon, the boundary beyond which matter and radiation is lost to its gravitational attraction. When combined with ideas of the physicist Stephen Hawking, Bekenstein's work yielded a precise formula for the entropy of a black hole. The Bekenstein–Hawking formula expresses the entropy as

where is the speed of light, is Boltzmann's constant, is the reduced Planck constant, is Newton's constant, and is the surface area of the event horizon.

Like any physical system, a black hole has an entropy defined in terms of the number of different microstates that lead to the same macroscopic features. The Bekenstein–Hawking entropy formula gives the expected value of the entropy of a black hole, but by the 1990s, physicists still lacked a derivation of this formula by counting microstates in a theory of quantum gravity. Finding such a derivation of this formula was considered an important test of the viability of any theory of quantum gravity such as string theory.

In a paper from 1996, Andrew Strominger and Cumrun Vafa showed how to derive the Beckenstein–Hawking formula for certain black holes in string theory. Their calculation was based on the observation that D-branes—which look like fluctuating membranes when they are weakly interacting—become dense, massive objects with event horizons when the interactions are strong. In other words, a system of strongly interacting D-branes in string theory is indistinguishable from a black hole. Strominger and Vafa analyzed such D-brane systems and calculated the number of different ways of placing D-branes in spacetime so that their combined mass and charge is equal to a given mass and charge for the resulting black hole. Their calculation reproduced the Bekenstein–Hawking formula exactly, including the factor of . Subsequent work by Strominger, Vafa, and others refined the original calculations and gave the precise values of the "quantum corrections" needed to describe very small black holes.

The black holes that Strominger and Vafa considered in their original work were quite different from real astrophysical black holes. One difference was that Strominger and Vafa considered only extremal black holes in order to make the calculation tractable. These are defined as black holes with the lowest possible mass compatible with a given charge. Strominger and Vafa also restricted attention to black holes in five-dimensional spacetime with unphysical supersymmetry.

Although it was originally developed in this very particular and physically unrealistic context in string theory, the entropy calculation of Strominger and Vafa has led to a qualitative understanding of how black hole entropy can be accounted for in any theory of quantum gravity. Indeed, in 1998, Strominger argued that the original result could be generalized to an arbitrary consistent theory of quantum gravity without relying on strings or supersymmetry. In collaboration with several other authors in 2010, he showed that some results on black hole entropy could be extended to non-extremal astrophysical black holes.

One approach to formulating string theory and studying its properties is provided by the anti-de Sitter/conformal field theory (AdS/CFT) correspondence. This is a theoretical result which implies that string theory is in some cases equivalent to a quantum field theory. In addition to providing insights into the mathematical structure of string theory, the AdS/CFT correspondence has shed light on many aspects of quantum field theory in regimes where traditional calculational techniques are ineffective. The AdS/CFT correspondence was first proposed by Juan Maldacena in late 1997. Important aspects of the correspondence were elaborated in articles by Steven Gubser, Igor Klebanov, and Alexander Markovich Polyakov, and by Edward Witten. By 2010, Maldacena's article had over 7000 citations, becoming the most highly cited article in the field of high energy physics.

In the AdS/CFT correspondence, the geometry of spacetime is described in terms of a certain vacuum solution of Einstein's equation called anti-de Sitter space. In very elementary terms, anti-de Sitter space is a mathematical model of spacetime in which the notion of distance between points (the metric) is different from the notion of distance in ordinary Euclidean geometry. It is closely related to hyperbolic space, which can be viewed as a disk as illustrated on the left. This image shows a tessellation of a disk by triangles and squares. One can define the distance between points of this disk in such a way that all the triangles and squares are the same size and the circular outer boundary is infinitely far from any point in the interior.

One can imagine a stack of hyperbolic disks where each disk represents the state of the universe at a given time. The resulting geometric object is three-dimensional anti-de Sitter space. It looks like a solid cylinder in which any cross section is a copy of the hyperbolic disk. Time runs along the vertical direction in this picture. The surface of this cylinder plays an important role in the AdS/CFT correspondence. As with the hyperbolic plane, anti-de Sitter space is curved in such a way that any point in the interior is actually infinitely far from this boundary surface.

This construction describes a hypothetical universe with only two space dimensions and one time dimension, but it can be generalized to any number of dimensions. Indeed, hyperbolic space can have more than two dimensions and one can "stack up" copies of hyperbolic space to get higher-dimensional models of anti-de Sitter space.

An important feature of anti-de Sitter space is its boundary (which looks like a cylinder in the case of three-dimensional anti-de Sitter space). One property of this boundary is that, within a small region on the surface around any given point, it looks just like Minkowski space, the model of spacetime used in nongravitational physics. One can therefore consider an auxiliary theory in which "spacetime" is given by the boundary of anti-de Sitter space. This observation is the starting point for AdS/CFT correspondence, which states that the boundary of anti-de Sitter space can be regarded as the "spacetime" for a quantum field theory. The claim is that this quantum field theory is equivalent to a gravitational theory, such as string theory, in the bulk anti-de Sitter space in the sense that there is a "dictionary" for translating entities and calculations in one theory into their counterparts in the other theory. For example, a single particle in the gravitational theory might correspond to some collection of particles in the boundary theory. In addition, the predictions in the two theories are quantitatively identical so that if two particles have a 40 percent chance of colliding in the gravitational theory, then the corresponding collections in the boundary theory would also have a 40 percent chance of colliding.

The discovery of the AdS/CFT correspondence was a major advance in physicists' understanding of string theory and quantum gravity. One reason for this is that the correspondence provides a formulation of string theory in terms of quantum field theory, which is well understood by comparison. Another reason is that it provides a general framework in which physicists can study and attempt to resolve the paradoxes of black holes.

In 1975, Stephen Hawking published a calculation which suggested that black holes are not completely black but emit a dim radiation due to quantum effects near the event horizon. At first, Hawking's result posed a problem for theorists because it suggested that black holes destroy information. More precisely, Hawking's calculation seemed to conflict with one of the basic postulates of quantum mechanics, which states that physical systems evolve in time according to the Schrödinger equation. This property is usually referred to as unitarity of time evolution. The apparent contradiction between Hawking's calculation and the unitarity postulate of quantum mechanics came to be known as the black hole information paradox.

The AdS/CFT correspondence resolves the black hole information paradox, at least to some extent, because it shows how a black hole can evolve in a manner consistent with quantum mechanics in some contexts. Indeed, one can consider black holes in the context of the AdS/CFT correspondence, and any such black hole corresponds to a configuration of particles on the boundary of anti-de Sitter space. These particles obey the usual rules of quantum mechanics and in particular evolve in a unitary fashion, so the black hole must also evolve in a unitary fashion, respecting the principles of quantum mechanics. In 2005, Hawking announced that the paradox had been settled in favor of information conservation by the AdS/CFT correspondence, and he suggested a concrete mechanism by which black holes might preserve information.

In addition to its applications to theoretical problems in quantum gravity, the AdS/CFT correspondence has been applied to a variety of problems in quantum field theory. One physical system that has been studied using the AdS/CFT correspondence is the quark–gluon plasma, an exotic state of matter produced in particle accelerators. This state of matter arises for brief instants when heavy ions such as gold or lead nuclei are collided at high energies. Such collisions cause the quarks that make up atomic nuclei to deconfine at temperatures of approximately two trillion kelvins, conditions similar to those present at around seconds after the Big Bang.

The physics of the quark–gluon plasma is governed by a theory called quantum chromodynamics, but this theory is mathematically intractable in problems involving the quark–gluon plasma. In an article appearing in 2005, Đàm Thanh Sơn and his collaborators showed that the AdS/CFT correspondence could be used to understand some aspects of the quark–gluon plasma by describing it in the language of string theory. By applying the AdS/CFT correspondence, Sơn and his collaborators were able to describe the quark gluon plasma in terms of black holes in five-dimensional spacetime. The calculation showed that the ratio of two quantities associated with the quark–gluon plasma, the shear viscosity and volume density of entropy, should be approximately equal to a certain universal constant. In 2008, the predicted value of this ratio for the quark–gluon plasma was confirmed at the Relativistic Heavy Ion Collider at Brookhaven National Laboratory.

The AdS/CFT correspondence has also been used to study aspects of condensed matter physics. Over the decades, experimental condensed matter physicists have discovered a number of exotic states of matter, including superconductors and superfluids. These states are described using the formalism of quantum field theory, but some phenomena are difficult to explain using standard field theoretic techniques. Some condensed matter theorists including Subir Sachdev hope that the AdS/CFT correspondence will make it possible to describe these systems in the language of string theory and learn more about their behavior.

So far some success has been achieved in using string theory methods to describe the transition of a superfluid to an insulator. A superfluid is a system of electrically neutral atoms that flows without any friction. Such systems are often produced in the laboratory using liquid helium, but recently experimentalists have developed new ways of producing artificial superfluids by pouring trillions of cold atoms into a lattice of criss-crossing lasers. These atoms initially behave as a superfluid, but as experimentalists increase the intensity of the lasers, they become less mobile and then suddenly transition to an insulating state. During the transition, the atoms behave in an unusual way. For example, the atoms slow to a halt at a rate that depends on the temperature and on Planck's constant, the fundamental parameter of quantum mechanics, which does not enter into the description of the other phases. This behavior has recently been understood by considering a dual description where properties of the fluid are described in terms of a higher dimensional black hole.

In addition to being an idea of considerable theoretical interest, string theory provides a framework for constructing models of real world physics that combine general relativity and particle physics. Phenomenology is the branch of theoretical physics in which physicists construct realistic models of nature from more abstract theoretical ideas. String phenomenology is the part of string theory that attempts to construct realistic or semi-realistic models based on string theory.

Partly because of theoretical and mathematical difficulties and partly because of the extremely high energies needed to test these theories experimentally, there is so far no experimental evidence that would unambiguously point to any of these models being a correct fundamental description of nature. This has led some in the community to criticize these approaches to unification and question the value of continued research on these problems.

The currently accepted theory describing elementary particles and their interactions is known as the standard model of particle physics. This theory provides a unified description of three of the fundamental forces of nature: electromagnetism and the strong and weak nuclear forces. Despite its remarkable success in explaining a wide range of physical phenomena, the standard model cannot be a complete description of reality. This is because the standard model fails to incorporate the force of gravity and because of problems such as the hierarchy problem and the inability to explain the structure of fermion masses or dark matter.

String theory has been used to construct a variety of models of particle physics going beyond the standard model. Typically, such models are based on the idea of compactification. Starting with the ten- or eleven-dimensional spacetime of string or M-theory, physicists postulate a shape for the extra dimensions. By choosing this shape appropriately, they can construct models roughly similar to the standard model of particle physics, together with additional undiscovered particles. One popular way of deriving realistic physics from string theory is to start with the heterotic theory in ten dimensions and assume that the six extra dimensions of spacetime are shaped like a six-dimensional Calabi–Yau manifold. Such compactifications offer many ways of extracting realistic physics from string theory. Other similar methods can be used to construct realistic or semi-realistic models of our four-dimensional world based on M-theory.

The Big Bang theory is the prevailing cosmological model for the universe from the earliest known periods through its subsequent large-scale evolution. Despite its success in explaining many observed features of the universe including galactic redshifts, the relative abundance of light elements such as hydrogen and helium, and the existence of a cosmic microwave background, there are several questions that remain unanswered. For example, the standard Big Bang model does not explain why the universe appears to be same in all directions, why it appears flat on very large distance scales, or why certain hypothesized particles such as magnetic monopoles are not observed in experiments.

Currently, the leading candidate for a theory going beyond the Big Bang is the theory of cosmic inflation. Developed by Alan Guth and others in the 1980s, inflation postulates a period of extremely rapid accelerated expansion of the universe prior to the expansion described by the standard Big Bang theory. The theory of cosmic inflation preserves the successes of the Big Bang while providing a natural explanation for some of the mysterious features of the universe. The theory has also received striking support from observations of the cosmic microwave background, the radiation that has filled the sky since around 380,000 years after the Big Bang.

In the theory of inflation, the rapid initial expansion of the universe is caused by a hypothetical particle called the inflaton. The exact properties of this particle are not fixed by the theory but should ultimately be derived from a more fundamental theory such as string theory. Indeed, there have been a number of attempts to identify an inflaton within the spectrum of particles described by string theory, and to study inflation using string theory. While these approaches might eventually find support in observational data such as measurements of the cosmic microwave background, the application of string theory to cosmology is still in its early stages.

In addition to influencing research in theoretical physics, string theory has stimulated a number of major developments in pure mathematics. Like many developing ideas in theoretical physics, string theory does not at present have a mathematically rigorous formulation in which all of its concepts can be defined precisely. As a result, physicists who study string theory are often guided by physical intuition to conjecture relationships between the seemingly different mathematical structures that are used to formalize different parts of the theory. These conjectures are later proved by mathematicians, and in this way, string theory serves as a source of new ideas in pure mathematics.

After Calabi–Yau manifolds had entered physics as a way to compactify extra dimensions in string theory, many physicists began studying these manifolds. In the late 1980s, several physicists noticed that given such a compactification of string theory, it is not possible to reconstruct uniquely a corresponding Calabi–Yau manifold. Instead, two different versions of string theory, type IIA and type IIB, can be compactified on completely different Calabi–Yau manifolds giving rise to the same physics. In this situation, the manifolds are called mirror manifolds, and the relationship between the two physical theories is called mirror symmetry.

Regardless of whether Calabi–Yau compactifications of string theory provide a correct description of nature, the existence of the mirror duality between different string theories has significant mathematical consequences. The Calabi–Yau manifolds used in string theory are of interest in pure mathematics, and mirror symmetry allows mathematicians to solve problems in enumerative geometry, a branch of mathematics concerned with counting the numbers of solutions to geometric questions.

Enumerative geometry studies a class of geometric objects called algebraic varieties which are defined by the vanishing of polynomials. For example, the Clebsch cubic illustrated on the right is an algebraic variety defined using a certain polynomial of degree three in four variables. A celebrated result of nineteenth-century mathematicians Arthur Cayley and George Salmon states that there are exactly 27 straight lines that lie entirely on such a surface.

Generalizing this problem, one can ask how many lines can be drawn on a quintic Calabi–Yau manifold, such as the one illustrated above, which is defined by a polynomial of degree five. This problem was solved by the nineteenth-century German mathematician Hermann Schubert, who found that there are exactly 2,875 such lines. In 1986, geometer Sheldon Katz proved that the number of curves, such as circles, that are defined by polynomials of degree two and lie entirely in the quintic is 609,250.

By the year 1991, most of the classical problems of enumerative geometry had been solved and interest in enumerative geometry had begun to diminish. The field was reinvigorated in May 1991 when physicists Philip Candelas, Xenia de la Ossa, Paul Green, and Linda Parks showed that mirror symmetry could be used to translate difficult mathematical questions about one Calabi–Yau manifold into easier questions about its mirror. In particular, they used mirror symmetry to show that a six-dimensional Calabi–Yau manifold can contain exactly 317,206,375 curves of degree three. In addition to counting degree-three curves, Candelas and his collaborators obtained a number of more general results for counting rational curves which went far beyond the results obtained by mathematicians.

Originally, these results of Candelas were justified on physical grounds. However, mathematicians generally prefer rigorous proofs that do not require an appeal to physical intuition. Inspired by physicists' work on mirror symmetry, mathematicians have therefore constructed their own arguments proving the enumerative predictions of mirror symmetry. Today mirror symmetry is an active area of research in mathematics, and mathematicians are working to develop a more complete mathematical understanding of mirror symmetry based on physicists' intuition. Major approaches to mirror symmetry include the homological mirror symmetry program of Maxim Kontsevich and the SYZ conjecture of Andrew Strominger, Shing-Tung Yau, and Eric Zaslow.

Group theory is the branch of mathematics that studies the concept of symmetry. For example, one can consider a geometric shape such as an equilateral triangle. There are various operations that one can perform on this triangle without changing its shape. One can rotate it through 120°, 240°, or 360°, or one can reflect in any of the lines labeled , , or in the picture. Each of these operations is called a "symmetry", and the collection of these symmetries satisfies certain technical properties making it into what mathematicians call a group. In this particular example, the group is known as the dihedral group of order 6 because it has six elements. A general group may describe finitely many or infinitely many symmetries; if there are only finitely many symmetries, it is called a finite group.

Mathematicians often strive for a classification (or list) of all mathematical objects of a given type. It is generally believed that finite groups are too diverse to admit a useful classification. A more modest but still challenging problem is to classify all finite "simple" groups. These are finite groups which may be used as building blocks for constructing arbitrary finite groups in the same way that prime numbers can be used to construct arbitrary whole numbers by taking products. One of the major achievements of contemporary group theory is the classification of finite simple groups, a mathematical theorem which provides a list of all possible finite simple groups.

This classification theorem identifies several infinite families of groups as well as 26 additional groups which do not fit into any family. The latter groups are called the "sporadic" groups, and each one owes its existence to a remarkable combination of circumstances. The largest sporadic group, the so-called monster group, has over elements, more than a thousand times the number of atoms in the Earth.

A seemingly unrelated construction is the -function of number theory. This object belongs to a special class of functions called modular functions, whose graphs form a certain kind of repeating pattern. Although this function appears in a branch of mathematics which seems very different from the theory of finite groups, the two subjects turn out to be intimately related. In the late 1970s, mathematicians John McKay and John Thompson noticed that certain numbers arising in the analysis of the monster group (namely, the dimensions of its irreducible representations) are related to numbers that appear in a formula for the -function (namely, the coefficients of its Fourier series). This relationship was further developed by John Horton Conway and Simon Norton who called it monstrous moonshine because it seemed so far fetched.

In 1992, Richard Borcherds constructed a bridge between the theory of modular functions and finite groups and, in the process, explained the observations of McKay and Thompson. Borcherds' work used ideas from string theory in an essential way, extending earlier results of Igor Frenkel, James Lepowsky, and Arne Meurman, who had realized the monster group as the symmetries of a particular version of string theory. In 1998, Borcherds was awarded the Fields medal for his work.

Since the 1990s, the connection between string theory and moonshine has led to further results in mathematics and physics. In 2010, physicists Tohru Eguchi, Hirosi Ooguri, and Yuji Tachikawa discovered connections between a different sporadic group, the Mathieu group, and a certain version of string theory. Miranda Cheng, John Duncan, and Jeffrey A. Harvey proposed a generalization of this moonshine phenomenon called umbral moonshine, and their conjecture was proved mathematically by Duncan, Michael Griffin, and Ken Ono. Witten has also speculated that the version of string theory appearing in monstrous moonshine might be related to a certain simplified model of gravity in three spacetime dimensions.

Some of the structures reintroduced by string theory arose for the first time much earlier as part of the program of classical unification started by Albert Einstein. The first person to add a fifth dimension to a theory of gravity was Gunnar Nordström in 1914, who noted that gravity in five dimensions describes both gravity and electromagnetism in four. Nordström attempted to unify electromagnetism with his theory of gravitation, which was however superseded by Einstein's general relativity in 1919. Thereafter, German mathematician Theodor Kaluza combined the fifth dimension with general relativity, and only Kaluza is usually credited with the idea. In 1926, the Swedish physicist Oskar Klein gave a physical interpretation of the unobservable extra dimension—it is wrapped into a small circle. Einstein introduced a non-symmetric metric tensor, while much later Brans and Dicke added a scalar component to gravity. These ideas would be revived within string theory, where they are demanded by consistency conditions.

String theory was originally developed during the late 1960s and early 1970s as a never completely successful theory of hadrons, the subatomic particles like the proton and neutron that feel the strong interaction. In the 1960s, Geoffrey Chew and Steven Frautschi discovered that the mesons make families called Regge trajectories with masses related to spins in a way that was later understood by Yoichiro Nambu, Holger Bech Nielsen and Leonard Susskind to be the relationship expected from rotating strings. Chew advocated making a theory for the interactions of these trajectories that did not presume that they were composed of any fundamental particles, but would construct their interactions from self-consistency conditions on the S-matrix. The S-matrix approach was started by Werner Heisenberg in the 1940s as a way of constructing a theory that did not rely on the local notions of space and time, which Heisenberg believed break down at the nuclear scale. While the scale was off by many orders of magnitude, the approach he advocated was ideally suited for a theory of quantum gravity.

Working with experimental data, R. Dolen, D. Horn and C. Schmid developed some sum rules for hadron exchange. When a particle and antiparticle scatter, virtual particles can be exchanged in two qualitatively different ways. In the s-channel, the two particles annihilate to make temporary intermediate states that fall apart into the final state particles. In the t-channel, the particles exchange intermediate states by emission and absorption. In field theory, the two contributions add together, one giving a continuous background contribution, the other giving peaks at certain energies. In the data, it was clear that the peaks were stealing from the background—the authors interpreted this as saying that the t-channel contribution was dual to the s-channel one, meaning both described the whole amplitude and included the other.

The result was widely advertised by Murray Gell-Mann, leading Gabriele Veneziano to construct a scattering amplitude that had the property of Dolen–Horn–Schmid duality, later renamed world-sheet duality. The amplitude needed poles where the particles appear, on straight line trajectories, and there is a special mathematical function whose poles are evenly spaced on half the real line—the gamma function— which was widely used in Regge theory. By manipulating combinations of gamma functions, Veneziano was able to find a consistent scattering amplitude with poles on straight lines, with mostly positive residues, which obeyed duality and had the appropriate Regge scaling at high energy. The amplitude could fit near-beam scattering data as well as other Regge type fits, and had a suggestive integral representation that could be used for generalization.

Over the next years, hundreds of physicists worked to complete the bootstrap program for this model, with many surprises. Veneziano himself discovered that for the scattering amplitude to describe the scattering of a particle that appears in the theory, an obvious self-consistency condition, the lightest particle must be a tachyon. Miguel Virasoro and Joel Shapiro found a different amplitude now understood to be that of closed strings, while Ziro Koba and Holger Nielsen generalized Veneziano's integral representation to multiparticle scattering. Veneziano and Sergio Fubini introduced an operator formalism for computing the scattering amplitudes that was a forerunner of world-sheet conformal theory, while Virasoro understood how to remove the poles with wrong-sign residues using a constraint on the states. Claud Lovelace calculated a loop amplitude, and noted that there is an inconsistency unless the dimension of the theory is 26. Charles Thorn, Peter Goddard and Richard Brower went on to prove that there are no wrong-sign propagating states in dimensions less than or equal to 26.

In 1969–70, Yoichiro Nambu, Holger Bech Nielsen, and Leonard Susskind recognized that the theory could be given a description in space and time in terms of strings. The scattering amplitudes were derived systematically from the action principle by Peter Goddard, Jeffrey Goldstone, Claudio Rebbi, and Charles Thorn, giving a space-time picture to the vertex operators introduced by Veneziano and Fubini and a geometrical interpretation to the Virasoro conditions.

In 1971, Pierre Ramond added fermions to the model, which led him to formulate a two-dimensional supersymmetry to cancel the wrong-sign states. John Schwarz and André Neveu added another sector to the fermi theory a short time later. In the fermion theories, the critical dimension was 10. Stanley Mandelstam formulated a world sheet conformal theory for both the bose and fermi case, giving a two-dimensional field theoretic path-integral to generate the operator formalism. Michio Kaku and Keiji Kikkawa gave a different formulation of the bosonic string, as a string field theory, with infinitely many particle types and with fields taking values not on points, but on loops and curves.

In 1974, Tamiaki Yoneya discovered that all the known string theories included a massless spin-two particle that obeyed the correct Ward identities to be a graviton. John Schwarz and Joel Scherk came to the same conclusion and made the bold leap to suggest that string theory was a theory of gravity, not a theory of hadrons. They reintroduced Kaluza–Klein theory as a way of making sense of the extra dimensions. At the same time, quantum chromodynamics was recognized as the correct theory of hadrons, shifting the attention of physicists and apparently leaving the bootstrap program in the dustbin of history.

String theory eventually made it out of the dustbin, but for the following decade all work on the theory was completely ignored. Still, the theory continued to develop at a steady pace thanks to the work of a handful of devotees. Ferdinando Gliozzi, Joel Scherk, and David Olive realized in 1977 that the original Ramond and Neveu Schwarz-strings were separately inconsistent and needed to be combined. The resulting theory did not have a tachyon, and was proven to have space-time supersymmetry by John Schwarz and Michael Green in 1984. The same year, Alexander Polyakov gave the theory a modern path integral formulation, and went on to develop conformal field theory extensively. In 1979, Daniel Friedan showed that the equations of motions of string theory, which are generalizations of the Einstein equations of general relativity, emerge from the renormalization group equations for the two-dimensional field theory. Schwarz and Green discovered T-duality, and constructed two superstring theories—IIA and IIB related by T-duality, and type I theories with open strings. The consistency conditions had been so strong, that the entire theory was nearly uniquely determined, with only a few discrete choices.

In the early 1980s, Edward Witten discovered that most theories of quantum gravity could not accommodate chiral fermions like the neutrino. This led him, in collaboration with Luis Álvarez-Gaumé, to study violations of the conservation laws in gravity theories with anomalies, concluding that type I string theories were inconsistent. Green and Schwarz discovered a contribution to the anomaly that Witten and Alvarez-Gaumé had missed, which restricted the gauge group of the type I string theory to be SO(32). In coming to understand this calculation, Edward Witten became convinced that string theory was truly a consistent theory of gravity, and he became a high-profile advocate. Following Witten's lead, between 1984 and 1986, hundreds of physicists started to work in this field, and this is sometimes called the first superstring revolution.

During this period, David Gross, Jeffrey Harvey, Emil Martinec, and Ryan Rohm discovered heterotic strings. The gauge group of these closed strings was two copies of E8, and either copy could easily and naturally include the standard model. Philip Candelas, Gary Horowitz, Andrew Strominger and Edward Witten found that the Calabi–Yau manifolds are the compactifications that preserve a realistic amount of supersymmetry, while Lance Dixon and others worked out the physical properties of orbifolds, distinctive geometrical singularities allowed in string theory. Cumrun Vafa generalized T-duality from circles to arbitrary manifolds, creating the mathematical field of mirror symmetry. Daniel Friedan, Emil Martinec and Stephen Shenker further developed the covariant quantization of the superstring using conformal field theory techniques. David Gross and Vipul Periwal discovered that string perturbation theory was divergent. Stephen Shenker showed it diverged much faster than in field theory suggesting that new non-perturbative objects were missing.

In the 1990s, Joseph Polchinski discovered that the theory requires higher-dimensional objects, called D-branes and identified these with the black-hole solutions of supergravity. These were understood to be the new objects suggested by the perturbative divergences, and they opened up a new field with rich mathematical structure. It quickly became clear that D-branes and other p-branes, not just strings, formed the matter content of the string theories, and the physical interpretation of the strings and branes was revealed—they are a type of black hole. Leonard Susskind had incorporated the holographic principle of Gerardus 't Hooft into string theory, identifying the long highly excited string states with ordinary thermal black hole states. As suggested by 't Hooft, the fluctuations of the black hole horizon, the world-sheet or world-volume theory, describes not only the degrees of freedom of the black hole, but all nearby objects too.

In 1995, at the annual conference of string theorists at the University of Southern California (USC), Edward Witten gave a speech on string theory that in essence united the five string theories that existed at the time, and giving birth to a new 11-dimensional theory called M-theory. M-theory was also foreshadowed in the work of Paul Townsend at approximately the same time. The flurry of activity that began at this time is sometimes called the second superstring revolution.

During this period, Tom Banks, Willy Fischler, Stephen Shenker and Leonard Susskind formulated matrix theory, a full holographic description of M-theory using IIA D0 branes. This was the first definition of string theory that was fully non-perturbative and a concrete mathematical realization of the holographic principle. It is an example of a gauge-gravity duality and is now understood to be a special case of the AdS/CFT correspondence. Andrew Strominger and Cumrun Vafa calculated the entropy of certain configurations of D-branes and found agreement with the semi-classical answer for extreme charged black holes. Petr Hořava and Witten found the eleven-dimensional formulation of the heterotic string theories, showing that orbifolds solve the chirality problem. Witten noted that the effective description of the physics of D-branes at low energies is by a supersymmetric gauge theory, and found geometrical interpretations of mathematical structures in gauge theory that he and Nathan Seiberg had earlier discovered in terms of the location of the branes.

In 1997, Juan Maldacena noted that the low energy excitations of a theory near a black hole consist of objects close to the horizon, which for extreme charged black holes looks like an anti-de Sitter space. He noted that in this limit the gauge theory describes the string excitations near the branes. So he hypothesized that string theory on a near-horizon extreme-charged black-hole geometry, an anti-de Sitter space times a sphere with flux, is equally well described by the low-energy limiting gauge theory, the N = 4 supersymmetric Yang–Mills theory. This hypothesis, which is called the AdS/CFT correspondence, was further developed by Steven Gubser, Igor Klebanov and Alexander Polyakov, and by Edward Witten, and it is now well-accepted. It is a concrete realization of the holographic principle, which has far-reaching implications for black holes, locality and information in physics, as well as the nature of the gravitational interaction. Through this relationship, string theory has been shown to be related to gauge theories like quantum chromodynamics and this has led to more quantitative understanding of the behavior of hadrons, bringing string theory back to its roots.

To construct models of particle physics based on string theory, physicists typically begin by specifying a shape for the extra dimensions of spacetime. Each of these different shapes corresponds to a different possible universe, or "vacuum state", with a different collection of particles and forces. String theory as it is currently understood has an enormous number of vacuum states, typically estimated to be around , and these might be sufficiently diverse to accommodate almost any phenomena that might be observed at low energies.

Many critics of string theory have expressed concerns about the large number of possible universes described by string theory. In his book "Not Even Wrong", Peter Woit, a lecturer in the mathematics department at Columbia University, has argued that the large number of different physical scenarios renders string theory vacuous as a framework for constructing models of particle physics. According to Woit,

Some physicists believe this large number of solutions is actually a virtue because it may allow a natural anthropic explanation of the observed values of physical constants, in particular the small value of the cosmological constant. The anthropic principle is the idea that some of the numbers appearing in the laws of physics are not fixed by any fundamental principle but must be compatible with the evolution of intelligent life. In 1987, Steven Weinberg published an article in which he argued that the cosmological constant could not have been too large, or else galaxies and intelligent life would not have been able to develop. Weinberg suggested that there might be a huge number of possible consistent universes, each with a different value of the cosmological constant, and observations indicate a small value of the cosmological constant only because humans happen to live in a universe that has allowed intelligent life, and hence observers, to exist.

String theorist Leonard Susskind has argued that string theory provides a natural anthropic explanation of the small value of the cosmological constant. According to Susskind, the different vacuum states of string theory might be realized as different universes within a larger multiverse. The fact that the observed universe has a small cosmological constant is just a tautological consequence of the fact that a small value is required for life to exist. Many prominent theorists and critics have disagreed with Susskind's conclusions. According to Woit, "in this case [anthropic reasoning] is nothing more than an excuse for failure. Speculative scientific ideas fail not just when they make incorrect predictions, but also when they turn out to be vacuous and incapable of predicting anything."

One of the fundamental properties of Einstein's general theory of relativity is that it is background independent, meaning that the formulation of the theory does not in any way privilege a particular spacetime geometry.

One of the main criticisms of string theory from early on is that it is not manifestly background independent. In string theory, one must typically specify a fixed reference geometry for spacetime, and all other possible geometries are described as perturbations of this fixed one. In his book "The Trouble With Physics", physicist Lee Smolin of the Perimeter Institute for Theoretical Physics claims that this is the principal weakness of string theory as a theory of quantum gravity, saying that string theory has failed to incorporate this important insight from general relativity.

Others have disagreed with Smolin's characterization of string theory. In a review of Smolin's book, string theorist Joseph Polchinski writes

Polchinski notes that an important open problem in quantum gravity is to develop holographic descriptions of gravity which do not require the gravitational field 
to be asymptotically anti-de Sitter. Smolin has responded by saying that the AdS/CFT correspondence, as it is currently understood, may not be strong enough to resolve all concerns about background independence.

Since the superstring revolutions of the 1980s and 1990s, string theory has become the dominant paradigm of high energy theoretical physics. Some string theorists have expressed the view that there does not exist an equally successful alternative theory addressing the deep questions of fundamental physics. In an interview from 1987, Nobel laureate David Gross made the following controversial comments about the reasons for the popularity of string theory:

Several other high-profile theorists and commentators have expressed similar views, suggesting that there are no viable alternatives to string theory.

Many critics of string theory have commented on this state of affairs. In his book criticizing string theory, Peter Woit views the status of string theory research as unhealthy and detrimental to the future of fundamental physics. He argues that the extreme popularity of string theory among theoretical physicists is partly a consequence of the financial structure of academia and the fierce competition for scarce resources. In his book "The Road to Reality", mathematical physicist Roger Penrose expresses similar views, stating "The often frantic competitiveness that this ease of communication engenders leads to bandwagon effects, where researchers fear to be left behind if they do not join in." Penrose also claims that the technical difficulty of modern physics forces young scientists to rely on the preferences of established researchers, rather than forging new paths of their own. Lee Smolin expresses a slightly different position in his critique, claiming that string theory grew out of a tradition of particle physics which discourages speculation about the foundations of physics, while his preferred approach, loop quantum gravity, encourages more radical thinking. According to Smolin,

Smolin goes on to offer a number of prescriptions for how scientists might encourage a greater diversity of approaches to quantum gravity research.






</doc>
<doc id="28307" url="https://en.wikipedia.org/wiki?curid=28307" title="Sin">
Sin

In a religious context, sin is the act of transgression against divine law. Sin can also be viewed as any thought or action that endangers the ideal relationship between an individual and God; or as any diversion from the perceived ideal order for human living. In Jainism, sin refers to anything that harms the possibility of the "jiva" (being) to attain "moksha" (supreme emancipation). In Islamic ethics, Muslims see sin as anything that goes against the commands of Allah (God). Judaism regards the violation of any of the 613 commandments as a sin.

The word derives from "Old English "syn(n)", for original *"sunjō". The stem may be related to that of Latin '"sons", "sont-is"' guilty. In Old English there are examples of the original general sense, ‘offence, wrong-doing, misdeed'". The English Biblical terms translated as "sin" or "syn" from the Biblical Greek and Jewish terms sometimes originate from words in the latter languages denoting the act or state of missing the mark; the original sense of New Testament Greek "hamartia" "sin", is failure, being in error, missing the mark, especially in spear throwing; Hebrew "hata" "sin" originates in archery and literally refers to missing the "gold" at the centre of a target, but hitting the target, i.e. error. "To sin" has been defined from a Greek concordance as "to miss the mark".

In the Bahá'í Faith, humans are considered naturally good (perfect), fundamentally spiritual beings. Human beings were created because of God's immeasurable love. However, the Bahá'í teachings compare the human heart to a mirror, which, if turned away from the light of the sun (i.e. God), is incapable of receiving God's love.

Buddhism believes in the principle of "karma", whereby suffering is the inevitable consequence of greed, anger, and delusion (known as the Three poisons). While there is no direct Buddhist equivalent of the Abrahamic concept of sin, wrongdoing is recognized in Buddhism. The concept of Buddhist ethics is consequentialist in nature and is not based upon duty towards any deity.
Karma means action, and in Buddhist context, motivation is the most important aspect of an action. Whether karma done with mind, body and/or speech is called 'good' or 'bad', depends on whether it would bring pleasant or unpleasant results to the person who does the action. 
One needs to purify negative karma Four Satipatthanas to free oneself from obstacles to liberation from the vicious circle of rebirth. The purification reduces suffering and in the end one reaches Nirvana, the ultimate purification by realizing selflessness or emptiness. An enlightened being is free of all the suffering and karmas, and will not be automatically born again.

In the Old Testament, some sins were punishable by death in different forms, while most sins are forgiven by burnt offerings. Christians consider the Old Covenant to be fulfilled by the Gospel.

In the New Testament the forgiveness of sin is effected through faith and repentance (Mark 1:15). Sin is forgiven when the sinner acknowledges, confesses, and repents for their sin as a part of believing in Jesus Christ. The sinner is expected to confess his sins to God as a part of an ongoing relationship, which also includes giving thanks to God. The sinful person has never before been in a favorable relationship with God. When, as a part of the process of salvation, a person is forgiven, they enter into a union with God which abides forever. In the Epistle to the Romans , it is mentioned that "the wages of sin is death", which is commonly interpreted as, if one repents for his sins, such person will inherit salvation.

In Jewish Christianity, sin is believed to alienate the sinner from God even though He has extreme love for mankind. It has damaged and completely severed the relationship of humanity to God. That relationship can only be restored through acceptance of Jesus Christ and his death on the cross as a satisfactory sacrifice for the sins of humanity. Humanity was destined for life with God when Adam disobeyed God. The Bible in says "For God so loved the world, as to give his only begotten Son; that whosoever believeth in him, may not perish, but may have life everlasting."

In Eastern Christianity, sin is viewed in terms of its effects on relationships, both among people and likewise between people and God. Also as in Jewish Christianity, Sin is likewise seen as the refusal to follow God's plan and the desire to be "like God" (as stated in Genesis 3:5) and thus in direct opposition to God's will (see the account of Adam and Eve in Genesis).
Original sin is a Western concept that states that sin entered the human world through Adam and Eve's sin in the Garden of Eden and that human beings have since lived with the consequences of this first sin.

The serpent who beguiled Eve to eat of the fruit was punished by having it and its kind being made to crawl on the ground and God set an enmity between them and Eve's descendants (Genesis 3:14-15). Eve was punished by the pains of childbirth and the sorrow of bringing about life that would eventually age, sicken and die (Genesis 3:16). The second part of the curse about being subordinate to Adam originates from her creation from one of Adam's ribs to be his helper (Genesis 2:18-25); the curse now clarifies that she must now obey her husband and desire only him. Adam was punished by having to work endlessly to feed himself and his family. The land would bring forth both thistles and thorns to be cleared and herbs and grain to be planted, nurtured, and harvested. The second part of the curse about his mortality is from his origin as red clay - he is from the land and he and his descendants would return to it when buried after death. When Adam's son Cain slew his brother Abel, he introduced murder into the world (Genesis 4:8-10). For his punishment, God banished him as a fugitive, but first marked him with a sign that would protect him and his descendants from harm (Genesis 4:11-16).

One concept of sin deals with things that exist on Earth, but not in Heaven. Food, for example, while a necessary good for the (health of the temporal) body, is not of (eternal) transcendental living and therefore its "excessive" savoring is considered a sin. The unforgivable sin (or eternal sin) is a sin that can never be forgiven;
Matthew 12:30-32 :
" He that is not with me, is against me: and he that gathereth not with me, scattereth. And Therefore I say to you: Every sin and blasphemy shall be forgiven men, but the blasphemy of the Spirit shall not be forgiven. And whosoever shall speak a word against the Son of man, it shall be forgiven him: but he that shall speak against the Holy Ghost, it shall not be forgiven him, neither in this world, nor in the world to come."

In Catholic Christianity sins are classified into grave sins called mortal sins and less serious sins called venial sin. Mortal sins cause one to lose salvation unless the sinner repents and venial sins require some sort of penance either on Earth or in Purgatory.

Jesus was said to have paid for the complete mass of sins past, present, and to come in future. Even inevitable sin is said to have already been cleansed.

The Lamb of God was and is God himself and is therefore sinless. In the Old Testament, Leviticus 16:21 states that ‘the laying on of hands’ was the action that the High Priest Aaron was ordered to do yearly by God to take sins of Israel's nation onto a spotless young lamb.

In Hinduism, the term "sin" ("" in Sanskrit) is often used to describe actions that create negative Karma by violating moral and ethical codes, which automatically brings negative consequences. This is somewhat similar to Abrahamic sin in the sense that pāpa is considered a crime against the laws of God, which is known as (1) Dharma, or moral order, and (2) one's own self, but another term "aparadha" is used for grave offences. The term papa cannot be taken however, in literal sense as that of a sin. This is because there is no consensus regarding the nature of ultimate reality or God in Hinduism. Only, the vedanta school being unambiguously theistic, whereas no anthropomorphic God exists in the rest five schools namely Samkhya, Nyaya Yoga, Vaishashikha, and Purva-Mimansa . The term "papa" however in the strictest sense refers to actions which bring about wrong/unfavourable consequences, not relating to a specific divine will in the absolute sense. To conclude, considering a lack of consensus regarding the nature of ultimate reality in Hinduism, it can be considered that "papa" has lesser insistence on God for it be translated as Sin, and that there is no exact equivalent to Sin in Hinduism.

In Islamic ethics, Muslims see sin as anything that goes against the commands of Allah (God), a breach of the laws and norms laid down by religion. Islamic terms for sin include "dhanb" and "khaṭīʾa", which are synonymous and refer to intentional sins; "khiṭʾ", which means simply a sin; and "ithm", which is used for grave sins.

In Jainism, the word for sin is the Sanskrit word पाप ("paap"), which is the antithesis of पुण्य ("punya") meaning merit. A "jiva" (a being) acquires sin based on its karma, if it hurts anyone, causes someone to hurt anyone, or commends hurting anyone by thought, speech or action. Anyone, here, refers to literally any living organism, but not limited to human beings.

No "jiva" can achieve "moksha" (ultimate emancipation) without ceasing to accumulate karma and shedding off the already accumulated karma entirely. A "jiva" accumulates karma if it resorts to violence, non-chastity, falsehood, stealing, and possessiveness. A "jiva" ceases to accumulate karma if he resorts to the golden trio of "samyak gyan" (right knowledge), "samyak darshan" (right sight) and "samyak charitra" (right character). A "jiva" begins to shed off the accumulated karma by resorting to penance, repentance, vows and by exterminating foes of lust, anger, attachment, aversion, ignorance and fallacy.

If a "jiva" does not give up sin, his karma will keep accumulating and no sin can be absolved without getting its fruit or repenting for it. Thus such a "jiva" is bound to remain in the worldly cycle of constant reincarnation, wherein it will keep taking rebirths, into any of the four broad types of living organisms, depending on the magnitude and nature of karma accumulated in previous birth(s). The four types are "dev" (beings of heaven, including deities), "manushya" (human), "tiryanch" (plants, animals, insects, etc.) and "naarki" (beings of hell). 

During this cycle of getting born and dying for infinity, the "jiva" will have to then live the life of the organism he is and while living it, the "jiva" will again perform more karma. This will again lead to rebirth and again performing more karma. Thus, the cycle continues.

It is important to note that Jains believe that for complete liberation, not only the "sinful karma" but even the "meritorious karma" needs to be shed off. This means that a jiva can truly attain moksha, only if the soul is completely and absolutely pure and devoid of any accumulation. For instance, sins may cause the "jiva" to be reborn, inter alia, in hell and merits may cause it to be reborn in heaven. But heaven, like hell is a part of worldly cycle of reincarnation and not supreme moksha of the soul. Thus, if a person hypothetically keeps performing only and exclusively good deeds in his life, he may still not attain "moksha", because he has not yet shed off previously accumulated sins through repentance and knowledge.

Jains believe that only a human "jiva" has the capacity and the will to attain "moksha". Hence the "jiva" should use this extremely rare opportunity of being born as a human to walk on the path that brings him closer to "moksha". In fact, Jains take the concept of avoiding sin so seriously that not only are they completely vegetarian but some devout Jains also abstain from eating underground grown food like potatoes, onions, etc. to avoid killing small organisms. Most of the Jains are also nonalcoholics and eat before sunset each day.

Mainstream Judaism regards the violation of any of the 613 commandments of the Mosaic law for Jews, or the seven Noahide laws for Gentiles as a sin. Judaism teaches that all humans are inclined to sin from birth. Sin has many classifications and degrees. Some sins are punishable with death by the court, others with death by heaven, others with lashes, and others without such punishment, but no sins with willful intent go without consequence. Unwillful violations of the mitzvot (without negligence) do not count as sins. "Sins by error" are considered as less severe sins. When the Temple yet stood in Jerusalem, people would offer sacrifices for their misdeeds. The atoning aspect of "korbanot" is carefully circumscribed. For the most part, "korbanot" only expiate such "sins by error", that is, sins committed because a person forgot or did not know that this thing was a sin. In some circumstances, lack of knowledge is considered close to deliberate intent. No atonement is needed for violations committed under duress, and for the most part, "korbanot" cannot atone for a deliberate sin. In addition, "korbanot" have no expiating effect unless the person making the offering sincerely repents his or her actions before making the offering, and makes restitution to any person who suffered harm through the violation.

Judaism teaches that all willful sin has consequences. The completely righteous suffer for their sins (by humiliation, poverty, and suffering that God sends them) in this world and receive their reward in the world to come. The in-between (not completely righteous or completely wicked), suffer for and repent their sins after death and thereafter join the righteous. The very evil do not repent even at the gates of hell. Such people prosper in this world to receive their reward for any good deed, but cannot be cleansed by and hence cannot leave "gehinnom", because they do not or cannot repent. This world can therefore seem unjust where the righteous suffer, while the wicked prosper. Many great thinkers have contemplated this.

In Mesopotamian mythology, Adamu (or Addamu/Admu, or Adapa) goes on trial for the "sin of casting down a divinity".
His crime is breaking the wings of the south wind.

Evil deeds fall into two categories in Shinto: "amatsu tsumi", "the most pernicious crimes of all", and "kunitsu tsumi", "more commonly called misdemeanors".




</doc>
<doc id="28309" url="https://en.wikipedia.org/wiki?curid=28309" title="Sonic Youth">
Sonic Youth

Sonic Youth was an American rock band based in New York City, formed in 1981. Founding members Thurston Moore (guitar, vocals), Kim Gordon (bass, vocals, guitar) and Lee Ranaldo (guitar, vocals) remained together for the entire history of the band, while Steve Shelley (drums) followed a series of short-term drummers in 1985, and rounded out the core line-up. 

Sonic Youth emerged from the experimental no wave art and music scene in New York before evolving into a more conventional rock band and becoming the most prominent of the American noise rock groups. Sonic Youth have been praised for having "redefined what rock guitar could do" using a wide variety of unorthodox guitar tunings and preparing guitars with objects like drum sticks and screwdrivers to alter the instruments' timbre. The band is considered to be a pivotal influence on the alternative and indie rock movements.

After gaining a large underground following and critical praise through releases with SST Records in the late 1980s, the band experienced mainstream success throughout the 1990s and 2000s after signing to major label DGC in 1990 and headlining the 1995 Lollapalooza festival. In 2011, Ranaldo announced that the band was "ending for a while" following the separation of married couple Gordon and Moore. Thurston Moore updated and clarified the position in May 2014: "Sonic Youth is on hiatus. The band is a democracy of sorts, and as long as Kim and I are working out our situation, the band can't really function reasonably." Gordon refers several times in her 2015 autobiography "Girl in a Band" to the band having "split up".

Shortly after guitarist Thurston Moore moved to New York City in early 1977, he formed a group, Room Tone, with his roommates, who later changed their name to the Coachmen. After the breakup of the Coachmen, Moore began jamming with Stanton Miranda, whose band, CKM, featured Kim Gordon. Moore and Gordon formed a band, appearing under names like Male Bonding and Red Milk and the Arcadians, before settling on Sonic Youth just before June 1981. The name came from combining the nickname of MC5's Fred "Sonic" Smith with "Youth" from reggae artist Big Youth. Gordon later recalled that "as soon as Thurston came up with the name Sonic Youth, a certain sound that was more of what we wanted to do came about." The band played Noise Fest in June 1981 at New York's White Columns gallery, where Lee Ranaldo was playing as a member of Glenn Branca's electric guitar ensemble. Their performance impressed Moore, who described them as "the most ferocious guitar band that I had ever seen in my life", and he invited Ranaldo to join the band. The new threesome played three songs at the festival later in the week without a drummer. Each band member took turns playing the drums, until they met drummer Richard Edson.

Branca signed Sonic Youth as the first act on his record label Neutral Records. In December 1981 the group recorded five songs in a studio in New York's Radio City Music Hall. The material was released as the "Sonic Youth (EP)" that, while largely ignored, was sent to a few key members of the US press, who gave it uniformly favorable reviews. The album featured a relatively conventional post-punk style, in contrast to their later releases. After their first release, Edson quit the group for an acting career and was replaced by Bob Bert.

During their early days as part of the New York music scene, Sonic Youth formed a friendship with fellow New York noise rock band Swans. The bands came to share the same rehearsal space, and Sonic Youth embarked on its first tour, a two-week journey through the southern United States starting in November 1982, supporting Swans. During a second tour with Swans of the Midwest the following month, tensions ran high and Moore constantly criticized Bert's drumming, which he felt was not "in the pocket". Bert was fired afterwards and replaced by Jim Sclavunos, who played drums on the band's first studio album, 1983's "Confusion Is Sex", which featured a dramatically louder and more dissonant sound than their debut EP. Sonic Youth set up a two-week tour of Europe for the summer of 1983. Sclavunos, however, quit after only a few months. The group asked Bert to rejoin, and he agreed, on the condition that he would not be fired again after the tour's conclusion. Bert went on to play on the band's "Kill Yr Idols" EP.

Sonic Youth found themselves well received in Europe, but the New York press largely ignored the local noise rock scene. Eventually, as the press began to take notice of the genre, Sonic Youth was grouped along with bands like Big Black, the Butthole Surfers and Pussy Galore under the "pigfucker" label by "Village Voice" editor Robert Christgau. After a substandard September concert in New York, another critic from "The Village Voice" panned it. Gordon wrote a scornful letter to the newspaper, criticizing it for not supporting its local music scene, to which Christgau responded by saying they are not obligated to support them. Moore retaliated by renaming the song "Kill Yr Idols" to "I Killed Christgau With My Big Fucking Dick", before the two eventually sorted out their differences amicably.

During another tour of Europe in 1984, Sonic Youth's disastrous London debut (where the band's equipment malfunctioned and Moore consequently destroyed the equipment onstage in frustration) actually resulted in rave reviews in "Sounds" and the "NME". By the time they returned to New York, they were so popular they played shows practically every week. That same year, Moore and Gordon were married, and Sonic Youth released "Bad Moon Rising", a self-described "Americana" album that served as a reaction to the state of the nation at the time. The album, recorded by Martin Bisi, was built around transitional pieces that Moore and Ranaldo had come up with in order to take up time onstage while the other guitarist was busy tuning his instrument; as a result, there are almost no breaks between the songs on the record, which feature walls of feedback and pounding rhythms. "Bad Moon Rising" featured an appearance by Lydia Lunch on the album's single "Death Valley '69", inspired by the Charles Manson Family murders. In contrast to their abrasive, atonal material of the time, the band considered the song relatively conventional. Due to a falling-out with Branca over disputed royalty payments from their Neutral releases, they were signed to Homestead Records by Gerard Cosloy and by Blast First in the UK (which founder Paul Smith created simply so he could distribute the band's records in Europe). While even the New York press ignored "Bad Moon Rising" upon its release, now viewing the band as too arty and pretentious, Sonic Youth was becoming quite critically acclaimed in the United Kingdom, where the new album had sold 5,000 copies in just six months.

Claiming he was bored with playing "Bad Moon Rising" live in its entirety for over a year, Bert quit the group and was replaced by Steve Shelley, formerly of the punk group The Crucifucks. The band was so impressed with Shelley's drumming after seeing him play live they hired him without an audition. Bert remained on good terms with the group; he and Shelley both appeared in the music video for "Death Valley '69", as Bert performed the drums on the song, but Shelley was the group's drummer when the video was made.

Sonic Youth had a long fascination with influential indie label SST Records. Ranaldo said, "It was the first record company we were on that we really would have given anything to be on." Sonic Youth eventually signed to the label in early 1986 and began recording "EVOL" with Martin Bisi in March of that year.

"EVOL" itself represented an evolution of sorts for the band: in addition to increasingly melodic material and the impact of new drummer Shelley's playing, the record also dealt with themes of celebrity, particularly with songs like "Madonna, Sean, and Me" (also known as "Expressway to Yr. Skull" and called "a classic" by Neil Young) and "Marilyn Moore". Signing to SST catapulted the band on to a national stage, something that did not happen to their peers in the New York underground. The mainstream music press subsequently began to take notice of the band. Robert Palmer of "The New York Times" declared that Sonic Youth was "making the most startlingly original guitar-based music since Jimi Hendrix" and even "People" praised "EVOL" as the "aural equivalent of a toxic waste dump." "EVOL" is also notable for a guest appearance by bass guitarist Mike Watt, a friend whom the band coaxed to come to New York after he was deeply depressed by the death of his bandmate, D. Boon.

Around the same time, the band formed a side-project with Watt under the alias "Ciccone Youth", which is a play on the names "Sonic Youth" and "Ciccone", the original surname of pop singer Madonna. As Ciccone Youth, the band released one single and one studio album during its career before disbanding in 1988. The single, "Into the Groove(y)", consisted of three tracks: "Into the Groove(y)" (a cover of Madonna's hit "Into the Groove", incorporating snippets of her recording) and the short "Tuff Titty Rap" on the A side (both performed by the Sonic Youth members), and "Burnin' Up" (performed by Watt with additional guitars by Greg Ginn) on the B side. The studio album,"The Whitey Album", included the previously released three tracks as well as three cover songs by other artists: "Addicted to Love" by Robert Palmer (recorded in a karaoke booth), and "Burnin' Up" and "Into the Groove" by Madonna.

On 1987's "Sister", Sonic Youth continued refining their blend of pop song structures with uncompromising experimentalism. Another loose concept album, "Sister" is partly inspired by the life and works of science fiction writer Philip K. Dick (the "sister" of the title was Dick's fraternal twin, who died shortly after her birth, and whose memory haunted Dick his entire life). "Sister" sold 60,000 copies and received very positive reviews, becoming the first Sonic Youth album to crack the Top 20 of the "Village Voice"<nowiki>'</nowiki>s Pazz & Jop critics poll.

Despite the critical success, the band was becoming increasingly dissatisfied with SST due to concerns about payment and other administrative practices. Sonic Youth decided to release their next record on Enigma Records, which was distributed by Capitol Records and partly owned by EMI. The 1988 double LP "Daydream Nation" was a critical success that earned Sonic Youth substantial acclaim. The album came in second on the "Village Voice" Pazz & Jop poll and topped the year-end album lists of the "NME", "CMJ" and "Melody Maker". In 2005, it was one of 50 recordings chosen that year by the Library of Congress to be added to the National Recording Registry. The lead single from the album, "Teen Age Riot", was the first song from the band to reach significant success, receiving heavy airplay in modern and college rock stations. A number of prominent music periodicals including "Rolling Stone" hailed "Daydream Nation" as one of the best albums of the decade and named Sonic Youth as the "Hot Band" in its "Hot" issue. Unfortunately, distribution problems arose and "Daydream Nation" was often difficult to find in stores. Moore considered Enigma a "cheap-jack Mafioso outfit" and the band began looking for a major label deal.

In 1990, Sonic Youth released "Goo" (their first album for Geffen), which featured the single "Kool Thing" on which Chuck D from rap group Public Enemy guested. "Kool Thing" was later featured in the Hal Hartley film "Simple Men" and the video game "" and was made available as a paid download for the "Rock Band" video game. The record is considered much more accessible than their previous work and became the band's best-selling record to date. 

In 1992, the band released "Dirty" on the DGC label. Their influence as tastemakers continued with their discovery of acclaimed skateboard video director Spike Jonze, who they recruited for the video for "100%", which also featured skateboarder turned actor Jason Lee. This song, along with the Gordon tune "JC", contains lyrical references to the murder of Joe Cole, a friend who worked with Black Flag as a roadie. The album features artwork by Los Angeles-based artist Mike Kelley. "Dirty" features a guest appearance by Ian MacKaye (Minor Threat, Fugazi) playing guitar on the track "Youth Against Fascism".

In 1993, the band contributed the track "Burning Spear" to the AIDS benefit album "No Alternative", produced by the Red Hot Organization.

In 1994, the band released "Experimental Jet Set, Trash and No Star", their best-charting release in the United States (until 2009's "The Eternal"), which peaked at No. 34 on the "Billboard" 200. The album was filled with low-key melodies and even produced a hit single, "Bull in the Heather". Moore and Gordon's daughter, Coco Hayley Moore, was born later in the year, and many of the songs from the album were never played live because there was never a full tour to support the album due to Gordon's pregnancy. In 1994, the band also released a cover of The Carpenters' 1971 hit "Superstar" for the tribute album "If I Were a Carpenter"; their version would later be featured in the 2007 film "Juno".

The band headlined the 1995 Lollapalooza festival with alternative rock groups Hole and Pavement. By that time, alternative rock had gained considerable mainstream attention, and the festival was parodied on "The Simpsons" 1996 episode "Homerpalooza", which featured voiceovers from the band. They also performed the final credits theme for that episode.

Gordon collaborated in Free Kitten, and started a clothing label X-Girl, based in Los Angeles. Ranaldo and Moore played with many experimental/noise musicians, including William Hooker, Nels Cline, Tom Surgal, Don Dietrich, Christian Marclay, DJ Spooky and Mission of Burma, among others. Shelley started up the Smells Like Records record label, as well as playing in backing bands for Chan Marshall (Cat Power) and Two Dollar Guitar. Thurston Moore also made several guest appearances on DJ Spooky's albums, which combined rock and hip hop.

From Sonic Youth's earliest days, Gordon had occasionally played guitar with the group. Around the time of "Washing Machine" and "A Thousand Leaves", she began playing guitar more frequently, resulting in a three-guitar and drums lineup. These songs were something of a shift for the group's sound, and would lead to the introduction of a fifth member a few years later.

The "Washing Machine" album began a shift in the band away from their punk roots, seeing them working with longer jam sections. Two tracks showed the new approach in full force: the title track "Washing Machine" is just under 10 minutes long, and "The Diamond Sea" is over 19 minutes long.

During the late 1990s and early 2000s, the band began releasing a series of highly experimental records on their own Hoboken, New Jersey-based label SYR. The music was mostly instrumental and improvised, and the album and track titles and even the liner notes and credits were in different languages: "" was in French, "" in Dutch, "" in Esperanto, "SYR5" in Japanese, "" in Lithuanian, "" in Arpitan and "" in Danish. "SYR3" was the first to feature Jim O'Rourke, who went on to become an official band member. Tracks from the SYR releases featured in their live sets in 1998, particularly "Anagrama" from "SYR1", and tracks from "SYR2" formed the basis of two tracks from "A Thousand Leaves".

Released in 1998, "A Thousand Leaves" has a dreamy, semi-improvised feel, and features extended jam sections on tracks such as "Wildflower Soul" and "Female Mechanic Now on Duty". The album also features two Ranaldo-led numbers, "Hoarfrost" and "Karen Koltrane". The only single to be released from this album, "Sunday", was accompanied by a video directed by Harmony Korine and starring Macaulay Culkin.

"" was subtitled "Goodbye, 20th Century" and featured works by avant-garde classical composers such as John Cage, Yoko Ono, Steve Reich, and Christian Wolff played by Sonic Youth along with several collaborators from the modern avant-garde music scene such as Christian Marclay, William Winant, Wharton Tiers, Takehisa Kosugi and others. The album received mixed reviews, but some critics praised the group's efforts at popularizing and reinterpreting the composers' works.

On July 4, 1999, Sonic Youth's instruments, amps and gear were stolen in the middle of the night while on tour in Orange County, California (see initial post on Usenet). Forced to start from scratch with new instruments, they recorded "NYC Ghosts & Flowers" and opened for Pearl Jam during the east coast leg of their 2000 tour.

In 2001, Sonic Youth collaborated with French avant-garde singer and poet Brigitte Fontaine on Fontaine's album "Kékéland".

When the September 11, 2001 attacks occurred, several members of the band were blocks away; Jim at their NYC studio (Echo Canyon on Murray Street) and Ranaldo and his wife Leah nearby at home. After the attacks, they curated the first U.S. outing of the All Tomorrow's Parties music festival in L.A. The festival was originally scheduled for October, but it was delayed until March the following year due to the attacks.

In the summer of 2002, "Murray Street" was released; many critics heralded a "return to form for SY", seemingly revitalized by the addition of Jim O'Rourke, who became a full member during this period, playing bass guitar, guitar and occasionally synthesizer. It was during this period that the band were filmed for Scott Crary's documentary "Kill Your Idols", depicting Sonic Youth as a key influence upon the post-punk revival then happening in New York. This was followed in 2004 by the release of "Sonic Nurse", an album similar in sound and approach to its immediate predecessor that also received positive reviews. "Pattern Recognition", a song named after the 2003 William Gibson novel, finds the band once again using Gibson's work for inspiration. The band also showed their pop culture commentary and sense of humor with the track "Mariah Carey and the Arthur Doyle Hand Cream", a faster-tempo song sung by Gordon, which spoofed Carey's life, including her short-lived relationship with rapper Eminem, which originally appeared on a 2003 split 7" with Erase Errata (on the album cover, the reference to "Mariah Carey" in the title was replaced by "Kim Gordon" due to potential copyright issues.) "Sonic Nurse" had decent sales, in part due to performances on TV talk shows including "Late Night with Conan O'Brien" and "The Tonight Show with Jay Leno". The band was also slated to perform in 2004's Lollapalooza tour along with acts such as Pixies and The Flaming Lips, but the concert was canceled due to lackluster ticket sales. When the band toured later that year, they played extensively from their 1980s catalog.

On October 6, 2005, "LA CityBeat" reported that some of the gear stolen in 1999 was surprisingly recovered and that it might be used for recording of the next album, then tentatively titled "Sonic Life". The report also said that Jim O'Rourke might be leaving the band soon; his departure was confirmed by Lee Ranaldo in an interview with Pitchfork Media. In May 2006, the group announced on their website that ex-Pavement member Mark Ibold would play bass for the band on their upcoming tour.

"Rather Ripped" was released in Europe on June 5, 2006 and in the USA on June 13, 2006. Compared to previous Sonic Youth recordings, the album features many short, conventionally-structured, melodic songs and fewer feedback-fuelled left-field improvisations (the band's avant-garde tendencies nowadays have been largely exorcised through SYR releases and solo outings rather than band albums). Later that summer, Sonic Youth played the 2006 Bonnaroo Festival, as well as Lollapalooza, promoting the album. In December, "Rolling Stone" made it their number three Album of the Year 2006.

The band released "" in December 2006. It features tracks previously available only on vinyl, limited-release compilations, B-sides to international singles, and some material that had never before been released. This marked the band's final Geffen release.

In April, 2007, the band became one of the earlier big-name rock bands to play China when they were brought on a China tour by Beijing and Shanghai based company Split Works.

In 2008, the band independently re-released "Master=Dik" for the first time on CD, exclusively at their online store. They also released two more editions to the SYR series, "" and "". "SYR7" was released on April 22, and "SYR8" was released July 28. On June 10, they also released a compilation album on Starbucks Music, called "Hits Are for Squares". The first fifteen tracks were selected by other celebrities, and track sixteen, "Slow Revolution", is a new recording by Sonic Youth.

Also in June, the band was the subject of an intensively researched biography, "Goodbye 20th Century: A Biography of Sonic Youth", written by music journalist David Browne. The book featured new interviews with the band as well as nearly one-hundred friends, family members and peers. It was published by Da Capo and included over sixty rare photos.

On August 30, 2008, the band premiered two new songs at the final McCarren Park Pool show. Thurston Moore stated that in November the band would start recording a new studio album. The band did not continue their contract with Geffen, being discontented at the way Geffen handled their last four or five albums. On September 8, it was confirmed by Matador's Matablog that Sonic Youth would release its sixteenth album (titled "The Eternal") in spring, 2009, on Matador Records. In December, it was also announced that the group had recently collaborated with John Paul Jones (of Led Zeppelin fame) on a piece that served as the soundtrack for a new Merce Cunningham Dance Company piece. This work was performed by the company on April 16–19, 2009, at the Brooklyn Academy of Music in celebration of Cunningham's 90th birthday. On February 12, 2009 the band revealed the cover art for "The Eternal" via their website and blog. The album, produced by John Agnello, was released on June 9. With the release, Matador Records also offered an exclusive live LP only available to those who preordered the album. The band scored and composed the soundtrack of the French thriller-drama "Simon Werner a Disparu", which premiered in May, 2010 as part of the Cannes International Film Festival. The soundtrack has been released in 2011 as "SYR9: Simon Werner a Disparu", the latest edition of the SYR series.

On October 14, 2011, Kim Gordon and Thurston Moore announced that they separated after 27 years of marriage through a statement by Matador. Matador also explained that plans for the band remained "uncertain", despite previously hinting that they would record new material later in the year.

In an interview on November 28, 2011, Lee Ranaldo said that Sonic Youth are "ending for a while". "I'm feeling optimistic about the future no matter what happens at this point", Ranaldo said. "It was a pretty good tour overall. I mean, there was a little bit of tiptoeing around and some different situations with the travelling – you know, they're not sharing a room any more or anything like that [...] It remains to be seen at this point what happens. I think they are certainly the last shows for a while and I guess I'd just leave it at that." Ranaldo also suggested there are no plans for Sonic Youth to record new material. "There's tons and tons of archival projects and things like that still going on," he said. "I'm just happy right now to let the future take its course."

In November 2013, Ranaldo said in response to the question of a possible reunion, "I fear not. Everybody is busy with their own projects, besides that Thurston and Kim aren't getting along together very well since their split. I think you can put a cross behind Sonic Youth, same as you can put it behind the names Mike Kelley and Lou Reed. Let them all rest in peace." In her 2015 autobiography "Girl in a Band", Gordon refers several times to the band having "split up" for good.

Sonic Youth are considered a pioneering band in the noise rock and alternative rock genres. Their music has also been labelled experimental rock, indie rock and post-punk.

Sonic Youth's sound relied heavily on the use of alternative tunings. Scordatura on stringed instruments has been used for centuries and alternative guitar tunings had been used for decades in blues music, and to a limited degree in rock music (such as with Lou Reed's Ostrich guitar on "The Velvet Underground & Nico"). Azerrad writes that early in their career,

[Sonic Youth] could only afford cheap guitars, and cheap guitars "sounded" like cheap guitars. But with weird tunings or something jammed under a particular fret, those humble instruments could sound rather amazing – bang a drum stick on a cheap Japanese Stratocaster copy in the right tuning, crank the amplifier to within an inch of its life and it will sound like church bells.

The tunings were painstakingly developed by Moore and Ranaldo during the band's rehearsals; Moore once reported that the odd tunings were an attempt to introduce new sounds: "When you're playing in standard tuning all the time [...] things sound pretty standard." Rather than re-tune for every song, Sonic Youth generally used a particular guitar for one or two songs, and would take dozens of instruments on tour. This would be the source of much trouble for the band, as some songs rely on specific guitars that have been uniquely prepared.

Moore said that they were heavily influenced by the Velvet Underground. Besides The Stooges, Branca, Patti Smith, Wire, Public Image Ltd and French avant-gardist Brigitte Fontaine, another influence was 1980s-era hardcore punk; after seeing Minor Threat perform in May 1982, Moore declared them "the greatest live band I have ever seen". He also saw The Faith performing in 1981 and had a strong admiration towards their only two records, a split LP with fellow Washington, D.C. hardcore band Void and the EP "Subject to Change". While recognizing that their own music was very different from hardcore, Moore and Gordon, especially, were impressed by hardcore's speed and intensity, and by the nationwide network of musicians and fans. "It was great", said Moore, "the whole thing with slam dancing and stage diving, that was far more exciting than pogoing and spitting. [...] I thought hardcore was very musical and very radical."

Thurston Moore and Lee Ranaldo expressed on numerous occasions their admiration for the music of Joni Mitchell, such as this quote by Thurston Moore: "Joni Mitchell! I've used elements of her songwriting and guitar playing, and no one would ever know about it." Additionally, as with Sonic Youth, Joni Mitchell has always used a number of alternative tunings. The band named a song after her, "Hey Joni".

Members of the band have also maintained relationships with other avant-garde artists from other genres and even other media, drawing influence from the work of John Cage and Henry Cowell. For a 1988 John Peel Session, Sonic Youth covered three songs by The Fall and "Victoria" by The Kinks, also covered by The Fall. Sonic Youth has featured album art by several well-known avant-garde visual artists, such as Mike Kelley, Tony Oursler and Gerhard Richter, whose paintings from his "Candles" series was used as artwork on "Daydream Nation".

Sonic Youth's sound was generated by their vast collection of unique and exclusive instruments; from guitars altered to meet the needs of the unique tunings employed to effects and amps designed to around their whims, Sonic Youth used a wide array of custom instruments in creating their sound.

Final lineup

Former members

Studio albums




</doc>
<doc id="28313" url="https://en.wikipedia.org/wiki?curid=28313" title="SCSI">
SCSI

Small Computer System Interface (SCSI, ) is a set of standards for physically connecting and transferring data between computers and peripheral devices. The SCSI standards define commands, protocols, electrical, optical and logical interfaces. SCSI is most commonly used for hard disk drives and tape drives, but it can connect a wide range of other devices, including scanners and CD drives, although not all controllers can handle all devices. The SCSI standard defines command sets for specific peripheral device types; the presence of "unknown" as one of these types means that in theory it can be used as an interface to almost any device, but the standard is highly pragmatic and addressed toward commercial requirements.

The ancestral SCSI standard, X3.131-1986, generally referred to as SCSI-1, was published by the X3T9 technical committee of the American National Standards Institute (ANSI) in 1986. SCSI-2 was published in August 1990 as X3.T9.2/86-109, with further revisions in 1994 and subsequent adoption of a multitude of interfaces. Further refinements have resulted in improvements in performance and support for ever-increasing storage data capacity.

SCSI is derived from "SASI", the "Shugart Associates System Interface", developed circa 1978 and publicly disclosed in 1981. Larry Boucher is considered to be the "father" of SASI and ultimately SCSI due to his pioneering work first at Shugart Associates and then at Adaptec.

A SASI controller provided a bridge between a hard disk drive's low-level interface and a host computer, which needed to read blocks of data. SASI controller boards were typically the size of a hard disk drive and were usually physically mounted to the drive's chassis. SASI, which was used in mini- and early microcomputers, defined the interface as using a 50-pin flat ribbon connector which was adopted as the SCSI-1 connector. SASI is a fully compliant subset of SCSI-1 so that many, if not all, of the then-existing SASI controllers were SCSI-1 compatible.

Until at least February 1982, ANSI developed the specification as "SASI" and "Shugart Associates System Interface;" however, the committee documenting the standard would not allow it to be named after a company. Almost a full day was devoted to agreeing to name the standard "Small Computer System Interface", which Boucher intended to be pronounced "sexy", but ENDL's Dal Allan pronounced the new acronym as "scuzzy" and that stuck.

A number of companies such as NCR Corporation, Adaptec and Optimem were early supporters of SCSI. The NCR facility in Wichita, Kansas is widely thought to have developed the industry's first SCSI controller chip; it worked the first time.

The "small" reference in "small computer system interface" is historical; since the mid-1990s, SCSI has been available on even the largest of computer systems.

Since its standardization in 1986, SCSI has been commonly used in the Amiga, Atari, Apple Macintosh and Sun Microsystems (now part of Oracle Corporation) computer lines and PC server systems. Apple started using the less-expensive parallel ATA (PATA, also known as "IDE") for its low-end machines with the Macintosh Quadra 630 in 1994, and added it to its high-end desktops starting with the Power Macintosh G3 in 1997. Apple dropped on-board SCSI completely in favor of IDE and FireWire with the (Blue & White) Power Mac G3 in 1999, while still offering a PCI SCSI host adapter as an option on up to the Power Macintosh G4 (AGP Graphics) models. Sun switched its lower-end range to Serial ATA (SATA). Commodore included SCSI on the Amiga 3000/3000T systems and it was an add-on to previous Amiga 500/2000 models. Starting with the Amiga 600/1200/4000 systems Commodore switched to the IDE interface. Atari included SCSI as standard in its Atari MEGA STE, Atari TT and Atari Falcon computer models. SCSI has never been popular in the low-priced IBM PC world, owing to the lower cost and adequate performance of ATA hard disk standard. However, SCSI drives and even SCSI RAIDs became common in PC workstations for video or audio production.

Recent physical versions of SCSISerial Attached SCSI (SAS), SCSI-over-Fibre Channel Protocol (FCP), and USB Attached SCSI (UAS)break from the traditional parallel SCSI bus and perform data transfer via serial communications using point-to-point links. Although much of the SCSI documentation talks about the parallel interface, all modern development efforts use serial interfaces. Serial interfaces have a number of advantages over parallel SCSI, including higher data rates, simplified cabling, longer reach, improved fault isolation and full-duplex capability. The primary reason for the shift to serial interfaces is the clock skew issue of high speed parallel interfaces, which makes the faster variants of parallel SCSI susceptible to problems caused by cabling and termination.

The non-physical iSCSI preserves the basic SCSI paradigm, especially the command set, almost unchanged, through embedding of SCSI-3 over TCP/IP. Therefore, iSCSI uses "logical connections" instead of physical links and can run on top of any network supporting IP. The actual physical links are realized on lower network layers, independently from iSCSI. Predominantly, Ethernet is used which is also of serial nature.

SCSI is popular on high-performance workstations, servers, and storage appliances. Almost all RAID subsystems on servers have used some kind of SCSI hard disk drives for decades (initially Parallel SCSI, recently SAS and Fibre Channel), though a number of manufacturers offer SATA-based RAID subsystems as a cheaper option. Moreover, SAS offers compatibility with SATA devices, creating a much broader range of options for RAID subsystems together with the existence of nearline SAS (NL-SAS) drives. Instead of SCSI, modern desktop computers and notebooks typically use SATA interfaces for internal hard disk drives, with M.2 and PCIe gaining popularity as SATA can bottleneck modern solid-state drives.

SCSI is available in a variety of interfaces. The first was parallel SCSI (also called SCSI Parallel Interface or SPI), which uses a parallel bus design. Since 2005, SPI was gradually replaced by Serial Attached SCSI (SAS), which uses a serial design but retains other aspects of the technology. Many other interfaces which do not rely on complete SCSI standards still implement the SCSI command protocol; others drop physical implementation entirely while retaining the SCSI architectural model. iSCSI, for example, uses TCP/IP as a transport mechanism, which is most often transported over Gigabit Ethernet or faster network links.

SCSI interfaces have often been included on computers from various manufacturers for use under Microsoft Windows, classic Mac OS, Unix, Commodore Amiga and Linux operating systems, either implemented on the motherboard or by the means of plug-in adaptors. With the advent of SAS and SATA drives, provision for parallel SCSI on motherboards was discontinued.

Initially, the "SCSI Parallel Interface" (SPI) was the only interface using the SCSI protocol. Its standardization started as a single-ended 8-bit bus in 1986, transferring up to 5 MB/s, and evolved into a low-voltage differential 16-bit bus capable of up to 320 MB/s. The last SPI-5 standard from 2003 also defined a 640 MB/s speed which failed to be realized.

Parallel SCSI specifications include several synchronous transfer modes for the parallel cable, and an asynchronous mode. The asynchronous mode is a classic request/acknowledge protocol, which allows systems with a slow bus or simple systems to also use SCSI devices. Faster synchronous modes are used more frequently.

Internal parallel SCSI cables are usually ribbons, with two or more 50–, 68–, or 80–pin connectors attached. External cables are typically shielded (but may not be), with 50– or 68–pin connectors at each end, depending upon the specific SCSI bus width supported. The 80–pin Single Connector Attachment (SCA) is typically used for hot-pluggable devices

Fibre Channel can be used to transport SCSI information units, as defined by the Fibre Channel Protocol for SCSI (FCP). These connections are hot-pluggable and are usually implemented with optical fiber.

Serial attached SCSI (SAS) uses a modified Serial ATA data and power cable.

iSCSI (Internet Small Computer System Interface) usually uses Ethernet connectors and cables as its physical transport, but can run over any physical transport capable of transporting IP.

The SCSI RDMA Protocol (SRP) is a protocol that specifies how to transport SCSI commands over a reliable RDMA connection. This protocol can run over any RDMA-capable physical transport, e.g. InfiniBand or Ethernet when using RoCE or iWARP.

USB Attached SCSI allows SCSI devices to use the Universal Serial Bus.

The Automation/Drive Interface − Transport Protocol (ADT) is used to connect removable media devices, such as tape drives, with the controllers of the libraries (automation devices) in which they are installed. The ADI standard specifies the use of RS-422 for the physical connections. The second-generation ADT-2 standard defines iADT, use of the ADT protocol over IP (Internet Protocol) connections, such as over Ethernet. The Automation/Drive Interface − Commands standards (ADC, ADC-2, and ADC-3) define SCSI commands for these installations.

In addition to many different hardware implementations, the SCSI standards also include an extensive set of command definitions. The SCSI command architecture was originally defined for parallel SCSI buses but has been carried forward with minimal change for use with iSCSI and serial SCSI. Other technologies which use the SCSI command set include the ATA Packet Interface, USB Mass Storage class and FireWire SBP-2.

In SCSI terminology, communication takes place between an initiator and a target. The initiator sends a command to the target, which then responds. SCSI commands are sent in a Command Descriptor Block (CDB). The CDB consists of a one byte operation code followed by five or more bytes containing command-specific parameters.

At the end of the command sequence, the target returns a status code byte, such as 00h for success, 02h for an error (called a Check Condition), or 08h for busy. When the target returns a Check Condition in response to a command, the initiator usually then issues a SCSI Request Sense command in order to obtain a key code qualifier (KCQ) from the target. The Check Condition and Request Sense sequence involves a special SCSI protocol called a Contingent Allegiance Condition.

There are four categories of SCSI commands: N (non-data), W (writing data from initiator to target), R (reading data), and B (bidirectional). There are about 60 different SCSI commands in total, with the most commonly used being:


Each device on the SCSI bus is assigned a unique SCSI identification number or ID. Devices may encompass multiple logical units, which are addressed by logical unit number (LUN). Simple devices have just one LUN, more complex devices may have multiple LUNs.

A "direct access" (i.e. disk type) storage device consists of a number of logical blocks, addressed by Logical Block Address (LBA). A typical LBA equates to 512 bytes of storage. The usage of LBAs has evolved over time and so four different command variants are provided for reading and writing data. The Read(6) and Write(6) commands contain a 21-bit LBA address. The Read(10), Read(12), Read Long, Write(10), Write(12), and Write Long commands all contain a 32-bit LBA address plus various other parameter options.

The capacity of a "sequential access" (i.e. tape-type) device is not specified because it depends, amongst other things, on the length of the tape, which is not identified in a machine-readable way. Read and write operations on a sequential access device begin at the current tape position, not at a specific LBA. The block size on sequential access devices can either be fixed or variable, depending on the specific device. Tape devices such as half-inch 9-track tape, DDS (4 mm tapes physically similar to DAT), Exabyte, etc., support variable block sizes.

On a parallel SCSI bus, a device (e.g. host adapter, disk drive) is identified by a "SCSI ID", which is a number in the range 0–7 on a narrow bus and in the range 0–15 on a wide bus. On earlier models a physical jumper or switch controls the SCSI ID of the initiator (host adapter). On modern host adapters (since about 1997), doing I/O to the adapter sets the SCSI ID; for example, the adapter often contains a BIOS program that runs when the computer boots up and that program has menus that let the operator choose the SCSI ID of the host adapter. Alternatively, the host adapter may come with software that must be installed on the host computer to configure the SCSI ID. The traditional SCSI ID for a host adapter is 7, as that ID has the highest priority during bus arbitration (even on a 16 bit bus).

The SCSI ID of a device in a drive enclosure that has a back plane is set either by jumpers or by the slot in the enclosure the device is installed into, depending on the model of the enclosure. In the latter case, each slot on the enclosure's back plane delivers control signals to the drive to select a unique SCSI ID. A SCSI enclosure without a back plane often has a switch for each drive to choose the drive's SCSI ID. The enclosure is packaged with connectors that must be plugged into the drive where the jumpers are typically located; the switch emulates the necessary jumpers. While there is no standard that makes this work, drive designers typically set up their jumper headers in a consistent format that matches the way that these switches implement.

Setting the bootable (or first) hard disk to SCSI ID 0 is an accepted IT community recommendation. SCSI ID 2 is usually set aside for the floppy disk drive while SCSI ID 3 is typically for a CD-ROM drive.

Note that a SCSI target device (which can be called a "physical unit") is often divided into smaller "logical units". For example, a high-end disk subsystem may be a single SCSI device but contain dozens of individual disk drives, each of which is a logical unit. Further, a RAID array may be a single SCSI device, but may contain many logical units, each of which is a "virtual" disk—a stripe set or mirror set constructed from portions of real disk drives. The SCSI ID, WWN, etc. in this case identifies the whole subsystem, and a second number, the logical unit number (LUN) identifies a disk device (real or virtual) within the subsystem.

It is quite common, though incorrect, to refer to the logical unit itself as a "LUN". Accordingly, the actual LUN may be called a "LUN number" or "LUN id".

In modern SCSI transport protocols, there is an automated process for the "discovery" of the IDs. The SSA initiator (normally the host computer through the 'host adaptor') "walk the loop" to determine what devices are connected and then assigns each one a 7-bit "hop-count" value. Fibre Channel – Arbitrated Loop (FC-AL) initiators use the LIP (Loop Initialization Protocol) to interrogate each device port for its WWN (World Wide Name). For iSCSI, because of the unlimited scope of the (IP) network, the process is quite complicated. These discovery processes occur at power-on/initialization time and also if the bus topology changes later, for example if an extra device is added.

While all SCSI controllers can work with read/write storage devices, i.e. disk and tape, some will not work with some other device types; older controllers are likely to be more limited, sometimes by their driver software, and more Device Types were added as SCSI evolved. Even CD-ROMs are not handled by all controllers. Device Type is a 5-bit field reported by a SCSI Inquiry Command; defined SCSI Peripheral Device Types include, in addition to many varieties of storage device, printer, scanner, communications device, and a catch-all "processor" type for devices not otherwise listed.

In larger SCSI servers, the disk-drive devices are housed in an intelligent enclosure that supports SCSI Enclosure Services (SES). The initiator can communicate with the enclosure using a specialized set of SCSI commands to access power, cooling, and other non-data characteristics.




</doc>
<doc id="28314" url="https://en.wikipedia.org/wiki?curid=28314" title="Super Nintendo Entertainment System">
Super Nintendo Entertainment System

The Super Nintendo Entertainment System (officially abbreviated the Super NES or SNES, and colloquially shortened to Super Nintendo) is a 16-bit home video game console developed by Nintendo that was released in 1990 in Japan and South Korea, 1991 in North America, 1992 in Europe and Australasia (Oceania), and 1993 in South America. In Japan, the system is called the Super Famicom, or SFC for short. In South Korea, it is known as the Super Comboy and was distributed by Hyundai Electronics. The system was released in Brazil on August 30, 1993, by Playtronic. Although each version is essentially the same, several forms of regional lockout prevent the different versions from being compatible with one another.

The SNES is Nintendo's second programmable home console, following the Nintendo Entertainment System (NES). The console introduced advanced graphics and sound capabilities compared with other systems at the time. The development of a variety of enhancement chips integrated in game cartridges helped to keep it competitive in the marketplace.

The SNES was a global success, becoming the best-selling console of the 16-bit era despite its relatively late start and the intense competition it faced in North America and Europe from Sega's Genesis console. The SNES remained popular well into the 32-bit era having sold 49.1 million worldwide (Comparative to the NES's 61.9 million). It continues to be popular among collectors and retro gamers, some of whom still make homebrew ROM images, in addition to its popularity in Nintendo's emulated rereleases, such as in WiiWare environment.

To compete with the popular Family Computer in Japan, NEC Corporation launched the PC Engine in 1987, and Sega followed suit with the Mega Drive in 1988. The two platforms were later launched in North America in 1989 as the TurboGrafx-16 and the Sega Genesis, respectively. Both systems were built on 16-bit architectures and offered improved graphics and sound over the 8-bit NES. However, it took several years for Sega's system to become successful. Nintendo executives were in no rush to design a new system, but they reconsidered when they began to see their dominance in the market slipping.

Designed by Masayuki Uemura, the designer of the original Famicom, the Super Famicom was released in Japan on Wednesday, November 21, 1990 for 25,000 yen ($210). It was an instant success; Nintendo's initial shipment of 300,000 units sold out within hours, and the resulting social disturbance led the Japanese government to ask video game manufacturers to schedule future console releases on weekends. The system's release also gained the attention of the Yakuza, leading to a decision to ship the devices at night to avoid robbery.

With the Super Famicom quickly outselling its chief rivals, Nintendo reasserted itself as the leader of the Japanese console market. Nintendo's success was partially due to the retention of most of its key third-party developers from its earlier system, including Capcom, Konami, Tecmo, Square, Koei, and Enix.

Nintendo released the Super Nintendo Entertainment System, a redesigned version of the Super Famicom, in North America for $199. It began shipping in limited quantities on August 23, 1991, with an official nationwide release date of September 9, 1991. The SNES was released in the United Kingdom and Ireland in April 1992 for £150, with a German release following a few weeks later.

Most of the PAL region versions of the console use the Japanese Super Famicom design, except for labeling and the length of the joypad leads. The Playtronic Super NES in Brazil, although PAL-M, uses the North American design. Both the NES and SNES were released in Brazil in 1993 by Playtronic, a joint venture between the toy company Estrela and consumer electronics company Gradiente.

The SNES and Super Famicom launched with few games, but these games were well received in the marketplace. In Japan, only two games were initially available: "Super Mario World" and "F-Zero". (A third game, "Bombuzal", was released during the launch week.) In North America, "Super Mario World" launched as a bundle with the console; other launch games included "F-Zero", "Pilotwings" (both of which demonstrated the console's "Mode 7" pseudo-3D rendering capability), "SimCity", and "Gradius III".

The rivalry between Nintendo and Sega resulted in what has been described as one of the most notable console wars in video game history, in which Sega positioned the Genesis as the "cool" console, with games aimed at older audiences, and edgy advertisements that occasionally attacked the competition. Nintendo however, scored an early public relations advantage by securing the first console conversion of Capcom's arcade classic "Street Fighter II" for SNES, which took over a year to make the transition to the Genesis. Despite the Genesis's head start, much larger library of games, and lower price point, the Genesis only represented an estimated 60% of the American 16-bit console market in June 1992, and neither console could maintain a definitive lead for several years. "Donkey Kong Country" is said to have helped establish the SNES's market prominence in the latter years of the 16-bit generation, and for a time, maintain against the PlayStation and Saturn. According to Nintendo, the company had sold more than 20 million SNES units in the U.S. According to a 2014 Wedbush Securities report based on NPD sales data, the SNES ultimately outsold the Genesis in the U.S. market.

During the NES era, Nintendo maintained exclusive control over games released for the system—the company had to approve every game, each third-party developer could only release up to five games per year (but some third parties got around this by using different names, for example Konami's "Ultra Games" brand), those games could not be released on another console within two years, and Nintendo was the exclusive manufacturer and supplier of NES cartridges. However, competition from Sega's console brought an end to this practice; in 1991, Acclaim began releasing games for both platforms, with most of Nintendo's other licensees following suit over the next several years; Capcom (which licensed some games to Sega instead of producing them directly) and Square were the most notable holdouts.

Nintendo continued to carefully review submitted games, scoring them on a 40-point scale and allocating marketing resources accordingly. Each region performed separate evaluations. Nintendo of America also maintained a policy that, among other things, limited the amount of violence in the games on its systems. One game, "Mortal Kombat", challenged this policy. A surprise hit in arcades in 1992, "Mortal Kombat" features splashes of blood and finishing moves that often depict one character dismembering the other. Because the Genesis version retained the gore while the SNES version did not, it outsold the SNES version by a ratio of three or four-to-one.

Game players were not the only ones to notice the violence in this game; U.S. Senators Herb Kohl and Joe Lieberman convened a Congressional hearing on December 9, 1993, to investigate the marketing of violent video games to children. While Nintendo took the high ground with moderate success, the hearings led to the creation of the Interactive Digital Software Association and the Entertainment Software Rating Board, and the inclusion of ratings on all video games. With these ratings in place, Nintendo decided its censorship policies were no longer needed.

While other companies were moving on to 32-bit systems, Rare and Nintendo proved that the SNES was still a strong contender in the market. In November 1994, Rare released "Donkey Kong Country", a platform game featuring 3D models and textures pre-rendered on SGI workstations. With its detailed graphics, fluid animation and high-quality music, "Donkey Kong Country" rivaled the aesthetic quality of games that were being released on newer 32-bit CD-based consoles. In the last 45 days of 1994, the game sold 6.1 million units, making it the fastest-selling video game in history to that date. This game sent a message that early 32-bit systems had little to offer over the SNES, and helped make way for the more advanced consoles on the horizon. According to TRSTS reports, two of the top five best-selling games in the U.S. for December 1996 were Super NES games.

In October 1997, Nintendo released a redesigned model of the SNES (the SNS-101 model referred to as "New-Style Super NES") in North America for US$99, with some units including the pack-in game "". Like the earlier redesign of the NES (model NES-101), the new model was slimmer and lighter than its predecessor, but it lacked S-Video and RGB output, and it was among the last major SNES-related releases in the region. A similarly redesigned Super Famicom Jr. was released in Japan at around the same time.

Nintendo ceased production of the SNES in 1999, about two years after releasing "Kirby's Dream Land 3" (its last first-party game for the system) on November 27, 1997, a year after releasing "Frogger" (its last third-party game for the system). In Japan, Nintendo continued production of the Super Famicom until September 25, 2003, and new games were produced until the year 2000, ending with the release of "Metal Slader Glory Director's Cut" on November 29, 2000.

Many popular SNES games have since been ported to the Game Boy Advance, which has similar video capabilities. In 2005, Nintendo announced that SNES games would be made available for download via the Wii and Wii U's Virtual Console service. On October 31, 2007, Nintendo Co., Ltd. announced that it would no longer repair Family Computer or Super Famicom systems due to an increasing shortage of the necessary parts.

The 16-bit design of the SNES incorporates graphics and sound co-processors that perform tiling and simulated 3D effects, a palette of 32,768 colors, and 8-channel ADPCM audio. These base platform features, plus the ability to dramatically extend them all through substantial chip upgrades inside of each cartridge, represent a leap over the 8-bit NES generation and some supposed significant advantages over 16-bit competitors such as the Genesis.

Nintendo employed several types of regional lockout, including both physical and hardware incompatibilities.
On a physical level, the cartridges are shaped differently for different regions. North American cartridges have a rectangular bottom with inset grooves matching protruding tabs in the console, while other regions' cartridges are narrower with a smooth curve on the front and no grooves. The physical incompatibility can be overcome with use of various adapters, or through modification of the console.

Internally, a regional lockout chip (CIC) within the console and in each cartridge prevents PAL region games from being played on Japanese or North American consoles and vice versa. The Japanese and North American machines have the same region chip. This can be overcome through the use of adapters, typically by inserting the imported cartridge in one slot and a cartridge with the correct region chip in a second slot. Alternatively, disconnecting one pin of the console's lockout chip will prevent it from locking the console; hardware in later games can detect this situation, so it later became common to install a switch to reconnect the lockout chip as needed.

PAL consoles face another incompatibility when playing out-of-region cartridges: the NTSC video standard specifies video at 60 Hz while PAL operates at 50 Hz, resulting in approximately 16.7% slower framerate. Additionally, PAL's higher resolution results in letterboxing of the output image. Some commercial PAL region releases exhibit this same problem and, therefore, can be played in NTSC systems without issue while others will face a 20% speedup if played in an NTSC console. To mostly correct this issue, a switch can be added to place the SNES PPU into a 60 Hz mode supported by most newer PAL televisions. Later games will detect this setting and refuse to run, requiring the switch to be thrown only after the check completes.

All versions of the SNES are predominantly gray, of slightly different shades. The original North American version, designed by Nintendo of America industrial designer Lance Barr (who previously redesigned the Famicom to become the NES), has a boxy design with purple sliding switches and a dark gray eject lever. The loading bay surface is curved, both to invite interaction and to prevent food or drinks from being placed on the console and spilling as had happened with the flat surfaced NES. The Japanese and European versions are more rounded, with darker gray accents and buttons. The North American SNS-101 model and the Japanese Super Famicom Jr. (model SHVC-101), all designed by Barr, are both smaller with a rounded contour; however, the SNS-101 buttons are purple where the Super Famicom Jr. buttons are gray. The European and American versions of the SNES controllers have much longer cables compared to the Japanese Super Famicom controllers.

All versions incorporate a top-loading slot for game cartridges, although the shape of the slot differs between regions to match the different shapes of the cartridges. The MULTI OUT connector (later used on the Nintendo 64 and GameCube) can output composite video, S-Video and RGB signals, as well as RF with an external RF modulator. Original versions additionally include a 28-pin expansion port under a small cover on the bottom of the unit and a standard RF output with channel selection switch on the back; the redesigned models output composite video only, requiring an external modulator for RF.
The ABS plastic used in the casing of some older SNES and Super Famicom consoles is particularly susceptible to oxidization with exposure to air, likely due to an incorrect mixture of the stabilizing or flame retarding additives. This, along with the particularly light color of the original plastic, causes affected consoles to quickly become yellow; if the sections of the casing came from different batches of plastic, a "two-tone" effect results.

The cartridge media of the console is officially referred to as Game Pak in most Western regions, and as in Japan and parts of Latin America. While the SNES can address 128 Mbit, only 117.75 Mbit are actually available for cartridge use. A fairly normal mapping could easily address up to 95 Mbit of ROM data (48 Mbit at FastROM speed) with 8 Mbit of battery-backed RAM. However, most available memory access controllers only support mappings of up to 32 Mbit. The largest games released ("Tales of Phantasia" and "Star Ocean") contain 48 Mbit of ROM data, while the smallest games contain only 2 Mbit.

Cartridges may also contain battery-backed SRAM to save the game state, extra working RAM, custom coprocessors, or any other hardware that will not exceed the maximum current rating of the console.

The standard SNES controller adds X and Y face buttons to the design of the NES iteration, arranging the four in a diamond shape, and adds two shoulder buttons. It features an ergonomic design by Lance Barr, later used for the NES-102 model controllers, also designed by Barr. The Japanese and PAL region versions incorporate the colors of the four action buttons into the system's logo. The North American version's buttons are colored to match the redesigned console; the X and Y buttons are lavender with concave faces, and the A and B buttons are purple with convex faces. Several later consoles derive elements of their controller design from the SNES, including the PlayStation, Dreamcast, Xbox, and Wii Classic Controller.

Throughout the course of its life, a number of peripherals were released which added to the functionality of the SNES. Many of these devices were modeled after earlier add-ons for the NES: the Super Scope is a light gun functionally similar to the NES Zapper (though the Super Scope features wireless capabilities) and the Super Advantage is an arcade-style joystick with adjustable turbo settings akin to the NES Advantage. Nintendo also released the SNES Mouse in conjunction with "Mario Paint". Hudson Soft, under license from Nintendo, released the Super Multitap, a multiplayer adapter for use with its popular series of "Bomberman" games. Some of the more unusual controllers include the BatterUP baseball bat, the Life Fitness Entertainment System (an exercise bike controller with built-in monitoring software), and the TeeV Golf golf club.

While Nintendo never released an adapter for playing NES games on the SNES, the Super Game Boy adapter cartridge allows games designed for Nintendo's portable Game Boy system to be played on the SNES. The Super Game Boy touts several feature enhancements over the Game Boy, including palette substitution, custom screen borders, and access to the SNES console's features by specially enhanced Game Boy games. Japan also saw the release of the Super Game Boy 2, which adds a communication port to enable a second Game Boy to connect for multiplayer games.

Like the NES before it, the SNES has unlicensed third-party peripherals, including a new version of the Game Genie cheat cartridge designed for use with SNES games.

Soon after the release of the SNES, companies began marketing backup devices such as the Super Wildcard, Super Pro Fighter Q, and Game Doctor. These devices create a backup of a cartridge. They can also be used to play illicit ROM images or to create copies of rented video games, violating copyright laws in many jurisdictions.
Japan saw the release of the Satellaview, a modem which attaches to the Super Famicom's expansion port and connected to the St.GIGA satellite radio station from April 23, 1995 to June 30, 2000. Satellaview users could download gaming news and specially designed games, which were frequently either remakes of or sequels to older Famicom games, and released in installments. In the United States, the relatively short-lived XBAND allowed users to connect to a network via a dial-up modem to compete against other players around the country.

During the SNES's life, Nintendo contracted with two different companies to develop a CD-ROM-based peripheral for the console to compete with Sega's CD-ROM based add-on, Sega CD. Although a SNES-CD prototype console was produced by Sony, Nintendo's deals with both Sony and Philips were canceled, with Philips gaining the right to release a series of games based on Nintendo franchises for its CD-i multimedia console and Sony going on to develop its own PlayStation console based on its initial dealings with Nintendo.

As part of the overall plan for the SNES, rather than include an expensive CPU that would still become obsolete in a few years, the hardware designers made it easy to interface special coprocessor chips to the console, just like the MMC chips used for most NES games. This is most often characterized by 16 additional pins on the cartridge card edge.

The Super FX is a RISC CPU designed to perform functions that the main CPU can not feasibly do. The chip is primarily used to create 3D game worlds made with polygons, texture mapping and light source shading. The chip can also be used to enhance 2D games.

The Nintendo fixed-point digital signal processor (DSP) chip allowed for fast vector-based calculations, bitmap conversions, both 2D and 3D coordinate transformations, and other functions. Four revisions of the chip exist, each physically identical but with different microcode. The DSP-1 version, including the later 1A and 1B bug fix revisions, is used most often; the DSP-2, DSP-3, and DSP-4 are used in only one game each.

Similar to the 5A22 CPU in the console, the SA-1 chip contains a 65c816 processor core clocked at 10 MHz, a memory mapper, DMA, decompression and bitplane conversion circuitry, several programmable timers, and CIC region lockout functionality.

In Japan, games could be downloaded cheaper than standard cartridges, from Nintendo Power kiosks onto special cartridges containing flash memory and a MegaChips MX15001TFC chip. The chip managed communication with the kiosks to download ROM images, and provided an initial menu to select which of the downloaded games would be played. Some were available both in cartridge and download form, while others were download only. The service closed on February 8, 2007.

Many cartridges contain other enhancement chips, most of which were created for use by a single company in a few games; the only limitations are the speed of the SNES itself to transfer data from the chip and the current limit of the console.

Like the NES before it, the SNES has retained a long-lived fan base. It has continued to thrive on the second-hand market, emulators, and remakes. The SNES has taken the same revival path as the NES.

Emulation projects began with the initial release of VSMC in 1994, and Super Pasofami became the first working SNES emulator in 1996. During that time, two competing emulation projects—Snes96 and Snes97—merged to form Snes9x. In 1997, SNES enthusiasts began programming an emulator named ZSNES. In 2004, higan began development as bsnes, in an effort to emulate the system as closely as possible.

Nintendo of America took the same stance against the distribution of SNES ROM image files and the use of emulators as it did with the NES, insisting that they represented flagrant software piracy. Proponents of SNES emulation cite discontinued production of the SNES constituting abandonware status, the right of the owner of the respective game to make a personal backup via devices such as the Retrode, space shifting for private use, the desire to develop homebrew games for the system, the frailty of SNES ROM cartridges and consoles, and the lack of certain foreign imports.

Emulation of the Super NES is also available on platforms such as Android, and iOS, the Nintendo DS line, the Gizmondo, the Dingoo and the GP2X by GamePark Holdings, as well as PDAs. While individual games have been included with emulators on some GameCube discs, Nintendo's Virtual Console service for the Wii marks the introduction of officially sanctioned general SNES emulation.

A dedicated mini-console, the Super NES Classic Edition, was released in September 2017 after the NES Classic Edition. The emulation-based system, which is physically modeled after the North American and European versions of the SNES in their respective regions, is bundled with two SNES-style controllers and comes preloaded with 21 games, including the previously unreleased "Star Fox 2".

Approximately 49.1 million Super Nintendo Systems were sold worldwide, with 23.35 million of those units sold in the Americas and 17.17 million in Japan. Although it could not quite repeat the success of the NES, which sold 61.91 million units worldwide, the SNES was the best-selling console of its era.

In 2007, GameTrailers named the SNES as the second-best console of all time in their list of top ten consoles that "left their mark on the history of gaming", citing its graphics, sound, and library of top-quality games. In 2015, they also named it the best Nintendo console of all time, saying, "The list of games we love from this console completely annihilates any other roster from the Big N." Technology columnist Don Reisinger proclaimed "The SNES is the greatest console of all time" in January 2008, citing the quality of the games and the console's dramatic improvement over its predecessor; fellow technology columnist Will Greenwald replied with a more nuanced view, giving the SNES top marks with his heart, the NES with his head, and the PlayStation (for its controller) with his hands. GamingExcellence also gave the SNES first place in 2008, declaring it "simply the most timeless system ever created" with many games that stand the test of time and citing its innovation in controller design, graphics capabilities, and game storytelling. At the same time, GameDaily rated it fifth for its graphics, audio, controllers, and games. In 2009, IGN named the Super Nintendo Entertainment System the fourth best video game console, complimenting its audio and number of AAA games.





</doc>
<doc id="28319" url="https://en.wikipedia.org/wiki?curid=28319" title="Smalltalk">
Smalltalk

Smalltalk is an object-oriented, dynamically typed, reflective programming language. Smalltalk was created as the language to underpin the "new world" of computing exemplified by "human–computer symbiosis." It was designed and created in part for educational use, more so for constructionist learning, at the Learning Research Group (LRG) of Xerox PARC by Alan Kay, Dan Ingalls, Adele Goldberg, Ted Kaehler, Scott Wallace, and others during the 1970s.

The language was first generally released as Smalltalk-80. Smalltalk-like languages are in continuing active development and have gathered loyal communities of users around them. ANSI Smalltalk was ratified in 1998 and represents the standard version of Smalltalk.

Smalltalk took second place for "most loved programming language" in the Stack Overflow Developer Survey in 2017, but it was not among the 26 most loved programming languages of the 2018 survey.

There are a large number of Smalltalk variants. The unqualified word "Smalltalk" is often used to indicate the Smalltalk-80 language, the first version to be made publicly available and created in 1980.

Smalltalk was the product of research led by Alan Kay at Xerox Palo Alto Research Center (PARC); Alan Kay designed most of the early Smalltalk versions, Adele Goldberg wrote most of the documentation, and Dan Ingalls implemented most of the early versions. The first version, termed Smalltalk-71, was created by Kay in a few mornings on a bet that a programming language based on the idea of message passing inspired by Simula could be implemented in "a page of code." A later variant used for research work is now termed Smalltalk-72 and influenced the development of the Actor model. Its syntax and execution model were very different from modern Smalltalk variants.

After significant revisions which froze some aspects of execution semantics to gain performance (by adopting a Simula-like class inheritance model of execution), Smalltalk-76 was created. This system had a development environment featuring most of the now familiar tools, including a class library code browser/editor. Smalltalk-80 added metaclasses, to help maintain the "everything is an object" (except private instance variables) paradigm by associating properties and behavior with individual classes, and even primitives such as integer and boolean values (for example, to support different ways to create instances).

Smalltalk-80 was the first language variant made available outside of PARC, first as Smalltalk-80 Version 1, given to a small number of firms (Hewlett-Packard, Apple Computer, Tektronix, and Digital Equipment Corporation (DEC)) and universities (UC Berkeley) for "peer review" and implementing on their platforms. Later (in 1983) a general availability implementation, named Smalltalk-80 Version 2, was released as an image (platform-independent file with object definitions) and a virtual machine specification. ANSI Smalltalk has been the standard language reference since 1998.

Two of the currently popular Smalltalk implementation variants are descendants of those original Smalltalk-80 images. Squeak is an open source implementation derived from Smalltalk-80 Version 1 by way of Apple Smalltalk. VisualWorks is derived from Smalltalk-80 version 2 by way of Smalltalk-80 2.5 and ObjectWorks (both products of ParcPlace Systems, a Xerox PARC spin-off company formed to bring Smalltalk to the market). As an interesting link between generations, in 2001 Vassili Bykov implemented Hobbes, a virtual machine running Smalltalk-80 inside VisualWorks. (Dan Ingalls later ported Hobbes to Squeak.)

During the late 1980s to mid-1990s, Smalltalk environments—including support, training and add-ons—were sold by two competing organizations: ParcPlace Systems and Digitalk, both California based. ParcPlace Systems tended to focus on the Unix/Sun microsystems market, while Digitalk focused on Intel-based PCs running Microsoft Windows or IBM's OS/2. Both firms struggled to take Smalltalk mainstream due to Smalltalk's substantial memory needs, limited run-time performance, and initial lack of supported connectivity to SQL-based relational database servers. While the high price of ParcPlace Smalltalk limited its market penetration to mid-sized and large commercial organizations, the Digitalk products initially tried to reach a wider audience with a lower price. IBM initially supported the Digitalk product, but then entered the market with a Smalltalk product in 1995 called VisualAge/Smalltalk. Easel introduced Enfin at this time on Windows and OS/2. Enfin became far more popular in Europe, as IBM introduced it into IT shops before their development of IBM Smalltalk (later VisualAge). Enfin was later acquired by Cincom Systems, and is now sold under the name ObjectStudio, and is part of the Cincom Smalltalk product suite.

In 1995, ParcPlace and Digitalk merged into ParcPlace-Digitalk and then rebranded in 1997 as ObjectShare, located in Irvine, CA. ObjectShare (NASDAQ: OBJS) was traded publicly until 1999, when it was delisted and dissolved. The merged firm never managed to find an effective response to Java as to market positioning, and by 1997 its owners were looking to sell the business. In 1999, Seagull Software acquired the ObjectShare Java development lab (including the original Smalltalk/V and Visual Smalltalk development team), and still owns VisualSmalltalk, although worldwide distribution rights for the Smalltalk product remained with ObjectShare who then sold them to Cincom. VisualWorks was sold to Cincom and is now part of Cincom Smalltalk. Cincom has backed Smalltalk strongly, releasing multiple new versions of VisualWorks and ObjectStudio each year since 1999.

Cincom, Gemstone and Object Arts, plus other vendors continue to sell Smalltalk environments. IBM has 'end of life'd VisualAge Smalltalk having in the late 1990s decided to back Java and it is, , supported by Instantiations, Inc. which has renamed the product VA Smalltalk and released several new versions. The open Squeak implementation has an active community of developers, including many of the original Smalltalk community, and has recently been used to provide the Etoys environment on the OLPC project, a toolkit for developing collaborative applications Croquet Project, and the Open Cobalt virtual world application. GNU Smalltalk is a free software implementation of a derivative of Smalltalk-80 from the GNU project. Pharo Smalltalk is a fork of Squeak oriented toward research and use in commercial environments.

A significant development, that has spread across all Smalltalk environments as of 2016, is the increasing usage of two web frameworks, Seaside and AIDA/Web, to simplify the building of complex web applications. Seaside has seen considerable market interest with Cincom, Gemstone, and Instantiations incorporating and extending it.

Smalltalk was one of many object-oriented programming languages based on Simula. Smalltalk is also one of the most influential programming languages. Virtually all of the object-oriented languages that came after—Flavors, CLOS, Objective-C, Java, Python, Ruby, and many others—were influenced by Smalltalk. Smalltalk was also one of the most popular languages with the Agile Methods, Rapid Prototyping, and Software Patterns communities. The highly productive environment provided by Smalltalk platforms made them ideal for rapid, iterative development.

Smalltalk emerged from a larger program of ARPA funded research that in many ways defined the modern world of computing. In addition to Smalltalk, working prototypes of things such as hypertext, GUIs, multimedia, the mouse, telepresence, and the Internet were developed by ARPA researchers in the 1960s. Alan Kay (one of the inventors of Smalltalk) also described a tablet computer he called the Dynabook which resembles modern tablet computers like the iPad.

Smalltalk environments were often the first to develop what are now common object-oriented software design patterns. One of the most popular is the model–view–controller (MVC) pattern for user interface design. The MVC pattern enables developers to have multiple consistent views of the same underlying data. It's ideal for software development environments, where there are various views (e.g., entity-relation, dataflow, object model, etc.) of the same underlying specification. Also, for simulations or games where the underlying model may be viewed from various angles and levels of abstraction.

In addition to the MVC pattern, the Smalltalk language and environment were highly influential in the history of the graphical user interface (GUI) and the "what you see is what you get" (WYSIWYG) user interface, font editors, and desktop metaphors for UI design. The powerful built-in debugging and object inspection tools that came with Smalltalk environments set the standard for all the integrated development environments, starting with Lisp Machine environments, that came after.

As in other object-oriented languages, the central concept in Smalltalk-80 (but not in Smalltalk-72) is that of an "object". An object is always an "instance" of a "class". Classes are "blueprints" that describe the properties and behavior of their instances. For example, a GUI's window class might declare that windows have properties such as the label, the position and whether the window is visible or not. The class might also declare that instances support operations such as opening, closing, moving and hiding. Each particular window object would have its own values of those properties, and each of them would be able to perform operations defined by its class.

A Smalltalk object can do exactly three things:

The state an object holds is always private to that object. Other objects can query or change that state only by sending requests (messages) to the object to do so. Any message can be sent to any object: when a message is received, the receiver determines whether that message is appropriate. Alan Kay has commented that despite the attention given to objects, messaging is the most important concept in Smalltalk: "The big idea is 'messaging'—that is what the kernel of Smalltalk/Squeak is all about (and it's something that was never quite completed in our Xerox PARC phase)."

Smalltalk is a "pure" object-oriented programming language, meaning that, unlike C++ and Java, there is no difference between values which are objects and values which are primitive types. In Smalltalk, primitive values such as integers, booleans and characters are also objects, in the sense that they are instances of corresponding classes, and operations on them are invoked by sending messages. A programmer can change or extend (through subclassing) the classes that implement primitive values, so that new behavior can be defined for their instances—for example, to implement new control structures—or even so that their existing behavior will be changed. This fact is summarized in the commonly heard phrase "In Smalltalk everything is an object", which may be more accurately expressed as "all values are objects", as variables are not.

Since all values are objects, classes are also objects. Each class is an instance of the "metaclass" of that class. Metaclasses in turn are also objects, and are all instances of a class called Metaclass. Code blocks—Smalltalk's way of expressing anonymous functions—are also objects.

Reflection is a term that computer scientists apply to software programs that have the ability to inspect their own structure, for example their parse tree or data types of input and output parameters. Reflection was first primarily a feature of interpreted languages such as Smalltalk and Lisp. Statements being interpreted means that the programs have access to information created as they were parsed and can often even modify their own structure.

Reflection is also a feature of having a meta-model as Smalltalk does. The meta-model is the model that describes the language, and developers can use the meta-model to do things like walk through, examine, and modify the parse tree of an object. Or find all the instances of a certain kind of structure (e.g., all instances of the Method class in the meta-model).

Smalltalk-80 is a totally reflective system, implemented in Smalltalk-80. Smalltalk-80 provides both structural and computational reflection. Smalltalk is a structurally reflective system which structure is defined by Smalltalk-80 objects. The classes and methods that define the system are also objects and fully part of the system that they help define. The Smalltalk compiler compiles textual source code into method objects, typically instances of codice_1. These get added to classes by storing them in a class's method dictionary. The part of the class hierarchy that defines classes can add new classes to the system. The system is extended by running Smalltalk-80 code that creates or defines classes and methods. In this way a Smalltalk-80 system is a "living" system, carrying around the ability to extend itself at run time.

Since the classes are objects, they can be asked questions such as "what methods do you implement?" or "what fields/slots/instance variables do you define?". So objects can easily be inspected, copied, (de)serialized and so on with generic code that applies to any object in the system.

Smalltalk-80 also provides computational reflection, the ability to observe the computational state of the system. In languages derived from the original Smalltalk-80 the current activation of a method is accessible as an object named via a pseudo-variable (one of the six reserved words), codice_2. By sending messages to codice_2 a method activation can ask questions like "who sent this message to me". These facilities make it possible to implement co-routines or Prolog-like back-tracking without modifying the virtual machine. The exception system is implemented using this facility. One of the more interesting uses of this is in the Seaside web framework which relieves the programmer of dealing with the complexity of a Web Browser's back button by storing continuations for each edited page and switching between them as the user navigates a web site. Programming the web server using Seaside can then be done using a more conventional programming style.

An example of how Smalltalk can use reflection is the mechanism for handling errors. When an object is sent a message that it does not implement, the virtual machine sends the object the codice_4 message with a reification of the message as an argument. The message (another object, an instance of codice_5) contains the selector of the message and an codice_6 of its arguments. In an interactive Smalltalk system the default implementation of codice_4 is one that opens an error window (a Notifier) reporting the error to the user. Through this and the reflective facilities the user can examine the context in which the error occurred, redefine the offending code, and continue, all within the system, using Smalltalk-80's reflective facilities.

By creating a class that understands (implements) only doesNotUnderstand:, one can create an instance that can intercept any message sent to it via its doesNotUnderstand: method. Such instances are called transparent proxies. Such proxies can then be used to implement a number of facilities such as distributed Smalltalk where messages are exchanged between multiple Smalltalk systems, database interfaces where objects are transparently faulted out of a database, promises, etc. The design of distributed Smalltalk influenced such systems as CORBA.

Smalltalk-80 syntax is rather minimalist, based on only a handful of declarations and reserved words. In fact, only six "keywords" are reserved in Smalltalk: codice_8, codice_9, codice_10, codice_11, codice_12, and codice_2. These are properly termed "pseudo-variables", identifiers that follow the rules for variable identifiers but denote bindings that a programmer cannot change. The codice_8, codice_9, and codice_10 pseudo-variables are singleton instances. codice_11 and codice_12 refer to the receiver of a message within a method activated in response to that message, but sends to codice_12 are looked up in the superclass of the method's defining class rather than the class of the receiver, which allows methods in subclasses to invoke methods of the same name in superclasses. codice_2 refers to the current activation record. The only built-in language constructs are message sends, assignment, method return and literal syntax for some objects. From its origins as a language for children of all ages, standard Smalltalk syntax uses punctuation in a manner more like English than mainstream coding languages. The remainder of the language, including control structures for conditional evaluation and iteration, is implemented on top of the built-in constructs by the standard Smalltalk class library. (For performance reasons, implementations may recognize and treat as special some of those messages; however, this is only an optimization and is not hardwired into the language syntax.)

The adage that "Smalltalk syntax fits on a postcard" refers to a code snippet by Ralph Johnson, demonstrating all the basic standard syntactic elements of methods:

The following examples illustrate the most common objects which can be written as literal values in Smalltalk-80 methods.

Numbers. The following list illustrates some of the possibilities.

The last two entries are a binary and a hexadecimal number, respectively. The number before the 'r' is the radix or base. The base does not have to be a power of two; for example 36rSMALLTALK is a valid number equal to 80738163270632 decimal.

Characters are written by preceding them with a dollar sign:

Strings are sequences of characters enclosed in single quotes:

To include a quote in a string, escape it using a second quote:

Double quotes do not need escaping, since single quotes delimit a string:

Two equal strings (strings are equal if they contain all the same characters) can be different objects residing in different places in memory. In addition to strings, Smalltalk has a class of character sequence objects called Symbol. Symbols are guaranteed to be unique—there can be no two equal symbols which are different objects. Because of that, symbols are very cheap to compare and are often used for language artifacts such as message selectors (see below).

Symbols are written as # followed by a string literal. For example:

If the sequence does not include whitespace or punctuation characters,
this can also be written as:

Arrays:

defines an array of four integers.

Many implementations support the following literal syntax for ByteArrays:

defines a ByteArray of four integers.

And last but not least, blocks (anonymous function literals)

Blocks are explained in detail further in the text.

Many Smalltalk dialects implement additional syntaxes for other objects, but the ones above are the essentials supported by all.

The two kinds of variables commonly used in Smalltalk are instance variables and temporary variables. Other variables and related terminology depend on the particular implementation. For example, VisualWorks has class shared variables and namespace shared variables, while Squeak and many other implementations have class variables, pool variables and global variables.

Temporary variable declarations in Smalltalk are variables declared inside a method (see below). They are declared at the top of the method as names separated by spaces and enclosed by vertical bars. For example:

declares a temporary variable named index which contains initially the value codice_10. 

Multiple variables may be declared within one set of bars: 

declares two variables: index and vowels. All variables are initialized. Variables are initialized to nil except the indexed variables of Strings, which are initialized to the null character or ByteArrays which are initialized to 0.

A variable is assigned a value via the ':=' syntax. So:

Assigns the string codice_22 to the formerly declared vowels variable. The string is an object (a sequence of characters between single quotes is the syntax for literal strings), created by the compiler at compile time.

In the original Parc Place image, the glyph of the underscore character ⟨_⟩ appeared as a left-facing arrow ⟨←⟩ (like in the 1963 version of the ASCII code). Smalltalk originally accepted this left-arrow as the only assignment operator. Some modern code still contains what appear to be underscores acting as assignments, hearkening back to this original usage. Most modern Smalltalk implementations accept either the underscore or the colon-equals syntax.

The message is the most fundamental language construct in Smalltalk. Even control structures are implemented as message sends. Smalltalk adopts by default a synchronous, single dynamic message dispatch strategy (as contrasted to the asynchronous, multiple dispatch strategy adopted by some other object-oriented languages).

The following example sends the message 'factorial' to number 42:

In this situation 42 is called the message "receiver", while 'factorial' is the message "selector". The receiver responds to the message by returning a value (presumably in this case the factorial of 42). Among other things, the result of the message can be assigned to a variable:

"factorial" above is what is called a "unary message" because only one object, the receiver, is involved. Messages can carry additional objects as "arguments", as follows:

In this expression two objects are involved: 2 as the receiver and 4 as the message argument. The message result, or in Smalltalk parlance, "the answer" is supposed to be 16. Such messages are called "keyword messages". A message can have more arguments, using the following syntax:

which answers the index of character 'o' in the receiver string, starting the search from index 6. The selector of this message is "indexOf:startingAt:", consisting of two pieces, or "keywords".

Such interleaving of keywords and arguments is meant to improve readability of code, since arguments are explained by their preceding keywords. For example, an expression to create a rectangle using a C++ or Java-like syntax might be written as:

It's unclear which argument is which. By contrast, in Smalltalk, this code would be written as:

The receiver in this case is "Rectangle", a class, and the answer will be a new instance of the class with the specified width and height.

Finally, most of the special (non-alphabetic) characters can be used as what are called "binary messages". These allow mathematical and logical operators to be written in their traditional form:

which sends the message "+" to the receiver 3 with 4 passed as the argument (the answer of which will be 7). Similarly,

is the message ">" sent to 3 with argument 4 (the answer of which will be false).

Notice, that the Smalltalk-80 language itself does not imply the meaning of those operators. The outcome of the above is only defined by how the receiver of the message (in this case a Number instance) responds to messages "+" and ">".

A side effect of this mechanism is operator overloading. A message ">" can also be understood by other objects, allowing the use of expressions of the form "a > b" to compare them.

An expression can include multiple message sends. In this case expressions are parsed according to a simple order of precedence. Unary messages have the highest precedence, followed by binary messages, followed by keyword messages. For example:

is evaluated as follows:


The answer of the last message sent is the result of the entire expression.

Parentheses can alter the order of evaluation when needed. For example,

will change the meaning so that the expression first computes "3 factorial + 4" yielding 10. That 10 then receives the second "factorial" message, yielding 3628800. 3628800 then receives "between:and:", answering false.

Note that because the meaning of binary messages is not hardwired into Smalltalk-80 syntax, all of them are considered to have equal precedence and are evaluated simply from left to right. Because of this, the meaning of Smalltalk expressions using binary messages can be different from their "traditional" interpretation:

is evaluated as "(3 + 4) * 5", producing 35. To obtain the expected answer of 23, parentheses must be used to explicitly define the order of operations:

Unary messages can be "chained" by writing them one after another:

which sends "factorial" to 3, then "factorial" to the result (6), then "log" to the result (720), producing the result 2.85733.

A series of expressions can be written as in the following (hypothetical) example, each separated by a period. This example first creates a new instance of class Window, stores it in a variable, and then sends two messages to it.

If a series of messages are sent to the same receiver as in the example above, they can also be written as a "cascade" with individual messages separated by semicolons:

This rewrite of the earlier example as a single expression avoids the need to store the new window in a temporary variable. According to the usual precedence rules, the unary message "new" is sent first, and then "label:" and "open" are sent to the answer of "new".

A block of code (an anonymous function) can be expressed as a literal value (which is an object, since all values are objects). This is achieved with square brackets:

Where ":params" is the list of parameters the code can take. This means that the Smalltalk code:

can be understood as:

or expressed in lambda terms as:

and

can be evaluated as

Or in lambda terms as:

The resulting block object can form a closure: it can access the variables of its enclosing lexical scopes at any time. Blocks are first-class objects.

Blocks can be executed by sending them the "value" message (compound variations exist in order to provide parameters to the block e.g. 'value:value:' and 'valueWithArguments:').

The literal representation of blocks was an innovation which on the one hand allowed certain code to be significantly more readable; it allowed algorithms involving iteration to be coded in a clear and concise way. Code that would typically be written with loops in some languages can be written concisely in Smalltalk using blocks, sometimes in a single line. But more importantly blocks allow control structure to be expressed using messages and polymorphism, since blocks defer computation and polymorphism can be used to select alternatives. So if-then-else in Smalltalk is written and implemented as

Note that this is related to functional programming, wherein patterns of computation (here selection) are abstracted into higher-order functions. For example, the message "select:" on a Collection is equivalent to the higher-order function filter on an appropriate functor.

Control structures do not have special syntax in Smalltalk. They are instead implemented as messages sent to objects. For example, conditional execution is implemented by sending the message ifTrue: to a Boolean object, passing as an argument the block of code to be executed if and only if the Boolean receiver is true.

The following code demonstrates this:

Blocks are also used to implement user-defined control structures, enumerators, visitors, exception handling, pluggable behavior and many other patterns.
For example:

In the last line, the string is sent the message select: with an argument that is a code block literal. The code block literal will be used as a predicate function that should answer true if and only if an element of the String should be included in the Collection of characters that satisfy the test represented by the code block that is the argument to the "select:" message.

A String object responds to the "select:" message by iterating through its members (by sending itself the message "do:"), evaluating the selection block ("aBlock") once with each character it contains as the argument. When evaluated (by being sent the message "value: each"), the selection block (referenced by the parameter "aBlock", and defined by the block literal "[:aCharacter | aCharacter isVowel]"), answers a boolean, which is then sent "ifTrue:". If the boolean is the object true, the character is added to a string to be returned.
Because the "select:" method is defined in the abstract class Collection, it can also be used like this:

The exception handling mechanism uses blocks as handlers (similar to CLOS-style exception handling):

The exception handler's "ex" argument provides access to the state of the suspended operation (stack frame, line-number, receiver and arguments etc.) and is also used to control how the computation is to proceed (by sending one of "ex proceed", "ex reject", "ex restart" or "ex return").

This is a stock class definition:

Often, most of this definition will be filled in by the environment. Notice that this is a message to the codice_23 class to create a subclass called codice_24. In other words: classes are first-class objects in Smalltalk which can receive messages just like any other object and can be created dynamically at execution time.

When an object receives a message, a method matching the message name is invoked. The following code defines a method publish, and so defines what will happen when this object receives the 'publish' message.

The following method demonstrates receiving multiple arguments and returning a value:

The method's name is codice_25. The return value is specified with the codice_26 operator.

Note that objects are responsible for determining dynamically at runtime which method to execute in response to a message—while in many languages this may be (sometimes, or even always) determined statically at compile time.

The following code:

creates (and returns) a new instance of the MessagePublisher class. This is typically assigned to a variable:

However, it is also possible to send a message to a temporary, anonymous object:
The Hello world program is used by virtually all texts to new programming languages as the first program learned to show the most basic syntax and environment of the language. For Smalltalk, the program is extremely simple to write. The following code, the message "show:" is sent to the object "Transcript" with the String literal 'Hello, world!' as its argument. Invocation of the "show:" method causes the characters of its argument (the String literal 'Hello, world!') to be displayed in the transcript ("terminal") window.

Note that a Transcript window would need to be open in order to see the results of this example.

Most popular programming systems separate static program code (in the form of class definitions, functions or procedures) from dynamic, or run time, program state (such as objects or other forms of program data). They load program code when a program starts, and any prior program state must be recreated explicitly from configuration files or other data sources. Any settings the program (and programmer) does not explicitly save must be set up again for each restart. A traditional program also loses much useful document information each time a program saves a file, quits, and reloads. This loses details such as undo history or cursor position. Image based systems don't force losing all that just because a computer is turned off, or an OS updates.

Many Smalltalk systems, however, do not differentiate between program data (objects) and code (classes). In fact, classes are objects. Thus, most Smalltalk systems store the entire program state (including both Class and non-Class objects) in an image file. The image can then be loaded by the Smalltalk virtual machine to restore a Smalltalk-like system to a prior state. This was inspired by FLEX, a language created by Alan Kay and described in his M.Sc. thesis.

Smalltalk images are similar to (restartable) core dumps and can provide the same functionality as core dumps, such as delayed or remote debugging with full access to the program state at the time of error. Other languages that model application code as a form of data, such as Lisp, often use image-based persistence as well. This method of persistence is powerful for rapid development because all the development information (e.g. parse trees of the program) is saved which facilitates debugging. However, it also has serious drawbacks as a true persistence mechanism. For one thing, developers may often want to hide implementation details and not make them available in a run time environment. For reasons of legality and maintenance, allowing anyone to modify a program at run time inevitably introduces complexity and potential errors that would not be possible with a compiled system that exposes no source code in the run time environment. Also, while the persistence mechanism is easy to use, it lacks the true persistence abilities needed for most multi-user systems. The most obvious is the ability to do transactions with multiple users accessing the same database in parallel.

Everything in Smalltalk-80 is available for modification from within a running program. This means that, for example, the IDE can be changed in a running system without restarting it. In some implementations, the syntax of the language or the garbage collection implementation can also be changed on the fly. Even the statement codice_27 is valid in Smalltalk, although executing it is not recommended.

Smalltalk programs are usually compiled to bytecode, which is then interpreted by a virtual machine or dynamically translated into machine-native code.






</doc>
