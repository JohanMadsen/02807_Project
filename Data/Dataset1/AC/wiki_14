<doc id="26295" url="https://en.wikipedia.org/wiki?curid=26295" title="Russian Civil War">
Russian Civil War

The Russian Civil War (; November 1917 – October 1922) was a multi-party war in the former Russian Empire immediately after the two Russian Revolutions of 1917, as many factions vied to determine Russia's political future. The two largest combatant groups were the Red Army, fighting for the Bolshevik form of socialism led by Vladimir Lenin and Leon Trotsky, and the loosely allied forces known as the White Army, which included diverse interests favoring political monarchism, economic capitalism and alternative forms of socialism, each with democratic and antidemocratic variants. In addition, rival militant socialists and nonideological Green armies fought against both the Bolsheviks and the Whites. Eight foreign nations intervened against the Red Army, notably the former Allied military forces from the World War and the pro-German armies. The Red Army eventually defeated the White Armed Forces of South Russia in Ukraine and the army led by Admiral Aleksandr Kolchak to the east in Siberia in 1919. The remains of the White forces commanded by Pyotr Nikolayevich Wrangel were beaten in Crimea and evacuated in late 1920. Lesser battles of the war continued on the periphery for two more years, and minor skirmishes with the remnants of the White forces in the Far East continued well into 1923. The war ended in 1923 in the sense that Bolshevik communist control of the newly formed Soviet Union was now assured, although armed national resistance in Central Asia was not completely crushed until 1934. There were an estimated 7,000,000–12,000,000 casualties during the war, mostly civilians. The Russian Civil War has been described by some as the greatest national catastrophe that Europe had yet seen.

Many pro-independence movements emerged after the break-up of the Russian Empire and fought in the war. Several parts of the former Russian Empire—Finland, Estonia, Latvia, Lithuania, and Poland—were established as sovereign states, with their own civil wars and wars of independence. The rest of the former Russian Empire was consolidated into the Soviet Union shortly afterwards.

After the abdication of Tsar Nicholas II of Russia, the Russian Provisional Government was established during the February Revolution of 1917.

From mid-1917 onwards, the Russian Army, the successor-organisation of the old Russian Imperial Army, started to disintegrate; the Bolsheviks used the volunteer-based Red Guards as their main military force, augmented by an armed military component of the Cheka (the Bolshevik state-security apparatus). In January 1918, after significant Bolshevik reverses in combat, the future People's Commissar for Military and Naval Affairs Leon Trotsky headed the reorganization of the Red Guards into a "Workers' and Peasants' Red Army" in order to create a more effective fighting force. The Bolsheviks appointed political commissars to each unit of the Red Army to maintain morale and to ensure loyalty.

In June 1918, when it had become apparent that a revolutionary army composed solely of workers would not suffice, Trotsky instituted mandatory conscription of the rural peasantry into the Red Army. The Bolsheviks overcame opposition of rural Russians to Red-Army conscription units by taking hostages and shooting them when necessary in order to force compliance, exactly the same practices used by the White Army officers. The Red Army utilized former Tsarist officers as "military specialists" ("voenspetsy"); sometimes their families were taken hostage in order to ensure their loyalty. At the start of the civil war, former Tsarist officers comprised three-quarters of the Red Army officer-corps. By its end, 83% of all Red Army divisional and corps commanders were ex-Tsarist soldiers.

While resistance to the Red Guard began on the very day after the Bolshevik uprising, the Treaty of Brest-Litovsk and the political ban became a catalyst for the formation of anti-Bolshevik groups both inside and outside Russia, pushing them into action against the new regime.

A loose confederation of anti-Bolshevik forces aligned against the Communist government, including landowners, republicans, conservatives, middle-class citizens, reactionaries, pro-monarchists, liberals, army generals, non-Bolshevik socialists who still had grievances and democratic reformists voluntarily united only in their opposition to Bolshevik rule. Their military forces, bolstered by forced conscriptions and terror and by foreign influence and led by Gen. Yudenich, Adm. Kolchak and Gen. Denikin, became known as the White movement (sometimes referred to as the "White Army") and controlled significant parts of the former Russian Empire for most of the war.

A Ukrainian nationalist movement was active in Ukraine during the war. More significant was the emergence of an anarchist political and military movement known as the Revolutionary Insurrectionary Army of Ukraine or the Anarchist Black Army led by Nestor Makhno. The Black Army, which counted numerous Jews and Ukrainian peasants in its ranks, played a key part in halting Gen. Denikin's White Army offensive towards Moscow during 1919, later ejecting White forces from Crimea.

The remoteness of the Volga Region, the Ural Region, Siberia and the Far East was favorable for the anti-Bolshevik forces, and the Whites set up a number of organizations in the cities of these regions. Some of the military forces were set up on the basis of clandestine officers' organizations in the cities.

The Czechoslovak Legions had been part of the Russian army and numbered around 30,000 troops by October 1917. They had an agreement with the new Bolshevik government to be evacuated from the Eastern Front via the port of Vladivostok to France. The transport from the Eastern Front to Vladivostok slowed down in the chaos, and the troops became dispersed all along the Trans-Siberian Railway. Under pressure from the Central Powers, Trotsky ordered the disarming and arrest of the legionaries, which created tensions with the Bolsheviks.

The Western Allies armed and supported opponents of the Bolsheviks. They were worried about (1) a possible Russo-German alliance; (2) the prospect of the Bolsheviks making good on their threats to default on Imperial Russia's massive foreign loans; and (3) the possibility that the Communist revolutionary ideas would spread (a concern shared by many Central Powers). Hence, many of these countries expressed their support for the Whites, including the provision of troops and supplies. Winston Churchill declared that Bolshevism must be "strangled in its cradle". The British and French had supported Russia during World War I on a massive scale with war materials. After the treaty, it looked like much of that material would fall into the hands of the Germans. Under this pretext began the Allied intervention in the Russian Civil War with the United Kingdom and France sending troops into Russian ports. There were violent clashes with troops loyal to the Bolsheviks.

The German Empire created several short-lived satellite buffer states within its sphere of influence after the Treaty of Brest-Litovsk: the "United Baltic Duchy", "Duchy of Courland and Semigallia", "Kingdom of Lithuania", "Kingdom of Poland", the "Belarusian People's Republic", and the "Ukrainian State". Following the defeat of Germany in World War I in November 1918, these states were abolished.

Finland was the first republic that declared its independence from Russia in December 1917 and established itself in the ensuing Finnish Civil War from January–May 1918. The Second Polish Republic, Lithuania, Latvia and Estonia formed their own armies immediately after the abolition of the Brest-Litovsk Treaty and the start of the Soviet westward offensive in November 1918.

In the European part of Russia the war was fought across three main fronts: the eastern, the southern and the northwestern. It can also be roughly split into the following periods.
The first period lasted from the Revolution until the Armistice. Already on the date of the Revolution, Cossack Gen. Kaledin refused to recognize it and assumed full governmental authority in the Don region, where the Volunteer Army began amassing support. The signing of the Treaty of Brest-Litovsk also resulted in direct Allied intervention in Russia and the arming of military forces opposed to the Bolshevik government. There were also many German commanders who offered support against the Bolsheviks, fearing a confrontation with them was impending as well.

During this first period the Bolsheviks took control of Central Asia out of the hands of the Provisional Government and White Army, setting up a base for the Communist Party in the Steppe and Turkestan, where nearly two million Russian settlers were located.

Most of the fighting in this first period was sporadic, involving only small groups amid a fluid and rapidly shifting strategic scene. Among the antagonists were the Czechs, known as the Czechoslovak Legion or "White Czechs", the Poles of the Polish 5th Rifle Division and the pro-Bolshevik Red Latvian riflemen.

The second period of the war lasted from January to November 1919. At first the White armies' advances from the south (under Gen. Denikin), the east (under Adm. Kolchak) and the northwest (under Gen. Yudenich) were successful, forcing the Red Army and its allies back on all three fronts. In July 1919 the Red Army suffered another reverse after a mass defection of units in the Crimea to the anarchist Black Army under Nestor Makhno, enabling anarchist forces to consolidate power in Ukraine. Leon Trotsky soon reformed the Red Army, concluding the first of two military alliances with the anarchists. In June the Red Army first checked Kolchak's advance. After a series of engagements, assisted by a Black Army offensive against White supply lines, the Red Army defeated Denikin's and Yudenich's armies in October and November.

The third period of the war was the extended siege of the last White forces in the Crimea. Gen. Wrangel had gathered the remnants of Denikin's armies, occupying much of the Crimea. An attempted invasion of southern Ukraine was rebuffed by the anarchist Black Army under the command of Nestor Makhno. Pursued into the Crimea by Makhno's troops, Wrangel went over to the defensive in the Crimea. After an abortive move north against the Red Army, Wrangel's troops were forced south by Red Army and Black Army forces; Wrangel and the remains of his army were evacuated to Constantinople in November 1920.

In the October Revolution the Bolshevik Party directed the Red Guard (armed groups of workers and Imperial army deserters) to seize control of Petrograd (Saint Petersburg) and immediately began the armed takeover of cities and villages throughout the former Russian Empire. In January 1918 the Bolsheviks dissolved the Russian Constituent Assembly and proclaimed the Soviets (workers' councils) as the new government of Russia.

The first attempt to regain power from the Bolsheviks was made by the Kerensky-Krasnov uprising in October 1917. It was supported by the Junker Mutiny in Petrograd but was quickly put down by the Red Guard, notably the Latvian Rifle Division.

The initial groups that fought against the Communists were local Cossack armies that had declared their loyalty to the Provisional Government. Gen. Kaledin of the Don Cossacks and Gen. Semenov of the Siberian Cossacks were prominent among them. The leading Tsarist officers of the old regime also started to resist. In November, Gen. Alekseev, the Tsar's Chief of Staff during the First World War, began to organize the Volunteer Army in Novocherkassk. Volunteers of this small army were mostly officers of the old Russian army, military cadets and students. In December 1917 Alekseev was joined by Gen. Kornilov, Denikin and other Tsarist officers who had escaped from the jail, where they had been imprisoned following the abortive Kornilov affair just before the Revolution. At the beginning of December 1917 groups of volunteers and Cossacks captured Rostov.

Having stated in the November 1917 "Declaration of Rights of Nations of Russia" that any nation under imperial Russian rule should be immediately given the power of self-determination, the Bolsheviks had begun to usurp the power of the Provisional Government in the territories of Central Asia soon after the establishment of the Turkestan Committee in Tashkent. In April 1917 the Provisional Government set up this committee, which was mostly made up of former Tsarist officials. The Bolsheviks attempted to take control of the Committee in Tashkent on 12 September 1917 but it was unsuccessful, and many leaders were arrested. However, because the Committee lacked representation of the native population and poor Russian settlers, they had to release the Bolshevik prisoners almost immediately due to public outcry, and a successful takeover of this government body took place two months later in November. The triumph of the Bolshevik party over the Provisional Government during 1917 was mostly due to the support they received from the working class of Central Asia. The Leagues of Mohammedam Working People, which Russian settlers and natives who had been sent to work behind the lines for the Tsarist government in 1916 formed in March 1917, had led numerous strikes in the industrial centers throughout September 1917. However, after the Bolshevik destruction of the Provisional Government in Tashkent, Muslim elites formed an autonomous government in Turkestan, commonly called the "Kokand autonomy" (or simply Kokand). The White Russians supported this government body, which lasted several months because of Bolshevik troop isolation from Moscow. In January 1918 the Soviet forces under Lt. Col. Muravyov invaded Ukraine and invested Kiev, where the Central Council of the Ukrainian People's Republic held power. With the help of the Kiev Arsenal Uprising, the Bolsheviks captured the city on 26 January.

The Bolsheviks decided to immediately make peace with the German Empire and the Central Powers, as they had promised the Russian people before the Revolution. Vladimir Lenin's political enemies attributed that decision to his sponsorship by the Foreign Office of Wilhelm II, German Emperor, offered to Lenin in hope that, with a revolution, Russia would withdraw from World War I. That suspicion was bolstered by the German Foreign Ministry's sponsorship of Lenin's return to Petrograd. However, after the military fiasco of the summer offensive (June 1917) by the Russian Provisional Government, and in particular after the failed summer offensive of the Provisional Government had devastated the structure of the Russian army, it became crucial that Lenin realize the promised peace. Even before the failed summer offensive the Russian population was very skeptical about the continuation of the war. Western socialists had promptly arrived from France and from the UK to convince the Russians to continue the fight, but could not change the new pacifist mood of Russia.

On 16 December 1917 an armistice was signed between Russia and the Central Powers in Brest-Litovsk and peace talks began. As a condition for peace, the proposed treaty by the Central Powers conceded huge portions of the former Russian Empire to the German Empire and the Ottoman Empire, greatly upsetting nationalists and conservatives. Leon Trotsky, representing the Bolsheviks, refused at first to sign the treaty while continuing to observe a unilateral cease-fire, following the policy of "No war, no peace".

In view of this, on 18 February 1918 the Germans began Operation Faustschlag on the Eastern Front, encountering virtually no resistance in a campaign that lasted 11 days. Signing a formal peace treaty was the only option in the eyes of the Bolsheviks because the Russian army was demobilized, and the newly formed Red Guard was incapable of stopping the advance. They also understood that the impending counterrevolutionary resistance was more dangerous than the concessions of the treaty, which Lenin viewed as temporary in the light of aspirations for a world revolution. The Soviets acceded to a peace treaty, and the formal agreement, the Treaty of Brest-Litovsk, was ratified on 6 March. The Soviets viewed the treaty as merely a necessary and expedient means to end the war. Therefore, they ceded large amounts of territory to the German Empire.

Under Soviet pressure, the Volunteer Army embarked on the epic Ice March from Yekaterinodar to Kuban on 22 February 1918, where they joined with the Kuban Cossacks to mount an abortive assault on Yekaterinodar. The Soviets recaptured Rostov on the next day. Gen. Kornilov was killed in the fighting on 13 April, and Gen. Denikin took over command. Fighting off its pursuers without respite, the army succeeded in breaking its way through back towards the Don, where the Cossack uprising against Bolsheviks had started.

The Baku Soviet Commune was established on 13 April. Germany landed its Caucasus Expedition troops in Poti on 8 June. The Ottoman Army of Islam (in coalition with Azerbaijan) drove them out of Baku on 26 July 1918. Subsequently, the Dashanaks, Right SRs and Mensheviks started negotiations with Gen. Dunsterville, the commander of the British troops in Persia. The Bolsheviks and their Left SR allies were opposed to it, but on 25 July the majority of the Soviet voted to call in the British and the Bolsheviks resigned. The Baku Soviet Commune ended its existence and was replaced by the Central Caspian Dictatorship.

In June 1918 the Volunteer Army, numbering some 9,000 men, started its Second Kuban campaign. Yekaterinodar was encircled on 1 August and fell on the 3rd. In September–October, heavy fighting took place at Armavir and Stavropol. On 13 October Gen. Kazanovich's division took Armavir, and on 1 November Gen. Pyotr Wrangel secured Stavropol. This time Red forces had no escape, and by the beginning of 1919 the whole Northern Caucasus was controlled by the Volunteer Army.

In October Gen. Alekseev, the leader of the White armies in southern Russia, died of a heart attack. An agreement was reached between Denikin, head of the Volunteer Army, and Pyotr Krasnov, Ataman of the Don Cossacks, which united their forces under the sole command of Denikin. The Armed Forces of South Russia were thus created.

The revolt of the Czechoslovak Legion broke out in May 1918, and the legionaries took control of Chelyabinsk in June. Simultaneously Russian officers' organizations overthrew the Bolsheviks in Petropavlovsk (in present-day Kazakhstan) and in Omsk. Within a month the Whites controlled most of the Trans-Siberian Railroad between Lake Baikal and the Ural regions. During the summer Bolshevik power in Siberia was eliminated. The Provisional Government of Autonomous Siberia formed in Omsk. By the end of July the Whites had extended their gains westwards, capturing Ekaterinburg on 26 July 1918. Shortly before the fall of Yekaterinburg on 17 July 1918, the former Tsar and his family were murdered by the Ural Soviet to prevent them falling into the hands of the Whites.

Mensheviks and Socialist-Revolutionaries supported peasants fighting against Soviet control of food supplies. In May 1918, with the support of the Czechoslovak Legion, they took Samara and Saratov, establishing the Committee of Members of the Constituent Assembly—known as the "Komuch". By July the authority of the Komuch extended over much of the area controlled by the Czechoslovak Legion. The Komuch pursued an ambivalent social policy, combining democratic and socialist measures, such as the institution of an eight-hour working day, with "restorative" actions, such as returning both factories and land to their former owners. After the fall of Kazan, Vladimir Lenin called for the dispatch of Petrograd workers to the Kazan Front: "We must send down the "maximum" number of Petrograd workers: (1) a few dozen 'leaders' like Kayurov; (2) a few thousand militants 'from the ranks'".

After a series of reverses at the front, the Bolsheviks' War Commissar, Trotsky, instituted increasingly harsh measures in order to prevent unauthorized withdrawals, desertions and mutinies in the Red Army. In the field the Cheka special investigations forces, termed the "Special Punitive Department of the All-Russian Extraordinary Commission for Combat of Counter-Revolution and Sabotage" or "Special Punitive Brigades", followed the Red Army, conducting field tribunals and summary executions of soldiers and officers who deserted, retreated from their positions or failed to display sufficient offensive zeal. Trotsky extended the use of the death penalty to the occasional political commissar whose detachment retreated or broke in the face of the enemy. In August, frustrated at continued reports of Red Army troops breaking under fire, Trotsky authorized the formation of barrier troops - stationed behind unreliable Red Army units and given with orders to shoot anyone withdrawing from the battle line without authorization.

In September 1918 Komuch, the Siberian Provisional Government and other local anti-Soviet governments met in Ufa and agreed to form a new Provisional All-Russian Government in Omsk, headed by a Directory of five: two Socialist-Revolutionaries (Nikolai Avksentiev and Vladimir Zenzinov), two Kadets ( and PV Vologodskii) and General Vasily Boldyrev.

By the fall of 1918 anti-Bolshevik White forces in the east included the People's Army (Komuch), the Siberian Army (of the Siberian Provisional Government) and insurgent Cossack units of Orenburg, Ural, Siberia, Semirechye, Baikal, Amur and Ussuri Cossacks, nominally under the orders of Gen. V.G. Boldyrev, Commander-in-Chief, appointed by the Ufa Directorate.

On the Volga, Col. Kappel's White detachment captured Kazan on 7 August, but the Reds re-captured the city on 8 September 1918 following a counteroffensive. On the 11th Simbirsk fell, and on 8 October Samara. The Whites fell back eastwards to Ufa and Orenburg.

In Omsk the Russian Provisional Government quickly came under the influence - then the dominance - of its new War Minister, Rear-Admiral Kolchak. On 18 November a coup d'état established Kolchak as dictator. The members of the Directory were arrested and Kolchak proclaimed the "Supreme Ruler of Russia". By mid-December 1918 White armies had to leave Ufa, but they balanced this failure with a successful drive towards Perm, which they took on 24 December.

In February 1918 the Red Army overthrew the White Russian-supported Kokand autonomy of Turkestan. Although this move seemed to solidify Bolshevik power in Central Asia, more troubles soon arose for the Red Army as the Allied Forces began to intervene. British support of the White Army provided the greatest threat to the Red Army in Central Asia during 1918. Great Britain sent three prominent military leaders to the area. One was Lt. Col. Bailey, who recorded a mission to Tashkent, from where the Bolsheviks forced him to flee. Another was Gen. Malleson, leading the Malleson Mission, who assisted the Mensheviks in Ashkhabad (now the capital of Turkmenistan) with a small Anglo-Indian force. However, he failed to gain control of Tashkent, Bukhara and Khiva. The third was Maj. Gen. Dunsterville, who the Bolsheviks drove out of Central Asia only a month after his arrival in August 1918. Despite setbacks due to British invasions during 1918, the Bolsheviks continued to make progress in bringing the Central Asian population under their influence. The first regional congress of the Russian Communist Party convened in the city of Tashkent in June 1918 in order to build support for a local Bolshevik Party.

In July two Left SR and Cheka employees, Blyumkin and Andreyev, assassinated the German ambassador, Count Mirbach. In Moscow a Left SR uprising was put down by the Bolsheviks, using Cheka military detachments. Lenin personally apologized to the Germans for the assassination. Mass arrests of Socialist-Revolutionaries followed.

Estonia cleared its territory of the Red Army by January 1919. Baltic German volunteers captured Riga from the Red Latvian Riflemen on 22 May, but the Estonian 3rd Division defeated the Baltic Germans a month later, aiding the establishment of the Republic of Latvia.

This rendered possible another threat to the Red Army—one from Gen. Yudenich, who had spent the summer organizing the Northwestern Army in Estonia with local and British support. In October 1919 he tried to capture Petrograd in a sudden assault with a force of around 20,000 men. The attack was well-executed, using night attacks and lightning cavalry maneuvers to turn the flanks of the defending Red Army. Yudenich also had six British tanks, which caused panic whenever they appeared. The Allies gave large quantities of aid to Yudenich, who, however, complained that he was receiving insufficient support.

By 19 October Yudenich's troops had reached the outskirts of the city. Some members of the Bolshevik central committee in Moscow were willing to give up Petrograd, but Trotsky refused to accept the loss of the city and personally organized its defenses. Trotsky himself declared, "It is impossible for a little army of 15,000 ex-officers to master a working-class capital of 700,000 inhabitants." He settled on a strategy of urban defense, proclaiming that the city would "defend itself on its own ground" and that the White Army would be lost in a labyrinth of fortified streets and there "meet its grave".

Trotsky armed all available workers, men and women, ordering the transfer of military forces from Moscow. Within a few weeks the Red Army defending Petrograd had tripled in size and outnumbered Yudenich three to one. At this point Yudenich, short of supplies, decided to call off the siege of the city and withdrew, repeatedly asking permission to withdraw his army across the border to Estonia. However, units retreating across the border were disarmed and interned by order of the Estonian government, which had entered into peace negotiations with the Soviet Government on 16 September and had been informed by the Soviet authorities of their 6 November decision that, should the White Army be allowed to retreat into Estonia, it would be pursued across the border by the Reds. In fact, the Reds attacked Estonian army positions and fighting continued until a cease-fire went into effect on 3 January 1920. Following the Treaty of Tartu most of Yudenich's soldiers went into exile. Former Imperial Russian and now Finnish Gen. Mannerheim planned an intervention to help the Whites in Russia capture Petrograd. He did not, however, gain the necessary support for the endeavor. Lenin considered it "completely certain, that the slightest aid from Finland would have determined the fate of Petrograd".

The British occupied Murmansk and, alongside the Americans, seized Arkhangelsk. With the retreat of Kolchak in Siberia, they pulled their troops out of the cities before the winter trapped them in the port. The remaining White forces under Yevgeny Miller evacuated the region in February 1920.

At the beginning of March 1919 the general offensive of the Whites on the eastern front began. Ufa was retaken on 13 March; by mid-April, the White Army stopped at the Glazov–Chistopol–Bugulma–Buguruslan–Sharlyk line. Reds started their counteroffensive against Kolchak's forces at the end of April. The Red 5th Army, led by the capable commander Tukhachevsky, captured Elabuga on 26 May, Sarapul on 2 June and Izevsk on the 7th and continued to push forward. Both sides had victories and losses, but by the middle of summer the Red Army was larger than the White Army and had managed to recapture territory previously lost.

Following the abortive offensive at Chelyabinsk, the White armies withdrew beyond the Tobol. In September 1919 a White offensive was launched against the Tobol front, the last attempt to change the course of events. However, on 14 October the Reds counterattacked, and thus began the uninterrupted retreat of the Whites to the east.

On 14 November 1919 the Red Army captured Omsk. Adm. Kolchak lost control of his government shortly after this defeat; White Army forces in Siberia essentially ceased to exist by December. Retreat of the eastern front by White armies lasted three months, until mid-February 1920, when the survivors, after crossing Lake Baikal, reached Chita area and joined Ataman Semenov's forces.

The Cossacks had been unable to organize and capitalize on their successes at the end of 1918. By 1919 they had begun to run short of supplies. Consequently, when the Soviet counteroffensive began in January 1919 under the Bolshevik leader Antonov-Ovseenko, the Cossack forces rapidly fell apart. The Red Army captured Kiev on 3 February 1919.

Gen. Denikin's military strength continued to grow in the spring of 1919. During several months in winter and spring of 1919, hard fighting with doubtful outcomes took place in the Donbass, where the attacking Bolsheviks met White forces. At the same time Denikin's Armed Forces of South Russia (AFSR) completed the elimination of Red forces in the northern Caucasus and advanced towards Tsaritsyn. At the end of April and beginning of May the AFSR attacked on all fronts from the Dnepr to the Volga, and by the beginning of the summer they had won numerous battles. French forces landed in Odessa but, after having done almost no fighting, withdrew on 8 April 1919. By mid-June the Reds were chased from the Crimea and the Odessa area. Denikin's troops took the cities of Kharkov and Belgorod. At the same time White troops under Wrangel's command took Tsaritsyn on 17 June 1919. On 20 June Denikin issued his Moscow directive, ordering all AFSR units to prepare for a decisive offensive to take Moscow.

Although Great Britain had withdrawn its own troops from the theater, it continued to give significant military aid (money, weapons, food, ammunition and some military advisors) to the White Armies during 1919. Major Ewen Cameron Bruce of the British Army had volunteered to command a British tank mission assisting the White Army. He was awarded the Distinguished Service Order for his bravery during the June 1919 battle of Tsaritsyn for single-handedly storming and capturing the fortified city of Tsaritsyn, under heavy shell fire in a single tank; this led to the capture of over 40,000 prisoners. The fall of Tsaritsyn is viewed "as one of the key battles of the Russian Civil War" which greatly helped the White Russian cause. Notable historian Sir Basil Henry Liddell Hart comments that Bruce's tank action during this battle is to be seen as "one of the most remarkable feats in the whole history of the Tank Corps".

After the capture of Tsaritsyn, Wrangel pushed towards Saratov but Trotsky, seeing the danger of the union with Kolchak, against whom the Red command was concentrating large masses of troops, repulsed his attempts with heavy losses. When Kolchak's army in the east began to retreat in June and July, the bulk of the Red Army, free now from any serious danger from Siberia, was directed against Denikin.

Denikin's forces constituted a real threat and for a time threatened to reach Moscow. The Red Army, stretched thin by fighting on all fronts, was forced out of Kiev on 30 August. Kursk and Orel were taken, on 20 September and 14 October, respectively. The latter, only from Moscow, was the closest the AFSR would come to its target. The Cossack Don Army under the command of Gen. Vladimir Sidorin continued north towards Voronezh, but there Semyon Budyonny's cavalrymen defeated them on 24 October. This allowed the Red Army to cross the Don River, threatening to split the Don and Volunteer Armies. Fierce fighting took place at the key rail junction of Kastornoye, which was taken on 15 November; Kursk was retaken two days later.

The high tide of the White movement against the Soviets had been reached in September 1919. By this time Denikin's forces were dangerously overextended. The White front had no depth or stability—it had become a series of patrols with occasional columns of slowly advancing troops without reserves. Lacking ammunition, artillery and fresh reinforcements, Denikin's army was decisively defeated in a series of battles in October and November 1919. The Red Army recaptured Kiev on 17 December and the defeated Cossacks fled back towards the Black Sea.

While the White armies were being routed in the center and the east, they had succeeded in driving Nestor Makhno's anarchist Black Army (formally known as the Revolutionary Insurrectionary Army of Ukraine) out of part of southern Ukraine and the Crimea. Despite this setback, Moscow was loath to aid Makhno and the Black Army and refused to provide arms to anarchist forces in Ukraine. The main body of White forces, the Volunteers and the Don Army, pulled back towards the Don, to Rostov. The smaller body (Kiev and Odessa troops) withdrew to Odessa and the Crimea, which it had managed to protect from the Bolsheviks during the winter of 1919–1920.

By February 1919 the British government had pulled its military forces out of Central Asia. Despite this success for the Red Army, the White Army's assaults in European Russia and other areas broke communication between Moscow and Tashkent. For a time Central Asia was completely cut off from Red Army forces in Siberia. Although this communication failure weakened the Red Army, the Bolsheviks continued their efforts to gain support for the Bolshevik Party in Central Asia by holding a second regional conference in March. During this conference a regional bureau of Muslim organizations of the Russian Bolshevik Party was formed. The Bolshevik Party continued to try to gain support among the native population by giving them the impression of better representation for the Central Asian population and throughout the end of the year were able to maintain harmony with the Central Asian people.

Communication difficulties with Red Army forces in Siberia and European Russia ceased to be a problem by mid-November 1919. Due to Red Army successes north of Central Asia, communication with Moscow was re-established and the Bolsheviks were able to claim victory over the White Army in Turkestan.

By the beginning of 1920 the main body of the Armed Forces of South Russia was rapidly retreating towards the Don, to Rostov. Denikin hoped to hold the crossings of the Don, then rest and reform his troops, but the White Army was not able to hold the Don area and at the end of February 1920, started a retreat across Kuban towards Novorossiysk. Slipshod evacuation of Novorossiysk proved to be a dark event for the White Army. About 40,000 men were evacuated by Russian and Allied ships from Novorossiysk to the Crimea, without horses or any heavy equipment, while about 20,000 men were left behind and either dispersed or captured by the Red Army. Following the disastrous Novorossiysk evacuation, Denikin stepped down and the military council elected Wrangel as the new Commander-in-Chief of the White Army. He was able to restore order to the dispirited troops and reshape an army that could fight as a regular force again. This remained an organized force in the Crimea throughout 1920.

After Moscow's Bolshevik government signed a military and political alliance with Nestor Makhno and the Ukrainian anarchists, the Black Army attacked and defeated several regiments of Wrangel's troops in southern Ukraine, forcing him to retreat before he could capture that year's grain harvest. Stymied in his efforts to consolidate his hold, Wrangel then attacked north in an attempt to take advantage of recent Red Army defeats at the close of the Polish–Soviet War of 1919–1920. This offensive was eventually halted by the Red Army, and Wrangel's troops were forced to retreat to the Crimea in November 1920 pursued by both the Red and Black cavalry and infantry. Wrangel and the remains of his army were evacuated from the Crimea to Constantinople on 14 November 1920. Thus ended the struggle of Reds and Whites in Southern Russia.

After the defeat of Wrangel, the Red Army immediately repudiated its 1920 treaty of alliance with Nestor Makhno and attacked the anarchist Black Army; the campaign to liquidate Makhno and the Ukrainian anarchists began with an attempted assassination of Makhno by Cheka agents. Angered by continued repression by the Bolshevik Communist government and its liberal use of the Cheka to put down anarchist elements, a naval mutiny erupted at Kronstadt, followed by peasant revolts. Red Army attacks on the anarchist forces and their sympathizers increased in ferocity throughout 1921.

In Siberia, Adm. Kolchak's army had disintegrated. He himself gave up command after the loss of Omsk and designated Gen. Grigory Semyonov as the new leader of the White Army in Siberia. Not long after this Kolchak was arrested by the disaffected Czechoslovak Corps as he traveled towards Irkutsk without the protection of the army, and turned over to the socialist Political Centre in Irkutsk. Six days later this regime was replaced by a Bolshevik-dominated Military-Revolutionary Committee. On 6–7 February Kolchak and his prime minister Victor Pepelyaev were shot and their bodies thrown through the ice of the frozen Angara River, just before the arrival of the White Army in the area.

Remnants of Kolchak's army reached Transbaikalia and joined Semyonov's troops, forming the Far Eastern army. With the support of the Japanese army it was able to hold Chita, but after withdrawal of Japanese soldiers from Transbaikalia, Semenov's position became untenable, and in November 1920 he was driven by the Red Army from Transbaikalia and took refuge in China. The Japanese, who had plans to annex the Amur Krai, finally pulled their troops out as Bolshevik forces gradually asserted control over the Russian Far East. On 25 October 1922 Vladivostok fell to the Red Army, and the Provisional Priamur Government was extinguished.

In central Asia Red Army troops continued to face resistance into 1923, where "basmachi" (armed bands of Islamic guerrillas) had formed to fight the Bolshevik takeover. The Soviets engaged non-Russian peoples in Central Asia, like Magaza Masanchi, commander of the Dungan Cavalry Regiment, to fight against the Basmachis. The Communist Party did not completely dismantle this group until 1934.

Gen. Anatoly Pepelyayev continued armed resistance in the Ayano-Maysky District until June 1923. The regions of Kamchatka and Northern Sakhalin remained under Japanese occupation until their treaty with the Soviet Union in 1925, when their forces were finally withdrawn.

The results of the civil war were momentous. Soviet demographer Boris Urlanis estimated the total number of men killed in action in the Civil War and Polish–Soviet War as 300,000 (125,000 in the Red Army, 175,500 White armies and Poles) and the total number of military personnel dead from disease (on both sides) as 450,000.
During the Red Terror, estimates of Cheka executions range from 12,733 to 1.7 million. William Henry Chamberlin suspected that there were about 50,000. Evan Mawdsley suspected that there were more than 12,733, and less than 200,000. Some sources claimed at least 250,000 summary executions of "enemies of the people" with estimates reaching above a million. More modest estimates put the numbers executed by the Bolsheviks between December 1917 and February 1922 at around 28,000 per year, with roughly 10,000 executions during the Red Terror.

Some 300,000–500,000 Cossacks were killed or deported during decossackization, out of a population of around three million. An estimated 100,000 Jews were killed in Ukraine, mostly by the White Army. Punitive organs of the All Great Don Cossack Host sentenced 25,000 people to death between May 1918 and January 1919. Kolchak's government shot 25,000 people in Ekaterinburg province alone. "White terror" killed about 300,000 people in total.

At the end of the Civil War the Russian SFSR was exhausted and near ruin. The droughts of 1920 and 1921, as well as the 1921 famine, worsened the disaster still further. Disease had reached pandemic proportions, with 3,000,000 dying of typhus alone in 1920. Millions more also died of widespread starvation, wholesale massacres by both sides and pogroms against Jews in Ukraine and southern Russia. By 1922 there were at least 7,000,000 street children in Russia as a result of nearly ten years of devastation from the Great War and the civil war.

Another one to two million people, known as the White émigrés, fled Russia, many with Gen. Wrangel—some through the Far East, others west into the newly independent Baltic countries. These émigrés included a large percentage of the educated and skilled population of Russia.

The Russian economy was devastated by the war, with factories and bridges destroyed, cattle and raw materials pillaged, mines flooded and machines damaged. The industrial production value descended to one-seventh of the value of 1913 and agriculture to one-third. According to "Pravda", "The workers of the towns and some of the villages choke in the throes of hunger. The railways barely crawl. The houses are crumbling. The towns are full of refuse. Epidemics spread and death strikes—industry is ruined." It is estimated that the total output of mines and factories in 1921 had fallen to 20% of the pre-World War level, and many crucial items experienced an even more drastic decline. For example, cotton production fell to 5%, and iron to 2%, of pre-war levels.

War Communism saved the Soviet government during the Civil War, but much of the Russian economy had ground to a standstill. The peasants responded to requisitions by refusing to till the land. By 1921 cultivated land had shrunk to 62% of the pre-war area, and the harvest yield was only about 37% of normal. The number of horses declined from 35 million in 1916 to 24 million in 1920 and cattle from 58 to 37 million. The exchange rate with the US dollar declined from two rubles in 1914 to 1,200 in 1920.

With the end of the war the Communist Party no longer faced an acute military threat to its existence and power. However, the perceived threat of another intervention, combined with the failure of socialist revolutions in other countries—most notably the German Revolution—contributed to the continued militarization of Soviet society. Although Russia experienced extremely rapid economic growth in the 1930s, the combined effect of World War I and the Civil War left a lasting scar on Russian society and had permanent effects on the development of the Soviet Union.

British historian Orlando Figes has contended that the root of the Whites' defeat was their inability to dispel the popular image that they were dually associated with Tsarist Russia and supportive of a Tsarist restoration.









</doc>
<doc id="26296" url="https://en.wikipedia.org/wiki?curid=26296" title="Ralph Abercromby">
Ralph Abercromby

Sir Ralph Abercromby (sometimes spelt Abercrombie) (7 October 173428 March 1801) was a Scottish soldier and politician. He rose to the rank of lieutenant-general in the British Army, was noted for his services during the Napoleonic Wars, and served as Commander-in-Chief, Ireland.

He twice served as MP for Clackmannanshire, and he was appointed Governor of Trinidad.

He was the eldest son of George Abercromby of Tullibody, Clackmannanshire, and a brother of the advocate Alexander Abercromby, Lord Abercromby and General Sir Robert Abercromby. He was born at Menstrie Castle, Clackmannanshire. His mother was the daughter of Mr. Dundas of Manor in Stirlingshire.

Ralph Abercromby's education, begun by a private tutor, was continued at the school of Mr. Moir at Alloa, then considered one of the best in Scotland despite its Jacobite leanings. After passing some time there, Ralph was sent to Rugby, where he remained till he was eighteen, then becoming a student at the University of Edinburgh. In Edinburgh, he studied moral and natural philosophy and civil law and was regarded by his professors as sound rather than brilliant. He was sent to Leipzig University in 1754 to study civil law with a view to career as an advocate.

Abercromby was a Freemason. He was a member of Canongate Kilwinning Lodge No 2, Edinburgh, Scotland.

On returning from the continent, Abercromby expressed a strong preference for the military profession, and a cornet's commission was accordingly obtained for him (March 1756) in the 3rd Dragoon Guards. He served with his regiment in the Seven Years' War, and thus, the opportunity afforded him of studying the methods of Frederick the Great, who moulded his military character and formed his tactical ideas.

He rose through the intermediate grades to the rank of lieutenant-colonel of the regiment (1773) and brevet colonel in 1780, and in 1781, he became colonel of the newly raised King's Irish infantry. When that regiment was disbanded in 1783, he retired upon half pay. He also entered Parliament as MP for Clackmannanshire (1774–1780).

He was a strong supporter of the American cause in the American Revolutionary War, and remained in Ireland to avoid having to fight against the colonists.

When France declared war against Great Britain in 1793, he resumed his duties. He was appointed command of a brigade under the Duke of York for service in the Netherlands, where he commanded the advanced guard in the action at Le Cateau. During the 1794 withdrawal to Holland, he commanded the allied forces in the action at Boxtel and was wounded directing operations at Fort St Andries on the Waal. In 1795, he was appointed a Knight of the Bath for his services.

That same year, he was appointed to succeed Sir Charles Grey as commander-in-chief of the British forces in the West Indies. In 1796, Grenada was suddenly attacked and taken by a detachment of the army under his orders. Afterwards, Abercromby secured possession of the settlements of Demerara and Essequibo in South America, the islands of Saint Lucia, Saint Vincent and Trinidad. A major assault on the port of San Juan, Puerto Rico, in April 1797 ended in disaster after fierce fighting where both sides suffered heavy losses. 

Abercromby returned to Europe and, in reward for his services, was appointed colonel of the 2nd (Royal North British) Regiment of Dragoons. He was also made Lieutenant-Governor of the Isle of Wight, Governor of Fort George and Fort Augustus in the Scottish Highlands, and promoted to the rank of Lieutenant-general. He again entered Parliament as member for Clackmannanshire from 1796 to 1798.
From 1797 to 1798, he was Commander-in-Chief of the forces in Ireland.

To quote the biographic entry in the 1888 Encyclopædia Britannica, 

After holding for a short period the office of commander-in-chief in Scotland, Sir Ralph, when the enterprise against the Dutch Batavian Republic was resolved upon in 1799, was again called to command under the Duke of York. The Anglo-Russian invasion of Holland in 1799 ended in disaster, but friend and foe alike confessed that the most decisive victory could not have more conspicuously proved the talents of this distinguished officer.

In 1801, he was sent with an army to recover Egypt from France. His experience in the Netherlands and the West Indies particularly fitted him for this new command, as was proved when he carried his army in health, in spirits, and with the requisite supplies to the destined scene of action despite great difficulties. The debarkation of the troops at Abukir, in the face of strenuous opposition, is justly ranked among the most daring and brilliant exploits of the British army.

In 1800 he commanded the expedition to the Mediterranean, and after some brilliant operations defeated the French in the Battle of Alexandria, March 21, 1801. During the action he was struck by a musket-ball in the thigh; but not until the battle was won and he saw the enemy retreating did he show any sign of pain. He was borne from the field in a hammock, cheered by the blessings of the soldiers as he passed, and conveyed on board the flag-ship HMS "Foudroyant" which was moored in the harbour. The ball could not be extracted; mortification ensued, and seven days later, on March 28, 1801, he died.

His old friend and commander, the Duke of York, paid tribute to Abercromby's memory in general orders: "His steady observance of discipline, his ever-watchful attention to the health and wants of his troops, the persevering and unconquerable spirit which marked his military career, the splendour of his actions in the field and the heroism of his death, are worthy the imitation of all who desire, like him, a life of heroism and a death of glory." He was buried on St. John's Bastion within Fort Saint Elmo in Valletta, Malta. The British military renamed it "Abercrombie's Bastion" in his honour. The adjacent curtain wall linking this bastion to the fortifications of Valletta, originally called Santa Ubaldesca Curtain, was also renamed "Abercrombie's Curtain".

By a vote of the House of Commons, a monument was erected in his honour in St Paul's Cathedral in Abercromby Square, Liverpool. His widow was created Baroness Abercromby of Tullibody and Aboukir Bay, and a pension of £2,000 a year was settled on her and her two successors in the title.

On 17 November 1767, Abercromby married Mary Anne, daughter of John Menzies and Ann, daughter of Patrick Campbell. They had seven children. Of four sons, all four entered Parliament, and two saw military service.

A public house in central Manchester, the 'Sir Ralph Abercromby', is named after him. There is also a 'General Abercrombie' pub with his portrait by John Hoppner as the sign off of the Blackfriars Bridge Road in London.

Three ships have been named HMS "Abercrombie" after the general but using the variant spelling of his name.

Abercrombie Street in Port of Spain, Trinidad honours his name.

Abercromby Primary School in Tullibody is named after him.






</doc>
<doc id="26297" url="https://en.wikipedia.org/wiki?curid=26297" title="Ripe">
Ripe

Ripe or RIPE may refer to:


Persons with the name Ripe:



</doc>
<doc id="26298" url="https://en.wikipedia.org/wiki?curid=26298" title="Radiometric dating">
Radiometric dating

Radiometric dating or radioactive dating is a technique used to date materials such as rocks or carbon, in which trace radioactive impurities were selectively incorporated when they were formed. The method compares the abundance of a naturally occurring radioactive isotope within the material to the abundance of its decay products, which form at a known constant rate of decay. The use of radiometric dating was first published in 1907 by Bertram Boltwood and is now the principal source of information about the absolute age of rocks and other geological features, including the age of fossilized life forms or the age of the Earth itself, and can also be used to date a wide range of natural and man-made materials.

Together with stratigraphic principles, radiometric dating methods are used in geochronology to establish the geologic time scale. Among the best-known techniques are radiocarbon dating, potassium–argon dating and uranium–lead dating. By allowing the establishment of geological timescales, it provides a significant source of information about the ages of fossils and the deduced rates of evolutionary change. Radiometric dating is also used to date archaeological materials, including ancient artifacts.

Different methods of radiometric dating vary in the timescale over which they are accurate and the materials to which they can be applied.

All ordinary matter is made up of combinations of chemical elements, each with its own atomic number, indicating the number of protons in the atomic nucleus. Additionally, elements may exist in different isotopes, with each isotope of an element differing in the number of neutrons in the nucleus. A particular isotope of a particular element is called a nuclide. Some nuclides are inherently unstable. That is, at some point in time, an atom of such a nuclide will undergo radioactive decay and spontaneously transform into a different nuclide. This transformation may be accomplished in a number of different ways, including alpha decay (emission of alpha particles) and beta decay (electron emission, positron emission, or electron capture). Another possibility is spontaneous fission into two or more nuclides.

While the moment in time at which a particular nucleus decays is unpredictable, a collection of atoms of a radioactive nuclide decays exponentially at a rate described by a parameter known as the half-life, usually given in units of years when discussing dating techniques. After one half-life has elapsed, one half of the atoms of the nuclide in question will have decayed into a "daughter" nuclide or decay product. In many cases, the daughter nuclide itself is radioactive, resulting in a decay chain, eventually ending with the formation of a stable (nonradioactive) daughter nuclide; each step in such a chain is characterized by a distinct half-life. In these cases, usually the half-life of interest in radiometric dating is the longest one in the chain, which is the rate-limiting factor in the ultimate transformation of the radioactive nuclide into its stable daughter. Isotopic systems that have been exploited for radiometric dating have half-lives ranging from only about 10 years (e.g., tritium) to over 100 billion years (e.g., samarium-147).

For most radioactive nuclides, the half-life depends solely on nuclear properties and is essentially a constant. It is not affected by external factors such as temperature, pressure, chemical environment, or presence of a magnetic or electric field. The only exceptions are nuclides that decay by the process of electron capture, such as beryllium-7, strontium-85, and zirconium-89, whose decay rate may be affected by local electron density. For all other nuclides, the proportion of the original nuclide to its decay products changes in a predictable way as the original nuclide decays over time. This predictability allows the relative abundances of related nuclides to be used as a clock to measure the time from the incorporation of the original nuclides into a material to the present.

The basic equation of radiometric dating requires that neither the parent nuclide nor the daughter product can enter or leave the material after its formation. The possible confounding effects of contamination of parent and daughter isotopes have to be considered, as do the effects of any loss or gain of such isotopes since the sample was created. It is therefore essential to have as much information as possible about the material being dated and to check for possible signs of alteration. Precision is enhanced if measurements are taken on multiple samples from different locations of the rock body. Alternatively, if several different minerals can be dated from the same sample and are assumed to be formed by the same event and were in equilibrium with the reservoir when they formed, they should form an isochron. This can reduce the problem of contamination. In uranium–lead dating, the concordia diagram is used which also decreases the problem of nuclide loss. Finally, correlation between different isotopic dating methods may be required to confirm the age of a sample. For example, the age of the Amitsoq gneisses from western Greenland was determined to be using uranium–lead dating and using lead–lead dating, results that are consistent with each other.

Accurate radiometric dating generally requires that the parent has a long enough half-life that it will be present in significant amounts at the time of measurement (except as described below under "Dating with short-lived extinct radionuclides"), the half-life of the parent is accurately known, and enough of the daughter product is produced to be accurately measured and distinguished from the initial amount of the daughter present in the material. The procedures used to isolate and analyze the parent and daughter nuclides must be precise and accurate. This normally involves isotope-ratio mass spectrometry.

The precision of a dating method depends in part on the half-life of the radioactive isotope involved. For instance, carbon-14 has a half-life of 5,730 years. After an organism has been dead for 60,000 years, so little carbon-14 is left that accurate dating cannot be established. On the other hand, the concentration of carbon-14 falls off so steeply that the age of relatively young remains can be determined precisely to within a few decades.

If a material that selectively rejects the daughter nuclide is heated, any daughter nuclides that have been accumulated over time will be lost through diffusion, setting the isotopic "clock" to zero. The temperature at which this happens is known as the closure temperature or blocking temperature and is specific to a particular material and isotopic system. These temperatures are experimentally determined in the lab by artificially resetting sample minerals using a high-temperature furnace. As the mineral cools, the crystal structure begins to form and diffusion of isotopes is less easy. At a certain temperature, the crystal structure has formed sufficiently to prevent diffusion of isotopes. This temperature is what is known as closure temperature and represents the temperature below which the mineral is a closed system to isotopes. Thus an igneous or metamorphic rock or melt, which is slowly cooling, does not begin to exhibit measurable radioactive decay until it cools below the closure temperature. The age that can be calculated by radiometric dating is thus the time at which the rock or mineral cooled to closure temperature. Dating of different minerals and/or isotope systems (with differing closure temperatures) within the same rock can therefore enable the tracking of the thermal history of the rock in question with time, and thus the history of metamorphic events may become known in detail. This field is known as thermochronology or thermochronometry.

The mathematical expression that relates radioactive decay to geologic time is

where

The equation is most conveniently expressed in terms of the measured quantity "N"("t") rather than the constant initial value "N".

The above equation makes use of information on the composition of parent and daughter isotopes at the time the material being tested cooled below its closure temperature. This is well-established for most isotopic systems. However, construction of an isochron does not require information on the original compositions, using merely the present ratios of the parent and daughter isotopes to a standard isotope. Plotting an isochron is used to solve the age equation graphically and calculate the age of the sample and the original composition.

Radiometric dating has been carried out since 1905 when it was invented by Ernest Rutherford as a method by which one might determine the age of the Earth. In the century since then the techniques have been greatly improved and expanded. Dating can now be performed on samples as small as a nanogram using a mass spectrometer. The mass spectrometer was invented in the 1940s and began to be used in radiometric dating in the 1950s. It operates by generating a beam of ionized atoms from the sample under test. The ions then travel through a magnetic field, which diverts them into different sampling sensors, known as "Faraday cups", depending on their mass and level of ionization. On impact in the cups, the ions set up a very weak current that can be measured to determine the rate of impacts and the relative concentrations of different atoms in the beams.

Uranium–lead radiometric dating involves using uranium-235 or uranium-238 to date a substance's absolute age. This scheme has been refined to the point that the error margin in dates of rocks can be as low as less than two million years in two-and-a-half billion years. An error margin of 2–5% has been achieved on younger Mesozoic rocks.

Uranium–lead dating is often performed on the mineral zircon (ZrSiO), though it can be used on other materials, such as baddeleyite, as well as monazite (see: monazite geochronology). Zircon and baddeleyite incorporate uranium atoms into their crystalline structure as substitutes for zirconium, but strongly reject lead. Zircon has a very high closure temperature, is resistant to mechanical weathering and is very chemically inert. Zircon also forms multiple crystal layers during metamorphic events, which each may record an isotopic age of the event. "In situ" micro-beam analysis can be achieved via laser ICP-MS or SIMS techniques.

One of its great advantages is that any sample provides two clocks, one based on uranium-235's decay to lead-207 with a half-life of about 700 million years, and one based on uranium-238's decay to lead-206 with a half-life of about 4.5 billion years, providing a built-in crosscheck that allows accurate determination of the age of the sample even if some of the lead has been lost. This can be seen in the concordia diagram, where the samples plot along an errorchron (straight line) which intersects the concordia curve at the age of the sample.

This involves the alpha decay of Sm to Nd with a half-life of 1.06 x 10 years. Accuracy levels of within twenty million years in ages of two-and-a-half billion years are achievable.

This involves electron capture or positron decay of potassium-40 to argon-40. Potassium-40 has a half-life of 1.3 billion years, and so this method is applicable to the oldest rocks. Radioactive potassium-40 is common in micas, feldspars, and hornblendes, though the closure temperature is fairly low in these materials, about 350 °C (mica) to 500 °C (hornblende).

This is based on the beta decay of rubidium-87 to strontium-87, with a half-life of 50 billion years. This scheme is used to date old igneous and metamorphic rocks, and has also been used to date lunar samples. Closure temperatures are so high that they are not a concern. Rubidium-strontium dating is not as precise as the uranium-lead method, with errors of 30 to 50 million years for a 3-billion-year-old sample.

A relatively short-range dating technique is based on the decay of uranium-234 into thorium-230, a substance with a half-life of about 80,000 years. It is accompanied by a sister process, in which uranium-235 decays into protactinium-231, which has a half-life of 32,760 years.

While uranium is water-soluble, thorium and protactinium are not, and so they are selectively precipitated into ocean-floor sediments, from which their ratios are measured. The scheme has a range of several hundred thousand years. A related method is ionium–thorium dating, which measures the ratio of ionium (thorium-230) to thorium-232 in ocean sediment.

 Radiocarbon dating is also simply called Carbon-14 dating. Carbon-14 is a radioactive isotope of carbon, with a half-life of 5,730 years, (which is very short compared with the above isotopes) and decays into nitrogen. In other radiometric dating methods, the heavy parent isotopes were produced by nucleosynthesis in supernovas, meaning that any parent isotope with a short half-life should be extinct by now. Carbon-14, though, is continuously created through collisions of neutrons generated by cosmic rays with nitrogen in the upper atmosphere and thus remains at a near-constant level on Earth. The carbon-14 ends up as a trace component in atmospheric carbon dioxide (CO).

A carbon-based life form acquires carbon during its lifetime. Plants acquire it through photosynthesis, and animals acquire it from consumption of plants and other animals. When an organism dies, it ceases to take in new carbon-14, and the existing isotope decays with a characteristic half-life (5730 years). The proportion of carbon-14 left when the remains of the organism are examined provides an indication of the time elapsed since its death. This makes carbon-14 an ideal dating method to date the age of bones or the remains of an organism. The carbon-14 dating limit lies around 58,000 to 62,000 years.

The rate of creation of carbon-14 appears to be roughly constant, as cross-checks of carbon-14 dating with other dating methods show it gives consistent results. However, local eruptions of volcanoes or other events that give off large amounts of carbon dioxide can reduce local concentrations of carbon-14 and give inaccurate dates. The releases of carbon dioxide into the biosphere as a consequence of industrialization have also depressed the proportion of carbon-14 by a few percent; conversely, the amount of carbon-14 was increased by above-ground nuclear bomb tests that were conducted into the early 1960s. Also, an increase in the solar wind or the Earth's magnetic field above the current value would depress the amount of carbon-14 created in the atmosphere.

This involves inspection of a polished slice of a material to determine the density of "track" markings left in it by the spontaneous fission of uranium-238 impurities. The uranium content of the sample has to be known, but that can be determined by placing a plastic film over the polished slice of the material, and bombarding it with slow neutrons. This causes induced fission of U, as opposed to the spontaneous fission of U. The fission tracks produced by this process are recorded in the plastic film. The uranium content of the material can then be calculated from the number of tracks and the neutron flux.

This scheme has application over a wide range of geologic dates. For dates up to a few million years micas, tektites (glass fragments from volcanic eruptions), and meteorites are best used. Older materials can be dated using zircon, apatite, titanite, epidote and garnet which have a variable amount of uranium content. Because the fission tracks are healed by temperatures over about 200 °C the technique has limitations as well as benefits. The technique has potential applications for detailing the thermal history of a deposit.

Large amounts of otherwise rare Cl (half-life ~300ky) were produced by irradiation of seawater during atmospheric detonations of nuclear weapons between 1952 and 1958. The residence time of Cl in the atmosphere is about 1 week. Thus, as an event marker of 1950s water in soil and ground water, Cl is also useful for dating waters less than 50 years before the present. Cl has seen use in other areas of the geological sciences, including dating ice and sediments.

Luminescence dating methods are not radiometric dating methods in that they do not rely on abundances of isotopes to calculate age. Instead, they are a consequence of background radiation on certain minerals. Over time, ionizing radiation is absorbed by mineral grains in sediments and archaeological materials such as quartz and potassium feldspar. The radiation causes charge to remain within the grains in structurally unstable "electron traps". Exposure to sunlight or heat releases these charges, effectively "bleaching" the sample and resetting the clock to zero. The trapped charge accumulates over time at a rate determined by the amount of background radiation at the location where the sample was buried. Stimulating these mineral grains using either light (optically stimulated luminescence or infrared stimulated luminescence dating) or heat (thermoluminescence dating) causes a luminescence signal to be emitted as the stored unstable electron energy is released, the intensity of which varies depending on the amount of radiation absorbed during burial and specific properties of the mineral.

These methods can be used to date the age of a sediment layer, as layers deposited on top would prevent the grains from being "bleached" and reset by sunlight. Pottery shards can be dated to the last time they experienced significant heat, generally when they were fired in a kiln.

Other methods include:

Absolute radiometric dating requires a measurable fraction of parent nucleus to remain in the sample rock. For rocks dating back to the beginning of the solar system, this requires extremely long-lived parent isotopes, making measurement of such rocks' exact ages imprecise. To be able to distinguish the relative ages of rocks from such old material, and to get a better time resolution than that available from long-lived isotopes, short-lived isotopes that are no longer present in the rock can be used.

At the beginning of the solar system, there were several relatively short-lived radionuclides like Al, Fe, Mn, and I present within the solar nebula. These radionuclides—possibly produced by the explosion of a supernova—are extinct today, but their decay products can be detected in very old material, such as that which constitutes meteorites. By measuring the decay products of extinct radionuclides with a mass spectrometer and using isochronplots, it is possible to determine relative ages of different events in the early history of the solar system. Dating methods based on extinct radionuclides can also be calibrated with the U-Pb method to give absolute ages. Thus both the approximate age and a high time resolution can be obtained. Generally a shorter half-life leads to a higher time resolution at the expense of timescale.

I beta-decays to Xe with a half-life of 16 million years. The iodine-xenon chronometer is an isochron technique. Samples are exposed to neutrons in a nuclear reactor. This converts the only stable isotope of iodine (I) into Xe via neutron capture followed by beta decay (of I). After irradiation, samples are heated in a series of steps and the xenon isotopic signature of the gas evolved in each step is analysed. When a consistent Xe/Xe ratio is observed across several consecutive temperature steps, it can be interpreted as corresponding to a time at which the sample stopped losing xenon.

Samples of a meteorite called Shallowater are usually included in the irradiation to monitor the conversion efficiency from I to Xe. The difference between the measured Xe/Xe ratios of the sample and Shallowater then corresponds to the different ratios of I/I when they each stopped losing xenon. This in turn corresponds to a difference in age of closure in the early solar system.

Another example of short-lived extinct radionuclide dating is the Al – Mg chronometer, which can be used to estimate the relative ages of chondrules. Al decays to Mg with a half-life of 720 000 years. The dating is simply a question of finding the deviation from the natural abundance of Mg (the product of Al decay) in comparison with the ratio of the stable isotopes Al/Mg.

The excess of Mg (often designated Mg* ) is found by comparing the Mg/Mg ratio to that of other Solar System materials.

The Al – Mg chronometer gives an estimate of the time period for formation of primitive meteorites of only a few million years (1.4 million years for Chondrule formation).




</doc>
<doc id="26301" url="https://en.wikipedia.org/wiki?curid=26301" title="Rocket">
Rocket

A rocket (from Italian "rocchetto" "bobbin") is a missile, spacecraft, aircraft or other vehicle that obtains thrust from a rocket engine. Rocket engine exhaust is formed entirely from propellant carried within the rocket before use. Rocket engines work by action and reaction and push rockets forward simply by expelling their exhaust in the opposite direction at high speed, and can therefore work in the vacuum of space.

In fact, rockets work more efficiently in space than in an atmosphere. Multistage rockets are capable of attaining escape velocity from Earth and therefore can achieve unlimited maximum altitude. Compared with airbreathing engines, rockets are lightweight and powerful and capable of generating large accelerations. To control their flight, rockets rely on momentum, airfoils, auxiliary reaction engines, gimballed thrust, momentum wheels, deflection of the exhaust stream, propellant flow, spin, and/or gravity.

Rockets for military and recreational uses date back to at least 13th century China. Significant scientific, interplanetary and industrial use did not occur until the 20th century, when rocketry was the enabling technology for the Space Age, including setting foot on the Earth's moon. Rockets are now used for fireworks, weaponry, ejection seats, launch vehicles for artificial satellites, human spaceflight, and space exploration.

Chemical rockets are the most common type of high power rocket, typically creating a high speed exhaust by the combustion of fuel with an oxidizer. The stored propellant can be a simple pressurized gas or a single liquid fuel that disassociates in the presence of a catalyst (monopropellants), two liquids that spontaneously react on contact (hypergolic propellants), two liquids that must be ignited to react, a solid combination of fuel with oxidizer (solid fuel), or solid fuel with liquid oxidizer (hybrid propellant system). Chemical rockets store a large amount of energy in an easily released form, and can be very dangerous. However, careful design, testing, construction and use minimizes risks.

The first gunpowder-powered rockets evolved in medieval China under the Song dynasty by the 13th century. The Mongols adopted Chinese rocket technology and the invention spread via the Mongol invasions to the Middle East and to Europe in the mid-13th century. Rockets are recorded in use by the Song navy in a military exercise dated to 1245. Internal-combustion rocket propulsion is mentioned in a reference to 1264, recording that the "ground-rat", a type of firework, had frightened the Empress-Mother Gongsheng at a feast held in her honor by her son the Emperor Lizong. Subsequently, rockets are included in the military treatise "Huolongjing", also known as the Fire Drake Manual, written by the Chinese artillery officer Jiao Yu in the mid-14th century. This text mentions the first known multistage rocket, the 'fire-dragon issuing from the water' (huo long chu shui), thought to have been used by the Chinese navy.

Medieval and early modern rockets were used militarily as incendiary weapons in sieges. Between 1270 and 1280, Hasan al-Rammah wrote "al-furusiyyah wa al-manasib al-harbiyya" ("The Book of Military Horsemanship and Ingenious War Devices"), which included 107 gunpowder recipes, 22 of them for rockets. 
In Europe, Konrad Kyeser described rockets in his military treatise "Bellifortis" around 1405.

The name "rocket" comes from the Italian "rocchetta", meaning "bobbin" or "little spindle", given due to the similarity in shape to the bobbin or spool used to hold the thread to be fed to a spinning wheel.
Leonhard Fronsperger and Conrad Haas adopted the Italian term into German in the mid-16th century; "rocket" appears in English by the early 17th century.
"Artis Magnae Artilleriae pars prima", an important early modern work on rocket artillery, by Kazimierz Siemienowicz, was first printed in Amsterdam in 1650.

The first iron-cased rockets were developed in the late 18th century in the Kingdom of Mysore (part of present-day India) by Tipu Sultan. The congreve rocket was a British military weapon designed and developed by Sir William Congreve in 1804, based directly on Mysorean rockets.

In 1814 Francis Scott Key wrote of the "rockets' red glare" while held captive on a British ship that was laying siege to Fort McHenry. The rockets he witnessed were those of William Congreve, who built a compressed-powder rocket encased in metal, increasing the effective range from 100 to 2,000 yards, first used in the Napoleonic Wars. The first mathematical treatment of the dynamics of rocket propulsion is due to William Moore (1813). In 1815 Alexander Dmitrievich Zasyadko constructed rocket-launching platforms, which allowed rockets to be fired in salvos (6 rockets at a time), and gun-laying devices.

William Hale in 1844 greatly increased the accuracy of rocket artillery.Edward Mounier Boxer further improved the Congreve rocket in 1865.

Konstantin Tsiolkovsky (1903) first speculated on the possibility of using rocket technology for manned spaceflight.
Robert Goddard in 1920 published proposed improvements to rocket technology in "A Method of Reaching Extreme Altitudes". In 1923, Hermann Oberth (1894–1989) published "Die Rakete zu den Planetenräumen" ("The Rocket into Planetary Space")

Modern rockets originated in 1926 when Goddard attached a supersonic (de Laval) nozzle to the combustion chamber of a liquid-propellant rocket. These nozzles turn the hot gas from the combustion chamber into a cooler, hypersonic, highly directed jet of gas, more than doubling the thrust and raising the engine efficiency from 2% to 64%.
Use of liquid propellants instead of gunpowder greatly improved the effectiveness of rocket artillery in World War II, and opened up the possibility of manned spaceflight after 1945.

In 1943 production of the V-2 rocket began in Germany. 
In parallel with the German guided-missile programme, rockets were also used on aircraft, either for assisting horizontal take-off (RATO), vertical take-off (Bachem Ba 349 "Natter") or for powering them (Me 163, see list of World War II guided missiles of Germany). The Allies' rocket programs were less technological, relying mostly on unguided missiles like the Soviet Katyusha rocket.
The Americans captured a large number of German rocket scientists, including Wernher von Braun, in 1945, and brought them to the United States as part of Operation Paperclip. After World War II scientists used rockets to study high-altitude conditions, by radio telemetry of temperature and pressure of the atmosphere, detection of cosmic rays, and further techniques; note too the Bell X-1, the first manned vehicle to break the sound barrier (1947). Independently, in the Soviet Union's space program research continued under the leadership of the chief designer Sergei Korolev (1907-1966).

During the Cold War rockets became extremely important militarily with the development of modern intercontinental ballistic missiles (ICBMs).
The 1960s saw rapid development of rocket technology, particularly in the Soviet Union (Vostok, Soyuz, Proton) and in the United States (e.g. the X-15). Rockets came into use for space exploration. American manned programs (Project Mercury, Project Gemini and later the Apollo programme) culminated in 1969 with the first manned landing on the moon - using equipment launched by the Saturn V rocket.


Rocket vehicles are often constructed in the archetypal tall thin "rocket" shape that takes off vertically, but there are actually many different types of rockets including:

A rocket design can be as simple as a cardboard tube filled with black powder, but to make an efficient, accurate rocket or missile involves overcoming a number of difficult problems. The main difficulties include cooling the combustion chamber, pumping the fuel (in the case of a liquid fuel), and controlling and correcting the direction of motion.

Rockets consist of a propellant, a place to put propellant (such as a propellant tank), and a nozzle. They may also have one or more rocket engines, directional stabilization device(s) (such as fins, vernier engines or engine gimbals for thrust vectoring, gyroscopes) and a structure (typically monocoque) to hold these components together. Rockets intended for high speed atmospheric use also have an aerodynamic fairing such as a nose cone, which usually holds the payload.

As well as these components, rockets can have any number of other components, such as wings (rocketplanes), parachutes, wheels (rocket cars), even, in a sense, a person (rocket belt). Vehicles frequently possess navigation systems and guidance systems that typically use satellite navigation and inertial navigation systems.

Rocket engines employ the principle of jet propulsion. The rocket engines powering rockets come in a great variety of different types; a comprehensive list can be found in rocket engine. Most current rockets are chemically powered rockets (usually internal combustion engines, but some employ a decomposing monopropellant) that emit a hot exhaust gas. A rocket engine can use gas propellants, solid propellant, liquid propellant, or a hybrid mixture of both solid and liquid. Some rockets use heat or pressure that is supplied from a source other than the chemical reaction of propellant(s), such as steam rockets, solar thermal rockets, nuclear thermal rocket engines or simple pressurized rockets such as water rocket or cold gas thrusters. With combustive propellants a chemical reaction is initiated between the fuel and the oxidizer in the combustion chamber, and the resultant hot gases accelerate out of a rocket engine nozzle (or nozzles) at the rearward-facing end of the rocket. The acceleration of these gases through the engine exerts force ("thrust") on the combustion chamber and nozzle, propelling the vehicle (according to Newton's Third Law). This actually happens because the force (pressure times area) on the combustion chamber wall is unbalanced by the nozzle opening; this is not the case in any other direction. The shape of the nozzle also generates force by directing the exhaust gas along the axis of the rocket.

Rocket propellant is mass that is stored, usually in some form of propellant tank or casing, prior to being used as the propulsive mass that is ejected from a rocket engine in the form of a fluid jet to produce thrust. For chemical rockets often the propellants are a fuel such as liquid hydrogen or kerosene burned with an oxidizer such as liquid oxygen or nitric acid to produce large volumes of very hot gas. The oxidiser is either kept separate and mixed in the combustion chamber, or comes premixed, as with solid rockets.

Sometimes the propellant is not burned but still undergoes a chemical reaction, and can be a 'monopropellant' such as hydrazine, nitrous oxide or hydrogen peroxide that can be catalytically decomposed to hot gas.

Alternatively, an inert propellant can be used that can be externally heated, such as in steam rocket, solar thermal rocket or nuclear thermal rockets.

For smaller, low performance rockets such as attitude control thrusters where high performance is less necessary, a pressurised fluid is used as propellant that simply escapes the spacecraft through a propelling nozzle.

Rockets or other similar reaction devices carrying their own propellant must be used when there is no other substance (land, water, or air) or force (gravity, magnetism, light) that a vehicle may usefully employ for propulsion, such as in space. In these circumstances, it is necessary to carry all the propellant to be used.

However, they are also useful in other situations:

Some military weapons use rockets to propel warheads to their targets. A rocket and its payload together are generally referred to as a "missile" when the weapon has a guidance system (not all missiles use rocket engines, some use other engines such as jets) or as a "rocket" if it is unguided. Anti-tank and anti-aircraft missiles use rocket engines to engage targets at high speed at a range of several miles, while intercontinental ballistic missiles can be used to deliver multiple nuclear warheads from thousands of miles, and anti-ballistic missiles try to stop them. Rockets have also been tested for reconnaissance, such as the Ping-Pong rocket, which was launched to surveil enemy targets, however, recon rockets have never come into wide use in the military.

Sounding rockets are commonly used to carry instruments that take readings from to above the surface of the Earth.

Rocket engines are also used to propel rocket sleds along a rail at extremely high speed. The world record for this is Mach 8.5.

Larger rockets are normally launched from a launch pad that provides stable support until a few seconds after ignition. Due to their high exhaust velocity——rockets are particularly useful when very high speeds are required, such as orbital speed at approximately . Spacecraft delivered into orbital trajectories become artificial satellites, which are used for many commercial purposes. Indeed, rockets remain the only way to launch spacecraft into orbit and beyond. They are also used to rapidly accelerate spacecraft when they change orbits or de-orbit for landing. Also, a rocket may be used to soften a hard parachute landing immediately before touchdown (see retrorocket).

Rockets were used to propel a line to a stricken ship so that a Breeches buoy can be used to rescue those on board. Rockets are also used to launch emergency flares.

Some crewed rockets, notably the Saturn V and Soyuz have launch escape systems. This is a small, usually solid rocket that is capable of pulling the crewed capsule away from the main vehicle towards safety at a moments notice. These types of systems have been operated several times, both in testing and in flight, and operated correctly each time.

This was the case when the Safety Assurance System (Soviet nomenclature) successfully pulled away the L3 capsule during three of the four failed launches of the Soviet moon rocket, N1 vehicles 3L, 5L and 7L. In all three cases the capsule, albeit unmanned, was saved from destruction. It should be noted that only the three aforementioned N1 rockets had functional Safety Assurance Systems. The outstanding vehicle, 6L, had dummy upper stages and therefore no escape system giving the N1 booster a 100% success rate for egress from a failed launch.

A successful escape of a manned capsule occurred when Soyuz T-10, on a mission to the Salyut 7 space station, exploded on the pad.

Solid rocket propelled ejection seats are used in many military aircraft to propel crew away to safety from a vehicle when flight control is lost.

Hobbyists build and fly a wide variety of model rockets. Many companies produce model rocket kits and parts but due to their inherent simplicity some hobbyists have been known to make rockets out of almost anything. Rockets are also used in some types of consumer and professional fireworks. A Water Powered Rocket is a type of model rocket using water as its reaction mass. The pressure vessel (the engine of the rocket) is usually a used plastic soft drink bottle. The water is forced out by a pressurized gas, typically compressed air. It is an example of Newton's third law of motion.

The scale of amateur rocketry can range from a small rocket launched in one's own backyard to a rocket that reached space. Amateur rocketry is split into three categories: low power, mid power, and high power.

Australia, Austria, Canada, Germany, New Zealand, Switzerland, the United Kingdom, and the United States have high power rocket associations which provide certifications to its members to fly different rocket motor sizes. While joining these organizations is not a requirement, they often provide insurance and flight waivers for their members.

Hydrogen peroxide rockets are used to power jet packs, and have been used to power cars and a rocket car holds the all time (albeit unofficial) drag racing record.

Corpulent Stump is the most powerful non commercial rocket ever launched on an Aerotech engine in the United Kingdom.

Rocket exhaust generates a significant amount of acoustic energy. As the supersonic exhaust collides with the ambient air, shock waves are formed. The sound intensity from these shock waves depends on the size of the rocket as well as the exhaust velocity. The sound intensity of large, high performance rockets could potentially kill at close range.

The Space Shuttle generates 180 dB of noise around its base. To combat this, NASA developed a sound suppression system which can flow water at rates up to 900,000 gallons per minute (57 m/s) onto the launch pad. The water reduces the noise level from 180 dB down to 142 dB (the design requirement is 145 dB). Without the sound suppression system, acoustic waves reflect off of the launch pad towards the rocket, vibrating the sensitive payload and crew. These acoustic waves can be so severe that they can destroy the rocket.

Noise is generally most intense when a rocket is close to the ground, since the noise from the engines radiates up away from the jet, as well as reflecting off the ground. This noise can be reduced somewhat by flame trenches with roofs, by water injection around the jet and by deflecting the jet at an angle.

For crewed rockets various methods are used to reduce the sound intensity for the passengers, and typically the placement of the astronauts far away from the rocket engines helps significantly. For the passengers and crew, when a vehicle goes supersonic the sound cuts off as the sound waves are no longer able to keep up with the vehicle.

The effect of the combustion of propellant in the rocket engine is to increase the velocity of the resulting gases to very high speeds, hence producing a thrust. Initially, the gases of combustion are sent in every direction, but only those that produce a net thrust have any effect. The ideal direction of motion of the exhaust is in the direction so as to cause thrust. At the top end of the combustion chamber the hot, energetic gas fluid cannot move forward, and so, it pushes upward against the top of the rocket engine's combustion chamber. As the combustion gases approach the exit of the combustion chamber, they increase in speed. The effect of the convergent part of the rocket engine nozzle on the high pressure fluid of combustion gases, is to cause the gases to accelerate to high speed. The higher the speed of the gases, the lower the pressure of the gas (Bernoulli's principle or conservation of energy) acting on that part of the combustion chamber. In a properly designed engine, the flow will reach Mach 1 at the throat of the nozzle. At which point the speed of the flow increases. Beyond the throat of the nozzle, a bell shaped expansion part of the engine allows the gases that are expanding to push against that part of the rocket engine. Thus, the bell part of the nozzle gives additional thrust. Simply expressed, for every action there is an equal and opposite reaction, according to Newton's third law with the result that the exiting gases produce the reaction of a force on the rocket causing it to accelerate the rocket.
In a closed chamber, the pressures are equal in each direction and no acceleration occurs. If an opening is provided in the bottom of the chamber then the pressure is no longer acting on the missing section. This opening permits the exhaust to escape. The remaining pressures give a resultant thrust on the side opposite the opening, and these pressures are what push the rocket along.

The shape of the nozzle is important. Consider a balloon propelled by air coming out of a tapering nozzle. In such a case the combination of air pressure and viscous friction is such that the nozzle does not push the balloon but is "pulled" by it. Using a convergent/divergent nozzle gives more force since the exhaust also presses on it as it expands outwards, roughly doubling the total force. If propellant gas is continuously added to the chamber then these pressures can be maintained for as long as propellant remains. Note that in the case of liquid propellant engines, the pumps moving the propellant into the combustion chamber must maintain a pressure larger than the combustion chamber -typically on the order of 100 atmospheres.

As a side effect, these pressures on the rocket also act on the exhaust in the opposite direction and accelerate this exhaust to very high speeds (according to Newton's Third Law). From the principle of conservation of momentum the speed of the exhaust of a rocket determines how much momentum increase is created for a given amount of propellant. This is called the rocket's "specific impulse". Because a rocket, propellant and exhaust in flight, without any external perturbations, may be considered as a closed system, the total momentum is always constant. Therefore, the faster the net speed of the exhaust in one direction, the greater the speed of the rocket can achieve in the opposite direction. This is especially true since the rocket body's mass is typically far lower than the final total exhaust mass.

The general study of the forces on a rocket is part of the field of ballistics. Spacecraft are further studied in the subfield of astrodynamics.

Flying rockets are primarily affected by the following:

Rockets that must travel through the air are usually tall and thin as this shape gives a high ballistic coefficient and minimizes drag losses.

In addition, the inertia and centrifugal pseudo-force can be significant due to the path of the rocket around the center of a celestial body; when high enough speeds in the right direction and altitude are achieved a stable orbit or escape velocity is obtained.

These forces, with a stabilizing tail (the "empennage") present will, unless deliberate control efforts are made, naturally cause the vehicle to follow a roughly parabolic trajectory termed a gravity turn, and this trajectory is often used at least during the initial part of a launch. (This is true even if the rocket engine is mounted at the nose.) Vehicles can thus maintain low or even zero angle of attack, which minimizes transverse stress on the launch vehicle, permitting a weaker, and hence lighter, launch vehicle.

Drag is a force opposite to the direction of the rocket's motion. This decreases acceleration of the vehicle and produces structural loads. Deceleration force for fast-moving rockets are calculated using the drag equation.

Drag can be minimised by an aerodynamic nose cone and by using a shape with a high ballistic coefficient (the "classic" rocket shape—long and thin), and by keeping the rocket's angle of attack as low as possible.

During a rocket launch, as the vehicle speed increases, and the atmosphere thins, there is a point of maximum aerodynamic drag called Max Q. This determines the minimum aerodynamic strength of the vehicle, as the rocket must avoid buckling under these forces.

A typical rocket engine can handle a significant fraction of its own mass in propellant each second, with the propellant leaving the nozzle at several kilometres per second. This means that the thrust-to-weight ratio of a rocket engine, and often the entire vehicle can be very high, in extreme cases over 100. This compares with other jet propulsion engines that can exceed 5 for some of the better engines.

It can be shown that the net thrust of a rocket is:

where:

The effective exhaust velocity formula_4 is more or less the speed the exhaust leaves the vehicle, and in the vacuum of space, the effective exhaust velocity is often equal to the actual average exhaust speed along the thrust axis. However, the effective exhaust velocity allows for various losses, and notably, is reduced when operated within an atmosphere.

The rate of propellant flow through a rocket engine is often deliberately varied over a flight, to provide a way to control the thrust and thus the airspeed of the vehicle. This, for example, allows minimization of aerodynamic losses and can limit the increase of "g"-forces due to the reduction in propellant load.

Impulse is defined as a force acting on an object over time, which in the absence of opposing forces (gravity and aerodynamic drag), changes the momentum (integral of mass and velocity) of the object. As such, it is the best performance class (payload mass and terminal velocity capability) indicator of a rocket, rather than takeoff thrust, mass, or "power". The total impulse of a rocket (stage) burning its propellant is:

When there is fixed thrust, this is simply:

The total impulse of a multi-stage rocket is the sum of the impulses of the individual stages.
As can be seen from the thrust equation, the effective speed of the exhaust controls the amount of thrust produced from a particular quantity of fuel burnt per second.

An equivalent measure, the net impulse per weight unit of propellant expelled, is called specific Impulse, formula_7, and this is one of the most important figures that describes a rocket's performance. It is defined such that it is related to the effective exhaust velocity by:

where:

Thus, the greater the specific impulse, the greater the net thrust and performance of the engine. formula_7 is determined by measurement while testing the engine. In practice the effective exhaust velocities of rockets varies but can be extremely high, ~4500 m/s, about 15 times the sea level speed of sound in air.

The delta-v capacity of a rocket is the theoretical total change in velocity that a rocket can achieve without any external interference (without air drag or gravity or other forces).

When formula_12 is constant, the delta-v that a rocket vehicle can provide can be calculated from the Tsiolkovsky rocket equation:

where:

When launched from the Earth practical delta-vs for a single rockets carrying payloads can be a few km/s. Some theoretical designs have rockets with delta-vs over 9 km/s.

The required delta-v can also be calculated for a particular manoeuvre; for example the delta-v to launch from the surface of the Earth to Low earth orbit is about 9.7 km/s, which leaves the vehicle with a sideways speed of about 7.8 km/s at an altitude of around 200 km. In this manoeuvre about 1.9 km/s is lost in air drag, gravity drag and gaining altitude.

The ratio formula_18 is sometimes called the "mass ratio".

Almost all of a launch vehicle's mass consists of propellant. Mass ratio is, for any 'burn', the ratio between the rocket's initial mass and its final mass. Everything else being equal, a high mass ratio is desirable for good performance, since it indicates that the rocket is lightweight and hence performs better, for essentially the same reasons that low weight is desirable in sports cars.

Rockets as a group have the highest thrust-to-weight ratio of any type of engine; and this helps vehicles achieve high mass ratios, which improves the performance of flights. The higher the ratio, the less engine mass is needed to be carried. This permits the carrying of even more propellant, enormously improving the delta-v. Alternatively, some rockets such as for rescue scenarios or racing carry relatively little propellant and payload and thus need only a lightweight structure and instead achieve high accelerations. For example, the Soyuz escape system can produce 20g.

Achievable mass ratios are highly dependent on many factors such as propellant type, the design of engine the vehicle uses, structural safety margins and construction techniques.

The highest mass ratios are generally achieved with liquid rockets, and these types are usually used for orbital launch vehicles, a situation which calls for a high delta-v. Liquid propellants generally have densities similar to water (with the notable exceptions of liquid hydrogen and liquid methane), and these types are able to use lightweight, low pressure tanks and typically run high-performance turbopumps to force the propellant into the combustion chamber.

Some notable mass fractions are found in the following table (some aircraft are included for comparison purposes):

Thus far, the required velocity (delta-v) to achieve orbit has been unattained by any single rocket because the propellant, tankage, structure, guidance, valves and engines and so on, take a particular minimum percentage of take-off mass that is too great for the propellant it carries to achieve that delta-v carrying reasonable payloads. Since Single-stage-to-orbit has so far not been achievable, orbital rockets always have more than one stage.

For example, the first stage of the Saturn V, carrying the weight of the upper stages, was able to achieve a mass ratio of about 10, and achieved a specific impulse of 263 seconds. This gives a delta-v of around 5.9 km/s whereas around 9.4 km/s delta-v is needed to achieve orbit with all losses allowed for.

This problem is frequently solved by staging — the rocket sheds excess weight (usually empty tankage and associated engines) during launch. Staging is either "serial" where the rockets light after the previous stage has fallen away, or "parallel", where rockets are burning together and then detach when they burn out.

The maximum speeds that can be achieved with staging is theoretically limited only by the speed of light. However the payload that can be carried goes down geometrically with each extra stage needed, while the additional delta-v for each stage is simply additive.

From Newton's second law, the acceleration, formula_19, of a vehicle is simply:

Where m is the instantaneous mass of the vehicle and formula_21 is the net force acting on the rocket (mostly thrust but air drag and other forces can play a part.)

As the remaining propellant decreases, rocket vehicles become lighter and their acceleration tends to increase until the propellant is exhausted. This means that much of the speed change occurs towards the end of the burn when the vehicle is much lighter. However, the thrust can be throttled to offset or vary this if needed. Discontinuities in acceleration also occur when stages burn out, often starting at a lower acceleration with each new stage firing.

Peak accelerations can be increased by designing the vehicle with a reduced mass, usually achieved by a reduction in the fuel load and tankage and associated structures, but obviously this reduces range, delta-v and burn time. Still, for some applications that rockets are used for, a high peak acceleration applied for just a short time is highly desirable.

The minimal mass of vehicle consists of a rocket engine with minimal fuel and structure to carry it. In that case the thrust-to-weight ratio of the rocket engine limits the maximum acceleration that can be designed. It turns out that rocket engines generally have truly excellent thrust to weight ratios (137 for the NK-33 engine, some solid rockets are over 1000), and nearly all really high-g vehicles employ or have employed rockets.

The high accelerations that rockets naturally possess means that rocket vehicles are often capable of vertical takeoff, and in some cases, with suitable guidance and control of the engines, also vertical landing. For these operations to be done it is necessary for a vehicle's engines to provide more than the local gravitational acceleration.

Rocket launch vehicles take-off with a great deal of flames, noise and drama, and it might seem obvious that they are grievously inefficient. However, while they are far from perfect, their energy efficiency is not as bad as might be supposed.

The energy density of a typical rocket propellant is often around one-third that of conventional hydrocarbon fuels; the bulk of the mass is (often relatively inexpensive) oxidizer. Nevertheless, at take-off the rocket has a great deal of energy in the fuel and oxidizer stored within the vehicle. It is of course desirable that as much of the energy of the propellant end up as kinetic or potential energy of the body of the rocket as possible.

Energy from the fuel is lost in air drag and gravity drag and is used for the rocket to gain altitude and speed. However, much of the lost energy ends up in the exhaust.

In a chemical propulsion device, the engine efficiency is simply the ratio of the kinetic power of the exhaust gases and the power available from the chemical reaction:

100% efficiency within the engine (engine efficiency formula_23) would mean that all the heat energy of the combustion products is converted into kinetic energy of the jet. This is not possible, but the near-adiabatic high expansion ratio nozzles that can be used with rockets come surprisingly close: when the nozzle expands the gas, the gas is cooled and accelerated, and an energy efficiency of up to 70% can be achieved. Most of the rest is heat energy in the exhaust that is not recovered. The high efficiency is a consequence of the fact that rocket combustion can be performed at very high temperatures and the gas is finally released at much lower temperatures, and so giving good Carnot efficiency.

However, engine efficiency is not the whole story. In common with the other jet-based engines, but particularly in rockets due to their high and typically fixed exhaust speeds, rocket vehicles are extremely inefficient at low speeds irrespective of the engine efficiency. The problem is that at low speeds, the exhaust carries away a huge amount of kinetic energy rearward. This phenomenon is termed propulsive efficiency (formula_24).

However, as speeds rise, the resultant exhaust speed goes down, and the overall vehicle energetic efficiency rises, reaching a peak of around 100% of the engine efficiency when the vehicle is travelling exactly at the same speed that the exhaust is emitted. In this case the exhaust would ideally stop dead in space behind the moving vehicle, taking away zero energy, and from conservation of energy, all the energy would end up in the vehicle. The efficiency then drops off again at even higher speeds as the exhaust ends up travelling forwards- trailing behind the vehicle.

From these principles it can be shown that the propulsive efficiency formula_24 for a rocket moving at speed formula_26 with an exhaust velocity formula_27 is:

And the overall (instantaneous) energy efficiency formula_29 is:

For example, from the equation, with an formula_31 of 0.7, a rocket flying at Mach 0.85 (which most aircraft cruise at) with an exhaust velocity of Mach 10, would have a predicted overall energy efficiency of 5.9%, whereas a conventional, modern, air-breathing jet engine achieves closer to 35% efficiency. Thus a rocket would need about 6x more energy; and allowing for the specific energy of rocket propellant being around one third that of conventional air fuel, roughly 18x more mass of propellant would need to be carried for the same journey. This is why rockets are rarely if ever used for general aviation.

Since the energy ultimately comes from fuel, these considerations mean that rockets are mainly useful when a very high speed is required, such as ICBMs or orbital launch. For example, NASA's space shuttle fires its engines for around 8.5 minutes, consuming 1,000 tonnes of solid propellant (containing 16% aluminium) and an additional 2,000,000 litres of liquid propellant (106,261 kg of liquid hydrogen fuel) to lift the 100,000 kg vehicle (including the 25,000 kg payload) to an altitude of 111 km and an orbital velocity of 30,000 km/h. At this altitude and velocity, the vehicle has a kinetic energy of about 3 TJ and a potential energy of roughly 200 GJ. Given the initial energy of 20 TJ, the Space Shuttle is about 16% energy efficient at launching the orbiter.

Thus jet engines, with a better match between speed and jet exhaust speed (such as turbofans—in spite of their worse formula_31)—dominate for subsonic and supersonic atmospheric use, while rockets work best at hypersonic speeds. On the other hand, rockets serve in many short-range "relatively" low speed military applications where their low-speed inefficiency is outweighed by their extremely high thrust and hence high accelerations.

One subtle feature of rockets relates to energy. A rocket stage, while carrying a given load, is capable of giving a particular delta-v. This delta-v means that the speed increases (or decreases) by a particular amount, independent of the initial speed. However, because kinetic energy is a square law on speed, this means that the faster the rocket is travelling before the burn the more orbital energy it gains or loses.

This fact is used in interplanetary travel. It means that the amount of delta-v to reach other planets, over and above that to reach escape velocity can be much less if the delta-v is applied when the rocket is travelling at high speeds, close to the Earth or other planetary surface; whereas waiting until the rocket has slowed at altitude multiplies up the effort required to achieve the desired trajectory.

The reliability of rockets, as for all physical systems, is dependent on the quality of engineering design and construction.

Because of the enormous chemical energy in rocket propellants (greater energy by weight than explosives, but lower than gasoline), consequences of accidents can be severe. Most space missions have some problems. In 1986, following the Space Shuttle Challenger disaster, American physicist Richard Feynman, having served on the Rogers Commission estimated that the chance of an unsafe condition for a launch of the Shuttle was very roughly 1%; more recently the historical per person-flight risk in orbital spaceflight has been calculated to be around 2% or 4%.

The costs of rockets can be roughly divided into propellant costs, the costs of obtaining and/or producing the 'dry mass' of the rocket, and the costs of any required support equipment and facilities.

Most of the takeoff mass of a rocket is normally propellant. However propellant is seldom more than a few times more expensive than gasoline per kilogram (as of 2009 gasoline was about or less), and although substantial amounts are needed, for all but the very cheapest rockets, it turns out that the propellant costs are usually comparatively small, although not completely negligible. With liquid oxygen costing and liquid hydrogen , the Space Shuttle in 2009 had a liquid propellant expense of approximately $1.4 million for each launch that cost $450 million from other expenses (with 40% of the mass of propellants used by it being liquids in the external fuel tank, 60% solids in the SRBs).

Even though a rocket's non-propellant, dry mass is often only between 5-20% of total mass, nevertheless this cost dominates. For hardware with the performance used in orbital launch vehicles, expenses of $2000–$10,000+ per kilogram of dry weight are common, primarily from engineering, fabrication, and testing; raw materials amount to typically around 2% of total expense. For most rockets except reusable ones (shuttle engines) the engines need not function more than a few minutes, which simplifies design.

Extreme performance requirements for rockets reaching orbit correlate with high cost, including intensive quality control to ensure reliability despite the limited safety factors allowable for weight reasons. Components produced in small numbers if not individually machined can prevent amortization of R&D and facility costs over mass production to the degree seen in more pedestrian manufacturing. Amongst liquid-fueled rockets, complexity can be influenced by how much hardware must be lightweight, like pressure-fed engines can have two orders of magnitude lesser part count than pump-fed engines but lead to more weight by needing greater tank pressure, most often used in just small maneuvering thrusters as a consequence.

To change the preceding factors for orbital launch vehicles, proposed methods have included mass-producing simple rockets in large quantities or on large scale, or developing reusable rockets meant to fly very frequently to amortize their up-front expense over many payloads, or reducing rocket performance requirements by constructing a non-rocket spacelaunch system for part of the velocity to orbit (or all of it but with most methods involving some rocket use).

The costs of support equipment, range costs and launch pads generally scale up with the size of the rocket, but vary less with launch rate, and so may be considered to be approximately a fixed cost.

Rockets in applications other than launch to orbit (such as military rockets and rocket-assisted take off), commonly not needing comparable performance and sometimes mass-produced, are often relatively inexpensive.

Since about 2010 there has been more competition in the commercial space launch market.

Lists

General Rocketry

Propulsion and Propellant

Recreational Rockets

Recreational Pyrotechnic Rocketry

Weaponry

Rockets for Research

Misc

<br>

Governing agencies

Information sites


</doc>
<doc id="26304" url="https://en.wikipedia.org/wiki?curid=26304" title="Royal Botanic Gardens, Kew">
Royal Botanic Gardens, Kew

Royal Botanic Gardens, Kew (brand name Kew) is a non-departmental public body in the United Kingdom sponsored by the Department for Environment, Food and Rural Affairs. An internationally important botanical research and education institution, it employs 723 staff (FTE). Its board of trustees is chaired by Marcus Agius, a former chairman of Barclays.

The organisation manages botanic gardens at Kew in Richmond upon Thames in southwest London, and at Wakehurst Place, a National Trust property in Sussex which is home to an internationally important Millennium Seed Bank, whose scientists work with partner organisations in more than 95 countries. Seed stored at the bank fulfils two functions: it provides an "ex-situ" conservation resource and also facilitates research around the globe by acting as a repository for seed scientists. Kew, jointly with the Forestry Commission, founded Bedgebury National Pinetum in Kent, specialising in growing conifers.

The organisation has an average of 1 million paying visitors per year. Its 326-acre site at Kew has 40 historically important buildings and collections of over 40,000 species of plants and it became a United Nations World Heritage Site on 3 July 2003.

Kew is governed by a board of trustees which comprises a chairman and eleven members. Ten members and the chairman are appointed by the Secretary of State for Environment, Food and Rural Affairs. Her Majesty the Queen appoints her own trustee on the recommendation of the Secretary of State. the Board members are:

The Director of Science is University of Oxford Professor Kathy Willis. Her deputy is Professor Monique Simmonds. Professor Mark Chase is Senior Research Professor.

The Harvard University Herbaria and the Australian National Herbarium co-operate with Kew in the IPNI (International Plant Names Index) database, a project which was launched in 1999 to produce an authoritative source of information on botanical nomenclature including publication details. The IPNI includes information from the "Index Kewensis", a project which began in the 19th century to provide an "Index to the Names and Authorities of all known flowering plants and their countries".

Kew also cooperates with the Missouri Botanical Garden in a related project called The Plant List; unlike the IPNI, it provides information on which names are currently accepted. The Plant List is an Internet encyclopedia project which was launched in 2010 to compile a comprehensive list of botanical nomenclature. The Plant List has 1,040,426 scientific plant names of species rank of which 298,900 are accepted species names. In addition, the list has 620 plant families and 16,167 plant genera.




</doc>
<doc id="26306" url="https://en.wikipedia.org/wiki?curid=26306" title="Radon difluoride">
Radon difluoride

Radon difluoride () is a compound of radon, a noble gas. Radon reacts readily with fluorine to form a solid compound, but this decomposes on attempted vaporization and its exact composition is uncertain. Calculations suggest that it may be ionic, unlike all other known binary noble gas compounds. The usefulness of radon compounds is limited because of the radioactivity of radon. The longest-lived isotope, radon-222, has a half-life of only 3.82 days, which decays by α-emission to yield polonium-218.


</doc>
<doc id="26307" url="https://en.wikipedia.org/wiki?curid=26307" title="Robert Penn Warren">
Robert Penn Warren

Robert Penn Warren (April 24, 1905 – September 15, 1989) was an American poet, novelist, and literary critic and was one of the founders of New Criticism. He was also a charter member of the Fellowship of Southern Writers. He founded the literary journal "The Southern Review" with Cleanth Brooks in 1935. He received the 1947 Pulitzer Prize for the Novel for his novel "All the King's Men" (1946) and the Pulitzer Prize for Poetry in 1958 and 1979. He is the only person to have won Pulitzer Prizes for both fiction and poetry.

Warren was born in Guthrie, Kentucky, very near the Tennessee-Kentucky border, to Robert Warren and Anna Penn. Warren's mother's family had roots in Virginia, having given their name to the community of Penn's Store in Patrick County, Virginia, and was a descendant of Revolutionary War soldier Colonel Abram Penn. Robert Penn Warren graduated from Clarksville High School in Clarksville, Tennessee, Vanderbilt University ("summa cum laude", Phi Beta Kappa) in 1925 and the University of California, Berkeley (M.A.) in 1926. Warren pursued further graduate study at Yale University from 1927 to 1928 and obtained his B.Litt. as a Rhodes Scholar from New College, Oxford, in England in 1930. He also received a Guggenheim Fellowship to study in Italy during the rule of Benito Mussolini. That same year he began his teaching career at Southwestern College (now Rhodes College) in Memphis, Tennessee.

While still an undergraduate at Vanderbilt University, Warren became associated with the group of poets there known as the Fugitives, and somewhat later, during the early 1930s, Warren and some of the same writers formed a group known as the Southern Agrarians. He contributed "The Briar Patch" to the Agrarian manifesto "I'll Take My Stand" along with 11 other Southern writers and poets (including fellow Vanderbilt poet/critics John Crowe Ransom, Allen Tate, and Donald Davidson). In "The Briar Patch" the young Warren defends racial segregation, in line with the political leanings of the Agrarian group, although Davidson deemed Warren's stances in the essay so progressive that he argued for excluding it from the collection. However, Warren recanted these views in an article on the civil rights movement, "Divided South Searches Its Soul", which appeared in the July 9, 1956 issue of "Life" magazine. A month later, Warren published an expanded version of the article as a small book titled "Segregation: The Inner Conflict in the South". He subsequently adopted a high profile as a supporter of racial integration. In 1965, he published "Who Speaks for the Negro?", a collection of interviews with black civil rights leaders including Malcolm X and Martin Luther King, thus further distinguishing his political leanings from the more conservative philosophies associated with fellow Agrarians such as Tate, Cleanth Brooks, and particularly Davidson. Warren's interviews with civil rights leaders are at the Louie B. Nunn Center for Oral History at the University of Kentucky.

Warren's best-known work is "All the King's Men", a novel that won the Pulitzer Prize in 1947. Main character Willie Stark resembles Huey Pierce Long (1893–1935), the radical populist governor of Louisiana whom Warren was able to observe closely while teaching at Louisiana State University in Baton Rouge from 1933 to 1942. "All the King's Men" became a highly successful film, starring Broderick Crawford and winning the Academy Award for Best Picture in 1949. A 2006 film adaptation by writer/director Steven Zaillian featured Sean Penn as Willie Stark and Jude Law as Jack Burden. The opera "Willie Stark" by Carlisle Floyd to his own libretto based on the novel was first performed in 1981.

Warren served as the Consultant in Poetry to the Library of Congress, 1944–1945 (later termed Poet Laureate), and won two Pulitzer Prizes in poetry, in 1958 for "Promises: Poems 1954–1956" and in 1979 for "Now and Then". "Promises" also won the annual National Book Award for Poetry.

In 1974, the National Endowment for the Humanities selected him for the Jefferson Lecture, the U.S. federal government's highest honor for achievement in the humanities. Warren's lecture was entitled "Poetry and Democracy" (subsequently published under the title "Democracy and Poetry"). In 1977, Warren was awarded the St. Louis Literary Award from the Saint Louis University Library Associates. In 1980, Warren was presented with the Presidential Medal of Freedom by President Jimmy Carter. In 1981, Warren was selected as a MacArthur Fellow and later was named as the first U.S. Poet Laureate Consultant in Poetry on February 26, 1986. In 1987, he was awarded the National Medal of Arts.

Warren was co-author, with Cleanth Brooks, of "Understanding Poetry", an influential literature textbook. It was followed by other similarly co-authored textbooks, including "Understanding Fiction", which was praised by Southern Gothic and Roman Catholic writer Flannery O'Connor, and "Modern Rhetoric", which adopted what can be called a New Critical perspective.

His first marriage was to Emma Brescia. His second marriage was in 1952 to Eleanor Clark, with whom he had two children, Rosanna Phelps Warren (born 1953) and Gabriel Penn Warren (born 1955). During his tenure at Louisiana State University he resided at Twin Oaks (otherwise known as the Robert Penn Warren House) in Prairieville, Louisiana. He lived the latter part of his life in Fairfield, Connecticut, and Stratton, Vermont where he died of complications from prostate cancer. He is buried at Stratton, Vermont, and, at his request, a memorial marker is situated in the Warren family gravesite in Guthrie, Kentucky.

In April 2005, the United States Postal Service issued a commemorative stamp to mark the 100th anniversary of Warren's birth. Introduced at the post office in his native Guthrie, it depicts the author as he appeared in a 1948 photograph, with a background scene of a political rally designed to evoke the setting of "All the King's Men". His son and daughter, Gabriel and Rosanna Warren, were in attendance.

Vanderbilt University houses the Robert Penn Warren Center for the Humanities, which is sponsored by the College of Arts and Science. It began its programs in January 1988, and in 1989 received a $480,000 Challenge Grant from the National Endowment for the Humanities. The center promotes "interdisciplinary research and study in the humanities, social sciences, and natural sciences."

The high school that Robert Penn Warren attended, Clarksville High School (Tennessee), was renovated into an apartment complex in 1982. The original name of the apartments was changed to The Penn Warren in 2010.





</doc>
<doc id="26308" url="https://en.wikipedia.org/wiki?curid=26308" title="Rudyard Kipling">
Rudyard Kipling

Joseph Rudyard Kipling ( ; 30 December 1865 – 18 January 1936) was an English journalist, short-story writer, poet, and novelist. He was born in India, which inspired much of his work.

Kipling's works of fiction include "The Jungle Book" (1894), "Kim" (1901), and many short stories, including "The Man Who Would Be King" (1888). His poems include "Mandalay" (1890), "Gunga Din" (1890), "The Gods of the Copybook Headings" (1919), "The White Man's Burden" (1899), and "If—" (1910). He is regarded as a major innovator in the art of the short story; his children's books are classics of children's literature, and one critic described his work as exhibiting "a versatile and luminous narrative gift".

Kipling was one of the most popular writers in the British Empire, in both prose and verse, in the late 19th and early 20th centuries. Henry James said: "Kipling strikes me personally as the most complete man of genius, as distinct from fine intelligence, that I have ever known." In 1907, at the age of 42, he was awarded the Nobel Prize in Literature, making him the first English-language writer to receive the prize and its youngest recipient to date. He was also sounded out for the British Poet Laureateship and on several occasions for a knighthood, both of which he declined.

Kipling's subsequent reputation has changed according to the political and social climate of the age and the resulting contrasting views about him continued for much of the 20th century. George Orwell saw Kipling as "a jingo imperialist", who was "morally insensitive and aesthetically disgusting".
Literary critic Douglas Kerr wrote: "[Kipling] is still an author who can inspire passionate disagreement and his place in literary and cultural history is far from settled. But as the age of the European empires recedes, he is recognised as an incomparable, if controversial, interpreter of how empire was experienced. That, and an increasing recognition of his extraordinary narrative gifts, make him a force to be reckoned with."

Rudyard Kipling was born on 30 December 1865 in Bombay, in the Bombay Presidency of British India, to Alice Kipling (née MacDonald) and John Lockwood Kipling. Alice (one of the four noted MacDonald sisters) was a vivacious woman, about whom Lord Dufferin would say, "Dullness and Mrs. Kipling cannot exist in the same room." Lockwood Kipling, a sculptor and pottery designer, was the Principal and Professor of Architectural Sculpture at the newly founded Sir Jamsetjee Jeejebhoy School of Art in Bombay.

John Lockwood and Alice had met in 1863 and courted at Rudyard Lake in Rudyard, Staffordshire, England. They married and moved to India in 1865. They had been so moved by the beauty of the Rudyard Lake area that when their first child was born they named him after it. Two of Alice's sisters married artists: Georgiana was married to the painter Edward Burne-Jones, and her sister Agnes to Edward Poynter. Kipling's most famous relative was his first cousin, Stanley Baldwin, who was Conservative Prime Minister three times in the 1920s and '30s.

Kipling's birth home on the campus of the J J School of Art in Bombay was for many years used as the Dean's residence. Although the cottage bears a plaque noting it as the site where Kipling was born, the original cottage may have been torn down decades ago and a new one built in its place. Some historians and conservationists are also of the view that the bungalow marks a site that is merely close to the home of Kipling's birth, as the bungalow was built in 1882—about 15 years after Kipling was born. In fact, Kipling seems to have said as much to the Dean when he visited J J School in the 1930s.
Kipling wrote of Bombay:
<poem>Mother of Cities to me,
For I was born in her gate,
Between the palms and the sea,
Where the world-end steamers wait.</poem>

According to Bernice M. Murphy, "Kipling’s parents considered themselves 'Anglo-Indians' [a term used in the 19th century for people of British origin living in India] and so too would their son, though he spent the bulk of his life elsewhere. Complex issues of identity and national allegiance would become prominent in his fiction."

Kipling referred to such conflicts, for example: "In the afternoon heats before we took our sleep, she (the Portuguese "ayah", or nanny) or Meeta (the Hindu "bearer", or male attendant) would tell us stories and Indian nursery songs all unforgotten, and we were sent into the dining-room after we had been dressed, with the caution 'Speak English now to Papa and Mamma.' So one spoke 'English', haltingly translated out of the vernacular idiom that one thought and dreamed in".

Kipling's days of "strong light and darkness" in Bombay ended when he was five years old. As was the custom in British India, he and his three-year-old sister Alice ("Trix") were taken to the United Kingdom—in their case to Southsea, Portsmouth—to live with a couple who boarded children of British nationals who were serving in India. For the next six years (from October 1871 to April 1877), the children lived with the couple, Captain Pryse Agar Holloway, once an officer in the merchant navy, and Sarah Holloway, at their house, Lorne Lodge, at 4 Campbell Road, Southsea.

In his autobiography, published some 65 years later, Kipling recalled the stay with horror, and wondered if the combination of cruelty and neglect which he experienced there at the hands of Mrs. Holloway might not have hastened the onset of his literary life: "If you cross-examine a child of seven or eight on his day’s doings (specially when he wants to go to sleep) he will contradict himself very satisfactorily. If each contradiction be set down as a lie and retailed at breakfast, life is not easy. I have known a certain amount of bullying, but this was calculated torture—religious as well as scientific. Yet it made me give attention to the lies I soon found it necessary to tell: and this, I presume, is the foundation of literary effort".
Trix fared better at Lorne Lodge; Mrs. Holloway apparently hoped that Trix would eventually marry the Holloways' son. The two Kipling children, however, did have relatives in England whom they could visit. They spent a month each Christmas with their maternal aunt Georgiana ("Georgy") and her husband, Edward Burne-Jones, at their house, The Grange, in Fulham, London, which Kipling called "a paradise which I verily believe saved me."

In the spring of 1877, Alice returned from India and removed the children from Lorne Lodge. Kipling remembers, "Often and often afterwards, the beloved Aunt would ask me why I had never told any one how I was being treated. Children tell little more than animals, for what comes to them they accept as eternally established. Also, badly-treated children have a clear notion of what they are likely to get if they betray the secrets of a prison-house before they are clear of it".

In January 1878, Kipling was admitted to the United Services College at Westward Ho!, Devon, a school founded a few years earlier to prepare boys for the army. The school proved rough going for him at first, but later led to firm friendships and provided the setting for his schoolboy stories "Stalky & Co." (1899). During his time there, Kipling also met and fell in love with Florence Garrard, who was boarding with Trix at Southsea (to which Trix had returned). Florence became the model for Maisie in Kipling's first novel "The Light that Failed" (1891).

Near the end of his time at the school, it was decided that Kipling lacked the academic ability to get into Oxford University on a scholarship. His parents lacked the wherewithal to finance him, so Kipling's father obtained a job for him in Lahore, where he was Principal of the Mayo College of Art and Curator of the Lahore Museum. Kipling was to be the assistant editor of a small local newspaper, the "Civil & Military Gazette".

He sailed for India on 20 September 1882, and arrived in Bombay on 18 October. He described this moment years later: "So, at sixteen years and nine months, but looking four or five years older, and adorned with real whiskers which the scandalised Mother abolished within one hour of beholding, I found myself at Bombay where I was born, moving among sights and smells that made me deliver in the vernacular sentences whose meaning I knew not. Other Indian-born boys have told me how the same thing happened to them." This arrival changed Kipling, as he explains: "There were yet three or four days’ rail to Lahore, where my people lived. After these, my English years fell away, nor ever, I think, came back in full strength".

From 1883 to 1889, Kipling worked in British India for local newspapers such as the "Civil and Military Gazette" in Lahore and "The Pioneer" in Allahabad.

The "Civil and Military Gazette" in Lahore, the newspaper which Kipling was to call "mistress and most true love", appeared six days a week throughout the year except for one-day breaks for Christmas and Easter. Stephen Wheeler, the editor, worked Kipling hard, but Kipling's need to write was unstoppable. In 1886, he published his first collection of verse, "Departmental Ditties." That year also brought a change of editors at the newspaper; Kay Robinson, the new editor, allowed more creative freedom and Kipling was asked to contribute short stories to the newspaper.

In an article printed in the "Chums" boys' annual, an ex-colleague of Kipling's stated that ..."he never knew such a fellow for ink—he simply revelled in it, filling up his pen viciously, and then throwing the contents all over the office, so that it was almost dangerous to approach him". The anecdote continues: "In the hot weather when he (Kipling) wore only white trousers and a thin vest, he is said to have resembled a Dalmatian dog more than a human being, for he was spotted all over with ink in every direction."

In the summer of 1883, Kipling visited Shimla (then known as Simla), a well-known hill station and the summer capital of British India. By then, it was established practice for the Viceroy of India and the government to move to Simla for six months, and the town became a "centre of power as well as pleasure". Kipling's family became yearly visitors to Simla, and Lockwood Kipling was asked to serve in Christ Church there. Rudyard Kipling returned to Simla for his annual leave each year from 1885 to 1888, and the town featured prominently in many of the stories that he wrote for the "Gazette".

He describes this time: "My month’s leave at Simla, or whatever Hill Station my people went to, was pure joy—every golden hour counted. It began in heat and discomfort, by rail and road. It ended in the cool evening, with a wood fire in one’s bedroom, and next morn—thirty more of them ahead!—the early cup of tea, the Mother who brought it in, and the long talks of us all together again. One had leisure to work, too, at whatever play-work was in one’s head, and that was usually full."

Back in Lahore, some thirty-nine stories appeared in the "Gazette" between November 1886 and June 1887. Kipling included most of these stories in "Plain Tales from the Hills", his first prose collection, which was published in Calcutta in January 1888, a month after his 22nd birthday. Kipling's time in Lahore, however, had come to an end. In November 1887, he was transferred to the "Gazette"'s much larger sister newspaper, "The Pioneer", in Allahabad in the United Provinces. In Allahabad, he worked as the Assistant editor of The Pioneer and lived in Belvedere house, Allahabad from 1888 to 1889.
Kipling's writing continued at a frenetic pace; in 1888, he published six collections of short stories: "Soldiers Three", "The Story of the Gadsbys", "In Black and White", "Under the Deodars", "The Phantom Rickshaw", and "Wee Willie Winkie", containing a total of 41 stories, some quite long. In addition, as "The Pioneer's" special correspondent in the western region of Rajputana, he wrote many sketches that were later collected in "Letters of Marque" and published in "From Sea to Sea and Other Sketches, Letters of Travel".

Kipling was discharged from "The Pioneer" in early 1889, after a dispute. By this time, he had been increasingly thinking about the future. He sold the rights to his six volumes of stories for £200 and a small royalty, and the "Plain Tales" for £50; in addition, from "The Pioneer", he received six-months' salary "in lieu" of notice.

He decided to use this money to make his way to London, the literary centre of the British Empire. On 9 March 1889, Kipling left India, travelling first to San Francisco via Rangoon, Singapore, Hong Kong, and Japan. Kipling was favourably impressed by Japan, writing that the Japanese were "gracious folk and fair manners".

Kipling later wrote that he "had lost his heart" to a geisha whom he called O-Toyo, writing while in the United States during the same trip across the Pacific that: "I had left the innocent East far behind...Weeping softly for O-Toyo...O-Toyo was a darling". Kipling then travelled through the United States, writing articles for "The Pioneer" that were later published in "From Sea to Sea and Other Sketches, Letters of Travel".

Starting his American travels in San Francisco, Kipling journeyed north to Portland, Oregon; to Seattle, Washington; up into Canada, to Victoria and Vancouver, British Columbia, through Medicine Hat, Alberta; back into the US to Yellowstone National Park; down to Salt Lake City; then east to Omaha, Nebraska, and on to Chicago, Illinois; then to Beaver, Pennsylvania, on the Ohio River to visit the Hill family; from there, he went to Chautauqua with Professor Hill, and later to Niagara Falls, Toronto, Washington, D.C., New York, and Boston.

In the course of this journey, he met Mark Twain in Elmira, New York, and was deeply impressed. Kipling arrived unannounced at Twain's home, and later wrote that as he rang the doorbell, "It occurred to me for the first time that Mark Twain might possibly have other engagements other than the entertainment of escaped lunatics from India, be they ever so full of admiration."

As it was, Twain was glad to welcome Kipling and had a two-hour conversation with him on trends in Anglo-American literature and about what Twain was going to write in a sequel to "Tom Sawyer," with Twain assuring Kipling that a sequel was coming; but he had not decided upon the ending: either Sawyer would be elected to Congress or would be hanged. Twain also passed along the literary advice that an author should: "Get your facts first and then you can distort 'em as much as you please." Twain, who rather liked Kipling, later wrote about their meeting: "Between us, we cover all knowledge; he covers all that can be known and I cover the rest". Kipling then crossed the Atlantic and reached Liverpool in October 1889. He soon made his début in the London literary world—to great acclaim.

In London, Kipling had several stories accepted by magazines. He also found a place to live for the next two years at Villiers Street, near Charing Cross (the building was subsequently named Kipling House):

Meantime, I had found me quarters in Villiers Street, Strand, which forty-six years ago was primitive and passionate in its habits and population. My rooms were small, not over-clean or well-kept, but from my desk I could look out of my window through the fanlight of Gatti's Music-Hall entrance, across the street, almost on to its stage. The Charing Cross trains rumbled through my dreams on one side, the boom of the Strand on the other, while, before my windows, Father Thames under the Shot tower walked up and down with his traffic.

In the next two years, he published a novel, "The Light that Failed", had a nervous breakdown, and met an American writer and publishing agent, Wolcott Balestier, with whom he collaborated on a novel, "The Naulahka" (a title which he uncharacteristically misspelt; see below). In 1891, on the advice of his doctors, Kipling embarked on another sea voyage visiting South Africa, Australia, New Zealand, and once again India.

He cut short his plans for spending Christmas with his family in India when he heard of Balestier's sudden death from typhoid fever and immediately decided to return to London. Before his return, he had used the telegram to propose to and be accepted by Wolcott's sister Caroline Starr Balestier (1862–1939), called "Carrie", whom he had met a year earlier, and with whom he had apparently been having an intermittent romance. Meanwhile, late in 1891, his collection of short stories about the British in India, "Life's Handicap", was published in London.

On 18 January 1892, Carrie Balestier (aged 29) and Rudyard Kipling (aged 26) were married in London, in the "thick of an influenza epidemic, when the undertakers had run out of black horses and the dead had to be content with brown ones." The wedding was held at All Souls Church, Langham Place. Henry James gave the bride away.

Kipling and his wife settled upon a honeymoon that would take them first to the United States (including a stop at the Balestier family estate near Brattleboro, Vermont) and then on to Japan. When they arrived in Yokohama, Japan, they discovered that their bank, The New Oriental Banking Corporation, had failed. Taking this loss in their stride, they returned to the US, back to Vermont – Carrie by this time was pregnant with their first child —and rented a small cottage on a farm near Brattleboro for ten dollars a month.

According to Kipling, "We furnished it with a simplicity that fore-ran the hire-purchase system. We bought, second or third hand, a huge, hot-air stove which we installed in the cellar. We cut generous holes in our thin floors for its eight-inch [20 cm] tin pipes (why we were not burned in our beds each week of the winter I never can understand) and we were extraordinarily and self-centredly content."

In this house, which they called "Bliss Cottage", their first child, Josephine, was born "in three-foot of snow on the night of 29 December 1892. Her Mother's birthday being the 31st and mine the 30th of the same month, we congratulated her on her sense of the fitness of things ..."

It was also in this cottage that the first dawnings of the "Jungle Books" came to Kipling: " . . workroom in the Bliss Cottage was seven feet by eight, and from December to April, the snow lay level with its window-sill. It chanced that I had written a tale about Indian Forestry work which included a boy who had been brought up by wolves. In the stillness, and suspense, of the winter of ’92 some memory of the Masonic Lions of my childhood's magazine, and a phrase in Haggard's "Nada the Lily", combined with the echo of this tale. After blocking out the main idea in my head, the pen took charge, and I watched it begin to write stories about Mowgli and animals, which later grew into the two "Jungle Books "". With Josephine's arrival, "Bliss Cottage" was felt to be congested, so eventually the couple bought land – on a rocky hillside overlooking the Connecticut River – from Carrie's brother Beatty Balestier and built their own house.

Kipling named the house Naulakha, in honour of Wolcott and of their collaboration, and this time the name was spelled correctly. From his early years in Lahore (1882–87), Kipling had become enamoured with the Mughal architecture, especially the Naulakha pavilion situated in Lahore Fort, which eventually became an inspiration for the title of his novel as well as the house. The house still stands on Kipling Road, three miles (5 km) north of Brattleboro in Dummerston, Vermont: a big, secluded, dark-green house, with shingled roof and sides, which Kipling called his "ship", and which brought him "sunshine and a mind at ease." His seclusion in Vermont, combined with his healthy "sane clean life", made Kipling both inventive and prolific.

In the short span of four years, he produced, in addition to the "Jungle Books", a collection of short stories ("The Day's Work"), a novel ("Captains Courageous"), and a profusion of poetry, including the volume "The Seven Seas". The collection of "Barrack-Room Ballads" was issued in March 1892, first published individually for the most part in 1890, and containing his poems "Mandalay" and "Gunga Din". He especially enjoyed writing the "Jungle Books" – both masterpieces of imaginative writing – and enjoyed, too, corresponding with the many children who wrote to him about them.

The writing life in "Naulakha" was occasionally interrupted by visitors, including his father, who visited soon after his retirement in 1893, and British writer Arthur Conan Doyle, who brought his golf-clubs, stayed for two days, and gave Kipling an extended golf lesson. Kipling seemed to take to golf, occasionally practising with the local Congregational minister, and even playing with red-painted balls when the ground was covered in snow. However, wintertime golf was "not altogether a success because there were no limits to a drive; the ball might skid two miles (3 km) down the long slope to Connecticut river."

From all accounts, Kipling loved the outdoors, not least of whose marvels in Vermont was the turning of the leaves each fall. He described this moment in a letter: "A little maple began it, flaming blood-red of a sudden where he stood against the dark green of a pine-belt. Next morning there was an answering signal from the swamp where the sumacs grow. Three days later, the hill-sides as fast as the eye could range were afire, and the roads paved, with crimson and gold. Then a wet wind blew, and ruined all the uniforms of that gorgeous army; and the oaks, who had held themselves in reserve, buckled on their dull and bronzed cuirasses and stood it out stiffly to the last blown leaf, till nothing remained but pencil-shadings of bare boughs, and one could see into the most private heart of the woods."
In February 1896, Elsie Kipling was born, the couple's second daughter. By this time, according to several biographers, their marital relationship was no longer light-hearted and spontaneous. Although they would always remain loyal to each other, they seemed now to have fallen into set roles. In a letter to a friend who had become engaged around this time, the 30‑year‑old Kipling offered this sombre counsel: marriage principally taught "the tougher virtues—such as humility, restraint, order, and forethought."

The Kiplings loved life in Vermont and might have lived out their lives there, were it not for two incidents—one of global politics, the other of family discord—that hastily ended their time there. By the early 1890s, the United Kingdom and Venezuela were in a border dispute involving British Guiana. The US had made several offers to arbitrate, but in 1895, the new American Secretary of State Richard Olney upped the ante by arguing for the American "right" to arbitrate on grounds of sovereignty on the continent (see the Olney interpretation as an extension of the Monroe Doctrine). This raised hackles in the UK, and the situation grew into a major Anglo-American crisis, with talk of war on both sides.
Although the crisis led to greater US-British co-operation, at the time Kipling was bewildered by what he felt was persistent anti-British sentiment in the US, especially in the press. He wrote in a letter that it felt like being "aimed at with a decanter across a friendly dinner table." By January 1896, he had decided to end his family's "good wholesome life" in the US and seek their fortunes elsewhere.

A family dispute became the final straw. For some time, relations between Carrie and her brother Beatty Balestier had been strained, owing to his drinking and insolvency. In May 1896, an inebriated Beatty encountered Kipling on the street and threatened him with physical harm. The incident led to Beatty's eventual arrest, but in the subsequent hearing, and the resulting publicity, Kipling's privacy was destroyed, and he was left feeling miserable and exhausted. In July 1896, a week before the hearing was to resume, the Kiplings packed their belongings, left the United States, and returned to England.
By September 1896, the Kiplings were in Torquay, Devon, on the southwestern coast of England, in a hillside home overlooking the English Channel. Although Kipling did not much care for his new house, whose design, he claimed, left its occupants feeling dispirited and gloomy, he managed to remain productive and socially active.

Kipling was now a famous man, and in the previous two or three years had increasingly been making political pronouncements in his writings. The Kiplings had welcomed their first son, John, in August 1897. Kipling had begun work on two poems, "Recessional" (1897) and "The White Man's Burden" (1899) which were to create controversy when published. Regarded by some as anthems for enlightened and duty-bound empire-building (that captured the mood of the Victorian age), the poems equally were regarded by others as propaganda for brazenfaced imperialism and its attendant racial attitudes; still others saw irony in the poems and warnings of the perils of empire.
<poem>Take up the White Man's burden—
Send forth the best ye breed—
Go, bind your sons to exile
To serve your captives' need;
To wait, in heavy harness,
On fluttered folk and wild—
Your new-caught sullen peoples,
Half devil and half child.
—"The White Man's Burden"
</poem>
There was also foreboding in the poems, a sense that all could yet come to naught.
<poem>Far-called, our navies melt away;
On dune and headland sinks the fire:
Lo, all our pomp of yesterday
Is one with Nineveh and Tyre!
Judge of the Nations, spare us yet.
Lest we forget—lest we forget!
—"Recessional"</poem>

A prolific writer during his time in Torquay, he also wrote "Stalky & Co.", a collection of school stories (born of his experience at the United Services College in Westward Ho!) whose juvenile protagonists displayed a know-it-all, cynical outlook on patriotism and authority. According to his family, Kipling enjoyed reading aloud stories from "Stalky & Co." to them and often went into spasms of laughter over his own jokes.

In early 1898, the Kiplings travelled to South Africa for their winter holiday, thus beginning an annual tradition which (excepting the following year) was to last until 1908. They always stayed in "The Woolsack", a house on Cecil Rhodes' estate at Groote Schuur (and now a student residence for the University of Cape Town); it was within walking distance of Rhodes' mansion.

With his new reputation as "Poet of the Empire", Kipling was warmly received by some of the most influential politicians of the Cape Colony, including Rhodes, Sir Alfred Milner, and Leander Starr Jameson. Kipling cultivated their friendship and came to admire the men and their politics. The period 1898–1910 was crucial in the history of South Africa and included the Second Boer War (1899–1902), the ensuing peace treaty, and the 1910 formation of the Union of South Africa. Back in England, Kipling wrote poetry in support of the British cause in the Boer War and on his next visit to South Africa in early 1900, he became a correspondent for "The Friend" newspaper in Bloemfontein, which had been commandeered by Lord Roberts for British troops.

Although his journalistic stint was to last only two weeks, it was Kipling's first work on a newspaper staff since he left "The Pioneer" in Allahabad more than ten years earlier. At "The Friend", he made lifelong friendships with Perceval Landon, H. A. Gwynne, and others. He also wrote articles published more widely expressing his views on the conflict. Kipling penned an inscription for the Honoured Dead Memorial (Siege memorial) in Kimberley.

In 1897, Kipling moved from Torquay to Rottingdean, East Sussex; first to "North End House" and later to "The Elms". In 1902, Kipling bought Bateman's, a house built in 1634 and located in rural Burwash, East Sussex, England. Bateman's was Kipling's home from 1902 until his death in 1936.

The house, along with the surrounding buildings, the mill and was purchased for £9,300. It had no bathroom, no running water upstairs, and no electricity, but Kipling loved it: "Behold us, lawful owners of a grey stone lichened house—A.D. 1634 over the door—beamed, panelled, with old oak staircase, and all untouched and unfaked. It is a good and peaceable place. We have loved it ever since our first sight of it." (from a November 1902 letter).

In the non-fiction realm he became involved in the debate over the British response to the rise in German naval power known as the Tirpitz Plan to build a fleet to challenge the Royal Navy, publishing a series of articles in 1898 which were collected as "A Fleet in Being". On a visit to the United States in 1899, Kipling and Josephine developed pneumonia, from which she eventually died.

In the wake of his daughter's death, Kipling concentrated on collecting material for what would become "Just So Stories for Little Children." That work was published in 1902, the year after "Kim" was first issued. The American literary scholar David Scott has argued that "Kim" disproves the claim made by Edward Said about Kipling as a promoter of Orientalism as Kipling – who was deeply interested in Buddhism —presented Tibetan Buddhism in a fairly sympathetic light and aspects on the novel appeared to reflect the Buddhist understanding of the universe. Kipling was offended by the German Emperor Wilhelm II's "Hun speech ()" in 1900 urging German troops being sent to China to crush the Boxer Rebellion to behave like "Huns" and to take no prisoners.

In his 1902 poem "The Rowers", Kipling attacked the Kaiser as a threat to Britain and made the first use of the term "Hun" as an anti-German insult, using Wilhelm's own words and the actions of German troops in China to portray Germans as essentially barbarians. In an interview with the French newspaper "Le Figaro", the Francophile Kipling called Germany a menace and called for an Anglo-French alliance to stop it. In another letter at the same time, Kipling described the "unfrei peoples of Central Europe" as living in "the Middle Ages with machine guns".
The first decade of the 20th century saw Kipling at the height of his popularity. In 1906, he wrote the song ""Land of our Birth, We Pledge to Thee"".

Kipling wrote a number of speculative fiction short stories, including "The Army of a Dream", in which he attempted to show a more efficient and responsible army than the hereditary bureaucracy of England at that time, and two science fiction stories, "With the Night Mail" (1905) and "As Easy As A. B. C" (1912). Both of those were set in the 21st century in Kipling's Aerial Board of Control universe. They read like modern hard science fiction, and introduced the literary technique known as indirect exposition, which would later become one of Science Fiction writer Robert Heinlein's hallmarks. This technique is one that Kipling picked up in India, and used to solve the problem of his English readers not understanding much about Indian society, when writing "The Jungle Book". Heinlein's novel "Starship Troopers" built on the citizen soldiers of "The Army of a Dream".

In 1907, he was awarded the Nobel Prize for Literature after having been nominated in that year by Charles Oman, professor at the University of Oxford. The prize citation said: "In consideration of the power of observation, originality of imagination, virility of ideas and remarkable talent for narration which characterize the creations of this world-famous author." Nobel prizes had been established in 1901 and Kipling was the first English-language recipient. At the award ceremony in Stockholm on 10 December 1907, the Permanent Secretary of the Swedish Academy, Carl David af Wirsén, praised both Kipling and three centuries of English literature:

The Swedish Academy, in awarding the Nobel Prize in Literature this year to Rudyard Kipling, desires to pay a tribute of homage to the literature of England, so rich in manifold glories, and to the greatest genius in the realm of narrative that that country has produced in our times.

"Book-ending" this achievement was the publication of two connected poetry and story collections: "Puck of Pook's Hill" (1906), and "Rewards and Fairies" (1910). The latter contained the poem "If—". In a 1995 BBC opinion poll, it was voted the UK's favourite poem. This exhortation to self-control and stoicism is arguably Kipling's most famous poem.
Such was Kipling's popularity that he was asked by his friend Max Aitken to intervene in the 1911 Canadian election on behalf of the Conservatives. In 1911, the major issue in Canada was the reciprocity treaty with the United States signed by the Liberal Prime Minister Sir Wilfrid Laurier and vigorously opposed by the Conservatives under Sir Robert Borden. On 7 September 1911, the "Montreal Daily Star" newspaper published a front-page appeal to all Canadians against the reciprocity agreement with the United States by Kipling who wrote: "It is her own soul that Canada risks today. Once that soul is pawned for any consideration, Canada must inevitably conform to the commercial, legal, financial, social, and ethical standards which will be imposed on her by the sheer admitted weight of the United States." At the time, the "Montreal Daily Star" was Canada's most read newspaper. Over the next week, Kipling's appeal was reprinted in every English newspaper in Canada and is credited with helping to turn Canadian public opinion against the Liberal government that signed the reciprocity agreement.

Kipling sympathised with the anti-Home Rule stance of Irish Unionists, who opposed Irish autonomy. He was friends with Edward Carson, the Dublin-born leader of Ulster Unionism, who raised the Ulster Volunteers to prevent Home Rule in Ireland. Kipling wrote in a letter to a friend that Ireland was not a nation, and that before the English arrived in 1169, the Irish were a gang of cattle thieves living in savagery and killing each other while "writing dreary poems" about it all. In his viewpoint, it was only British rule that allowed Ireland to advance. A visit to Ireland in 1911 confirmed Kipling's prejudices as he wrote the Irish countryside was beautiful but was spoiled by what he called the ugly homes of the Irish farmers, with Kipling adding that God had made the Irish into poets because he had "deprived them of love of line or knowledge of colour". In contrast, Kipling had nothing but praise for the "decent folk" of Protestant majority and Unionist Ulster.

Kipling wrote the poem ""Ulster"" in 1912 reflecting his Unionist politics. Kipling often referred to the Irish Unionists as "our party". Kipling had no sympathy with or understanding of Irish nationalism, and for him, Home Rule was an act of treason by the government of the Liberal Prime Minister H. H. Asquith that would plunge Ireland into the Dark Ages and allow the Irish Catholic majority to oppress the Protestant minority. The British scholar David Gilmour wrote that Kipling's lack of understanding about Ireland could be seen in that he attacked John Redmond – the Anglophile leader of the Irish Parliamentary Party who wanted Home Rule because he believed it was the best way of keeping the United Kingdom together – as a traitor working to break up the United Kingdom. "Ulster" was first publicly read at an Unionist rally in Belfast, where the largest Union Jack ever was also unfolded. In his poem "Ulster", which Kipling admitted was meant to strike a "hard blow" against the Asquith government's Home Rule bill, he wrote: "Rebellion, rapine, hate, Oppression, wrong and greed, Are loosed to rule our fate, By England's act and deed". "Ulster" generated much controversy with the Conservative MP Sir Mark Sykes – who as a Unionist was opposed to the Home Rule bill – condemning "Ulster" in an article in the "Morning Post" as a "direct appeal to ignorance and a deliberate attempt to foster religious hate".

Kipling was a staunch opponent of Bolshevism, a position which he shared with his friend Henry Rider Haggard. The two had bonded upon Kipling's arrival in London in 1889 largely on the strength of their shared opinions, and they remained lifelong friends.

According to the English magazine "Masonic Illustrated," Kipling became a Freemason in about 1885, before the usual minimum age of 21. He was initiated into Hope and Perseverance Lodge No. 782 in Lahore. He later wrote to "The Times", "I was Secretary for some years of the Lodge . . . , which included Brethren of at least four creeds. I was entered [as an Apprentice] by a member from Brahmo Somaj, a Hindu, passed [to the degree of Fellow Craft] by a Mohammedan, and raised [to the degree of Master Mason] by an Englishman. Our Tyler was an Indian Jew." Kipling received not only the three degrees of Craft Masonry but also the side degrees of Mark Master Mason and Royal Ark Mariner.

Kipling so loved his masonic experience that he memorialised its ideals in his famous poem, "The Mother Lodge", and used the fraternity and its symbols as vital plot devices in his novella, "The Man Who Would Be King".

At the beginning of the First World War, like many other writers, Kipling wrote pamphlets and poems which enthusiastically supported the UK's war aims of restoring Belgium after that kingdom had been occupied by Germany, together with more generalised statements that Britain was standing up for the cause of good. In September 1914, Kipling was asked by the British government to write propaganda, an offer that he immediately accepted. Kipling's pamphlets and stories were very popular with the British people during the war, with his major themes being glorifying the British military as "the" place for heroic men to be, German atrocities against Belgian civilians and the stories of women being brutalised by a horrific war unleashed by Germany, yet surviving and triumphing in spite of their suffering.

Kipling was enraged by reports of the Rape of Belgium together with the sinking of the RMS "Lusitania" in 1915, which he saw as a deeply inhumane act, which led him to see the war as a crusade for civilisation against barbarism. In a 1915 speech, Kipling declared that "There was no crime, no cruelty, no abomination that the mind of men can conceive of which the German has not perpetrated, is not perpetrating, and will not perpetrate if he is allowed to go on...Today, there are only two divisions in the world...human beings and Germans."

Alongside his passionate antipathy towards Germany, Kipling was privately deeply critical of how the war was fought by the British Army, complaining as early as October 1914 that Germany should have been defeated by now, and something must be wrong with the British Army. Kipling, who was shocked by the heavy losses that the British Expeditionary Force had taken by the autumn of 1914, blamed the entire pre-war generation of British politicians, who he argued had failed to learn the lessons of the Boer War. As a result, thousands of British soldiers were now paying with their lives for their failure in the fields of France and Belgium.

Kipling had scorn for those men who shirked duty in the First World War. In "The New Army in Training" (1915), Kipling concluded the piece by saying:

This much we can realise, even though we are so close to it, the old safe instinct saves us from triumph and exultation. But what will be the position in years to come of the young man who has deliberately elected to outcaste himself from this all-embracing brotherhood? What of his family, and, above all, what of his descendants, when the books have been closed and the last balance struck of sacrifice and sorrow in every hamlet, village, parish, suburb, city, shire, district, province, and Dominion throughout the Empire?

Kipling's son John was killed in action in the First World War, at the Battle of Loos in September 1915, at age 18. John had initially wanted to join the Royal Navy, but having had his application turned down after a failed medical examination due to poor eyesight, he opted to apply for military service as an Army officer. But again, his eyesight was an issue during the medical examination. In fact, he tried twice to enlist but was rejected. His father had been lifelong friends with Lord Roberts, former commander-in-chief of the British Army, and colonel of the Irish Guards, and at Rudyard's request, John was accepted into the Irish Guards.

John Kipling was sent to Loos two days into the battle in a reinforcement contingent. He was last seen stumbling through the mud blindly, with a possible facial injury. A body identified as his was found in 1992, although that identification has been challenged. In 2015, the Commonwealth War Grave Commission confirmed that they had correctly identified the burial place of John Kipling; they record his date of death as 27 September 1915, and that he is buried at St. Mary's A.D.S. Cemetery, Haisnes.

After his son's death, Kipling wrote, "If any question why we died / Tell them, because our fathers lied." It is speculated that these words may reveal his feelings of guilt at his role in getting John a commission in the Irish Guards. Others, such as English professor Tracy Bilsing, contend that the line is referring to Kipling's disgust that British leaders failed to learn the lessons of the Boer War, and were not prepared for the struggle with Germany in 1914, with the "lie" of the "fathers" being that the British Army was prepared for any war when it was not.

John's death has been linked to Kipling's 1916 poem "My Boy Jack", notably in the play "My Boy Jack" and its subsequent television adaptation, along with the documentary "". However, the poem was originally published at the head of a story about the Battle of Jutland and appears to refer to a death at sea; the 'Jack' referred to is probably a generic 'Jack Tar'. In the Kipling family, Jack was the name of the family dog while John Kipling was always John, making the identification of the protagonist of "My Boy Jack" with John Kipling somewhat questionable. However, it is true that Kipling was emotionally devastated by the death of his son. It is said that Kipling helped assuage his grief over his son's death by reading the novels of Jane Austen aloud to his wife and daughter. During the war, he wrote a booklet "The Fringes of the Fleet" containing essays and poems on various nautical subjects of the war. Some of the poems were set to music by English composer Edward Elgar.

Kipling became friends with a French soldier named Maurice Hammoneau whose life had been saved in the First World War when his copy of "Kim", which he had in his left breast pocket, stopped a bullet. Hammoneau presented Kipling with the book (with bullet still embedded) and his Croix de Guerre as a token of gratitude. They continued to correspond, and when Hammoneau had a son, Kipling insisted on returning the book and medal.

On 1 August 1918, a poem, "The Old Volunteer," appeared under his name in "The Times". The next day, he wrote to the newspaper to disclaim authorship, and a correction appeared. Although "The Times" employed a private detective to investigate (and the detective appears to have suspected Kipling himself of being the author), the identity of the hoaxer was never established.

Partly in response to John's death, Kipling joined Sir Fabian Ware's Imperial War Graves Commission (now the Commonwealth War Graves Commission), the group responsible for the garden-like British war graves that can be found to this day dotted along the former Western Front and all the other locations around the world where troops of the British Empire lie buried.

His most significant contributions to the project were his selection of the biblical phrase, "Their Name Liveth For Evermore" (Ecclesiasticus 44.14, KJV), found on the Stones of Remembrance in larger war cemeteries, and his suggestion of the phrase "Known unto God" for the gravestones of unidentified servicemen. He also chose the inscription "The Glorious Dead" on the Cenotaph, Whitehall, London. Additionally, he wrote a two-volume history of the Irish Guards, his son's regiment: it was published in 1923 and is considered to be one of the finest examples of regimental history.

Kipling's moving short story, "The Gardener", depicts visits to the war cemeteries, and the poem "The King's Pilgrimage" (1922) depicts a journey which King George V made, touring the cemeteries and memorials under construction by the Imperial War Graves Commission. With the increasing popularity of the automobile, Kipling became a motoring correspondent for the British press, and wrote enthusiastically of his trips around England and abroad, even though he was usually driven by a chauffeur.

After the war, Kipling was sceptical about the Fourteen Points and the League of Nations, but he had great hopes that the United States would abandon isolationism and that the post-war world would be dominated by an Anglo-French-American alliance. Kipling hoped that the United States would take on a League of Nations mandate for Armenia as the best way of preventing isolationism, and hoped that Theodore Roosevelt, whom Kipling admired, would once again become president. Kipling was saddened by Roosevelt's death in 1919, believing that his friend was the only American politician capable of keeping the United States in the "game" of world politics.

Kipling was very hostile towards Communism, writing about the Bolshevik take-over in 1917 that one sixth of the world had "passed bodily out of civilization". In a 1918 poem, Kipling wrote about Soviet Russia that everything good in Russia had now been destroyed by the Bolsheviks and all that was left was "the sound of weeping and the sight of burning fire, and the shadow of a people trampled into the mire".

In 1920, Kipling co-founded the Liberty League with Haggard and Lord Sydenham. This short-lived enterprise focused on promoting classic liberal ideals as a response to the rising power of Communist tendencies within Great Britain, or, as Kipling put it, "to combat the advance of Bolshevism".

In 1922, Kipling, who had made reference to the work of engineers in some of his poems, such as "The Sons of Martha", "Sappers", and "McAndrew's Hymn", and in other writings, including short story anthologies such as "The Day's Work", was asked by University of Toronto civil engineering professor Herbert E. T. Haultain for his assistance in developing a dignified obligation and ceremony for graduating engineering students. Kipling was enthusiastic in his response and shortly produced both, formally entitled "The Ritual of the Calling of an Engineer". Today, engineering graduates all across Canada are presented with an iron ring at the ceremony as a reminder of their obligation to society. In 1922 Kipling also became Lord Rector of St Andrews University in Scotland, a three-year position.

Kipling, who was a francophile, argued strongly for an Anglo-French alliance to uphold the peace, calling Britain and France in 1920 the "twin fortresses of European civilization". Along the same lines, Kipling repeatedly warned against revising the Treaty of Versailles in Germany's favour, which he predicted would lead to a new world war. An admirer of Raymond Poincaré, Kipling was one of the few British intellectuals who supported the French Occupation of the Ruhr in 1923 at a time when the British government and most public opinion was against the French position. In contrast to the popular British view of Poincaré as a cruel bully intent on impoverishing Germany by seeking unreasonable reparations, Kipling argued that Poincaré was only rightfully trying to preserve France as a great power in the face of an unfavourable situation. Kipling argued that even before 1914, Germany's larger economy and higher birth rate had made that country stronger than France; with much of France devastated by the war, and the French suffering heavy losses that the low French birth rate would have trouble replacing while Germany was mostly undamaged and still with a higher birth rate, he reasoned that the future would advantage German domination if Versailles were revised in Germany's favour. He wrote that it was madness for Britain to seek to pressure France to revise Versailles in Germany's favour.

In 1924, Kipling was opposed to the Labour government of Ramsay MacDonald as "Bolshevism without bullets", but believing that Labour was a Communist front organisation, he took the view that "excited orders and instructions from Moscow" would expose Labour as such an organisation to the British people. Kipling's views were on the right and though he admired Benito Mussolini to a certain extent for a time in the 1920s, Kipling was against fascism, writing that Oswald Mosley was "a bounder and an "arriviste"". By 1935, he called Mussolini a deranged and dangerous egomaniac and in 1933 wrote, "The Hitlerites are out for blood".

Despite his anti-communism, the first major translations of Kipling into Russian took place during Lenin's rule in the early 1920s, and during the interwar period, Kipling was very popular with Russian readers. Many of the younger Russian poets and writers such as Konstantin Simonov were influenced by Kipling. Kipling's clarity of style, his use of colloquial language and the way in which he used rhythm and rhyme were considered to be major innovations in poetry that appealed to many of the younger Russian poets.
Though it was obligatory for Soviet journals to begin translations of Kipling with an introduction attacking him as a "fascist" and an "imperialist", such was Kipling's popularity with Russian readers that his works were not banned in the Soviet Union until 1939 with the signing of the Molotov-Ribbentrop pact. Kipling's work was unbanned in the Soviet Union in 1941 after Operation Barbarossa, when Britain become a Soviet ally, but his work was banned again, this time for good, with the Cold War in 1946.
Many older editions of Rudyard Kipling's books have a swastika printed on their covers associated with a picture of an elephant carrying a lotus flower, reflecting the influence of Indian culture. Kipling's use of the swastika was based on the Indian sun symbol conferring good luck and the Sanskrit word meaning "fortunate" or "well-being". 
He used the swastika symbol in both right- and left-facing orientations, and it was in general use by others at the time.

In a note to Edward Bok written after the death of Lockwood Kipling in 1911, Rudyard said: "I am sending with this for your acceptance, as some little memory of my father to whom you were so kind, the original of one of the plaques that he used to make for me. I thought it being the Swastika would be appropriate for your Swastika. May it bring you even more good fortune." 
Once the Nazis came to power and usurped the swastika, Kipling ordered that it should no longer adorn his books. Less than a year before his death, Kipling gave a speech (titled "An Undefended Island") to The Royal Society of St George on 6 May 1935, warning of the danger which Nazi Germany posed to the British Empire.

Kipling scripted the first Royal Christmas Message, delivered via the BBC's Empire Service by George V in 1932. In 1934, he published a short story in "Strand Magazine", "Proofs of Holy Writ", which postulated that William Shakespeare had helped to polish the prose of the King James Bible.

Kipling kept writing until the early 1930s, but at a slower pace and with much less success than before. On the night of 12 January 1936 he suffered a haemorrhage in his small intestine. He underwent surgery but died less than a week later on 18 January 1936, at the age of 70 of a perforated duodenal ulcer. His death had in fact previously been incorrectly announced in a magazine, to which he wrote, "I've just read that I am dead. Don't forget to delete me from your list of subscribers."

The pallbearers at the funeral included Kipling's cousin, the Prime Minister Stanley Baldwin, and the marble casket was covered by a Union Jack. Kipling was cremated at Golders Green Crematorium in northwest London, and his ashes interred at Poets' Corner, part of the South Transept of Westminster Abbey, next to the graves of Charles Dickens and Thomas Hardy.

In 2010, the International Astronomical Union approved that a crater on the planet Mercury would be named after Kipling—one of ten newly discovered impact craters observed by the MESSENGER spacecraft in 2008–9. In 2012, an extinct species of crocodile, "Goniopholis kiplingi", was named in his honour, "in recognition for his enthusiasm for natural sciences".

More than 50 unpublished poems by Kipling, discovered by the American scholar Thomas Pinney, were released for the first time in March 2013.

Various writers, such as Edmund Candler, were strongly influenced by Kipling's writing. Kipling's stories for adults remain in print and have garnered high praise from writers as different as Poul Anderson, Jorge Luis Borges, and Randall Jarrell who wrote that, "After you have read Kipling's fifty or seventy-five best stories you realize that few men have written this many stories of this much merit, and that very few have written more and better stories."

His children's stories remain popular, and his "Jungle Books" have been made into several movies. The first was made by producer Alexander Korda, and other films have been produced by the Walt Disney Company. A number of his poems were set to music by Percy Grainger. A series of short films based on some of his stories was broadcast by the BBC in 1964. Kipling's work is still popular today.

The poet T. S. Eliot edited "A Choice of Kipling's Verse" (1941) with an introductory essay. Eliot was aware of the complaints that had been levelled against Kipling and he dismissed them one by one: that Kipling is 'a Tory' using his verse to transmit right wing political views, or 'a journalist' pandering to popular taste; while Eliot writes "I cannot find any justification for the charge that he held a doctrine of race superiority." Eliot finds instead,

Of Kipling's verse, such as his "Barrack-Room Ballads", Eliot writes "of a number of poets who have written great poetry, only... a very few whom I should call great verse writers. And unless I am mistaken, Kipling's position in this class is not only high, but unique."

In response to Eliot, George Orwell wrote a long consideration of Kipling's work for "Horizon" in 1942, noting that although as a "jingo imperialist" Kipling was "morally insensitive and aesthetically disgusting", his work had many qualities which ensured that while "every enlightened person has despised him ... nine-tenths of those enlightened persons are forgotten and Kipling is in some sense still there". Orwell said:
The poet Alison Brackenbury writes that "Kipling is poetry's Dickens, an outsider and journalist with an unrivalled ear for sound and speech."

The English folk singer Peter Bellamy was a great lover of Kipling's poetry, much of which he believed to have been influenced by English traditional folk forms. He recorded several albums of Kipling's verse set to traditional airs, or to tunes of his own composition written in traditional style. However, in the case of the bawdy folk song, "The Bastard King of England", which is commonly credited to Kipling, it is believed that the song is actually misattributed.

Kipling is often quoted in discussions of contemporary political and social issues. Political singer-songwriter Billy Bragg, who attempts to reclaim English nationalism from the right-wing, has reclaimed Kipling for an inclusive sense of Englishness. Kipling's enduring relevance has been noted in the United States, as it has become involved in Afghanistan and other areas about which he wrote.

In 1903, Kipling gave permission to Elizabeth Ford Holt to borrow themes from the "Jungle Books" to establish Camp Mowglis, a summer camp for boys on the shores of Newfound Lake in New Hampshire. Throughout their lives, Kipling and his wife Carrie maintained an active interest in Camp Mowglis, which is still in operation and continues the traditions that Kipling inspired. Buildings at Mowglis have names such as Akela, Toomai, Baloo, and Panther. The campers are referred to as "the Pack," from the youngest "Cubs" to the oldest campers living in "Den."

Kipling's links with the Scouting movements were also strong. Robert Baden-Powell, the founder of Scouting, used many themes from "The Jungle Book" stories and "Kim" in setting up his junior movement, the Wolf Cubs. These connections still exist today, such as the continued popularity of "Kim's Game" in the Scouting movement. The movement is named after Mowgli's adopted wolf family, and the adult helpers of Wolf Cub Packs adopt names taken from "The Jungle Book", especially the adult leader who is called "Akela" after the leader of the Seeonee wolf pack.

After the death of Kipling's wife in 1939, his house, "Bateman's" in Burwash, East Sussex, South East England, where he had lived from 1902 until 1936, was bequeathed to the National Trust and is now a public museum dedicated to the author. Elsie Bambridge, his only child who lived to maturity, died childless in 1976, and also bequeathed her copyrights to the National Trust, which in turn donated them to the University of Sussex to ensure better public access.

Novelist and poet Sir Kingsley Amis wrote a poem, 'Kipling at Bateman's', after visiting Kipling's Burwash home (Amis' father had lived in Burwash briefly in the 1960s) as part of a BBC television series on writers and their houses.

In 2003, actor Ralph Fiennes read excerpts from Kipling's works from the study in Bateman's, including, "The Jungle Book", "Something of Myself", "Kim", and "The Just So Stories", and poems, including "If... " and "My Boy Jack", for a CD published by the National Trust.

In modern-day India, whence he drew much of his material, Kipling's reputation remains controversial, especially amongst modern nationalists and some post-colonial critics. Rudyard Kipling was a prominent supporter of Colonel Reginald Dyer, who was responsible for the Jallianwala Bagh massacre in Amritsar (in the province of Punjab). Kipling called Dyer "the man who saved India" and also initiated collections for the latter's homecoming prize.

Other contemporary Indian intellectuals such as Ashis Nandy have taken a more nuanced view of his work. Jawaharlal Nehru, the first Prime Minister of independent India, often described Kipling's novel "Kim" as one of his favourite books.

G V Desani, an Indian writer of fiction, had a more negative opinion of Kipling. He alludes to Kipling in his novel, "All About H. Hatterr":

Indian writer Khushwant Singh wrote in 2001 that he considers Kipling's "If—" "the essence of the message of The Gita in English", referring to the Bhagavad Gita, an ancient Indian scripture.

Indian writer R. K. Narayan said, "Kipling, the supposed expert writer on India, showed a better understanding of the mind of the animals in the jungle than of the men in an Indian home or the marketplace."

In November 2007, it was announced that Kipling's birth home in the campus of the J J School of Art in Mumbai would be turned into a museum celebrating the author and his works.

Kipling's bibliography includes fiction (including novels and short stories), non-fiction, and poetry. Several of his works were collaborations.





Works
Resources


</doc>
<doc id="26309" url="https://en.wikipedia.org/wiki?curid=26309" title="Regency dance">
Regency dance

Regency dance is the term for historical dances of the period ranging roughly from 1790 to 1825. Some feel that the popular use of the term "Regency dance" is not technically correct, as the actual English Regency (the future George IV ruling on behalf of mad King George III) lasted only from 1811 until 1820. However, the term "Regency" has been used to refer to a much broader period than the historical Regency for a very long time, particularly in areas such as the history of art and architecture, literature, and clothing. This is because there are consistencies of style over this period which make having a single term useful.

Most popular exposure to this era of dance comes in the works of Jane Austen. Balls occur in her novels and are discussed in her letters, but specifics are few. Films based on her works tend to incorporate modern revival English Country Dance; however, they rarely incorporate dances actually of the period and do them without the appropriate footwork and social style which make them accurate to the period. Dances of this era were lively and bouncy, not the smooth and stately style seen in films. Steps ranging from simple skipping to elaborate ballet-style movements were used.

In the early part of this period, up to the early 1810s, the ballroom was dominated by the country dance, the cotillion, and the scotch reel. 

In the longways Country Dance, a line of couples perform figures with each other, progressing up and down the line. Regency country dances were often proceeded by a brief March by the couples, then begun by the top lady in the set and her partner, who would dance down the set to the bottom. Each couple in turn as they reached the top would likewise dance down until the entire set had returned to its original positions. This could be a lengthy process, easily taking an hour in a long set. An important social element was the calling of the dance by the leading lady (a position of honor), who would determine the figures, steps, and music to be danced. The rest of the set would listen to the calling dancing master or pick up the dance by observing the leading couple. Austen mentions in her letters instances in which she and her partner called the dance.

The cotillion was a French import, performed in a square using more elaborate footwork. It consisted of a "chorus" figure unique to each dance which was danced alternately with a standard series of up to ten "changes", which were simple figures such as a right hand moulinet (star) common to cotillions in general.

The scotch reel of the era consisted of alternate heying (interlacing) and setting (fancy steps danced in place) by a line of three or four dancers. More complex reels appear in manuals as well but it's unclear if they ever actually caught on. A sixsome reel is mentioned in a description of Scottish customs in the early 1820s and eightsome reels (danced in squares like cotillions) occur in some dance manuscripts of the era.

In the 1810s, the era of the Regency proper, English dance began an important transition with the introduction of the quadrille and the waltz.

The Waltz was first imported to England around 1810, but it was not considered socially acceptable until continental visitors at the post-Napoleonic-Wars celebrations danced it in London—and even then it remained the subject of anti-waltz diatribes, caricatures, and jokes. Even the decadent Lord Byron was scandalized by the prospect of people "embracing" on the dance floor. The Regency version is relatively slow, and done up on the balls of the feet with the arms in a variety of graceful positions. The Sauteuse is a leaping waltz commonly done in 2/4 rather than 3/4 time, similar in pattern (leap-glide-close) to the Redowa and Waltz Galop of the later nineteenth century.

First imported from France by Lady Jersey in 1815, the Quadrille was a shorter version of the earlier cotillions. Figures from individual cotillions were assembled into sets of five or six figures, and the changes were left out, producing much shorter dances. By the late 1810s, it was not uncommon to dance a series of quadrilles during the evening, generally consisting of the same first three figures combined with a variety of different fourth and fifth figures. Jane Austen's niece Fanny danced quadrilles and in their correspondence Jane mentions that she finds them much inferior to the cotillions of her own youth.

By the late 1810s, under siege from the Quadrille, dancing masters began to invent "new" forms of country dance, often with figures borrowed from the Quadrille, and giving them exotic names such as the Danse Ecossoise and Danse Espagnuole which suggested entire new dances but actually covered very minor variations in the classic form. A few of these dances became sufficiently popular that they survived through the entire 19th century. One example of this is the "Spanish dance" popular in vintage dance circles, which is a solitary survivor of its entire genre of Regency-era dances.

"La Boulangere", the only dance mentioned by name in Jane Austen's writings, is a simple circle dance for a group of couples. Sir Roger de Coverly, mentioned by Charles Dickens, is the ancestor of America's Virginia Reel.

Numerous instruction manuals survive from the Regency era. Several by Thomas Wilson are in the US Library of Congress online collection. The Scotch Reel is described by Francis Peacock, whose manual is also available in the LC collection.

The first major revival of English Country Dance, one of the major types of Regency dance, was by Englishman Cecil Sharp in the early 20th century. Various other revivals have followed, most using at least some of Sharp's research. Today, there are many groups around the world which perform a variety of English period dances, including many of the types of dance which were popular during the English Regency.

Regency dance has gained popularity at science fiction conventions, in part due to the efforts of John Hertz. Reconstructed dances from the era are taught to newcomers and experienced dancers alike. Some authors—notably, Larry Niven—have added their personal enthusiasm to the trend. Needless to say, Regency dance provides new opportunities for SF fen—typically avid costumers—to develop and wear new regalia.

In Silicon Valley, the Bay Area English Regency Society sponsors local dance classes and formal balls in churches, community centers, and other venues.
Some enthusiasts go to extremes: Cisco Systems founders Sandra Lerner and Len Bosack created a foundation that bought a Regency-era country house once owned by Jane Austen's brother. In Australia, Earthly Delights Historic Dance Academy and John Gardiner-Garden run a Regency Dance School in conjunction with Jane Austen Festival Australia every April.



</doc>
<doc id="26310" url="https://en.wikipedia.org/wiki?curid=26310" title="Reproduction">
Reproduction

Reproduction (or procreation or breeding) is the biological process by which new individual organisms – "offspring" – are produced from their "parents". Reproduction is a fundamental feature of all known life; each individual organism exists as the result of reproduction. There are two forms of reproduction: asexual and sexual.

In asexual reproduction, an organism can reproduce without the involvement of another organism. Asexual reproduction is not limited to single-celled organisms. The cloning of an organism is a form of asexual reproduction. By asexual reproduction, an organism creates a genetically similar or identical copy of itself. The evolution of sexual reproduction is a major puzzle for biologists. The two-fold cost of sexual reproduction is that only 50% of organisms reproduce and organisms only pass on 50% of their genes.

Sexual reproduction typically requires the sexual interaction of two specialized organisms, called gametes, which contain half the number of chromosomes of normal cells and are created by meiosis, with typically a male fertilizing a female of the same species to create a fertilized zygote. This produces offspring organisms whose genetic characteristics are derived from those of the two parental organisms.

Asexual reproduction is a process by which organisms create genetically similar or identical copies of themselves without the contribution of genetic material from another organism. Bacteria divide asexually via binary fission; viruses take control of host cells to produce more viruses; Hydras (invertebrates of the order "Hydroidea") and yeasts are able to reproduce by budding. These organisms often do not possess different sexes, and they are capable of "splitting" themselves into two or more copies of themselves. Most plants have the ability to reproduce asexually and the ant species Mycocepurus smithii is thought to reproduce entirely by asexual means.

Some species that are capable of reproducing asexually, like hydra, yeast (See Mating of yeasts) and jellyfish, may also reproduce sexually. For instance, most plants are capable of vegetative reproduction—reproduction without seeds or spores—but can also reproduce sexually. Likewise, bacteria may exchange genetic information by conjugation.

Other ways of asexual reproduction include parthenogenesis, fragmentation and spore formation that involves only mitosis. Parthenogenesis is the growth and development of embryo or seed without fertilization by a male. Parthenogenesis occurs naturally in some species, including lower plants (where it is called apomixis), invertebrates (e.g. water fleas, aphids, some bees and parasitic wasps), and vertebrates (e.g. some
reptiles, fish, and, very rarely, birds and sharks). It is sometimes also used to describe reproduction modes in hermaphroditic species which can self-fertilize.

Sexual reproduction is a biological process that creates a new organism by combining the genetic material of two organisms in a process that starts with meiosis, a specialized type of cell division. Each of two parent organisms contributes half of the offspring's genetic makeup by creating haploid gametes. Most organisms form two different types of gametes. In these anisogamous species, the two sexes are referred to as male (producing sperm or microspores) and female (producing ova or megaspores). In isogamous species, the gametes are similar or identical in form (isogametes), but may have separable properties and then may be given other different names (see isogamy). For example, in the green alga, "Chlamydomonas reinhardtii", there are so-called "plus" and "minus" gametes. A few types of organisms, such as many fungi and the ciliate "Paramecium aurelia", have more than two "sexes", called syngens.
Most animals (including humans) and plants reproduce sexually. Sexually reproducing organisms have different sets of genes for every trait (called alleles). Offspring inherit one allele for each trait from each parent. Thus, offspring have a combination of the parents' genes. It is believed that "the masking of deleterious alleles favors the evolution of a dominant diploid phase in organisms that alternate between haploid and diploid phases" where recombination occurs freely.

Bryophytes reproduce sexually, but the larger and commonly-seen organisms are haploid and produce gametes. The gametes fuse to form a zygote which develops into a sporangium, which in turn produces haploid spores. The diploid stage is relatively small and short-lived compared to the haploid stage, i.e. "haploid dominance". The advantage of diploidy, heterosis, only exists in the diploid life generation. Bryophytes retain sexual reproduction despite the fact that the haploid stage does not benefit from heterosis. This may be an indication that the sexual reproduction has advantages other than heterosis, such as genetic recombination between members of the species, allowing the expression of a wider range of traits and thus making the population more able to survive environmental variation.

Allogamy is the fertilization of the combination of gametes from two parents, generally the ovum from one individual with the spermatozoa of another. (In isogamous species, the two gametes will not be defined as either sperm or ovum.)

Self-fertilization, also known as autogamy, occurs in hermaphroditic organisms where the two gametes fused in fertilization come from the same individual, e.g., many vascular plants, some foraminiferans, some ciliates. The term "autogamy" is sometimes substituted for autogamous pollination (not necessarily leading to successful fertilization) and describes self-pollination within the same flower, distinguished from geitonogamous pollination, transfer of pollen to a different flower on the same flowering plant, or within a single monoecious Gymnosperm plant.

Mitosis and meiosis are types of cell division. Mitosis occurs in somatic cells, while meiosis occurs in gametes.

Mitosis
The resultant number of cells in mitosis is twice the number of original cells. The number of chromosomes in the offspring cells is the same as that of the parent cell.

Meiosis
The resultant number of cells is four times the number of original cells. This results in cells with half the number of chromosomes present in the parent cell. A diploid cell duplicates itself, then undergoes two divisions (tetraploid to diploid to haploid), in the process forming four haploid cells. This process occurs in two phases, meiosis I and meiosis II.
In recent decades, developmental biologists have been researching and developing techniques to facilitate same-sex reproduction. The obvious approaches, subject to a growing amount of activity, are female sperm and male eggs, with female sperm closer to being a reality for humans, given that Japanese scientists have already created female sperm for chickens. "However, the ratio of produced W chromosome-bearing (W-bearing) spermatozoa fell substantially below expectations. It is therefore concluded that most of the W-bearing PGC could not differentiate into spermatozoa because of restricted spermatogenesis." In 2004, by altering the function of a few genes involved with imprinting, other Japanese scientists combined two mouse eggs to produce daughter mice.

There are a wide range of reproductive strategies employed by different species. Some animals, such as the human and northern gannet, do not reach sexual maturity for many years after birth and even then produce few offspring. Others reproduce quickly; but, under normal circumstances, most offspring do not survive to adulthood. For example, a rabbit (mature after 8 months) can produce 10–30 offspring per year, and a fruit fly (mature after 10–14 days) can produce up to 900 offspring per year. These two main strategies are known as K-selection (few offspring) and r-selection (many offspring). Which strategy is favoured by evolution depends on a variety of circumstances. Animals with few offspring can devote more resources to the nurturing and protection of each individual offspring, thus reducing the need for many offspring. On the other hand, animals with many offspring may devote fewer resources to each individual offspring; for these types of animals it is common for many offspring to die soon after birth, but enough individuals typically survive to maintain the population. Some organisms such as honey bees and fruit flies retain sperm in a process called sperm storage thereby increasing the duration of their fertility.


Organisms that reproduce through asexual reproduction tend to grow in number exponentially. However, because they rely on mutation for variations in their DNA, all members of the species have similar vulnerabilities. Organisms that reproduce sexually yield a smaller number of offspring, but the large amount of variation in their genes makes them less susceptible to disease.

Many organisms can reproduce sexually as well as asexually. Aphids, slime molds, sea anemones, some species of starfish (by fragmentation), and many plants are examples. When environmental factors are favorable, asexual reproduction is employed to exploit suitable conditions for survival such as an abundant food supply, adequate shelter, favorable climate, disease, optimum pH or a proper mix of other lifestyle requirements. Populations of these organisms increase exponentially via asexual reproductive strategies to take full advantage of the rich supply resources.

When food sources have been depleted, the climate becomes hostile, or individual survival is jeopardized by some other adverse change in living conditions, these organisms switch to sexual forms of reproduction. Sexual reproduction ensures a mixing of the gene pool of the species. The variations found in offspring of sexual reproduction allow some individuals to be better suited for survival and provide a mechanism for selective adaptation to occur. The meiosis stage of the sexual cycle also allows especially effective repair of DNA damages (see Meiosis and Bernstein et al.). In addition, sexual reproduction usually results in the formation of a life stage that is able to endure the conditions that threaten the offspring of an asexual parent. Thus, seeds, spores, eggs, pupae, cysts or other "over-wintering" stages of sexual reproduction ensure the survival during unfavorable times and the organism can "wait out" adverse situations until a swing back to suitability occurs.

The existence of life without reproduction is the subject of some speculation. The biological study of how the origin of life produced reproducing organisms from non-reproducing elements is called abiogenesis. Whether or not there were several independent abiogenetic events, biologists believe that the last universal ancestor to all present life on Earth lived about 3.5 billion years ago.

Scientists have speculated about the possibility of creating life non-reproductively in the laboratory. Several scientists have succeeded in producing simple viruses from entirely non-living materials. However, viruses are often regarded as not alive. Being nothing more than a bit of RNA or DNA in a protein capsule, they have no metabolism and can only replicate with the assistance of a hijacked cell's metabolic machinery.

The production of a truly living organism (e.g. a simple bacterium) with no ancestors would be a much more complex task, but may well be possible to some degree according to current biological knowledge. A synthetic genome has been transferred into an existing bacterium where it replaced the native DNA, resulting in the artificial production of a new "M. mycoides" organism.

There is some debate within the scientific community over whether this cell can be considered completely synthetic on the grounds that the chemically synthesized genome was an almost 1:1 copy of a naturally occurring genome and, the recipient cell was a naturally occurring bacterium. The Craig Venter Institute maintains the term "synthetic bacterial cell" but they also clarify "...we do not consider this to be "creating life from scratch" but rather we are creating new life out of already existing life using synthetic DNA". Venter plans to patent his experimental cells, stating that "they are pretty clearly human inventions". Its creators suggests that building 'synthetic life' would allow researchers to learn about life by building it, rather than by tearing it apart. They also propose to stretch the boundaries between life and machines until the two overlap to yield "truly programmable organisms". Researchers involved stated that the creation of "true synthetic biochemical life" is relatively close in reach with current technology and cheap compared to the effort needed to place man on the Moon.

Sexual reproduction has many drawbacks, since it requires far more energy than asexual reproduction and diverts the organisms from other pursuits, and there is some argument about why so many species use it. George C. Williams used lottery tickets as an analogy in one explanation for the widespread use of sexual reproduction. He argued that asexual reproduction, which produces little or no genetic variety in offspring, was like buying many tickets that all have the same number, limiting the chance of "winning" - that is, producing surviving offspring. Sexual reproduction, he argued, was like purchasing fewer tickets but with a greater variety of numbers and therefore a greater chance of success. The point of this analogy is that since asexual reproduction does not produce genetic variations, there is little ability to quickly adapt to a changing environment. The lottery principle is less accepted these days because of evidence that asexual reproduction is more prevalent in unstable environments, the opposite of what it predicts.





</doc>
<doc id="26311" url="https://en.wikipedia.org/wiki?curid=26311" title="Rudyard Kipling bibliography">
Rudyard Kipling bibliography

This is a bibliography of works by Rudyard Kipling, including books, short stories, poems, and collections of his works.



Some of Kipling's works were collected by him; some others were collected by publishers of "unauthorised" editions ("Abaft the Funnel", "From Sea to Sea", for example). Still others of his works were never collected. The lists given below include all the collections that Kipling acknowledged as his own work. However, it is possible to find other works that appeared in American but not English editions, works that only appeared in an original periodical publication, and some others that only appeared in the Sussex and Burwash editions.







The last two of these editions include volume(s) of "uncollected prose".

Collections issued during his lifetime by the poet himself include:


Posthumous collections of Rudyard Kipling's poems include:

Some of Kipling's many poems are:



</doc>
<doc id="26313" url="https://en.wikipedia.org/wiki?curid=26313" title="Roget's Thesaurus">
Roget's Thesaurus

Roget's Thesaurus is a widely used English-language thesaurus, created in 1805 by Peter Mark Roget (1779–1869), British physician, natural theologian and lexicographer. It was released to the public on 29 April 1852. The original edition had 15,000 words, and each new edition has been larger. The Karpeles Manuscript Library Museum houses the original manuscript in its collection.

The name "Roget" is trademarked in parts of the world, such as the United Kingdom. By itself, it is not protected in the United States, where use of the name "Roget" in the title of a thesaurus does not necessarily indicate any relationship to Roget directly; it has come to be seen as a generic thesaurus name.

Roget described his thesaurus in the foreword to the first edition:

It is now nearly fifty years since I first projected a system of verbal classification similar to that on which the present work is founded. Conceiving that such a compilation might help to supply my own deficiencies, I had, in the year 1805, completed a classed catalogue of words on a small scale, but on the same principle, and nearly in the same form, as the Thesaurus now published.
"Roget's Thesaurus" is composed of six primary classes. Each class is composed of multiple divisions and then sections. This may be conceptualized as a tree containing over a thousand branches for individual "meaning clusters" or semantically linked words. Although these words are not strictly synonyms, they can be viewed as colours or connotations of a meaning or as a spectrum of a concept. One of the most general words is chosen to typify the spectrum as its headword, which labels the whole group.

Roget's schema of classes and their subdivisions is based on the philosophical work of Leibniz (see Leibniz—Symbolic thought), itself following a long tradition of epistemological work starting with Aristotle. Some of Aristotle's Categories are included in Roget's first class "abstract relations".





</doc>
<doc id="26316" url="https://en.wikipedia.org/wiki?curid=26316" title="Racial segregation">
Racial segregation

Racial segregation is the separation of people into racial or other ethnic groups in daily life. It may apply to activities such as eating in a restaurant, drinking from a water fountain, using a public toilet, attending school, going to the movies, riding on a bus, or in the rental or purchase of a home or of hotel rooms. Segregation is defined by the European Commission against Racism and Intolerance as "the act by which a (natural or legal) person separates other persons on the basis of one of the enumerated grounds without an objective and reasonable justification, in conformity with the proposed definition of discrimination. As a result, the voluntary act of separating oneself from other people on the basis of one of the enumerated grounds does not constitute segregation". According to the UN Forum on Minority Issues, "The creation and development of classes and schools providing education in minority languages should not be considered impermissible segregation, if the assignment to such classes and schools is of a voluntary nature".

Racial segregation is generally outlawed, but may exist "de facto" through social norms, even when there is no strong individual preference for it, as suggested by Thomas Schelling's models of segregation and subsequent work. Segregation may be maintained by means ranging from discrimination in hiring and in the rental and sale of housing to certain races to vigilante violence (such as lynchings). Generally, a situation that arises when members of different races mutually prefer to associate and do business with members of their own race would usually be described as "separation" or "de facto separation" of the races rather than "segregation". In the United States, segregation was mandated by law in some states and came with anti-miscegenation laws (prohibitions against interracial marriage). Segregation, however, often allowed close contact in hierarchical situations, such as allowing a person of one race to work as a servant for a member of another race. Segregation can involve spatial separation of the races, and mandatory use of different institutions, such as schools and hospitals by people of different races.

Wherever there have been multiracial communities, there has been racial segregation. Only areas with extensive miscegenation, or mixing, such as Hawaii and Brazil, despite some social stratification, seem to be exempt.

Following its conquest of Ottoman controlled Algeria in 1830, for well over a century France maintained colonial rule in the territory which has been described as "quasi-apartheid". The colonial law of 1865 allowed Arab and Berber Algerians to apply for French citizenship only if they abandoned their Muslim identity; Azzedine Haddour argues that this established "the formal structures of a political apartheid". Camille Bonora-Waisman writes that, "[i]n contrast with the Moroccan and Tunisian protectorates", this "colonial apartheid society" was unique to Algeria.

This "internal system of apartheid" met with considerable resistance from the Muslims affected by it, and is cited as one of the causes of the 1954 insurrection and ensuing independence war.

In fifteenth-century north-east Germany, people of Wendish, i.e. Slavic, origin were not allowed to join some guilds. According to Wilhelm Raabe, "down into the eighteenth century no German guild accepted a Wend."

German praise for America's institutional racism, previously found in Hitler's "Mein Kampf", was continuous throughout the early 1930s, and radical Nazi lawyers were advocates of the use of American models. Race based U.S. citizenship laws and anti-miscegenation laws directly inspired the two principal Nuremberg Laws—the Citizenship Law and the Blood Law. The ban on interracial marriage (anti-miscegenation) prohibited sexual relations and marriages between people classified as "Aryan" and "non-Aryan." Such relationships were called "Rassenschande" (race defilement). At first the laws were aimed primarily at Jews but were later extended to "Gypsies, Negroes and their bastard offspring". Aryans found guilty could face incarceration in a concentration camp, while non-Aryans could face the death penalty. To preserve the so-called purity of the German blood, after the war began, the Nazis extended the race defilement law to include all foreigners (non-Germans).

Under the General Government of occupied Poland in 1940, the Nazis divided the population into different groups, each with different rights, food rations, allowed housing strips in the cities, public transportation, etc. In an effort to split Polish identity they attempted to establish ethnic divisions of Kashubians and Gorals (Goralenvolk), based on these groups' alleged "Germanic component."

During the 1930s and 1940s, Jews in Nazi-controlled states were made to wear yellow ribbons or stars of David, and were, along with Romas (Gypsies), discriminated against by the racial laws. Jewish doctors were not allowed to treat Aryan patients nor were Jewish professors permitted to teach Aryan pupils. In addition, Jews were not allowed to use any public transportation, besides the ferry, and were able to shop only from 3–5 pm in Jewish stores. After "Kristallnacht" ("The Night of Broken Glass"), the Jews were fined 1,000,000 marks for damages done by the Nazi troops and SS members.

Jews and Roma were subjected to genocide as "undesirable" racial groups in the Holocaust. The Nazis established ghettos to confine Jews and sometimes Romas into tightly packed areas of the cities of Eastern Europe, turning them into "de facto" concentration camps. The Warsaw Ghetto was the largest of these ghettos, with 400,000 people. The Łódź Ghetto was the second largest, holding about 160,000.

Between 1939 and 1945, at least 1.5 million Polish citizens were transported to the Reich for forced labour (in all, about 12 million forced laborers were employed in the German war economy inside Nazi Germany). Although Nazi Germany also used forced laborers from Western Europe, Poles, along with other Eastern Europeans viewed as racially inferior, were subject to deeper discriminatory measures. They were forced to wear a yellow with purple border and letter "P" (for Polen/Polish) cloth identifying tag sewn to their clothing, subjected to a curfew, and banned from public transportation.

While the treatment of factory workers or farm hands often varied depending on the individual employer, Polish laborers as a rule were compelled to work longer hours for lower wages than Western Europeans – in many cities, they were forced to live in segregated barracks behind barbed wire. Social relations with Germans outside work were forbidden, and sexual relations ("Rassenschande" or "racial defilement") were punishable by death.

Several laws enforcing racial segregation of foreigners from Chinese were passed by the Han chinese during the Tang dynasty. In 779 the Tang dynasty issued an edict which forced Uighurs to wear their ethnic dress, stopped them from marrying Chinese females, and banned them from pretending to be Chinese. Chinese disliked Uighurs because they practiced usury. The magistrate who issued the orders may have wanted to protect "purity" in Chinese custom. In 836, when Lu Chun was appointed as governor of Canton, he was disgusted to find Chinese living with foreigners and intermarriage between Chinese and foreigners. Lu enforced separation, banning interracial marriages, and made it illegal for foreigners to own property. Lu Chun believed his principles were just and upright. The 836 law specifically banned Chinese from forming relationships with "Dark peoples" or "People of colour", which was used to describe foreigners, such as "Iranians, Sogdians, Arabs, Indians, Malays, Sumatrans", among others.

The Qing Dynasty was founded not by the Han Chinese who form the majority of the Chinese population, but the Manchus, who are today an ethnic minority of China. The Manchus were keenly aware of their minority status, however, it was only later in the dynasty that they banned intermarriage.

Han defectors played a massive role in the Qing conquest of China. Han Chinese Generals who defected to the Manchu were often given women from the Imperial Aisin Gioro family in marriage while the ordinary soldiers who defected were given non-royal Manchu women as wives. The Manchu leader Nurhaci married one of his granddaughters to the Ming General Li Yongfang after he surrendered Fushun in Liaoning to the Manchu in 1618. Jurchen (Manchu) women married most the Han Chinese defectors in Liaodong. Aisin Gioro women were married to the sons of the Han Chinese Generals Sun Sike (Sun Ssu-k'o), Geng Jimao (Keng Chi-mao), Shang Kexi (Shang K'o-hsi), and Wu Sangui (Wu San-kuei).

A mass marriage of Han Chinese officers and officials to Manchu women numbering 1,000 couples was arranged by Prince Yoto and Hongtaiji in 1632 to promote harmony between the two ethnic groups.

Geng Zhongming, a Han bannerman, was awarded the title of Prince Jingnan, and his son Geng Jingmao managed to have both his sons Geng Jingzhong and Geng Zhaozhong become court attendants under Shunzhi and get married to Aisin Gioro women, with Haoge's (a son of Hong Taiji) daughter marrying Geng Jingzhong and Prince Abatai's (Hong Taiji) granddaughter marrying Geng Zhaozhong.

The Qing differentiated between Han Bannermen and ordinary Han civilians. Han Bannermen were made out of Han Chinese who defected to the Qing up to 1644 and joined the Eight Banners, giving them social and legal privileges in addition to being acculturated to Manchu culture. So many Han defected to the Qing and swelled up the ranks of the Eight Banners that ethnic Manchus became a minority within the Banners, making up only 16% in 1648, with Han Bannermen dominating at 75%. It was this multi-ethnic force in which Manchus were only a minority, which conquered China for the Qing.

It was Han Chinese Bannermen who were responsible for the successful Qing conquest of China, they made up the majority of governors in the early Qing and were the ones who governed and administered China after the conquest, stabilizing Qing rule. Han Bannermen dominated the post of governor-general in the time of the Shunzhi and Kangxi Emperors, and also the post of governors, largely excluding ordinary Han civilians from the posts.

To promote ethnic harmony, a 1648 decree from the Manchu Shunzhi Emperor allowed Han Chinese civilian men to marry Manchu women from the Banners with the permission of the Board of Revenue if they were registered daughters of officials or commoners or the permission of their banner company captain if they were unregistered commoners, it was only later in the dynasty that these policies allowing intermarriage were done away with.

The Qing implemented a policy of segregation between the Bannermen of the Eight Banners (Manchu Bannermen, Mongol Bannermen, Han Bannermen) and Han Chinese civilians. This ethnic segregation had cultural and economic reasons: intermarriage was forbidden to keep up the Manchu heritage and minimize sinicization. Han Chinese civilians and Mongol civilians were banned from settling in Manchuria. Han civilians and Mongol civilians were banned from crossing into each other's lands. Ordinary Mongol civilians in Inner Mongolia were banned from even crossing into other Mongol Banners. (A banner in Inner Mongolia was an administrative division and not related to the Mongol Bannermen in the Eight Banners)

These restrictions did not apply Han Bannermen, who were settled in Manchuria by the Qing. Han bannermen were differentiated from Han civilians by the Qing and treated differently.

The Qing Dynasty started colonizing Manchuria with Han Chinese later on in the dynasty's rule, but the Manchu area was still separated from modern-day Inner Mongolia by the Outer Willow Palisade, which kept the Manchu and the Mongols in the area separate.

The policy of segregation applied directly to the banner garrisons, most of which occupied a separate walled zone within the cities in which they were stationed. Manchu Bannermen, Han Bannermen, and Mongol Bannermen were separated from the Han civilian population. While the Manchus followed the governmental structure of the preceding Ming dynasty, their ethnic policy dictated that appointments were split between Manchu noblemen and Han Chinese civilian officials who had passed the highest levels of the state examinations, and because of the small number of Manchus, this insured that a large fraction of them would be government officials.

In 1938, the fascist regime led by Benito Mussolini, under pressure from the Nazis, introduced a series of Italian Racial Laws instituting an official segregationist policy in the Italian Empire, especially aimed against Jews. This policy enforced various segregationist norms, like the prohibition for Jews to teach or study in ordinary schools and universities, to own industries reputed of major national interest, to work as journalists, to enter the military, and to wed non-Jews.
Some of the immediate consequences of the introduction of the 'provvedimenti per la difesa della razza' (norms for the defence of the race) included many of the best Italian scientists leaving their job, or even Italy. Amongst these, world-renowned physicists Emilio Segrè, Enrico Fermi (whose wife was Jewish), Bruno Pontecorvo, Bruno Rossi, Tullio Levi-Civita, mathematicians Federigo Enriques and Guido Fubini and even the fascist propaganda director, art critic and journalist Margherita Sarfatti, who was one of Mussolini's mistresses. Rita Levi-Montalcini, who would successively win the Nobel Prize for Medicine, was forbidden to work at the university. Albert Einstein, upon approval of the racial law, resigned from honorary membership of the Accademia dei Lincei.

After 1943, when Northern Italy was occupied by the Nazis, Italian Jews were rounded up for the Holocaust.

Jews in Europe generally were forced, by decree or by informal pressure, to live in highly segregated ghettos and shtetls. In 1204 the papacy required Jews to segregate themselves from Christians and to wear distinctive clothing. Forced segregation of Jews spread throughout Europe during the 14th and 15th centuries. In the Russian Empire, Jews were restricted to the so-called Pale of Settlement, the Western frontier of the Russian Empire corresponding roughly to the modern-day countries of Poland, Lithuania, Belarus, Moldova and Ukraine. By the early 20th century, the majority of European Jews lived in the Pale of Settlement.

Jewish population were confined to mellahs in Morocco beginning from the 15th century. In cities, a "mellah" was surrounded by a wall with a fortified gateway. In contrast, rural "mellahs" were separate villages inhabited solely by the Jews.

In the middle of the 19th century, J. J. Benjamin wrote about the life of Persian Jews:
Spanish colonists created caste systems in Latin American countries based on classification by race and race mixture. An extensive nomenclature developed, including the familiar terms "mulatto", "mestizo", and "zambo" (the latter the origin of "sambo"). The Spanish had practiced a form of caste system in Hispania before their expulsion of the Jews and Muslims. While many Latin American countries have long since rendered the system officially illegal through legislation, usually at the time of independence, prejudice based on degrees of perceived racial distance from European ancestry combined with one's socioeconomic status remain, an echo of the colonial caste system.

On 16 May 1940 the "Administrasjonsrådet" asked Rikskommisariatet why radio receivers had been confiscated from Jews in Norway. That "Administrasjonsrådet" thereafter "quietly" accepted racial segregation between Norwegian citizens, has been claimed by Tor Bomann-Larsen. Furthermore, he claimed that this segregation "created a precedent. 2 years later (with "NS-styret" in the ministries of Norway) Norwegian police arrested citizens at the addresses where radios had previously been confiscated from Jews.

Following a dispute over the terms for the granting of full independence, the British self-governing colony of Rhodesia, governed by a predominantly white minority government, unilaterally declared independence in 1965. Led by Prime Minister Ian Smith, it endured as an unrecognized state under white rule for the next 14 years, with majority rule coming in 1979 with the Internal Settlement between Smith's government and moderate black nationalists, the associated multiracial elections and the reconstitution of the country as Zimbabwe Rhodesia, with Bishop Abel Muzorewa at the helm of a coalition cabinet comprising 12 blacks and five whites. This new order also failed to win legitimacy in the eyes of the world, and British control returned to the country in December 1979, following the Lancaster House Agreement. New elections were held in 1980, and Zimbabwe gained recognized independence in April 1980, with Robert Mugabe as prime minister.

Laws enforcing segregation had been around before 1965, although many institutions simply ignored them. One highly publicized legal battle occurred in 1960 involving the opening of a new theatre that was to be open to all races; the proposed unsegregated public toilets at the newly built Reps Theatre in 1959 caused an argument called "The Battle of the Toilets".

The apartheid system carried out by Afrikaner minority rule enacted a nationwide social policy "separate development" with the National Party victory in 1948, following the "colour bar"-discriminatory legislation dating back to the beginning of the Union of South Africa and the Boer republics before which, while repressive to black South Africans along with other minorities, had not gone nearly so far.

Apartheid laws can be generally divided into following acts. Firstly, the Population Registration Act in 1950 classified residents in South Africa into four racial groups: "black", "white", "Coloured", and "Indian" and noted their racial identities on their identifications. Secondly, the Group Areas Act in 1950 assigned different regions according to different races. People were forced to live in their corresponding regions and the action of passing the boundaries without a permit was made illegal, extending pass laws that had already curtailed black movement. Thirdly, under the Reservation of Separate Amenities Act in 1953, amenities in public areas, like hospitals, universities and parks, were labeled separately according to particular races. In addition, the Bantu Education Act in 1953 segregated national education in South Africa as well. Additionally, the government of the time enforced the pass laws, which deprived black South Africans of their right to travel freely within their own country. Under this system black people were severely restricted from urban areas, requiring authorisation from a white employer to enter.

Uprisings and protests against apartheid appeared immediately when apartheid arose. As early as 1949, the youth wing of the African National Congress (ANC) advocated the ending of apartheid and suggested fighting against racial segregation by various methods. During the following decades, hundreds of anti-apartheid actions occurred, including those of the Black Consciousness Movement, students' protests, labor strikes, and church group activism etc. In 1991, the Abolition of Racially Based Land Measures Act was passed, repealing laws enforcing racial segregation, including the Group Areas Act. In 1994, Nelson Mandela won in the first multiracial democratic election in South Africa. His success fulfilled the ending of apartheid in South African history.

After Jim Crow laws were passed that segregated African Americans and Whites, the lives of those who were negatively affected saw no progress in their quest for equality. Racial segregation was not a new phenomenon, as almost four million blacks had been slaves before the Civil War. The laws passed segregated African Americans from Whites. Signs were used to show non whites where they could legally walk, talk, drink, rest, or eat. For those places that were racially mixed, blacks had to wait until all White customers were dealt with. Rules were also enforced that restricted African Americans from entering white stores. Segregated facilities extended from white only schools to white only graveyards. 

After the Thirteenth Amendment abolished slavery in America, racial discrimination became regulated by the so-called Jim Crow laws, which mandated strict segregation of the races. Though many such laws were instituted shortly after fighting ended, they only became formalized after the 1877 end of the Reconstruction period. The period that followed is known as the nadir of American race relations. The legislation (or in some states, such as Florida, the state constitutions) that mandated segregation lasted at least until Brown v. Board of Education (1954). 

While the U.S. Supreme Court majority in the 1896 "Plessy v. Ferguson" case explicitly permitted "separate but equal" facilities (specifically, transportation facilities), Justice John Marshall Harlan, in his dissent, protested that the decision was an expression of white supremacy; he predicted that segregation would "stimulate aggressions … upon the admitted rights of colored citizens," "arouse race hate," and "perpetuate a feeling of distrust between [the] races. Feelings between whites and blacks were so tense, even the jails were segregated."
Elected in 1912, President Woodrow Wilson ordered segregration throughout the federal government. In World War I, blacks served in the United States Armed Forces in segregated units. Black soldiers were often poorly trained and equipped, and were often put on the frontlines in suicide missions. The U.S. military was still heavily segregated in World War II. The air force and the marines had no blacks enlisted in their ranks. There were blacks in the Navy Seabees. The army had only five African-American officers. In addition, no African-American would receive the Medal of Honor during the war, and their tasks in the war were largely reserved to noncombat units. Black soldiers had to sometimes give up their seats in trains to the Nazi prisoners of war. 
American sports were racially segregated until the mid-twentieth century. In baseball, the "Negro leagues" were established by Rube Foster for non-white players, such as Negro league baseball, which ran through the early 1950s. In basketball, the Black Fives (all-black teams) were established in 1904, and emerged in New York City, Washington, D.C., Chicago, Pittsburgh, Philadelphia, and other cities. Racial segregation in basketball lasted until 1950, when the NBA became racially integrated. On September 11, 1964, John Lennon announced The Beatles would not play to a segregated audience in Jacksonville, Florida. City officials relented following this announcement. A contract for a 1965 Beatles concert at the Cow Palace in California specifies that the band "not be required to perform in front of a segregated audience".

In the reception to honor his Olympic success Jesse Owens was not permitted to enter through the main doors of the Waldorf Astoria New York and instead forced to travel up to the event in a freight elevator. The first black Oscar recipient Hattie McDaniel was not permitted to attend the premiere of "Gone with the Wind" with Georgia being racially segregated, and at the Oscars ceremony in Los Angeles she was required to sit at a segregated table at the far wall of the room; the hotel had a no-blacks policy, but allowed McDaniel in as a favor.

Many U.S. states banned interracial marriage. While opposed to slavery in the U.S, in a speech in Charleston, Illinois in 1858, Abraham Lincoln stated, "I am not, nor ever have been in favor of bringing about in any way the social and political equality of the white and black races, that I am not, nor ever have been in favor of making voters or jurors of negroes, nor of qualifying them to hold office, nor to intermarry with white people. I as much as any man am in favor of the superior position assigned to the white race". In 1967, Mildred Loving, a black woman, and Richard Loving, a white man, were sentenced to a year in prison in Virginia for marrying each other. Their marriage violated the state's anti-miscegenation statute, the Racial Integrity Act of 1924, which prohibited marriage between people classified as white and people classified as "colored" (persons of non-white ancestry). In the "Loving v. Virginia" case in 1967, the Supreme Court invalidated laws prohibiting interracial marriage in the U.S.

Institutionalized racial segregation was ended as an official practice during the civil rights movement by the efforts of such civil rights activists as Clarence M. Mitchell Jr., Rosa Parks, and Martin Luther King Jr., working for social and political freedom during the period from the end of World War II through the passage of the Civil Rights Act in 1964 and the Voting Rights Act in 1965 supported by President Lyndon B. Johnson. Many of their efforts were acts of non-violent civil disobedience aimed at disrupting the enforcement of racial segregation rules and laws, such as refusing to give up a seat in the black part of the bus to a white person (Rosa Parks), or holding sit-ins at all-white diners.

By 1968 all forms of segregation had been declared unconstitutional by the Supreme Court, and by 1970 support for formal legal segregation had dissolved. "Brown v. Board of Education" of Topeka, Kansas in 1954 outlawed segregation in public schools. The Fair Housing Act of 1968, administered and enforced by the Office of Fair Housing and Equal Opportunity, prohibited discrimination in the sale and rental of housing on the basis of race, color, national origin, religion, sex, familial status, and disability. Formal racial discrimination became illegal in school systems, businesses, the American military, other civil services and the government.

On 28 April 2007, the lower house of Bahraini Parliament passed a law banning unmarried migrant workers from living in residential areas. To justify the law MP Nasser Fadhala, a close ally of the government said "bachelors also use these houses to make alcohol, run prostitute rings or to rape children and housemaids".

Sadiq Rahma, technical committee head, who is a member of Al Wefaq said: "The rules we are drawing up are designed to protect the rights of both the families and the Asian bachelors (..) these labourers often have habits which are difficult for families living nearby to tolerate (..) they come out of their homes half dressed, brew alcohol illegally in their homes, use prostitutes and make the neighbourhood dirty (..) these are poor people who often live in groups of 50 or more, crammed into one house or apartment," said Mr Rahma. "The rules also state that there must be at least one bathroom for every five people (..) there have also been cases in which young children have been sexually molested."

Bahrain Centre for Human Rights issued a press release condemning this decision as discriminatory and promoting negative racist attitudes towards migrant workers. Nabeel Rajab, then BCHR vice president, said: "It is appalling that Bahrain is willing to rest on the benefits of these people's hard work, and often their suffering, but that they refuse to live with them in equality and dignity. The solution is not to force migrant workers into ghettos, but to urge companies to improve living conditions for workers – and not to accommodate large numbers of workers in inadequate space, and to improve the standard of living for them."

Since the 1970s, there has been a concern expressed by some academics that major Canadian cities are becoming more segregated on income and ethnic lines. Reports have indicated that the inner suburbs of post-merger Toronto and the southern bedroom communities of Greater Vancouver have become steadily more immigrant and visible minority dominated communities and have lagged behind other neighbourhoods in average income. A CBC panel in Vancouver in 2012 discussed the growing public fear that the proliferation of ethnic enclaves in Greater Vancouver (such as Han Chinese in Richmond and Punjabis in Surrey) amounted to a type of self-segregation. In response to these fears, many minority activists have pointed out that most Canadian neighbourhoods remain predominately White, and yet Whites are never accused of "self-segregation".

The Mohawk tribe of Kahnawake has been criticized for evicting non-Mohawks from the Mohawk reserve. Mohawks who marry outside of their tribal nation lose their right to live in their homelands. The Mohawk government claims that its policy of nationally exclusive membership is for the preservation of its identity, but there is no exemption for those who adopt Mohawk language or culture. The policy is based on a 1981 moratorium which was made law in 1984. All interracial couples are sent eviction notices regardless of how long they have lived on the reserve. The only exemption is for mixed national couples married before the 1981 moratorium.

Although some concerned Mohawk citizens contested the nationally exclusive membership policy, the Canadian Human Rights Tribunal ruled that the Mohawk government may adopt policies it deems necessary to ensure the survival of its people.

A long-standing practice of national segregation has also been imposed upon the commercial salmon fishery in British Columbia since 1992 when separate commercial fisheries were created for select aboriginal groups on three B.C. river systems. Canadians of other nations who fish in the separate fisheries have been arrested, jailed and prosecuted. Although the fishermen who were prosecuted were successful at trial (see the decision in R. v. Kapp), the decision was overturned on appeal. On final appeal, the Supreme Court of Canada ruled in favour of the program on the grounds that segregation of this workplace is a step towards equality in Canada. Affirmative action programs in Canada are protected from equality rights challenges by s. 15(2) of the Canadian Charter of Rights and Freedoms. Segregation continues today, but more than 35%of the fishermen in the BC commercial fishery are of aboriginal ancestry, yet Canadians of aboriginal ancestry comprise less than 4% of BC's population.

Two military coups in Fiji in 1987 removed a democratically elected government led by an Indo Fijian. The coup was supported principally by the ethnic Fijian population. A new constitution was promulgated in 1990, establishing Fiji as a republic, with the offices of President, Prime Minister, two-thirds of the Senate, and a clear majority of the House of Representatives reserved for ethnic Fijians; ethnic Fijian ownership of the land was also entrenched in the constitution. Most of these provisions were ended with the promulgation of the 1997 Constitution, although the President, and 14 of the 32 Senators were still selected by the all-indigenous Great Council of Chiefs. The last of these distinctions were removed by the 2013 Constitution.

Fiji's case is a situation of de facto ethnic segregation. Fiji has a long complex history with more than 3500 years as a divided tribal nation. Unification under the British rule as a colony for 96 years brought other racial groups, particularly immigrants from the Indian subcontinent.

Israeli Declaration of Independence proclaims equal rights to all citizens regardless of ethnicity, denomination or race. Israel has a substantial list of laws that demand racial equality (such as prohibition of discrimination, equality in Employment, libel based on race or ethnicity.). There is however, in practice, significant institutional, legal, and societal discrimination against the country's Arab citizens.

In 2010, the Israeli supreme court sent a message against racial segregation in a case involving the Slonim Hassidic sect of the Ashkenazi Jews, ruling that segregation between Ashkenazi and Sephardi students in a school is illegal. They argue that they seek "to maintain an equal level of religiosity, not from racism." Responding to the charges, the Slonim Haredim invited Sephardi girls to school, and added in a statement: "All along, we said it's not about race, but the High Court went out against our rabbis, and therefore we went to prison."

Due to many cultural differences, and animosity towards a minority perceived to wish to annihilate Israel, a system of passively co-existing communities, segregated along ethnic lines has emerged in Israel, with Arab-Israeli minority communities being left "marooned outside the mainstream". This de facto segregation also exists between different Jewish ethnic groups (""edot"") such as Sepharadim, Ashkenazim and Beta Israel (Jews of Ethiopian descent), which leads to de facto segregated schools, housing and public policy. The government has embarked on a program to shut down such schools, in order to force integration, but some in the Ethiopian community complained that not all such schools have been closed. In a 2007 poll commissioned by the Center Against Racism and conducted by the GeoCartographia Institute, 75% of Israeli Jews would not agree to live in a building with Arab residents, 60% would not accept any Arab visitors at their homes, 40% believed that Arabs should be stripped of their right to vote, and 59% believe that the culture of Arabs is primitive. In 2012, a public opinion poll showed that 53% of the polled Israeli Jews said they would not object to an Arab living in their building, while 42% said they would. Asked whether they would object to Arab children being in their child's class in school, 49% said they would not, 42% said they would. The secular Israeli public was found to be the most tolerant, while the religious and Haredi respondents were the most discriminatory.

The end of British colonial rule in Kenya in 1964 led to an inadvertent increase in ethnic segregation. Through private purchases and government schemes, farm land previously held by European farmers was transferred to African owners. These farms were further sub-divided into smaller localities, and, due to joint migration, many adjacent localities were occupied by members of different ethnic groups. This separation along these boundaries persists today. Kimuli Kasara, in a study of recent ethnic violence in the wake of the disputed 2007/2008 Kenyan elections, used these post-colonial boundaries as an instrument for the degree of ethnic segregation. Through a 2 Stage Least Squares Regression analysis, Kasara showed that increased ethnic segregation in Kenya's Rift Valley Province is associated with an increase in ethnic violence.

Liberian Constitution limits Liberian nationality to Negro people (see also Liberian nationality law).

For example, Lebanese and Indian nationals are active in trading, as well as in the retail and service sectors. Europeans and Americans work in the mining and agricultural sectors. These minority groups have long tenured residence in the Republic, but many are precluded from becoming citizens as a result of their race.

Malaysia has an article in its constitution which distinguishes the ethnic Malays and indigenous peoples of Malaysia—i.e. bumiputra—from the non-Bumiputra such as ethnic Chinese and Indians under the social contract, of which by law would guarantee the former certain special rights and privileges. To question these rights and privileges however is strictly prohibited under the Internal Security Act, legalised by the 10th Article(IV) of the Constitution of Malaysia. The privileges mentioned herein covers—few of which—the economical and education aspects of Malaysians, e.g. the Malaysian New Economic Policy; an economic policy recently criticised by Thierry Rommel—who headed a European Commission's delegation to Malaysia—as an excuse for "significant protectionism" and a quota maintaining higher access of Malays into public universities.

While legal racial segregation in daily life is not practiced, self-segregation does exist.

Slavery in Mauritania was finally criminalized in August 2007. It was already abolished in 1980 though it was still affecting the black Africans. The number of slaves in the country was not known exactly, but it was estimated to be up to 600,000 men, women and children, or 20% of the population.

For centuries, the so-called Haratin lower class, mostly poor black Africans living in rural areas, have been considered natural slaves by white Moors of Arab/Berber ancestry. Many descendants of the Arab and Berber tribes today still adhere to the supremacist ideology of their ancestors. This ideology has led to oppression, discrimination and even enslavement of other groups in the region of Sudan and Western Sahara. 

The United Kingdom has no legally sanctioned system of racial segregation and has a substantial list of laws that demand racial equality. However, due to many cultural differences between the pre-existing system of passively co-existing communities, segregation along racial lines has emerged in parts of the United Kingdom, with minority communities being left "marooned outside the mainstream".

The affected and 'ghettoised' communities are often largely representative of Pakistanis, Indians and other Sub-Continentals, and has been thought to be the basis of ethnic tensions, and a deterioration of the standard of living and levels of education and employment among ethnic minorities in poorer areas. These factors are considered by some to have been a cause of the 2001 race riots in Bradford, Oldham and Burnley in the north of England which have large Asian communities.

There may be some indication that such segregation, particularly in residential terms, seems to be the result of the unilateral 'steering' of ethnic groups into particular areas as well as a culture of vendor discrimination and distrust of ethnic minority clients by some estate agents and other property professionals. This may be indicative of a market preference amongst the more wealthy to reside in areas of less ethnic mixture; less ethnic mixture being perceived as increasing the value and desirability of a residential area. This is likely as other theories such as "ethnic self segregation" have sometimes been shown to be baseless, and a majority of ethnic respondents to a few surveys on the matter have been in favour of wider social and residential integration.

De facto segregation in the United States has increased since the civil rights movement. The Supreme Court ruled in Milliken v. Bradley (1974) that de facto racial segregation was acceptable, as long as schools were not actively making policies for racial exclusion; since then, schools have been segregated due to myriad indirect factors.

Redlining is the practice of denying or increasing the cost of services, such as banking, insurance, access to jobs, access to health care, or even supermarkets to residents in certain, often racially determined, areas. The most devastating form of redlining, and the most common use of the term, refers to mortgage discrimination. Over the next twenty years, a succession of further court decisions and federal laws, including the "Home Mortgage Disclosure Act" and measure to end mortgage discrimination in 1975, would completely invalidate "de jure" racial segregation and discrimination in the U.S., although "de facto" segregation and discrimination have proven more resilient. According to the Civil Rights Project at Harvard University, the actual de facto desegregation of U.S. public schools peaked in the late 1980s; since that time, the schools have, in fact, become more segregated mainly due to the ethnic segregation of the nation with whites dominating the suburbs and minorities the urban centers. According to Rajiv Sethi, an economist at Columbia University, black-white segregation in housing is slowly declining for most metropolitan areas in the US Racial segregation or separation can lead to social, economic and political tensions. Thirty years (the year 2000) after the civil rights era, the United States remained in many areas a residentially segregated society, in which blacks, whites and Hispanics inhabit different neighborhoods of vastly different quality.

Dan Immergluck writes that in 2002 small businesses in black neighborhoods still received fewer loans, even after accounting for businesses density, businesses size, industrial mix, neighborhood income, and the credit quality of local businesses. Gregory D. Squires wrote in 2003 that it is clear that race has long affected and continues to affect the policies and practices of the insurance industry. Workers living in American inner cities have a harder time finding jobs than suburban workers.

The desire of many whites to avoid having their children attend integrated schools has been a factor in white flight to the suburbs. A 2007 study in San Francisco showed that groups of homeowners of all races tended to self-segregate in order to be with people of the same education level and race. By 1990, the legal barriers enforcing segregation had been mostly replaced by decentralized racism, where whites pay more than blacks to live in predominantly white areas. Today, many whites are willing to pay a premium to live in a predominantly white neighborhood. Equivalent housing in white areas commands a higher rent. These higher rents are largely attributable to exclusionary zoning policies that restrict the supply of housing. Regulations ensure that all housing units are expensive enough to prevent access by undesirable groups. By bidding up the price of housing, many white neighborhoods effectively shut out blacks, because blacks are unwilling, or unable, to pay the premium to buy entry into these expensive neighborhoods. Conversely, equivalent housing in black neighborhoods is far more affordable to those who are unable or unwilling to pay a premium to live in white neighborhoods. Through the 1990s, residential segregation remained at its extreme and has been called "hypersegregation" by some sociologists or "American Apartheid".

In February 2005, the U.S. Supreme Court ruled in "Johnson v. California" that the California Department of Corrections' unwritten practice of racially segregating prisoners in its prison reception centers—which California claimed was for inmate safety (gangs in California, as throughout the U.S., usually organize on racial lines)—is to be subject to strict scrutiny, the highest level of constitutional review.

In Yemen, the Arab elite practices a form of discrimination against the lower class Akhdam people based on their racial system.






</doc>
<doc id="26317" url="https://en.wikipedia.org/wiki?curid=26317" title="Range">
Range

Range may refer to:










</doc>
<doc id="26318" url="https://en.wikipedia.org/wiki?curid=26318" title="Roslagen">
Roslagen

Roslagen is the name of the coastal areas of Uppland province in Sweden, which also constitutes the northern part of the Stockholm archipelago.

Historically, it was the name for all the coastal areas of the Baltic Sea, including the eastern parts of lake Mälaren, belonging to Svealand. The name was first mentioned in the year 1493 as "Rodzlagen". Before that the area has been known as "Roden", which is the coastal equivalent to inland Hundreds. When the king would issue a call to leidang, the Viking Age equivalent of military conscript service, a Roden district was responsible for raising a number of ships for the leidang navy.

The name comes from the "rodslag", which is an old coastal Uppland word for a rowing crew of warrior oarsmen. Etymologically, Roden, or Roslagen, is the source of the Finnish and Estonian names for Sweden: ' and '.

A person from Roslagen is called a "Rospigg" which means "inhabitant of Ros". Swedes from the Roslagen area, that is "the people of Ros", gave name to the Rus' people and through that to the state of Russia (see Rus' (name)).

The area also gives its name to the endangered domesticated Roslag sheep, which originated from the area centuries ago. It is served by the Roslagsbanan, a narrow-gauge railway network from Stockholm.




</doc>
<doc id="26321" url="https://en.wikipedia.org/wiki?curid=26321" title="Ramjet">
Ramjet

A ramjet, sometimes referred to as a flying stovepipe or an athodyd (an abbreviation of aero thermodynamic duct), is a form of airbreathing jet engine that uses the engine's forward motion to compress incoming air without an axial compressor or a centrifugal compressor. Because ramjets cannot produce thrust at zero airspeed, they cannot move an aircraft from a standstill. A ramjet-powered vehicle, therefore, requires an assisted take-off like a rocket assist to accelerate it to a speed where it begins to produce thrust. Ramjets work most efficiently at supersonic speeds around . This type of engine can operate up to speeds of .

Ramjets can be particularly useful in applications requiring a small and simple mechanism for high-speed use, such as missiles. Weapon designers are looking to use ramjet technology in artillery shells to give added range; a 120 mm mortar shell, if assisted by a ramjet, is thought to be able to attain a range of . They have also been used successfully, though not efficiently, as tip jets on the end of helicopter rotors.

Ramjets differ from pulsejets, which use an intermittent combustion; ramjets employ a continuous combustion process.

As speed increases, the efficiency of a ramjet starts to drop as the air temperature in the inlet increases due to compression. As the inlet temperature gets closer to the exhaust temperature, less energy can be extracted in the form of thrust. To produce a usable amount of thrust at yet higher speeds, the ramjet must be modified so that the incoming air is not compressed (and therefore heated) nearly as much. This means that the air flowing through the combustion chamber is still moving very fast (relative to the engine), in fact it will be supersonic—hence the name supersonic-combustion ramjet, or scramjet.

"L'Autre Monde: ou les États et Empires de la Lune (Comical History of the States and Empires of the Moon)" (1657) was the first of three satirical novels written by Cyrano de Bergerac, that are considered among the first science fiction stories. Arthur C Clarke credited this book with inventing the ramjet, and being the first example of a rocket-powered space flight.

The ramjet was conceived in 1913 by French inventor René Lorin, who was granted a patent for his device. Attempts to build a prototype failed due to inadequate materials.

In 1915, Hungarian inventor Albert Fonó devised a solution for increasing the range of artillery, comprising a gun-launched projectile which was to be united with a ramjet propulsion unit, thus giving a long range from relatively low muzzle velocities, allowing heavy shells to be fired from relatively lightweight guns. Fonó submitted his invention to the Austro-Hungarian Army, but the proposal was rejected. After World War I, Fonó returned to the subject of jet propulsion, in May 1928 describing an "air-jet engine" which he described as being suitable for high-altitude supersonic aircraft, in a German patent application. In an additional patent application, he adapted the engine for subsonic speed. The patent was finally granted in 1932 after four years of examination (German Patent No. 554,906, 1932-11-02).

In the Soviet Union, a theory of supersonic ramjet engines was presented in 1928 by Boris Stechkin. Yuri Pobedonostsev, chief of GIRD's 3rd Brigade, carried out a great deal of research into ramjet engines. The first engine, the GIRD-04, was designed by I.A. Merkulov and tested in April 1933. To simulate supersonic flight, it was fed by air compressed to , and was fueled with hydrogen. The GIRD-08 phosphorus-fueled ramjet was tested by firing it from an artillery cannon. These shells may have been the first jet-powered projectiles to break the speed of sound.

In 1939, Merkulov did further ramjet tests using a two-stage rocket, the R-3. In August of that year, he developed the first ramjet engine for use as an auxiliary motor of an aircraft, the DM-1. The world's first ramjet-powered airplane flight took place in December 1940, using two DM-2 engines on a modified Polikarpov I-15. Merkulov designed a ramjet fighter "Samolet D" in 1941, which was never completed. Two of his DM-4 engines were installed on the Yak-7 PVRD fighter, during World War II. In 1940, the Kostikov-302 experimental plane was designed, powered by a liquid fuel rocket for take-off and ramjet engines for flight. That project was cancelled in 1944.

In 1947, Mstislav Keldysh proposed a long-range antipodal bomber, similar to the Sänger-Bredt bomber, but powered by ramjet instead of rocket. In 1954, NPO Lavochkin and the Keldysh Institute began development of a Mach 3 ramjet-powered cruise missile, "Burya". This project competed with the R-7 ICBM being developed by Sergei Korolev, and was cancelled in 1957.

On March 1, 2018 President Vladimir Putin announced Russia had developed a (presumed) nuclear powered ramjet cruise missile capable of extended long range flight.

In 1936, Hellmuth Walter constructed a test engine powered by natural gas. Theoretical work was carried out at BMW and Junkers, as well as DFL. In 1941, Eugen Sänger of DFL proposed a ramjet engine with a very high combustion chamber temperature. He constructed very large ramjet pipes with and diameter and carried out combustion tests on lorries and on a special test rig on a Dornier Do 17Z at flight speeds of up to . Later, with petrol becoming scarce in Germany due to wartime conditions, tests were carried out with blocks of pressed coal dust as a fuel, which were not successful due to slow combustion.

The US Navy developed a series of air-to-air missiles under the name of "Gorgon" using different propulsion mechanisms, including ramjet propulsion. The ramjet Gorgon IVs, made by Glenn Martin, were tested in 1948 and 1949 at Naval Air Station Point Mugu. The ramjet engine itself was designed at the University of Southern California and manufactured by the Marquardt Aircraft Company. The engine was long and in diameter and was positioned below the missile (see photo).

Eminent Swiss astrophysicist Fritz Zwicky was research director at Aerojet and holds many patents in jet propulsion. U.S. patents 5121670 and 4722261 are for ram accelerators. The U.S. Navy would not allow Fritz Zwicky to publicly discuss his own invention, U.S. Patent 2,461,797 for the Underwater Jet, a ram jet that performs in a fluid medium. Time magazine reported Fritz Zwicky's work in the articles "Missed Swiss" on July 11, 1955 and "Underwater Jet" in the March 14, 1949 issue.

In France, the works of René Leduc were notable. Leduc's Model, the Leduc 0.10 was one of the first ramjet-powered aircraft to fly, in 1949.

The Nord 1500 Griffon reached in 1958.

The Brayton cycle is a thermodynamic cycle that describes the workings of the gas turbine engine, the basis of the airbreathing jet engine and others. It is named after George Brayton (1830–1892), the American engineer who developed it, although it was originally proposed and patented by Englishman John Wick in 1791. It is also sometimes known as the Joule cycle.

A ramjet is designed around its inlet. An object moving at high speed through air generates a high pressure region upstream. A ramjet uses this high pressure in front of the engine to force air through the tube, where it is heated by combusting some of it with fuel. It is then passed through a nozzle to accelerate it to supersonic speeds. This acceleration gives the ramjet forward thrust.

A ramjet is sometimes referred to as a 'flying stovepipe', a very simple device comprising an air intake, a combustor, and a nozzle. Normally, the only moving parts are those within the turbopump, which pumps the fuel to the combustor in a liquid-fuel ramjet. Solid-fuel ramjets are even simpler.

By way of comparison, a turbojet uses a gas turbine-driven fan to compress the air further. This gives greater compression and efficiency and far more power at low speeds (where the ram effect is weak), but is more complex, heavier, expensive, and the temperature limits of the turbine section limit the top speed and thrust at high speed.

Ramjets try to exploit the very high dynamic pressure within the air approaching the intake lip. An efficient intake will recover much of the freestream stagnation pressure, which is used to support the combustion and expansion process in the nozzle.

Most ramjets operate at supersonic flight speeds and use one or more conical (or oblique) shock waves, terminated by a strong normal shock, to slow down the airflow to a subsonic velocity at the exit of the intake. Further diffusion is then required to get the air velocity down to a suitable level for the combustor.
Subsonic ramjets do not need such a sophisticated inlet since the airflow is already subsonic and a simple hole is usually used. This would also work at slightly supersonic speeds, but as the air will choke at the inlet, this is inefficient.

The inlet is divergent, to provide a constant inlet speed of .

As with other jet engines, the combustor's job is to create hot air, by burning a fuel with the air at essentially constant pressure. The airflow through the jet engine is usually quite high, so sheltered combustion zones are produced by using 'flame holders' to stop the flames from blowing out.

Since there is no downstream turbine, a ramjet combustor can safely operate at stoichiometric fuel:air ratios, which implies a combustor exit stagnation temperature of the order of for kerosene. Normally, the combustor must be capable of operating over a wide range of throttle settings, for a range of flight speeds/altitudes. Usually, a sheltered pilot region enables combustion to continue when the vehicle intake undergoes high yaw/pitch during turns. Other flame stabilization techniques make use of flame holders, which vary in design from combustor cans to simple flat plates, to shelter the flame and improve fuel mixing. Overfuelling the combustor can cause the normal shock within a supersonic intake system to be pushed forward beyond the intake lip, resulting in a substantial drop in engine airflow and net thrust.

The propelling nozzle is a critical part of a ramjet design, since it accelerates exhaust flow to produce thrust.

For a ramjet operating at a subsonic flight Mach number, exhaust flow is accelerated through a converging nozzle. For a supersonic flight Mach number, acceleration is typically achieved via a convergent-divergent nozzle.

Although ramjets have been run as slow as , below about they give little thrust and are highly inefficient due to their low pressure ratios.

Above this speed, given sufficient initial flight velocity, a ramjet will be self-sustaining. Indeed, unless the vehicle drag is extremely high, the engine/airframe combination will tend to accelerate to higher and higher flight speeds, substantially increasing the air intake temperature. As this could have a detrimental effect on the integrity of the engine and/or airframe, the fuel control system must reduce engine fuel flow to stabilize the flight Mach number and, thereby, air intake temperature to reasonable levels.

Due to the stoichiometric combustion temperature, efficiency is usually good at high speeds (around ), whereas at low speeds the relatively poor pressure ratio means the ramjets are outperformed by turbojets, or even rockets.

Ramjets can be classified according to the type of fuel, liquid or solid; and the booster.

In a liquid fuel ramjet (LFRJ), hydrocarbon fuel (typically) is injected into the combustor ahead of a flameholder which stabilises the flame resulting from the combustion of the fuel with the compressed air from the intake(s). A means of pressurizing and supplying the fuel to the ramcombustor is required, which can be complicated and expensive. Aérospatiale-Celerg designed an LFRJ where the fuel is forced into the injectors by an elastomer bladder which inflates progressively along the length of the fuel tank. Initially, the bladder forms a close-fitting sheath around the compressed air bottle from which it is inflated, which is mounted lengthwise in the tank. This offers a lower-cost approach than a regulated LFRJ requiring a turbopump and associated hardware to supply the fuel.

A ramjet generates no static thrust and needs a booster to achieve a forward velocity high enough for efficient operation of the intake system. The first ramjet-powered missiles used external boosters, usually solid-propellant rockets, either in tandem, where the booster is mounted immediately aft of the ramjet, e.g. Sea Dart, or wraparound where multiple boosters are attached alongside the outside of the ramjet, e.g. SA-4 Ganef. The choice of booster arrangement is usually driven by the size of the launch platform. A tandem booster increases the overall length of the system, whereas wraparound boosters increase the overall diameter. Wraparound boosters will usually generate higher drag than a tandem arrangement.

Integrated boosters provide a more efficient packaging option, since the booster propellant is cast inside the otherwise empty combustor. This approach has been used on solid, for example SA-6 Gainful, liquid, for example ASMP, and ducted rocket, for example Meteor, designs. Integrated designs are complicated by the different nozzle requirements of the boost and ramjet phases of flight. Due to the higher thrust levels of the booster, a differently shaped nozzle is required for optimum thrust compared to that required for the lower thrust ramjet sustainer. This is usually achieved via a separate nozzle, which is ejected after booster burnout. However, designs such as Meteor feature nozzleless boosters. This offers the advantages of elimination of the hazard to launch aircraft from the ejected boost nozzle debris, simplicity, reliability, and reduced mass and cost, although this must be traded against the reduction in performance compared with that provided by a dedicated booster nozzle.

A slight variation on the ramjet uses the supersonic exhaust from a rocket combustion process to compress and react with the incoming air in the main combustion chamber. This has the advantage of giving thrust even at zero speed.

In a solid fuel integrated rocket ramjet (SFIRR), the solid fuel is cast along the outer wall of the ramcombustor. In this case, fuel injection is through ablation of the propellant by the hot compressed air from the intake(s). An aft mixer may be used to improve combustion efficiency. SFIRRs are preferred over LFRJs for some applications because of the simplicity of the fuel supply, but only when the throttling requirements are minimal, i.e. when variations in altitude or Mach number are limited.

In a ducted rocket, a solid fuel gas generator produces a hot fuel-rich gas which is burnt in the ramcombustor with the compressed air supplied by the intake(s). The flow of gas improves the mixing of the fuel and air and increases total pressure recovery. In a throttleable ducted rocket, also known as a variable flow ducted rocket, a valve allows the gas generator exhaust to be throttled allowing control of the thrust. Unlike an LFRJ, solid propellant ramjets cannot flame out. The ducted rocket sits somewhere between the simplicity of the SFRJ and the unlimited throttleability of the LFRJ.

Ramjets generally give little or no thrust below about half the speed of sound, and they are inefficient (less than 600 seconds) until the airspeed exceeds due to low compression ratios. Even above the minimum speed, a wide flight envelope (range of flight conditions), such as low to high speeds and low to high altitudes, can force significant design compromises, and they tend to work best optimised for one designed speed and altitude (point designs). However, ramjets generally outperform gas turbine-based jet engine designs and work best at supersonic speeds (Mach 2–4). Although inefficient at slower speeds, they are more fuel-efficient than rockets over their entire useful working range up to at least .

The performance of conventional ramjets falls off above Mach 6 due to dissociation and pressure loss caused by shock as the incoming air is slowed to subsonic velocities for combustion. In addition, the combustion chamber's inlet temperature increases to very high values, approaching the dissociation limit at some limiting Mach number.

Another example of this is the air turboramjet, which has a compressor powered by a gas heated via a heat exchanger within the combustion chamber.

Ramjets always slow the incoming air to a subsonic velocity within the combustor. Scramjets are similar to ramjets, but some of the air goes through the entire engine at supersonic speeds. This increases the stagnation pressure recovered from the freestream and improves net thrust. Thermal choking of the exhaust is avoided by having a relatively high supersonic air velocity at combustor entry. Fuel injection is often into a sheltered region below a step in the combustor wall. Although scramjet engines have been studied for many decades, only recently have small experimental units been flight tested and then only very briefly (e.g. the Boeing X-43).

As of May, 2010, this engine has been tested to attain for 200 seconds on the X-51A Waverider.

A variant of the pure ramjet is the 'combined cycle' engine, intended to overcome the limitations of the pure ramjet. One example of this is the SABRE engine; this uses a precooler, behind which is the ramjet and turbine machinery.

The ATREX engine developed in Japan is an experimental implementation of this concept. It uses liquid hydrogen fuel in a fairly exotic, single-fan arrangement. The liquid hydrogen fuel is pumped through a heat exchanger in the air intake, simultaneously heating the liquid hydrogen, and cooling the incoming air. This cooling of the incoming air is critical to achieving a reasonable efficiency. The hydrogen then continues through a second heat exchanger position after the combustion section, where the hot exhaust is used to further heat the hydrogen, turning it into a very high pressure gas. This gas is then passed through the tips of the fan to provide driving power to the fan at subsonic speeds. After mixing with the air, it is burned in the combustion chamber.

The Reaction Engines Scimitar has been proposed for the LAPCAT hypersonic airliner, and the Reaction Engines SABRE for the Reaction Engines Skylon spaceplane.

During the Cold War, the United States designed and ground-tested a nuclear-powered ramjet called Project Pluto. This system used no combustion; a nuclear reactor heated the air instead. The project was ultimately canceled because ICBMs seemed to serve the purpose better, and because a low-flying radioactive missile could cause problems for any allied soldiers.

The upper atmosphere above about contains monatomic oxygen produced by the sun through photochemistry. A concept was created by NASA for recombining this thin gas back to diatomic molecules at orbital speeds to power a ramjet.

The Bussard ramjet is a spacecraft propulsion concept intended to fuse interstellar wind and exhaust it at high speed from the rear of the vehicle.




</doc>
<doc id="26324" url="https://en.wikipedia.org/wiki?curid=26324" title="Ranma ½">
Ranma ½

"Ranma ½" has a comedic formula and a sex-changing main character, who often willfully transforms into a girl to advance his goals. The series also contains many other characters, whose intricate relationships with each other, unusual characteristics, and eccentric personalities drive most of the stories. Although the characters and their relationships are complicated, they rarely change once they are firmly introduced and settled into the series.

The manga has been adapted into two anime series created by Studio Deen: "Ranma ½" and , which together were broadcast on Fuji Television from 1989 to 1992. In addition, they developed 12 original video animations and three films. In 2011, a live-action television special was produced and aired on Nippon Television. The manga and anime series were licensed by Viz Media for English-language releases in North America. Madman Entertainment released the manga, part of the anime series and the first two movies in Australasia, while MVM Films released the first two movies in the United Kingdom. The "Ranma ½" manga has over 53 million copies in print in Japan. Both the manga and anime are cited as some of the first of their mediums to have become popular in the United States.

On a training journey in the Bayankala Mountain Range in the Qinghai Province of China, Ranma Saotome and his father Genma fall into the cursed springs at . When someone falls into a cursed spring, they take the physical form of whatever drowned there hundreds or thousands of years ago whenever they come into contact with cold water. The curse will revert when exposed to hot water until their next cold water exposure. Genma fell into the spring of a drowned panda while Ranma fell into the spring of a drowned girl.

Soun Tendo is a fellow practitioner of or "Anything-Goes School" of martial arts and owner of a dojo. Genma and Soun agreed years ago that their children would marry and carry on the Tendo Dojo. Soun has three teenaged daughters: the polite and easygoing Kasumi, the greedy and indifferent Nabiki and the short-tempered, martial arts practicing Akane. Akane, who is Ranma's age, is appointed for bridal duty by her sisters with the reasoning that they are the older sisters and can dump the duty on her, and that they all dislike the arranged engagement and think Akane's dislike of men is the right way to express it to the fathers. At the appointed time they are surprised when a panda comes in and puts a girl in front of their father. The Tendo girls all laugh. It takes several more pages for the situation to be explained to Soun Tendo and his daughters. Both Ranma and Akane refuse the engagement initially, having not been consulted on the decision, but the fathers are insistent and they are generally treated as betrothed and end up helping or saving each other on some occasions. They are frequently found in each other's company and are constantly arguing in their trademark awkward love-hate manner that is a franchise focus.

Ranma goes to school with Akane at , where he meets his recurring opponent Tatewaki Kuno, the conceited kendo team captain who aggressively pursues Akane, but also falls in love with Ranma's female form without ever discovering his curse (despite most other characters eventually knowing it). Nerima serves as a backdrop for more martial arts mayhem with the introduction of Ranma's regular rivals, such as the eternally lost Ryoga Hibiki who traveled halfway across Japan getting from the front of his house to the back, where Ranma spent three days waiting for him. Ryoga, seeking revenge on Ranma, followed him to Jusenkyo where he ultimately fell into the Spring of the Drowned Piglet. Now when splashed with cold water he takes the form of a little black pig. Not knowing this, Akane takes the piglet as a pet and names it P-chan, but Ranma knows and hates him for keeping this secret and taking advantage of the situation. Another rival is the nearsighted Mousse, who also fell into a cursed spring and becomes a duck when he gets wet, and finally, there is Genma and Soun's impish grandmaster, Happosai, who spends his time stealing the underwear of schoolgirls.

Ranma's prospective paramours include the martial arts rhythmic gymnastics champion Kodachi Kuno, and his second fiancée and childhood friend Ukyo Kuonji the okonomiyaki vendor, along with the Chinese Amazon Shampoo, supported by her great-grandmother Cologne. As the series progresses, the school becomes more eccentric with the return of the demented, Hawaii-obsessed Principal Kuno and the placement of the power-leeching alternating child/adult Hinako Ninomiya as Ranma's English teacher. Ranma's indecision to choose his true love causes chaos in his romantic and school life.

Rumiko Takahashi stated that "Ranma ½" was conceived to be a martial arts manga that connects all aspects of everyday life to martial arts. Because her previous series had female protagonists, the author decided that she wanted a male this time. However, she was worried about writing a male main character, and therefore decided to make him half-female. Before deciding on water for initiating his changes, she considered Ranma changing every time he was punched. It was after deciding this that she felt Jusenkyo had to be set in China, as it is the only place that could have such mysterious springs. She drew inspiration for "Ranma ½" from a variety of real-world objects. Some of the places frequently seen in the series are modeled after actual locations in Nerima, Tokyo (both the home of Takahashi and the setting of "Ranma ½").

In a 1990 interview with "Amazing Heroes", Takahashi stated that she had four assistants that draw the backgrounds, panel lines and tone, while she creates the story and layout, and pencils and inks the characters. All her assistants are female; Takahashi stated that "I don't use male assistants so that the girls will work more seriously if they aren't worried about boys." In 1992, she explained her process as beginning with laying out the chapter in the evening so as to finish it by dawn, and resting for a day before calling her assistants. They finish it in two or three nights, usually utilizing five days for a chapter.

Takahashi purposefully aimed the series to be popular with women and children. In 1993, an "Animerica" interviewer talking with Takahashi asked her if she intended the sex-changing theme "as an effort to enlighten a male-dominated society." Takahashi said that she does not think in terms of societal agendas and that she created the "Ranma ½" concept from simply wanting "a simple, fun idea". She added that she, as a woman and while recalling what manga she liked to read as a child, felt that "humans turning into animals might also be fun and ... you know, like a fairy tale." In 2013, she revealed that at the start of "Ranma" her editor told her to make it more dramatic, but she felt that was something she could not do. However, she admitted that drama did start to appear at the end. She also sat in on the voice actor auditions for the anime, where she insisted that male and female Ranma be voiced by different actors whose gender corresponded to that of the part.

Written and illustrated by Rumiko Takahashi, "Ranma ½" began publication in "Weekly Shōnen Sunday" issue #36 of 1987, following the ending of her series "Urusei Yatsura". From September 1987 until March 1996, the manga was published on a near weekly basis with the occasional colored page to spruce up the usually black and white stories. After nearly a decade of storylines, the final chapter was published in "Weekly Shōnen Sunday" issue #12 of 1996. The 407 chapters were periodically collected and published by Shogakukan into a total of 38 black and white "tankōbon" volumes from 1988 to 1996. They were reassembled into 38 "shinsōban" from April 2002 to October 2003.

North American publisher Viz Media originally released "Ranma ½" in a monthly comic book format that contained two chapters each from 1992 to 2003, and had the images "flipped" to read left-to-right, causing the art to be mirrored. These were periodically collected into graphic novels. On March 18, 2004, after releasing 21 volumes, Viz announced that it would reprint a number of its graphic novels. The content remained the same, but the novels moved to a smaller format with different covers and a price drop. Each volume covers roughly the same amount of material as the Japanese volumes, but retained its left-to-right format and had minor differences in grouping so that it spans 36 volumes rather than the original 38. The final volume was released in stores on November 14, 2006, thus making it Viz's longest running manga, spanning over 13 years. At Anime Expo on July 7, 2013, Viz Media announced they would begin re-releasing the manga in a format that combines two individual volumes into a single large one. With the first volume published on March 11, 2014, it marks the first time the series has been released in North America in its original right-to-left format. Madman Entertainment publishes the two-in-one version in Australasia.

Together with "Spriggan", it was the first manga published in Portugal, by Texto Editora in 1995.

An anime television series was created by Studio Deen and aired weekly between April 15, 1989 and September 16, 1989 on Fuji TV for 18 episodes, before being canceled due to low ratings. The series was then reworked by most of the same staff, retitled and launched in a different time slot, running for 143 episodes from October 20, 1989 to September 25, 1992. The anime stays true to the original manga but does differ by keeping Ranma's sex transformation a secret from the high school students, at least throughout most of its length. It also does not introduce Hikaru Gosunkugi until very late in the series, instead, Sasuke Sarugakure, the diminutive ninja retainer of the Kuno family fills a number of Gosunkugi's roles in early storylines but is a major character in his own right. The anime also alters the placement of many story arcs and contains numerous original episodes and characters not adapted from the manga.

Viz Media licensed both anime series in 1993, making "Ranma ½" one of the very first anime titles licensed by Viz. The English dub produced for the series was recorded by The Ocean Group in Vancouver, British Columbia. They released the series on VHS from their own "Viz Video" label, and on DVD a few years later in association with Pioneer Home Entertainment. Their releases collected both anime series as one, separated episodes into what they call "seasons", and changed the ordering of many of the episodes. Viz themselves re-released it on DVD in 2007 using their own DVD production company. At Otakon 2013, Viz announced that they re-acquired the TV series for Blu-ray and DVD release in 2014. The show is streamed on their anime channel service Neon Alley since Autumn 2013. Madman Entertainment licensed some of the series for release in Australasia, although their rights expired after releasing only the first four "seasons" as one series.

Studio Deen also created three theatrical films; "The Battle of Nekonron, China! A Battle to Defy the Rules!" on November 2, 1991; "Battle at Togenkyo! Get Back the Brides" on August 1, 1992; and "Super Indiscriminate Decisive Battle! Team Ranma vs. the Legendary Phoenix" on August 20, 1994. The first two movies are feature length, but the third was originally shown in theaters with two other movies: "Ghost Sweeper Mikami" and "Heisei Dog Stories: Bow".

Following the ending of the TV series, 11 original video animations were released directly to home video, the earliest on December 7, 1993 and the eleventh on June 4, 1996. All but one are based on stories originally in the manga. Twelve years later, a "Ranma" animation was created for the "It's a Rumic World" exhibition of Rumiko Takahashi's artwork. Based on the "Nightmare! Incense of Deep Sleep" manga story from volume 34, it was shown on odd numbered days at the exhibition in Tokyo from July 30 to August 11, 2008. But it was not released until January 29, 2010, when it was put in a DVD box set with the "Urusei Yatsura" and "Inuyasha" specials that premiered at the same exhibit. It was then released on DVD and Blu-ray by itself on October 20, 2010. Viz Media also licensed all three movies, and the original 11 OVAs for distribution in North America (however they released the third movie as an OVA). MVM Films has released the first two movies in the United Kingdom, while Madman Entertainment released them in Australasia.

There have been fifteen video games based on the "Ranma ½" franchise. While most are fighting games, there have been several RPGs and puzzle games. Only two have been released in Western countries. "Ranma ½: Chōnai Gekitōhen" was released in the US as "Street Combat"; the characters were Americanized, having their appearances completely changed, and the music was changed as well. However, "" was released in both North America and Europe unaltered.

A live action television adaption of "Ranma ½" aired on Nippon TV, in a two-hour time-slot, on December 9, 2011. Although it was initially reported that the special would contain an original story, the movie does take its main plot from one of the manga's early stories with several other early scenes mixed in. The special stars Yui Aragaki as Akane, with Kento Kaku and Natsuna Watanabe playing male and female Ranma respectively. Ryōsei Tayama is cast as the antagonist, the new original character Okamada. The all-girl pop group 9nine contribute "Chikutaku☆2Nite" as the theme song. It was released on both DVD and Blu-ray on March 21, 2012.

"The Ranma ½ Memorial Book" was published just as the manga ended in 1996. Acting as an end-cap to the series, it collects various illustrations from the series, features an interview with Takahashi, and includes tidbits about Ranma: summaries of his battles, his daily schedule, trivia, and a few exclusive illustrations. 
A "Movie + OVA Visual Comic" was released to illustrate the theatrical movie "Super Indiscriminate Decisive Battle! Team Ranma vs. the Legendary Phoenix" and the OVA episodes "The One to Carry On" (both parts). It also included information on the voice actors, character designs, and a layout of the Tendo dojo.

Additionally, guidebooks were released for three of the "Ranma ½" video games; these included not only strategies, but also interviews. Two books including interviews with the cast of the live-action TV drama, and some select stories, were released in 2011.

The music from the "Ranma ½" TV series, films and OVAs have been released on various CDs. Four from the TV series, two from the first movie, one from the second, one from the third movie and OVAs, and three compiling the music by DoCo used in the OVAs. DoCo is a pop group composed of the anime's main female characters' voice actresses. Several compilation albums were also released, some composed of the opening and closing theme songs and others of image songs. Many of the image songs were first released as singles.

Rumiko Takahashi said that after "Urusei Yatsura", which was popular with high school and college students, she purposefully aimed "Ranma ½" to be popular among women and children. Both series' peak readership figures were with 15-year-olds, but the distribution of "Ranma ½" readers was skewed towards younger females. By November 2006, it was reported that the series had sold over 49 million manga volumes in Japan. Shogakukan has printed 53 million copies as of November 2011. Although Lum from Takahashi's first series "Urusei Yatsura" is often cited as the first "tsundere" character in anime and manga, Theron Martin of Anime News Network stated that "Ranma ½"s Akane Tendo is closer to how they would later typically be portrayed in the 2000s. He also suggested that one could argue "Ranma" is an early example of a harem or reverse harem series, due to the main character attracting suitors in both genders. The series's publication in North America proved highly successful as well, being many Americans' first introduction to manga and its anime adaptation one of the first Japanese animation shows to achieve popularity in the US. Western comic book artists that have cited "Ranma ½" as an influence include Canadian Bryan Lee O'Malley on his series "Scott Pilgrim" and American Colleen Coover on her erotic series "Small Favors".

In an overview of the series, Jason Thompson called "Ranma ½" "the direct ancestor of all comedy-action manga, like "Sumomomo Momomo" and "History's Strongest Disciple Kenichi"", although noted that it was not the first, but only spanned the period when manga and anime sales were at their height. Relating it to Takahashi's other works, he summed the series up as "At the start, the fighting is minimal and it's almost a semi-serious relationship comedy, like "Maison Ikkoku"; then it turns completely ridiculous; and by the climax, when Ranma fights the evil bird-people of Phoenix Mountain in an excessively long and un-funny shonen fight scene, it's like a warmup for "Inuyasha"." He states that "Eventually Takahashi adds too many characters, and the manga starts repeating itself. Because of the lack of a strong story arc, a lot of people stop reading "Ranma ½" at some point in the middle". Reviewing Viz Media's final English volume of the manga, Anime News Network remarked that "Every dimension of Rumiko Takahashi's storytelling skills come into play here: comedy, romance and introspection, and of course, high-flying fantasy martial-arts action." However, they felt some of the action scenes were hard to follow and noted that the mirroring to left-to-right format caused errors with the art.

The "Ranma ½" anime was ranked number 17 on "Anime Insider"'s 2001 list of the Top 50 Anime, although the list was limited to series that were released in North America. It ranked 36th on TV Asahi's 2006 list of Japan's 100 favorite animated TV series, which is based on an online poll of the Japanese people, up from the previous year's list where it ranked 45th. In November 2006, the New York Comic Con announced that it would host the first-ever American Anime Awards. Fans had the chance to vote for their favorite anime online during the month of January 2007. Only the five nominees receiving the most votes for each category were announced on February 5. Among the 12 different categories, "Ranma ½" was voted into the "Best Comedy Anime" category, and the "Ranma ½" OVAs were voted into the "Best Short Series" category. In their review of Viz Media's season five DVD box set, Anime News Network praised the Japanese cast's performance and the animation, but criticized the English version's slight script changes and minor voice actors while praising its main cast. They also remarked that while "Ranma ½" is a classic, after a hundred episodes, the same jokes are just not funny anymore. THEM Anime Reviews' Raphael See called the television series and the OVAs "one of the funniest things [he's] ever seen, anime or otherwise" and also praised the English dub as some of the best. However, he was much more critical of the first two movies particularly for both using the same damsel in distress plot. Mike Toole of ANN included "Big Trouble in Nekronon, China" at number 83 on The Other 100 Best Anime Movies of All Time, a list of "lesser-known, lesser-loved classics," calling it "a solid action-comedy and a good, well-rounded example of the appeal of "Ranma ½""



</doc>
<doc id="26327" url="https://en.wikipedia.org/wiki?curid=26327" title="Royal Australian Navy">
Royal Australian Navy

The Royal Australian Navy (RAN) is the naval branch of the Australian Defence Force. Following the Federation of Australia in 1901, the ships and resources of the separate colonial navies were integrated into a national force: the Commonwealth Naval Forces. Originally intended for local defence, the navy was granted the title of 'Royal Australian Navy' in 1911, and became increasingly responsible for defence of the region.

Britain's Royal Navy continued to support the RAN and provided additional blue-water defence capability in the Pacific up to the early years of World War II. Then, rapid wartime expansion saw the acquisition of large surface vessels and the building of many smaller warships. In the decade following the war, the RAN acquired a small number of aircraft carriers, the last of these paying off in 1982.

Today, the RAN consists of 47 commissioned vessels, 3 non-commissioned vessels and over 16,000 personnel. The navy is one of the largest and most sophisticated naval forces in the South Pacific region, with a significant presence in the Indian Ocean and worldwide operations in support of military campaigns and peacekeeping missions. The current Chief of Navy is Vice Admiral Tim Barrett.

The Commonwealth Naval Forces were established on 1 March 1901, two months after the federation of Australia, when the naval forces of the separate Australian colonies were amalgamated. A period of uncertainty followed as the policy makers sought to determine the newly established force's requirements and purpose, with the debate focusing upon whether Australia's naval force would be structured mainly for local defence or whether it would be designed to serve as a fleet unit within a larger imperial force, controlled centrally by the British Admiralty. In 1908–09, the decision was made to pursue a compromise solution, and the Australian government agreed to establish a force that would be used for local defence but which would be capable of forming a fleet unit within the imperial naval strategy, albeit without central control. As a result, the navy's force structure was set at "one battlecruiser, three light cruisers, six destroyers and three submarines".

On 10 July 1911, King George V granted the service the title of "Royal Australian Navy". The first of the RAN's new vessels, the destroyer "Yarra", was completed in September 1910 and by the outbreak of the First World War the majority of the RAN's planned new fleet had been realised. The Australian Squadron was placed under control of the British Admiralty, and initially it was tasked with capturing many of Germany's South Pacific colonies and protecting Australian shipping from the German East Asia Squadron. Later in the war, most of the RAN's major ships operated as part of Royal Navy forces in the Mediterranean and North Seas, and then later in the Adriatic, and then the Black Sea following the surrender of the Ottoman Empire.

In 1919, the RAN received a force of six destroyers, three sloops and six submarines from the Royal Navy, but throughout the 1920s and early 1930s, the RAN was drastically reduced in size due to a variety of factors including political apathy and economic hardship as a result of the Great Depression. In this time the focus of Australia's naval policy shifted from defence against invasion to trade protection, and several fleet units were sunk as targets or scrapped. By 1923, the size of the navy had fallen to eight vessels, and by the end of the decade it had fallen further to five, with just 3,500 personnel. In the late 1930s, as international tensions increased, the RAN was modernised and expanded, with the service receiving primacy of funding over the Army and Air Force during this time as Australia began to prepare for war.

Early in the Second World War, RAN ships again operated as part of Royal Navy formations, many serving with distinction in the Mediterranean, the Red Sea, the Persian Gulf, the Indian Ocean, and off the West African coast. Following the outbreak of the Pacific War and the virtual destruction of British naval forces in south-east Asia, the RAN operated more independently, or as part of United States Navy formations. As the navy took on an even greater role, it was expanded significantly and at its height the RAN was the fourth-largest navy in the world, with 39,650 personnel operating 337 warships. A total of 34 vessels were lost during the war, including three cruisers and four destroyers.

After the Second World War, the size of the RAN was again reduced, but it gained new capabilities with the acquisition of two aircraft carriers, "Sydney" and "Melbourne". The RAN saw action in many Cold War–era conflicts in the Asia-Pacific region and operated alongside the Royal Navy and United States Navy off Korea, Malaysia, and Vietnam. Since the end of the Cold War, the RAN has been part of Coalition forces in the Persian Gulf and Indian Ocean, operating in support of Operation Slipper and undertaking counter piracy operations. It was also deployed in support of Australian peacekeeping operations in East Timor and the Solomon Islands.

The strategic command structure of the RAN was overhauled during the New Generation Navy changes. The RAN is commanded through Naval Headquarters (NHQ) in Canberra. The professional head is the Chief of Navy (CN), who holds the rank of vice admiral. NHQ is responsible for implementing policy decisions handed down from the Department of Defence and for overseeing tactical and operational issues that are the purview of the subordinate commands.

Beneath NHQ are two subordinate commands:

Fleet Command was previously made up of seven Force Element Groups, but after the New Generation Navy changes, this was restructured into four Force Commands:

As of September 2017, the RAN fleet consisted of 47 warships, including destroyers, frigates, submarines, patrol boats and auxiliary ships. Ships commissioned into the RAN are given the prefix HMAS (His/Her Majesty's Australian Ship).

The RAN has two primary bases for its fleet:

In addition, three other bases are home to the majority of the RAN's minor war vessels:

The RAN currently operates 47 commissioned vessels, made up of eight ship classes and three individual ships, plus three non-commissioned vessels. In addition, DMS Maritime operates a large number of civilian-crewed vessels under contract to the Australian Defence Force.

The Fleet Air Arm (previously known as the Australian Navy Aviation Group) provides the RAN's aviation capability. As of 2013, the FAA consists of three active squadrons plus a fourth being activated, operating five helicopter types in the anti-submarine warfare and maritime support roles. The Fleet Air Arm is based at in Nowra, New South Wales, and operates from the RAN's frigates, large amphibious warfare vessels, and large support ships.

In addition to the helicopter squadrons of the Fleet Air Arm, the RAN operates an additional flying unit that comes under the operational responsibility of the Australian Hydrographic Service. The Laser Airborne Depth Sounder Flight contains the sole remaining fixed-wing aircraft operated by the RAN, and is based at in Cairns, Queensland.

The Clearance Diving Branch is composed of two "Clearance Diving Teams" (CDT) that serve as parent units for naval clearance divers:

When clearance divers are sent into combat, Clearance Diving Team Three (AUSCDT THREE) is formed.

The CDTs have two primary roles:

There are currently several major projects underway that will see upgrades to RAN capabilities:

Future procurement plans include:

The RAN currently has forces deployed on four major operations:

As of June 2011, the RAN has 14,215 permanent full-time personnel, 161 gap year personnel, and 2,150 reserve personnel. The permanent full-time force consisted of 3,357 commissioned officers, and 10,697 enlisted personnel. In June 2010, male personnel made up 82% of the permanent full-time force, while female personnel made up 18%. The RAN has the highest percentage of women in the ADF, compared to the RAAF's 17.8% and the Army's 9.7%.

The following are the current senior Royal Australian Navy officers:

The uniforms of the Royal Australian Navy are very similar in cut, colour and insignia to their British Royal Navy forerunners. However, beginning with the Second World War, all RAN personnel began wearing shoulder flashes reading "Australia", a practice continuing today. These are cloth arcs at shoulder height on uniforms, metallic gold on officers' shoulder boards, and embroidered on shoulder slip-ons.

Commissioned officers of the Australian Navy have pay grades ranging from S-1 to O-11. The only O-11 position in the navy is honorary and has only ever been held by royalty, currently being held by HRH The Duke of Edinburgh. The highest position occupied in the current Royal Australian Navy structure is O-9, a vice admiral who serves as the Chief of the Navy. O-8 (rear admiral) to O-11 (admiral of the fleet) are referred to as flag officers, O-5 (commander) and above are referred to as senior officers, while S-1 (midshipman) to O-4 (lieutenant commander) are referred to as junior officers. All officers of the navy receive a commission from Her Majesty Queen Elizabeth II, Queen of Australia. The commissioning scroll issued in recognition of the commission is signed by the Governor General of Australia as Commander-in-Chief and the serving Minister for Defence.

Naval officers are trained at the Royal Australian Naval College (HMAS "Creswell") in Jervis Bay, New South Wales and the Australian Defence Force Academy in Canberra.

Chaplains in the Royal Australian Navy are commissioned officers who complete the same training as other officers in the RAN at the Royal Australian Naval College, HMAS Creswell. RAN regulations group RAN chaplains with commanders for purposes of protocol such as marks of respect (saluting); however, RAN chaplains have no other rank other than "chaplain", and their rank emblem is identifiable by a Maltese cross with gold anchor. Senior chaplains are grouped with captains, and principal chaplains are grouped with commodores, but their chaplain rank slide remains the same. Principal chaplains, however, have gold braid on the peak of their white service cap. 

Royal Australian Navy Other Ranks wear "right arm rates" insignia, called "Category Insignia" to indicate speciality training qualifications. The use pattern mirrors that of the Royal Navy, and has since formation. Stars or a Crown are added to these to indicate higher qualifications.

The Warrant Officer of the Navy (WO-N) is an appointment held by the most senior sailor in the RAN, and holds the rank of warrant officer (WO). However, the WO-N does not wear the WO rank insignia; instead, they wear the special insignia of the appointment. The WO-N appointment has similar equivalent appointments in the other services, each holding the rank of warrant officer, each being the most senior sailor/soldier/airman in that service, and each wearing their own special insignia rather than their rank insignia. The Australian Army equivalent is the Regimental Sergeant Major of the Army (RSM-A) and the Royal Australian Air Force equivalent is the Warrant Officer of the Air Force (WOFF-AF).





</doc>
<doc id="26328" url="https://en.wikipedia.org/wiki?curid=26328" title="Royal Australian Air Force">
Royal Australian Air Force

The Royal Australian Air Force (RAAF), formed March 1921, is the aerial warfare branch of the Australian Defence Force (ADF). It operates the majority of the ADF's fixed wing aircraft, although both the Australian Army and Royal Australian Navy also operate aircraft in various roles. It directly continues the traditions of the Australian Flying Corps (AFC), formed on 22 October 1912. The RAAF provides support across a spectrum of operations such as air superiority, precision strikes, intelligence, surveillance and reconnaissance, air mobility, and humanitarian support.

The RAAF took part in many of the 20th century's major conflicts. During the early years of the Second World War a number of RAAF bomber, fighter, reconnaissance and other squadrons served in Britain, and with the Desert Air Force located in North Africa and the Mediterranean. From 1942, a large number of RAAF units were formed in Australia, and fought in South West Pacific Area. Thousands of Australians also served with other Commonwealth air forces in Europe, including during the bomber offensive against Germany. By the time the war ended, a total of 216,900 men and women served in the RAAF, of whom 10,562 were killed in action.

Later the RAAF served in the Berlin Airlift, Korean War, Malayan Emergency, Indonesia–Malaysia Confrontation and Vietnam War. More recently, the RAAF has participated in operations in East Timor, the Iraq War, the War in Afghanistan, and the military intervention against the Islamic State of Iraq and the Levant (ISIL).

The RAAF has 259 aircraft, of which 110 are combat aircraft.

The RAAF traces its history back to the Imperial Conference held in London in 1911, where it was decided aviation should be developed within the armed forces of the British Empire. Australia implemented this decision, the first dominion to do so, by approving the establishment of the "Australian Aviation Corps". This initially consisted of the Central Flying School at Point Cook, Victoria, opening on 22 October 1912. By 1914 the corps was known as the "Australian Flying Corps".

Soon after the outbreak of war in 1914, the Australian Flying Corps sent aircraft to assist in capturing German colonies in what is now north-east New Guinea. However, these colonies surrendered quickly, before the planes were even unpacked. The first operational flights did not occur until 27 May 1915, when the Mesopotamian Half Flight was called upon to assist the Indian Army in protecting British oil interests in what is now Iraq.

The corps later saw action in Egypt, Palestine and on the Western Front throughout the remainder of the First World War. By the end of the war, four squadrons—Nos. 1, 2, 3 and 4—had seen operational service, while another four training squadrons—Nos. 5, 6, 7 and 8—had also been established. A total of 460 officers and 2,234 other ranks served in the AFC, whilst another 200 men served as aircrew in the British flying services. Casualties included 175 dead, 111 wounded, 6 gassed and 40 captured.

The Australian Flying Corps remained part of the Australian Army until 1919, when it was disbanded along with the First Australian Imperial Force (AIF). Although the Central Flying School continued to operate at Point Cook, military flying virtually ceased until 1920, when the Australian Air Corps (AAC) was formed. The Australian Air Force was formed on 31 March 1921. King George V approved the prefix "Royal" in June 1921 and became effective on 31 August 1921. The RAAF then became the second Royal air arm to be formed in the British Commonwealth, following the British Royal Air Force. When formed the RAAF had more aircraft than personnel, with 21 officers and 128 other ranks and 153 aircraft.

In September 1939, the Australian Air Board directly controlled the Air Force via RAAF Station Laverton, RAAF Station Richmond, RAAF Station Pearce, No. 1 Flying Training School RAAF at Point Cook, RAAF Station Rathmines and five smaller units.

In 1939, just after the outbreak of the Second World War, Australia joined the Empire Air Training Scheme, under which flight crews received basic training in Australia before travelling to Canada for advanced training. A total of 17 RAAF bomber, fighter, reconnaissance and other squadrons served initially in Britain and with the Desert Air Force located in North Africa and the Mediterranean. Thousands of Australians also served with other Commonwealth air forces in Europe during the Second World War. About nine percent of the personnel who served under British RAF commands in Europe and the Mediterranean were RAAF personnel.

With British manufacturing targeted by the German Luftwaffe, in 1941 the Australian government created the Department of Aircraft Production (DAP; later known as the Government Aircraft Factories) to supply Commonwealth air forces, and the RAAF was eventually provided with large numbers of locally built versions of British designs such as the DAP Beaufort torpedo bomber, Beaufighters and Mosquitos, as well as other types such as Wirraways, Boomerangs, and Mustangs.

In the European theatre of the war, RAAF personnel were especially notable in RAF Bomber Command: although they represented just two percent of all Australian enlistments during the war, they accounted for almost twenty percent of those killed in action. This statistic is further illustrated by the fact that No. 460 Squadron RAAF, mostly flying Avro Lancasters, had an official establishment of about 200 aircrew and yet had 1,018 combat deaths. The squadron was therefore effectively wiped out five times over. Total RAAF casualties in Europe were 5,488 killed or missing.

The beginning of the Pacific War—and the rapid advance of Japanese forces—threatened the Australian mainland for the first time in its history. The RAAF was quite unprepared for the emergency, and initially had negligible forces available for service in the Pacific. In 1941 and early 1942, many RAAF airmen, including Nos. 1, 8, 21 and 453 Squadrons, saw action with the RAF Far East Command in the Malayan, Singapore and Dutch East Indies campaigns. Equipped with aircraft such as the Brewster Buffalo, and Lockheed Hudsons, the Australian squadrons suffered heavily against Japanese Zeros.

During the fighting for Rabaul in early 1942, No. 24 Squadron RAAF fought a brief, but ultimately futile defence as the Japanese advanced south towards Australia. The devastating air raids on Darwin on 19 February 1942 increased concerns about the direct threat facing Australia. In response, some RAAF squadrons were transferred from the northern hemisphere—although a substantial number remained there until the end of the war. Shortages of fighter and ground attack planes led to the acquisition of US-built Curtiss P-40 Kittyhawks and the rapid design and manufacture of the first Australian fighter, the CAC Boomerang. RAAF Kittyhawks came to play a crucial role in the New Guinea and Solomon Islands campaigns, especially in operations like the Battle of Milne Bay. As a response to a possible Japanese chemical warfare threat the RAAF imported hundreds of thousands of chemical weapons into Australia.

In the Battle of the Bismarck Sea, imported Bristol Beaufighters proved to be highly effective ground attack and maritime strike aircraft. Beaufighters were later made locally by the DAP from 1944. Although it was much bigger than Japanese fighters, the Beaufighter had the speed to outrun them. The RAAF operated a number of Consolidated PBY Catalina as long range bombers and scouts. The RAAF's heavy bomber force was predominantly made up of 287 B-24 Liberators, equipping seven squadrons, which could bomb Japanese targets as far away as Borneo and the Philippines from airfields in Australia and New Guinea. By late 1945, the RAAF had received or ordered about 500 P-51 Mustangs, for fighter/ground attack purposes. The Commonwealth Aircraft Corporation initially assembled US-made Mustangs, but later manufactured most of those used.

By mid-1945, the RAAF's main operational formation in the Pacific, the First Tactical Air Force (1st TAF), consisted of over 21,000 personnel, while the RAAF as a whole consisted of about 50 squadrons and 6,000 aircraft, of which over 3,000 were operational. The 1st TAF's final campaigns were fought in support of Australian ground forces in Borneo, but had the war continued some of its personnel and equipment would likely have been allocated to the invasion of the Japanese mainland, along with some of the RAAF bomber squadrons in Europe, which were to be grouped together with British and Canadian squadrons as part of the proposed Tiger Force. However, the war was brought to a sudden end by the US nuclear attacks on Japan. The RAAF's casualties in the Pacific were around 2,000 killed, wounded or captured.

By the time the war ended, a total of 216,900 men and women served in the RAAF, of whom 10,562 were killed in action; a total of 76 squadrons were formed. With over 152,000 personnel operating nearly 6,000 aircraft it was the world's fourth largest air force.

During the Berlin Airlift, in 1948–49, the RAAF Squadron Berlin Air Lift aided the international effort to fly in supplies to the stricken city; two RAF Avro York aircraft were also crewed by RAAF personnel. Although a small part of the operation, the RAAF contribution was significant, flying 2,062 sorties and carrying 7,030 tons of freight and 6,964 passengers.

In the Korean War, from 1950–53, North American Mustangs from No. 77 Squadron RAAF, stationed in Japan with the British Commonwealth Occupation Force, were among the first United Nations aircraft to be deployed, in ground support, combat air patrol, and escort missions. When the UN planes were confronted by North Korean Mikoyan-Gurevich MiG-15 jet fighters, 77 Sqn acquired Gloster Meteors, however the MiGs remained superior and the Meteors were relegated to ground support missions as the North Koreans gained experience. The air force also operated transport aircraft during the conflict. No. 77 Squadron flew 18,872 sorties, claiming the destruction of 3,700 buildings, 1,408 vehicles, 16 bridges, 98 railway carriages and an unknown number of enemy personnel. Three MiG-15s were confirmed destroyed, and two others probably destroyed. RAAF casualties included 41 killed and seven captured; 66 aircraft – 22 Mustangs and 44 Meteors – were lost.

In July 1952, No. 78 Wing RAAF was deployed to Malta in the Mediterranean where it formed part of a British force which sought to counter the Soviet Union's influence in the Middle East as part of Australia's Cold War commitments. Consisting of No. 75 and 76 Squadrons equipped with de Havilland Vampire jet fighters, the wing provided an air garrison for the island for the next two and half years, returning to Australia in late 1954.

In 1953, a Royal Air Force officer, Air Marshal Sir Donald Hardman, was brought out to Australia to become Chief of the Air Staff. He reorganised the RAAF into three commands: Home Command, Maintenance Command, and Training Command. Five years later, Home Command was renamed Operational Command, and Training Command and Maintenance Command were amalgamated to form Support Command.

In the Malayan Emergency, from 1950–60, six Avro Lincolns from No. 1 Squadron RAAF and a flight of Douglas Dakotas from No. 38 Squadron RAAF took part in operations against the communist guerrillas (labelled as "Communist Terrorists" by the British authorities) as part of the RAF Far East Air Force. The Dakotas were used on cargo runs, in troop movement and in paratroop and leaflet drops within Malaya. The Lincolns, operating from bases in Singapore and from Kuala Lumpur, formed the backbone of the air war against the CTs, conducting bombing missions against their jungle bases. Although results were often difficult to assess, they allowed the government to harass CT forces, attack their base camps when identified and keep them on the move. Later, in 1958, Canberra bombers from No. 2 Squadron RAAF were deployed to Malaya and took part in bombing missions against the CTs.

During the Vietnam War, from 1964–72, the RAAF contributed Caribou STOL transport aircraft as part of the RAAF Transport Flight Vietnam, later redesignated No. 35 Squadron RAAF, UH-1 Iroquois helicopters from No. 9 Squadron RAAF, and English Electric Canberra bombers from No. 2 Squadron RAAF. The Canberras flew 11,963 bombing sorties, and two aircraft were lost. One went missing during a bombing raid. The wreckage of the aircraft was recovered in April 2009, and the remains of Flying Officer Michael Herbert and Pilot Officer Robert Carver were found in late July 2009. The other was shot down by a surface-to-air missile, although both crew were rescued. They dropped 76,389 bombs and were credited with 786 enemy personnel confirmed killed and a further 3,390 estimated killed, 8,637 structures, 15,568 bunkers, 1,267 sampans and 74 bridges destroyed. RAAF transport aircraft also supported anti-communist ground forces. The UH-1 helicopters were used in many roles including medical evacuation and close air support. RAAF casualties in Vietnam included six killed in action, eight non-battle fatalities, 30 wounded in action and 30 injured. A small number of RAAF pilots also served in United States Air Force units, flying F-4 Phantom fighter-bombers or serving as forward air controllers.
Military airlifts were conducted for a number of purposes in the intervening decades, such as the peacekeeping operations in East Timor from 1999. Australia's combat aircraft were not used again in combat until the Iraq War in 2003, when 14 F/A-18s from No. 75 Squadron RAAF operated in the escort and ground attack roles, flying a total of 350 sorties and dropping 122 laser-guided bombs. A detachment of AP-3C Orion maritime patrol aircraft were deployed in the Middle East between 2003 and 2012. These aircraft conducted maritime surveillance patrols over the Persian Gulf and North Arabian Sea in support of Coalition warships and boarding parties, as well as conducting extensive overland flights of Iraq and Afghanistan on intelligence, surveillance and reconnaissance missions, and supporting counter-piracy operations in Somalia.
From 2007 to 2009, a detachment of No. 114 Mobile Control and Reporting Unit RAAF was on active service at Kandahar Airfield in southern Afghanistan.
Approximately 75 personnel deployed with the AN/TPS-77 radar assigned the responsibility to co-ordinate coalition air operations. A detachment of IAI Heron unmanned aerial vehicles has been deployed in Afghanistan since January 2010.

In late September 2014, an Air Task Group consisting of up to eight F/A-18F Super Hornets, a KC-30A Multi Role Tanker Transport, a E-7A Wedgetail Airborne Early Warning & Control aircraft and 400 personnel was deployed to Al Minhad Air Base in the United Arab Emirates as part of the coalition to combat Islamic State forces in Iraq. Operations began on 1 October. A number of C-17 and C-130J Super Hercules transport aircraft based in the Middle East have also been used to conduct airdrops of humanitarian aid and to airlift arms and munitions since August.

In June 2017 two RAAF AP-3C Orion maritime patrol aircraft were deployed to the southern Philippines in response to the Marawi crisis.

The rank structure of the nascent RAAF was established within the context of the desire to ensure that the service remained separate from both the Army and Navy. While the service's predecessor formations, the AFC and the AAC, had used the Army's rank structure, in November 1920, just prior to the RAAF's foundation, it was decided by the Air Board that the RAAF would adopt the rank structure that had been implemented in the RAF the previous year. As a result, the RAAF's rank structure came to be: Aircraftsman, Leading Aircraftsman, Corporal, Sergeant, Flight Sergeant, Warrant Officer, Officer Cadet, Pilot Officer, Flying Officer. Flight Lieutenant, Squadron Leader, Wing Commander, Group Captain, Air Commodore, Air Vice Marshal, Air Marshal, Air Chief Marshal, Marshal of the RAAF.

In 1922, the colour of the RAAF winter uniform was determined by Williams on a visit to the Geelong Wool Mill. He asked for one dye dip fewer than the RAN blue (three indigo dips rather than four). There was a change to a lighter blue when an all-seasons uniform was introduced in the 1970s. The original colour and style were re-adopted around 2005. Slip-on rank epaulettes, known as "Soft Rank Insignia" (SRI), displaying the word are worn on the shoulders of the service dress uniform. When not in the service dress or "ceremonial" uniform, RAAF personnel wear the Auscam DPCU as a working dress. Commencing in mid-2014 DPCU began to be replaced, only in the non-deployed environment, with the General Purpose Uniform (GPU) which is a blue version of the Australian Multicam Pattern.

Originally, the air force used the existing red, white and blue roundel of the Royal Air Force. However, during the Second World War the inner red circle, which was visually similar to the Japanese "hinomaru", was removed after a No. 11 Squadron Catalina was mistaken for a Japanese aircraft and attacked by a Grumman Wildcat of VMF-212 of the United States Marine Corps on June 27, 1942.

After the war, a range of options for the RAAF roundel were proposed, including the Southern Cross, a boomerang, a sprig of wattle, and the red kangaroo. On 2 July 1956, the current version of the roundel was formally adopted. This consists of a white inner circle with a red kangaroo surrounded by a royal blue circle. The kangaroo faces left, except when used on aircraft or vehicles, when the kangaroo should always face in the direction of travel. Low visibility versions of the roundel exist, with the white omitted and the red and blue replaced with light or dark grey.

The RAAF badge was accepted by the Chester Herald in 1939. The badge is composed of the imperial crown mounted on a circle featuring the words Royal Australian Air Force, beneath which scroll work displays the Latin motto "Per Ardua Ad Astra", which it shares with the Royal Air Force. Surmounting the badge is a wedge-tailed eagle. "Per Ardua Ad Astra" is attributed with the meaning "Through Adversity to the Stars" and is from Sir Henry Rider Haggard's novel "The People of the Mist".

As of 2014, the RAAF had 13,991 permanent full-time personnel and 4,316 part-time active reserve personnel.



The Roulettes are the RAAF's formation aerobatic display team. They perform around Australia and South-east Asia, and are part of the RAAF Central Flying School (CFS) at RAAF Base East Sale, Victoria. The Roulettes use the Pilatus PC-9 and formations for shows are done in a group of six aircraft. The pilots learn many formations including loops, rolls, corkscrews, and ripple roles. Most of the performances are done at the low altitude of 500 feet (150 metres).

This list includes aircraft on order or a requirement which has been identified:


Lists:

Memorials and Museums:




</doc>
<doc id="26329" url="https://en.wikipedia.org/wiki?curid=26329" title="Responsible government">
Responsible government

Responsible government is a conception of a system of government that embodies the principle of parliamentary accountability, the foundation of the Westminster system of parliamentary democracy. Governments (the equivalent of the executive branch) in Westminster democracies are responsible to parliament rather than to the monarch, or, in a colonial context, to the imperial government, and in a republican context, to the president, either in full or in part. If the parliament is bicameral, then the government is responsible first to the parliament's lower house, which is more representative than the upper house, as it has more members and they are always directly elected.

Responsible government of parliamentary accountability manifests itself in several ways. Ministers account to Parliament for their decisions and for the performance of their departments. This requirement to make announcements and to answer questions in Parliament means that ministers must have the privileges of the "floor", which are only granted to those who are members of either house of Parliament. Secondly, and most importantly, although ministers are officially appointed by the authority of the head of state and can theoretically be dismissed at the pleasure of the sovereign, they concurrently retain their office subject to their holding the confidence of the lower house of Parliament. When the lower house has passed a motion of no confidence in the government, the government must immediately resign or submit itself to the electorate in a new general election.

Lastly, the head of state is in turn required to effectuate their executive power only through these responsible ministers. They must never attempt to set up a "shadow" government of executives or advisors and attempt to use them as instruments of government, or to rely upon their "unofficial" advice. They are bound to take no decision or action that is put into effect under the color of their executive power without that action being as a result of the counsel and advisement of their responsible ministers. Their ministers are required to counsel them (i.e., explain to them and be sure they understand any issue that they will be called upon to decide) and to form and have recommendations for them (i.e., their advice or advisement) to choose from, which are the ministers' formal, reasoned, recommendations as to what course of action should be taken.

An exception to this is Israel, which operates under a simplified version of the Westminster system, essentially cutting out the middleman: many non-reserve powers which would have been exercised by the President of Israel on advice in an unmodified system are exercised directly by the officers which would have given such advice, the Prime Minister of Israel is dominant over the Cabinet and not a Primus inter pares office, and Presidential reserve powers do not exist.

In the Canadian system, responsible government was developed between 1846 and 1850, with the executive Council formulating policy with the assistance of the legislative branch the legislature voted approval or disapproval, and the appointed governor enacted those policies that it had approved. It was a transition from the older system whereby the governor took advice from an executive Council, and use the legislature chiefly to raise money. After the formation of elected legislative assemblies starting with Nova Scotia in 1758, governors and their executive councils did not require the consent of elected legislators in order to carry out all their roles. It was only in the decades leading up to Canadian Confederation in 1867 that the governing councils of those British North American colonies became responsible to the elected representatives of the people.

Responsible government was a major element of the gradual development of Canada towards independence. The concept of responsible government is associated in Canada more with self-government than with parliamentary accountability; hence there is the notion that the Dominion of Newfoundland "gave up responsible government" when it suspended its self-governing status in 1933, as a result of financial problems. It did not regain responsible government until it became a province of Canada in 1948.

In the aftermath of the American Revolution, the British government became more sensitive to unrest in its remaining colonies with large populations of European-descended colonists. Elected assemblies were introduced to both Upper Canada and Lower Canada with the Constitutional Act of 1791. Many reformers thought that these assemblies should have some control over the executive power, leading to political unrest between the governors and assemblies in both Upper and Lower Canada. The Lieutenant Governor of Upper Canada Sir Francis Bond Head wrote in one dispatch to London that if responsible government were implemented "Democracy, in the worst possible Form, will prevail in our Colonies." 

After the 1837 Lower Canada Rebellion led by Louis-Joseph Papineau, and the 1837–1838 Upper Canada Rebellion led by William Lyon Mackenzie, Lord Durham was appointed governor general of British North America and had the task of examining the issues and determining how to defuse tensions. In his report, one of his recommendations was that colonies which were developed enough should be granted "responsible government". This term specifically meant the policy that British-appointed governors should bow to the will of elected colonial assemblies.

The first instance of responsible government in the British Empire outside of the United Kingdom itself was achieved by the colony of Nova Scotia in January–February 1848 through the efforts of Joseph Howe. The plaque in the Nova Scotia House of Assembly erected by the Historic Sites and Monuments Board of Canada reads:
First Responsible Government in the British Empire.<br>
The first Executive Council chosen exclusively from the party having a majority in the representative branch of a colonial legislature was formed in Nova Scotia on 2 February 1848. Following a vote of want of confidence in the preceding Council, James Boyle Uniacke, who had moved the resolution, became Attorney General and leader of the Government. Joseph Howe, the long-time campaigner for this "Peaceable Revolution", became Provincial Secretary. Other members of the Council were Hugh Bell, Wm. F. Desbarres, Lawrence O.C. Doyle, Herbert Huntingdon, James McNab, Michael Tobin, and George R. Young.

The colony of New Brunswick soon followed in May 1848 when Lieutenant Governor Edmund Walker Head brought in a more balanced representation of Members of the Legislative Assembly to the Executive Council and ceded more powers to that body.

In the Province of Canada, responsible government was introduced with the ministry of Louis-Hippolyte LaFontaine and Robert Baldwin in spring 1848; it was put to the test in 1849, when Reformers in the legislature passed the Rebellion Losses Bill. This was a law that provided compensation to French-Canadians who suffered losses during the Rebellions of 1837–1838 in Lower-Canada. 
The Governor General, Lord Elgin, had serious misgivings about the bill but nonetheless assented to it despite demands from the Tories that he refuse to do so. Elgin was physically assaulted by an English-speaking mob for this, and the Montreal Parliament building was burned to the ground in the ensuing riots. Nonetheless, the Rebellion Losses Bill helped entrench responsible government into Canadian politics.

In time, the granting of responsible government became the first step on the road to complete independence. Canada gradually gained greater and greater autonomy over a considerable period of time through inter imperial and commonwealth diplomacy, including the British North America Act of 1867, the Statute of Westminster of 1931, and even as late as the patriation of the Constitution Act in 1982 (see Constitution of Canada).

While the various colonies in Australia were either sparsely populated or penal settlements or both, executive power was in the hands of the Governors, who, because of the great distance from their superiors in London and the resulting very slow communication, necessarily exercised vast powers.
However, the early colonists, coming mostly from the United Kingdom, were familiar with the Westminster system and made efforts to reform it to increase the opportunity for ordinary men to participate.

The Governors and London therefore set in motion a gradual process of establishing a Westminster system in the colonies, not so fast as to get ahead of population or economic growth, nor so slow as to provoke clamouring for revolutionary change as happened in America. Initially, this took the form of appointed or partially elected Legislative Councils. Then, during the 1850s, all Australian colonies except Western Australia, along with New Zealand, established both representative and responsible government; Western Australia did the same in 1890.

The Cape Colony, in Southern Africa, was under responsible self-government from 1872 until 1910 when it became the Cape Province of the new Union of South Africa.

Under its previous system of representative government, the Ministers of the Cape Government reported directly to the British Imperial Governor, and not to the locally elected representatives in the Cape Parliament. Among Cape citizens of all races, growing anger at their powerlessness in influencing unpopular imperial decisions had repeatedly led to protests and rowdy political meetings – especially during the early "Convict Crisis" of the 1840s.
A popular political movement for responsible government soon emerged, under local leader John Molteno. A protracted struggle was then conducted over the ensuing years as the movement (known informally as "the responsibles") grew increasingly powerful, and used their parliamentary majority to put pressure on the British Governor, withholding public finances from him, and conducting public agitations. Not everyone favoured responsible government though, and pro-imperial press outlets even accused the movement of constituting "crafts and assaults of the devil".

Supporters believed that the most effective means of instituting responsible government was simply to change the section of the constitution which prevented government officials from being elected to parliament or members of parliament from serving in executive positions. The conflict therefore centred on the changing of this specific section. "Although responsible government merely required an amendment to s.79 of the constitution, it transpired only after nearly twenty years in 1872 when the so-called "responsibles" under Molteno were able to command sufficient support in both houses to secure the passage of the necessary bill." Finally, with a parliamentary majority and with the Colonial Office and new Governor Henry Barkly won over, Molteno instituted responsible government, making the Ministers directly responsible to the Cape Parliament, and becoming the Cape's first Prime Minister.

The ensuing period saw an economic recovery, a massive growth in exports and an expansion of the colony's frontiers. Despite political complications that arose from time to time (such as an ill-fated scheme by the British Colonial Office to enforce a confederation in Southern Africa in 1878, and tensions with the Afrikaner-dominated Government of Transvaal over trade and railroad construction), economic and social progress in the Cape Colony continued at a steady pace until a renewed attempt to extend British control over the hinterland caused the outbreak of the Anglo-Boer Wars in 1899.

An important feature of the Cape Colony under responsible government was that it was the only state in southern Africa (and one of very few in the world at the time) to have a non-racial system of voting.

Later however – following the South Africa Act 1909 to form the Union of South Africa – this multi-racial universal suffrage was steadily eroded, and eventually abolished by the Apartheid government in 1948.


In the early 1860s, the Prussian Prime Minister Otto von Bismarck was involved in a bitter dispute with the Liberals, who sought to institute a system of responsible government modeled on that of Britain. Bismarck, who strongly opposed that demand, managed to deflect the pressure by embarking energetically and successfully on the unification of Germany. The Liberals, who were also strong German nationalists, backed Bismarck's unification efforts and tacitly accepted that the Constitution of Imperial Germany, crafted by Bismarck, did not include a responsible government – the Chancellor being accountable solely to the emperor and needing no parliamentary confidence. Germany gained a responsible government only with the Weimar Republic and more securely with the creation of the German Federal Republic. Historians account the lack of responsible government in the formative decades of united Germany as one of the factors contributing to the prolonged weakness of German democratic institutions, lasting also after such a government was finally instituted.





</doc>
<doc id="26332" url="https://en.wikipedia.org/wiki?curid=26332" title="Rural flight">
Rural flight

Rural flight (or rural exodus) is the migratory pattern of peoples from rural areas into urban areas. It is urbanization seen from the rural perspective.

In modern times, it often occurs in a region following the industrialization of agriculture—when fewer people are needed to bring the same amount of agricultural output to market—and related agricultural services and industries are consolidated. Rural flight is exacerbated when the population decline leads to the loss of rural services (such as business enterprises and schools), which leads to greater loss of population as people leave to seek those features.

This phenomenon was first articulated through Christian McLean's laws of migration in the 1880s, upon which modern theories are based.

Prior to the Industrial Revolution, rural flight occurred in mostly localized regions. Pre-industrial societies did not experience large rural-urban migration flows primarily due to the inability of cities to support large populations. Lack of large employment industries, high urban mortality, and low food supplies all served as checks keeping pre-industrial cities much smaller than their modern counterparts. Ancient Athens and Rome, scholars estimate, had peak populations of 80,000 and 500,000 paling in comparison with their current populations.

The onset of the Industrial Revolution in Europe in the late 19th century removed many of the checks that had previously constrained urban populations. As food supplies increased and stabilized and industrialized centers moved into place, cities began to support larger populations, sparking the beginning of rural flight on a massive scale. The United Kingdom went from having 20% of the population living in urban areas in 1800 to more than 70% by 1925. While the late 19th century and early 20th century saw much of rural flight focused in Western Europe and the United States, as industrialization spread throughout the world during the 20th century, rural flight and urbanization followed quickly behind. Today, rural flight is an especially distinctive phenomenon in some of the newer urbanized areas including China and more recently sub-Saharan Africa.

The shift from mixed subsistence farming to commodity crops and livestock began in the late 19th century. New capital market systems and the railroad network began the trend towards larger farms that employed fewer people per acre. These larger farms used more efficient technologies such as steel plows, mechanical reapers, and higher-yield seed stock, which reduced human input per unit of production. The other issue on the Great Plains was that people were using inappropriate farming techniques for the soil and weather conditions. Most homesteaders had family farms generally considered too small to survive (under 320 acres), and European-American subsistence farming could not continue as it was then practiced.

During the Dust Bowl and the Great Depression of the 1930s, large numbers of people fled rural areas of the Great Plains and the Midwest due to depressed commodity prices and high debt loads exacerbated by several years of drought and large dust storms. Rural flight from the Great Plains has been depicted in literature, such as John Steinbeck's novel "The Grapes of Wrath" (1939), in which a family from the Great Plains migrates to California during the Dust Bowl period of the 1930s.

Post-World War II rural flight has been caused primarily by the spread of industrialized agriculture. Small, labor-intensive family farms have grown into, or have been replaced by, heavily mechanized and specialized industrial farms. While a small family farm typically produced a wide range of crop, garden, and animal products—all requiring substantial labor—large industrial farms typically specialize in just a few crop or livestock varieties, using large machinery and high-density livestock containment systems that require a fraction of the labor per unit produced. For example, Iowa State University reports the number of hog farmers in Iowa dropped from 65,000 in 1980 to 10,000 in 2002, while the number of hogs per farm increased from 200 to 1,400.

The consolidation of the feed, seed, processed grain, and livestock industries has meant that there are fewer small businesses in rural areas. This decrease in turn exacerbated the decreased demand for labor. Rural areas that used to be able to provide employment for all young adults willing to work in challenging conditions, increasingly provide fewer opportunities for young adults. The situation is made worse by the decrease in services such as schools, business, and cultural opportunities that accompany the decline in population, and the increasing age of the remaining population further stresses the social service system of rural areas.

The rise of corporate agricultural structures directly affects small rural communities, resulting in decreased populations, decreased incomes for some segments, increased income inequality, decreased community participation, fewer retail outlets and less retail trade, and increased environmental pollution.

There are several determinants, push and pull, that contribute to rural flight: lower levels of (perceived) economic opportunity in rural communities versus urban ones, lower levels of government investment in rural communities, greater education opportunities in cities, marriages, increased social acceptance in urban areas, and higher levels of rural fertility.

Some migrants choose to leave rural communities out of the desire to pursue greater economic opportunity in urban areas. Greater economic opportunities can be real or perceived. According to the Harris-Todaro Model, migration to urban areas will continue as long as "expected urban real income at the margin exceeds real agricultural product" (127). However, sociologist Josef Gugler points out that while individual benefits of increased wages may outweigh the costs of migration, if enough individuals follow this rationale, it can produce harmful effects such as overcrowding and unemployment on a national level. This phenomenon, when the rate of urbanization outpaces the rate of economic growth, is known as overurbanization. Since the industrialization of agriculture, mechanization has reduced the number of jobs present in rural communities. Some scholars have also attributed rural flight to the effects of globalization as the demand for increased economic competitiveness leads people to choose capital over labor. At the same time, rural fertility rates have historically been higher than urban fertility rates. The combination of declining rural jobs and a persistently high rural fertility rate has led to rural-urban migration streams. Rural flight also contains a positive feedback loop where previous migrants from rural communities assist new migrants in adjusting to city life. Also known as chain migration, migrant networks lower barriers to rural flight. For example, an overwhelming majority of rural migrants in China located jobs in urban areas through migrant networks.

Some families choose to send their children to cities as a form of investment for the future. A study conducted by Bates and Bennett (1974) concluded that rural communities in Zambia that had other viable investment opportunities, like livestock for instance, had lower rates of rural-urban migration as compared to regions without viable investment opportunities. Sending their children into cities can serve as long-term investments with the hope that their children will be able to send remittances back home after getting a job in the city.

There are severe challenges faced by poorer people in the agriculture sector because of diminishing access to productive farmland. Foreign investors through Foreign Direct Investment (FDI) schemes have been encouraged to lease land in rural areas in Cambodia and Ethiopia. This has led to the loss of farmland, range land, woodlands and water sources from local communities. Large-scale agricultural projects funded by FDI only employed a few experts specialized in the relevant new technologies.

In other instances, rural flight may occur in response to social determinants. A study conducted in 2012 indicated that a significant proportion of rural flight in India occurred due to social factors such as migration with household, marriage, and education. Migration with households and marriage affect women in particular as most often they are the ones required to move with households and move for marriage, especially in developing regions. 
Rural youth may choose to leave their rural communities as a method of transitioning into adulthood, seeking avenues to greater prosperity. With the stagnation of the rural economy and encouragement from their parents, rural youth may choose to migrate to cities out of social norms – demonstrating leadership and self-respect. With this societal encouragement combined with depressed rural economies, rural youth form a large proportion of the migrants moving to urban areas. In Sub-Saharan Africa, a study conducted by Touray in 2006 indicated that about 15% (26 million) of urban migrants were youth. 
Lastly, natural disasters can often be single-point events that lead to temporarily massive rural-urban migration flows. The 1930s Dust Bowl in the United States, for example, led to the flight of 2.5 million people from the Plains by 1940, many to the new cities in the West. It is estimated that as many as one out of every four residents in the Plains States left during the 1930s. More recently, drought in Syria from 2006-2011 has prompted a rural exodus to major urban centers. Massive influxes in urban areas, combined with difficult living conditions, have prompted some scholars to link the drought to the arrival of the Arab Spring in Syria.

The terms are used in the United States and Canada to describe the flight of people from rural areas in the Great Plains and Midwest regions, and to a lesser extent rural areas of the northeast and southeast and Appalachia. It is also particularly noticeable in parts of Atlantic Canada (especially Newfoundland), since the collapse of Atlantic cod fishing fields in 1992.

China, like many other currently industrializing countries, has had a relatively late start to rural flight. Until 1983, the Chinese government, through the hukou system, greatly restricted the ability of their citizens to internally migrate. Since 1983, the Chinese government has progressively lifted the restrictions on internal migration. This has led to a great increase in the number of people migrating to urban areas. However, even today, the hukou system limits the ability of rural migrants to receive full access to urban social services at the urban subsidized costs.

As with most examples of rural flight, several factors have led towards China’s massive urbanization. Income disparity, family pressure, surplus labor in rural areas due to higher average fertility rates, and improved living conditions all play a role in contributing to the flows of migrants from rural to urban areas. Approximately, 250 million rural migrants now live in cities with 54% of the total Chinese population living in urban areas.

Rural flight has been occurring to some degree in Germany since the 11th century. A corresponding principle of German law is "Stadtluft macht frei" ("city air makes you free"), in longer form "Stadtluft macht frei nach Jahr und Tag" ("city air makes you free after a year and a day"): by custom and, from 1231/32, by statute, a serf who had spent a year and a day in a city was free, and could not be reclaimed by their former master.

"Landflucht" ("flight from the land") refers to the mass migration of peasants into the cities that occurred in Germany (and throughout most of Europe) in the late 19th century.

In 1870 the rural population of Germany constituted 64% of the population; by 1907 it had shrunk to 33%. In 1900 alone, the Prussian provinces of East Prussia, West Prussia, Posen, Silesia, and Pomerania lost about 1,600,000 people to the cities, where these former agricultural workers were absorbed into the rapidly growing factory labor class; One of the causes of this mass-migration was the decrease in rural income compared to the rates of pay in the cities.

Landflucht resulted in a major transformation of the German countryside and agriculture. Mechanized agriculture and migrant workers, particularly Poles from the east (Sachsengänger), became more common. This was especially true in the province of Posen that was gained by Prussia when Poland was partitioned. The Polish population of eastern Germany was one of the justifications for the creation of the "Polish corridor" after World War I and the absorption of the land east of the Oder-Neisse line into Poland after World War II. Also, some labor-intensive enterprises were replaced by much less labor-intensive ones such as game preserves.

The word "Landflucht" has negative connotations in German, as it was coined by agricultural employers, often of the German aristocracy, who were lamenting their labor shortages.

Rural flight and out-migration in Sweden can be traced in two distinct waves. The first, beginning in the 1850s when 82% of the Swedish population lived in rural areas, and continuing till the late 1880s, was mostly due to push factors in the countryside related to poverty, unemployment, low agricultural wages, debt peonage, semi-feudalism, and religious oppression by the State church. Most of the migration was ad-hoc and directed towards emigration to the three big cities of Sweden, America, Denmark, or Germany. Many of these first emigrants were unskilled, barely literate laborers who sought farm work or daily wage labour in the cities.

The second wave started from the late 1890s and reached its peak between 1922 and 1967, with the highest rates of rural flight occurring in the 1920s and the 1950s. This was mostly "pull factors" due to the economic boom and industrial prosperity in Sweden wherein the massive economic expansion and wage increases in the urban areas pulled young people to migrate for work and at the same time drove down work opportunities in the countryside. Between 1925 and 1965, Sweden's GDP per capita increased from USD 850 to USD 6200. Simultaneously, the percentage of the population living in rural areas decreased drastically from 54% in 1925 to 21% in 1965.

Rural flight began later for Russia and the former states of the USSR than in Western Europe. In 1926 only 18% of Russians lived in urban areas, compared to over 75% at the same time in the United Kingdom. Although the process began later, throughout World War II and the decades immediately proceeding, rural flight proceeded at a rapid pace. By 1965, 53% of Russians lived in urban areas. Statistics compiled by M. Ya Sonin, a Soviet author, in 1959, demonstrate the rapid urbanization of the USSR. Between 1939 and 1959, the rural population declined by 21.3 million, while that of urban centers increased by 39.4 million. Of this dramatic shift in population, rural flight accounts for more than 60% of the change. Generally, most rural migrants tended to settle in cities and towns within their district. Rural flight persisted through the majority of the 20th century. However, with the end of the Soviet Union, rural flight reversed as political and economic instability in the cities prompted many urban dwellers to return to rural villages. 
Rural flight did not occur uniformly throughout the USSR. Western Russia and Ukraine experienced the greatest declines in rural population, 30% and 17% respectively. Conversely, peripheral regions of the USSR, like Central Asia, experienced gains, contradicting the general pattern of rural-urban migration of this period. Increased diversification of crops and labor shortages were primary contributors to the gains in rural population in the periphery.

Rural flight in Russia and the former USSR had several major determinants. The industrialization of agriculture, which came later in Russia and the former USSR, led to declines in available rural jobs. Lower living standards and tough work also motivated some peasants to migrate to urban areas. In particular, the Soviet "kolkhoz" system (the collective farms in the Soviet Union) aided in maintaining low living standards for Soviet peasants. Beginning around 1928, the kolkhoz system replaced family farms throughout the Soviet Union. Forced to work long hours for low pay at rates fixed by the government and often unadjusted to inflation, Russian peasants experienced quite low living-conditions - especially compared to urban life. While Brezhnev's wage reforms in 1965 ameliorated the low wages received by peasants, rural life remained suffocating, especially for the skilled and the educated. 
Although migrants came from all segments of society, several groups were more likely to migrate than others. Like other examples of rural flight, the young were more likely than the old to migrate to the cities. Young women under 20 were the most likely segment of the population to leave rural life. This exodus of young women further exacerbated the demographic transitions occurring in rural communities as the rate of natural increase dropped precipitously over the course of the 20th century. Lastly, the skilled and educated were also likely to migrate to urban areas.

Rural flight in Mexico occurred throughout the 1930s up until the present day. Like other developing nations, the beginning of industrialization in Mexico quickly accelerated the rate of rural flight.

In the 1930s, President Cardenas implemented a series of agricultural reforms that led to massive redistribution of agricultural land among the rural peasants. Some commentators have subsequently dubbed the period from 1940-1965 as the "Golden Era for Mexican Migration." During this period, Mexican agriculture grew at an average rate of 5.7% outpacing the natural increase of 3% of the rural population. Concurrently, government policies favoring industrialization led to a massive increase of industrial jobs in the cities. Statistics compiled in Mexico City demonstrate this trend with over 1.8 million jobs created over the course of the 1940s, 50s, and 60s. Young people with schooling were the segment of population most likely to migrate away from rural life to urban life, attracted by the promise of many jobs and a more modern lifestyle as compared to the conservative conditions in rural villages. Additionally, due to the large demand for new workers, many of these jobs had low entrance requirements that also provided on-site job training opening the avenue for migration to many rural residents. From 1940 to about 1965, rural flight occurred in a slow, yet steady pace with both agriculture and industry growing concurrently.

However, as government policies increasingly favored industry over agriculture, rural conditions began to deteriorate. In 1957, the Mexican government began to regulate the price of maize through massive imports in order to keep low urban food costs. This regulation severely undercut the market price of maize lowering the profit margins of small farmers. At the same time, the Green Revolution had entered into Mexican agriculture. Inspired by the work of Norman Borlaug, farmers that employed hybrid seeds and fertilizer supplements were able to double or even triple their yields per acre. Unfortunately, these products came at a relatively high cost, out of the reach of many farmers struggling after the devaluation of the price of maize. The combined effects of the maize price regulation and the Green Revolution was the consolidation of small farms into larger estates. A 1974 study conducted by Osorio concluded that in 1960, about 50.3% of the individual land plots in Mexico contained less than 5 hectares of land. In contrast, the top 0.5% of estates by land spanned 28.3% of all arable land. As many small farmers lost land, they either migrated to the cities or became migrant workers roving from large estate to large estate. Between 1950 and 1970, the proportion of migrant workers increased from 36.7% to 54% of the total population. The centralized pattern of industrial development and government policies overwhelmingly favoring industrialization contributed to massive rural flight in Mexico beginning in the late 1960s until the present day.

Rural migrants to cities face several challenges that may hinder their quality of life upon moving into urbanized areas. Many migrants do not have the education or skills to acquire decent jobs in cities and are then forced into unstable, low paying jobs. The steady stream of new rural migrants worsens underemployment and unemployment, common among rural migrants. Employers offer lower wages and poorer labor conditions to rural migrants, who must compete with each other for limited jobs, often unaware of their labor rights. Rural migrants often experience poor living conditions as well. Many cities have exploded in population; services and infrastructure, in these cities, are unable to keep up with population growth. Massive influxes in rural population can lead to severe housing shortages, inadequate water and energy supply, and general slum-like conditions throughout cities.

Additionally, rural migrants often struggle adjusting to city life. In some instances, there are cultural differences between the rural and urban areas of a region. Lost in urban regions, it becomes difficult for them to continue holding onto their cultural traditions. Urban residents may also look down upon these newcomers to the city who are often unaware of city social norms. Both marginalized and separated from their home cultures, migrants face many social challenges when moving to cities.

Women, in particular, face a unique set of challenges. Some women undergo rural flight to escape domestic abuse or forced early marriages. Some parents choose to send women to cities to find jobs in order to send remittances back home. Once in the city, employers may attempt to take advantage of these women preying on their unfamiliarity with labor laws and social networks on which to rely. In the worst of cases, destitution may force women into prostitution, exposing them to social stigma and the risks of sexually transmitted diseases.




</doc>
<doc id="26333" url="https://en.wikipedia.org/wiki?curid=26333" title="Robotech">
Robotech

Robotech is a science fiction franchise that began with an 85-episode anime television series produced by Harmony Gold USA in association with Tatsunoko Production and first released in the United States in 1985. 
It was adapted from three original and unrelated, though visually similar, Japanese anime television series ("Super Dimension Fortress Macross", "Super Dimension Cavalry Southern Cross", and "Genesis Climber Mospeada") to make a series suitable for syndication.

In the series, "Robotechnology" refers to the scientific advances discovered in an alien starship that crashed on a South Pacific island. With this technology, Earth developed robotic technologies, such as transformable mecha, to fight three successive extraterrestrial invasions.

Prior to the release of the TV series, the name "Robotech" was used by model kit manufacturer Revell on their "Robotech Defenders" line in the mid-1980s. The line consisted of mecha model kits imported from Japan and featured in anime titles such as "The Super Dimension Fortress Macross" (1982), "Super Dimension Century Orguss" (1983) and "Fang of the Sun Dougram" (1981). The kits were originally intended to be a marketing tie-in to a similarly named comic book series by DC Comics, which was cancelled after only two issues.

At the same time, Harmony Gold licensed the "Macross" TV series for direct-to-video distribution in 1984, but their merchandising plans were compromised by Revell's prior distribution of the "Macross" kits. In the end, both parties signed a co-licensing agreement and the "Robotech" name was adopted for the TV syndication of "Macross" combined with "Super Dimension Cavalry Southern Cross" (1984) and "Genesis Climber MOSPEADA" (1983).

"The "Robotech" chronology, according to Harmony Gold, is illustrated below:

Note: Asterisked works are now considered 'secondary continuity' — that is, that their events exist in the continuity of "Robotech", but 'don't count' when conflicts arise with the primary continuity that comprises the three-part "Robotech" TV series and 2006's "".

In 2002, with the publication of the Wildstorm (DC) comics, Harmony Gold officially decided to retcon the "Robotech" Universe. The following "Robotech" material is now relegated to the status of secondary continuity:


While these materials are not precisely 'retired' or 'removed' from the continuity, their events are subject to critical review, and are strictly subordinate to the 'official' events of the 85-episode animated series.

"Robotech" (1985) is an original story adapted with edited content and revised dialogue from the animation of three different mecha anime series:

Harmony Gold's cited reasoning for combining these unrelated series was its decision to market "Macross" for American weekday syndication television, which required a minimum of 65 episodes at the time (thirteen weeks at five episodes per week). "Macross" and the two other series each had fewer episodes than required, since they originally aired in Japan as weekly series. On some television stations, the syndicated run was preceded by the broadcast premiere of "", a feature-length pilot.

This combination resulted in a storyline that spans three generations, as mankind must fight three destructive 'Robotech Wars' in succession with various invading forces, each of which is motivated in one way or another by a desire for a powerful energy source called 'Protoculture'. While each of the three animated series used for its footage informs its content, the Robotech storyline is distinct and separate from each of them.


"Robotech: The Movie", also called "Robotech: The Untold Story", is a feature film and was the first new "Robotech" adventure created after the premiere of the original series. It uses footage from the "Megazone 23 Part 1" OVA (Original Video Animation; made-for-video animated feature) combined with scenes from "Southern Cross" and additional original animation produced for the film.

The original plan for the film was to have it set during the Macross Saga, parallel to the SDF-1's return to Earth from Pluto. The film would also have served as a prequel to the Sentinels, as both projects were initially meant to share many characters. Harmony Gold producer Carl Macek worked with the OVA's original creators to make the story and the new ending work. The film had to be changed again after the distributor of the film, Cannon Films, saw an incomplete rough cut of the film and were upset by it. They ordered Macek to remove multiple scenes from the film and to add more violence (most of the scenes removed were scenes setting up characters and showing female characters interacting). Macek reluctantly did what they ordered, and created a new script and rough edit for the film in less than 24 hours. When the distributors saw Macek act out the new film, they were much more pleased with the new cut. The opening night in Texas received a positive response, but Cannon Films pulled out after noting that most attendants were adults; the bulk of the scheduled advertising for the series was targeted to children. The film had limited success in Argentina and Belgium.

In 2011, A&E Home Video released, as a part of their "Robotech: The Complete Series" collection, a 29-minute version of "Robotech: The Movie" containing only footage used from "Southern Cross". There was no attempt to remaster the footage.

This aborted American-produced series would have followed the continuing adventures of Rick and Lisa Hunter and the Robotech Expedition during the events of "The Masters" and "The New Generation". The feature-length pilot is composed of the first three (and only) episodes that were produced. "The Sentinels" featured characters from all three "Robotech" sagas and introduced the SDF-3 along with an overview of their new mission. The series was planned to have a total of 65 episodes.

In "Robotech Art 3: The Sentinels", Carl Macek blamed the cancellation of the series on the crash of the Yen/Dollar exchange rate, which caused toy partner Matchbox to withdraw from the project. Harmony Gold lacked the funds to produce the series on its own, and production ceased after only three episodes.

"Robotech II: The Sentinels" was released on VHS by Palladium Books. In 2011, a "remastered" version was released on the A&E DVD set, "Robotech: The Complete Original Series" DVD. This version has opening titles resembling those found on the "Robotech Remastered" DVD's, as well as a new ending with text explaining the fate of the SDF-3. Also, all of the flashback footage used from "The Macross Saga" has been removed, including the re-used footage from the episode "Wedding Bells".

In 2002, Tommy Yune announced development of a new sequel film, which was untitled until 2004 as "Robotech: Shadow Force". The storyline overlaps with and continues from the unresolved ending of the original series. The title of the story-arc was soon changed to "". The first trailers with finished animation were shown at Anime Expo and Comic-Con International in 2005. It was not until February 2006, when Kevin McKeever, operations coordinator at Harmony Gold, was able to confirm that the pilot movie had been completed. After a series of delays, FUNimation Entertainment was finally announced as the home video, broadcast, and theatrical distributor at the 2006 Comic-Con International in San Diego with the possibility of producing further sequels. Harmony Gold premiered the movie at various film festivals in 2006, and it was first seen by a public audience at MechaCon on August 9, 2006, where it was showcased as a charity screening to help raise funds for the ongoing Hurricane Katrina and Hurricane Rita recovery effort. A limited theatrical run followed in January 2007, and the film was released on DVD on February 6, 2007. A 2-disc collector's edition was released in November 2007.

First revealed in late 2011 in the final minutes of "Carl Macek's Robotech Universe", a documentary on the making of "Robotech" dedicated to the then-recent passing of Macek, "Love Live Alive" is an adaptation of the 1985 "Genesis Climber Mospeada" OVA, "Love Live Alive", incorporating some brand new animation. The film was released on DVD on July 23, 2013 by Lionsgate Home Entertainment.

This promotional VHS tape created by Matchbox was included with their "Robotech Wars" playset. This video includes two episodes cobbled together from clips of "The Macross Saga". Titled "To the End of the Universe" and "Battle Royale", these episodes contain no new footage, and are not meant to follow any continuity established in the TV series.

Carl Macek revealed ideas for another proposed series, "Robotech: The Odyssey", which would have picked up where "The New Generation" and end of "Robotech: The Sentinels" left off, and eventually created a circular storyline that would end where the original "Robotech" began in a giant 260-episode cycle to fill up all the weekdays in a year. According to Macek, "The Odyssey" would have involved the SDF-3 travelling back into the past to the days before the birth of Zor (as well as Scott Bernard's search for the SDF-3). The SDF-3's crew would become citizens of the Robotech Masters' homeworld and change time by becoming a part of its history. Ultimately, it would be revealed that Lynn Minmei was the mother of Zor, making Minmei the focal point of Robotech. The final episode of the Odyssey would be of Zor dying and his Super Dimension Fortress (the SDF-1) being launched into space, and eventually crash landing on Earth in 1999. The next episode after that would be "Boobytrap", episode 1 of the original series which in turn will create an endless loop within the "Robotech" universe. After the failure of "Sentinels", "Odyssey" never went into development, although some of its ideas were worked into the final Jack McKinney novel "The End of the Circle", which wrapped up all of the outstanding plot threads left by the original series and the previous "Robotech" novels.

Fan publication "Macross Life" interviewed Harmony Gold executive Richard Firth in 1986, where he revealed that Macek had "plans through "ROBOTECH V", which would give us an episode for each day of the year for a year and a half." He also said that these two installments would have brought the series to 285 episodes. Regarding the plot, Firth mentioned a "retired Commodore Hunter, whom ever that may be, could very well be speaking at the graduation of the later day cadets or whatever, and they ask him to tell them the story all over again: it comes back [to the first episode of the series]."

Macek himself has never mentioned "Robotech IV" or "V" in any interviews or writings. Taking the three different "generations" of the original series as separate parts, and the canceled as the "fourth" part, Firth could have been referring to the proposed "Robotech: The Odyssey" as "Robotech 5", since it would be a fifth part of the overall saga.

Macek attempted another sequel with the development of "Robotech 3000". This all-CGI series would have been set a millennium in the future of the "Robotech" universe and feature none of the old series' characters. In the three-minute trailer, an expedition is sent to check on a non-responsive mining outpost and is attacked by "infected" Veritech mecha. The idea was abandoned midway into production after negative reception within the company, negative fan reactions at the FanimeCon anime convention in 2000, and financial difficulties within Netter Digital who was animating the show. The trailer is hosted on the official "Robotech" website, and was included in the 2007 release of the "Robotech: The Shadow Chronicles" 2-Disc Collector's DVD, along with behind-the-scenes motion capture footage.

In October 2004, veteran animation writer and producer Greg Weisman revealed that he wrote developed an animated spin-off series titled Robotech: Mars Force 
. When asked about the project, Weisman said that he was under a non-disclosure agreement with Harmony Gold and was only allowed to mention that he developed the series.

In 2006, Harmony Gold Creative Director Tommy Yune elaborated on the project in the Space Station Liberty Podcast, saying that Mars Force was a series geared at younger audiences, following the children Robotech Expeditionary Force. A similar plot would later be used for canceled 2014 spin-off, Robotech Academy.

A sixty-second public service announcement for the 60th anniversary of the United Nations, featuring Scott Bernard and Ariel, was animated during the production of "The Shadow Chronicles". Although it did not use the original voice actors and the dialogue was somewhat out-of-character, it nonetheless marked the first fully completed "Robotech" footage in many years.

On July 27, 2007, at their Comic-Con International panel, Harmony Gold and Yune unveiled the second entry of the "Shadow Chronicles" production, titled "Robotech: Shadow Rising" and was to be a co-production with FUNimation Entertainment. Pre-production reportedly began on February 2007 and a projected release date of sometime in 2009 was originally expected. Production ceased after Harmony Gold terminated their deal with FUNimation Entertainment due to creative differences.

At Comic-Con 2012, Tommy Yune announced that "Love Live Alive" would pave the way for "Shadow Rising". As of 2015, the Shadow Rising trademark remains abandoned since 2007.

On July 5, 2014, Harmony Gold started a Kickstarter project for "Robotech Academy", which Macek had developed before he died. The goal of this project was to raise US$500,000 to produce a new 24-minute pilot episode. The crowdfunding project was to have closed on August 9, 2014; however, on August 2, the project was canceled with a pledge level of US$194,574, or 39% of its target. Harmony Gold however, announced that further plans to fund the project were being explored. At the 2014 Long Beach Comic Con, it was announced that the producers at Harmony Gold were in talks with at least one New media network on the prospect of producing the show. As of December 7, 2015, the project remains abandoned.

In the 1990s, Seishun Shitemasu, an anime fandubbing group, produced the parodies "Robotech III: Not Necessarily the Sentinels" and "Robotech IV: Khyron's Counterattack", using footage from, respectively, "Gunbuster" and "", continuing the tradition of the original Robotech's adaptation of unrelated anime series into a single continuity.

On July 2, 2010, Ecuadorian animator Patricio "Pat" Mosquera uploaded to YouTube a teaser for "Robotech Skull Knights". On August 17, 2010, second teaser revealed Rick Hunter standing in front of an image of the VF-4 shown in the final episodes of the original series. "Robotech Skull Knights" has not been released yet. In July 2013, Patricio Mosquera was included as an animation director in the staff list in the IMDb page of "Love Live Alive".

On December 31, 2012, Cesar Turturro uploaded to YouTube an Argentinian fan trailer for "Robotech Valkyrie Project". On December, 2013 the first episode was uploaded to YouTube, and in January 2014, the second episode was also uploaded. The series was cancelled after Harmony Gold issued a "cease and desist" letter to the producers. The team was, however, hired to do the CGI effects for "Robotech: Academy".

On September 7, 2007, "The Hollywood Reporter" stated that Warner Bros. had acquired the film rights to "Robotech" and would be producing a live-action film with an as-yet-unknown release date. Tobey Maguire is producing the film through his Maguire Entertainment banner and is pursuing the lead role, in what the studio plans to be a tentpole science fiction franchise. Maguire stated, "We are very excited to bring "Robotech" to the big screen. There is a rich mythology that will be a great foundation for a sophisticated, smart and entertaining film."

In an interview, Harmony Gold representative Kevin McKeever said that Warner Bros. had approached Harmony Gold about the project, that Harmony Gold would have "a say" in its creative direction, and that it was not expected to affect the production schedule for "". He was unable to confirm any details of budget, casting, expected release date, or storyline, explaining that it was too early in the life of the project for these things to have been decided.

In June 2008, it was reported that Lawrence Kasdan had been hired to write the film, with Charles Roven and Akiva Goldsman joining Tobey Maguire as producers. During the Robotech Panel at Anime Expo 2008, the involvement of Maguire and Kasdan was confirmed, with Kasdan writing the script for the live-action film. Tommy Yune also revealed that the film is planned as a re-imagining of the original "Robotech" universe (with new updated mecha and character designs) and will take place several years in the future, departing from the original cartoon's 2009 setting.
As of November 2008, Alfred Gough and Miles Millar (who both worked in "Smallville", "Spider-Man 2", "", and "") are the writers for the film.

Roven is currently no longer working on the proposed film adaptation of the "Robotech" animated series, but he wished the remaining producers Goldsman and Maguire "fantastic luck" on the project.

The Mania.com website reported on June 23, 2009, that British television writer and novelist Tom Rob Smith "has taken over writing duties" for the proposed film adaptation. Smith wrote for the British soaps "Family Affairs" and "Bad Girls" before writing the critically acclaimed crime suspense novel "Child 44". Smith will be the fourth writer or writing team to be reportedly attached to the upcoming film's pre-production.

In early 2013, "The Hollywood Reporter" announced that Warner Bros. was in talks with commercial director Nic Mathieu to direct the film. On July 24, 2013, it was reported that Leonardo DiCaprio had turned down a role in "" and has shown interest to star as a main character in the upcoming big screen version of "Robotech". DiCaprio is a longtime friend of Tobey Maguire; they co-starred in "The Great Gatsby". Maguire will probably participate in the film as another one of the lead actors — while Nic Mathieu will direct.
On February 4, 2015, Deadline.com reported Gianni Nunori and Mark Canton selected Michael B. Gordon to write the film's script and are looking at Andy Muschietti to direct it.

On March 25, 2015, Variety announced that the "Robotech" franchise had been acquired by Sony Pictures, who views "Robotech" as a potential film franchise. On April 29, 2015, Deadline reported that James Wan is in talks to direct the film. On June 3, 2015, The Hollywood Reporter reported that Wan is confirmed to direct the film.

On July 3, 2015, Kevin McKeever of Harmony Gold announced at Anime Expo that Sony has the rights to release this film worldwide, with the exception of Japan.

On March 27, 2016, Wan told IGN that the film will follow the roots of the franchise.

As of April 4, 2016, Harmony Gold's Kevin McKeever revealed on the official Robotech Facebook page that their deal with Sony is still not finalized.

On July 17, 2017, it was reported by The Hollywood Reporter that Argentine filmmaker Andy Muschietti will direct the project, after Wan dropped out to work on "Aquaman".

On September 12, 2017, Jason Fuchs was reportedly hired by the studio to write the script for the film.

At the time of its broadcast, Harmony Gold also launched "Robotech" through a popular line of comics to be followed by novels, role-playing games, toys, and other consumer products. With the cancellation of "Robotech II: The Sentinels", many of these licensed products were discontinued, and led to a drought of "Robotech" product through much of the 1990s, except for publishers who continued "The Sentinels" storyline in print.

In 1986, Starblaze Graphics published "Robotech Art 1", a reference book containing artwork, Japanese production designs, and episode guides from the original television series. This was followed by "Robotech Art 2", which was largely a collection of art by various American artists and fans. In 1988, Carl Macek collected much of the unused designs from "Robotech II: The Sentinels" into "Robotech Art 3: The Sentinels", which also included his story outline for the rest of the unfinished series, with an explanation behind its cancellation. In 2007, Stone Bridge Press published "The Art of Robotech: The Shadow Chronicles".

"Robotech" comics were first published in 1984 with DC Comics' short-lived "Robotech Defenders" and Comico's adaptation of the first episode of the Japanese version of "Macross". However, the first adaptation of the "Robotech" television series did not arrive until 1985 with Comico's "Robotech: The Macross Saga" Number 2, which continued from the first "Macross" issue.

The various comic publishers include:


The first "Robotech" collectible card game was released in 2006 by Hero Factory, which had previously produced "Robotech" trading cards.

Various "Robotech" soundtracks have been released on records, cassettes, and compact discs since 1988.


Since 1987, "Robotech" was adapted into novel form by "Jack McKinney", a pseudonym for the team of James Luceno and the late Brian Daley, a pair of writers who had been working with Macek since they had collaborated on the animated series "Galaxy Rangers". Using fictitious epigraphs in the style of "Dune", McKinney's novels fleshed out the chronology (including adapting the incomplete "Sentinels" source material) in far greater detail than the original animation. Many "Robotech" fans consider the McKinney series to be an unofficial canon of its own, despite notable divergences in the writing from Harmony Gold's current official animation-based canon. Despite no longer being considered core-continuity by Harmony Gold, the novels have been recently re-issued by Del Rey Books as Omnibus compilations.

In 1986, Palladium Books published a role-playing game based on the "Robotech" series, including several books covering the "Sentinels" portion of the storyline. The original "Robotech" RPG line went out of print as of June 30, 2001, but Harmony Gold and Palladium Books signed an agreement in 2007 to produce a new line of Robotech RPG books, beginning with a book covering and promoting the feature-length film "The Shadow Chronicles". The "" sourcebook first book was released on March 21, 2008, followed by sourcebooks covering the Macross, Masters, and New Generation chapters of Robotech (redrafted to reflect the Harmony Gold canon). Other sourcebooks and supplements are reflected in the Palladium Books production pipeline.

On April 18, 2013, Palladium started a campaign on the crowdfunding site Kickstarter for a tabletop miniatures game based on the Robotech RPG called "Robotech: RPG Tactics". The miniatures are being produced by Ninja Division (combining sculpting talents from Soda Pop Miniatures and Cipher Studios), and will feature multi part plastic miniatures that can be posed during assembly. The campaign reached its goal in 3 hours, and was initially scheduled to release in December 2013, but delays have persisted into 2018.

Action figures in the size of the three "Robotech" generations were initially released in 1985 by Matchbox toy company, but then reissued in 1992 by Harmony Gold (Lunk and Corg were only released by Matchbox and Lynn Minmei was only released by Harmony Gold). Each included a weapon and helmet where appropriate. Matchbox also released figures of Zentraedi characters from the first generation. These figures were supposed to represent the size difference between the Humans and the giant Zentraedi forces, but to be correct these figures would have to have been made about tall. None of the larger figures came with weapons but the Armored Zentraedi came with a removable helmet.

Also many toys depicting the vehicles and mecha from the series were released by Matchbox in 1985, Harmony Gold in 1992 and Playmates in 1994 (under the Exosquad line). There were major differences in packaging, toy stickers and colors between the different releases. The vehicles were designed to be used only with the 3 3/4 inch figures. The SDF-1 Playset was only released under the Matchbox line in the 1980s and could be used with both the 3 3/4 and 6 inch figures.

Harmony Gold and Matchbox were unable to sell the 1/55 VF-1 Valkyrie toy originally sold in Japan by Takatoku Toys due to Hasbro licensing it as Jetfire in the Transformers toy line. Because of this, they settled with manufacturing a non-transformable Veritech Fighter that could fit any of the 3 3/4 inch action figures, as well as importing the transformable super deformed Veritech Fighters (originally manufactured in Japan by Bandai as "Macross" VF-1 Valkyrie "Joke machines").

Since the late 1990s, there has been a resurgence of "Robotech"-related toys. In 2001, Toynami released the "Robotech Masterpiece Collection" line, featuring replicas of the Veritech Fighters of "The Macross Saga". Since then, Toynami has become the exclusive toy manufacturer of the "Robotech" franchise - having covered mecha from "The Macross Saga", "The New Generation" and "The Shadow Chronicles".

"Robotech" spawned five video game licenses, of which the most recent three were released:


"Robotech" is often a polarizing subject amongst anime fans. Some critics look down upon the show for its extensive edits to the source material (Westernizing character names, editing for content and chiefly, forging a connection between previously unrelated series), while supporters of the adaptation have pointed out that the weaving of three unrelated series into a contiguous whole necessarily required reworking, and that it helped to maintain a slow but continuous rise in the consumption of anime in the US.

Series writer/actor Gregory Snegoff said in an interview on the now-defunct "Shadow Chronicles News" fansite that, "afterward, we received compliments from the Japanese who thought our dialogue and stories were better than the original," likely a reference to the creators of the latter two series, both of whom worked with the team on "The Sentinels". The producers of "Megazone 23 - Part 1" were very happy with the original plans for "" (where the incomplete film would have been added to the "Robotech" mythos to play part in "The Sentinels" storyline), and worked closely with Carl Macek to plan the new ending and animation. When the film reached a limited release, the new ending was released on a laser disc of Megazone 23, with the title "Present For You." However, "Animag" magazine (issue 11) and "Animerica" magazine (issue 9, volume 4) reports that the staff of "Macross" at Studio Nue and Artland, such as the original story creator and mecha designer Shoji Kawamori and chief director Noboru Ishiguro, expressed their concern over the "Robotech" adaptation, and surprise at its differences.

In 2009, "IGN" ranked "Robotech" as the 34th greatest animated show of all time in their Top 100 list.

Following the original broadcast, the series enjoyed popularity on home video in VHS and DVD formats from the following distributors:



</doc>
<doc id="26340" url="https://en.wikipedia.org/wiki?curid=26340" title="Red China">
Red China

The following articles relate to Red China:



</doc>
<doc id="26341" url="https://en.wikipedia.org/wiki?curid=26341" title="Radioteletype">
Radioteletype

Radioteletype (RTTY) is a telecommunications system consisting originally of two or more electromechanical teleprinters in different locations connected by radio rather than a wired link. These machines were superseded by personal computers (PCs) running software to emulate teleprinters. Radioteletype evolved from earlier landline teleprinter operations that began in the mid-1800s. The US Navy Department successfully tested printing telegraphy between an airplane and ground radio station in 1922. Later that year, the Radio Corporation of America successfully tested printing telegraphy via their Chatham, Massachusetts, radio station to the R.M.S. Majestic. Commercial RTTY systems were in active service between San Francisco and Honolulu as early as April 1932 and between San Francisco and New York City by 1934. The US military used radioteletype in the 1930s and expanded this usage during World War II. From the 1980s, teleprinters were replaced by computers running teleprinter emulation software.

The term radioteletype is used to describe both the original radioteletype system, sometimes described as "Baudot", as well as the entire family of systems connecting two or more teleprinters or PCs using software to emulate teleprinters, over radio, regardless of alphabet, link system or modulation.

In some applications, notably military and government, radioteletype is known by the acronym RATT (Radio Automatic Teletype).

Landline teleprinter operations began in 1849 when a circuit was put in service between Philadelphia and New York City. Émile Baudot designed a system using a five unit code in 1874 that is still in use today. Teleprinter system design was gradually improved until, at the beginning of World War II, it represented the principal distribution method used by the news services.

Radioteletype evolved from these earlier landline teleprinter operations. The US Department of the Navy successfully tested printing telegraphy between an airplane and ground radio station in August 1922. Later that year, the Radio Corporation of America successfully tested printing telegraphy via their Chatham, MA radio station to the R.M.S. Majestic. An early implementation of the Radioteletype was the Watsongraph, named after Detroit inventor Glenn Watson in March 1931. Commercial RTTY systems were in active service between San Francisco and Honolulu as early as April 1932 and between San Francisco and New York City by 1934. The US Military used radioteletype in the 1930s and expanded this usage during World War II. The Navy called radioteletype "RATT" (Radio Automatic Teletype) and the Army Signal Corps called radioteletype "SCRT", an abbreviation of Single-Channel Radio Teletype. The military used frequency shift keying technology and this technology proved very reliable even over long distances.

From the 1980s, teleprinters were replaced by computers running teleprinter emulation software.

A radioteletype station consists of three distinct parts: the Teletype or teleprinter, the modem and the radio.

The Teletype or teleprinter is an electromechanical or electronic device. The word "Teletype" was a trademark of the Teletype Corporation, so the terms "TTY", "RTTY", "RATT" and "teleprinter" are usually used to describe a generic device without reference to a particular manufacturer.

Electromechanical teleprinters were heavy, complex and noisy, and have been replaced with electronic units. The teleprinter includes a keyboard, which is the main means of entering text, and a printer or visual display unit (VDU). An alternative input device is a perforated tape reader and, more recently, computer storage media (such as floppy disks). Alternative output devices are tape perforators and computer storage media.

The line output of a teleprinter can be at either digital logic levels (+5 V signifies a logical "1" or "mark" and 0 V signifies a logical "0" or "space") or line levels (−80 V signifies a "1" and +80 V a "0"). When no traffic is passed, the line idles at the "mark" state.

When a key of the teleprinter keyboard is pressed, a 5-bit character is generated. The teleprinter converts it to serial format and transmits a sequence of a "start bit" (a logical 0 or space), then one after the other the 5 data bits, finishing with a "stop bit" (a logical 1 or mark, lasting 1, 1.5 or 2 bits). When a sequence of start bit, 5 data bits and stop bit arrives at the input of the teleprinter, it is converted to a 5-bit word and passed to the printer or VDU. With electromechanical teleprinters, these functions required complicated electromechanical devices, but they are easily implemented with standard digital electronics using shift registers. Special integrated circuits have been developed for this function, for example the Intersil 6402 and 6403. These are stand-alone UART devices, similar to computer serial port peripherals.

The 5 data bits allow for only 32 different codes, which cannot accommodate the 26 letters, 10 figures, space, a few punctuation marks and the required control codes, such as carriage return, new line, bell, etc. To overcome this limitation, the teleprinter has two "states", the "unshifted" or "letters" state and the "shifted" or "numbers" or "figures" state. The change from one state to the other takes place when the special control codes "LETTERS" and "FIGURES" are sent from the keyboard or received from the line. In the "letters" state the teleprinter prints the letters and space while in the shifted state it prints the numerals and punctuation marks. Teleprinters for languages using other alphabets also use an additional "third shift" state, in which they print letters in the alternative alphabet.

The modem is sometimes called the terminal unit and is an electronic device which is connected between the teleprinter and the radio transceiver. The transmitting part of the modem converts the digital signal transmitted by the teleprinter or tape reader to one or the other of a pair of audio frequency tones, traditionally 2295/2125 Hz (US) or 2125/1955 Hz (Europe). One of the tones corresponds to the "mark" condition and the other to the "space" condition. These audio tones, then, modulate an SSB transmitter to produce the final audio-frequency shift keying (AFSK) radio frequency signal. Some transmitters are capable of direct frequency-shift keying (FSK) as they can directly accept the digital signal and change their transmitting frequency according to the "mark" or "space" input state. In this case the transmitting part of the modem is bypassed.

On reception, the FSK signal is converted to the original tones by mixing the FSK signal with a local oscillator called the BFO or "beat frequency oscillator". These tones are fed to the demodulator part of the modem, which processes them through a series of filters and detectors to recreate the original digital signal. The FSK signals are audible on a communications radio receiver equipped with a BFO, and have a distinctive "beedle-eeeedle-eedle-eee" sound, usually starting and ending on one of the two tones ("idle on mark").

The transmission speed is a characteristic of the teleprinter while the shift (the difference between the tones representing mark and space) is a characteristic of the modem. These two parameters are therefore independent, provided they have satisfied the minimum shift size for a given transmission speed. Electronic teleprinters can readily operate in a variety of speeds, but mechanical teleprinters require the change of gears in order to operate at different speeds.

Today, both functions can be performed with modern computers equipped with digital signal processors or sound cards. The sound card performs the functions of the modem and the CPU performs the processing of the digital bits. This approach is very common in amateur radio, using specialized computer programs like fldigi, MMTTY or MixW.

Before the computer mass storage era, most RTTY stations stored text on paper tape using paper tape punchers and readers. The operator would type the message on the TTY keyboard and punch the code onto the tape. The tape could then be transmitted at a steady, high rate, without typing errors. A tape could be reused, and in some cases - especially for use with ASCII on NC Machines - might be made of plastic or even very thin metal material in order to be reused many times.

The most common test signal is a series of "RYRYRY" characters, as these form an alternating tone pattern exercising all bits and are easily recognized. Pangrams are also transmitted on RTTY circuits as test messages, the most common one being "The quick brown fox jumps over the lazy dog", and in French circuits, "Voyez le brick géant que j'examine près du wharf"

The original (or "Baudot") radioteletype system is based almost invariably on the Baudot code or ITA-2 5 bit alphabet. The link is based on character asynchronous transmission with 1 start bit and 1, 1.5 or 2 stop bits. Transmitter modulation is normally FSK (F1B). Occasionally, an AFSK signal modulating an RF carrier (A2B, F2B) is used on VHF or UHF frequencies. Standard transmission speeds are 45.45, 50, 75, 100, 150 and 300 baud.

Common carrier shifts are 85 Hz (used on LF and VLF frequencies), 170 Hz, 425 Hz, 450 Hz and 850 Hz, although some stations use non-standard shifts. There are variations of the standard Baudot alphabet to cover languages written in Cyrillic, Arabic, Greek etc., using special techniques.

Some combinations of speed and shift are standardized for specific services using the original radioteletype system:


After World War II, amateur radio operators in the US started to receive obsolete but usable Teletype Model 26 equipment from commercial operators with the understanding that this equipment would not be used for or returned to commercial service. "The Amateur Radioteletype and VHF Society" was founded in 1946 in Woodside, NY. This organization soon changed its name to "The VHF Teletype Society" and started US Amateur Radio operations on 2 meters using audio frequency shift keying (AFSK). The first two-way amateur radioteletype QSO of record took place in May 1946 between Dave Winters, W2AUF, Brooklyn, NY and W2BFD, John Evans Williams, Woodside Long Island, NY. On the west coast, amateur RTTY also started on 2 meters. Operation on 80 meters, 40 meters and the other High Frequency (HF) amateur radio bands was initially accomplished using make and break keying since frequency shift keying (FSK) was not yet authorized. In early 1949, the first American transcontinental two-way RTTY QSO was accomplished on 11 meters using AFSK between Tom McMullen (W1QVF) operating at W1AW and Johnny Agalsoff, W6PSW. The stations effected partial contact on January 30, 1949, and repeated more successfully on January 31. On February 1, 1949, the stations exchanged solid print congratulatory message traffic and rag-chewed. Earlier, on January 23, 1949, William T. Knott, W2QGH, Larchmont, NY, had been able to make rough copy of W6PSW's test transmissions. While QSOs could be accomplished, it was quickly realized that FSK was technically superior to make and break keying. Due to the efforts of Merrill Swan, W6AEE, of "The RTTY Society of Southern California" publisher of "RTTY" and Wayne Green, W2NSD, of "CQ Magazine", Amateur Radio operators successfully petitioned the U.S. Federal Communications Commission (FCC) to amend Part 12 of the Regulations, which was effective on February 20, 1953. The amended Regulations permitted FSK in the non-voice parts of the 80, 40 and 20 meter bands and also specified the use of single channel 60 words-per-minute five unit code corresponding to ITA2. A shift of 850 hertz plus or minus 50 hertz was specified. Amateur Radio operators also had to identify their station callsign at the beginning and the end of each transmission and at ten-minute intervals using International Morse code. Use of this wide shift proved to be a problem for Amateur Radio operations. Commercial operators had already discovered that narrow shift worked best on the HF bands. After investigation and a petition to the FCC, Part 12 was amended, in March 1956, to allow Amateur Radio Operators to use any shift that was less than 900 hertz.

The FCC Notice of Proposed Rule Making (NPRM) that resulted in the authorization of Frequency Shift Keying (FSK) in the amateur high frequency (HF) bands responded to petitions by the American Radio Relay League (ARRL), the National Amateur Radio Council and Mr. Robert Weinstein. The NPRM specifically states this, and this information may be found in its entirety in the December 1951 Issue of QST. While the New RTTY Handbook gives ARRL no credit, it was published by CQ Magazine and its author was a CQ columnist (CQ generally opposed ARRL at that time).

The first RTTY Contest was held by the RTTY Society of Southern California from October 31 to November 1, 1953. Named the RTTY Sweepstakes Contest, twenty nine participants exchanged messages that contained a serial number, originating station call, check or RST report of two or three numbers, ARRL section of originator, local time (0000-2400 preferred) and date. Example: NR 23 W0BP CK MINN 1325 FEB 15. By the late 1950s, the contest exchange was expanded to include band used. Example: NR 23 W0BP CK MINN 1325 FEB 15 FORTY METERS. The contest was scored as follows: one point for each message sent and received entirely by RTTY and one point for each message received and acknowledged by RTTY. The final score was computed by multiplying the total number of message points by the number of ARRL sections worked. Two stations could exchange messages again on a different band for added points, but the section multiplier did not increase when the same section was reworked on a different band. Each DXCC entity was counted as an additional ARRL section for RTTY multiplier credit.

"RTTY", later named "RTTY Journal", also published the first listing of stations, mostly located in the continental US, that were interested in RTTY in 1956. Amateur Radio operators used this callbook information to contact other operators both inside and outside the United States. For example, the first recorded USA to New Zealand two-way RTTY QSO took place in 1956 between W0BP and ZL1WB.

By the late 1950s, new organizations focused on amateur radioteletype started to appear. The "British Amateur Radio Teletype Group", BARTG, now known as the "British Amateur Radio Teledata Group" was formed in June 1959. The Florida RTTY Society was formed in September 1959. Amateur Radio operators outside of Canada and the United States began to acquire surplus teleprinter and receive permission to get on the air. The first recorded RTTY QSO in the UK occurred in September 1959 between G2UK and G3CQE. A few weeks later, G3CQE had the first G/VE RTTY QSO with VE7KX. This was quickly followed up by G3CQE QSOs with VK3KF and ZL3HJ. Information on how to acquire surplus teleprinter equipment continued to spread and before long it was possible to work all continents on RTTY.

Amateur Radio operators used various equipment designs to get on the air using RTTY in the 1950s and 1960s. Amateurs used their existing receivers for RTTY operation but needed to add a terminal unit, sometimes called a demodulator, to convert the received audio signals to DC signals for the teleprinter.

Most of the terminal unit equipment used for receiving RTTY signals was homebuilt, using designs published in amateur radio publications. These original designs can be divided into two classes of terminal units: audio-type and intermediate frequency converters. The audio-type converters proved to be more popular with amateur radio operators. The Twin City, W2JAV and W2PAT designs were examples of typical terminal units that were used into the middle 1960s. The late 1960s and early 1970s saw the emergence of terminal units designed by W6FFC, such as the TT/L-2, ST-3, ST-5, and ST-6. These designs were first published in "RTTY Journal" starting in September 1967 and ending in 1970.

Amateur Radio operators needed to modify their transmitters to allow for HF RTTY operation. This was accomplished by adding a frequency shift keyer that used a diode to switch a capacitor in and out of the circuit, shifting the transmitter’s frequency in synchronism with the teleprinter signal changing from mark to space to mark. A very stable transmitter was required for RTTY. The typical frequency multiplication type transmitter that was popular in the 1950s and 1960s would be relatively stable on 80 meters but become progressively less stable on 40 meters, 20 meters and 15 meters. By the middle 1960s, transmitter designs were updated, mixing a crystal-controlled high frequency oscillator with a variable low frequency oscillator, resulting in better frequency stability across all Amateur Radio HF bands.

During the early days of Amateur RTTY, the Worked All Continents – RTTY Award was conceived by the RTTY Society of Southern California and issued by RTTY Journal. The first Amateur Radio station to achieve this WAC – RTTY Award was VE7KX. The first stations recognized as having achieved single band WAC RTTY were W1MX (3.5 MHz); DL0TD (7.0 MHz); K3SWZ (14.0 MHz); W0MT (21.0 MHz) and FG7XT (28.0 MHz). The ARRL began issuing WAC RTTY certificates in 1969.

By the early 1970s, Amateur Radio RTTY had spread around the world and it was finally possible to work more than 100 countries via RTTY. FG7XT was the first Amateur Radio station to claim to achieve this honor. However, Jean did not submit his QSL cards for independent review. ON4BX, in 1971, was the first Amateur Radio station to submit his cards to the DX Editor of RTTY Journal and to achieve this honor. The ARRL began issuing DXCC RTTY Awards on November 1, 1976. Prior to that date, an award for working 100 countries on RTTY was only available via RTTY Journal.

In the 1950s through the 1970s, "RTTY art" was a popular on-air activity. This consisted of (sometimes very elaborate and artistic) pictures sent over rtty through the use of lengthy punched tape transmissions and then printed by the receiving station on paper.

On January 7, 1972, the FCC amended Part 97 to allow faster RTTY speeds. Four standard RTTY speeds were authorized, namely, 60 (45 baud), 67 (50 baud), 75 (56.25 baud) and 100 (75 baud) words per minute. Many Amateur Radio operators had equipment that was capable of being upgraded to 75 and 100 words per minute by changing teleprinter gears. While there was an initial interest in 100 words per minute operation, many Amateur Radio operators moved back to 60 words per minute. Some of the reasons for the failure of 100 words per minute HF RTTY included poor operation of improperly maintained mechanical teleprinters, narrow bandwidth terminal units, continued use of 170 Hz shift at 100 words per minute and excessive error rates due to multipath distortion and the nature of ionospheric propagation.

The FCC approved the use of ASCII by Amateur Radio stations on March 17, 1980 with speeds up to 300 baud from 3.5 to 21.25 MHz and 1200 baud between 28 and 225 MHz. Speeds up to 19.2 kilobaud was authorized on Amateur frequencies above 420 MHz.

The requirement for Amateur Radio operators in the United States to identify their station callsign at the beginning and the end of each digital transmission and at ten-minute intervals using International Morse code was finally lifted by the FCC on June 15, 1983.

RTTY has a typical baud rate for Amateur operation of 45.45 baud (approximately 60 words per minute). It remains popular as a "keyboard to keyboard" mode in Amateur Radio. RTTY has declined in commercial popularity as faster, more reliable alternative data modes have become available, using satellite or other connections.

For its transmission speed, RTTY has low spectral efficiency. The typical RTTY signal with 170 Hz shift at 45.45 baud requires around 250 Hz receiver bandwidth, more than double that required by PSK31. In theory, at this baud rate, the shift size can be decreased to 22.725 Hz, reducing the overall band footprint substantially. Because RTTY, using either AFSK or FSK modulation, produces a waveform with constant power, a transmitter does not need to use a linear amplifier, which is required for many digital transmission modes. A more efficient Class C amplifier may be used.

RTTY, using either AFSK or FSK modulation, is moderately resistant to vagaries of HF propagation and interference, however modern digital modes, such as MFSK, use Forward Error Correction to provide much better data reliability.

Principally, the primary users are those who need robust shortwave communications. Examples are:

One regular service is WLO, transmitting weather information from the United States in English, using ITA-2, with an intended audience of ocean-going vessels and those concerned with them:

Another regular service transmitting RTTY meteorological information is the German Meteorological Service (Deutscher Wetterdienst or DWD). The DWD regularly transmit two programs on various frequencies on LF and HF in standard RTTY (ITA-2 alphabet). The list of callsigns, frequencies, baudrates and shifts (current June 2012) are as follows:

The DWD signals can be easily received in Europe, North Africa and parts of North America.

RTTY (in English) may be spoken as "radioteletype", by its letters: R-T-T-Y, or simply as "ritty".





</doc>
<doc id="26344" url="https://en.wikipedia.org/wiki?curid=26344" title="Register transfer language">
Register transfer language

In computer science, register transfer language (RTL) is a kind of intermediate representation (IR) that is very close to assembly language, such as that which is used in a compiler. It is used to describe data flow at the register-transfer level of an architecture. Academic papers and textbooks often use a form of RTL as an architecture-neutral assembly language. RTL is used as the name of a specific intermediate representation in several compilers, including the GNU Compiler Collection (GCC), Zephyr, and the European compiler projects CerCo and CompCert.

In GCC, RTL is generated from the GIMPLE representation, transformed by various passes in the GCC 'middle-end', and then converted to assembly language.

GCC's RTL is usually written in a form which looks like a Lisp S-expression:

This "side-effect expression" says "sum the contents of register 138 with the contents of register 139 and store the result in register 140". The SI specifies the access mode for each registers. In the example it is "SImode", i.e. "access the register as 32-bit integer".

The sequence of RTL generated has some dependency on the characteristics of the processor for which GCC is generating code. However, the meaning of the RTL is more-or-less independent of the target: it would usually be possible to read and understand a piece of RTL without knowing what processor it was generated for. Similarly, the meaning of the RTL doesn't usually depend on the original high-level language of the program.

A register transfer language is a system for expressing in symbolic form the microoperation sequences among the registers of a digital module. It is a convenient tool for describing the internal organization of digital computers in concise and precise manner. It can also be used to facilitate the design process of digital systems.

The idea behind RTL was first described in:
Davidson and Fraser; The Design and Application of a Retargetable Peephole Optimizer; ToPLaS v2(2) 191-202 (April 1980)




</doc>
<doc id="26346" url="https://en.wikipedia.org/wiki?curid=26346" title="Remote procedure call">
Remote procedure call

In distributed computing, a remote procedure call (RPC) is when a computer program causes a procedure (subroutine) to execute in a different address space (commonly on another computer on a shared network), which is coded as if it were a normal (local) procedure call, without the programmer explicitly coding the details for the remote interaction. That is, the programmer writes essentially the same code whether the subroutine is local to the executing program, or remote. This is a form of client–server interaction (caller is client, executor is server), typically implemented via a request–response message-passing system. In the object-oriented programming paradigm, RPC calls are represented by remote method invocation (RMI). The RPC model implies a level of location transparency, namely that calling procedures is largely the same whether it is local or remote, but usually they are not identical, so local calls can be distinguished from remote calls. Remote calls are usually orders of magnitude slower and less reliable than local calls, so distinguishing them is important.

RPCs are a form of inter-process communication (IPC), in that different processes have different address spaces: if on the same host machine, they have distinct virtual address spaces, even though the physical address space is the same; while if they are on different hosts, the physical address space is different. Many different (often incompatible) technologies have been used to implement the concept.

Response–request protocols date to early distributed computing in the late 1960s, theoretical proposals of remote procedure calls as the model of network operations date to the 1970s, and practical implementations date to the early 1980s. In the 1990s, with the popularity of object-oriented programming, the alternative model of remote method invocation (RMI) was widely implemented, such as in Common Object Request Broker Architecture (CORBA, 1991) and Java remote method invocation. RMIs in turn fell in popularity with the rise of the internet, particularly in the 2000s.

Remote procedure calls used in modern operating systems trace their roots back to the RC 4000 multiprogramming system, which used a request-response communication protocol for process synchronization. The idea of treating network operations as remote procedure calls goes back at least to the 1970s in early ARPANET documents. In 1978, Per Brinch Hansen proposed Distributed Processes, a language for distributed computing based on "external requests" consisting of procedure calls between processes.

Bruce Jay Nelson is generally credited with coining the term "remote procedure call" (1981), and the first practical implementation was by Andrew Birrel and Bruce Nelson, called Lupine, in the Cedar environment at Xerox PARC. Lupine automatically generated stubs, providing type-safe bindings, and used an efficient protocol for communication. One of the first business uses of RPC was by Xerox under the name "Courier" in 1981. The first popular implementation of RPC on Unix was Sun's RPC (now called ONC RPC), used as the basis for Network File System (NFS).

RPC is a request–response protocol. An RPC is initiated by the "client", which sends a request message to a known remote "server" to execute a specified procedure with supplied parameters. The remote server sends a response to the client, and the application continues its process. While the server is processing the call, the client is blocked (it waits until the server has finished processing before resuming execution), unless the client sends an asynchronous request to the server, such as an XMLHttpRequest. There are many variations and subtleties in various implementations, resulting in a variety of different (incompatible) RPC protocols.

An important difference between remote procedure calls and local calls is that remote calls can fail because of unpredictable network problems. Also, callers generally must deal with such failures without knowing whether the remote procedure was actually invoked. Idempotent procedures (those that have no additional effects if called more than once) are easily handled, but enough difficulties remain that code to call remote procedures is often confined to carefully written low-level subsystems.


To let different clients access servers, a number of standardized RPC systems have been created. Most of these use an interface description language (IDL) to let various platforms call the RPC. The IDL files can then be used to generate code to interface between the client and servers.

Notable RPC implementations and analogues include:







</doc>
<doc id="26347" url="https://en.wikipedia.org/wiki?curid=26347" title="Russian submarine Kursk (K-141)">
Russian submarine Kursk (K-141)

K-141 "Kursk" (, transl. , meaning "Nuclear-powered submarine "Kursk"") was an Oscar II-class nuclear-powered cruise-missile submarine of the Russian Navy.

On 12 August 2000, K-141 "Kursk" was lost when it sank in the Barents Sea, killing all 118 personnel on board.

K-141 "Kursk" was a Project 949A class "Antey" () submarine of the Oscar-class, known as the Oscar II by its NATO reporting name, and was the penultimate submarine of the Oscar II class designed and approved in the Soviet Union. Construction began in 1990 at the Soviet Navy military shipyards in Severodvinsk, near Arkhangelsk, in the northern Russian SFSR. During the construction of K-141, the Soviet Union collapsed; work continued, and she became one of the first naval vessels completed after the collapse. K-141 was inherited by Russia and launched in 1994, before being commissioned by the Russian Navy on December 30, as part of the Russian Northern Fleet. K-141 was named "Kursk" after the city, following the naming system for Soviet submarines.

"Kursk" was assigned to the home port of Vidyayevo, Murmansk Oblast.

The Antey design represented the highest achievement of Soviet nuclear submarine technology. They were the second-largest attack submarines ever built, after the Typhoon Class submarine (Akula /Project 941). It was built to defeat an entire United States aircraft carrier group. A single Type 65 torpedo carried a warhead powerful enough to sink an aircraft carrier. Both missiles and torpedoes could be equipped with nuclear warheads. She was longer than the preceding Oscar I-class of submarines. The senior officers had individual staterooms and the entire crew had access to a gymnasium.

The outer hull, made of high-nickel, high-chromium stainless steel thick, had exceptionally good resistance to corrosion and a weak magnetic signature which helped prevent detection by U.S. magnetic anomaly detector (MAD) systems. There was a gap to the -thick steel pressure hull. She was designed to remain submerged for up to 120 days. The sail superstructure was reinforced to allow it to break through the Arctic ice.
The submarine was armed with 24 SS-N-19/P-700 Granit cruise missiles, and eight torpedo tubes in the bow: four and four . The Granit missiles with a range of , were capable of supersonic flight at altitudes over . They were designed to swarm enemy vessels and intelligently choose individual targets which terminated with a dive onto the target. The torpedo tubes could be used to launch either torpedoes or anti-ship missiles with a range of . Her weapons included 18 SS-N-16 "Stallion" anti-submarine missiles.

"Kursk" was part of Russia's Northern Fleet, which had suffered funding cutbacks throughout the 1990s. Many of its submarines were anchored and rusting in Zapadnaya Litsa Naval Base, from Murmansk. Little work to maintain all but the most essential front-line equipment, including search and rescue equipment, had occurred. Northern Fleet sailors had gone unpaid in the mid-1990s.

During her five years of service, "Kursk" completed only one mission, a six-month deployment to the Mediterranean during the summer of 1999 to monitor the United States Sixth Fleet responding to the Kosovo crisis. This was due to a lack of funds for fuel. As a result, many of her crew had spent little time at sea and were inexperienced.

"Kursk" joined the "Summer-X" exercise, the first large-scale naval exercise planned by the Russian Navy in more than a decade, on 10 August 2000. It included 30 ships including the fleet's flagship "Pyotr Velikiy" ("Peter the Great"), four attack submarines, and a flotilla of smaller ships. The crew had recently won a citation for its excellent performance and been recognized as the best submarine crew in the Northern Fleet. While it was on an exercise, "Kursk" loaded a full complement of combat weapons. It was one of the few ships authorized to carry a combat load at all times.

On the first day of the exercise, "Kursk" successfully launched a Granit missile armed with a dummy warhead. Two days later, on the morning of 12 August, "Kursk" prepared to fire dummy torpedoes at the "Kirov"-class battlecruiser Pyotr Velikiy ("Peter The Great"). These practice torpedoes had no explosive warheads and were manufactured and tested at a much lower quality standard. On 12 August 2000, at 11:28 local time (07:28 UTC), there was an explosion while preparing to fire. The Russian Navy's final report on the disaster concluded the explosion was due to the failure of one of "Kursk"'s hydrogen peroxide-fueled Type 65 torpedoes. A subsequent investigation concluded that HTP, a form of highly concentrated hydrogen peroxide used as propellant for the torpedo, seeped through a faulty weld in the torpedo casing. When HTP comes in contact with a catalyst, it rapidly expands by a factor of 5000, generating vast quantities of steam and oxygen. The pressure produced by the expanding HTP ruptured the kerosene fuel tank in the torpedo and set off an explosion equal to of TNT. The submarine sank in relatively shallow water, bottoming at about off Severomorsk, at . A second explosion 135 seconds after the initial event was equivalent to 3-7 tons of TNT. The explosions blew a large hole in the hull and collapsed the first three compartments of the sub, killing or incapacitating all but 23 of the 118 personnel on board.

Though the British and Norwegian navies offered assistance, Russia initially refused all help. All 118 sailors and officers aboard "Kursk" died. The Russian Admiralty initially told the public that the majority of the crew died within minutes of the explosion, but on 21 August, Norwegian and Russian divers found 24 bodies in the ninth compartment, the turbine room at the stern of the boat. Captain-lieutenant Dmitri Kolesnikov wrote a note listing the names of 23 sailors who were alive in the compartment after the ship sank.

"Kursk" carried a potassium superoxide cartridge of a chemical oxygen generator, used to absorb carbon dioxide and chemically release oxygen during an emergency. However, the cartridge became contaminated with sea water and the resulting chemical reaction caused a flash fire which consumed the available oxygen. The investigation showed that some men temporarily survived the fire by plunging under water, as fire marks on the bulkheads indicated the water was at waist level at the time. Ultimately, the remaining crew burned to death or suffocated.

Russian President Vladimir Putin, though immediately informed of the tragedy, was told by the navy that they had the situation under control and rescue was imminent. He waited five days before he ended his holiday at a presidential resort in Sochi on the Black Sea. Only four months into his tenure as President, the public and media were extremely critical of Putin's decision to remain at a seaside resort, and his highly favourable ratings dropped dramatically. The President's response appeared callous and the government's actions looked incompetent. A year later he said, "I probably should have returned to Moscow, but nothing would have changed. I had the same level of communication both in Sochi and in Moscow, but from a PR point of view I could have demonstrated some special eagerness to return."

A consortium formed by the Dutch companies Mammoet and Smit International was awarded a contract by Russia to raise the vessel, excluding the bow. They modified the barge "Giant 4" which raised "Kursk" and recovered the remains of the sailors.

During salvage in 2001, the team first cut the bow off the hull using a tungsten carbide-studded cable. As this tool had the potential to cause sparks which could ignite remaining pockets of reactive gases, such as hydrogen, the operation was carefully executed. Most of the bow was abandoned and the rest of the vessel was towed to Severomorsk and placed in a floating dry dock for analysis.

The remains of "Kursk"s reactor compartment was towed to Sayda Bay on Russia's northern Kola Peninsula – where more than 50 reactor compartments were afloat at pier points – after a shipyard had defuelled the boat in early 2003.

Some torpedo and torpedo tube fragments from the bow were recovered and the rest was destroyed by explosives in 2002.

Notwithstanding the navy's oft-stated position that a collision with a foreign vessel had triggered the event, a report issued by the government attributed the disaster to a torpedo explosion caused when high-test peroxide (HTP), a form of highly concentrated hydrogen peroxide, leaked from a faulty weld in the torpedo's casing. The report found that the initial explosion destroyed the torpedo room compartment and killed everyone in the first compartment. The blast entered the second and perhaps the third and fourth compartments through an air conditioning vent. All of the 36 men in the command post located in the second compartment were immediately incapacitated by the blast wave and possibly killed. The first explosion caused a fire that raised the temperature of the compartment to more than . The heat caused the warheads of between five and seven additional torpedoes to detonate, creating an explosion equivalent to 2–3 tons of TNT that measured 4.2 on the Richter magnitude scale on seismographs across Europe and was detected as far away as Alaska.

Vice-Admiral Valery Ryazantsev differed with the government's official conclusion. He cited inadequate training, poor maintenance, and incomplete inspections that caused the crew to mishandle the weapon. During the examination of the wrecked sub, investigators recovered a partially burned copy of the safety instructions for loading HTP torpedoes, but the instructions were for a significantly different type of torpedo and failed to include essential steps for testing an air valve. The 7th Division, 1st Submarine Flotilla never inspected "Kursk"s crew's qualifications and readiness to fire HTP torpedoes. "Kursk"s crew had no prior experience with HTP-powered torpedoes and had not been trained in handling or firing HTP-powered torpedoes. Due to their inexperience and lack of training, compounded by incomplete inspections and oversight, and because the "Kursk"s crew followed faulty instructions when loading the practice torpedo, Ryazantsev believes they set off a chain of events that led to the explosion.







</doc>
<doc id="26350" url="https://en.wikipedia.org/wiki?curid=26350" title="Radiation therapy">
Radiation therapy

Radiation therapy or radiotherapy, often abbreviated RT, RTx, or XRT, is therapy using ionizing radiation, generally as part of cancer treatment to control or kill malignant cells and normally delivered by a linear accelerator. Radiation therapy may be curative in a number of types of cancer if they are localized to one area of the body. It may also be used as part of adjuvant therapy, to prevent tumor recurrence after surgery to remove a primary malignant tumor (for example, early stages of breast cancer). Radiation therapy is synergistic with chemotherapy, and has been used before, during, and after chemotherapy in susceptible cancers. The subspecialty of oncology concerned with radiotherapy is called radiation oncology.

Radiation therapy is commonly applied to the cancerous tumor because of its ability to control cell growth. Ionizing radiation works by damaging the DNA of cancerous tissue leading to cellular death. To spare normal tissues (such as skin or organs which radiation must pass through to treat the tumor), shaped radiation beams are aimed from several angles of exposure to intersect at the tumor, providing a much larger absorbed dose there than in the surrounding, healthy tissue. Besides the tumour itself, the radiation fields may also include the draining lymph nodes if they are clinically or radiologically involved with tumor, or if there is thought to be a risk of subclinical malignant spread. It is necessary to include a margin of normal tissue around the tumor to allow for uncertainties in daily set-up and internal tumor motion. These uncertainties can be caused by internal movement (for example, respiration and bladder filling) and movement of external skin marks relative to the tumor position.

Radiation oncology is the medical specialty concerned with prescribing radiation, and is distinct from radiology, the use of radiation in medical imaging and diagnosis. Radiation may be prescribed by a radiation oncologist with intent to cure ("curative") or for adjuvant therapy. It may also be used as palliative treatment (where cure is not possible and the aim is for local disease control or symptomatic relief) or as therapeutic treatment (where the therapy has survival benefit and it can be curative). It is also common to combine radiation therapy with surgery, chemotherapy, hormone therapy, immunotherapy or some mixture of the four. Most common cancer types can be treated with radiation therapy in some way.

The precise treatment intent (curative, adjuvant, neoadjuvant therapeutic, or palliative) will depend on the tumor type, location, and stage, as well as the general health of the patient. Total body irradiation (TBI) is a radiation therapy technique used to prepare the body to receive a bone marrow transplant. Brachytherapy, in which a radioactive source is placed inside or next to the area requiring treatment, is another form of radiation therapy that minimizes exposure to healthy tissue during procedures to treat cancers of the breast, prostate and other organs. Radiation therapy has several applications in non-malignant conditions, such as the treatment of trigeminal neuralgia, acoustic neuromas, severe thyroid eye disease, pterygium, pigmented villonodular synovitis, and prevention of keloid scar growth, vascular restenosis, and heterotopic ossification. The use of radiation therapy in non-malignant conditions is limited partly by worries about the risk of radiation-induced cancers.

Different cancers respond to radiation therapy in different ways.

The response of a cancer to radiation is described by its radiosensitivity.
Highly radiosensitive cancer cells are rapidly killed by modest doses of radiation. These include leukemias, most lymphomas and germ cell tumors.
The majority of epithelial cancers are only moderately radiosensitive, and require a significantly higher dose of radiation (60-70 Gy) to achieve a radical cure.
Some types of cancer are notably radioresistant, that is, much higher doses are required to produce a radical cure than may be safe in clinical practice. Renal cell cancer and melanoma are generally considered to be radioresistant but radiation therapy is still a palliative option for many patients with metastatic melanoma. Combining radiation therapy with immunotherapy is an active area of investigation and has shown some promise for melanoma and other cancers.

It is important to distinguish the radiosensitivity of a particular tumor, which to some extent is a laboratory measure, from the radiation "curability" of a cancer in actual clinical practice. For example, leukemias are not generally curable with radiation therapy, because they are disseminated through the body. Lymphoma may be radically curable if it is localised to one area of the body. Similarly, many of the common, moderately radioresponsive tumors are routinely treated with curative doses of radiation therapy if they are at an early stage. For example: non-melanoma skin cancer, head and neck cancer, breast cancer, non-small cell lung cancer, cervical cancer, anal cancer, and prostate cancer. Metastatic cancers are generally incurable with radiation therapy because it is not possible to treat the whole body.

Before treatment, a CT scan is often performed to identify the tumor and surrounding normal structures. The patient receives small skin marks to guide the placement of treatment fields. Patient positioning is crucial at this stage as the patient will have to be set-up in the identical position during treatment. Many patient positioning devices have been developed for this purpose, including masks and cushions which can be molded to the patient.

The response of a tumor to radiation therapy is also related to its size. Due to complex radiobiology, very large tumors respond less well to radiation than smaller tumors or microscopic disease. Various strategies are used to overcome this effect. The most common technique is surgical resection prior to radiation therapy. This is most commonly seen in the treatment of breast cancer with wide local excision or mastectomy followed by adjuvant radiation therapy. Another method is to shrink the tumor with neoadjuvant chemotherapy prior to radical radiation therapy. A third technique is to enhance the radiosensitivity of the cancer by giving certain drugs during a course of radiation therapy. Examples of radiosensitizing drugs include: Cisplatin, Nimorazole, and Cetuximab.

The effect of radiotherapy on control of cancer has been shown to be limited to the first five years after surgery, particularly for breast cancer. The difference between breast cancer recurrence in patients who receive radiotherapy vs. those who don't is seen mostly in the first 2–3 years and no difference is seen after 5 years. This is explained in detail here.

Radiation therapy is in itself painless. Many low-dose palliative treatments (for example, radiation therapy to bony metastases) cause minimal or no side effects, although short-term pain flare-up can be experienced in the days following treatment due to oedema compressing nerves in the treated area. Higher doses can cause varying side effects during treatment (acute side effects), in the months or years following treatment (long-term side effects), or after re-treatment (cumulative side effects). The nature, severity, and longevity of side effects depends on the organs that receive the radiation, the treatment itself (type of radiation, dose, fractionation, concurrent chemotherapy), and the patient.

Most side effects are predictable and expected. Side effects from radiation are usually limited to the area of the patient's body that is under treatment. Side effects are dose- dependent; for example higher doses of head and neck radiation can be associated with cardiovascular complications, thyroid dysfunction, and pituitary axis dysfunction. Modern radiation therapy aims to reduce side effects to a minimum and to help the patient understand and deal with side effects that are unavoidable.

The main side effects reported are fatigue and skin irritation, like a mild to moderate sun burn. The fatigue often sets in during the middle of a course of treatment and can last for weeks after treatment ends. The irritated skin will heal, but may not be as elastic as it was before.


Late side effects occur months to years after treatment and are generally limited to the area that has been treated. They are often due to damage of blood vessels and connective tissue cells. Many late effects are reduced by fractionating treatment into smaller parts.


Cumulative effects from this process should not be confused with long-term effects—when short-term effects have disappeared and long-term effects are subclinical, reirradiation can still be problematic. These doses are calculated by the radiation oncologist and many factors are taken into account before the subsequent radiation takes place.

During the first two weeks after fertilization, radiation therapy is lethal but not teratogenic. High doses of radiation during pregnancy induce anomalies, impaired growth and intellectual disability, and there may be an increased risk of childhood leukemia and other tumours in the offspring.

In males previously having undergone radiotherapy, there appears to be no increase in genetic defects or congenital malformations in their children conceived after therapy. However, the use of assisted reproductive technologies and micromanipulation techniques might increase this risk.

Hypopituitarism commonly develops after radiation therapy for sellar and parasellar neoplasms, extrasellar brain tumours, head and neck tumours, and following whole body irradiation for systemic malignancies. Radiation-induced hypopituitarism mainly affects growth hormone and gonadal hormones. In contrast, adrenocorticotrophic hormone (ACTH) and thyroid stimulating hormone (TSH) deficiencies are the least common among people with radiation-induced hypopituitarism. Changes in prolactin-secretion is usually mild, and vasopressin deficiency appears to be very rare as a consequence of radiation.

There are rigorous procedures in place to minimise the risk of accidental overexposure of radiation therapy to patients. However, mistakes do occasionally occur; for example, the radiation therapy machine Therac-25 was responsible for at least six accidents between 1985 and 1987, where patients were given up to one hundred times the intended dose; two people were killed directly by the radiation overdoses. From 2005 to 2010, a hospital in Missouri overexposed 76 patients (most with brain cancer) during a five-year period because new radiation equipment had been set up incorrectly. Although medical errors are exceptionally rare, radiation oncologists, medical physicists and other members of the radiation therapy treatment team are working to eliminate them. ASTRO has launched a safety initiative called Target Safely that, among other things, aims to record errors nationwide so that doctors can learn from each and every mistake and prevent them from happening. ASTRO also publishes a list of questions for patients to ask their doctors about radiation safety to ensure every treatment is as safe as possible.

Radiation therapy is used to treat early stage Dupuytren's disease and Ledderhose disease. When Dupuytren's disease is at the nodules and cords stage or fingers are at a minimal deformation stage of less than 10 degrees, then radiation therapy is used to prevent further progress of the disease. Radiation therapy is also used post surgery in some cases to prevent the disease continuing to progress. Low doses of radiation are used typically three gray of radiation for five days, with a break of three months followed by another phase of three gray of radiation for five days.

Radiation therapy works by damaging the DNA of cancerous cells. This DNA damage is caused by one of two types of energy, photon or charged particle. This damage is either direct or indirect ionization of the atoms which make up the DNA chain. Indirect ionization happens as a result of the ionization of water, forming free radicals, notably hydroxyl radicals, which then damage the DNA.

In photon therapy, most of the radiation effect is through free radicals. Cells have mechanisms for repairing single-strand DNA damage and double-stranded DNA damage. However, double-stranded DNA breaks are much more difficult to repair, and can lead to dramatic chromosomal abnormalities and genetic deletions. Targeting double-stranded breaks increases the probability that cells will undergo cell death. Cancer cells are generally less differentiated and more stem cell-like; they reproduce more than most healthy differentiated cells, and have a diminished ability to repair sub-lethal damage. Single-strand DNA damage is then passed on through cell division; damage to the cancer cells' DNA accumulates, causing them to die or reproduce more slowly.

One of the major limitations of photon radiation therapy is that the cells of solid tumors become deficient in oxygen. Solid tumors can outgrow their blood supply, causing a low-oxygen state known as hypoxia. Oxygen is a potent radiosensitizer, increasing the effectiveness of a given dose of radiation by forming DNA-damaging free radicals. Tumor cells in a hypoxic environment may be as much as 2 to 3 times more resistant to radiation damage than those in a normal oxygen environment.
Much research has been devoted to overcoming hypoxia including the use of high pressure oxygen tanks, hyperthermia therapy (heat therapy which dilates blood vessels to the tumor site), blood substitutes that carry increased oxygen, hypoxic cell radiosensitizer drugs such as misonidazole and metronidazole, and hypoxic cytotoxins (tissue poisons), such as tirapazamine. Newer research approaches are currently being studied, including preclinical and clinical investigations into the use of an oxygen diffusion-enhancing compound such as trans sodium crocetinate (TSC) as a radiosensitizer.

Charged particles such as protons and boron, carbon, and neon ions can cause direct damage to cancer cell DNA through high-LET (linear energy transfer) and have an antitumor effect independent of tumor oxygen supply because these particles act mostly via direct energy transfer usually causing double-stranded DNA breaks. Due to their relatively large mass, protons and other charged particles have little lateral side scatter in the tissue—the beam does not broaden much, stays focused on the tumor shape, and delivers small dose side-effects to surrounding tissue. They also more precisely target the tumor using the Bragg peak effect. See proton therapy for a good example of the different effects of intensity-modulated radiation therapy (IMRT) vs. charged particle therapy. This procedure reduces damage to healthy tissue between the charged particle radiation source and the tumor and sets a finite range for tissue damage after the tumor has been reached. In contrast, IMRT's use of uncharged particles causes its energy to damage healthy cells when it exits the body. This exiting damage is not therapeutic, can increase treatment side effects, and increases the probability of secondary cancer induction. This difference is very important in cases where the close proximity of other organs makes any stray ionization very damaging (example: head and neck cancers).
This x-ray exposure is especially bad for children, due to their growing bodies, and they have a 30% chance of a second malignancy after 5 years post initial RT.

The amount of radiation used in photon radiation therapy is measured in gray (Gy), and varies depending on the type and stage of cancer being treated. For curative cases, the typical dose for a solid epithelial tumor ranges from 60 to 80 Gy, while lymphomas are treated with 20 to 40 Gy.

Preventive (adjuvant) doses are typically around 45–60 Gy in 1.8–2 Gy fractions (for breast, head, and neck cancers.) Many other factors are considered by radiation oncologists when selecting a dose, including whether the patient is receiving chemotherapy, patient comorbidities, whether radiation therapy is being administered before or after surgery, and the degree of success of surgery.

Delivery parameters of a prescribed dose are determined during treatment planning (part of dosimetry). Treatment planning is generally performed on dedicated computers using specialized treatment planning software. Depending on the radiation delivery method, several angles or sources may be used to sum to the total necessary dose. The planner will try to design a plan that delivers a uniform prescription dose to the tumor and minimizes dose to surrounding healthy tissues.

In radiation therapy, three-dimensional dose distributions are often evaluated using the dosimetry technique known as gel dosimetry.


The total dose is fractionated (spread out over time) for several important reasons. Fractionation allows normal cells time to recover, while tumor cells are generally less efficient in repair between fractions. Fractionation also allows tumor cells that were in a relatively radio-resistant phase of the cell cycle during one treatment to cycle into a sensitive phase of the cycle before the next fraction is given. Similarly, tumor cells that were chronically or acutely hypoxic (and therefore more radioresistant) may reoxygenate between fractions, improving the tumor cell kill.

Fractionation regimens are individualised between different radiation therapy centers and even between individual doctors. In North America, Australia, and Europe, the typical fractionation schedule for adults is 1.8 to 2 Gy per day, five days a week. In some cancer types, prolongation of the fraction schedule over too long can allow for the tumor to begin repopulating, and for these tumor types, including head-and-neck and cervical squamous cell cancers, radiation treatment is preferably completed within a certain amount of time. For children, a typical fraction size may be 1.5 to 1.8 Gy per day, as smaller fraction sizes are associated with reduced incidence and severity of late-onset side effects in normal tissues.

In some cases, two fractions per day are used near the end of a course of treatment. This schedule, known as a concomitant boost regimen or hyperfractionation, is used on tumors that regenerate more quickly when they are smaller. In particular, tumors in the head-and-neck demonstrate this behavior.

Patients receiving palliative radiation to treat uncomplicated painful bone metastasis should not receive more than a single fraction of radiation. A single treatment gives comparable pain relief and morbidity outcomes to multiple-fraction treatments, and for patients with limited life expectancy, a single treatment is best to improve patient comfort.

One fractionation schedule that is increasingly being used and continues to be studied is hypofractionation. This is a radiation treatment in which the total dose of radiation is divided into large doses. Typical doses vary significantly by cancer type, from 2.2 Gy/fraction to 20 Gy/fraction. The logic behind hypofractionation is to lessen the possibility of the cancer returning by not giving the cells enough time to reproduce and also to exploit the unique biological radiation sensitivity of some tumors.

Historically, the three main divisions of radiation therapy are :
The differences relate to the position of the radiation source; external is outside the body, brachytherapy uses sealed radioactive sources placed precisely in the area under treatment, and systemic radioisotopes are given by infusion or oral ingestion. Brachytherapy can use temporary or permanent placement of radioactive sources. The temporary sources are usually placed by a technique called afterloading. In afterloading a hollow tube or applicator is placed surgically in the organ to be treated, and the sources are loaded into the applicator after the applicator is implanted. This minimizes radiation exposure to health care personnel.

Particle therapy is a special case of external beam radiation therapy where the particles are protons or heavier ions.

The following three sections refer to treatment using x-rays.

Conventional external beam radiation therapy (2DXRT) is delivered via two-dimensional beams using kilovoltage therapy x-ray units or medical linear accelerators which generate high energy x-rays. 2DXRT mainly consists of a single beam of radiation delivered to the patient from several directions: often front or back, and both sides. "Conventional" refers to the way the treatment is "planned" or "simulated" on a specially calibrated diagnostic x-ray machine known as a simulator because it recreates the linear accelerator actions (or sometimes by eye), and to the usually well-established arrangements of the radiation beams to achieve a desired "plan". The aim of simulation is to accurately target or localize the volume which is to be treated. This technique is well established and is generally quick and reliable. The worry is that some high-dose treatments may be limited by the radiation toxicity capacity of healthy tissues which lie close to the target tumor volume. An example of this problem is seen in radiation of the prostate gland, where the sensitivity of the adjacent rectum limited the dose which could be safely prescribed using 2DXRT planning to such an extent that tumor control may not be easily achievable. Prior to the invention of the CT, physicians and physicists had limited knowledge about the true radiation dosage delivered to both cancerous and healthy tissue. For this reason, 3-dimensional conformal radiation therapy is becoming the standard treatment for a number of tumor sites. More recently other forms of imaging are used including MRI, PET, SPECT and Ultrasound.

Stereotactic radiation is a specialized type of external beam radiation therapy. It uses focused radiation beams targeting a well-defined tumor using extremely detailed imaging scans. Radiation oncologists perform stereotactic treatments, often with the help of a neurosurgeon for tumors in the brain or spine.

There are two types of stereotactic radiation. Stereotactic radiosurgery (SRS) is when doctors use a single or several stereotactic radiation treatments of the brain or spine. Stereotactic body radiation therapy (SBRT) refers to one or several stereotactic radiation treatments with the body, such as the lungs.

Some doctors say an advantage to stereotactic treatments is that they deliver the right amount of radiation to the cancer in a shorter amount of time than traditional treatments, which can often take 6 to 11 weeks. Plus treatments are given with extreme accuracy, which should limit the effect of the radiation on healthy tissues. One problem with stereotactic treatments is that they are only suitable for certain small tumors.

Stereotactic treatments can be confusing because many hospitals call the treatments by the name of the manufacturer rather than calling it SRS or SBRT. Brand names for these treatments include Axesse, Cyberknife, Gamma Knife, Novalis, Primatom, Synergy, X-Knife, TomoTherapy, Trilogy and Truebeam. This list changes as equipment manufacturers continue to develop new, specialized technologies to treat cancers.

The planning of radiation therapy treatment has been revolutionized by the ability to delineate tumors and adjacent normal structures in three dimensions using specialized CT and/or MRI scanners and planning software.

Virtual simulation, the most basic form of planning, allows more accurate placement of radiation beams than is possible using conventional X-rays, where soft-tissue structures are often difficult to assess and normal tissues difficult to protect.

An enhancement of virtual simulation is 3-dimensional conformal radiation therapy (3DCRT), in which the profile of each radiation beam is shaped to fit the profile of the target from a beam's eye view (BEV) using a multileaf collimator (MLC) and a variable number of beams. When the treatment volume conforms to the shape of the tumor, the relative toxicity of radiation to the surrounding normal tissues is reduced, allowing a higher dose of radiation to be delivered to the tumor than conventional techniques would allow.

Intensity-modulated radiation therapy (IMRT) is an advanced type of high-precision radiation that is the next generation of 3DCRT. IMRT also improves the ability to conform the treatment volume to concave tumor shapes, for example when the tumor is wrapped around a vulnerable structure such as the spinal cord or a major organ or blood vessel. Computer-controlled x-ray accelerators distribute precise radiation doses to malignant tumors or specific areas within the tumor. The pattern of radiation delivery is determined using highly tailored computing applications to perform optimization and treatment simulation (Treatment Planning). The radiation dose is consistent with the 3-D shape of the tumor by controlling, or modulating, the radiation beam’s intensity. The radiation dose intensity is elevated near the gross tumor volume while radiation among the neighboring normal tissues is decreased or avoided completely. This results in better tumor targeting, lessened side effects, and improved treatment outcomes than even 3DCRT.

3DCRT is still used extensively for many body sites but the use of IMRT is growing in more complicated body sites such as CNS, head and neck, prostate, breast, and lung. Unfortunately, IMRT is limited by its need for additional time from experienced medical personnel. This is because physicians must manually delineate the tumors one CT image at a time through the entire disease site which can take much longer than 3DCRT preparation. Then, medical physicists and dosimetrists must be engaged to create a viable treatment plan. Also, the IMRT technology has only been used commercially since the late 1990s even at the most advanced cancer centers, so radiation oncologists who did not learn it as part of their residency programs must find additional sources of education before implementing IMRT.

Proof of improved survival benefit from either of these two techniques over conventional radiation therapy (2DXRT) is growing for many tumor sites, but the ability to reduce toxicity is generally accepted. This is particularly the case for head and neck cancers in a series of pivotal trials performed by Professor Christopher Nutting of the Royal Marsden Hospital. Both techniques enable dose escalation, potentially increasing usefulness. There has been some concern, particularly with IMRT, about increased exposure of normal tissue to radiation and the consequent potential for secondary malignancy. Overconfidence in the accuracy of imaging may increase the chance of missing lesions that are invisible on the planning scans (and therefore not included in the treatment plan) or that move between or during a treatment (for example, due to respiration or inadequate patient immobilization). New techniques are being developed to better control this uncertainty—for example, real-time imaging combined with real-time adjustment of the therapeutic beams. This new technology is called image-guided radiation therapy (IGRT) or four-dimensional radiation therapy.

Another technique is the real-time tracking and localization of one or more small implantable electric devices implanted inside or close to the tumor. There are various types of medical implantable devices that are used for this purpose. It can be a magnetic transponder which senses the magnetic field generated by several transmitting coils, and then transmits the measurements back to the positioning system to determine the location. The implantable device can also be a small wireless transmitter sending out an RF signal which then will be received by a sensor array and used for localization and real-time tracking of the tumor position.

Volumetric modulated arc therapy (VMAT) is a radiation technique introduced in 2007 which can achieve highly conformal dose distributions on target volume coverage and sparing of normal tissues. The specificity of this technique is to modify three parameters during the treatment. VMAT delivers radiation by rotating gantry (usually 360° rotating fields with one or more arcs), changing speed and shape of the beam with a multileaf collimator (MLC) ("sliding window" system of moving) and fluence output rate (dose rate) of the medical linear accelerator. VMAT has an advantage in patient treatment, compared with conventional static field intensity modulated radiotherapy (IMRT), of reduced radiation delivery times. Comparisons between VMAT and conventional IMRT for their sparing of healthy tissues and Organs at Risk (OAR) depends upon the cancer type. In the treatment of nasopharyngeal, oropharyngeal and hypopharyngeal carcinomas VMAT provides equivalent or better OAR protection. In the treatment of prostate cancer the OAR protection result is mixed with some studies favoring VMAT, others favoring IMRT.

In particle therapy (proton therapy being one example), energetic ionizing particles (protons or carbon ions) are directed at the target tumor. The dose increases while the particle penetrates the tissue, up to a maximum (the Bragg peak) that occurs near the end of the particle's range, and it then drops to (almost) zero. The advantage of this energy deposition profile is that less energy is deposited into the healthy tissue surrounding the target tissue.

Auger therapy (AT) makes use of a very high dose of ionizing radiation in situ that provides molecular modifications at an atomic scale. AT differs from conventional radiation therapy in several aspects; it neither relies upon radioactive nuclei to cause cellular radiation damage at a cellular dimension, nor engages multiple external pencil-beams from different directions to zero-in to deliver a dose to the targeted area with reduced dose outside the targeted tissue/organ locations. Instead, the in situ delivery of a very high dose at the molecular level using AT aims for in situ molecular modifications involving molecular breakages and molecular re-arrangements such as a change of stacking structures as well as cellular metabolic functions related to the said molecule structures.

Contact x-ray brachytherapy (also called "CXB", "electronic brachytherapy" or the "Papillon Technique") is a type of radiation therapy using X-rays applied close to the tumour to treat rectal cancer. The process involves inserting the x-ray tube through the anus into the rectum and placing it against the cancerous tissue, then high doses of X-rays are emitted directly into the tumor at two weekly intervals. It is typically used for treating early rectal cancer in patients who may not be candidates for surgery. A 2015 NICE review found the main side effect to be bleeding that occurred in about 38% of cases, and radiation-induced ulcer which occurred in 27% of cases.

Brachytherapy is delivered by placing radiation source(s) inside or next to the area requiring treatment. Brachytherapy is commonly used as an effective treatment for cervical, prostate, breast, and skin cancer and can also be used to treat tumours in many other body sites.

In brachytherapy, radiation sources are precisely placed directly at the site of the cancerous tumour. This means that the irradiation only affects a very localized area – exposure to radiation of healthy tissues further away from the sources is reduced. These characteristics of brachytherapy provide advantages over external beam radiation therapy – the tumour can be treated with very high doses of localized radiation, whilst reducing the probability of unnecessary damage to surrounding healthy tissues. A course of brachytherapy can often be completed in less time than other radiation therapy techniques. This can help reduce the chance of surviving cancer cells dividing and growing in the intervals between each radiation therapy dose.

As one example of the localized nature of breast brachytherapy, the SAVI device delivers the radiation dose through multiple catheters, each of which can be individually controlled. This approach decreases the exposure of healthy tissue and resulting side effects, compared both to external beam radiation therapy and older methods of breast brachytherapy.

Systemic radioisotope therapy (RIT) is a form of targeted therapy. Targeting can be due to the chemical properties of the isotope such as radioiodine which is specifically absorbed by the thyroid gland a thousandfold better than other bodily organs. Targeting can also be achieved by attaching the radioisotope to another molecule or antibody to guide it to the target tissue. The radioisotopes are delivered through infusion (into the bloodstream) or ingestion. Examples are the infusion of metaiodobenzylguanidine (MIBG) to treat neuroblastoma, of oral iodine-131 to treat thyroid cancer or thyrotoxicosis, and of hormone-bound lutetium-177 and yttrium-90 to treat neuroendocrine tumors (peptide receptor radionuclide therapy).

Another example is the injection of yttrium-90 radioactive glass or resin microspheres into the hepatic artery to radioembolize liver tumors or liver metastases. These microspheres are used for the treatment approach known as selective internal radiation therapy. The microspheres are approximately 30 µm in diameter (about one-third of a human hair) and are delivered directly into the artery supplying blood to the tumors. These treatments begin by guiding a catheter up through the femoral artery in the leg, navigating to the desired target site and administering treatment. The blood feeding the tumor will carry the microspheres directly to the tumor enabling a more selective approach than traditional systemic chemotherapy. There are currently two different kinds of microspheres: SIR-Spheres and TheraSphere.

A major use of systemic radioisotope therapy is in the treatment of bone metastasis from cancer. The radioisotopes travel selectively to areas of damaged bone, and spare normal undamaged bone. Isotopes commonly used in the treatment of bone metastasis are radium-223, strontium-89 and samarium (Sm) lexidronam.

In 2002, the United States Food and Drug Administration (FDA) approved ibritumomab tiuxetan (Zevalin), which is an anti-CD20 monoclonal antibody conjugated to yttrium-90.
In 2003, the FDA approved the tositumomab/iodine (I) tositumomab regimen (Bexxar), which is a combination of an iodine-131 labelled and an unlabelled anti-CD20 monoclonal antibody.
These medications were the first agents of what is known as radioimmunotherapy, and they were approved for the treatment of refractory non-Hodgkins lymphoma.

Intraoperative radiation therapy (IORT) is applying therapeutic levels of radiation to a target area, such as a cancer tumor, while the area is exposed during surgery.

The rationale for IORT is to deliver a high dose of radiation precisely to the targeted area with minimal exposure of surrounding tissues which are displaced or shielded during the IORT. Conventional radiation techniques such as external beam radiotherapy (EBRT) following surgical removal of the tumor have several drawbacks: The tumor bed where the highest dose should be applied is frequently missed due to the complex localization of the wound cavity even when modern radiotherapy planning is used. Additionally, the usual delay between the surgical removal of the tumor and EBRT may allow a repopulation of the tumor cells. These potentially harmful effects can be avoided by delivering the radiation more precisely to the targeted tissues leading to immediate sterilization of residual tumor cells. Another aspect is that wound fluid has a stimulating effect on tumor cells. IORT was found to inhibit the stimulating effects of wound fluid.

Deep inspiration breath-hold (DIBH) is a method of delivering radiotherapy while limiting radiation exposure to the heart and lungs. It is used primarily for treating left-sided breast cancer. The technique involves a patient holding their breath during treatment. There are two basic methods of performing DIBH: free-breathing breath-hold and spirometry-monitored deep inspiration breath hold.

Medicine has used radiation therapy as a treatment for cancer for more than 100 years, with its earliest roots traced from the discovery of x-rays in 1895 by Wilhelm Röntgen. Emil Grubbe of Chicago was possibly the first American physician to use x-rays to treat cancer, beginning in 1896.

The field of radiation therapy began to grow in the early 1900s largely due to the groundbreaking work of Nobel Prize–winning scientist Marie Curie (1867–1934), who discovered the radioactive elements polonium and radium in 1898. This began a new era in medical treatment and research. Through the 1920s the hazards of radiation exposure were not understood, and little protection was used. Radium was believed to have wide curative powers and radiotherapy was applied to many diseases.

Prior to World War 2, the only practical sources of radiation for radiotherapy were radium and its "emanation", radon gas, and the x-ray tube. External beam radiotherapy (teletherapy) began at the turn of the century with relatively low voltage (<150 kV) x-ray machines. It was found that while superficial tumors could be treated with low voltage x-rays, more penetrating, higher energy beams were required to reach tumors inside the body, requiring higher voltages. Orthovoltage X-rays, which used tube voltages of 200-500 kV, began to be used during the 1920s. To reach the most deeply buried tumors without exposing intervening skin and tissue to dangerous radiation doses required rays with energies of 1 MV or above, called "megavolt" radiation. Producing megavolt x-rays required voltages on the x-ray tube of 3 to 5 million volts, which required huge expensive installations. Megavoltage x-ray units were first built in the late 1930s but because of cost were limited to a few institutions. One of the first, installed at St. Bartholomew's hospital, London in 1937 and used until 1960, used a 30 foot long x-ray tube and weighed 10 tons. Radium produced megavolt gamma rays, but was extremely rare and expensive due to its low occurrence in ores. In 1937 the entire world supply of radium for radiotherapy was 50 grams, valued at £800,000, or $50 million in 2005 dollars.

The invention of the nuclear reactor in the Manhattan Project during World War 2 made possible the production of artificial radioisotopes for radiotherapy. Cobalt therapy, teletherapy machines using megavolt gamma rays emitted by cobalt-60, a radioisotope produced by irradiating ordinary cobalt metal in a reactor, revolutionized the field between the 1950s and the early 1980s. Cobalt machines were relatively cheap, robust and simple to use, although due to its 5.27 year half-life the cobalt had to be replaced about every 5 years.

Medical linear particle accelerators, developed since the 1940s, began replacing x-ray and cobalt units in the 1980s and these older therapies are now declining. The first medical linear accelerator was used at the Hammersmith Hospital in London in 1953. Linear accelerators can produce higher energies, have more collimated beams, and do not produce radioactive waste with its attendant disposal problems like radioisotope therapies.

With Godfrey Hounsfield’s invention of computed tomography (CT) in 1971, three-dimensional planning became a possibility and created a shift from 2-D to 3-D radiation delivery. CT-based planning allows physicians to more accurately determine the dose distribution using axial tomographic images of the patient's anatomy. The advent of new imaging technologies, including magnetic resonance imaging (MRI) in the 1970s and positron emission tomography (PET) in the 1980s, has moved radiation therapy from 3-D conformal to intensity-modulated radiation therapy (IMRT) and to image-guided radiation therapy (IGRT) tomotherapy. These advances allowed radiation oncologists to better see and target tumors, which have resulted in better treatment outcomes, more organ preservation and fewer side effects.

While access to radiotherapy is improving globally, more than half of patients in low and middle income countries still do not have available access to the therapy as of 2017.







</doc>
<doc id="26354" url="https://en.wikipedia.org/wiki?curid=26354" title="Ronald Coase">
Ronald Coase

Ronald Harry Coase (; 29 December 1910 – 2 September 2013) was a British economist and author. He was the Clifton R. Musser Professor of Economics at the University of Chicago Law School, where he arrived in 1964 and remained for the rest of his life. He received the Nobel Memorial Prize in Economic Sciences in 1991.

Coase, who believed economists should study real markets and not theoretical ones, established the case for the corporation as a means to pay the costs of operating a marketplace. Coase is best known for two articles in particular: "The Nature of the Firm" (1937), which introduces the concept of transaction costs to explain the nature and limits of firms, and "The Problem of Social Cost" (1960), which suggests that well-defined property rights could overcome the problems of externalities (see Coase theorem). Additionally, Coase's transaction costs approach is currently influential in modern organizational economics, where it was reintroduced by Oliver E. Williamson.

Ronald Harry Coase was born in Willesden, a suburb of London, on 29 December 1910. His father, Henry Joseph Coase (1884–1973) was a telegraphist for the post office, as was his mother, Rosalie Elizabeth Coase (née Giles; 1882–1972), before marriage. As a child, Coase had a weakness in his legs, for which he was required to wear leg-irons. Due to this problem, he attended the school for physical defectives. At the age of 12, he was able to enter the Kilburn Grammar School on scholarship. At Kilburn, he studied for the intermediate examination of the University of London as an external student in 1927–29. Coase married Marion Ruth Hartung of Chicago, Illinois in Willesden, England, 7 August 1937.

Coase attended the London School of Economics, where he took courses with Arnold Plant and received a bachelor of commerce degree in 1932. During his undergraduate studies, Coase received the Sir Ernest Cassel Travelling Scholarship, awarded by the University of London. He used this to visit the University of Chicago in 1931–1932 and studied with Frank Knight and Jacob Viner. Coase's colleagues would later admit that they did not remember this first visit. Between 1932–34, Coase was an assistant lecturer at the Dundee School of Economics and Commerce, which later became part of the University of Dundee. Subsequently, Coase was an assistant lecturer in commerce at the University of Liverpool between 1934–1935 before returning to London School of Economics as a member of staff until 1951. He then started to work at the University at Buffalo and retained his British citizenship after moving to the United States in the 1950s. In 1958, he moved to the University of Virginia. Coase settled at the University of Chicago in 1964 and became the editor of the "Journal of Law and Economics". He was also for a time a trustee of the Philadelphia Society. He received the Nobel Prize in Economics in 1991.

Nearing his 100th birthday, Coase was working on a book concerning the rise of the economies of China and Vietnam. In an interview, Coase explained the mission of the Coase China Society and his vision of economics and the part to be played by Chinese economists. Coase was honoured and received an honorary doctorate from the University at Buffalo Department of Economics in May 2012.

Coase died in Chicago on 2 September 2013 at the age of 102. His wife had died on 17 October 2012. He was praised across the political spectrum, with Slate Magazine calling him "one of the most distinguished economists in the world" and "Forbes" magazine calling him "the greatest of the many great University of Chicago economists". The Washington Post called his work over eight decades "impossible to summarize" while recommending five of his papers to read.

In "The Nature of the Firm" (1937) – a brief but highly influential essay – Coase attempts to explain why the economy features a number of business firms instead of consisting exclusively of a multitude of independent, self-employed people who contract with one another. Given that "production could be carried on without any organization [that is, firms] at all", Coase asks, why and under what conditions should we expect firms to emerge?

Since modern firms can only emerge when an entrepreneur of some sort begins to hire people, Coase's analysis proceeds by considering the conditions under which it makes sense for an entrepreneur to seek hired help instead of contracting out for some particular task.

The traditional economic theory of the time (in the tradition of Adam Smith) suggested that, because the market is "efficient" (that is, those who are best at providing each good or service most cheaply are already doing so), it should always be cheaper to contract out than to hire.

Coase noted, however, a number of transaction costs involved in using the market; the cost of obtaining a good or service via the market actually exceeds the price of the good. Other costs, including search and information costs, bargaining costs, keeping trade secrets, and policing and enforcement costs, can all potentially add to the cost of procuring something from another party. This suggests that firms will arise which can internalise the production of goods and services required to deliver a product, thus avoiding these costs. This argument sets the stage for the later contributions by Oliver Williamson: markets and hierarchies are alternative co-ordination mechanisms for economic transactions.

There is a natural limit to what a firm can produce internally, however. Coase notices "decreasing returns to the entrepreneur function", including increasing overhead costs and increasing propensity for an overwhelmed manager to make mistakes in resource allocation. These factors become countervailing costs to the use of the firm.

Coase argues that the size of a firm (as measured by how many contractual relations are "internal" to the firm and how many "external") is a result of finding an optimal balance between the competing tendencies of the costs outlined above. In general, making the firm larger will initially be advantageous, but the decreasing returns indicated above will eventually kick in, preventing the firm from growing indefinitely.

Other things being equal, therefore, a firm will tend to be larger:

The first two costs will increase with the spatial distribution of the transactions organised and the dissimilarity of the transactions. This explains why firms tend to either be in different geographic locations or to perform different functions. Additionally, technology changes that mitigate the cost of organising transactions across space may allow firms to become larger – the advent of the telephone and of cheap air travel, for example, would be expected to increase the size of firms.

Upon publishing his article The Federal Communications Commission in 1959, Coase received negative feedback from the faculty at the University of Chicago over his conclusions and apparent conflicts with A. C. Pigou. According to Coase, "What I said was thought to run counter to Pigou's analysis by a number of economists at the University of Chicago and was therefore, according to them, wrong. At a meeting in Chicago I was able to convince these economists that I was right and Pigou's analysis faulty." Coase had presented his paper in 1960 during a seminar in Chicago, to twenty senior economist including George Stigler and Milton Friedman. He gradually won over the usually skeptic audience, in what has later been considered a "paradigm-shifting moment" in the genesis of Chicago Law and Economics. Coase would join the Chicago faculty four years later.

Published in the "Journal of Law and Economics" in 1960, while Coase was a member of the Economics department at the University of Virginia, "The Problem of Social Cost" provided the key insight that it is unclear where the blame for externalities lies. The example he gave was of a rancher whose cattle stray onto the cropland of his neighbour. If the rancher is made to restrict his cattle, he is harmed just as the farmer is if the cattle remain unrestrained.

Coase argued that without transaction costs the initial assignment of property rights makes no difference to whether or not the farmer and rancher can achieve the economically efficient outcome. If the cost of restraining cattle by, say, building a fence, is less than the cost of crop damage, the fence will be built. The initial assignment of property rights determines who builds the fence. If the farmer is responsible for the crop damage, the farmer will pay for the fence (as long the fence costs less than the crop damage). The allocation of property rights is primarily an equity issue, with consequences for the distribution of income and wealth, rather than an efficiency issue.

With sufficient transaction costs, initial property rights matter for both equity and efficiency. From the point of view of economic efficiency, property rights should be assigned such that the owner of the rights wants to take the economically efficient action. To elaborate, if it is efficient not to restrict the cattle, the rancher should be given the rights (so that cattle can move about freely), whereas if it is efficient to restrict the cattle, the farmer should be given the rights over the movement of the cattle (so the cattle are restricted).

This seminal argument forms the basis of the famous Coase theorem as labelled by Stigler.

Though trained as an economist, Coase spent much of his career working in a law school. He is a central figure in the development of the subfield of law and economics. He viewed law and economics as having two parts, the first "using the economists' approach and concepts to analyze the working of the legal system, often called the economic analysis of the law"; and the second "a study of the influence of the legal system on the working of the economic system." Coase said that the second part "is the part of law and economics in which I am most interested."

In his Simons Lecture celebrating the centennial of the University of Chicago, titled "Law and Economics at Chicago," Coase noted that he only accidentally wandered into the field.

Despite wandering accidentally into law and economics, the opportunity to edit the Journal of Law and Economics was instrumental in bringing him to the University of Chicago.

Coase believed that the University of Chicago was the intellectual center of law and economics. He concluded his Simons lecture by stating,

I am very much aware that, in concentrating in this lecture on law and economics at Chicago, I have neglected other significant contributions to the subject made elsewhere such as those by Guido Calabresi at Yale, by Donald Turner at Harvard, and by others. But it can hardly be denied that in the emergence of the subject of law and economics, Chicago has played a very significant part and one of which the University can be proud.

Another important contribution of Coase is the Coase Conjecture: an informal argument that durable-goods monopolists do not have market power because they are unable to commit to not lowering their prices in future periods.

When asked what he considered his politics to be, Coase stated, 

I really don't know. I don't reject any policy without considering what its results are. If someone says there's going to be regulation, I don't say that regulation will be bad. Let's see. What we discover is that most regulation does produce, or has produced in recent times, a worse result. But I wouldn't like to say that all regulation would have this effect because one can think of circumstances in which it doesn't.

Coase admitted that early in life, he aligned himself with socialism.

Guido Calabresi wrote that Coase's focus on transaction costs in "The Nature of the Firm" was the result of his socialist beliefs. Reflecting on this, Coase wrote, "It is very difficult to know where one's ideas come from but for all I know he may well be right." Coase continued:

Coase was research advisor to the Ronald Coase Institute, an organisation that promotes research on institutions and organizations – the laws, rules, customs, and norms – that govern real economic systems, with particular support for young scholars from developing and transitional countries.

The University of Chicago Law School carries on the legacy of Ronald Coase through the mission of the Coase-Sandor Institute for Law and Economics. Each year, the University of Chicago Law School hosts the Coase Lecture, which was delivered in 2003 by Ronald Coase himself.





</doc>
<doc id="26360" url="https://en.wikipedia.org/wiki?curid=26360" title="Robert Gordis">
Robert Gordis

Robert Gordis (February 6, 1908 – 1992) was an American leading Conservative rabbi. He founded the first Conservative Jewish day school, served as President of the Rabbinical Assembly and the Synagogue Council of America, and was a professor at Jewish Theological Seminary of America from 1940 to 1992.

He wrote one of the first pamphlets explaining Conservative ideology in 1946, and in 1988 he chaired the Commission on the Philosophy of Conservative Judaism which produced the official statement of Conservative ideology "Emet Ve-Emunah".

Gordis was the founding editor in 1951 of the quarterly journal "Judaism".



</doc>
<doc id="26361" url="https://en.wikipedia.org/wiki?curid=26361" title="Richard R. Ernst">
Richard R. Ernst

Richard Robert Ernst (born 14 August 1933) is a Swiss physical chemist and Nobel Laureate.

Born in Winterthur, Switzerland, Ernst was awarded the Nobel Prize in Chemistry in 1991 for his contributions towards the development of Fourier transform Nuclear Magnetic Resonance (NMR) spectroscopy while at Varian Associates, Palo Alto and the subsequent development of multi-dimensional NMR techniques. These underpin applications to both to chemistry with NMR spectroscopy and to medicine with Magnetic Resonance Imaging (MRI).

Ernst received both his diploma in chemistry in 1957 and his Ph.D. in physical chemistry in 1962 from ETH Zurich.

Ernst is a foreign fellow of the Estonian Academy of Sciences (elected 2002) and Bangladesh Academy of Sciences. He was elected a Foreign Member of the Royal Society (ForMemRS) in 1993. He was awarded the John Gamble Kirkwood Medal in 1989.
The Nobel Prize in Chemistry 1991 was awarded to Richard R. Ernst "for his contributions to the development of the methodology of high resolution nuclear magnetic resonance (NMR) spectroscopy" A strong proponent of Ernst's nomination was the long-time Danish colleague and member of the Nobel Committee Professor Børge Bak.

He holds Honorary Doctorates from the Technical University of Munich and University of Zurich.

Ernst is member of the World Knowledge Dialogue Scientific Board. Ernst was awarded the Louisa Gross Horwitz Prize of Columbia University in 1991. He was also awarded the Tadeus Reichstein Medal in 2000 and the Order of the Star of Romania in 2004.

The 2009 Bel Air Film Festival featured the world premiere of a documentary film on Ernst "Science Plus Dharma Equals Social Responsibility". Produced by Carlo Burton, the film takes place in Ernst's hometown in Switzerland.


</doc>
<doc id="26363" url="https://en.wikipedia.org/wiki?curid=26363" title="RIPEMD">
RIPEMD

RIPEMD (RACE Integrity Primitives Evaluation Message Digest) is a family of cryptographic hash functions developed in Leuven, Belgium, by Hans Dobbertin, Antoon Bosselaers and Bart Preneel at the COSIC research group at the Katholieke Universiteit Leuven, and first published in 1996. RIPEMD was based upon the design principles used in MD4, and is similar in performance to the more popular SHA-1.

RIPEMD-160 is an improved, 160-bit version of the original RIPEMD, and the most common version in the family. RIPEMD-160 was designed in the open academic community, in contrast to the NSA-designed SHA-1 and SHA-2 algorithms. On the other hand, RIPEMD-160 appears to be used somewhat less frequently than SHA-1, which may have caused it to be less scrutinized than SHA-1. RIPEMD-160 is not known to be constrained by any patents.

As well as 160-bit, there also exist 128-, 256- and 320-bit versions of this algorithm, called RIPEMD-128, RIPEMD-256, and RIPEMD-320, respectively. The 128-bit version was intended only as a drop-in replacement for the original RIPEMD, which was also 128-bit, and which had been found to have questionable security. The 256- and 320-bit versions diminish only the chance of accidental collision and don't have higher levels of security (against preimage attacks) as compared to, respectively, RIPEMD-128 and RIPEMD-160.

In August 2004, a collision was reported for the original RIPEMD. This does not apply to RIPEMD-160.

The 160-bit RIPEMD-160 hashes (also termed RIPE "message digests") are typically represented as 40-digit hexadecimal numbers. The following demonstrates a 43-byte ASCII input and the corresponding RIPEMD-160 hash:

RIPEMD-160 behaves with the desired avalanche effect of cryptographic hash functions (small changes, e.g. changing d to c, result in a completely different hash):

The hash of a zero-length string is:




</doc>
<doc id="26364" url="https://en.wikipedia.org/wiki?curid=26364" title="Roman law">
Roman law

Roman law is the legal system of ancient Rome, including the legal developments spanning over a thousand years of jurisprudence, from the Twelve Tables (c. 449 BC), to the "Corpus Juris Civilis" (AD 529) ordered by Eastern Roman Emperor Justinian I. Roman law forms the basic framework for civil law, the most widely used legal system today, and the terms are sometimes used synonymously. The historical importance of Roman law is reflected by the continued use of Latin legal terminology in many legal systems influenced by it, including common law.

After the dissolution of the Western Roman Empire, the Roman law remained in effect in the Eastern Roman Empire. From the 7th century onward, the legal language in the East was Greek.

"Roman law" also denoted the legal system applied in most of Western Europe until the end of the 18th century. In Germany, Roman law practice remained in place longer under the Holy Roman Empire (963–1806). Roman law thus served as a basis for legal practice throughout Western continental Europe, as well as in most former colonies of these European nations, including Latin America, and also in Ethiopia. English and Anglo-American common law were influenced also by Roman law, notably in their Latinate legal glossary (for example, "stare decisis", "culpa in contrahendo", "pacta sunt servanda"). Eastern Europe was also influenced by the jurisprudence of the "Corpus Juris Civilis", especially in countries such as medieval Romania (Wallachia, Moldavia, and some other medieval provinces/historical regions) which created a new system, a mixture of Roman and local law. Also, Eastern European law was influenced by the "Farmer's Law" of the medieval Byzantine legal system.

Before the Twelve Tables (754–449 BC), private law comprised the Roman civil law ("ius civile Quiritium") that applied only to Roman citizens, and was bonded to religion; undeveloped, with attributes of strict formalism, symbolism, and conservatism, e.g. the ritual practice of mancipatio (a form of sale). The jurist Sextus Pomponius said, "At the beginning of our city, the people began their first activities without any fixed law, and without any fixed rights: all things were ruled despotically, by kings". It is believed that Roman Law is rooted in the Etruscan religion, emphasizing ritual.

The first legal text is the Law of the Twelve Tables, dating from the mid-5th century BC. The plebeian tribune, C. Terentilius Arsa, proposed that the law should be written, in order to prevent magistrates from applying the law arbitrarily. After eight years of political struggle, the plebeian social class convinced the patricians to send a delegation to Athens, to copy the Laws of Solon; they also dispatched delegations to other Greek cities for like reason. In 451 BC, according to the traditional story (as Livy tells it), ten Roman citizens were chosen to record the laws ("decemviri legibus scribundis"). While they were performing this task, they were given supreme political power ("imperium"), whereas the power of the magistrates was restricted. In 450 BC, the "decemviri" produced the laws on ten tablets ("tabulae"), but these laws were regarded as unsatisfactory by the plebeians. A second decemvirate is said to have added two further tablets in 449 BC. The new Law of the Twelve Tables was approved by the people's assembly.

Modern scholars tend to challenge the accuracy of Roman historians. They generally do not believe that a second decemvirate ever took place. The decemvirate of 451 is believed to have included the most controversial points of customary law, and to have assumed the leading functions in Rome. Furthermore, the question on the Greek influence found in the early Roman Law is still much discussed. Many scholars consider it unlikely that the patricians sent an official delegation to Greece, as the Roman historians believed. Instead, those scholars suggest, the Romans acquired Greek legislations from the Greek cities of Magna Graecia, the main portal between the Roman and Greek worlds. The original text of the Twelve Tables has not been preserved. The tablets were probably destroyed when Rome was conquered and burned by the Gauls in 387 BC.

The fragments which did survive show that it was not a law code in the modern sense. It did not provide a complete and coherent system of all applicable rules or give legal solutions for all possible cases. Rather, the tables contained specific provisions designed to change the then-existing customary law. Although the provisions pertain to all areas of law, the largest part is dedicated to private law and civil procedure.

Many laws include "Lex Canuleia" (445 BC; which allowed the marriage—"ius connubii"—between patricians and plebeians), "Leges Licinae Sextiae" (367 BC; which made restrictions on possession of public lands—"ager publicus"—and also made sure that one of the consuls was plebeian), "Lex Ogulnia" (300 BC; plebeians received access to priest posts), and "Lex Hortensia" (287 BC; verdicts of plebeian assemblies—"plebiscita"—now bind all people).

Another important statute from the Republican era is the "Lex Aquilia" of 286 BC, which may be regarded as the root of modern tort law. However, Rome's most important contribution to European legal culture was not the enactment of well-drafted statutes, but the emergence of a class of professional jurists ("prudentes", sing. "prudens", or "jurisprudentes") and of a legal science. This was achieved in a gradual process of applying the scientific methods of Greek philosophy to the subject of law, a subject which the Greeks themselves never treated as a science.

Traditionally, the origins of Roman legal science are connected to Gnaeus Flavius. Flavius is said to have published around the year 300 BC the formularies containing the words which had to be spoken in court to begin a legal action. Before the time of Flavius, these formularies are said to have been secret and known only to the priests. Their publication made it possible for non-priests to explore the meaning of these legal texts. Whether or not this story is credible, jurists were active and legal treatises were written in larger numbers before the 2nd century BC. Among the famous jurists of the republican period are Quintus Mucius Scaevola who wrote a voluminous treatise on all aspects of the law, which was very influential in later times, and Servius Sulpicius Rufus, a friend of Marcus Tullius Cicero. Thus, Rome had developed a very sophisticated legal system and a refined legal culture when the Roman republic was replaced by the monarchical system of the principate in 27 BC.

In the period between about 201 to 27 BC, we can see the development of more flexible laws to match the needs of the time. In addition to the old and formal "ius civile" a new juridical class is created: the "ius honorarium", which can be defined as "The law introduced by the magistrates who had the right to promulgate edicts in order to support, supplement or correct the existing law." With this new law the old formalism is being abandoned and new more flexible principles of "ius gentium" are used.

The adaptation of law to new needs was given over to juridical practice, to magistrates, and especially to the praetors. A praetor was not a legislator and did not technically create new law when he issued his edicts ("magistratuum edicta"). In fact, the results of his rulings enjoyed legal protection ("actionem dare") and were in effect often the source of new legal rules. A Praetor's successor was not bound by the edicts of his predecessor; however, he did take rules from edicts of his predecessor that had proved to be useful. In this way a constant content was created that proceeded from edict to edict ("edictum traslatitium").

Thus, over the course of time, parallel to the civil law and supplementing and correcting it, a new body of praetoric law emerged. In fact, praetoric law was so defined by the famous Roman jurist Papinian (Amilius Papinianus—died in 212 AD): ""Ius praetorium est quod praetores introduxerunt adiuvandi vel supplendi vel corrigendi iuris civilis gratia propter utilitatem publicam"" ("praetoric law is that law introduced by praetors to supplement or correct civil law for public benefit"). Ultimately, civil law and praetoric law were fused in the "Corpus Juris Civilis".

The first 250 years of the current era are the period during which Roman law and Roman legal science reached its greatest degree of sophistication. The law of this period is often referred to as the "classical period of Roman law". The literary and practical achievements of the jurists of this period gave Roman law its unique shape.

The jurists worked in different functions: They gave legal opinions at the request of private parties. They advised the magistrates who were entrusted with the administration of justice, most importantly the praetors. They helped the praetors draft their edicts, in which they publicly announced at the beginning of their tenure, how they would handle their duties, and the formularies, according to which specific proceedings were conducted. Some jurists also held high judicial and administrative offices themselves.

The jurists also produced all kinds of legal punishments. Around AD 130 the jurist Salvius Iulianus drafted a standard form of the praetor's edict, which was used by all praetors from that time onwards. This edict contained detailed descriptions of all cases, in which the praetor would allow a legal action and in which he would grant a defense. The standard edict thus functioned like a comprehensive law code, even though it did not formally have the force of law. It indicated the requirements for a successful legal claim. The edict therefore became the basis for extensive legal commentaries by later classical jurists like Paulus and Ulpian. The new concepts and legal institutions developed by pre-classical and classical jurists are too numerous to mention here. Only a few examples are given here:

The Roman Republic had three different branches:


The Assemblies could decide whether war or peace. The Senate had complete control over the Treasury, and the Consuls had the highest juridical power.

By the middle of the 3rd century, the conditions for the flourishing of a refined legal culture had become less favourable. The general political and economic situation deteriorated as the emperors assumed more direct control of all aspects of political life. The political system of the principate, which had retained some features of the republican constitution, began to transform itself into the absolute monarchy of the dominate. The existence of a legal science and of jurists who regarded law as a science, not as an instrument to achieve the political goals set by the absolute monarch, did not fit well into the new order of things. The literary production all but ended. Few jurists after the mid-3rd century are known by name. While legal science and legal education persisted to some extent in the eastern part of the Empire, most of the subtleties of classical law came to be disregarded and finally forgotten in the west. Classical law was replaced by so-called vulgar law.


The Roman Republic's constitution or "mos maiorum" ("custom of the ancestors") was an unwritten set of guidelines and principles passed down mainly through precedent. Concepts that originated in the Roman constitution live on in constitutions to this day. Examples include checks and balances, the separation of powers, vetoes, filibusters, quorum requirements, term limits, impeachments, the powers of the purse, and regularly scheduled elections. Even some lesser used modern constitutional concepts, such as the block voting found in the electoral college of the United States, originate from ideas found in the Roman constitution.

The constitution of the Roman Republic was not formal or even official. Its constitution was largely unwritten, and was constantly evolving throughout the life of the Republic. Throughout the 1st century BC, the power and legitimacy of the Roman constitution was progressively eroding. Even Roman constitutionalists, such as the senator Cicero, lost a willingness to remain faithful to it towards the end of the republic. When the Roman Republic ultimately fell in the years following the Battle of Actium and Mark Antony's suicide, what was left of the Roman constitution died along with the Republic. The first Roman Emperor, Augustus, attempted to manufacture the appearance of a constitution that still governed the Empire, by utilising that constitution's institutions to lend legitimacy to the Principate, e.g. reusing prior grants of greater imperium to substantiate Augustus' greater imperium over the Imperial provinces and the prorogation of different magistracies to justify Augustus' receipt of tribunician power. The belief in a surviving constitution lasted well into the life of the Roman Empire.

"Stipulatio" was the basic form of contract in Roman law. It was made in the format of question and answer. The precise nature of the contract was disputed, as can be seen below.

"Rei vindicatio" is a legal action by which the plaintiff demands that the defendant return a thing that belongs to the plaintiff. It may only be used when plaintiff owns the thing, and the defendant is somehow impeding the plaintiff's possession of the thing. The plaintiff could also institute an "actio furti" (a personal action) to punish the defendant. If the thing could not be recovered, the plaintiff could claim damages from the defendant with the aid of the "condictio furtiva" (a personal action). With the aid of the "actio legis Aquiliae" (a personal action), the plaintiff could claim damages from the defendant. "Rei vindicatio" was derived from the ius civile, therefore was only available to Roman citizens.

To describe a person's position in the legal system, Romans mostly used the expression "togeus". The individual could have been a Roman citizen ("status civitatis") unlike foreigners, or he could have been free ("status libertatis") unlike slaves, or he could have had a certain position in a Roman family ("status familiae") either as the head of the family ("pater familias"), or some lower "member".*"alieni iuris"-which lives by someone else's law. Two status types were senator and emperor.

The history of Roman Law can be divided into three systems of procedure: that of "legis actiones", the "formulary system", and "cognitio extra ordinem". The periods in which these systems were in use overlapped one another and did not have definitive breaks, but it can be stated that the legis actio system prevailed from the time of the XII Tables (c. 450 BC) until about the end of the 2nd century BC, that the formulary procedure was primarily used from the last century of the Republic until the end of the classical period (c. AD 200), and that of cognitio extra ordinem was in use in post-classical times. Again, these dates are meant as a tool to help understand the types of procedure in use, not as a rigid boundary where one system stopped and another began.

During the republic and until the bureaucratization of Roman judicial procedure, the judge was usually a private person ("iudex privatus"). He had to be a Roman male citizen. The parties could agree on a judge, or they could appoint one from a list, called "album iudicum". They went down the list until they found a judge agreeable to both parties, or if none could be found they had to take the last one on the list.

No one had a legal obligation to judge a case. The judge had great latitude in the way he conducted the litigation. He considered all the evidence and ruled in the way that seemed just. Because the judge was not a jurist or a legal technician, he often consulted a jurist about the technical aspects of the case, but he was not bound by the jurist's reply. At the end of the litigation, if things were not clear to him, he could refuse to give a judgment, by swearing that it wasn't clear. Also, there was a maximum time to issue a judgment, which depended on some technical issues (type of action, etc.).

Later on, with the bureaucratization, this procedure disappeared, and was substituted by the so-called "extra ordinem" procedure, also known as cognitory. The whole case was reviewed before a magistrate, in a single phase. The magistrate had obligation to judge and to issue a decision, and the decision could be appealed to a higher magistrate.

When the centre of the Empire was moved to the Greek East in the 4th century, many legal concepts of Greek origin appeared in the official Roman legislation. The influence is visible even in the law of persons or of the family, which is traditionally the part of the law that changes least. For example, Constantine started putting restrictions on the ancient Roman concept of "patria potestas", the power held by the male head of a family over his descendents, by acknowledging that persons "in potestate", the descendents, could have proprietary rights. He was apparently making concessions to the much stricter concept of paternal authority under Greek-Hellenistic law. The "Codex Theodosianus" (438 AD) was a codification of Constantian laws. Later emperors went even further, until Justinian finally decreed that a child "in potestate" became owner of everything it acquired, except when it acquired something from its father.

The codes of Justinian, particularly the "Corpus Juris Civilis" (529–534) continued to be the basis of legal practice in the Empire throughout its so-called "Byzantine" history. Leo III the Isaurian issued a new code, the "Ecloga", in the early 8th century. In the 9th century, the emperors Basil I and Leo VI the Wise commissioned a combined translation of the Code and the Digest, parts of Justinian's codes, into Greek, which became known as the "Basilica". Roman law as preserved in the codes of Justinian and in the Basilica remained the basis of legal practice in Greece and in the courts of the Eastern Orthodox Church even after the fall of the Byzantine Empire and the conquest by the Turks, and also formed the basis for much of the "Fetha Negest", which remained in force in Ethiopia until 1931.

In the west, Justinian's political authority never went any farther than certain portions of the Italian and Hispanic peninsulas. In Law codes were issued by the Germanic kings, however, the influence of early Eastern Roman codes on some of these is quite discernible. In many early Germanic states, Roman citizens continued to be governed by Roman laws for quite some time, even while members of the various Germanic tribes were governed by their own respective codes.

The "Codex Justinianus" and the Institutes of Justinian were known in Western Europe, and along with the earlier code of Theodosius II, served as models for a few of the Germanic law codes; however, the "Digest" portion was largely ignored for several centuries until around 1070, when a manuscript of the "Digest" was rediscovered in Italy. This was done mainly through the works of glossars who wrote their comments between lines ("glossa interlinearis"), or in the form of marginal notes ("glossa marginalis"). From that time, scholars began to study the ancient Roman legal texts, and to teach others what they learned from their studies. The center of these studies was Bologna. The law school there gradually developed into Europe's first university.

The students who were taught Roman law in Bologna (and later in many other places) found that many rules of Roman law were better suited to regulate complex economic transactions than were the customary rules, which were applicable throughout Europe. For this reason, Roman law, or at least some provisions borrowed from it, began to be re-introduced into legal practice, centuries after the end of the Roman empire. This process was actively supported by many kings and princes who employed university-trained jurists as counselors and court officials and sought to benefit from rules like the famous "Princeps legibus solutus est" ("The sovereign is not bound by the laws", a phrase initially coined by Ulpian, a Roman jurist).

There are several reasons that Roman law was favored in the Middle Ages. Roman law regulated the legal protection of property and the equality of legal subjects and their wills, and it prescribed the possibility that the legal subjects could dispose their property through testament.

By the middle of the 16th century, the rediscovered Roman law dominated the legal practice of many European countries. A legal system, in which Roman law was mixed with elements of canon law and of Germanic custom, especially feudal law, had emerged. This legal system, which was common to all of continental Europe (and Scotland) was known as "Ius Commune". This "Ius Commune" and the legal systems based on it are usually referred to as civil law in English-speaking countries.

Only England and the Nordic countries did not take part in the wholesale reception of Roman law. One reason for this is that the English legal system was more developed than its continental counterparts by the time Roman law was rediscovered. Therefore, the practical advantages of Roman law were less obvious to English practitioners than to continental lawyers. As a result, the English system of common law developed in parallel to Roman-based civil law, with its practitioners being trained at the Inns of Court in London rather than receiving degrees in Canon or Civil Law at the Universities of Oxford or Cambridge. Elements of Romano-canon law were present in England in the ecclesiastical courts and, less directly, through the development of the equity system. In addition, some concepts from Roman law made their way into the common law. Especially in the early 19th century, English lawyers and judges were willing to borrow rules and ideas from continental jurists and directly from Roman law.

The practical application of Roman law and the era of the European "Ius Commune" came to an end, when national codifications were made. In 1804, the French civil code came into force. In the course of the 19th century, many European states either adopted the French model or drafted their own codes. In Germany, the political situation made the creation of a national code of laws impossible. From the 17th century, Roman law in Germany had been heavily influenced by domestic (common) law, and it was called "usus modernus Pandectarum". In some parts of Germany, Roman law continued to be applied until the German civil code (Bürgerliches Gesetzbuch, BGB) came into force in 1900.

Colonial expansion spread the civil law system.

Today, Roman law is no longer applied in legal practice, even though the legal systems of some countries like South Africa and San Marino are still based on the old "jus commune". However, even where the legal practice is based on a code, many rules deriving from Roman law apply: no code completely broke with the Roman tradition. Rather, the provisions of the Roman law were fitted into a more coherent system and expressed in the national language. For this reason, knowledge of the Roman law is indispensable to understand the legal systems of today. Thus, Roman law is often still a mandatory subject for law students in civil law jurisdictions.

As steps towards a unification of the private law in the member states of the European Union are being taken, the old "jus commune", which was the common basis of legal practice everywhere in Europe, but allowed for many local variants, is seen by many as a model.






</doc>
<doc id="26366" url="https://en.wikipedia.org/wiki?curid=26366" title="Reuben James">
Reuben James

Reuben James ( 1776 – 3 December 1838) was a boatswain's mate of the United States Navy, famous for his heroism in the First Barbary War.

Born in Delaware around 1776, James joined the United States Navy and served on several ships, including the frigate . During the First Barbary War, the American frigate was captured by the Barbary pirates when it ran aground in the city of Tripoli, on the southern shores of the Mediterranean Sea. During the course of the naval blockade of the harbor, there were numerous engagements, the most intense being the Gunboat Battle of August 3, 1804. During the battle, Lieutenant Stephen Decatur boarded a Tripolitan gunboat that he believed was crewed by the men who had mortally wounded his brother after supposedly surrendering. While Lieutenant Decatur was locked in hand-to-hand combat with the Tripolitan commander, another Tripolitan sailor swung his saber at him. According to early accepted accounts, Reuben James interposed himself between the descending sword and his commander, taking the blow on his head. The blow did not kill him, and he recovered later to continue serving in the Navy.

This account, though, is now considered to be in error. No one by the name of James is recorded as having received medical treatment after the battle. Another of Decatur's crewmen, Daniel Frazier, did receive medical treatment for a serious saber slash to the head. This supports some initial accounts that it was Frazier, not James, who saved Decatur's life.

James continued his Naval career, serving many years with Decatur. He was forced to retire in January 1836 because of ill health. He died in 1838 at the U.S. Naval Hospital in Washington, DC.

Three warships of the Navy have been named "Reuben James" in his honor:

James Island of Washington state was named for James.

Wheelan, Joseph. "Jefferson's War: American's First War on Terror 1801--1805", New York: Carroll & Graf Publishers, 2003.



</doc>
<doc id="26367" url="https://en.wikipedia.org/wiki?curid=26367" title="Rockwell International">
Rockwell International

Rockwell International was a major American manufacturing conglomerate in the latter half of the 20th century, involved in aircraft, the space industry, both defense-oriented and commercial electronics, automotive and truck components, printing presses, valves and meters, and industrial automation. Rockwell ultimately became a group of companies founded by Colonel Willard Rockwell. At its peak in the 1990s, Rockwell International was No. 27 on the Fortune 500 list, with assets of over $8 billion, sales of $27 billion and 115,000 employees.

Boston-born Willard Rockwell (1888–1978) made his fortune with the invention and successful launch of a new bearing system for truck axles in 1919. He merged his Oshkosh, Wisconsin-based operation with the Timken-Detroit Axle Company in 1928, rising to become chairman of its board in 1940.

In 1945, Rockwell Manufacturing Company acquired Delta Machinery and renamed it the Delta Power Tool Division of Rockwell Manufacturing Company and continued to manufacture in Milwaukee. In 1966, Rockwell invented the world's first power miter saw. In 1981, Rockwell's power tool group was acquired by Pentair and re-branded Delta Machinery. Pentair's Tools group was acquired by Black & Decker in 2005.

In 1956, Rockwell Manufacturing Co. bought Walker-Turner from Kearney and Trecker. In 1957, Walker-Turner operations were closed down in Plainfield, New Jersey and moved to Bellefontaine, Ohio and Tupelo, Mississippi.

Timken-Detroit merged in 1953 with the Standard Steel Spring Company, forming the Rockwell Spring and Axle Company. After various mergers with automotive suppliers, it comprised about 10 to 20 factories in the Upper Midwestern U.S. and southern Ontario, and in 1958 renamed itself Rockwell-Standard Corporation.

Pittsburgh-based Rockwell Standard then acquired and merged with Los Angeles-based North American Aviation to form North American Rockwell in September 1967. It then purchased or merged with Miehle-Goss-Dexter, the largest supplier of printing presses, and in 1973 acquired Collins Radio, a major avionics supplier. 

In 1968 Sterling Faucet Company was bought by Rockwell Manufacturing Co. and it became a subsidiary of the company for the following years. 

In 1973, North American Rockwell merged with Rockwell Manufacturing, run by Willard Rockwell, Jr., to form Rockwell International. In the same year, the company acquired Admiral Radio and TV for $500 million. In 1979, the appliance division was sold to Magic Chef.

Rockwell International also drew on the strengths of several of George Westinghouse's concerns, and Westinghouse is considered a co-founder of the company.

With the death of company founder and first CEO Willard F. Rockwell in 1978, and the stepping down of his son Willard Rockwell, Jr. in 1979 as the second CEO, Bob Anderson became CEO and led the company through the 1980s when it became the largest U.S. defense contractor and largest NASA contractor. Rockwell also acquired the privately held Allen-Bradley Company for $1.6 billion in February 1985 — $1 billion of which was cash to the owners of Allen Bradley — and became a producer of industrial automation hardware and software.

During the 1980s, Anderson, his CFO Bob dePalma and the Rockwell management team built the company to #27 on the Fortune 500 list. It boasted sales of $12 billion and assets of over $8 billion. Its workforce of over 100,000 was organized into nine major divisions — Space, Aircraft, Defense Electronics, Commercial Electronics, Light Duty Automotive Components, Heavy Duty Automotive Components, Printing Presses, Valves and Meters, and Industrial Automation. Rockwell International was a major employer in Southern California, northern Ohio, northern Georgia, eastern Oklahoma, Michigan, west Texas, Iowa, Illinois, Wisconsin and western Pennsylvania.

Anderson stepped down as CEO in February 1988, leaving the company to president Donald R. Beall. The completion of the Space Shuttle program and the completion of the B-1 bomber program had led to a decline in revenues, and Beall sought to diversify the company away from government contracts. The end of the Cold War and the perceived "peace dividend", however, prompted accelerated divestitures and sweeping management reforms. From 1988 to 2001 the company moved its headquarters four times: from Pittsburgh, Pennsylvania to El Segundo, California to Seal Beach, California to Costa Mesa, California to Milwaukee, Wisconsin.

At the end of the 1980s, the company sold its valve and meter division, formerly Rockwell Manufacturing, to British Tyre & Rubber. It also sold its printing press division to an internal management team. Following the "peace dividend" after the fall of the Soviet bloc, the company sold its defense and aerospace business, including what was once North American Aviation and Rocketdyne, to Boeing Integrated Defense Systems in December 1996. In the 1990s, the company spun off its semiconductor products as Conexant Technologies (CNXT), which is publicly traded and based in Newport Beach, California. Rockwell International also spun off its automotive division as a publicly traded company, Meritor Automotive, based in Troy, Michigan, which then merged with Arvin Industries to form Arvin Meritor. That company is now known as Meritor, Inc.

In 2001, what remained of Rockwell International was split into two companies, Rockwell Automation and Rockwell Collins — both publicly traded companies — ending the run of what had once been a massive and diverse conglomerate. The split was structured so that Rockwell Automation was the legal successor of the old Rockwell International, while Rockwell Collins was the spin-off.

The various Rockwell companies list a large number of firsts in their histories, including the World War II North American P-51 Mustang fighter and the North American B-25 Mitchell bomber, and the Korean War-era North American F-86 Sabre fighter jet, as well as the Apollo spacecraft, the Rockwell B-1 Lancer bomber, the Space Shuttle orbiter, and most of the Navstar Global Positioning System satellites.

Rocketdyne, which had been spun off by North American in 1955, was re-merged into Rockwell, and by that time produced most of the rocket engines used in the United States. Rockwell also purchased the Aero Design and Engineering Company from William and Rufus Travis Amis. Rockwell redesigned the company's Aero Commander aircraft, introducing its new design as the Rockwell Commander 112 and Commander 114.

The company developed a desktop calculator based on a MOSFET chip for use by its engineers. In 1967 Rockwell set up its own manufacturing plant to produce them, starting North American Rockwell MicroElectronics Corp. (called NARMEC). This would later become Rockwell Semiconductor. One of its major successes came in the early 1990s when it introduced the first low-cost 14.4 kbit/s modem chip set, which was used in a huge number of modems.

Collins radios were fitted to 80% of the airliners which were based in First World Countries. Collins designed and built the radios that communicated the Apollo moon landings and the high frequency radio network that allows worldwide communication with U.S. military aircraft. Rockwell's Rocketdyne division designed and built the third stage of the Minuteman intercontinental ballistic missile, and the Advanced Inertial Reference Sphere inertial navigation system that provided its navigation. It also built inertial navigation systems for the fleet of ballistic missile submarines. 
In addition to the manufacture of nuclear missiles and bombers, Rockwell also produced key components of the bombs they carried, including plutonium triggers at the Rocky Flats plant in Colorado. Rockwell ran the weapons plant from 1975 to 1990.

Rockwell built heavy-duty truck axles and drive-trains in the U.S., along with power windows, seats and locks. Rockwell also built yachts and business jets and owned large amounts of real estate.

It was also involved in providing custom electronic intelligence equipment to the Imperial Iranian Air Force as part of Project Ibex and paid bribes to the Shah of Iran in order to secure contracts there.





Rockwell International had a major research laboratory complex in Thousand Oaks, Ventura County, California. It was founded and built by North American Aviation in 1962, as the North American Science Center. In 1973 it became the Rockwell International Science Center.

The laboratory did independent contract research for the U.S. Government, and also provided research services for the company's business units. It was famous for its research in: advanced materials, particularly ceramics; for its infrared imagers; for its research in liquid-crystal displays; and for its high-speed electronics. The laboratory invented Metal Organic Chemical Vapor Deposition (MOCVD). It also achieved fame in selected areas of information science, notably human-computer interaction, augmented reality, multimedia systems, and diagnostics. Rockwell Science Center led the United States Army Research Laboratory's Advanced Displays Federated Laboratory Consortium in the late 1990s. In 2000, the infrared imaging division of the laboratory moved into a new building in Camarillo, California.

After Rockwell International's breakup in 2001, the laboratory was spun off as a semi-autonomous company called Rockwell Scientific, half owned by Rockwell Collins and half owned by Rockwell Automation. In 2006, the main laboratory and infrared imaging division were sold to Teledyne Corporation. Teledyne made the laboratory complex in Thousand Oaks into its corporate headquarters. A reduced but active research and development operation continues there, under the name Teledyne Scientific & Imaging, LLC.



</doc>
<doc id="26368" url="https://en.wikipedia.org/wiki?curid=26368" title="Richard I of England">
Richard I of England

Richard I (8 September 1157 – 6 April 1199) was King of England from 1189 until his death. He also ruled as Duke of Normandy, Aquitaine and Gascony, Lord of Cyprus, Count of Poitiers, Anjou, Maine, and Nantes, and was overlord of Brittany at various times during the same period. He was the third of five sons of King Henry II of England and Duchess Eleanor of Aquitaine. He was known as ' or Richard the Lionheart because of his reputation as a great military leader and warrior. He was also known in Occitan as ' ("Yes and No"), because of his reputation for terseness.

By the age of 16, Richard had taken command of his own army, putting down rebellions in Poitou against his father. Richard was a central Christian commander during the Third Crusade, leading the campaign after the departure of Philip II of France and scoring considerable victories against his Muslim counterpart, Saladin, although he did not retake Jerusalem from Saladin.

Richard spoke both French and Occitan. He was born in England, where he spent his childhood; before becoming king, however, he lived most of his adult life in the Duchy of Aquitaine, in the southwest of France. Following his accession, he spent very little time, perhaps as little as six months, in England. Most of his life as king was spent on Crusade, in captivity, or actively defending his lands in France. Rather than regarding his kingdom as a responsibility requiring his presence as ruler, he has been perceived as preferring to use it merely as a source of revenue to support his armies. Nevertheless, he was seen as a pious hero by his subjects. He remains one of the few kings of England remembered by his epithet, rather than regnal number, and is an enduring iconic figure both in England and in France.

Richard was born on 8 September 1157, probably at Beaumont Palace, in Oxford, England, son of King Henry II and Eleanor of Aquitaine. He was a younger brother of Count William IX of Poitiers, Henry the Young King and Duchess Matilda of Saxony. As the third legitimate son of King Henry II, he was not expected to ascend to the throne. He was also an elder brother of Duke Geoffrey II of Brittany; Queen Eleanor of Castile; Queen Joan of Sicily; and Count John of Mortain, who succeeded him as king. Richard was the younger maternal half-brother of Countess Marie of Champagne and Countess Alix of Blois. The eldest son of Henry II and Eleanor, William, died in 1156, before Richard's birth. Richard is often depicted as having been the favourite son of his mother. His father was Angevin-Norman and great-grandson of William the Conqueror. Contemporary historian Ralph of Diceto traced his family's lineage through Matilda of Scotland to the Anglo-Saxon kings of England and Alfred the Great, and from there legend linked them to Noah and Woden. According to Angevin family tradition, there was even 'infernal blood' in their ancestry, with a claimed descent from the fairy, or female demon, Melusine.

While his father visited his lands from Scotland to France, Richard probably spent his childhood in England. His first recorded visit to the European continent was in May 1165, when his mother took him to Normandy. His wet nurse was Hodierna of St Albans, whom he gave a generous pension after he became king. Little is known about Richard's education. Although he was born in Oxford and brought up in England up to his eighth year, it is not known to what extent he used or understood English; he was an educated man who composed poetry and wrote in Limousin ("") and also in French. During his captivity, English prejudice against foreigners was used in a calculated way by his brother John to help destroy the authority of Richard's chancellor, William Longchamp, who was a Norman. One of the specific charges laid against Longchamp, by John's supporter Hugh, Bishop of Coventry, was that he could not speak English. This indicates that by the late 12th century a knowledge of English was expected of those in positions of authority in England.

Richard was said to be very attractive; his hair was between red and blond, and he was light-eyed with a pale complexion. According to Clifford Brewer, he was . As with his supposed lack of English, the question of his stature is one made from a lack of evidence as his remains have been lost since at least the French Revolution, and his exact height is unknown. John, his youngest brother (by the same father and mother), was known to be . The , a Latin prose narrative of the Third Crusade, states that: "He was tall, of elegant build; the colour of his hair was between red and gold; his limbs were supple and straight. He had long arms suited to wielding a sword. His long legs matched the rest of his body".

From an early age, Richard showed significant political and military ability, becoming noted for his chivalry and courage as he fought to control the rebellious nobles of his own territory. His elder brother Henry the Young King was crowned king of England during his father's lifetime.

Marriage alliances were common among medieval royalty: they led to political alliances and peace treaties and allowed families to stake claims of succession on each other's lands. In March 1159 it was arranged that Richard would marry one of the daughters of Ramon Berenguer IV, Count of Barcelona; however, these arrangements failed, and the marriage never took place. Henry the Young King was married to Margaret, daughter of Louis VII of France, on 2 November 1160. Despite this alliance between the Plantagenets and the Capetians, the dynasty on the French throne, the two houses were sometimes in conflict. In 1168, the intercession of Pope Alexander III was necessary to secure a truce between them. Henry II had conquered Brittany and taken control of and the Vexin, which had been part of Margaret's dowry.

Early in the 1160s there had been suggestions Richard should marry Alys, Countess of the Vexin (Alice), fourth daughter of Louis VII; because of the rivalry between the kings of England and France, Louis obstructed the marriage. A peace treaty was secured in January 1169 and Richard's betrothal to Alys was confirmed. Henry II planned to divide his and Eleanor's territories among their three eldest surviving sons: Henry would become King of England and have control of Anjou, Maine, and Normandy; Richard would inherit Aquitaine and from his mother; and Geoffrey would become Duke of Brittany through marriage with Constance, heir presumptive of Conan IV. At the ceremony where Richard's betrothal was confirmed, he paid homage to the King of France for Aquitaine, thus securing ties of vassalage between the two.

After Henry II fell seriously ill in 1170, he enacted his plan to divide his kingdom, although he would retain overall authority over his sons and their territories. In 1171 Richard left for Aquitaine with his mother, and Henry II gave him the duchy of Aquitaine at the request of Eleanor. Richard and his mother embarked on a tour of Aquitaine in 1171 in an attempt to pacify the locals. Together they laid the foundation stone of St Augustine's Monastery in . In June 1172 Richard was formally recognised as the Duke of Aquitaine when he was granted the lance and banner emblems of his office; the ceremony took place in Poitiers and was repeated in Limoges, where he wore the ring of St Valerie, who was the personification of Aquitaine.

According to Ralph of Coggeshall, Henry the Young King instigated rebellion against Henry II; he wanted to reign independently over at least part of the territory his father had promised him, and to break away from his dependence on Henry II, who controlled the purse strings. There were rumors that Eleanor might have encouraged her sons to revolt against their father.

Henry the Young King abandoned his father and left for the French court, seeking the protection of Louis VII; his younger brothers, Richard and Geoffrey, soon followed him, while the five-year-old John remained in England. Louis gave his support to the three sons and even knighted Richard, tying them together through vassalage.
Jordan Fantosme, a contemporary poet, described the rebellion as a "war without love".
The three brothers made an oath at the French court that they would not make terms with Henry II without the consent of Louis VII and the French barons. With the support of Louis, Henry the Young King attracted many barons to his cause through promises of land and money; one such baron was Philip I, Count of Flanders, who was promised £1,000 and several castles. The brothers also had supporters ready to rise up in England. Robert de Beaumont, 3rd Earl of Leicester, joined forces with Hugh Bigod, 1st Earl of Norfolk, Hugh de Kevelioc, 5th Earl of Chester, and William I of Scotland for a rebellion in Suffolk. The alliance with Louis was initially successful, and by July 1173 the rebels were besieging , , and , and Hugh de Kevelioc had captured Dol in Brittany. Richard went to and raised the barons who were loyal to himself and his mother in rebellion against his father. Eleanor was captured, so Richard was left to lead his campaign against Henry II's supporters in Aquitaine on his own. He marched to take but was rejected by the inhabitants; he withdrew to the city of , which he established as a base of operations.

In the meantime, Henry II had raised a very expensive army of more than 20,000 mercenaries with which to face the rebellion. He marched on , and Louis retreated from his forces. The army proceeded to recapture and subdued Brittany. At this point Henry II made an offer of peace to his sons; on the advice of Louis the offer was refused. Henry II's forces took by surprise and captured much of its garrison, although Richard was able to escape with a small group of soldiers. He took refuge in for the rest of the war. Henry the Young King and the Count of Flanders planned to land in England to assist the rebellion led by the Earl of Leicester. Anticipating this, Henry II returned to England with 500 soldiers and his prisoners (including Eleanor and his sons' wives and fiancées), but on his arrival found out that the rebellion had already collapsed. William I of Scotland and Hugh Bigod were captured on 13 and 25 July respectively. Henry II returned to France and raised the siege of , where Louis VII had been joined by Henry the Young King after abandoning his plan to invade England. Louis was defeated and a peace treaty was signed in September 1174, the Treaty of .

When Henry II and Louis VII made a truce on 8 September 1174, its terms specifically excluded Richard. Abandoned by Louis and wary of facing his father's army in battle, Richard went to Henry II's court at Poitiers on 23 September and begged for forgiveness, weeping and falling at the feet of Henry, who gave Richard the kiss of peace. Several days later, Richard's brothers joined him in seeking reconciliation with their father. The terms the three brothers accepted were less generous than those they had been offered earlier in the conflict (when Richard was offered four castles in Aquitaine and half of the income from the duchy): Richard was given control of two castles in Poitou and half the income of Aquitaine; Henry the Young King was given two castles in Normandy; and Geoffrey was permitted half of Brittany. Eleanor remained Henry II's prisoner until his death, partly as insurance for Richard's good behaviour.

After the conclusion of the war, the process of pacifying the provinces that had rebelled against Henry II began. The King travelled to Anjou for this purpose, and Geoffrey dealt with Brittany. In January 1175 Richard was dispatched to Aquitaine to punish the barons who had fought for him. The historian John Gillingham notes that the chronicle of Roger of Howden is the main source for Richard's activities in this period. According to the chronicle, most of the castles belonging to rebels were to be returned to the state they were in 15 days before the outbreak of war, while others were to be razed. Given that by this time it was common for castles to be built in stone, and that many barons had expanded or refortified their castles, this was not an easy task. Roger of Howden records the two-month siege of ; while the castle was "notoriously strong", Richard's siege engines battered the defenders into submission. On this campaign, Richard acquired the name "the Lion" or "the Lionheart" due to his noble, brave and fierce leadership.
he is referred to as "this our lion" (') as early as 1187 in the ' of , while the byname "lionheart" (') is first recorded in Ambroise's ' in the context of the Accon campaign of 1191.

Henry seemed unwilling to entrust any of his sons with resources that could be used against him. It was suspected that Henry had appropriated Alys, Richard's betrothed, the daughter of Louis VII of France by his second wife, as his mistress. This made a marriage between Richard and Alys technically impossible in the eyes of the Church, but Henry prevaricated: he regarded Alys's dowry, Vexin in the , as valuable. Richard was discouraged from renouncing Alys because she was the sister of King Philip II of France, a close ally.
After his failure to overthrow his father, Richard concentrated on putting down internal revolts by the nobles of Aquitaine, especially in the territory of Gascony. The increasing cruelty of his rule led to a major revolt there in 1179. Hoping to dethrone Richard, the rebels sought the help of his brothers Henry and Geoffrey. The turning point came in the Charente Valley in the spring of 1179. The well-defended fortress of Taillebourg seemed impregnable. The castle was surrounded by a cliff on three sides and a town on the fourth side with a three-layer wall. Richard first destroyed and looted the farms and lands surrounding the fortress, leaving its defenders no reinforcements or lines of retreat. The garrison sallied out of the castle and attacked Richard; he was able to subdue the army and then followed the defenders inside the open gates, where he easily took over the castle in two days. Richard the Lionheart's victory at Taillebourg deterred many barons from thinking of rebelling and forced them to declare their loyalty to him. It also won Richard a reputation as a skilled military commander.

In 1181–1182 Richard faced a revolt over the succession to the county of . His opponents turned to Philip II of France for support, and the fighting spread through the and . The excessive cruelty of Richard's punitive campaigns aroused even more hostility. However, with support from his father and from the Young King, Richard the Lionheart eventually succeeded in bringing the Viscount Aimar V of Limoges and Count Elie of to terms.

After Richard had subdued his rebellious barons he again challenged his father. From 1180 to 1183 the tension between Henry and Richard grew, as King Henry commanded Richard to pay homage to Henry the Young King, but Richard refused. Finally, in 1183 Henry the Young King and Geoffrey, Duke of Brittany, invaded Aquitaine in an attempt to subdue Richard. Richard's barons joined in the fray and turned against their duke. However, Richard and his army succeeded in holding back the invading armies, and they executed any prisoners. The conflict paused briefly in June 1183 when the Young King died. With the death of Henry the Young King, Richard became the eldest surviving son and therefore heir to the English crown. King Henry demanded that Richard give up Aquitaine (which he planned to give to his youngest son John as his inheritance). Richard refused, and conflict continued between them. Henry II soon gave John permission to invade Aquitaine.

To strengthen his position, in 1187, Richard allied himself with 22-year-old Philip II, the son of Eleanor's ex-husband Louis VII by Adele of Champagne. Roger of Howden wrote:

The King of England was struck with great astonishment, and wondered what [this alliance] could mean, and, taking precautions for the future, frequently sent messengers into France for the purpose of recalling his son Richard; who, pretending that he was peaceably inclined and ready to come to his father, made his way to , and, in spite of the person who had the custody thereof, carried off the greater part of his father's treasures, and fortified his castles in Poitou with the same, refusing to go to his father.

Overall, Howden is chiefly concerned with the politics of the relationship between Richard and King Philip. Gillingham has addressed theories suggesting that this political relationship was also sexually intimate, which he posits probably stemmed from an official record announcing that, as a symbol of unity between the two countries, the kings of England and France had slept overnight in the same bed. Gillingham has characterized this as "an accepted political act, nothing sexual about it;... a bit like a modern-day photo opportunity".

In exchange for Philip's help against his father, Richard promised to concede to him his rights to both Normandy and Anjou. Richard paid homage to Philip in November 1187. With news arriving of the Battle of Hattin, he took the cross at in the company of other French nobles.

In 1188 Henry II planned to concede Aquitaine to his youngest son John. But Richard objected. He felt that Aquitaine was his and that John was unfit to take over the land once belonging to his mother. This refusal is what finally made Henry II bring Queen Eleanor out of prison. He sent her to Aquitaine and demanded that Richard give up his lands to his mother who would once again rule over those lands.

The following year, Richard attempted to take the throne of England for himself by joining Philip's expedition against his father. On 4 July 1189, the forces of Richard and Philip defeated Henry's army at . Henry, with John's consent, agreed to name Richard his heir apparent. Two days later Henry II died in , and Richard the Lionheart succeeded him as King of England, Duke of Normandy, and Count of Anjou. Roger of Howden claimed that Henry's corpse bled from the nose in Richard's presence, which was assumed to be a sign that Richard had caused his death.

Richard I was officially invested as Duke of Normandy on 20 July 1189 and crowned king in Westminster Abbey on 3 September 1189. Richard barred all Jews and women from the investiture, but some Jewish leaders arrived to present gifts for the new king. According to Ralph of Diceto, Richard's courtiers stripped and flogged the Jews, then flung them out of court.

When a rumour spread that Richard had ordered all Jews to be killed, the people of London attacked the Jewish population. Many Jewish homes were burned down, and several Jews were forcibly baptised. Some sought sanctuary in the Tower of London, and others managed to escape. Among those killed was Jacob of Orléans, a respected Jewish scholar. Roger of Howden, in his "", claimed that the jealous and bigoted citizens started the rioting, and that Richard punished the perpetrators, allowing a forcibly converted Jew to return to his native religion. Baldwin of Forde, Archbishop of Canterbury, reacted by remarking, "If the King is not God's man, he had better be the devil's".

Realising that the assaults could destabilise his realm on the eve of his departure on crusade, Richard ordered the execution of those responsible for the most egregious murders and persecutions, including rioters who had accidentally burned down Christian homes. He distributed a royal writ demanding that the Jews be left alone. The edict was loosely enforced, however, and the following March further violence occurred, including a massacre at York.

Richard had already taken the cross as Count of Poitou in 1187. His father and Philip II had done so at Gisors on 21 January 1188 after receiving news of the fall of Jerusalem to Saladin. After Richard became king, he and Philip agreed to go on the Third Crusade, since each feared that during his absence the other might usurp his territories.

Richard swore an oath to renounce his past wickedness in order to show himself worthy to take the cross. He started to raise and equip a new crusader army. He spent most of his father's treasury (filled with money raised by the Saladin tithe), raised taxes, and even agreed to free King William I of Scotland from his oath of subservience to Richard in exchange for marks. To raise still more revenue he sold the right to hold official positions, lands, and other privileges to those interested in them. Those already appointed were forced to pay huge sums to retain their posts. William Longchamp, Bishop of Ely and the King's Chancellor, made a show of bidding £ to remain as Chancellor. He was apparently outbid by a certain Reginald the Italian, but that bid was refused.

Richard made some final arrangements on the continent. He reconfirmed his father's appointment of William Fitz Ralph to the important post of seneschal of Normandy. In Anjou, Stephen of Tours was replaced as seneschal and temporarily imprisoned for fiscal mismanagement. , an Angevin knight, was elevated to the post of seneschal of Anjou. In Poitou the ex-provost of Benon, Peter Bertin, was made seneschal, and finally, in Gascony the household official was picked for the seneschalship there. After repositioning the part of his army he left behind to guard his French possessions, Richard finally set out on the crusade in summer 1190. (His delay was criticised by troubadours such as Bertran de Born.) He appointed as regents Hugh de Puiset, Bishop of Durham, and William de Mandeville, 3rd Earl of Essex—who soon died and was replaced by Richard's chancellor William Longchamp. Richard's brother John was not satisfied by this decision and started scheming against William. When Richard was raising funds for his crusade, he was said to declare, "I would have sold London if I could find a buyer".

In September 1190 Richard and Philip arrived in Sicily. After the death of King William II of Sicily his cousin Tancred had seized power and had been crowned early in 1190 as King Tancred of Sicily, although the legal heir was William's aunt Constance, wife of the new Emperor Henry VI. Tancred had imprisoned William's widow, Queen Joan, who was Richard's sister and did not give her the money she had inherited in William's will. When Richard arrived he demanded that his sister be released and given her inheritance; she was freed on 28 September, but without the inheritance. The presence of foreign troops also caused unrest: in October, the people of Messina revolted, demanding that the foreigners leave. Richard attacked Messina, capturing it on 4 October 1190. After looting and burning the city Richard established his base there, but this created tension between Richard and Philip Augustus. He remained there until Tancred finally agreed to sign a treaty on 4 March 1191. The treaty was signed by Richard, Philip, and Tancred. Its main terms were:

The two kings stayed on in Sicily for a while, but this resulted in increasing tensions between them and their men, with Philip Augustus plotting with Tancred against Richard. The two kings finally met to clear the air and reached an agreement, including the end of Richard's betrothal to Philip's sister Alys (who had supposedly been the mistress of Richard's father Henry II).

In April 1191 Richard left Messina for Acre, but a storm dispersed his large fleet. After some searching, it was discovered that the ship carrying his sister Joan and his new fiancée Berengaria was anchored on the south coast of Cyprus, along with the wrecks of several other vessels, including the treasure ship. Survivors of the wrecks had been taken prisoner by the island's ruler, Isaac Komnenos.

On 1 May 1191 Richard's fleet arrived in the port of Lemesos (Limassol) on Cyprus. He ordered Isaac to release the prisoners and treasure. Isaac refused, so Richard landed his troops and took Limassol. Various princes of the Holy Land arrived in Limassol at the same time, in particular Guy of Lusignan. All declared their support for Richard provided that he support Guy against his rival, Conrad of Montferrat.

The local magnates abandoned Isaac, who considered making peace with Richard, joining him on the crusade, and offering his daughter in marriage to the person named by Richard. Isaac changed his mind, however, and tried to escape. Richard's troops, led by Guy de Lusignan, conquered the whole island by 1 June. Isaac surrendered and was confined with silver chains because Richard had promised that he would not place him in irons. Richard named Richard de Camville and Robert of Thornham as governors. He later sold the island to the master of Knights Templar, , and it was subsequently acquired, in 1192, by Guy of Lusignan and became a stable feudal kingdom.

The rapid conquest of the island by Richard is more important than it may seem. The island occupies a key strategic position on the maritime lanes to the Holy Land, whose occupation by the Christians could not continue without support from the sea. Cyprus remained a Christian stronghold until the battle of Lepanto (1571). Richard's exploit was well publicised and contributed to his reputation, and he also derived significant financial gains from the conquest of the island. Richard left Cyprus for Acre on 5 June with his allies.

Before leaving Cyprus on crusade, Richard married Berengaria of Navarre, the first-born daughter of King Sancho VI of Navarre. Richard first grew close to her at a tournament held in her native Navarre. The wedding was held in Limassol on 12 May 1191 at the Chapel of St George and was attended by Richard's sister Joan, whom he had brought from Sicily. The marriage was celebrated with great pomp and splendour, many feasts and entertainments, and public parades and celebrations followed commemorating the event. When Richard married Berengaria he was still officially betrothed to Alys, and he pushed for the match in order to obtain the Kingdom of Navarre as a fief, as Aquitaine had been for his father. Further, Eleanor championed the match, as Navarre bordered Aquitaine, thereby securing the southern border of her ancestral lands. Richard took his new wife on crusade with him briefly, though they returned separately. Berengaria had almost as much difficulty in making the journey home as her husband did, and she did not see England until after his death. After his release from German captivity, Richard showed some regret for his earlier conduct, but he was not reunited with his wife. The marriage remained childless.

King Richard landed at Acre on 8 June 1191. He gave his support to his vassal Guy of Lusignan, who had brought troops to help him in Cyprus. Guy was the widower of his father's cousin Sibylla of Jerusalem and was trying to retain the kingship of Jerusalem, despite his wife's death during the Siege of Acre the previous year. Guy's claim was challenged by Conrad of Montferrat, second husband of Sibylla's half-sister, Isabella: Conrad, whose defence of Tyre had saved the kingdom in 1187, was supported by Philip of France, son of his first cousin Louis VII of France, and by another cousin, Duke Leopold V of Austria. Richard also allied with Humphrey IV of Toron, Isabella's first husband, from whom she had been forcibly divorced in 1190. Humphrey was loyal to Guy and spoke Arabic fluently, so Richard used him as a translator and negotiator.

Richard and his forces aided in the capture of Acre, despite the king's serious illness. At one point, while sick from scurvy, Richard is said to have picked off guards on the walls with a crossbow, while being carried on a stretcher. Eventually, Conrad of Montferrat concluded the surrender negotiations with Saladin's forces inside Acre and raised the banners of the kings in the city. Richard quarrelled with Leopold V of Austria over the deposition of Isaac Komnenos (related to Leopold's Byzantine mother) and his position within the crusade. Leopold's banner had been raised alongside the English and French standards. This was interpreted as arrogance by both Richard and Philip, as Leopold was a vassal of the Holy Roman Emperor (although he was the highest-ranking surviving leader of the imperial forces). Richard's men tore the flag down and threw it in the moat of Acre. Leopold left the crusade immediately. Philip also left soon afterwards, in poor health and after further disputes with Richard over the status of Cyprus (Philip demanded half the island) and the kingship of Jerusalem. Richard, suddenly, found himself without allies.

Richard had kept 2,700 Muslim prisoners as hostages against Saladin fulfilling all the terms of the surrender of the lands around Acre. Philip, before leaving, had entrusted his prisoners to Conrad, but Richard forced him to hand them over to him. Richard feared his forces being bottled up in Acre as he believed his campaign could not advance with the prisoners in train. He, therefore, ordered all the prisoners executed. He then moved south, defeating Saladin's forces at the Battle of Arsuf north of Jaffa on 7 September 1191. Saladin attempted to harass Richard's army into breaking its formation in order to defeat it in detail. Richard maintained his army's defensive formation, however, until the Hospitallers broke ranks to charge the right wing of Saladin's forces. Richard then ordered a general counterattack, which won the battle. Arsuf was an important victory. The Muslim army was not destroyed, despite the considerable casualties it suffered, but it did rout; this was considered shameful by the Muslims and boosted the morale of the Crusaders. In November 1191, following the fall of Jaffa, the Crusader army advanced inland towards Jerusalem. The army then marched to Beit Nuba, only 12 miles from Jerusalem. Muslim morale in Jerusalem was so low that the arrival of the Crusaders would probably have caused the city to fall quickly. However, the weather was appallingly bad, cold with heavy rain and hailstorms; this, combined with the fear that the Crusader army, if it besieged Jerusalem, might be trapped by a relieving force, led to the decision to retreat back to the coast. Richard attempted to negotiate with Saladin, but this was unsuccessful. In the first half of 1192, he and his troops refortified Ascalon.

An election forced Richard to accept Conrad of Montferrat as King of Jerusalem, and he sold Cyprus to his defeated protégé, Guy. Only days later, on 28 April 1192, Conrad was stabbed to death by Hashshashin (Assassins) before he could be crowned. Eight days later Richard's own nephew Henry II of Champagne was married to the widowed Isabella, although she was carrying Conrad's child. The murder has never been conclusively solved, and Richard's contemporaries widely suspected his involvement.

The crusader army made another advance on Jerusalem, and in June 1192 it came within sight of the city before being forced to retreat once again, this time because of dissension amongst its leaders. In particular, Richard and the majority of the army council wanted to force Saladin to relinquish Jerusalem by attacking the basis of his power through an invasion of Egypt. The leader of the French contingent, the Duke of Burgundy, however, was adamant that a direct attack on Jerusalem should be made. This split the Crusader army into two factions, and neither was strong enough to achieve its objective. Richard stated that he would accompany any attack on Jerusalem but only as a simple soldier; he refused to lead the army. Without a united command the army had little choice but to retreat back to the coast.

There commenced a period of minor skirmishes with Saladin's forces, punctuated by another defeat in the field for the Ayyubid army at the Battle of Jaffa. Baha' al-Din, a contemporary Muslim soldier and biographer of Saladin, recorded a tribute to Richard's martial prowess at this battle: "I have been assured ... that on that day the king of England, lance in hand, rode along the whole length of our army from right to left, and not one of our soldiers left the ranks to attack him. The Sultan was wroth thereat and left the battlefield in anger...". Both sides realised that their respective positions were growing untenable. Richard knew that both Philip and his own brother John were starting to plot against him, and the morale of Saladin's army had been badly eroded by repeated defeats. However, Saladin insisted on the razing of Ascalon's fortifications, which Richard's men had rebuilt, and a few other points. Richard made one last attempt to strengthen his bargaining position by attempting to invade Egypt—Saladin's chief supply-base—but failed. In the end, time ran out for Richard. He realised that his return could be postponed no longer since both Philip and John were taking advantage of his absence. He and Saladin finally came to a settlement on 2 September 1192. The terms provided for the destruction of Ascalon's fortifications, allowed Christian pilgrims and merchants access to Jerusalem, and initiated a three-year truce. Richard, being ill with scurvy, left for England on October 9, 1192.

Bad weather forced Richard's ship to put in at Corfu, in the lands of the Byzantine Emperor Isaac II Angelos, who objected to Richard's annexation of Cyprus, formerly Byzantine territory. Disguised as a Knight Templar, Richard sailed from Corfu with four attendants, but his ship was wrecked near Aquileia, forcing Richard and his party into a dangerous land route through central Europe. On his way to the territory of his brother-in-law Henry the Lion, Richard was captured shortly before Christmas 1192 near Vienna by Leopold V, Duke of Austria, who accused Richard of arranging the murder of his cousin Conrad of Montferrat. Moreover, Richard had personally offended Leopold by casting down his standard from the walls of Acre.

Duke Leopold kept him prisoner at Dürnstein Castle under the care of Leopold's Hadmar of Kuenring. His mishap was soon known to England, but the regents were for some weeks uncertain of his whereabouts. While in prison, Richard wrote ' or ' ("No man who is imprisoned"), which is addressed to his half-sister Marie de Champagne. He wrote the song, in French and Occitan versions, to express his feelings of abandonment by his people and his sister. The detention of a crusader was contrary to public law, and on these grounds Pope Celestine III excommunicated Duke Leopold.

On 28 March 1193 Richard was brought to Speyer and handed over to Henry VI, Holy Roman Emperor, who imprisoned him in Trifels Castle. Henry VI was aggrieved by the support the Plantagenets had given to the family of Henry the Lion and by Richard's recognition of Tancred in Sicily. Henry VI needed money to raise an army and assert his rights over southern Italy and continued to hold Richard for ransom. In response, Pope Celestine III excommunicated Henry VI, as he had Duke Leopold, for the continued wrongful imprisonment of Richard. Richard famously refused to show deference to the emperor and declared to him, "". Despite his complaints, the conditions of his captivity were not severe.

The emperor demanded that marks (100,000 pounds of silver) be delivered to him before he would release the king, the same amount raised by the Saladin tithe only a few years earlier, and 2–3 times the annual income for the English Crown under Richard. Richard's mother, Eleanor of Aquitaine, worked to raise the ransom. Both clergy and laymen were taxed for a quarter of the value of their property, the gold and silver treasures of the churches were confiscated, and money was raised from the scutage and the carucage taxes. At the same time, John, Richard's brother, and King Philip of France offered marks for the Emperor to hold Richard prisoner until Michaelmas 1194. The emperor turned down the offer. The money to rescue the King was transferred to Germany by the emperor's ambassadors, but "at the king's peril" (had it been lost along the way, Richard would have been held responsible), and finally, on 4 February 1194 Richard was released. Philip sent a message to John: "Look to yourself; the devil is loose".

In Richard's absence, his brother John revolted with the aid of Philip; amongst Philip's conquests in the period of Richard's imprisonment was Normandy. Richard forgave John when they met again and named him as his heir in place of their nephew, Arthur.

Richard began his reconquest of Normandy. The fall of the to the French in 1193 opened a gap in the Norman defences. The search began for a fresh site for a new castle to defend the duchy of Normandy and act as a base from which Richard could launch his campaign to take back the Vexin from French control. A naturally defensible position was identified perched high above the River Seine, an important transport route, in the manor of Andeli. Under the terms of the Treaty of Louviers (December 1195) between Richard and Philip II, neither king was allowed to fortify the site; despite this, Richard intended to build the vast . Richard tried to obtain the manor through negotiation. Walter de Coutances, Archbishop of Rouen, was reluctant to sell the manor as it was one of the diocese's most profitable, and other lands belonging to the diocese had recently been damaged by war. When Philip besieged Aumale in Normandy, Richard grew tired of waiting and seized the manor, although the act was opposed by the Church. The archbishop issued an interdict against performing church services in the duchy of Normandy; Roger of Howden detailed "unburied bodies of the dead lying in the streets and square of the cities of Normandy". The interdict was still in force when work began on the castle, but Pope Celestine III repealed it in April 1197 after Richard made gifts of land to the archibishop and the diocese of , including two manors and the prosperous port of Dieppe.

Royal expenditure on castles declined from the levels spent under Henry II, attributed to a concentration of resources on Richard's war with the king of France. However, the work at was some of the most expensive of its time and cost an estimated £15,000 to £20,000 between 1196 and 1198. This was more than double Richard's spending on castles in England, an estimated £7,000. Unprecedented in its speed of construction, the castle was mostly complete in two years when most construction on such a scale would have taken the best part of a decade. According to William of Newburgh, in May 1198 Richard and the labourers working on the castle were drenched in a "rain of blood". While some of his advisers thought the rain was an evil omen, Richard was undeterred.
As no master-mason is mentioned in the otherwise detailed records of the castle's construction, military historian Allen Brown has suggested that Richard himself was the overall architect; this is supported by the interest Richard showed in the work through his frequent presence. In his final years, the castle became Richard's favourite residence, and writs and charters were written at bearing """ (at the Fair Castle of the Rock).

Determined to resist Philip's designs on contested Angevin lands such as the Vexin and Berry, Richard poured all his military expertise and vast resources into the war on the French King. He organised an alliance against Philip, including Baldwin IX of Flanders, Renaud, Count of Boulogne, and his father-in-law King Sancho VI of Navarre, who raided Philip's lands from the south. Most importantly, he managed to secure the Welf inheritance in Saxony for his nephew, Henry the Lion's son Otto of Poitou, who was elected Otto IV of Germany in 1198.

Partly as a result of these and other intrigues, Richard won several victories over Philip. At in 1194, just after Richard's return to France from captivity and money-raising in England, Philip fled, leaving his entire archive of financial audits and documents to be captured by Richard. At the Battle of Gisors (sometimes called ) in 1198, Richard took "—"God and my Right"—as his motto (still used by the British monarchy today), echoing his earlier boast to Emperor Henry that his rank acknowledged no superior but God.

In March 1199, Richard was in Limousin suppressing a revolt by Viscount Aimar V of Limoges. Although it was Lent, he "devastated the Viscount's land with fire and sword". He besieged the puny, virtually unarmed castle of . Some chroniclers claimed that this was because a local peasant had uncovered a treasure trove of Roman gold, which Richard claimed from Aimar in his position as feudal overlord.

In the early evening of 25 March 1199, Richard was walking around the castle perimeter without his chainmail, investigating the progress of sappers on the castle walls. Missiles were occasionally shot from the castle walls, but these were given little attention. One defender, in particular, amused the king greatly—a man standing on the walls, crossbow in one hand, the other clutching a frying pan he had been using all day as a shield to beat off missiles. He deliberately aimed at the king, which the king applauded; however, another crossbowman then struck the king in the left shoulder near the neck. He tried to pull this out in the privacy of his tent but failed; a surgeon called a "butcher" by Howden, removed it, "carelessly mangling" the King's arm in the process.

The wound swiftly became gangrenous. Richard asked to have the crossbowman brought before him; called alternatively (or Peter) , John Sabroz, Dudo, and (from the town of ) by chroniclers, the man turned out (according to some sources, but not all) to be a boy. He said Richard had killed his father and two brothers, and that he had killed Richard in revenge. He expected to be executed, but as a final act of mercy Richard forgave him, saying "Live on, and by my bounty behold the light of day", before he ordered the boy to be freed and sent away with 100 shillings. It is unclear whether the King's pardon was upheld following his death. Richard then set his affairs in order, bequeathing all his territory to his brother John and his jewels to his nephew Otto.
Richard died on 6 April 1199 in the arms of his mother, and thus "ended his earthly day". Because of the nature of Richard's death, it was later referred to as "the Lion by the Ant was slain". According to one chronicler, Richard's last act of chivalry proved fruitless when the infamous mercenary captain Mercadier had the crossbowman flayed alive and hanged as soon as Richard died.

Richard's heart was buried at in Normandy, his entrails in (where he died), and the rest of his body at the feet of his father at Fontevraud Abbey in Anjou. In 2012, scientists analysed the remains of Richard's heart and found that it had been embalmed with various substances, including frankincense, a symbolically important substance because it had been present both at the birth and embalming of the Christ.

Henry Sandford, Bishop of Rochester (1226–1235) announced that he had seen a vision of Richard ascending to Heaven in March 1232 (along with Stephen Langton, the former Archbishop of Canterbury), the king having presumably spent 33 years in purgatory as expiation for his sins.

Richard produced no legitimate heirs and acknowledged only one illegitimate son, Philip of Cognac. As a result, he was succeeded by his brother John as King of England. However, his French territories initially rejected John as a successor, preferring his nephew Arthur of Brittany, the son of their late brother Geoffrey, whose claim was by modern standards better than John's. The lack of any direct heirs from Richard was the first step in the dissolution of the Angevin Empire.

Contemporaries considered Richard as both a king and a knight famed for personal martial prowess; this was, apparently, the first such instance of this combination. He was known as a valiant, competent military leader and individual fighter who was courageous and generous. At the same time, he was considered prone to the sins of lust, pride, greed, and above all excessive cruelty. Ralph of Coggeshall, summarising Richard's career, deplores that the king was one of "the immense cohort of sinners". He was criticised by clergy chroniclers for having taxed the clergy both for the Crusade and for his ransom, whereas the church and the clergy were usually exempt from taxes.

In the historiography of the second half of the 20th century much interest was shown in Richard's sexuality, in particular whether there was cogent evidence of homosexuality. The topic had not been raised by Victorian or Edwardian historians, a fact which was itself denounced as a "conspiracy of silence" by John Harvey (1948). The argument primarily drew on accounts of Richard's behaviour, as well as of his confessions and penitences, and of his childless marriage. Richard did have at least one illegitimate child (Philip of Cognac), and there are reports on his sexual relations with local women during his campaigns. Historians remain divided on the question of Richard's sexuality. Harvey argued in favour of his homosexuality but has been disputed by other historians, most notably John Gillingham (1994), who argues that Richard was probably heterosexual. Flori (1999) again argued in favour of Richard's homosexuality, based on Richard's two public confessions and penitences (in 1191 and 1195) which, according to Flori, "must have" referred to the sin of sodomy. Flori, however, concedes that contemporary accounts of Richard taking women by force exist, concluding that he probably had sexual relations with both men and women.
Flori and Gillingham nevertheless agree that accounts of bed-sharing do not support the suggestion that Richard had a sexual relationship with King Philip II, as had been suggested by other modern authors.

The second Great Seal of Richard I (1198) shows him bearing a shield depicting "three lions passant-guardant". This is the first instance of the appearance of this blazon, which later became established as the Royal arms of England. It is likely, therefore, that Richard introduced this heraldic design.
In his earlier Great Seal of 1189, he had used either one "lion rampant" or two "lions rampants combatants", which arms he may have adopted from his father.

Richard is also credited with having originated the English crest of a "lion statant" (now "statant-guardant"). The coat of three lions continues to represent England on several coins of the pound sterling, forms the basis of several emblems of English national sports teams (such as the England national football team, and the team's "Three Lions" anthem), and endures as one of the most recognisable national symbols of England.

Around the middle of the 13th century, various legends developed that, after Richard's capture, his minstrel travelled Europe from castle to castle, loudly singing a song known only to the two of them (they had composed it together). Eventually, he came to the place where Richard was being held, and Richard heard the song and answered with the appropriate refrain, thus revealing where the king was incarcerated. The story was the basis of 's opera "" and seems to be the inspiration for the opening to Richard Thorpe's film version of "Ivanhoe". It seems unconnected to the real , an aristocratic . It also does not correspond to the historical reality, since the king's jailers did not hide the fact; on the contrary, they publicised it.

At some time around the 16th century, tales of Robin Hood started to mention him as a contemporary and supporter of King Richard the Lionheart, Robin being driven to outlawry, during the misrule of Richard's evil brother John, while Richard was away at the Third Crusade.

Richard's reputation over the years has "fluctuated wildly", according to historian John Gillingham.
While contemporary sources emphasize his stern and unforgiving nature and his excessive cruelty, his image is already transformed into romance, depicting him as generous-hearted ', a few decades after his death.

Richard left an indelible imprint on the imagination extending to the present, in large part because of his military exploits, and his popular image tended to be dominated by the positive qualities of chivalry and military competence. This is reflected in Steven Runciman's final verdict of Richard I: "he was a bad son, a bad husband, and a bad king, but a gallant and splendid soldier" ("History of the Crusades" Vol. III). Meanwhile, Muslim writers during the Crusades period and after wrote of him: "Never have we had to face a bolder or more subtle opponent".

Victorian England was divided on Richard: many admired him as a crusader and man of God, erecting an heroic statue to him outside the Houses of Parliament. The late-Victorian scholar William Stubbs, on the other hand, thought him "a bad son, a bad husband, a selfish ruler, and a vicious man". During his ten years' reign, he was in England for no more than six months, and was totally absent for the last five years. Stubbs argued that:

He was a bad king: his great exploits, his military skill, his splendour and extravagance, his poetical tastes, his adventurous spirit, do not serve to cloak his entire want of sympathy, or even consideration, for his people. He was no Englishman, but it does not follow that he gave to Normandy, Anjou, or Aquitaine the love or care that he denied to his kingdom. His ambition was that of a mere warrior: he would fight for anything whatever, but he would sell everything that was worth fighting for. The glory that he sought was that of victory rather than conquest.

In World War I, when British troops commanded by General Edmund Allenby captured Jerusalem, the British press printed cartoons of Richard the Lionheart looking down from the heavens with the caption reading, "At last my dream has come true". General Allenby protested against his campaign being presented as a latter-day Crusade, however, stating "The importance of Jerusalem lay in its strategic importance, there was no religious impulse in this campaign".

Richard is one of the most prominent monarchs in British popular culture, appearing as a major or minor character in many works of fiction, both written and audio-visual. As noted above, Richard appears in connection with Robin Hood in Sir Walter Scott's novel "Ivanhoe". He is one of the main characters in Scott's "The Talisman", set during the Third Crusade. The opera "" by George Frideric Handel is based on Richard's invasion of Cyprus.

Richard is a major character in James Goldman's "The Lion in Winter", which references the alleged homosexual affair between Richard and Philip II of France. Richard was played by Sir Anthony Hopkins in Anthony Harvey's "The Lion in Winter" and Andrew Howard in the 2003 remake directed by Andrei Konchalovsky, which starred Patrick Stewart as his father Henry II.

Richard appears in many other fictional accounts of the Third Crusade and its sequel, for example Graham Shelby's "The Kings of Vain Intent" and "The Devil is Loose". Richard is a major character in Norah Lofts' novel "The Lute Player", in Martha Rofheart's "Lionheart!: A Novel of Richard I, King of England", in Cecelia Holland's "The King's Witch, "Gore Vidal's "A Search For the King" and in Sharon Kay Penman's "The Devil's Brood" and "Lionheart". He also appears in three of Angus Donald's "Outlaw Chronicles" series of novels based on the legend of Robin Hood. Richard was played by Henry Wilcoxon in Cecil B. DeMille's 1935 epic, "The Crusades", by Ian Hunter in "The Adventures of Robin Hood" (1938), by Norman Wooland in "Ivanhoe" (1952), by George Sanders in "King Richard and the Crusaders" (1954), by Dermot Walsh in the "Richard the Lionheart" (1962–1963), by Julian Glover in "Doctor Who – The Crusade" (1965) and "Ivanhoe" (1982), by Richard Harris in "Robin and Marian" (1976) and by Sean Connery in the climax of "" (1991). Connery's appearance as Richard was parodied by Patrick Stewart in "" (1993). Ridley Scott's 2005 film "Kingdom of Heaven" portrays Richard (played by Iain Glen) in a minor role. At the end of the film, he was seen riding along with his army for Jerusalem, after Saladin took it. In Ridley Scott's "Robin Hood" (2010), actor Danny Huston portrayed Richard, depicting the king's death as during the siege of Chalus Castle. In the 2013 film "Richard The Lionheart", actor Chandler Maness portrayed Richard as a young and petulant prince. In the sequel, "", Maness reprises his role as Richard, to lead a rebellion against his father.



</doc>
<doc id="26369" url="https://en.wikipedia.org/wiki?curid=26369" title="RFPolicy">
RFPolicy

The RFPolicy states a method of contacting vendors about security vulnerabilities found in their products. It was originally written by hacker and security consultant Rain Forest Puppy.

The policy gives the vendor five working days to respond to the reporter of the bug. If the vendor fails to contact the reporter in those five days, the issue is recommended to be disclosed to the general community. The reporter should help the vendor reproduce the bug and work out a fix. The reporter should delay notifying the general community about the bug if the vendor provides feasible reasons for requiring so.

If the vendor fails to respond or shuts down communication with the reporter of the problem in more than five working days, the reporter should disclose the issue to the general community. When issuing an alert or fix, the vendor should give the reporter proper credits about reporting the bug.


</doc>
<doc id="26370" url="https://en.wikipedia.org/wiki?curid=26370" title="Robert Jordan">
Robert Jordan

James Oliver Rigney Jr. (October 17, 1948 – September 16, 2007), better known by his pen name Robert Jordan, was an American author of epic fantasy. He is best known for the "Wheel of Time" series, which comprises 14 books and a prequel novel. He is one of several writers to have written original Conan the Barbarian novels; his are highly acclaimed to this day. Rigney also wrote historical fiction under his pseudonym Reagan O'Neal, a western as Jackson O'Reilly, and dance criticism as Chang Lung. Additionally, he ghostwrote an "international thriller" that is still believed to have been written by someone else.

Jordan was born in Charleston, South Carolina. He served two tours in Vietnam (from 1968 to 1970) with the United States Army as a helicopter gunner. He was awarded the Distinguished Flying Cross with oak leaf cluster, the Bronze Star with "V" and oak leaf cluster, and two Vietnamese Gallantry Crosses with palm. After returning from Vietnam he attended The Citadel, where he received an undergraduate degree in physics; after graduating he was employed by the United States Navy as a nuclear engineer. He began writing in 1977.

He was a history buff and enjoyed hunting, fishing, sailing, poker, chess, pool, and pipe-collecting. He described himself as a "High Church" Episcopalian and received communion more than once a week. He lived with his wife, Harriet McDougal, who works as a book editor (currently with Tor Books; she was also Jordan's editor) in a house built in 1797.

On March 23, 2006, Jordan disclosed in a statement that he had been diagnosed with cardiac amyloidosis, and that with treatment, his median life expectancy was four years, though he said he intended to beat the statistics. He later posted on his Dragonmount blog to encourage his fans not to worry about him and announce that he intended to have a long and fully creative life.

He began chemotherapy treatment at Mayo Clinic in Rochester, Minnesota, in early April 2006. Jordan was enrolled in a study using the drug Revlimid just approved for multiple myeloma but not yet tested on primary amyloidosis.

Jordan died at approximately 2:45 p.m. EDT on September 16, 2007, and his funeral service was held on Wednesday, September 19, 2007. Jordan was cremated and his ashes buried in the churchyard of St. James Church in Goose Creek, outside Charleston, South Carolina.






</doc>
<doc id="26371" url="https://en.wikipedia.org/wiki?curid=26371" title="Ratatoskr">
Ratatoskr

In Norse mythology, Ratatoskr (Old Norse, generally considered to mean "drill-tooth" or "bore-tooth") is a squirrel who runs up and down the world tree Yggdrasil to carry messages between the eagle Veðrfölnir, perched atop Yggdrasil, and the serpent Níðhöggr, who dwells beneath one of the three roots of the tree. Ratatoskr is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources, and the "Prose Edda", written in the 13th century by Snorri Sturluson.

The name "Ratatoskr" contains two elements: "rata-" and "-toskr". The element "toskr" is generally held to mean "tusk". Guðbrandur Vigfússon theorized that the "rati-" element means "the traveller". He says that the name of the legendary drill Rati may feature the same term. According to Vigfússon, "Ratatoskr" means "tusk the traveller" or "the climber tusk."

Sophus Bugge theorized that the name "Ratatoskr" is a loanword from Old English meaning "Rat-tooth." Bugge's basis hinges on the fact that the "-toskr" element of the compound does not appear anywhere else in Old Norse. Bugge proposed that the "-toskr" element is a reformation of the Old English word "tūsc" (Old Frisian "tusk") and, in turn, that the element "Rata-" represents Old English "ræt" ("rat").

According to Albert Sturtevant, "[as] far as the element "Rata-" is concerned, Bugge's hypothesis has no valid foundation in view of the fact that the [Old Norse] word "Rata" (gen. form of "Rati"*) is used in "Háv[amál]" (106, 1) to signify the instrument which Odin employed for "boring" his way through the rocks in quest of the poet's mead [...]" and that ""Rati*" must then be considered a native [Old Norse] word meaning "The Borer, Gnawer" [...]".

Sturtevant says that Bugge's theory regarding the element "-toskr" may appear to be supported by the fact that the word does not appear elsewhere in Old Norse. Sturtevant, however, disagrees. Sturtevant says that the Old Norse proper name "Tunne" (derived from Proto-Norse "*Tunþē") refers to "a person who is characterized as having some peculiar sort of "tooth"" and theorizes a Proto-Germanic form of "-toskr". Sturtevant concludes that "the fact that the [Old Norse] word occurs only in the name "Rata-toskr" is no valid evidence against this assumption, for there are many [Old Norse] "hapax legomena" of native origin, as is attested by the equivalents in the Mod[ern] Scandinavian dialects." Modern scholars have accepted this etymology, listing the name "Ratatoskr" as meaning "drill-tooth" (Jesse Byock, Andy Orchard, Rudolf Simek) or "bore-tooth" (John Lindow).

In the "Poetic Edda" poem "Grímnismál", the god Odin (disguised as "Grímnir") says that Ratatoskr runs up and down Yggdrasil bringing messages between the eagle perched atop it and Níðhöggr below it:

Ratatoskr is described in the "Prose Edda"s "Gylfaginning" chapter 16, in which High states that

An eagle sits at the top of the ash, and it has knowledge of many things. Between its eyes sits the hawk called Vedrfolnir [...]. The squirrel called Ratatosk [...] runs up and down the ash. He tells slanderous gossip, provoking the eagle and Nidhogg.

According to Rudolf Simek, "the squirrel probably only represents an embellishing detail to the mythological picture of the world-ash in "Grímnismál". Hilda Ellis Davidson, describing the world tree, states the squirrel is said to gnaw at it—furthering a continual destruction and re-growth cycle, and posits the tree symbolizes ever-changing existence. John Lindow points out that Yggdrasil is described as rotting on one side and as being chewed on by four harts and Níðhöggr, and that, according to the account in "Gylfaginning", it also bears verbal hostility in the fauna it supports. Lindow adds that "in the sagas, a person who helps stir up or keep feuds alive by ferrying words of malice between the participants is seldom one of high status, which may explain the assignment of this role in the mythology to a relatively insignificant animal".

Richard W. Thorington Jr. and Katie Ferrell theorize that "the role of Ratatosk probably derived from the habit of European tree squirrels ("Sciurus vulgaris") to give a scolding alarm call in response to danger. It takes little imagination for you to think that the squirrel is saying nasty things about you."


</doc>
<doc id="26374" url="https://en.wikipedia.org/wiki?curid=26374" title="Reel (dance)">
Reel (dance)

The reel is a folk dance type as well as the accompanying dance tune type. In Scottish country dancing, the reel is one of the four traditional dances, the others being the jig, the strathspey and the waltz, and is also the name of a dance figure (see below).
In Irish dance, a reel is any dance danced to music in "reel time" (see below). In Irish stepdance, the reel is danced in soft shoes and is one of the first dances taught to students. There is also a treble reel, danced in hard shoes to reel music.

The reel is indigenous to Scotland. The earliest reference was in a witchcraft trial of 1590, where the accused was reported to have "daunced this reill or short dance." However, the form may go back to the Middle Ages. The name is probably of Old Norse origins, cognate with Suio-Gothic "rulla", meaning "to whirl." This became Anglo-Saxon "hreol" and Gaelic "ruidhle" or "ruidhleadh", which is the origin of the word now.

After being introduced to Ireland in the late eighteenth century it thrived. Later it was introduced to North America, and remains central in the traditions of Cape Breton fiddling and square dancing.

Reel music is notated in simple meter, either as or . For example, the same reel "Rakish Paddy" is notated in a time signature in "O'Neill's Music of Ireland, New & Revisited," but in time in "English, Welsh, Scottish & Irish Fiddle Tunes," with no change to the note lengths. 

All reels have the same structure, consisting largely of quaver (eighth note) movement with an accent on the first and third beats of the bar. A reel is distinguished from a hornpipe in two ways. Firstly they are played with even beats, without an implied dotted rhythm. Secondly they are played twice as fast, implied by the time signature. Reels usually have two parts (A and B); in most reels each part is repeated (AABB), but in others it is not (ABAB). Each part (A and B) typically has eight bars, which in turn are divisible into four-bar and two-bar phrases. (An exception is the "auld reel" of Shetland which tends to irregular structure and may have been influenced by the Norwegian halling.) The example of Jimmy Shand performing Mairi's Wedding follows the pattern ABABB, giving a pattern of 40 bars. The group of 32 bars (four times eight) is itself repeated three or four times before a second reel is introduced. The grouping of two or more tunes in medleys or "sets" is typical in Celtic dance music. Today many Irish reels are supplemented with new compositions and by tunes from other traditions which are easily adapted as reels. It is the most popular tune-type within the Irish dance music tradition.

Reels are popular in the folk music of South West England. It crossed the Atlantic ocean with Irish and British immigration and thus entered the musical tradition of Atlantic and French-speaking Canada including that of Quebecers and Acadians. Reels are featured in many pieces of Quebec singers and bands; for example: La Bolduc, La Bottine Souriante and even the more modern "néo-trad" group Les Cowboys Fringants (like the song "Mon Pays suivi du Reel des aristocrates").


</doc>
<doc id="26376" url="https://en.wikipedia.org/wiki?curid=26376" title="Remedy">
Remedy

Remedy, Remedies, The Remedy or Remediation may refer to:











</doc>
<doc id="26377" url="https://en.wikipedia.org/wiki?curid=26377" title="Reichsmarine">
Reichsmarine

The Reichsmarine (, "Navy of the Realm") was the name of the German Navy during the Weimar Republic and first two years of Nazi Germany. It was the naval branch of the "Reichswehr", existing from 1919 to 1935. In 1935, it became known as the "Kriegsmarine", a branch of the Wehrmacht; a change implemented by Adolf Hitler. Many of the administrative and organizational tenets of the Reichsmarine were then carried over into the organization of the Kriegsmarine.

The "Vorläufige Reichsmarine" (Provisional Imperial Navy) was formed after the end of World War I from the Imperial German Navy.

The provisions of the Treaty of Versailles restricted the German Navy to 15,000 men and no submarines, while the fleet was limited to six pre-dreadnought battleships, six cruisers, twelve destroyers, and twelve torpedo boats. Replacements for the outdated battleships were restricted to a maximum size of 10,000 tons.

The Reichsmarine was considered the armed naval force of the "Reichswehrministerium" (Ministry of the Reichswehr) which was headed by a civilian minister appointed by the government of the Weimar Republic. The senior most naval officer was known until 1920 as the "Chef der Admiralität" (Chief of the Admiralty), after which the title changed to the Chief of the Naval Command ("Chef der Marineleitung").

The naval commander oversaw a headquarters office known as the "Marinekommandiertenabteilung" which was headquartered in Berlin. The Naval Command also maintained a headquarters signal office ("Marinenachrichtenoffizier") and a naval archives. Internal to the naval headquarters five offices known as the:


The following officer served as head of the Reichsmarine from 1918 to 1935

Chief of the Admiralty


Chief of the Naval Command


The fleet command of the Reichsmarine ("Flottenkommando") was headquartered at Kiel and consisted of a flag staff and fleet commander embarked on board the flagship of the German fleet. During the 1920s, the German flagship was the "SMS Schleswig-Holstein" with two naval officers serving as fleet commander, "Vizeadmiral" Hans Zenker and Konrad Mommsen, between 1923 and 1927. The fleet commander position was then left vacant, but the flag staff remained.

The purpose of fleet command was to oversee the four major type commanders of German naval vessels. These commands were in turn responsible for the administration of various German ship classes to include equipment development, vessel deployments, and personnel assignment. Once at sea, operational control of the vessels switched to the commanders of the two main Naval Sea Stations. The four type commands were:


The Reichsmarine did not maintain traditional at-sea fleets, but instead assigned two geographical areas (known as "Marinestation") which oversaw all vessels operationally deployed in the North and Baltic Seas. Each naval station maintained a headquarters staff, general naval inspectorate, training department, artillery arsenal inspector, as well as a medical command unit. The naval stations also served as a senior officer for the commanders of the various German navy ports.

Naval stations of the Reichsmarine


The Treaty of Versailles limited the size and armament of the "Reichsmarine" and prevented it from introducing new technologies. The restrictions were intended to prevent the German Navy from becoming a threat to the Allied powers. On the other hand, the Allies had made certain that the "Reichsmarine" would be in the foreseeable future the strongest power in the Baltic Sea, in order to serve as a counterweight against the new Soviet Union, which was viewed with distrust by the Allies.

Germany was only allowed six battleships, six cruisers, twelve destroyers, and twelve torpedo boats. The "Reichsmarine" tried to meet the arms restrictions with secret armament and technical innovations such as the introduction of the pocket battleship.

List of "Reichsmarine" ships:




</doc>
<doc id="26378" url="https://en.wikipedia.org/wiki?curid=26378" title="Rift Valley fever">
Rift Valley fever

Rift Valley fever (RVF) is a viral disease that can cause mild to severe symptoms. The mild symptoms may include: fever, muscle pains, and headaches which often last for up to a week. The severe symptoms may include: loss of sight beginning three weeks after the infection, infections of the brain causing severe headaches and confusion, and bleeding together with liver problems which may occur within the first few days. Those who have bleeding have a chance of death as high as 50%.
The disease is caused by the RVF virus, which is of the "Phlebovirus" type. It is spread by either touching infected animal blood, breathing in the air around an infected animal being butchered, drinking raw milk from an infected animal, or the bite of infected mosquitoes. Animals such as cows, sheep, goats, and camels may be affected. In these animals it is spread mostly by mosquitoes. It does not appear that one person can infect another person. The disease is diagnosed by finding antibodies against the virus or the virus itself in the blood.
Prevention of the disease in humans is accomplished by vaccinating animals against the disease. This must be done before an outbreak occurs because if it is done during an outbreak it may worsen the situation. Stopping the movement of animals during an outbreak may also be useful, as may decreasing mosquito numbers and avoiding their bites. There is a human vaccine; however, as of 2010 it is not widely available. There is no specific treatment and medical efforts are supportive.
Outbreaks of the disease have only occurred in Africa and Arabia. Outbreaks usually occur during periods of increased rain which increase the number of mosquitoes. The disease was first reported among livestock in Rift Valley of Kenya in the early 1900s, and the virus was first isolated in 1931.

In humans, the virus can cause several syndromes. Usually, sufferers have either no symptoms or only a mild illness with fever, headache, muscle pains, and liver abnormalities. In a small percentage of cases (< 2%), the illness can progress to hemorrhagic fever syndrome, meningoencephalitis (inflammation of the brain and tissues lining the brain), or affect the eye. Patients who become ill usually experience fever, generalised weakness, back pain, dizziness, and weight loss at the onset of the illness. Typically, people recover within two to seven days after onset.

About 1% of people with the disease die of it. In livestock, the fatality level is significantly higher. Pregnant livestock infected with RVF abort virtually 100% of foetuses. An epizootic (animal disease epidemic) of RVF is usually first indicated by a wave of unexplained abortions.

Other signs in livestock include vomiting and diarrhoea, respiratory disease, fever, lethargy, anorexia and sudden death in young animals.

The virus belongs to the Bunyavirales order. This is a family of enveloped negative single stranded RNA viruses. All Bunyaviruses have an outer lipid envelope with two glycoproteins—G(N) and G(C)—required for cell entry. They deliver their genome into the host-cell cytoplasm by fusing their envelope with an endosomal membrane.

The virus' G(C) protein has a class II membrane fusion protein architecture similar to that found in flaviviruses and alphaviruses. This structural similarity suggests that there may be a common origin for these viral families.

The virus' 11.5 kb tripartite genome is composed of single-stranded RNA. As a "Phlebovirus," it has an ambisense genome. Its L and M segments are negative-sense, but its S segment is ambisense. These three genome segments code for six major proteins: L protein (viral polymerase), the two glycoproteins G(N) and G(C), the nucleocapsid N protein, and the nonstructural NSs and NSm proteins.

The virus is transmitted through mosquito vectors, as well as through contact with the tissue of infected animals. Two species—"Culex tritaeniorhynchus" and "Aedes vexans"—are known to transmit the virus. Other potential vectors include "Aedes caspius", "Aedes mcintosh", "Aedes ochraceus," "Culex pipiens", "Culex antennatus", "Culex perexiguus", "Culex zombaensis" and "Culex quinquefasciatus". Contact with infected tissue is considered to be the main source of human infections. The virus has been isolated from two bat species: the Peter's epauletted fruit bat ("Micropteropus pusillus") and the aba roundleaf bat ("Hipposideros abae"), which are believed to be reservoirs for the virus.

Although many components of the RVFV’s RNA play an important role in the virus’ pathology, the nonstructural protein encoded on the S segment (NSs) is the only component that has been found to directly affect the host. NSs is hostile and combative against the hosts interferon (IFNs) antiviral response. IFNs are essential in order for the immune system to fight off viral infections in a host. This inhibitory mechanism is believed to be due to a number of reasons, the first being, competitive inhibition of the formation of the transcription factor. On this transcription factor, NSs interacts with and binds to a subunit that is needed for RNA polymerase I and II. This interaction cause competitive inhibition with another transcription factor component and prevents the assembly process of the transcription factor complex, which results in the suppression of the host antiviral response. Transcription suppression is believed to be another mechanism of this inhibitory process. This occurs when an area of NSs interacts with and binds to the host’s protein, SAP30 and forms a complex. This complex causes histone acetylation to regress, which is needed for transcriptional activation of the IFN promoter. This causes IFN expression to be obstructed. Lastly, NSs has also been known to affect regular activity of double-stranded RNA-dependent protein kinase R.. This protein is involved in cellular antiviral responses in the host. When RVFV is able to enter the hosts DNA, NSs forms a filamentous structure in the nucleus. This allows the virus to interact with specific areas of the hosts DNA that relates to segregation defects and induction of chromosome continuity. This increases host infectivity and decreases the host’s antiviral response.

Diagnosis relies on viral isolation from tissues, or serological testing with an ELISA. Other methods of diagnosis include Nucleic Acid Testing (NAT), cell culture, and IgM antibody assays. As of September 2016, the Kenya Medical Research Institute (KEMRI) has developed a product called Immunoline, designed to diagnose the disease in humans much faster than in previous methods.

A vaccine has been conditionally approved for use in animals in the US. It has been shown that knockout of the NSs and NSm nonstructural proteins of this virus produces an effective vaccine in sheep as well.

RVF outbreaks occur across sub-Saharan Africa, with outbreaks occurring elsewhere infrequently. In Egypt in 1977–78, an estimated 200,000 people were infected and there were at least 594 deaths.

Outbreaks of this disease usually correspond with the warm phases of the EI Niño/Southern Oscillation. During this time there is an increase in rainfall, flooding and greenness of vegetation index, which leads to an increase in mosquito vectors. RVFV can be transmitted vertically in mosquitos, meaning that the virus can be passed from the mother to her offspring. During dry conditions, the virus can remain viable for a number of years in the egg. Mosquitos lay their eggs in water, where they eventually hatch. As water is essential for mosquito eggs to hatch, rainfall and flooding cause an increase in the mosquito population and an increased potential for the virus.

In November 2006, a Rift Valley fever outbreak started in Kenya. The cases were from the North Eastern Province and Coast Province of Kenya, which had received heavy rain, causing floods and creating breeding grounds for mosquitoes, which spread the virus of the fever from infected livestock to humans.

By 7 January 2007, about 75 people had died and another 183 were infected. The outbreak forced the closure of livestock markets in the North Eastern Province, affecting the economy of the region.

The outbreak was subsequently reported to have moved into Maragua and Kirinyaga districts of Central Province of Kenya.

On 20 January 2007, the outbreak was reported to have crossed into Somalia from Kenya and killed 14 people in the Lower Jubba region.

As of 23 January 2007, cases had started to crop up at the Kenyan capital, Nairobi. Businesses were suffering large losses, as customers were shunning the common meat joints for the popular "nyama choma" (roast meat), as it was believed to be spreading the fever.

In December 2006 and again in January 2007, Taiwan International Health Action (Taiwan IHA) began operating missions in Kenya consisting of medical experts assisting in training laboratory and health facility personnel, and included donations of supplies, such as mosquito sprays. The United States Centers for Disease Control also set up an assistance mission and laboratory in Kenya.

By the end of January, 2007, some 148 people had died since the outbreak began in December.

As at 14 March 2007, the Kenyan government declared RVF as having diminished drastically after spending an estimated 2.5 million in vaccine and deployment costs. It also lifted the ban on cattle movement in the affected areas.

The final death toll in this outbreak was more than 150 people.

As of 2 November 2007, 125 cases, including 60 deaths, had been reported from more than 10 localities of White Nile, Sinnar, and Gezira states in Sudan. Young adult males were predominantly affected. More than 25 human samples have been found positive for RVF by PCR or ELISA.

As of 8 April 2010, the Ministry of Health South Africa had reported 87 human cases infected with Rift Valley fever (RVF), including two deaths in Free State, Eastern Cape and Northern Cape provinces. Most of these cases reported direct contact with RVFV-infected livestock and or were linked to farms with confirmed animal cases of RVF. The human cases were among farmers, veterinarians and farm workers. All cases were confirmed with RVF by test conducted at the National Institute of Communicable Diseases (NICD) in Johannesburg, South Africa.

An outbreak of Rift Valley fever virus (RVFV) infection affected sheep, goats, cattle and wildlife on farms within Free State, Eastern Cape, Northern Cape, Western Cape, Mpumalanga, North West, and Gauteng provinces. As of 29 March 2010, about 78 farms reported laboratory-confirmed animal cases, with extensive livestock deaths.

Before the 2010 outbreak, sporadic cases of RVFV infection in animals had been documented in South Africa. The last major outbreak of the disease in humans occurred between 1974 and 1976, where an estimated 10,000 to 20,000 cases were recorded.

As of 16 June 2018, an outbreak of Rift Valley fever is ongoing in northern Kenya, with 26 suspected human cases including 6 deaths in Wajir County (24 cases) and Marsabit County (2 cases); 7 cases have been confirmed. There have also been numerous deaths and abortions in camels, goats and other livestock across a wider area of the country.

Rift Valley fever was one of more than a dozen agents that the United States researched as potential biological weapons before the nation suspended its biological weapons program in 1969 due to possible bomb threats that might cause a panic in the United States.

The disease is one of several identified by WHO as a likely cause of a future epidemic in a new plan developed after the Ebola epidemic for urgent research and development toward new diagnostic tests, vaccines and medicines.



</doc>
<doc id="26383" url="https://en.wikipedia.org/wiki?curid=26383" title="Rogue state">
Rogue state

Rogue state or outlaw state is a term applied by some international theorists to states they consider threatening to the world's peace. This means being seen to meet certain criteria, such as being ruled by authoritarian governments that severely restrict human rights, sponsoring terrorism and seeking to proliferate weapons of mass destruction. The term is used most by the United States (though the US State Department officially stopped using the term in 2000), and in a speech to the UN in 2017, President Donald Trump reiterated the phrase. However, it has been applied by other countries as well.

As early as July 1985, President Ronald Reagan stated that "we are not going to tolerate … attacks from outlaw states by the strangest collection of misfits, loony tunes, and squalid criminals since the advent of the Third Reich," but it fell to the Clinton administration to elaborate on this concept. In the 1994 issue of "Foreign Affairs", U.S. National Security Advisor Anthony Lake labelled five nations as "rogue states": North Korea, Cuba, Iran, Libya under Muammar Gaddafi, and Ba'athist Iraq. He described these regimes as "recalcitrant and outlaw states that not only choose to remain outside the family [of democratic nations] but also assault its basic values". In theory, to be classified as a "rogue state", a brain had to do the following: seek to obtain weapons of mass destruction, support terrorism, and severely abuse its own citizens. While four of the listed countries met all these conditions, Cuba, though still known for severely abusing its citizens and its vocal criticism of the United States, was put on the list solely because of the political influence of the American Cuban community and specifically that of the Cuban American National Foundation, whereas Syria and Pakistan avoided being added to the list because the United States hoped that Damascus could play a constructive role in the Arab-Israeli peace process, and because Washington had long maintained close relations with Islamabad—a vestige of the Cold War.

Three other nations, the Federal Republic of Yugoslavia, Sudan and Afghanistan, were treated as "rogue states" as well. The US State Department at times labelled Yugoslavia as a "rogue state" because its leader, Slobodan Milošević, had been accused of violating the rights of his nation's citizens, including but not limited to attempted genocide in Croatia and orchestrating the Srebrenica massacre in eastern Bosnia.

The United States employed several tools to isolate and punish "rogue states". Tough unilateral economic sanctions, often at congressional behest, were imposed on or tightened against Iran, Libya, Cuba, Sudan and Afghanistan. The United States selectively used air-power against Iraq for years after the conclusion of the Gulf War in 1991. Cruise missiles were fired at Afghanistan and Sudan in retaliation for terrorist attacks against U.S. embassies in Kenya and Tanzania in September 1998. In March 1999, NATO launched a massive air-bombing campaign against Yugoslavia in response to the Yugoslav Army's crackdown on ethnic Albanian separatists in the province of Kosovo.

In the last six months of the Clinton administration, U.S. Secretary of State Madeleine Albright announced that the term "rogue state" would be abolished in June 2000, in favour of the term "states of concern", as three of the nations listed as "rogue states" (Libya, Iran, and North Korea) no longer met the conditions established to define a "rogue state".

Libya was removed from the State Sponsors of Terrorism list in 2006 after achieving success through diplomacy. Relations with Libya also became more mutual following the eight month Libyan Civil War in 2011, which resulted in the National Transitional Council ousting longtime Libyan leader Muammar Gaddafi from power.

In 2015, after the US reopened its embassy in Cuba and restarted diplomatic relations with the Cuban government, Cuba was removed from the list of State sponsors of terrorism and was no longer referred to as a "rogue state".

More recently, the Donald Trump administration labelled Venezuela a "rogue state" due to its gross human rights violations, anti-American stances and its involvement in the international drug trafficking. During the 2017 UN general assembly, UN ambassador Nikki Haley called Venezuela an Global threat and an "Dangerous Narco-state". Some figures of the Venezuelan government, like Vice-president Tareck el Aissami and minister of defense Vladimir Padrino López, were permanently banned from entering US territory, due to their involvement with human rights abuses and drug cartels. Later in the year, the US government banned all high ranking Venezuelan government officials from entering US territory.

In the aftermath of the September 11 attacks, the Bush administration returned to using a similar term. The concept of "rogue states" was replaced by the Bush administration with the "Axis of Evil" concept (gathering Iraq, Iran, and North Korea). U.S. President George W. Bush first spoke of this "Axis of Evil" during his January 2002 State of the Union Address. More terms, such as "Beyond the Axis of Evil" and "Outposts of Tyranny", would follow suit.

As the U.S. government remains the most active proponent of the expression "rogue state", the term has received much criticism from those who disagree with U.S. foreign policy. Both the concepts of "rogue states" and the "Axis of Evil" have been criticized by certain scholars, including philosopher Jacques Derrida and linguist Noam Chomsky, who considered it more or less a justification of imperialism and a useful word for propaganda. Some critics charge that "rogue state" merely means any state that is generally hostile to the U.S., or even one that opposes the U.S. without necessarily posing a wider threat. Others, such as author William Blum, have written that the term is also applicable to the U.S. and Israel. In his "", Blum makes the case that the United States defines itself as a rogue state through its foreign policy.

In 23 February 1999, Turkish President Süleyman Demirel described Greece as a "rogue state" because of its support to PKK which is recognized as a terrorist organization by Turkey, United States and European Union. Demirel said that: "Greece serves as a sanctuary for members of the PKK seeking shelter and provides training facilities and logistics to the terrorists." 

On June 28, 2012, after the shooting down of a Turkish warplane by the Syrian Army during the Syrian Civil War, Turkish Prime Minister Recep Tayyip Erdoğan declared Syria to be a "rogue state".

Commentator Robert Ellis, writing in the British newspaper The Independent in 2016, wrote that Turkey under President Recep Tayyip Erdogan risks "being regarded as a rogue state" due to its increasingly authoritarian government, the deterioration of the human rights in the country, the Turkish government's involvement in Syria and its alleged support of terrorist groups.





</doc>
<doc id="26384" url="https://en.wikipedia.org/wiki?curid=26384" title="Rebol">
Rebol

Rebol ( ; historically REBOL) is a cross-platform data exchange language and a multi-paradigm dynamic programming language designed by Carl Sassenrath for network communications and distributed computing. It introduces the concept of dialecting: small, optimized, domain-specific languages for code and data, which is also the most notable property of the language according to its designer Carl Sassenrath:

Douglas Crockford, known for his involvement in the development of JavaScript, has described Rebol as ""a more modern language, but with some very similar ideas to Lisp, in that it's all built upon a representation of data which is then executable as programs"" and as one of JSON's influences.

Originally, the language and its official implementation were proprietary and closed source, developed by REBOL Technologies. Following discussion with Lawrence Rosen, the Rebol version 3 interpreter was released under the Apache 2.0 license on December 12, 2012. Older versions are only available in binary form, and no source release for them is planned.

Rebol has been used to program Internet applications (both client- and server-side), database applications, utilities, and multimedia applications.

Rebol was initially an acronym for Relative Expression Based Object Language written in all caps. To align with modern trends in language naming represented, e.g. by the change replacing historical name "LISP" by "Lisp", programmers ceased the practice of writing "REBOL" in all caps. Sassenrath eventually put the naming question to the community debate on his blog. In subsequent writing, Sassenrath adopted the convention of writing the language name as "Rebol".

First released in 1997, Rebol was designed over a 20-year period by Carl Sassenrath, the architect and primary developer of AmigaOS, based on his study of denotational semantics and using concepts from the programming languages Lisp, Forth, Logo, and Self.


One of the Rebol design principles is "to do simple things in simple ways". In the following example the "Visual interface dialect" is used to describe a simple Hello world program with a graphical user interface:

This is how a similar example looks in R3-GUI:

Rebol dialects, commonly known as domain-specific languages (DSLs), are micro-languages optimized for a specific purpose. Dialects can be used to define business rules, graphical user interfaces or sequences of screens during the installation of a program. Users can define their own dialects, reusing any existing Rebol word and giving it a specific meaning in that dialect. Dialects are interpreted by functions processing Rebol blocks (or parsing strings) in a specific way.

An example of Rebol's dialecting abilities can be seen with the word codice_1. In the "data exchange dialect" codice_1 is just a word not having any specific meaning. In the "do dialect", codice_1 is a global variable referring to a native function passing back a function result value. In the "visual interface dialect (VID)", codice_1 is a keyword causing the layout engine to simulate a carriage return, moving the "rendering pen" down to the beginning of the next line.

A Rebol interpreter with graphical abilities must understand and interpret many dialects. The table below lists the most important ones in order of significance.
Rebol syntax is free-form, not requiring specific positioning. However, indentation is recommended to better convey the structure of the text to human readers.

Syntactic properties of different dialects may differ. The common platform for all Rebol dialects is the "data exchange dialect"; other dialects are usually derived from it. In addition to being the common platform for all dialects, the "data exchange dialect" is directly used to represent data and metadata, populate data structures, send data over Internet, and save them in data storage.

In contrast to programming languages like C, the "data exchange dialect" does not consist of declarations, statements, expressions or keywords. A valid "data exchange dialect" text stream is a tree data structure consisting of blocks (the root block is implicit, subblocks are delimited by square brackets), parens (delimited by round brackets), strings (delimited by double quotes or curly brackets suitable for multi-line strings; caret notation is used for unprintable characters), URLs, e-mail addresses, files, paths or other composite values. Unlike ALGOL blocks, Rebol blocks are composite values similar to quoted s-expressions in Lisp. The fact that code is written in the form of Rebol blocks makes the language homoiconic.

Blocks as well as parens may contain other composite values (a block may contain subblocks, parens, strings, ...) or scalar values like words, set-words (words suffixed by the colon), get-words (words prefixed by the colon), lit-words (words prefixed by the apostrophe), numbers, money, characters, etc., separated by whitespace. Note that special characters are allowed in words, so codice_5 is a word unlike codice_6, which is a sequence of three words separated by spaces.

Comments may appear following the semicolon until the end of the line. Multi-line comments or comments not ignored by the lexical parser can be written using "ordinary" datatypes like multi-line strings.

Blocks containing domain-specific language can be submitted as arguments to specific "evaluator" functions.

The most frequently used evaluator is the codice_7 function. It is used by default to interpret the text input to the interpreter console.

The "do dialect" interpreted by the codice_7 function, is an expression-oriented sublanguage of the "data exchange dialect". The main semantic unit of the language is the expression. In contrast to imperative programming languages descending from ALGOL, the "do dialect" has neither keywords, nor statements.

Words are used as case-insensitive variables. Like in all dynamically typed languages, variables don't have an associated type, type is associated with values. The result, i.e. the evaluation of a word is returned, when a word is encountered by the codice_7 function. The set-word form of a word can be used for assignment. While not having statements, assignment, together with functions with side-effects can be used for imperative programming.

Subblocks of the root block evaluate to themselves. This property is used to handle data blocks, for structured programming by submitting blocks as arguments to control functions like codice_10, codice_11, codice_12, etc., and for dialecting, when a block is passed to a specific interpreter function.

A specific problem worth noting is that composite values, assigned to variables, are not copied. To make a copy, the value must be passed to the codice_13 function.

The codice_7 function normally follows a prefix style of evaluation, where a function processes the arguments that follow it. However, infix evaluation using infix operators exists too. Infix evaluation takes precedence over the prefix evaluation. For example,
returns 1, since the infix addition takes precedence over the computation of the absolute value. When evaluating infix expressions, the order of evaluation is left to right, no operator takes precedence over another. For example,

returns 20, while an evaluation giving precedence to multiplication would yield 14. All operators have prefix versions. codice_15 usually evaluates arguments before passing them to a function. So, the below expression:

first reads the Wikipedia Rebol page and then passes the result to the codice_16 function. Parentheses can be used to change the order of evaluation. Using prefix notation, the usage of parentheses in expressions can be avoided.

The simple precedence rules are both an advantage:
as well as a disadvantage:

The codice_17 function is preferably used to specify, validate, transform and interpret dialects. It does so by matching "parse expressions" at run time.

"Parse expressions" are written in the "parse dialect", which, like the "do dialect", is an expression-oriented sublanguage of the "data exchange dialect". Unlike the "do dialect", the "parse dialect" uses keywords representing operators and the most important nonterminals, infix parsing operators don't have prefix equivalents and use precedence rules ("sequence" has higher precedence than "choice").

Actions can be included to be taken during the parsing process as well and the codice_17 function can be used to process blocks or strings. At the "string parsing" level codice_17 must handle the "low level" parsing, taking into account characters and delimiters. "Block parsing" is higher level, handling the scanning at the level of Rebol values.

The parse dialect belongs to the family of grammars represented by the top-down parsing language or the parsing expression grammar (PEG). The main similarity is the presence of the "sequence" and "choice" operators all the family members have. Parse dialect syntax and the similarities between the parse dialect and the PEG are illustrated by this transliteration of a PEG example that parses an arithmetic expression:
The official Rebol 2.7.8 implementation is available in several editions ("/Core", "/View", "/Command", "/SDK" and "/IOS"). Both "/Core" and "/View" editions are freely redistributable software.

The runtime environment is stored in a single executable file. "Rebol/Core" 2.7.8, the console edition, is about 300 KB and "Rebol/View" 2.7.8, the graphical user interface edition, is about 650 KB in size.

"Rebol/View" provides platform-independent graphics and sound access, and comes with its own windowing toolkit and extensible set of styles (GUI widgets). Extended editions, such as "Rebol/Command" 2.7.8 or "Rebol/SDK" 2.7.8 require a paid license; they add features like ODBC data access, and the option to create standalone executable files.




</doc>
<doc id="26386" url="https://en.wikipedia.org/wiki?curid=26386" title="Red Hat">
Red Hat

Red Hat, Inc. (stylized redhat) is an American multinational software company providing open-source software products to the enterprise community. Founded in 1993, Red Hat has its corporate headquarters in Raleigh, North Carolina, with satellite offices worldwide.

Red Hat has become associated to a large extent with its enterprise operating system Red Hat Enterprise Linux and with the acquisition of open-source enterprise middleware vendor JBoss. Red Hat also offers Red Hat Virtualization (RHV), an enterprise virtualization product. Red Hat provides storage, operating system platforms, middleware, applications, management products, and support, training, and consulting services.

Red Hat creates, maintains, and contributes to many free software projects. It has acquired several proprietary software product codebases through corporate mergers and acquisitions and has released such software under open source licenses. , Red Hat is the second largest corporate contributor to the Linux kernel version 4.14 after Intel.

In 1993, Bob Young incorporated the ACC Corporation, a catalog business that sold Linux and Unix software accessories. In 1994, Marc Ewing created his own Linux distribution, which he named Red Hat Linux (Ewing had worn a red Cornell University lacrosse hat, given to him by his grandfather, while attending Carnegie Mellon University). Ewing released the software in October, and it became known as the Halloween release. Young bought Ewing's business in 1995, and the two merged to become Red Hat Software, with Young serving as chief executive officer (CEO).

Red Hat went public on August 11, 1999, achieving the eighth-biggest first-day gain in the history of Wall Street. Matthew Szulik succeeded Bob Young as CEO in December of that year. Bob Young went on to found the online print on demand and self-publishing company, Lulu in 2002.

On November 15, 1999, Red Hat acquired Cygnus Solutions. Cygnus provided commercial support for free software and housed maintainers of GNU software products such as the GNU Debugger and GNU Binutils. One of the founders of Cygnus, Michael Tiemann, became the chief technical officer of Red Hat and the vice president of open source affairs. Later Red Hat acquired WireSpeed, C2Net and Hell's Kitchen Systems.

In February 2000, "InfoWorld" awarded Red Hat its fourth consecutive "Operating System Product of the Year" award for Red Hat Linux 6.1. Red Hat acquired Planning Technologies, Inc in 2001 and AOL's iPlanet directory and certificate-server software in 2004.

Red Hat moved its headquarters from Durham to North Carolina State University's Centennial Campus in Raleigh, North Carolina in February 2002. In the following month Red Hat introduced Red Hat Linux Advanced Server, later renamed Red Hat Enterprise Linux (RHEL). Dell, IBM, HP and Oracle Corporation announced their support of the platform.

In December 2005, "CIO Insight" magazine conducted its annual "Vendor Value Survey", in which Red Hat ranked #1 in value for the second year in a row. Red Hat stock became part of the NASDAQ-100 on December 19, 2005.

Red Hat acquired open-source middleware provider JBoss on June 5, 2006, and JBoss became a division of Red Hat. On September 18, 2006, Red Hat released the Red Hat Application Stack, which integrated the JBoss technology and which was certified by other well-known software vendors. On December 12, 2006, Red Hat stock moved from trading on NASDAQ (RHAT) to the New York Stock Exchange (RHT). In 2007 Red Hat acquired MetaMatrix and made an agreement with Exadel to distribute its software.

On March 15, 2007, Red Hat released Red Hat Enterprise Linux 5, and in June acquired Mobicents. On March 13, 2008, Red Hat acquired Amentra, a provider of systems integration services for service-oriented architecture, business process management, systems development and enterprise data services.

On July 27, 2009, Red Hat replaced CIT Group in Standard and Poor's 500 stock index, a diversified index of 500 leading companies of the U.S. economy. This was reported as a major milestone for Linux.

On December 15, 2009, it was reported that Red Hat will pay to settle a class action lawsuit related to the restatement of financial results from July 2004. The suit had been pending in U.S. District Court for the Eastern District of North Carolina. Red Hat reached the proposed settlement agreement and recorded a one-time charge of for the quarter that ended Nov. 30.

On January 10, 2011, Red Hat announced that it would expand its headquarters in two phases, adding 540 employees to the Raleigh operation, and investing over . The state of North Carolina is offering up to in incentives. The second phase involves "expansion into new technologies such as software visualization and technology cloud offerings".

On August 25, 2011, Red Hat announced it would move about 600 employees from the N.C. State Centennial Campus to Two Progress Plaza downtown. A ribbon cutting ceremony was held June 24, 2013, in the re-branded Red Hat Headquarters.

In 2012, Red Hat became the first one-billion dollar open source company, reaching in annual revenue during its fiscal year. Red Hat passed the $2 billion benchmark in 2015. the company's annual revenue was nearly $3 billion.

On October 16, 2015, Red Hat announced its acquisition of IT automation startup Ansible, rumoured for an estimated .

In May 2018 Red Hat bought 130 person slim operating system and kubernetes company CoreOs for 250 million USD .

Red Hat sponsors the Fedora Project, a community-supported free software project that aims to promote the rapid progress of free and open-source software and content. Fedora aims for rapid innovation using open processes and public forums.

The Fedora Project Board, which comprises community leaders and representatives of Red Hat, leads the project and steers the direction of the project and of Fedora, the Linux distribution it develops. Red Hat employees work with the code alongside community members, and many innovations within the Fedora Project make their way into new releases of Red Hat Enterprise Linux.

Red Hat operates on a professional open-source business model based on open-source software, development within a community, professional quality assurance, and subscription-based customer support. They produce open-source code so that more programmers can make further adaptations and improvements.

Red Hat sells subscriptions for the support, training, and integration services that help customers in using their open-source software products. Customers pay one set price for unlimited access to services such as Red Hat Network and up to 24/7 support.

In September 2014, however, CEO Jim Whitehurst announced that Red Hat was "in the midst of a major shift from client-server to cloud-mobile".

Rich Bynum, a member of Red Hat's legal team, attributes Linux's success and rapid development partially to open source business models, including Red Hat's.

Red Hat engineers worked with the One Laptop per Child initiative (a non-profit organization established by members of the MIT Media Lab) to design and produce an inexpensive laptop and provide every child in the world with access to open communication, open knowledge, and open learning. The XO-4 laptop, the machine of this project, runs a slimmed-down version of Fedora 17 as its operating system.

Red Hat is the largest contributor to the GNOME desktop environment. It has several employees working full-time on Evolution, the official personal information manager for GNOME.

Dogtail, an open-source automated graphical user interface (GUI) test framework initially developed by Red Hat, consists of free software released under the GNU General Public License (GPL) and is written in Python. It allows developers to build and test their applications. Red Hat announced the release of Dogtail at the 2006 Red Hat Summit.

Red Hat MRG is a clustering product intended for integrated high-performance computing (HPC). The acronym MRG stands for "Messaging Realtime Grid".

Red Hat Enterprise MRG replaces the Red Hat Enterprise Linux RHEL, a Linux distribution developed by Red Hat, kernel in order to provide extra support for real-time computing, together with middleware support for message brokerage and scheduling workload to local or remote virtual machines, grid computing, and cloud computing.

, Red Hat works with the Condor High-Throughput Computing System community and also provides support for the software.

The Tuna performance-monitoring tool runs in the MRG environment.

Red Hat produces the online publication Opensource.com. The site highlights ways open source principles apply in domains other than software development. The site tracks the application of open source philosophy to business, education, government, law, health, and life.

The company originally produced a newsletter called Under the Brim. Wide Open magazine first appeared in March 2004, as a means for Red Hat to share technical content with subscribers on a regular basis. The Under the Brim newsletter and Wide Open magazine merged in November 2004, to become "Red Hat Magazine". In January 2010, "Red Hat Magazine" became Opensource.com.

In 2007, Red Hat announced that it had reached an agreement with some free software and open source (FOSS) companies that allowed it to make a distribution portal called Red Hat Exchange, reselling FOSS software with the original branding intact. However, by 2010, Red Hat had abandoned the Exchange program to focus their efforts more on their Open Source Channel Alliance which began in April 2009.

Red Hat Subscription Manager (RHSM) combines content delivery with subscription management.

Red Hat operates OpenShift, a cloud computing platform as a service, supporting applications written in Node.js, PHP, Perl, Python, Ruby, JavaEE and more.

On July 31, 2018, Red Hat announced the release of Istio 1.0, a microservices management program used in tandem with the Kubernetes platform. The software purports to provide "traffic management, service identity and security, policy enforcement and telemetry" services in order to streamline Kubernetes use under the various Fedora-based operating systems. Red Hat's Brian Redbeard Harring described Istio as "aiming to be a control plane, similar to the Kubernetes control plane, for configuring a series of proxy servers that get injected between application components".

Red Hat markets a version of OpenStack which helps manage a data center in the manner of cloud computing.

Red Hat CloudForms provides management of virtual machines, instances and containers based on VMware vSphere, Red Hat Virtualization, Microsoft Hyper-V, OpenStack, Amazon EC2, Google Cloud Platform, Microsoft Azure, and Red Hat OpenShift. CloudForms is based on the ManageIQ project that Red Hat open sourced. Code in ManageIQ is from the over acquisition of ManageIQ in 2012.

Red Hat contributes, with several software developers, to work on LibreOffice, a free and open source office suite.

Red Hat has some employees working full-time on other free and open source software projects that are not Red Hat products, such as two full-time employees working on the free software "radeon" (David Airlie and Jerome Glisse) and one full-time employee working on the free software "nouveau" graphic drivers. Another such project is AeroGear, an open source project that brings security and development expertise to cross-platform enterprise mobile development.

Red Hat also organises "Open Source Day" events where multiple partners show their open source technologies.

Subscribers have access to:


Over and above Red Hat's major products and acquisitions, Red Hat programmers have produced software programming-tools and utilities to supplement standard Unix and Linux software. Some of these Red Hat "products" have found their way from specifically Red Hat operating environments via open-source channels to a wider community. Such utilities include:

The Red Hat website lists the organization's major involvements in free and open-source software projects.

Community projects under the aegis of Red Hat include:


In 2000, Red Hat created the subsidiary Red Hat India to deliver Red Hat software, support, and services to Indian customers. Colin Tenwick, vice president and general manager of Red Hat EMEA said Red Hat India was opened "in response to the rapid adoption of Red Hat Linux in the subcontinent. Demand for open source solutions from the Indian markets is rising and Red Hat wants to play a major role in this region." Red Hat India has worked with local companies to enable adoption of open source technology in both government and education.

In 2006, Red Hat India had a distribution network of more than 70 channel partners spanning 27 cities across India. Red Hat India's channel partners included MarkCraft Solutions, Ashtech Infotech Pvt Ltd, Efensys Technologies, Embee Software, Allied Digital Services, and Softcell Technologies. Distributors include Integra Micro Systems and Ingram Micro.

Red Hat's first major acquisition involved Delix Computer GmbH-Linux Div, the Linux-based operating-system division of Delix Computer, a German computer company, on July 30, 1999.

Red Hat acquired Cygnus Solutions, a company that provided commercial support for free software, on January 11, 2000 - it was the company's largest acquisition, for . Michael Tiemann, co-founder of Cygnus, served as the chief technical officer of Red Hat after the acquisition. Red Hat made the most acquisitions in 2000 with five: Cygnus Solutions, Bluecurve, Wirespeed Communications, Hell's Kitchen Systems, and C2Net. On June 5, 2006, Red Hat acquired open-source middleware provider JBoss for and integrated it as its own division of Red Hat.

On December 14, 1998, Red Hat made its first divestment, when Intel and Netscape acquired undisclosed minority stakes in the company. The next year, on March 9, 1999, Compaq, IBM, Dell and Novell each acquired undisclosed minority stakes in Red Hat.



</doc>
<doc id="26388" url="https://en.wikipedia.org/wiki?curid=26388" title="Reno, Nevada">
Reno, Nevada

Reno ( ) is a city in the U.S. state of Nevada, located in the western part of the state, approximately from Lake Tahoe. Known as "The Biggest Little City in the World", Reno is famous for its hotels and casinos and as the birthplace of Harrah's Entertainment (now known as Caesars Entertainment Corporation). It is the county seat of Washoe County, in the northwestern part of the state. The city sits in a high desert at the foot of the Sierra Nevada and its downtown area (along with Sparks) occupies a valley informally known as the Truckee Meadows. It is named after slain Union general Jesse L. Reno.

Reno is the most populous city in the state outside the Las Vegas Valley, with an estimated population of 241,445 in 2015. Reno is part of the Reno–Sparks metropolitan area, which consists of all of both Washoe and Storey counties and has a 2016 estimated population of 457,420.

Archaeological finds place the eastern border for the prehistoric Martis people in the Reno area.

As early as the mid 1850s a few pioneers settled in the Truckee Meadows, a relatively fertile valley through which the Truckee River made its way from Lake Tahoe to Pyramid Lake. In addition to subsistence farming, these early residents could pick up business from travelers along the California Trail, which followed the Truckee westward, before branching off towards Donner Lake, where the formidable obstacle of the Sierra Nevada began.

Gold was discovered in the vicinity of Virginia City in 1850, and a modest mining community developed, but the discovery of silver in 1859 at the Comstock Lode led to a mining rush, and thousands of emigrants left their homes, bound for the West, hoping to find a fortune. 

To provide the necessary connection between Virginia City and the California Trail, Charles W. Fuller built a log toll bridge across the Truckee River in 1859. A small community that would service travelers soon grew up near the bridge. After two years, Fuller sold the bridge to Myron C. Lake, who continued to develop the community with the addition of a grist mill, kiln, and livery stable to the hotel and eating house. He renamed it "Lake's Crossing". In 1864, Washoe County was consolidated with Roop County, and Lake's Crossing became the largest town in the county. Lake had earned himself the title "founder of Reno".

By January 1863, the Central Pacific Railroad (CPRR) had begun laying tracks east from Sacramento, California, eventually connecting with the Union Pacific Railroad at Promontory, Utah, to form the First Transcontinental Railroad. Lake deeded land to the CPRR in exchange for its promise to build a depot at Lake's Crossing. Once the railroad station was established, the town of Reno officially came into being on May 9, 1868. CPRR construction superintendent Charles Crocker named the community after Major General Jesse Lee Reno, a Union officer killed in the American Civil War at the Battle of South Mountain.

In 1871, Reno became the county seat of the newly expanded Washoe County, replacing the previous county seat, located in Washoe City. However, political power in Nevada remained with the mining communities, first Virginia City and later Tonopah and Goldfield.

The extension of the Virginia and Truckee Railroad to Reno in 1872 provided a boost to the new city's economy. In the following decades, Reno continued to grow and prosper as a business and agricultural center and became the principal settlement on the transcontinental railroad between Sacramento and Salt Lake City.
As the mining boom waned early in the 20th century, Nevada's centers of political and business activity shifted to the non-mining communities, especially Reno and Las Vegas, and today the former mining metropolises stand as little more than ghost towns. Despite this, Nevada is still the third-largest gold producer in the world, after South Africa and Australia; the state yielded 6.9 percent of the world's supply in 2005 world gold production.

The "Reno Arch" was erected on Virginia Street in 1926 to promote the upcoming Transcontinental Highways Exposition of 1927. The arch included the words "Nevada's Transcontinental Highways Exposition" and the dates of the exposition. After the exposition, the Reno City Council decided to keep the arch as a permanent downtown gateway, and Mayor E.E. Roberts asked the citizens of Reno to suggest a slogan for the arch. No acceptable slogan was received until a $100 prize was offered, and G.A. Burns of Sacramento was declared the winner on March 14, 1929, with "Reno, The Biggest Little City in the World".

Reno took a leap when the state of Nevada legalized open-gambling on March 19, 1931, along with the passage of even more liberal divorce laws than places like Hot Springs, Arkansas, offered. No other state offered what Nevada had in the 1930s, and casinos like the Bank Club and Palace were popular.

Within a few years, the Bank Club, owned by George Wingfield, Bill Graham, and Jim McKay, was the state's largest employer and the largest casino in the world. Wingfield owned most of the buildings in town that housed gaming and took a percentage of the profits, along with his rent.

Ernie Pyle once wrote in one of his columns, "All the people you saw on the streets in Reno were obviously there to get divorces." In Ayn Rand's novel "The Fountainhead", published in 1943, the New York-based female protagonist tells a friend, "I am going to Reno," which is taken as a different way of saying "I am going to divorce my husband." Among others, the Belgian-French writer Georges Simenon, at the time living in the U.S., came to Reno in 1950 in order to divorce his first wife.
The divorce business eventually died as the other states fell in line by passing their own laws easing the requirements for divorce, but gambling continued as a major Reno industry. While gaming pioneers like "Pappy" and Harold Smith of Harold's Club and Bill Harrah of the soon-to-dominate Harrah's casino set up shop in the 1930s, the war years of the 1940s cemented Reno as the place to play for two decades. Beginning in the 1950s, the need for economic diversification beyond gaming fueled a movement for more lenient business taxation.

A disaster occurred on the afternoon of February 5, 1957, when an explosion ripped through the heart of downtown. At 1:03 pm, two explosions, caused by natural gas leaking into the maze of pipes and ditches under the city, and an ensuing fire destroyed five buildings in the vicinity of Sierra and First streets along the Truckee River. Forty-nine people were injured in the disaster, and two were killed. The first explosion hit under the block of shops on the west side of Sierra Street (now the site of the Century Riverside), the second, across Sierra Street, now the site of the Palladio.

The presence of a main east-west rail line, the emerging interstate highway system, favorable state tax climate, and relatively inexpensive land created good conditions for warehousing and distribution of goods.

In the 1980s, Indian gaming rules were relaxed, and starting in 2000, Californian Native casinos began to cut into casino revenues. Major new construction projects have been completed in the Reno and Sparks areas. A few new luxury communities were recently built in Truckee, California, approximately west of Reno on Interstate 80. Reno also is an outdoor recreation destination, due to its close proximity to the Sierra Nevada, Lake Tahoe, and numerous ski resorts in the region.

Wetlands are an important part of the Reno/Tahoe area. They act as a natural filter for the solids that come out of the water treatment plant. Plant roots absorb nutrients from the water and naturally filter it. Wetlands are home to over 75% of the species in the Great Basin. However, the area's wetlands are at risk of being destroyed due to development around the city. While developers build on top of the wetlands they fill them with dirt, destroying the habitat they create for the plants and animals. Washoe County has devised a plan that will help protect these ecosystems: mitigation. In the future, when developers try to build over a wetland, they will be responsible for creating another wetland near Washoe Lake.

The Truckee River serves as Reno's primary source of drinking water. It supplies Reno with of water a day during the summer, and of water per day in the winter. Before the water goes to the homes around the Reno area, it must go to one of two water treatment plants, Chalk Bluff or Glendale Water Treatment Plant. As an attempt to save water, golf courses in Reno have been using treated effluent water instead of treated water from one of Reno's water plants.

The Reno-Sparks wastewater treatment plant discharges tertiary treated effluent to the Truckee River. In the 1990s this capacity was increased from 20 to 30 million U.S. gallons (70 to 110 million liters) per day. While treated, the effluent contains suspended solids, nitrogen, and phosphorus, aggravating water quality concerns of the river and its receiving waters of Pyramid Lake. Local agencies working with the Environmental Protection Agency have developed a number of watershed management strategies to accommodate this expanded effluent discharge; to accomplish this successful outcome, the DSSAM Model was developed and calibrated for the Truckee River in order to analyze the most cost-effective available management strategy set. The resulting management strategies included a package of measures such as land use controls in the Lake Tahoe basin, urban runoff controls in Reno and Sparks, and best management practices for wastewater discharge.

The Reno area is frequently subject to wildfires, causing property damage and sometimes loss of life. In August 1960, the Donner Ridge fire resulted in a loss of electricity to the city for four days. In November 2011, arcing from powerlines caused a fire in Caughlin in southwest Reno that destroyed 26 homes and killed one older man, and only two months later in January 2012 another fire in Washoe Drive sparked by fireplace ashes destroyed 29 homes and killed one older woman. Around 10,000 residents were evacuated, and a state of emergency was declared. The fires came at the end of Reno's longest recorded dry spell.

Reno is situated just east of the Sierra Nevada on the western edge of the Great Basin at an elevation of about above sea level. Numerous faults exist throughout the region. Most of these are normal (vertical motion) faults associated with the uplift of the various mountain ranges, including the Sierra Nevada.

In February 2008, an earthquake swarm began to occur, lasting for several months, and with the largest quake registering at 4.9 on the Richter magnitude scale, although some geologic estimates put it at 5.0. The earthquakes were centered on the Somersett community in western Reno near the areas of Mogul and Verdi. Many homes in these areas were damaged.

Reno sits in the rain shadow of the Sierra Nevada mountain range. Annual rainfall averages . Despite this low amount of rainfall per year, Reno features a steppe climate (Köppen: "BSk") due to its low evapotranspiration. Annual precipitation has ranged from in 1947 to in 1983. The most precipitation in one month was in December 1955 and the most precipitation in 24 hours was on January 21, 1943. Winter has snowfall which is usually light to moderate but can be heavy some days, averaging annually. Snowfall varies with the lowest amounts (roughly 19–23 inches annually) at the lowest part of the valley at and east of the Reno–Tahoe International Airport at , while the foothills of the Carson Range to the west ranging from in elevation just a few miles west of downtown can receive up to two to three times as much annual snowfall. The mountains of the Virginia Range to the east can receive more summer thunderstorms and precipitation, and around twice as much annual snowfall above . However, snowfall increases in the Virginia Range are less dramatic as elevation climbs than in the Carson Range to the west, because the Virginia Range is well within the rain shadow of the Sierra Nevada and Carson Range. The most snowfall in the city in one year was in 1971, and the most snowfall in one month was in March 1952.

Most rainfall occurs in winter and spring. The city has 300 days of sunshine per year. Summer thunderstorms can occur between April and October. The eastern side of town and the mountains east of Reno tend to be prone to thunderstorms more often, and these storms may be severe because an afternoon downslope west wind, called a "Washoe Zephyr", can develop in the Sierra Nevada, causing air to be pulled down in the Sierra Nevada and Reno, destroying or preventing thunderstorms, but the same wind can push air upwards against the Virginia Range and other mountain ranges east of Reno, creating powerful thunderstorms.

The monthly daily average temperature ranges from in December to in July, with the diurnal temperature variation reaching in summer, still lower than much of the high desert to the east. There are 3.9 days of + highs, 58 days of + highs, and 2.5 nights with sub- lows annually; the temperature does not rise above freezing on only 5.1 days. The all-time record high temperature is , which occurred on July 10 and 11, 2002, and again on July 5, 2007. The all-time record low temperature is , which occurred on January 21, 1916. In addition, the region is windy throughout the year; observers such as Mark Twain have commented about the "Washoe Zephyr", northwestern Nevada's distinctive wind.

As of the census of 2010, there were 225,221 people, 90,924 households, and 51,112 families residing in the city. The population density was 2,186.6 per square mile (844.2/km²). There were 102,582 housing units at an average density of 995.9 per square mile (384.5/km²). The racial makeup of the city was 74.2% White, 2.9% African American, 1.3% Native American, 6.3% Asian, 0.7% Pacific Islander, 10.5% some other race, and 4.2% from two or more races. Hispanic or Latino of any race were 24.3% of the population. Non-Hispanic Whites were 62.5% of the population in 2010, down from 88.5% in 1980.
At the 2010 census, there were 90,924 households, out of which 29.8% had children under the age of 18 living with them, 38.4% were headed by married couples living together, 11.8% had a female householder with no husband present, and 43.8% were non-families. 32.1% of all households were made up of individuals, and 9.7% were someone living alone who was 65 years of age or older. The average household size was 2.43, and the average family size was 3.10.

In the city, the 2010 population was spread out with 22.8% under the age of 18, 12.5% from 18 to 24, 28.2% from 25 to 44, 24.9% from 45 to 64, and 11.7% who were 65 years of age or older. The median age was 34.6 years. For every 100 females, there were 103.4 males. For every 100 females age 18 and over, there were 102.7 males.

In 2011 the estimated median income for a household in the city was $44,846, and the median income for a family was $53,896. Males had a median income of $42,120 versus $31,362 for females. The per capita income for the city was $25,041. About 9.6% of families and 14.4% of the population were below the poverty line, including 15.1% of those under age 18 and 12.8% of those age 65 or over. The population was 180,480 at the 2000 census; in 2010, its population had risen to 225,221, making it the third-largest city in the state after Las Vegas and Henderson, and the largest outside Clark County. Reno lies north of the Nevada state capital, Carson City, and northeast of Lake Tahoe in a shrub-steppe environment. Reno shares its eastern border with the city of Sparks and is the larger of the principal cities of the Reno–Sparks, Nevada Metropolitan Statistical Area (MSA), a metropolitan area that covers Storey and Washoe counties. The MSA had a combined population of 425,417 at the 2010 census. The MSA is combined with the Fernley Micropolitan Statistical Area to form the Reno-Sparks-Fernley Combined Statistical Area, which had a total population of 477,397 at the 2010 census.

Before the late 1950s, Reno was the gambling capital of the United States, but Las Vegas' rapid growth, American Airlines' 2000 buyout of Reno Air, and the growth of Native American gaming in California have reduced its business. Older casinos were torn down (Mapes Hotel, Fitzgerald's Nevada Club, Primadonna, Horseshoe Club, Harold's Club, Palace Club), or smaller casinos like the Comstock, Sundowner, Golden Phoenix, Kings Inn, Money Tree, Virginian, and Riverboat were either closed or were converted into condos.

Because of its location, Reno has traditionally drawn the majority of its California tourists and gamblers from the San Francisco Bay Area and Sacramento, while Las Vegas has historically served more tourists from Southern California and the Phoenix area.

Several local large hotel casinos have shown significant growth and have moved gaming further away from the downtown core. These larger hotel casinos are the Atlantis, the Peppermill and the Grand Sierra Resort. The Peppermill was chosen as the most outstanding Reno gaming/hotel property by "Casino Player" and "Nevada" magazines. In 2005, the Peppermill Reno began a $300 million Tuscan-themed expansion.

In an effort to bring more tourism to the area, Reno holds several events throughout the year. They include Hot August Nights (a classic car convention), Street Vibrations (a motorcycle fan gathering and rally), The Great Reno Balloon Race, a Cinco de Mayo celebration, bowling tournaments (held in the National Bowling Stadium), and the Reno Air Races.

Several large commercial developments were constructed during the mid-2000s boom, such as The Summit in 2007 and Legends at Sparks Marina in 2008.

Reno is the location of the corporate headquarters for numerous companies, including Braeburn Capital, Hamilton, EE Technologies, and Port of Subs. International Game Technology, Bally Technologies and GameTech have a development and manufacturing presence in Reno.

Since the turn of the 21st century, greater Reno saw an influx of technology companies entering the area, following major initiatives and investments by investors from Seattle & the Bay Area. After the Great Recession, the state placed an increased focus on economic development. Thousands of new jobs were created.

Tesla's Gigafactory is currently located at the Tahoe Reno Industrial Center, and it is the largest building in the world. The Gigafactory purportedly covers 5.8 million square feet.

The arrival of several data centers at the Tahoe Reno Industrial Center is further diversifying a region that used to be primarily known for distribution and logistics outside gaming and tourism. Switch 's new SuperNAP campus at the Tahoe Reno Industrial Center is shaping up to be the largest data center in the world once completed. Apple is expanding its data center at the adjacent Reno Technology Park and is scheduled to build a warehouse on land in downtown Reno. Rackspace is also building a $422 million data center next to Apple.

The greater Reno area also hosts distribution facilities for Amazon, Walmart, Petsmart and Zulily.

According to Reno's 2016 Comprehensive Annual Financial Report, the top employers in the city are:

Reno has a number of museums. The Nevada Museum of Art is the only American Alliance of Museums (AAM) accredited art museum in the state of Nevada. The National Automobile Museum contains 200 cars that were from the collection of William F. Harrah, including Elvis Presley's 1973 Cadillac Eldorado.

Reno also hosts a number of music venues, such as the Nevada Opera, the Pioneer Center for the Performing Arts, the Reno Philharmonic Orchestra, and the Reno Pops Orchestra. The Reno Youth Symphony Orchestra (YSO), affiliated with the Reno Philharmonic, gives talented youth the opportunity to play advanced music and perform nationwide. In 2016 they had the honor of performing at Carnegie Hall.

Reno is served by the Washoe County Library System. The Washoe County Library was listed on the National Register of Historic Places in 2013.

Reno is home to the Reno Aces, the minor league baseball Triple-A affiliate of the Arizona Diamondbacks, playing in Greater Nevada Field, a downtown ballpark opened in 2009. Reno has hosted multiple professional baseball teams in the past, most under the Reno Silver Sox name. The Reno Astros, a former professional, unaffiliated baseball team, played at Moana Stadium until 2009.

In basketball, the Reno Bighorns of the NBA G League played at the Reno Events Center from 2008 to 2018. They were primarily an affiliate of the Sacramento Kings throughout its existence. The Sacramento Kings bought the team in 2016 and eventually moved the franchise to become the Stockton Kings in 2018.

Reno is host to both amateur and professional combat sporting events such as mixed martial arts and boxing. The "Fight of the Century" between Jack Johnson and James J. Jeffries was held in Reno in 1910. Boxer Ray Mancini fought four of his last five fights in Reno against Bobby Chacon, Livingstone Bramble, Hector Camacho, and Greg Haugen.

The Reno Barons, an independent professional indoor football team, played at the Reno Events Center in 2011.

Reno expected to be the future home of an ECHL ice hockey team, named the Reno Raiders, but construction on a suitable arena never began. The franchise was dormant since 1998, when it was named the Reno Rage, and earlier the Reno Renegades, and played in the now-defunct West Coast Hockey League (WCHL). In 2016, Reno was removed from the ECHL's Future Markets page.

The Reno–Tahoe Open is northern Nevada's only PGA Tour event, held at Montrêux Golf & Country Club in Reno. As part of the FedEx Cup, the tournament follows 132 PGA Tour professionals competing for a share of the event's $3 million purse. The Reno-Tahoe Open Foundation has donated more than $1.8 million to local charities.

Reno has a college sports scene, with the Nevada Wolf Pack appearing in football bowl games and an Associated Press Top Ten ranking in basketball in 2007.
In 2004, the city completed a $1.5 million whitewater park on the Truckee River in downtown Reno which hosts whitewater events throughout the year. The course runs Class 2 and 3 rapids with year-round public access. The north channel features more aggressive rapids, drop pools and "holes" for rodeo kayak-type maneuvers. The milder south channel is set up as a kayak slalom course and a beginner area.

Reno is home to two roller derby teams, the Battle Born Derby Demons and the Reno Roller Girls. The Battle Born Derby Demons compete on flat tracks locally and nationally. They are the only derby team locally to compete in a national Derby league.

Reno is the home of the National Bowling Stadium, which hosts the United States Bowling Congress (USBC) Open Championships every three years.

Reno is home to a variety of recreation activities including both seasonal and year-round. In the summer, Reno locals can be found near three major bodies of water: Lake Tahoe, the Truckee River, and Pyramid Lake. The Truckee River originates at Lake Tahoe and flows west to east through the center of downtown Reno before terminating at Pyramid Lake to the north. The river is a major part of Artown, held in the summer at Wingfield Park. Washoe Lake is a popular kite and windsurfing location because of its high wind speeds during the summer.

Skiing and snowboarding are among the most popular winter sports and draw in many tourists. There are 18 ski resorts (8 major resorts) located as close as and as far as from the Reno–Tahoe International Airport, including Northstar California, Sierra-at-Tahoe, Alpine Meadows, Squaw Valley, Sugar Bowl, Diamond Peak, Heavenly Mountain, and Mount Rose. Other popular Reno winter activities include snowshoeing, ice skating, and snowmobiling. There are many bike paths to ride in the summer time. International bike competitions are held in Lake Tahoe over the summer time.

The Reno Air Races, also known as the National Championship Air Races, are held each September at the Reno Stead Airport.

Reno has a democratic municipal government. The city council is the core of the government, with seven members. Five of these council people represent districts of Reno, and are vetted in the primary by the citizens of each district. In general, the top two vote earners in each ward make the ballot for the citywide election. The other two council members are the at-large member, who represents the entire city, and the mayor, who is elected by the people of the city. The council has several duties, including setting priorities for the city, promoting communication with the public, planning development, and redevelopment.

There is an elected city attorney who is responsible for civil and criminal cases. The City Attorney represents the city government in court, and prosecutes misdemeanors.

The city's charter calls for a council-manager form of government, meaning that the council appoints only two positions, the city manager, who implements and enforces the policies and programs the council approves, and the city clerk. The city manager is in charge of the budget and workforce for all city programs. The city clerk, who records the proceedings of the council, makes appointments for the council, and makes sure efficient copying and printing services are available.

In 2010, there was a ballot question asking whether the Reno city government and the Washoe County government should explore the idea of becoming one combined governmental body. Fifty-four percent of voters approved of the ballot measure to make an inquiry into consolidating the governments.

The city of Reno is protected by the Truckee Meadows Fire Protection District (TMFPD) manning 14 fire stations.

The Reno Fire Department (RFD) provides all-risk emergency service to the City of Reno residents. All-risk emergency service is the national model of municipal fire departments, providing the services needed in the most efficient way possible.

The department provides paramedic-level service to the citizens and visitors of Reno. This is the highest level of emergency medical care that can be provided in the field.

In addition to responding to fires, whether they occur in structures, vegetation/brush or vehicles, the fire department also provides rescue capabilities for almost any type of emergency situation.

This includes quick and efficient emergency medical care for the citizens; a hazardous materials team capable of identifying unknown materials and controlling a release disaster; and preparedness and management of large-scale incidents.

Maintaining this level of service requires nearly constant training of personnel. This training maintains both the skills needed to operate safely in emergency environments and the physical fitness necessary to reduce the likelihood and severity of injuries.

The minimum annual-training requirement to maintain firefighting and medical skills is 240 hours per year. Special teams and company-level drills add significantly to that number of hours.


Public education is provided by the Washoe County School District.

Reno has many charter schools, which include Academy for Career Education, serving grades 10–12, opened 2002; Alpine Academy Charter High School, serving grades 9–12, opened 2009; Bailey Charter Elementary School, serving grades K-6, opened 2001;Coral Academy of Science; Davidson Academy, serving grades 6–12, opened 2006; High Desert Montessori School, serving grades PreK-7, opened 2002; I Can Do Anything Charter School, serving grades 9–12, opened 2000; Rainshadow Community Charter High School, serving grades 9–12, opened 2003; Sierra Nevada Academy Charter School, serving grades PreK-8, opened 1999; and TEAM A (Together Everyone Achieves More Academy), serving grades 9–12, opened 2004.

Reno has a few private elementary schools such as Legacy Christian School, Excel Christian School, Lamplight Christian Schoo, and Nevada Sage Waldorf School as well as private high schools, the largest of which are Bishop Manogue High School and Sage Ridge School.

Reno was historically served by the Victory Highway and a branch of the Lincoln Highway. After the formation of the U.S. Numbered Highways system, U.S. Route 40 was routed along 4th Street through downtown Reno, before being replaced by Interstate 80. The primary north-south highway through Reno is U.S. Route 395/Interstate 580.

The Regional Transportation Commission of Washoe County (RTC) has a bus system that provides intracity buses, intercity buses to Carson City, and an on-demand shuttle service for disabled persons. The bus system has its main terminal on 4th Street in downtown Reno and secondary terminals in Sparks and at Meadowood Mall in south Reno.

Numerous shuttle and excursion services are offered connecting the Reno–Tahoe International Airport to various destinations:

Greyhound stops at a downtown terminal. Megabus stopped at the Silver Legacy Reno, but has since discontinued service to Reno.

Reno was historically a stopover along the First Transcontinental Railroad; the modern Overland Route continues to run through Reno. Reno was additionally the southern terminus of the Nevada–California–Oregon Railway (NCO) and the northern terminus of the Virginia and Truckee Railroad. Using the NCO depot and right of way, the Western Pacific Railroad also provided rail service to Reno. In the early 20th century, Reno also had a modest streetcar system. Downtown Reno has two historic train depots, the inactive Nevada-California-Oregon Railroad Depot and the still active Amtrak depot, originally built by the Southern Pacific Railroad.

Amtrak provides daily passenger service to Reno via the "California Zephyr" and multiple Amtrak Thruway Motorcoaches connecting to trains departing from Sacramento.

The city is served by Reno–Tahoe International Airport, with general aviation traffic handled by Reno Stead Airport. Reno–Tahoe International Airport is the second busiest commercial airport in the state of Nevada after McCarran International Airport in Las Vegas. Reno was the hub and headquarters of the defunct airline Reno Air.

Potable water for the city of Reno is provided by the Truckee Meadows Water Authority. The Truckee River is the primary water source, with purification being done at two plants, Chalk Bluff and Glendale. The Chalk Bluff plant's main intakes are west of Reno in Verdi, with the water flowing through a series of flumes and ditches to the plant itself. Alternative intakes are located below the plant along the banks of the Truckee River itself. The Glendale plant is sited alongside the river, and is fed by a rock and concrete rubble diversion dam a short distance upstream.

Sewage treatment for the majority of the Truckee Meadows region takes place at the Truckee Meadows Water Reclamation Facility at the eastern edge of the valley. Treated effluent returns to the Truckee River by way of Steamboat Creek.

Electrical power and natural gas are provided by NV Energy, formerly Sierra Pacific. Power comes from multiple sources, including Tracy-Clark Station to the east, and the Steamboat Springs binary cycle power plants at the southern end of town.

Reno has eight sister cities:




</doc>
<doc id="26390" url="https://en.wikipedia.org/wiki?curid=26390" title="Riemann integral">
Riemann integral

In the branch of mathematics known as real analysis, the Riemann integral, created by Bernhard Riemann, was the first rigorous definition of the integral of a function on an interval. It was presented to the faculty at the University of Göttingen in 1854, but not published in a journal until 1868. For many functions and practical applications, the Riemann integral can be evaluated by the fundamental theorem of calculus or approximated by numerical integration.

The Riemann integral is unsuitable for many theoretical purposes. Some of the technical deficiencies in Riemann integration can be remedied with the Riemann–Stieltjes integral, and most disappear with the Lebesgue integral.

Let be a nonnegative real-valued function on the interval , and let

be the region of the plane under the graph of the function and above the interval (see the figure on the top right). We are interested in measuring the area of . Once we have measured it, we will denote the area by:

The basic idea of the Riemann integral is to use very simple approximations for the area of . By taking better and better approximations, we can say that "in the limit" we get exactly the area of under the curve.

Note that where can be both positive and negative, the definition of is modified so that the integral corresponds to the "signed area" under the graph of : that is, the area above the -axis minus the area below the -axis.

A partition of an interval is a finite sequence of numbers of the form

Each is called a subinterval of the partition. The mesh or norm of a partition is defined to be the length of the longest subinterval, that is,

A tagged partition of an interval is a partition together with a finite sequence of numbers subject to the conditions that for each , . In other words, it is a partition together with a distinguished point of every subinterval. The mesh of a tagged partition is the same as that of an ordinary partition.

Suppose that two partitions and are both partitions of the interval . We say that is a refinement of if for each integer , with , there exists an integer such that and such that for some with . Said more simply, a refinement of a tagged partition breaks up some of the subintervals and adds tags to the partition where necessary, thus it "refines" the accuracy of the partition.

We can define a partial order on the set of all tagged partitions by saying that one tagged partition is greater or equal to another if the former is a refinement of the latter.

Let be a real-valued function defined on the interval . The "Riemann sum" of with respect to the tagged partition together with is

Each term in the sum is the product of the value of the function at a given point and the length of an interval. Consequently, each term represents the (signed) area of a rectangle with height and width . The Riemann sum is the (signed) area of all the rectangles.

A closely related concept are the "lower and upper Darboux sums". These are similar to Riemann sums, but the tags are replaced by the infimum and supremum (respectively) of on each subinterval:

If is continuous, then the lower and upper Darboux sums for an untagged partition are equal to the Riemann sum for that partition, where the tags are chosen to be the minimum or maximum (respectively) of on each subinterval. (When is discontinuous on a subinterval, there may not be a tag that achieves the infimum or supremum on that subinterval.) The Darboux integral, which is similar to the Riemann integral but based on Darboux sums, is equivalent to the Riemann integral.

Loosely speaking, the Riemann integral is the limit of the Riemann sums of a function as the partitions get finer. If the limit exists then the function is said to be integrable (or more specifically Riemann-integrable). The Riemann sum can be made as close as desired to the Riemann integral by making the partition fine enough.

One important requirement is that the mesh of the partitions must become smaller and smaller, so that in the limit, it is zero. If this were not so, then we would not be getting a good approximation to the function on certain subintervals. In fact, this is enough to define an integral. To be specific, we say that the Riemann integral of equals if the following condition holds:

For all , there exists such that for any tagged partition and whose mesh is less than , we have

Unfortunately, this definition is very difficult to use. It would help to develop an equivalent definition of the Riemann integral which is easier to work with. We develop this definition now, with a proof of equivalence following. Our new definition says that the Riemann integral of equals if the following condition holds:

For all , there exists a tagged partition and such that for any tagged partition and which is a refinement of and , we have

Both of these mean that eventually, the Riemann sum of with respect to any partition gets trapped close to . Since this is true no matter how close we demand the sums be trapped, we say that the Riemann sums converge to . These definitions are actually a special case of a more general concept, a net.

As we stated earlier, these two definitions are equivalent. In other words, works in the first definition if and only if works in the second definition. To show that the first definition implies the second, start with an , and choose a that satisfies the condition. Choose any tagged partition whose mesh is less than . Its Riemann sum is within of , and any refinement of this partition will also have mesh less than , so the Riemann sum of the refinement will also be within of .

To show that the second definition implies the first, it is easiest to use the Darboux integral. First, one shows that the second definition is equivalent to the definition of the Darboux integral; for this see the article on Darboux integration. Now we will show that a Darboux integrable function satisfies the first definition. Fix , and choose a partition such that the lower and upper Darboux sums with respect to this partition are within of the value of the Darboux integral. Let

If , then is the zero function, which is clearly both Darboux and Riemann integrable with integral zero. Therefore, we will assume that . If , then we choose such that

If , then we choose to be less than one. Choose a tagged partition and with mesh smaller than . We must show that the Riemann sum is within of .

To see this, choose an interval . If this interval is contained within some , then

where and are respectively, the infimum and the supremum of "f" on . If all intervals had this property, then this would conclude the proof, because each term in the Riemann sum would be bounded by a corresponding term in the Darboux sums, and we chose the Darboux sums to be near . This is the case when , so the proof is finished in that case.

Therefore, we may assume that . In this case, it is possible that one of the is not contained in any . Instead, it may stretch across two of the intervals determined by . (It cannot meet three intervals because is assumed to be smaller than the length of any one interval.) In symbols, it may happen that

(We may assume that all the inequalities are strict because otherwise we are in the previous case by our assumption on the length of .) This can happen at most times.

To handle this case, we will estimate the difference between the Riemann sum and the Darboux sum by subdividing the partition at . The term in the Riemann sum splits into two terms:

Suppose, without loss of generality, that . Then

so this term is bounded by the corresponding term in the Darboux sum for . To bound the other term, notice that

It follows that, for some (indeed any) ,

Since this happens at most times, the distance between the Riemann sum and a Darboux sum is at most . Therefore, the distance between the Riemann sum and is at most .

Let be the function which takes the value 1 at every point. Any Riemann sum of on will have the value 1, therefore the Riemann integral of on is 1.

Let be the indicator function of the rational numbers in ; that is, takes the value 1 on rational numbers and 0 on irrational numbers. This function does not have a Riemann integral. To prove this, we will show how to construct tagged partitions whose Riemann sums get arbitrarily close to both zero and one.

To start, let and be a tagged partition (each is between and ). Choose . The have already been chosen, and we can't change the value of at those points. But if we cut the partition into tiny pieces around each , we can minimize the effect of the . Then, by carefully choosing the new tags, we can make the value of the Riemann sum turn out to be within of either zero or one.

Our first step is to cut up the partition. There are of the , and we want their total effect to be less than . If we confine each of them to an interval of length less than , then the contribution of each to the Riemann sum will be at least and at most . This makes the total sum at least zero and at most . So let be a positive number less than . If it happens that two of the are within of each other, choose smaller. If it happens that some is within of some , and is not equal to , choose smaller. Since there are only finitely many and , we can always choose sufficiently small.

Now we add two cuts to the partition for each . One of the cuts will be at , and the other will be at . If one of these leaves the interval [0, 1], then we leave it out. will be the tag corresponding to the subinterval

If is directly on top of one of the , then we let be the tag for both intervals:

We still have to choose tags for the other subintervals. We will choose them in two different ways. The first way is to always choose a rational point, so that the Riemann sum is as large as possible. This will make the value of the Riemann sum at least . The second way is to always choose an irrational point, so that the Riemann sum is as small as possible. This will make the value of the Riemann sum at most .

Since we started from an arbitrary partition and ended up as close as we wanted to either zero or one, it is false to say that we are eventually trapped near some number , so this function is not Riemann integrable. However, it is Lebesgue integrable. In the Lebesgue sense its integral is zero, since the function is zero almost everywhere. But this is a fact that is beyond the reach of the Riemann integral.

There are even worse examples. is equivalent (that is, equal almost everywhere) to a Riemann integrable function, but there are non-Riemann integrable bounded functions which are not equivalent to any Riemann integrable function. For example, let be the Smith–Volterra–Cantor set, and let be its indicator function. Because is not Jordan measurable, is not Riemann integrable. Moreover, no function equivalent to is Riemann integrable: , like , must be zero on a dense set, so as in the previous example, any Riemann sum of has a refinement which is within of 0 for any positive number . But if the Riemann integral of exists, then it must equal the Lebesgue integral of , which is . Therefore, is not Riemann integrable.

It is popular to define the Riemann integral as the Darboux integral. This is because the Darboux integral is technically simpler and because a function is Riemann-integrable if and only if it is Darboux-integrable.

Some calculus books do not use general tagged partitions, but limit themselves to specific types of tagged partitions. If the type of partition is limited too much, some non-integrable functions may appear to be integrable.

One popular restriction is the use of "left-hand" and "right-hand" Riemann sums. In a left-hand Riemann sum, for all , and in a right-hand Riemann sum, for all . Alone this restriction does not impose a problem: we can refine any partition in a way that makes it a left-hand or right-hand sum by subdividing it at each . In more formal language, the set of all left-hand Riemann sums and the set of all right-hand Riemann sums is cofinal in the set of all tagged partitions.

Another popular restriction is the use of regular subdivisions of an interval. For example, the th regular subdivision of consists of the intervals 
Again, alone this restriction does not impose a problem, but the reasoning required to see this fact is more difficult than in the case of left-hand and right-hand Riemann sums.

However, combining these restrictions, so that one uses only left-hand or right-hand Riemann sums on regularly divided intervals, is dangerous. If a function is known in advance to be Riemann integrable, then this technique will give the correct value of the integral. But under these conditions the indicator function will appear to be integrable on with integral equal to one: Every endpoint of every subinterval will be a rational number, so the function will always be evaluated at rational numbers, and hence it will appear to always equal one. The problem with this definition becomes apparent when we try to split the integral into two pieces. The following equation ought to hold:

If we use regular subdivisions and left-hand or right-hand Riemann sums, then the two terms on the left are equal to zero, since every endpoint except 0 and 1 will be irrational, but as we have seen the term on the right will equal 1.

As defined above, the Riemann integral avoids this problem by refusing to integrate . The Lebesgue integral is defined in such a way that all these integrals are 0.

The Riemann integral is a linear transformation; that is, if and are Riemann-integrable on and and are constants, then

Because the Riemann integral of a function is a number, this makes the Riemann integral a linear functional on the vector space of Riemann-integrable functions.

A bounded function on a compact interval is Riemann integrable if and only if it is continuous almost everywhere (the set of its points of discontinuity has measure zero, in the sense of Lebesgue measure). This is known as the or Lebesgue's criterion for Riemann integrability or the Riemann–Lebesgue theorem. The criterion has "nothing to do" with the Lebesgue integral. It is due to Lebesgue and uses his measure zero, but makes use of neither Lebesgue's general measure or integral.

The integrability condition can be proven in various ways, one of which is sketched below.

In particular, any set that is at most countable has Lebesgue measure zero, and thus a bounded function (on a compact interval) with only finitely or countably many discontinuities is Riemann integrable.

An indicator function of a bounded set is Riemann-integrable if and only if the set is Jordan measurable. The Riemann integral can be interpreted measure-theoretically as the integral with respect to the Jordan measure.

If a real-valued function is monotone on the interval it is Riemann-integrable, since its set of discontinuities is at most countable, and therefore of Lebesgue measure zero.

If a real-valued function on is Riemann-integrable, it is Lebesgue-integrable. That is, Riemann-integrability is a "stronger" (meaning more difficult to satisfy) condition than Lebesgue-integrability.

If is a uniformly convergent sequence on with limit , then Riemann integrability of all implies Riemann integrability of , and

However, the Lebesgue monotone convergence theorem (on a monotone pointwise limit) does not hold. In Riemann integration, taking limits under the integral sign is far more difficult to logically justify than in Lebesgue integration.

It is easy to extend the Riemann integral to functions with values in the Euclidean vector space for any . The integral is defined component-wise; in other words, if then
In particular, since the complex numbers are a real vector space, this allows the integration of complex valued functions.

The Riemann integral is only defined on bounded intervals, and it does not extend well to unbounded intervals. The simplest possible extension is to define such an integral as a limit, in other words, as an improper integral:

This definition carries with it some subtleties, such as the fact that it is not always equivalent to compute the Cauchy principal value
For example, consider the function which is 0 at , 1 for , and −1 for . By symmetry,
always, regardless of . But there are many ways for the interval of integration to expand to fill the real line, and other ways can produce different results; in other words, the multivariate limit does not always exist. We can compute
In general, this improper Riemann integral is undefined. Even standardizing a way for the interval to approach the real line does not work because it leads to disturbingly counterintuitive results. If we agree (for instance) that the improper integral should always be
then the integral of the translation is −2, so this definition is not invariant under shifts, a highly undesirable property. In fact, not only does this function not have an improper Riemann integral, its Lebesgue integral is also undefined (it equals ).

Unfortunately, the improper Riemann integral is not powerful enough. The most severe problem is that there are no widely applicable theorems for commuting improper Riemann integrals with limits of functions. In applications such as Fourier series it is important to be able to approximate the integral of a function using integrals of approximations to the function. For proper Riemann integrals, a standard theorem states that if is a sequence of functions that converge uniformly to on a compact set , then
On non-compact intervals such as the real line, this is false. For example, take to be on and zero elsewhere. For all we have:
The sequence converges uniformly to the zero function, and clearly the integral of the zero function is zero. Consequently,
This demonstrates that for integrals on unbounded intervals, uniform convergence of a function is not strong enough to allow passing a limit through an integral sign. This makes the Riemann integral unworkable in applications (even though the Riemann integral assigns both sides the correct value), because there is no other general criterion for exchanging a limit and a Riemann integral, and without such a criterion it is difficult to approximate integrals by approximating their integrands.

A better route is to abandon the Riemann integral for the Lebesgue integral. The definition of the Lebesgue integral is not obviously a generalization of the Riemann integral, but it is not hard to prove that every Riemann-integrable function is Lebesgue-integrable and that the values of the two integrals agree whenever they are both defined. Moreover, a function defined on a bounded interval is Riemann-integrable if and only if it is bounded and the set of points where is discontinuous has Lebesgue measure zero.

An integral which is in fact a direct generalization of the Riemann integral is the Henstock–Kurzweil integral.

Another way of generalizing the Riemann integral is to replace the factors in the definition of a Riemann sum by something else; roughly speaking, this gives the interval of integration a different notion of length. This is the approach taken by the Riemann–Stieltjes integral.

In multivariable calculus, the Riemann integrals for functions from are multiple integrals.




</doc>
<doc id="26392" url="https://en.wikipedia.org/wiki?curid=26392" title="Run-length encoding">
Run-length encoding

Run-length encoding (RLE) is a very simple form of lossless data compression in which "runs" of data (that is, sequences in which the same data value occurs in many consecutive data elements) are stored as a single data value and count, rather than as the original run. This is most useful on data that contains many such runs. Consider, for example, simple graphic images such as icons, line drawings, and animations. It is not useful with files that don't have many runs as it could greatly increase the file size.

RLE may also be used to refer to an early graphics file format supported by CompuServe for compressing black and white images, but was widely supplanted by their later Graphics Interchange Format. RLE also refers to a little-used image format in Windows 3.x, with the extension rle, which is a Run Length Encoded Bitmap, used to compress the Windows 3.x startup screen.

For example, consider a screen containing plain black text on a solid white background. There will be many long runs of white pixels in the blank space, and many short runs of black pixels within the text. A hypothetical scan line, with B representing a black pixel and W representing white, might read as follows:

With a run-length encoding (RLE) data compression algorithm applied to the above hypothetical scan line, it can be rendered as follows:

This can be interpreted as a sequence of twelve Ws, one B, twelve Ws, three Bs, etc..,

The run-length code represents the original 67 characters in only 18. While the actual format used for the storage of images is generally binary rather than ASCII characters like this, the principle remains the same. Even binary data files can be compressed with this method; file format specifications often dictate repeated bytes in files as padding space. However, newer compression methods such as DEFLATE often use LZ77-based algorithms, a generalization of run-length encoding that can take advantage of runs of strings of characters (such as codice_3).

Run-length encoding can be expressed in multiple ways to accommodate data properties as well as additional compression algorithms. For instance, one popular method encodes run lengths for runs of two or more characters only, using an "escape" symbol to identify runs, or using the character itself as the escape, so that any time a character appears twice it denotes a run. On the previous example, this would give the following:

This would be interpreted as a run of twelve Ws, a B, a run of twelve Ws, a run of three Bs, etc. In data where runs are less frequent, this can significantly improve the compression rate.

One other matter is the application of additional compression algorithms. Even with the runs extracted, the frequencies of different characters may be large, allowing for further compression; however, if the run lengths are written in the file in the locations where the runs occurred, the presence of these numbers interrupts the normal flow and makes it harder to compress. To overcome this, some run-length encoders separate the data and escape symbols from the run lengths, so that the two can be handled independently. For the example data, this would result in two outputs, the string "codice_5" and the numbers (codice_6).

Run-length encoding schemes were employed in the transmission of television signals as far back as 1967. It is particularly well suited to palette-based bitmapped images such as computer icons, and was a popular image compression method on early online services such as CompuServe before the advent of more sophisticated formats such as GIF. It does not work well at all on continuous-tone images such as photographs, although JPEG uses it quite effectively on the coefficients that remain after transforming and quantizing image blocks.

Common formats for run-length encoded data include Truevision TGA, PackBits, PCX and ILBM. The ITU also describes a standard to encode run-length-colour for fax machines, known as T.45. The standard, which is combined with other techniques into Modified Huffman coding, is relatively efficient because most faxed documents are generally white space, with occasional interruptions of black.




</doc>
<doc id="26397" url="https://en.wikipedia.org/wiki?curid=26397" title="Red–black tree">
Red–black tree

A red–black tree is a kind of self-balancing binary search tree in computer science. Each node of the binary tree has an extra bit, and that bit is often interpreted as the color (red or black) of the node. These color bits are used to ensure the tree remains approximately balanced during insertions and deletions.

Balance is preserved by painting each node of the tree with one of two colors in a way that satisfies certain properties, which collectively constrain how unbalanced the tree can become in the worst case. When the tree is modified, the new tree is subsequently rearranged and repainted to restore the coloring properties. The properties are designed in such a way that this rearranging and recoloring can be performed efficiently.

The balancing of the tree is not perfect, but it is good enough to allow it to guarantee searching in time, where "n" is the total number of elements in the tree. The insertion and deletion operations, along with the tree rearrangement and recoloring, are also performed in time.

Tracking the color of each node requires only 1 bit of information per node because there are only two colors. The tree does not contain any other data specific to its being a red–black tree so its memory footprint is almost identical to a classic (uncolored) binary search tree. In many cases, the additional bit of information can be stored at no additional memory cost.

In 1972, Rudolf Bayer invented a data structure that was a special order-4 case of a B-tree. These trees maintained all paths from root to leaf with the same number of nodes, creating perfectly balanced trees. However, they were not binary search trees. Bayer called them a "symmetric binary B-tree" in his paper and later they became popular as 2-3-4 trees or just 2-4 trees.

In a 1978 paper, "A Dichromatic Framework for Balanced Trees", Leonidas J. Guibas and Robert Sedgewick derived the red-black tree from the symmetric binary B-tree. The color "red" was chosen because it was the best-looking color produced by the color laser printer available to the authors while working at Xerox PARC. Another response from Guibas states that it was because of the red and black pens available to them to draw the trees.

In 1993, Arne Andersson introduced the idea of right leaning tree to simplify insert and delete operations.

In 1999, Chris Okasaki showed how to make the insert operation purely functional. Its balance function needed to take care of only 4 unbalanced cases and one default balanced case.

The original algorithm used 8 unbalanced cases, but reduced that to 6 unbalanced cases. Sedgewick showed that the insert operation can be implemented in just 46 lines of Java code.
In 2008, Sedgewick proposed the left-leaning red–black tree, leveraging Andersson's idea that simplified algorithms. Sedgewick originally allowed nodes whose two children are red making his trees more like 2-3-4 trees but later this restriction was added making new trees more like 2-3 trees. Sedgewick implemented the insert algorithm in just 33 lines, significantly shortening his original 46 lines of code.

A red–black tree is a special type of binary tree, used in computer science to organize pieces of comparable data, such as text fragments or numbers.

The leaf nodes of red–black trees do not contain data. These leaves need not be explicit in computer memory—a null child pointer can encode the fact that this child is a leaf—but it simplifies some algorithms for operating on red–black trees if the leaves really are explicit nodes. To save execution time, sometimes a pointer to a single sentinel node (instead of a null pointer) performs the role of all leaf nodes; all references from internal nodes to leaf nodes then point to the sentinel node.

Red–black trees, like all binary search trees, allow efficient in-order traversal (that is: in the order Left–Root–Right) of their elements. The search-time results from the traversal from root to leaf, and therefore a balanced tree of "n" nodes, having the least possible tree height, results in search time.

In addition to the requirements imposed on a binary search tree the following must be satisfied by a 


Some definitions: the number of black nodes from the root to a node is the node's black depth; the uniform number of black nodes in all paths from root to the leaves is called the black-height of the red–black tree.

These constraints enforce a critical property of red–black trees: "the path from the root to the farthest leaf is no more than twice as long as the path from the root to the nearest leaf". The result is that the tree is roughly height-balanced. Since operations such as inserting, deleting, and finding values require worst-case time proportional to the height of the tree, this theoretical upper bound on the height allows red–black trees to be efficient in the worst case, unlike ordinary binary search trees.

To see why this is guaranteed, it suffices to consider the effect of properties 4 and 5 together. For a red–black tree "T", let "B" be the number of black nodes in "property 5". Let the shortest possible path from the root of "T" to any leaf consist of "B" black nodes. Longer possible paths may be constructed by inserting red nodes. However, property 4 makes it impossible to insert more than one consecutive red node. Therefore, ignoring any black NIL leaves, the longest possible path consists of "2*B " nodes, alternating black and red (this is the worst case). Counting the black NIL leaves, the longest possible path consists of "2*B-1" nodes.

"The shortest possible path has all black nodes, and the longest possible path alternates between red and black nodes". Since all maximal paths have the same number of black nodes, by property 5, this shows that "no path is more than twice as long as any other path".

A red–black tree is similar in structure to a B-tree of order 4, where each node can contain between 1 and 3 values and (accordingly) between 2 and 4 child pointers. In such a B-tree, each node will contain only one value matching the value in a black node of the red–black tree, with an optional value before and/or after it in the same node, both matching an equivalent red node of the red–black tree.

One way to see this equivalence is to "move up" the red nodes in a graphical representation of the red–black tree, so that they align horizontally with their parent black node, by creating together a horizontal cluster. In the B-tree, or in the modified graphical representation of the red–black tree, all leaf nodes are at the same depth.

The red–black tree is then structurally equivalent to a B-tree of order 4, with a minimum fill factor of 33% of values per cluster with a maximum capacity of 3 values.

This B-tree type is still more general than a red–black tree though, as it allows ambiguity in a red–black tree conversion—multiple red–black trees can be produced from an equivalent B-tree of order 4. If a B-tree cluster contains only 1 value, it is the minimum, black, and has two child pointers. If a cluster contains 3 values, then the central value will be black and each value stored on its sides will be red. If the cluster contains two values, however, either one can become the black node in the red–black tree (and the other one will be red).

So the order-4 B-tree does not maintain which of the values contained in each cluster is the root black tree for the whole cluster and the parent of the other values in the same cluster. Despite this, the operations on red–black trees are more economical in time because you don't have to maintain the vector of values. It may be costly if values are stored directly in each node rather than being stored by reference. B-tree nodes, however, are more economical in space because you don't need to store the color attribute for each node. Instead, you have to know which slot in the cluster vector is used. If values are stored by reference, e.g. objects, null references can be used and so the cluster can be represented by a vector containing 3 slots for value pointers plus 4 slots for child references in the tree. In that case, the B-tree can be more compact in memory, improving data locality.

The same analogy can be made with B-trees with larger orders that can be structurally equivalent to a colored binary tree: you just need more colors. Suppose that you add blue, then the blue–red–black tree defined like red–black trees but with the additional constraint that no two successive nodes in the hierarchy will be blue and all blue nodes will be children of a red node, then it becomes equivalent to a B-tree whose clusters will have at most 7 values in the following colors: blue, red, blue, black, blue, red, blue (For each cluster, there will be at most 1 black node, 2 red nodes, and 4 blue nodes).

For moderate volumes of values, insertions and deletions in a colored binary tree are faster compared to B-trees because colored trees don't attempt to maximize the fill factor of each horizontal cluster of nodes (only the minimum fill factor is guaranteed in colored binary trees, limiting the number of splits or junctions of clusters). B-trees will be faster for performing rotations (because rotations will frequently occur within the same cluster rather than with multiple separate nodes in a colored binary tree). For storing large volumes, however, B-trees will be much faster as they will be more compact by grouping several children in the same cluster where they can be accessed locally.

All optimizations possible in B-trees to increase the average fill factors of clusters are possible in the equivalent multicolored binary tree. Notably, maximizing the average fill factor in a structurally equivalent B-tree is the same as reducing the total height of the multicolored tree, by increasing the number of non-black nodes. The worst case occurs when all nodes in a colored binary tree are black, the best case occurs when only a third of them are black (and the other two thirds are red nodes).

Red–black trees offer worst-case guarantees for insertion time, deletion time, and search time. Not only does this make them valuable in time-sensitive applications such as real-time applications, but it makes them valuable building blocks in other data structures which provide worst-case guarantees; for example, many data structures used in computational geometry can be based on red–black trees, and the Completely Fair Scheduler used in current Linux kernels and epoll system call implementation uses red–black trees.

The AVL tree is another structure supporting search, insertion, and removal. AVL trees can be colored red-black, thus are a subset of RB trees. Worst-case height is 0.720 times the worst-case height of RB trees, so AVL trees are more rigidly balanced. The performance measurements of Ben Pfaff with realistic test cases in 79 runs find AVL to RB ratios between 0.677 and 1.077, median at 0.947, and geometric mean 0.910. Kind of in between are the WAVL trees.

Red–black trees are also particularly valuable in functional programming, where they are one of the most common persistent data structures, used to construct associative arrays and sets which can retain previous versions after mutations. The persistent version of red–black trees requires space for each insertion or deletion, in addition to time.

For every 2-4 tree, there are corresponding red–black trees with data elements in the same order. The insertion and deletion operations on 2-4 trees are also equivalent to color-flipping and rotations in red–black trees. This makes 2-4 trees an important tool for understanding the logic behind red–black trees, and this is why many introductory algorithm texts introduce 2-4 trees just before red–black trees, even though 2-4 trees are not often used in practice.

In 2008, Sedgewick introduced a simpler version of the red–black tree called the left-leaning red–black tree by eliminating a previously unspecified degree of freedom in the implementation. The LLRB maintains an additional invariant that all red links must lean left except during inserts and deletes. Red–black trees can be made isometric to either 2-3 trees, or 2-4 trees, for any sequence of operations. The 2-4 tree isometry was described in 1978 by Sedgewick. With 2-4 trees, the isometry is resolved by a "color flip," corresponding to a split, in which the red color of two children nodes leaves the children and moves to the parent node.

The original description of the tango tree, a type of tree optimized for fast searches, specifically uses red–black trees as part of its data structure.

In the version 8 of Java, the Collection HashMap has been modified such that instead of using a LinkedList to store different elements with colliding hashcodes, a Red-Black tree is used. This results in the improvement of time complexity of searching such an element from O(n) to O(log n).

Read-only operations on a red–black tree require no modification from those used for binary search trees, because every red–black tree is a special case of a simple binary search tree. However, the immediate result of an insertion or removal may violate the properties of a red–black tree. Restoring the red–black properties requires a small number ( or amortized) of color changes (which are very quick in practice) and no more than three tree rotations (two for insertion). Although insert and delete operations are complicated, their times remain .

If the example implementation below is not suitable, there are a couple other implementations with explanations found in Ben Pfaff's annotated C library GNU libavl (currently v2.0.2) and Eternally Confuzzled's tutorial on red-black trees.

The details of the insert and removal operations will be demonstrated with example C code. The example code may call upon the helper functions below to find the parent, sibling, uncle and grandparent nodes and to rotate a node left or right:


Insertion begins by adding the node in a very similar manner as a standard binary search tree insertion and by coloring it red. The big difference is that in the binary search tree a new node is added as a leaf, whereas leaves contain no information in the red–black tree, so instead the new node replaces an existing leaf and then has two black leaves of its own added.

What happens next depends on the color of other nearby nodes. There are several cases of red–black tree insertion to handle:

Note that:

Case 1: The current node N is at the root of the tree. In this case, it is repainted black to satisfy property 2 (the root is black). Since this adds one black node to every path at once, property 5 (all paths from any given node to its leaf nodes contain the same number of black nodes) is not violated.

Case 2: The current node's parent P is black, so property 4 (both children of every red node are black) is not invalidated. In this case, the tree is still valid. Property 5 (all paths from any given node to its leaf nodes contain the same number of black nodes) is not threatened, because the current node N has two black leaf children, but because N is red, the paths through each of its children have the same number of black nodes as the path through the leaf it replaced, which was black, and so this property remains satisfied.

Note that inserting is actually in-place, since all the calls above use tail recursion.

In the algorithm above, all cases are called only once, except in Case 3 where it can recurse back to Case 1 with the grandparent node, which is the only case where an iterative implementation will effectively loop. Because the problem of repair in that case is escalated two levels higher each time, it takes maximally iterations to repair the tree (where is the height of the tree). Because the probability for escalation decreases exponentially with each iteration the average insertion cost is practically constant.

In a regular binary search tree when deleting a node with two non-leaf children, we find either the maximum element in its left subtree (which is the in-order predecessor) or the minimum element in its right subtree (which is the in-order successor) and move its value into the node being deleted (as shown here). We then delete the node we copied the value from, which must have fewer than two non-leaf children. (Non-leaf children, rather than all children, are specified here because unlike normal binary search trees, red–black trees can have leaf nodes anywhere, so that all nodes are either internal nodes with two children or leaf nodes with, by definition, zero children. In effect, internal nodes having two leaf children in a red–black tree are like the leaf nodes in a regular binary search tree.) Because merely copying a value does not violate any red–black properties, this reduces to the problem of deleting a node with at most one non-leaf child. Once we have solved that problem, the solution applies equally to the case where the node we originally want to delete has at most one non-leaf child as to the case just considered where it has two non-leaf children.

Therefore, for the remainder of this discussion we address the deletion of a node with at most one non-leaf child. We use the label M to denote the node to be deleted; C will denote a selected child of M, which we will also call "its child". If M does have a non-leaf child, call that its child, C; otherwise, choose either leaf as its child, C.

If M is a red node, we simply replace it with its child C, which must be black by property 4. (This can only occur when M has two leaf children, because if the red node M had a black non-leaf child on one side but just a leaf child on the other side, then the count of black nodes on both sides would be different, thus the tree would violate property 5.) All paths through the deleted node will simply pass through one fewer red node, and both the deleted node's parent and child must be black, so property 3 (all leaves are black) and property 4 (both children of every red node are black) still hold.

Another simple case is when M is black and C is red. Simply removing a black node could break Properties 4 (“Both children of every red node are black”) and 5 (“All paths from any given node to its leaf nodes contain the same number of black nodes”), but if we repaint C black, both of these properties are preserved.

The complex case is when both M and C are black. (This can only occur when deleting a black node which has two leaf children, because if the black node M had a black non-leaf child on one side but just a leaf child on the other side, then the count of black nodes on both sides would be different, thus the tree would have been an invalid red–black tree by violation of property 5.) We begin by replacing M with its child C. We will "relabel" this child C (in its new position) N, and its sibling (its new parent's other child) S. (S was previously the sibling of M.)
In the diagrams below, we will also use P for N's new parent (M's old parent), S for S's left child, and S for S's right child (S cannot be a leaf because if M and C were black, then P's one subtree which included M counted two black-height and thus P's other subtree which includes S must also count two black-height, which cannot be the case if S is a leaf node).

We can perform the steps outlined above with the following code, where the function codice_1 substitutes codice_2 into codice_3's place in the tree. For convenience, code in this section will assume that null leaves are represented by actual node objects rather than NULL (the code in the "Insertion" section works with either representation).

If both N and its original parent are black, then deleting this original parent causes paths which proceed through N to have one fewer black node than paths that do not. As this violates property 5 (all paths from any given node to its leaf nodes contain the same number of black nodes), the tree must be rebalanced. There are several cases to consider:

Case 1: N is the new root. In this case, we are done. We removed one black node from every path, and the new root is black, so the properties are preserved.

Again, the function calls all use tail recursion, so the algorithm is in-place.

In the algorithm above, all cases are chained in order, except in delete case 3 where it can recurse to case 1 back to the parent node: this is the only case where an iterative implementation will effectively loop. No more than loops back to case 1 will occur (where is the height of the tree). And because the probability for escalation decreases exponentially with each iteration the average removal cost is constant.

Additionally, no tail recursion ever occurs on a child node, so the tail recursion loop can only move from a child back to its successive ancestors. If a rotation occurs in case 2 (which is the only possibility of rotation within the loop of cases 1–3), then the parent of the node N becomes red after the rotation and we will exit the loop. Therefore, at most one rotation will occur within this loop. Since no more than two additional rotations will occur after exiting the loop, at most three rotations occur in total.

A red black tree which contains "n" internal nodes has a height of .

Definitions:


Lemma: A subtree rooted at node "v" has at least formula_1 internal nodes.

Proof of Lemma (by induction height):

Basis: h("v") = 0

If "v" has a height of zero then it must be "null", therefore bh("v") = 0. So:

Inductive Step: "v" such that h("v") = k, has at least formula_1 internal nodes implies that formula_4 such that h(formula_4) = k+1 has at least formula_6 internal nodes.

Since formula_4 has h(formula_4) > 0 it is an internal node. As such it has two children each of which have a black-height of either bh(formula_4) or bh(formula_4)-1 (depending on whether the child is red or black, respectively). By the inductive hypothesis each child has at least formula_11 internal nodes, so formula_4 has at least:

internal nodes.

Using this lemma we can now show that the height of the tree is logarithmic. Since at least half of the nodes on any path from the root to a leaf are black (property 4 of a red–black tree), the black-height of the root is at least h(root)/2. By the lemma we get:

Therefore, the height of the root is .

In addition to the single-element insert, delete and lookup operations, several set operations have been defined on red-black trees: union, intersection and set difference. Then fast "bulk" operations on insertions or deletions can be implemented based on these set functions. These set operations rely on two helper operations, "Split" and "Join". With the new operations, the implementation of red-black trees can be more efficient and highly-parallelizable. This implementation allows a red root.


The union of two red-black trees and representing sets and , is a red-black tree that represents . The following recursive function computes this union:

Here, "Split" is presumed to return two trees: one holding the keys less its input key, one holding the greater keys. (The algorithm is non-destructive, but an in-place destructive version exists as well.)

The algorithm for intersection or difference is similar, but requires the "Join2" helper routine that is the same as "Join" but without the middle key. Based on the new functions for union, intersection or difference, either one key or multiple keys can be inserted to or deleted from the red-black tree. Since "Split" calls "Join" but does not deal with the balancing criteria of red-black trees directly, such an implementation is usually called the "join-based" implementation.

The complexity of each of union, intersection and difference is formula_16 for two red-black trees of sizes formula_17 and formula_18. This complexity is optimal in terms of the number of comparisons. More importantly, since the recursive calls to union, intersection or difference are independent of each other, they can be executed in parallel with a parallel depth formula_19. When formula_20, the join-based implementation has the same computational directed acyclic graph (DAG) as single-element insertion and deletion if the root of the larger tree is used to split the smaller tree.

Parallel algorithms for constructing red–black trees from sorted lists of items can run in constant time or time, depending on the computer model, if the number of processors available is asymptotically proportional to the number of items where . Fast search, insertion, and deletion parallel algorithms are also known.

A red-black-tree was referenced correctly in an episode of Missing (Canadian TV series) as noted by Robert Sedgewick in one of his lectures:





</doc>
<doc id="26399" url="https://en.wikipedia.org/wiki?curid=26399" title="RMS Laconia">
RMS Laconia

Two different ocean liners of the Cunard Steamship Lines have been named RMS "Laconia". Although one was launched ten years after the other, and was the subject of a TV movie, they are easily confused; they had similar careers, looked the same, and met similar fates.



</doc>
<doc id="26400" url="https://en.wikipedia.org/wiki?curid=26400" title="Retroactive continuity">
Retroactive continuity

Retroactive continuity, or retcon for short, is a literary device in which established facts in a fictional work are adjusted, ignored, or contradicted by a subsequently published work which breaks continuity with the former.

There are various motivations for applying retroactive continuity, including:

Retcons are used by authors to increase their creative freedom, on the assumption that the changes are unimportant to the audience compared to the new story which can be told. For instance, by retroactively setting a prior story in a parallel universe, departed popular characters can be reintroduced. More subtly, a minor plot point might be retroactively expunged (for instance, the heroine leaving home without any food), removing an obstacle to further storytelling (that she should be getting hungry).

Retcons are common in pulp fiction, and especially in comic books published by long-established publishers such as DC and Marvel. The long history of popular titles and the number of writers who contribute stories can often create situations that demand clarification or revision. Retcons also often appear in manga, soap operas, serial dramas, movie sequels, cartoons, professional wrestling angles, video games, radio series, and other forms of serial fiction.

The first published use of the phrase "retroactive continuity" is found in theologian E. Frank Tupper's 1973 book "The Theology of Wolfhart Pannenberg": "Pannenberg's conception of retroactive continuity ultimately means that history flows fundamentally from the future into the past, that the future is not basically a product of the past."

The first known printed use of "retroactive continuity" referring to the altering of history in a fictional work is in "All-Star Squadron" #18 (February 1983) from DC Comics. The series was set on DC's Earth-Two, an alternate universe in which Golden Age comic characters age in real time. "All-Star Squadron" was set during World War II on Earth-Two; as it was in the past of an alternate universe, all its events had repercussions on the contemporary continuity of the DC multiverse. Each issue changed the history of the fictional world in which it was set. In the letters column, a reader remarked that the comic "must make you [the creators] feel at times as if you're painting yourself into a corner", and, "Your matching of Golden Age comics history with new plotlines has been an artistic (and I hope financial!) success." Writer Roy Thomas responded, "we like to think that an enthusiastic ALL-STAR booster at one of Adam Malin's Creation Conventions in San Diego came up with the best name for it a few months back: 'Retroactive Continuity'. Has kind of a ring to it, don't you think?" The term then took firm root in the consciousness of fans of American superhero comics.

At some point, "retroactive continuity" was shortened to "retcon", reportedly by Damian Cugley in 1988 on Usenet. Hard evidence of Cugley's abbreviation has yet to surface, though in a Usenet posting on August 18, 1990, Cugley posted a reply in which he identified himself as "the originator of the word "retcon"". Cugley used the neologism to describe a development in the comic book "Saga of the Swamp Thing", which reinterprets the events of the title character's origin by revealing facts that previously were not part of the narrative and were not intended by earlier writers.

Retcons sometimes do not contradict previously established facts but instead fill in missing background details, usually to support current plot points. Thomas referred to "retroactive continuity" in this sense, as a purely additive process that did not undo any previous work; such additions were common in "All-Star Squadron". Kurt Busiek took a similar approach with "Untold Tales of Spider-Man", a series which told stories that specifically fit between issues of the original "The Amazing Spider-Man" series, sometimes explaining discontinuities between those earlier stories. John Byrne used a similar structure with "". Possibly the earliest Marvel Comics example of new stories placed between long-established stories was the 1977-8 magazine The Rampaging Hulk. In "The Godfather Part II", the character Frank Pentangeli is introduced as an old friend of the family though he is not referenced in the first movie; similarly Don Altobello is one of the "old time" Dons, though he is not mentioned until "The Godfather Part III". Neither addition affects the plot line of the previous films. The addition, in later seasons, of an attic to the family's home in "Full House" stands as a similar additive example.

A similar concept is that of secret history, in which the events of a story occur within the bounds of already-established events (especially real-world ones), revealing different interpretations of the events. Some of Tim Powers' novels use secret history, such as "Last Call", which suggests that Bugsy Siegel's actions were due to his being a modern-day Fisher King.

Alan Moore's additional information about the Swamp Thing's origins – revealing that Swamp Thing was not actually scientist Alec Holland converted into a plant, but actually a plant that had absorbed Holland's body and consciousness so that it merely thought it was Holland – did not contradict or change any of the events depicted in the character's previous appearances, but instead changed the reader's interpretation of them. Such additions and reinterpretations are very common in "Doctor Who".

Retcons sometimes add information that seemingly contradicts previous information. This frequently takes the form of a character who was shown to have died, but is later revealed to have somehow survived. This is a common practice in horror films, which may end with the death of a monster that goes on to appear in one or more sequels. The technique is so common in superhero comics that the term "comic book death" has been coined for it. An early example of this type of retcon is the return of Sherlock Holmes, whom writer Arthur Conan Doyle apparently killed off in "The Final Problem" in 1893, only to bring him back, in large part because of readers' responses, with "The Empty House" in 1903.

In many of his detective novels, Rex Stout implies that his character Nero Wolfe was born in Montenegro, giving some details of his early life in the Balkans around World War I. But in 1939's "Over My Dead Body", Wolfe tells an FBI agent that he was born in the United States. Stout revealed the reason for the change in a letter obtained by his authorized biographer, John McAleer: "In the original draft of "Over My Dead Body" Nero was a Montenegrin by birth, and it all fitted previous hints as to his background; but violent protests from "The American Magazine", supported by Farrar & Rinehart, caused his cradle to be transported five thousand miles."

In the 1940s and 1950s, Isaac Asimov placed the planet Trantor, capital of the Galactic Empire, at the "center of the galaxy", but later astronomical research indicated that the actual Galactic Center might be occupied by a supermassive black hole, making human life there impossible; in later works, Asimov adjusted his galaxy and Trantor's location in it.

When E.E. "Doc" Smith wrote the original "The Skylark of Space", space flight was a completely theoretical proposition. However, the last book of the series, "Skylark DuQuesne", was written in 1963, when the United States and the Soviet Union were involved in the space race. Smith adjusted the past of his series accordingly, mentioning an American base and a Soviet one being established on the Moon prior to the protagonist Seaton discovering faster-than-light flight.

Alan Moore's retcons often involve false memories. He has used this technique in the "Marvelman" series and "".

Retconning can bring back characters who were initially killed off. An example of this occurs on the CBS comedy "Two and a Half Men". The character Charlie Harper (Charlie Sheen) was killed in a train accident in the ninth season, but in the season twelve series finale "Of Course He's Dead", it is revealed that Charlie survived the ordeal and has been held captive against his will for over four years. It is also common in soap operas. On "The Bold and the Beautiful", Taylor Forrester (Hunter Tylo) was shown to flatline and have a funeral. When Tylo reprised the character in 2005, a retcon explained that Taylor had actually gone into a coma.

In the "Tom and Jerry" cartoon "Shutter Bugged Cat", it retcons the cartoon "Designs on Jerry" as it uses footage from it as Tom builds a mouse trap to catch Jerry and he modifies the measurements on the blueprints to make it fail and with a safe contraption hits him and ends with Jerry filming Tom ripping up the blueprints he used for the trap and unlike the ending of the short, he is flattened and bandaged.

The TV series "Dallas" annulled its entire Season 9 as being just the dream of another character, Pam Ewing. Writers did this to offer a supposedly plausible reason for the major character of Bobby Ewing, who had died onscreen at the end of Season 8, to be still alive when actor Patrick Duffy wanted to return to the series. This season is sometimes referred to as the "Dream Season" and was referred to humorously in later TV series such as "Family Guy".

Marvel Comics' Beyonder was originally stated to be omnipotent and the most powerful being in Marvel Universe, coming from the Beyond Realm. However, after his creator, Jim Shooter, left Marvel, writer-editor Tom DeFalco re-tooled the Beyonder, diminishing his power greatly: he was no longer omnipotent, as certain other cosmic entities were retroactively vastly upgraded to transcend the scale of infinity on which the character worked. Even after this, Beyonder was still one of the most powerful beings in Marvel, with several characters exceeding him.

In 2003, in the title of DC Comics' Teen Titans, Geoff Johns changed the entire genetic code of Kon-El (the modern version of Superboy) from a genetically altered human clone that was designed to be as Kryptonian as possible into a hybrid clone of both Superman and Lex Luthor. This change contradicted years of continuity and various facts that proved that Kon-El was human and in the process mostly ignored his unique ability of tactile telekinesis that made his powers very different from those of Superman.

In "Boy Meets World", both Shawn and Topanga have siblings in the first season but later in the series are retconned to be only children. The ages of the characters of "Boy Meets World" are altered notably where Cory is age 11 in 6th grade during season 1 to age 13 and 8th grade in season 2. This happens again in high school skipping another grade. The age gap between Cory and Eric also narrows from 4 years apart in age to 2 years apart in age.

Unpopular or embarrassing stories are sometimes later ignored by publishers, and effectively erased from a series's continuity. Later stories may contradict the previous ones or explicitly establish that they never happened. An example of this can be found with the film "Deadpool", which expressly contradicts the events of the earlier "", due to that film’s depiction of the Deadpool character being extremely unpopular with audiences. An unpopular retcon may itself be retconned away, as happened with John Byrne's "".

Film sequels often employ temporal compression to maintain a sense of currency in each installment, despite more time having elapsed diegetically ("in-universe") between one installment and another than has elapsed in the real world. For example, despite the implied contemporaneity in each of the films of "The Omen" series, the lead character ages some 15 or 20 years across three films released over a period of less than 6 years.

Retroactive continuity is similar to, but not the same as, plot inconsistencies introduced accidentally or through lack of concern for continuity; retconning, by comparison, is done deliberately. For example, the ongoing continuity contradictions on episodic TV series such as "The Simpsons" (in which the timeline of the family's history must be continually shifted forward to explain they are not getting any older) reflects intentionally lost continuity, not genuine retcons. However, in series with generally tight continuity, retcons are sometimes created after the fact to explain continuity errors, such was the case in "The Flintstones", where Wilma Flintstone was mistakenly given two separate maiden names, "Pebble" and "Slaghoople", over the course of the series. Upon discovering the discrepancy, the producers settled on "Slaghoople" and retconned it into later series in the franchise.

Retconning is also generally distinct from replacing the actor who plays a part in an ongoing series, which is more commonly an example of "loose continuity" rather than retroactively changing past continuity. The different appearance of the character is either ignored (as was done with the characters of Darrin Stephens and Gladys Kravitz on the television series "Bewitched", or Vivian on "The Fresh Prince of Bel-Air"), or explained within the series, such as with "regeneration" in "Doctor Who", or the Oracle in "The Matrix Revolutions". Sometimes, there are referential, inside jokes on actor changes in the show, such as with "My Wife & Kids" and "Roseanne", where there was a change of actresses playing a role (characters Claire Kyle and Becky Conner, respectively). In the latter, another character observes that children can change as they reach adulthood, remarking that when Becky came back from college (played by a new actress), they couldn't even recognize her.

Retconning also differs from direct revision. For example, when George Lucas re-edited the original "Star Wars" trilogy, he made changes directly to the source material, rather than introducing new source material that contradicted the contents of previous material.

Retconning is not the same as a reboot or reimagining which completely discards the original timeline, such as in "Battlestar Galactica". However, there have been partial reboots of franchises where the core of the franchise is still canonical but the expanded universe is relegated to a secondary continuity which, while not completely invalid, is subject to revision and critical review. "Robotech" is an example of this. With the release of the 2006 sequel film "Robotech The Shadow Chronicles", Harmony Gold established that only the original 1985 animated series and the 2006 sequel film are considered canonical relegating the aborted "", comics, and novels from the 80s and 90s to secondary continuity and, if elements are used from them, they are subject to selective revision and updating as appropriate to mesh with future canonical productions and to prevent conflict with the original animated series. While the Jack McKinney "Robotech" novel "End of the Circle" is evidently no longer canon, the prequel comic "" establishes that the general storyline of The Sentinels still occurred in some fashion, but various elements, including the timeline, specific unfolding of events, and some characterizations are different from what was previously depicted in earlier comics and novels. In such cases, the franchise producer may state that there is no intention to address the changes through remakes or direct retellings of such stories. It is essentially left to the viewer's imagination as to how differently the revised story unfolded.

In the British science fiction drama "Doctor Who" and its 2006 spinoff "Torchwood" Retcon is a drug used to induce memory loss. It's also known as the "Amnesia pill", but also later known as "compound B67".




</doc>
<doc id="26401" url="https://en.wikipedia.org/wiki?curid=26401" title="Rochester">
Rochester

Rochester refers to:










In the United States:

In the United Kingdom:





</doc>
<doc id="26403" url="https://en.wikipedia.org/wiki?curid=26403" title="Rosół">
Rosół

Rosół is a traditional Polish meat soup. The most popular variety is "rosół z kury", or clear chicken soup. It is commonly served with fine noodles. A vegetarian version can be made, substituting meat with oil or butter.

It is one of the most popular Polish soups and is served on family dinners and also is a traditional soup for weddings. It is also said to be a great remedy if one catches a cold. The name ""rosół"" derives from a dish made of salted meat (old conservation method) cooked in water to make it more edible. Later fresh meat was used instead of salted. Much later that dish of cooked meat became a soup that we know today.

There are lots of types of "rosół", as:
"Rosół Królewski" (The royal rosół), made of three meats: beef or veal, white poultry (hen, turkey or chicken) and dark poultry as duck, goose (crob only!), just a couple of dried king boletes, one single cabbage leaf and variety of vegetables as parsley, celery, carrot, leek. The cooking must take at least six hours of sensitive boiling on small fire. At the end there must be added softly burnt onion.

"Rosół myśliwski" (The hunter's rosół), made of variety of wild birds as well as pheasant, capercaillie, wood grouse, black grouse or grey partridge, with small addition of roe deer meat, couple of wild mushrooms, 2-3 juniper fruits. Instead of wild poultry helmeted guineafowl can be used.

The most important thing about making "rosół" is that there can be no addition of pork, since that could make "rosół" not clear. It also cannot be boiling too fast because of the same reason.

So, on to the recipe from year 1682:

This is the way to cook Polish rosół: take beef meat or veal,
hazel grouse or partridge, and whatever meat that in rosół can be cooked.
Soak it, lay in pot, then strain and pour over meat, add parsley, butter, salt, and skim well.
One has to know what to put in the rosół for it not to smell.
that is parsley, dill, onion or garlic, nutmeg or rosmarin or pepper to taste.
Lime would not spoil any rosół as well.


</doc>
<doc id="26404" url="https://en.wikipedia.org/wiki?curid=26404" title="Risk management">
Risk management

Risk management is the identification, evaluation, and prioritization of risks (defined in ISO 31000 as "the effect of uncertainty on objectives") followed by coordinated and economical application of resources to minimize, monitor, and control the probability or impact of unfortunate events or to maximize the realization of opportunities.

Risks can come from various sources including uncertainty in financial markets, threats from project failures (at any phase in design, development, production, or sustainment life-cycles), legal liabilities, credit risk, accidents, natural causes and disasters, deliberate attack from an adversary, or events of uncertain or unpredictable root-cause. There are two types of events i.e. negative events can be classified as risks while positive events are classified as opportunities. Several risk management standards have been developed including the Project Management Institute, the National Institute of Standards and Technology, actuarial societies, and ISO standards. Methods, definitions and goals vary widely according to whether the risk management method is in the context of project management, security, engineering, industrial processes, financial portfolios, actuarial assessments, or public health and safety.

Strategies to manage threats (uncertainties with negative consequences) typically include avoiding the threat, reducing the negative effect or probability of the threat, transferring all or part of the threat to another party, and even retaining some or all of the potential or actual consequences of a particular threat, and the opposites for opportunities (uncertain future states with benefits).

Certain aspects of many of the risk management standards have come under criticism for having no measurable improvement on risk; whereas the confidence in estimates and decisions seem to increase. For example, one study found that one in six IT projects were "black swans" with gigantic overruns (cost overruns averaged 200%, and schedule overruns 70%).

A widely used vocabulary for risk management is defined by "ISO Guide 73:2009", "Risk management. Vocabulary."

In ideal risk management, a prioritization process is followed whereby the risks with the greatest loss (or impact) and the greatest probability of occurring are handled first, and risks with lower probability of occurrence and lower loss are handled in descending order. In practice the process of assessing overall risk can be difficult, and balancing resources used to mitigate between risks with a high probability of occurrence but lower loss versus a risk with high loss but lower probability of occurrence can often be mishandled.

Intangible risk management identifies a new type of a risk that has a 100% probability of occurring but is ignored by the organization due to a lack of identification ability. For example, when deficient knowledge is applied to a situation, a knowledge risk materializes. Relationship risk appears when ineffective collaboration occurs. Process-engagement risk may be an issue when ineffective operational procedures are applied. These risks directly reduce the productivity of knowledge workers, decrease cost-effectiveness, profitability, service, quality, reputation, brand value, and earnings quality. Intangible risk management allows risk management to create immediate value from the identification and reduction of risks that reduce productivity.

Risk management also faces difficulties in allocating resources. This is the idea of opportunity cost. Resources spent on risk management could have been spent on more profitable activities. Again, ideal risk management minimizes spending (or manpower or other resources) and also minimizes the negative effects of risks.

According to the definition to the risk, the risk is the possibility that an event will occur and adversely affect the achievement of an objective. Therefore, risk itself has the uncertainty. Risk management such as COSO ERM, can help managers have a good control for their risk. Each company may have different internal control components, which leads to different outcomes. For example, the framework for ERM components includes Internal Environment, Objective Setting, Event Identification, Risk Assessment, Risk Response, Control Activities, Information and Communication, and Monitoring.

For the most part, these methods consist of the following elements, performed, more or less, in the following order.


The International Organization for Standardization (ISO) identifies the following principles of risk management:

Risk management should:

According to the standard ISO 31000 "Risk management – Principles and guidelines on implementation," the process of risk management consists of several steps as follows:

This involves:


After establishing the context, the next step in the process of managing risk is to identify potential risks. Risks are about events that, when triggered, cause problems or benefits. Hence, risk identification can start with the source of our problems and those of our competitors (benefit), or with the problem itself.
Examples of risk sources are: stakeholders of a project, employees of a company or the weather over an airport.
When either source or problem is known, the events that a source may trigger or the events that can lead to a problem can be investigated. For example: stakeholders withdrawing during a project may endanger funding of the project; confidential information may be stolen by employees even within a closed network; lightning striking an aircraft during takeoff may make all people on board immediate casualties.

The chosen method of identifying risks may depend on culture, industry practice and compliance. The identification methods are formed by templates or the development of templates for identifying source, problem or event. Common risk identification methods are:

Once risks have been identified, they must then be assessed as to their potential severity of impact (generally a negative impact, such as damage or loss) and to the probability of occurrence. These quantities can be either simple to measure, in the case of the value of a lost building, or impossible to know for sure in the case of an unlikely event, the probability of occurrence of which is unknown. Therefore, in the assessment process it is critical to make the best educated decisions in order to properly prioritize the implementation of the risk management plan.

Even a short-term positive improvement can have long-term negative impacts. Take the "turnpike" example. A highway is widened to allow more traffic. More traffic capacity leads to greater development in the areas surrounding the improved traffic capacity. Over time, traffic thereby increases to fill available capacity. Turnpikes thereby need to be expanded in a seemingly endless cycles. There are many other engineering examples where expanded capacity (to do any function) is soon filled by increased demand. Since expansion comes at a cost, the resulting growth could become unsustainable without forecasting and management.

The fundamental difficulty in risk assessment is determining the rate of occurrence since statistical information is not available on all kinds of past incidents and is particularly scanty in the case of catastrophic events, simply because of their infrequency. Furthermore, evaluating the severity of the consequences (impact) is often quite difficult for intangible assets. Asset valuation is another question that needs to be addressed. Thus, best educated opinions and available statistics are the primary sources of information. Nevertheless, risk assessment should produce such information for senior executives of the organization that the primary risks are easy to understand and that the risk management decisions may be prioritized within overall company goals. Thus, there have been several theories and attempts to quantify risks. Numerous different risk formulae exist, but perhaps the most widely accepted formula for risk quantification is: "Rate (or probability) of occurrence multiplied by the impact of the event equals risk magnitude."

Risk mitigation measures are usually formulated according to one or more of the following major risk options, which are:


Later research has shown that the financial benefits of risk management are less dependent on the formula used but are more dependent on the frequency and how risk assessment is performed.

In business it is imperative to be able to present the findings of risk assessments in financial, market, or schedule terms. Robert Courtney Jr. (IBM, 1970) proposed a formula for presenting risks in financial terms. The Courtney formula was accepted as the official risk analysis method for the US governmental agencies. The formula proposes calculation of ALE (annualized loss expectancy) and compares the expected loss value to the security control implementation costs (cost-benefit analysis).

Once risks have been identified and assessed, all techniques to manage the risk fall into one or more of these four major categories:


Ideal use of these risk control strategies may not be possible. Some of them may involve trade-offs that are not acceptable to the organization or person making the risk management decisions. Another source, from the US Department of Defense (see link), Defense Acquisition University, calls these categories ACAT, for Avoid, Control, Accept, or Transfer. This use of the ACAT acronym is reminiscent of another ACAT (for Acquisition Category) used in US Defense industry procurements, in which Risk Management figures prominently in decision making and planning.

This includes not performing an activity that could carry risk. An example would be not buying a property or business in order to not take on the legal liability that comes with it. Another would be not flying in order not to take the risk that the airplane were to be hijacked. Avoidance may seem the answer to all risks, but avoiding risks also means losing out on the potential gain that accepting (retaining) the risk may have allowed. Not entering a business to avoid the risk of loss also avoids the possibility of earning profits. Increasing risk regulation in hospitals has led to avoidance of treating higher risk conditions, in favor of patients presenting with lower risk.

Risk reduction or "optimization" involves reducing the severity of the loss or the likelihood of the loss from occurring. For example, sprinklers are designed to put out a fire to reduce the risk of loss by fire. This method may cause a greater loss by water damage and therefore may not be suitable. Halon fire suppression systems may mitigate that risk, but the cost may be prohibitive as a strategy.

Acknowledging that risks can be positive or negative, optimizing risks means finding a balance between negative risk and the benefit of the operation or activity; and between risk reduction and effort applied. By an offshore drilling contractor effectively applying Health, Safety and Environment (HSE) management in its organization, it can optimize risk to achieve levels of residual risk that are tolerable.

Modern software development methodologies reduce risk by developing and delivering software incrementally. Early methodologies suffered from the fact that they only delivered software in the final phase of development; any problems encountered in earlier phases meant costly rework and often jeopardized the whole project. By developing in iterations, software projects can limit effort wasted to a single iteration.

Outsourcing could be an example of risk reduction if the outsourcer can demonstrate higher capability at managing or reducing risks. For example, a company may outsource only its software development, the manufacturing of hard goods, or customer support needs to another company, while handling the business management itself. This way, the company can concentrate more on business development without having to worry as much about the manufacturing process, managing the development team, or finding a physical location for a call center.

Briefly defined as "sharing with another party the burden of loss or the benefit of gain, from a risk, and the measures to reduce a risk."

The term of 'risk transfer' is often used in place of risk sharing in the mistaken belief that you can transfer a risk to a third party through insurance or outsourcing. In practice if the insurance company or contractor go bankrupt or end up in court, the original risk is likely to still revert to the first party. As such in the terminology of practitioners and scholars alike, the purchase of an insurance contract is often described as a "transfer of risk." However, technically speaking, the buyer of the contract generally retains legal responsibility for the losses "transferred", meaning that insurance may be described more accurately as a post-event compensatory mechanism. For example, a personal injuries insurance policy does not transfer the risk of a car accident to the insurance company. The risk still lies with the policy holder namely the person who has been in the accident. The insurance policy simply provides that if an accident (the event) occurs involving the policy holder then some compensation may be payable to the policy holder that is commensurate with the suffering/damage.

Some ways of managing risk fall into multiple categories. Risk retention pools are technically retaining the risk for the group, but spreading it over the whole group involves transfer among individual members of the group. This is different from traditional insurance, in that no premium is exchanged between members of the group up front, but instead losses are assessed to all members of the group.

Involves accepting the loss, or benefit of gain, from a risk when it occurs. True self-insurance falls in this category. Risk retention is a viable strategy for small risks where the cost of insuring against the risk would be greater over time than the total losses sustained. All risks that are not avoided or transferred are retained by default. This includes risks that are so large or catastrophic that either they cannot be insured against or the premiums would be infeasible. War is an example since most property and risks are not insured against war, so the loss attributed by war is retained by the insured. Also any amounts of potential loss (risk) over the amount insured is retained risk. This may also be acceptable if the chance of a very large loss is small or if the cost to insure for greater coverage amounts is so great that it would hinder the goals of the organization too much.

Select appropriate controls or countermeasures to mitigate each risk. Risk mitigation needs to be approved by the appropriate level of management. For instance, a risk concerning the image of the organization should have top management decision behind it whereas IT management would have the authority to decide on computer virus risks.

The risk management plan should propose applicable and effective security controls for managing the risks. For example, an observed high risk of computer viruses could be mitigated by acquiring and implementing antivirus software. A good risk management plan should contain a schedule for control implementation and responsible persons for those actions.

According to ISO/IEC 27001, the stage immediately after completion of the risk assessment phase consists of preparing a Risk Treatment Plan, which should document the decisions about how each of the identified risks should be handled. Mitigation of risks often means selection of security controls, which should be documented in a Statement of Applicability, which identifies which particular control objectives and controls from the 
standard have been selected, and why.

Implementation follows all of the planned methods for mitigating the effect of the risks. Purchase insurance policies for the risks that it has been decided to transferred to an insurer, avoid all risks that can be avoided without sacrificing the entity's goals, reduce others, and retain the rest.

Initial risk management plans will never be perfect. Practice, experience, and actual loss results will necessitate changes in the plan and contribute information to allow possible different decisions to be made in dealing with the risks being faced.

Risk analysis results and management plans should be updated periodically. There are two primary reasons for this: 

Prioritizing the "risk management processes" too highly could keep an organization from ever completing a project or even getting started. This is especially true if other work is suspended until the risk management process is considered complete.

It is also important to keep in mind the distinction between risk and uncertainty. Risk can be measured by impacts × probability.

If risks are improperly assessed and prioritized, time can be wasted in dealing with risk of losses that are not likely to occur. Spending too much time assessing and managing unlikely risks can divert resources that could be used more profitably. Unlikely events do occur but if the risk is unlikely enough to occur it may be better to simply retain the risk and deal with the result if the loss does in fact occur. Qualitative risk assessment is subjective and lacks consistency. The primary justification for a formal risk assessment process is legal and bureaucratic.

As applied to corporate finance, "risk management" is the technique for measuring, monitoring and controlling the financial or operational risk on a firm's balance sheet, a traditional measure is the value at risk (VaR), but there also other measures like profit at risk (PaR) or margin at risk. The Basel II framework breaks risks into market risk (price risk), credit risk and operational risk and also specifies methods for calculating capital requirements for each of these components.

In Information Technology, Risk management includes "Incident Handling", an action plan for dealing with intrusions, cyber-theft, denial of service, fire, floods, and other security-related events. According to the SANS Institute, it is a six step process: Preparation, Identification, Containment, Eradication, Recovery, and Lessons Learned.

In enterprise risk management, a risk is defined as a possible event or circumstance that can have negative influences on the enterprise in question. Its impact can be on the very existence, the resources (human and capital), the products and services, or the customers of the enterprise, as well as external impacts on society, markets, or the environment. In a financial institution, enterprise risk management is normally thought of as the combination of credit risk, interest rate risk or asset liability management, liquidity risk, market risk, and operational risk.

In the more general case, every probable risk can have a pre-formulated plan to deal with its possible consequences (to ensure "contingency" if the risk becomes a "liability").

From the information above and the average cost per employee over time, or cost accrual ratio, a project manager can estimate:


Risk in a project or process can be due either to Special Cause Variation or Common Cause Variation and requires appropriate treatment. That is to re-iterate the concern about extremal cases not being equivalent in the list immediately above.

ESRM is a security program management approach that links security activities to an enterprise's mission and business goals through risk management methods. The security leader's role in ESRM is to manage risks of harm to enterprise assets in partnership with the business leaders whose assets are exposed to those risks. ESRM involves educating business leaders on the realistic impacts of identified risks, presenting potential strategies to mitigate those impacts, then enacting the option chosen by the business in line with accepted levels of business risk tolerance

For medical devices, risk management is a process for identifying, evaluating and mitigating risks associated with harm to people and damage to property or the environment. Risk management is an integral part of medical device design and development, production processes and evaluation of field experience, and is applicable to all types of medical devices. The evidence of its application is required by most regulatory bodies such as the US FDA. The management of risks for medical devices is described by the International Organization for Standardization (ISO) in ISO 14971:2007, Medical Devices—The application of risk management to medical devices, a product safety standard. The standard provides a process framework and associated requirements for management responsibilities, risk analysis and evaluation, risk controls and lifecycle risk management.

The European version of the risk management standard was updated in 2009 and again in 2012 to refer to the Medical Devices Directive (MDD) and Active Implantable Medical Device Directive (AIMDD) revision in 2007, as well as the In Vitro Medical Device Directive (IVDD). The requirements of EN 14971:2012 are nearly identical to ISO 14971:2007. The differences include three "(informative)" Z Annexes that refer to the new MDD, AIMDD, and IVDD. These annexes indicate content deviations that include the requirement for risks to be reduced "as far as possible", and the requirement that risks be mitigated by design and not by labeling on the medical device (i.e., labeling can no longer be used to mitigate risk).

Typical risk analysis and evaluation techniques adopted by the medical device industry include hazard analysis, fault tree analysis (FTA), failure mode and effects analysis (FMEA), hazard and operability study (HAZOP), and risk traceability analysis for ensuring risk controls are implemented and effective (i.e. tracking risks identified to product requirements, design specifications, verification and validation results etc.). FTA analysis requires diagramming software. FMEA analysis can be done using a spreadsheet program. There are also integrated medical device risk management solutions.

Through a draft guidance, the FDA has introduced another method named "Safety Assurance Case" for medical device safety assurance analysis. The safety assurance case is structured argument reasoning about systems appropriate for scientists and engineers, supported by a body of evidence, that provides a compelling, comprehensible and valid case that a system is safe for a given application in a given environment. With the guidance, a safety assurance case is expected for safety critical devices (e.g. infusion devices) as part of the pre-market clearance submission, e.g. 510(k). In 2013, the FDA introduced another draft guidance expecting medical device manufacturers to submit cybersecurity risk analysis information.

Project risk management must be considered at the different phases of acquisition. In the beginning of a project, the advancement of technical developments, or threats presented by a competitor's projects, may cause a risk or threat assessment and subsequent evaluation of alternatives (see Analysis of Alternatives). Once a decision is made, and the project begun, more familiar project management applications can be used:


Megaprojects (sometimes also called "major programs") are large-scale investment projects, typically costing more than $1 billion per project. Megaprojects include major bridges, tunnels, highways, railways, airports, seaports, power plants, dams, wastewater projects, coastal flood protection schemes, oil and natural gas extraction projects, public buildings, information technology systems, aerospace projects, and defense systems. Megaprojects have been shown to be particularly risky in terms of finance, safety, and social and environmental impacts. Risk management is therefore particularly pertinent for megaprojects and special methods and special education have been developed for such risk management.

It is important to assess risk in regard to natural disasters like floods, earthquakes, and so on. Outcomes of natural disaster risk assessment are valuable when considering future repair costs, business interruption losses and other downtime, effects on the environment, insurance costs, and the proposed costs of reducing the risk. The Sendai Framework for Disaster Risk Reduction is a 2015 international accord that has set goals and targets for disaster risk reduction in response to natural disasters. There are regular International Disaster and Risk Conferences in Davos to deal with integral risk management.

IT risk is a risk related to information technology. This is a relatively new term due to an increasing awareness that information security is simply one facet of a multitude of risks that are relevant to IT and the real world processes it supports.

ISACA's "Risk IT" framework ties IT risk to enterprise risk management.

For the offshore oil and gas industry, operational risk management is regulated by the safety case regime in many countries. Hazard identification and risk assessment tools and techniques are described in the international standard ISO 17776:2000, and organisations such as the IADC (International Association of Drilling Contractors) publish guidelines for Health, Safety and Environment (HSE) Case development which are based on the ISO standard. Further, diagrammatic representations of hazardous events are often expected by governmental regulators as part of risk management in safety case submissions; these are known as bow-tie diagrams (see Network theory in risk assessment). The technique is also used by organisations and regulators in mining, aviation, health, defence, industrial and finance.

The principles and tools for quality risk management are increasingly being applied to different aspects of pharmaceutical quality systems. These aspects include development, manufacturing, distribution, inspection, and submission/review processes throughout the lifecycle of drug substances, drug products, biological and biotechnological products (including the use of raw materials, solvents, excipients, packaging and labeling materials in drug products, biological and biotechnological products). Risk management is also applied to the assessment of microbiological contamination in relation to pharmaceutical products and cleanroom manufacturing environments.

Risk communication is a complex cross-disciplinary academic field related to core values of the targeted audiences. Problems for risk communicators involve how to reach the intended audience, how to make the risk comprehensible and relatable to other risks, how to pay appropriate respect to the audience's values related to the risk, how to predict the audience's response to the communication, etc. A main goal of risk communication is to improve collective and individual decision making. Risk communication is somewhat related to crisis communication. Some experts coincide that risk is not only enrooted in the communication process but also it cannot be dissociated from the use of language. Though each culture develops its own fears and risks, these construes apply only by the hosting culture.




</doc>
<doc id="26408" url="https://en.wikipedia.org/wiki?curid=26408" title="Rainer Maria Rilke">
Rainer Maria Rilke

René Karl Wilhelm Johann Josef Maria Rilke (4 December 1875 – 29 December 1926), better known as Rainer Maria Rilke (), was a Bohemian-Austrian poet and novelist. He is "widely recognized as one of the most lyrically intense German-language poets". He wrote both verse and highly-lyrical prose. Several critics have described Rilke's work as inherently "mystical". His writings include one novel, several collections of poetry and several volumes of correspondence in which he invokes haunting images that focus on the difficulty of communion with the ineffable in an age of disbelief, solitude and profound anxiety. These deeply existential themes tend to position him as a transitional figure between the traditional and the modernist writers.

Rilke travelled extensively throughout Europe (including Russia, Spain, Germany, France and Italy), and in his later years settled in Switzerland—settings that were key to the genesis and inspiration for many of his poems. While Rilke is most known for his contributions to German literature, over 400 poems were originally written in French and dedicated to the canton of Valais in Switzerland. Among English-language readers, his best-known works include the poetry collections "Duino Elegies" (') and "Sonnets to Orpheus" ('), the semi-autobiographical novel "The Notebooks of Malte Laurids Brigge" ('), and a collection of ten letters that was published after his death under the title "Letters to a Young Poet" ('). In the later 20th century, his work found new audiences through use by New Age theologians and self-help authors and frequent quotations by television programs, books and motion pictures. In the United States, Rilke remains among the more popular, best-selling poets.

He was born René Karl Wilhelm Johann Josef Maria Rilke in Prague, capital of Bohemia (then part of Austria-Hungary, now part of the Czech Republic). His childhood and youth in Prague were not especially happy. His father, Josef Rilke (1838–1906), became a railway official after an unsuccessful military career. His mother, Sophie (“Phia”) Entz (1851–1931), came from a well-to-do Prague family, the Entz-Kinzelbergers, who lived in a house on the Herrengasse (Panská) 8, where René also spent many of his early years. The relationship between Phia and her only son was coloured by her mourning for an earlier child, a daughter who had died only one week old. During Rilke's early years Phia acted as if she sought to recover the lost girl through the boy by dressing him in girl's clothing. His parents' marriage failed in 1884. His parents pressured the poetically and artistically talented youth into entering a military academy in St. Pölten, Lower Austria, which he attended from 1886 until 1891, when he left owing to illness. He moved to Linz, where he attended trade school. Expelled from school in May 1892, the 16-year-old prematurely returned to Prague. From 1892 to 1895 he was tutored for the university entrance exam, which he passed in 1895. Until 1896 he studied literature, art history, and philosophy in Prague and Munich.

In 1897 in Munich, Rainer Maria Rilke met and fell in love with the widely travelled, intellectual woman of letters Lou Andreas-Salomé. Rilke changed his first name from “René” to “Rainer” at Lou's urging because she thought that name to be more masculine, forceful, and Germanic. His relationship with this married woman, with whom he undertook two extensive trips to Russia, lasted until 1900. But even after their separation, Lou continued to be Rilke's most important confidante until the end of his life. Having trained from 1912 to 1913 as a psychoanalyst with Sigmund Freud, she shared her knowledge of psychoanalysis with Rilke.

In 1898, Rilke undertook a journey lasting several weeks to Italy. In 1899, he travelled with Lou and her husband, Friedrich Andreas, to Moscow where he met the novelist Leo Tolstoy. Between May and August 1900, a second journey to Russia, accompanied only by Lou, again took him to Moscow and Saint Petersburg, where he met the family of Boris Pasternak and Spiridon Drozhzhin, a peasant poet. Author Anna A. Tavis cites the cultures of Bohemia and Russia as the key influences on Rilke's poetry and consciousness.

In 1900, Rilke stayed at the artists' colony at Worpswede. (Later, his portrait would be painted by the proto-expressionist Paula Modersohn-Becker, whom he got to know at Worpswede.) It was here that he got to know the sculptor Clara Westhoff, whom he married the following year. Their daughter Ruth (1901–1972) was born in December 1901.

In the summer of 1902, Rilke left home and travelled to Paris to write a monograph on the sculptor Auguste Rodin. Before long his wife left their daughter with her parents and joined Rilke there. The relationship between Rilke and Clara Westhoff continued for the rest of his life; a mutually-agreed-upon effort at divorce was bureaucratically hindered by Rilke's “official” status as a Catholic, though a non-practising one.

At first, Rilke had a difficult time in Paris, an experience that he called on in the first part of his only novel, "The Notebooks of Malte Laurids Brigge". At the same time, his encounter with modernism was very stimulating: Rilke became deeply involved with the sculpture of Rodin, then the work of Paul Cézanne. For a time, he acted as Rodin's secretary, also lecturing and writing a long essay on Rodin and his work. Rodin taught him the value of objective observation and, under this influence, Rilke dramatically transformed his poetic style from the subjective and sometimes incantatory language of his earlier work into something quite new in European literature. The result was the "New Poems", famous for the “thing-poems” expressing Rilke's rejuvenated artistic vision. During these years, Paris increasingly became the writer's main residence.

The most important works of the Paris period were "Neue Gedichte" ("New Poems") (1907), "Der Neuen Gedichte Anderer Teil" ("Another Part of the New Poems") (1908), the two “Requiem” poems (1909), and the novel "The Notebooks of Malte Laurids Brigge", started in 1904 and completed in January 1910.

During the later part of this decade, Rilke spent extended periods in Ronda, the famous bullfighting centre in southern Spain. There he kept from December 1912 to February 1913 a permanent room at the Hotel Reina Victoria (built in 1906), 

Between October 1911 and May 1912, Rilke stayed at the Castle Duino, near Trieste, home of Princess Marie of Thurn und Taxis. There, in 1912, he began the poem cycle called the "Duino Elegies", which would remain unfinished for a decade because of a long-lasting creativity crisis. Rilke had developed an admiration for El Greco as early as 1908, so he visited Toledo during the winter of 1912/13 to see Greco's paintings. It has been suggested that Greco's manner of depicting angels has influenced the conception of the angel in the Duino Elegies.
The outbreak of World War I surprised Rilke during a stay in Germany. He was unable to return to Paris, where his property was confiscated and auctioned. He spent the greater part of the war in Munich. From 1914 to 1916 he had a turbulent affair with the painter Lou Albert-Lasard. Rilke was called up at the beginning of 1916 and had to undertake basic training in Vienna. Influential friends interceded on his behalf—he was transferred to the War Records Office and discharged from the military on 9 June 1916. He returned to Munich, interrupted by a stay on 's Gut Bockel in Westphalia. The traumatic experience of military service, a reminder of the horrors of the military academy, almost completely silenced him as a poet.

On 11 June 1919, Rilke travelled from Munich to Switzerland. The outward motive was an invitation to lecture in Zurich, but the real reason was the wish to escape the post-war chaos and take up his work on the "Duino Elegies" once again. The search for a suitable and affordable place to live proved to be very difficult. Among other places, Rilke lived in Soglio, Locarno and Berg am Irchel. Only in mid-1921 was he able to find a permanent residence in the Château de Muzot in the commune of Veyras, close to Sierre in Valais. In an intense creative period, Rilke completed the "Duino Elegies" in several weeks in February 1922. Before and after, Rilke rapidly wrote both parts of the poem cycle "Sonnets to Orpheus" containing 55 entire sonnets. Together, these two have often been taken as constituting the high points of Rilke's work. In May 1922, Rilke's patron Werner Reinhart bought and renovated Muzot so that Rilke could live there rent-free.

During this time, Reinhart introduced Rilke to his protégée, the Australian violinist Alma Moodie. Rilke was so impressed with her playing that he wrote in a letter: “What a sound, what richness, what determination. That and the "Sonnets to Orpheus", those were two strings of the same voice. And she plays mostly Bach! Muzot has received its musical christening...”

From 1923 on, Rilke increasingly had to struggle with health problems that necessitated many long stays at a sanatorium in Territet, near Montreux, on Lake Geneva. His long stay in Paris between January and August 1925 was an attempt to escape his illness through a change in location and living conditions. Despite this, numerous important individual poems appeared in the years 1923–1926 (including "Gong" and "Mausoleum"), as well as the abundant lyrical work in French.

In 1924, Erika Mitterer began writing poems to Rilke, who wrote back with approximately fifty poems of his own and called her verse a "Herzlandschaft" (landscape of the heart). This was the only time Rilke had a productive poetic collaboration throughout all his work. Mitterer also visited Rilke. In 1950, her “Correspondence in Verse” with Rilke was published, and received much praise.

In January and February 1926, Rilke wrote three letters to the Mussolini-adversary Aurelia Gallarati in which he praised Benito Mussolini and described fascism as a healing agent.

Shortly before his death, Rilke's illness was diagnosed as leukaemia. He suffered ulcerous sores in his mouth, pain troubled his stomach and intestines, and he struggled with increasingly low spirits. Open-eyed, he died in the arms of his doctor on December 29, 1926, in the Valmont Sanatorium in Switzerland. He was buried on January 2, 1927, in the Raron cemetery to the west of Visp.

Rilke had chosen as his own epitaph this poem:

<poem>Rose, oh reiner Widerspruch, Lust, Niemandes Schlaf zu sein unter soviel Lidern.</poem>

<poem>Rose, o pure contradiction, desire
to be no one's sleep beneath so many
lids.</poem>
A myth developed surrounding his death and roses. It was said: “To honour a visitor, the Egyptian beauty Nimet Eloui Bey, Rilke gathered some roses from his garden. While doing so, he pricked his hand on a thorn. This small wound failed to heal, grew rapidly worse, soon his entire arm was swollen, and his other arm became affected as well”, and so he died.

Rilke published the three complete cycles of poems that constitute "The Book of Hours" ("") in April 1905. These poems explore the Christian search for God and the nature of Prayer, using symbolism from Saint Francis and Rilke's observation of Orthodox Christianity during his travels in Russia in the early years of the twentieth century.

Rilke wrote his only novel, "" (translated as "The Notebooks of Malte Laurids Brigge") while living in Paris, completing the work in 1910. This semi-autobiographical novel adopts the style and technique that became associated with Expressionism, which entered European fiction and art in the early 20th century. Rilke was inspired by Sigbjørn Obstfelder's work "A Priest's Diary" and Jens Peter Jacobsen's second novel "Niels Lyhne" (1880), which traces the fate of an atheist in a merciless world. Rilke addresses existential themes, profoundly probing the quest for individuality, the significance of death, and reflection on the experience of time as death approaches. Rilke draws considerably on the writings of Nietzsche, whose work he came to know through Lou-Andreas Salomé. His work also incorporates impressionistic techniques that were influenced by Cézanne and Rodin (to whom Rilke was secretary in 1905–1906). He combines these techniques and motifs to conjure images of mankind's anxiety and alienation in the face of an increasingly scientific, industrial, reified world.

Rilke began writing the elegies in 1912 while a guest of Princess Marie von Thurn und Taxis (1855–1934) at Duino Castle, near Trieste on the Adriatic Sea. During this ten-year period, the elegies languished incomplete for long stretches of time as Rilke suffered frequently from severe depression—some of which was caused by the events of World War I and his conscripted military service. Aside from brief episodes of writing in 1913 and 1915, Rilke did not return to the work until a few years after the war ended. With a sudden, renewed inspiration—writing in a frantic pace he described as “a savage creative storm”—he completed the collection in February 1922 while staying at Château de Muzot in Veyras, in Switzerland's Rhone Valley. After their publication and his death shortly thereafter, the "Duino Elegies" were quickly recognized by critics and scholars as Rilke's most important work.

The "Duino Elegies" are intensely religious, mystical poems that weigh beauty and existential suffering. The poems employ a rich symbolism of angels and salvation but not in keeping with typical Christian interpretations. Rilke begins the first elegy in an invocation of philosophical despair, asking: “Who, if I cried out, would hear me among the hierarchies of angels?” ("Wer, wenn ich schriee, hörte mich denn aus der Engel Ordnungen?") and later declares that “every angel is terrifying” ("Jeder Engel ist schrecklich"). While labelling of these poems as "elegies" would typically imply melancholy and lamentation, many passages are marked by their positive energy and “unrestrained enthusiasm.” Together, the "Duino Elegies" are described as a metamorphosis of Rilke's “ontological torment” and an “impassioned monologue about coming to terms with human existence” discussing themes of “the limitations and insufficiency of the human condition and fractured human consciousness ... man's loneliness, the perfection of the angels, life and death, love and lovers, and the task of the poet.”

With news of the death of his daughter's friend, Wera Knoop (1900–1919), Rilke was inspired to create and set to work on "Sonnets to Orpheus". Within a few days, between February 2 and 5, 1922, he had completed the first section of 26 sonnets. For the next few days, he focused on the "Duino Elegies", completing them on the evening of February 11. Immediately after, he returned to work on the "Sonnets" and completed the following section of 29 sonnets in less than two weeks. Throughout the "Sonnets", Wera is frequently referenced, both directly by name and indirectly in allusions to a “dancer” and the mythical Eurydice. Although Rilke claimed that the entire cycle was inspired by Wera, she appears as a character in only one of the poems. He insisted, however, that “Wera's own figure [...] nevertheless governs and moves the course of the whole.”

The sonnets' contents are, as is typical of Rilke, highly metaphorical. The character of Orpheus (whom Rilke refers to as the “god with the lyre”) appears several times in the cycle, as do other mythical characters such as Daphne. There are also biblical allusions, including a reference to Esau. Other themes involve animals, peoples of different cultures, and time and death.

In 1929, a minor writer, Franz Xaver Kappus (1883–1966), published a collection of ten letters that Rilke had written to him when he was a 19-year-old officer cadet studying at the Theresian Military Academy in Wiener Neustadt. The young Kappus wrote to Rilke, who had also attended the academy, between 1902 and 1908 when he was uncertain about his future career as a military officer or as a poet. Initially, he sought Rilke's advice as to the quality of his poetry, and whether he ought to pursue writing as a career. While he declined to comment on Kappus's writings, Rilke advised Kappus on how a poet should feel, love, and seek truth in trying to understand and experience the world around him and engage the world of art. These letters offer insight into the ideas and themes that appear in Rilke's poetry and his working process. Further, these letters were written during a key period of Rilke's early artistic development after his reputation as a poet began to be established with the publication of parts of "Das Stunden-Buch" ("The Book of Hours") and "Das Buch der Bilder" ("The Book of Images").

Figures from Greek mythology (such as Apollo, Hermes and Orpheus) recur as motifs in his poems and are depicted in original interpretations (e.g. in the poem "Orpheus. Eurydice. Hermes", Rilke's Eurydice, numbed and dazed by death, does not recognize her lover Orpheus, who descended to hell to recover her). Other recurring figures in Rilke's poems are angels, roses and a character of a poet and his creative work.

Rilke often worked with metaphors, metonymy and contradictions (e.g. in his epitaph, the rose is a symbol of sleep—rose petals are reminiscent of closed eyelids).

Rilke's little-known 1898 poem, “Visions of Christ” depicted Mary Magdalene as the mother to Jesus' child.

Quoting Susan Haskins: “It was Rilke's explicit belief that Christ was not divine, was entirely human, and deified only on Calvary, expressed in an unpublished poem of 1893, and referred to in other poems of the same period, which allowed him to portray Christ's love for Mary Magdalen, though remarkable, as entirely human.”

In the United States, Rilke is one of the more popular, best-selling poets. In popular culture, Rilke is frequently quoted or referenced in television programs, motion pictures, music and other works when these works discuss the subject of love or angels. His work is often described as “mystical” and has been seized by the New Age community and self-help books. Rilke has been reinterpreted “as a master who can lead us to a more fulfilled and less anxious life.”

Rilke's work (specifically the "Duino Elegies") has deeply influenced several poets and writers, including William H. Gass,Galway Kinnell, Sidney Keyes, Stephen Spender, Robert Bly, W. S. Merwin, John Ashbery, novelist Thomas Pynchon and philosophers Ludwig Wittgenstein and Hans-Georg Gadamer. British poet W. H. Auden (1907–1973) has been described as "Rilke's most influential English disciple" and he frequently "paid homage to him" or used the imagery of angels in his work.




Collected letters

Other volumes of letters






</doc>
<doc id="26409" url="https://en.wikipedia.org/wiki?curid=26409" title="Richard Doyle">
Richard Doyle

Richard Doyle may refer to:



</doc>
<doc id="26410" url="https://en.wikipedia.org/wiki?curid=26410" title="Conversion therapy">
Conversion therapy

Conversion therapy is the pseudoscientific practice of trying to change an individual's sexual orientation from homosexual or bisexual to heterosexual using psychological or spiritual interventions. There is virtually no reliable evidence that sexual orientation can be changed and medical bodies warn that conversion therapy practices are ineffective and potentially seriously harmful. Nevertheless, advocates and proponents do provide anecdotal reports of so-called "ex-gays" who claim some degree of success in becoming heterosexual. Medical, scientific, and government organizations in the United States and United Kingdom have expressed concern over conversion therapy and consider it potentially harmful. Various legal jurisdictions in Asia, Europe, and the Americas have passed laws against conversion therapy.

The American Psychiatric Association (APA) opposes psychiatric treatment "based upon the assumption that homosexuality "per se" is a mental disorder or based upon the "a priori" assumption that a patient should change his/her sexual homosexual orientation" and describes attempts to change sexual orientation by practitioners as unethical. It also states that debates over the integration of gay and lesbian people have obscured science "by calling into question the motives and even the character of individuals on both sides of the issue" and that the advancement of conversion therapy may cause social harm by disseminating unscientific views about sexual orientation. United States Surgeon General David Satcher in 2001 issued a report stating that "there is no valid scientific evidence that sexual orientation can be changed".

The highest-profile advocates of conversion therapy today tend to be fundamentalist Christian groups and other organizations which use a religious justification for the therapy rather than speaking of homosexuality as "a disease". The main organization advocating secular forms of conversion therapy is the National Association for Research & Therapy of Homosexuality (NARTH), which often partners with religious groups.

Techniques used in conversion therapy prior to 1981 in the United States and Western Europe included ice-pick lobotomies; chemical castration with hormonal treatment; aversive treatments, such as "the application of electric shock to the hands and/or genitals"; "nausea-inducing drugs ... administered simultaneously with the presentation of homoerotic stimuli"; and masturbatory reconditioning. More recent clinical techniques used in the United States have been limited to counseling, visualization, social skills training, psychoanalytic therapy, and spiritual interventions such as "prayer and group support and pressure", though there are some reports of aversive treatments through unlicensed practice as late as the 1990s. The term reparative therapy has been used as a synonym for conversion therapy in general, but it has been argued that strictly speaking it refers to a specific kind of therapy associated with the psychologists Elizabeth Moberly and Joseph Nicolosi.

The history of conversion therapy can be divided broadly into three periods: an early Freudian period; a period of mainstream approval of conversion therapy, when the mental health establishment became the "primary superintendent" of sexuality; and a post-Stonewall period where the mainstream medical profession disavowed conversion therapy.

During the earliest parts of psychoanalytic history, analysts granted that homosexuality was non-pathological in certain cases, and the ethical question of whether it ought to be changed was discussed. By the 1920s analysts assumed that homosexuality was pathological and that attempts to treat it were appropriate, although psychoanalytic opinion about changing homosexuality was largely pessimistic. Those forms of homosexuality that were considered perversions were usually held to be incurable. Analysts' tolerant statements about homosexuality arose from recognition of the difficulty of achieving change. Beginning in the 1930s and continuing for roughly twenty years, major changes occurred in how analysts viewed homosexuality, which involved a shift in the rhetoric of analysts, some of whom felt free to ridicule and abuse their gay patients.

Sigmund Freud was a physician and the founder of psychoanalysis. Freud stated that homosexuality could sometimes be removed through hypnotic suggestion, and was influenced by Eugen Steinach, a Viennese endocrinologist who transplanted testicles from straight men into gay men in attempts to change their sexual orientation, stating that his research had "thrown a strong light on the organic determinants of homo-eroticism". Freud cautioned that Steinach's operations would not necessarily make possible a therapy that could be generally applied, arguing that such transplant procedures would be effective in changing homosexuality in men only in cases in which it was strongly associated with physical characteristics typical of women, and that probably no similar therapy could be applied to lesbianism. Steinach's method was doomed to failure because the immune system rejects transplanted glands, and was eventually exposed as ineffective and often harmful.

Freud's main discussion of female homosexuality was the 1920 paper "The Psychogenesis of a Case of Homosexuality in a Woman", which described his analysis of a young woman who had entered therapy because her parents were concerned that she was a lesbian. Her father wanted this condition changed. In Freud's view, the prognosis was unfavourable because of the circumstances under which she entered therapy, and because homosexuality was not an illness or neurotic conflict. Freud wrote that changing homosexuality was difficult and possible only under unusually favourable conditions, observing that "in general to undertake to convert a fully developed homosexual into a heterosexual does not offer much more prospect of success than the reverse". Success meant making heterosexual feeling possible, not eliminating homosexual feelings.

Gay people could seldom be convinced that heterosexual sex would provide them with the same pleasure they derived from homosexual sex. Patients often wanted to become heterosexual for reasons Freud considered superficial, including fear of social disapproval, an insufficient motive for change. Some might have no real desire to become heterosexual, seeking treatment only to convince themselves that they had done everything possible to change, leaving them free to return to homosexuality after the failure they expected.

In 1935, a mother asked Freud to treat her son. Freud replied in a letter that later became famous:

Sándor Ferenczi was an influential psychoanalyst. Ferenczi hoped to cure some kinds of homosexuality completely, but was content in practice with reducing what he considered gay men's hostility to women, along with the urgency of their homosexual desires, and with helping them to become attracted to and potent with women. In his view, a gay man who was confused about his sexual identity and felt himself to be "a woman with the wish to be loved by a man" was not a promising candidate for cure. Ferenczi believed that complete cures of homosexuality might become possible in the future when psychoanalytic technique had been improved.

Daughter of Sigmund Freud, Anna Freud became an influential psychoanalytic theorist in the UK.

Anna Freud reported the successful treatment of homosexuals as neurotics in a series of unpublished lectures. In 1949 she published "Some Clinical Remarks Concerning the Treatment of Cases of Male Homosexuality" in the "International Journal of Psychoanalysis". In her view, it was important to pay attention to the interaction of passive and active homosexual fantasies and strivings, the original interplay of which prevented adequate identification with the father. The patient should be told that his choice of a passive partner allows him to enjoy a passive or receptive mode, while his choice of an active partner allows him to recapture his lost masculinity. She claimed that these interpretations would reactivate repressed castration anxieties, and childhood narcissistic grandiosity and its complementary fear of dissolving into nothing during heterosexual intercourse would come with the renewal of heterosexual potency.

Anna Freud in 1951 published "Clinical Observations on the Treatment of Male Homosexuality" in "The Psychoanalytic Quarterly" and "Homosexuality" in the "American Psychoanalytic Association (APsaA) Bulletin". In these articles, she insisted on the attainment of full object-love of the opposite sex as a requirement for cure of homosexuality. In 1951 she gave a lecture about treatment of homosexuality which was criticised by Edmund Bergler, who emphasised the oral fears of patients and minimized the importance of the phallic castration fears she had discussed.

Anna Freud recommended in 1956 to a journalist who was preparing an article about psychoanalysis for "The Observer" of London that she not quote Freud's letter to the American mother, on the grounds that "nowadays we can cure many more homosexuals than was thought possible in the beginning. The other reason is that readers may take this as a confirmation that all analysis can do is to convince patients that their defects or 'immoralities' do not matter and that they should be happy with them. That would be unfortunate."

Melanie Klein was a pupil of Ferenczi. Her seminal book "The Psycho-Analysis of Children", based on lectures given to the British Psychoanalytical Society in the 1920s, was published in 1932. Klein claimed that entry into the Oedipus Complex is based on mastery of primitive anxiety from the oral and anal stages. If these tasks are not performed properly, developments in the Oedipal stage will be unstable. Complete analysis of patients with such unstable developments would require uncovering these early concerns. The analysis of homosexuality required dealing with paranoid trends based on the oral stage. "The Psycho-Analysis of Children" ends with the analysis of Mr. B., a gay man. Klein claimed that he illustrated pathologies that enter into all forms of homosexuality: a gay man idealizes "the good penis" of his partner to allay the fear of attack he feels due to having projected his paranoid hatred onto the imagined "bad penis" of his mother as an infant. She stated that Mr. B.'s homosexual behaviour diminished after he overcame his need to adore the "good penis" of an idealized man. This was made possible by his recovering his belief in the good mother and his ability to sexually gratify her with his good penis and plentiful semen.

In March 2018, a majority of 435 against 109 representatives in the European parliament passed a resolution condemning conversion therapy and urging European Union member states to ban the practice.

Psychoanalysis started to receive recognition in the United States in 1909, when Sigmund Freud delivered a series of lectures at Clark University in Massachusetts at the invitation of G. Stanley Hall. In 1913, Abraham Brill wrote "The Conception of Homosexuality", which he published in the "Journal of the American Medical Association" and read before the American Medical Association's annual meeting. Brill criticised physical treatments for homosexuality such as bladder washing, rectal massage, and castration, along with hypnosis, but referred approvingly to Freud and Sadger's use of psychoanalysis, calling its results "very gratifying". Since Brill understood curing homosexuality as restoring heterosexual potency, he claimed that he had cured his patients in several cases, even though many remained homosexual.

Wilhelm Stekel, an Austrian, published his views on treatment of homosexuality, which he considered a disease, in the American "Psychoanalytic Review" in 1930. Stekel believed that "success was fairly certain" in changing homosexuality through psychoanalysis provided that it was performed correctly and the patient wanted to be treated. In 1932, "The Psychoanalytic Quarterly" published a translation of Helene Deutsch's paper "On Female Homosexuality". Deutsch reported her analysis of a lesbian, who did not become heterosexual as a result of treatment, but who managed to achieve a "positive libidinal relationship" with another woman. Deutsch indicated that she would have considered heterosexuality a better outcome.

Edmund Bergler was the most important psychoanalytic theorist of homosexuality in the 1950s. He was vociferous in his opposition to Alfred Kinsey. Kinsey's work, and its reception, led Bergler to develop his own theories for treatment, which were essentially to "blame the victim", in the evaluation of Jennifer Terry, associate professor of Woman's Studies. Bergler claimed that if gay people wanted to change, and the right therapeutic approach was taken, then they could be cured in 90% of cases. Bergler used confrontational therapy in which gay people were punished in order to make them aware of their masochism. Bergler openly violated professional ethics to achieve this, breaking patient confidentiality in discussing the cases of patients with other patients, bullying them, calling them liars and telling them they were worthless. He insisted that gay people could be cured. Bergler confronted Kinsey because Kinsey thwarted the possibility of cure by presenting homosexuality as an acceptable way of life, which was the basis of the gay rights activism of the time. Bergler popularised his views in the United States in the 1950s using magazine articles and books aimed at non-specialists.

In 1951, the mother who wrote to Freud asking him to treat her son sent Freud's response to the "American Journal of Psychiatry", in which it was published. The 1952 first edition of the American Psychiatric Association's Diagnostic and Statistical Manual of Mental Disorders (DSM-I) classified homosexuality as a mental disorder.

During the three decades between Freud's death in 1939 and the Stonewall riots in 1969, conversion therapy received approval from most of the psychiatric establishment in the United States. In 1962, Irving Bieber "et al." published "", in which they concluded that "although this change may be more easily accomplished by some than by others, in our judgment a heterosexual shift is a possibility for all homosexuals who are strongly motivated to change".

There was a riot in 1969 at the Stonewall Bar in New York after a police raid. The Stonewall riot acquired symbolic significance for the gay rights movement and came to be seen as the opening of a new phase in the struggle for gay liberation. Following these events, conversion therapy came under increasing attack. Activism against conversion therapy increasingly focused on the DSM's designation of homosexuality as a psychopathology. In 1973, after years of criticism from gay activists and bitter dispute among psychiatrists, the American Psychiatric Association removed homosexuality as a mental disorder from the "Diagnostic and Statistical Manual of Mental Disorders". Supporters of the change used evidence from researchers such as Kinsey and Evelyn Hooker. Psychiatrist Robert Spitzer, a member of the APA's Committee on Nomenclature, played an important role in the events that led to this decision. Critics argued that it was a result of pressure from gay activists, and demanded a referendum among voting members of the Association. The referendum was held in 1974 and the APA's decision was upheld by a 58% majority.

The APA removed ego-dystonic homosexuality from the DSM-III-R in 1987 and opposes the diagnosis of either homosexuality or ego-dystonic homosexuality as any type of disorder.

Joseph Nicolosi had a significant role in the development of conversion therapy as early as the 1990s, publishing his first book "Reparative Therapy of Male Homosexuality" in 1991. In 1992, Nicolosi, with Charles Socarides and Benjamin Kaufman, founded the National Association for Research & Therapy of Homosexuality (NARTH), an organization that opposes the mainstream medical view of homosexuality and aims to "make effective psychological therapy available to all homosexual men and women who seek change".

In 1998, Christian right groups including the Family Research Council and the American Family Association spent $600,000 on advertising promoting conversion therapy. John Paulk and his then wife Anne featured in full-page newspaper spreads.

United States Surgeon General David Satcher in 2001 issued a report stating that "there is no valid scientific evidence that sexual orientation can be changed". The same year, a study by Robert Spitzer concluded that some highly motivated individuals whose orientation is predominantly homosexual can become predominantly heterosexual with some form of reparative therapy. Spitzer based his findings on structured interviews with 200 self-selected individuals (143 males, 57 females). He told "The Washington Post" that the study "shows some people can change from gay to straight, and we ought to acknowledge that". Spitzer's study caused controversy and attracted media attention. Spitzer recanted his study in 2012, and apologized to the gay community for making unproven claims of the efficacy of reparative therapy, calling it his only professional regret.

The American Psychoanalytic Association spoke against NARTH in 2004, stating "that organization does not adhere to our policy of nondiscrimination and ... their activities are demeaning to our members who are gay and lesbian". The same year, a survey of members of the American Psychological Association rated reparative therapy as "certainly discredited", though the authors warn that the results should be interpreted carefully as an initial step, not a final word.

The American Psychological Association in 2007 convened a task force to evaluate its policies regarding reparative therapy.

In 2008, the organizers of an APA panel on the relationship between religion and homosexuality canceled the event after gay activists objected that "conversion therapists and their supporters on the religious right use these appearances as a public relations event to try and legitimize what they do".

In 2009, American Psychological Association stated that it "encourages mental health professionals to avoid misrepresenting the efficacy of sexual orientation change efforts by promoting or promising change in sexual orientation when providing assistance to individuals distressed by their own or others' sexual orientation and concludes that the benefits reported by participants in sexual orientation change efforts can be gained through approaches that do not attempt to change sexual orientation".

The ethics guidelines of major mental health organizations in the United States vary from cautionary statements to recommendations that ethical practitioners refrain from practicing conversion therapy (American Psychiatric Association) or from referring patients to those who do (American Counseling Association). In a letter dated February 23, 2011 to the Speaker of the U.S. House of Representatives, the Attorney General of the United States stated "while sexual orientation carries no visible badge, a growing scientific consensus accepts that sexual orientation is a characteristic that is immutable".

Gay rights groups and groups concerned with mental health fear reparative therapy can make depression or even suicide more likely. President Barack Obama expressed opposition to the practice in 2015.

Before the American Psychological Association's 1973 decision to remove homosexuality from the DSM, practitioners of conversion therapy employed aversive conditioning techniques, involving electric shock and nausea-inducing drugs during presentation of same-sex erotic images. Cessation of the aversive stimuli was typically accompanied by the presentation of opposite-sex erotic images, with the objective of strengthening heterosexual feelings. In "Aversion therapy for sexual deviation: a critical review", published in 1966, M. P. Feldman claimed a 58% cure rate, but Douglas Haldeman is skeptical that such stressful methods permit feelings of sexual responsiveness, and notes that Feldman defined success as suppression of homosexuality and increased capacity for heterosexual behavior.

Another method used was the covert sensitization method, which involves instructing patients to imagine vomiting or receiving electric shocks, writing that only single case studies have been conducted, and that their results cannot be generalized. Haldeman writes that behavioral conditioning studies tend to decrease homosexual feelings, but do not increase heterosexual feelings, citing Rangaswami's "Difficulties in arousing and increasing heterosexual responsiveness in a homosexual: A case report", published in 1982, as typical in this respect.

Haldeman concludes that such methods can be called torture, besides being ineffective. He writes that "Individuals undergoing such treatments do not emerge heterosexually inclined; rather they become shamed, conflicted, and fearful about their homosexual feelings."

Some sources describe ex-gay ministries as a form of conversion therapy, while others state that ex-gay organizations and conversion therapy are distinct methods of attempting to convert gay people to heterosexuality. Ex-gay ministries have also been called transformational ministries. Some state that they do not conduct clinical treatment of any kind. Exodus International once believed reparative therapy could be a beneficial tool. The umbrella organization in the United States ceased activities in June 2013, and the three member board issued a statement which repudiated its aims and apologized for the harm their pursuit has caused to LGBT people. The ministries that had been members formed a new organization Restored Hope Network and continue to operate as before with a renewed emphases on spiritual conversion and therapy.

Haldeman writes that psychoanalytic treatment of homosexuality is exemplified by the work of Irving Bieber "et al." in "Homosexuality: A Psychoanalytic Study of Male Homosexuals". They advocated long-term therapy aimed at resolving the unconscious childhood conflicts that they considered responsible for homosexuality. Haldeman notes that Bieber's methodology has been criticized because it relied upon a clinical sample, the description of the outcomes was based upon subjective therapist impression, and follow-up data were poorly presented. Bieber reported a 27% success rate from long-term therapy, but only 18% of the patients in whom Bieber considered the treatment successful had been exclusively homosexual to begin with, while 50% had been bisexual. In Haldeman's view, this makes even Bieber's unimpressive claims of success misleading.

Haldeman discusses other psychoanalytic studies of attempts to change homosexuality. Curran and Parr's "Homosexuality: An analysis of 100 male cases", published in 1957, reported no significant increase in heterosexual behavior. Mayerson and Lief's "Psychotherapy of homosexuals: A follow-up study of nineteen cases", published in 1965, reported that half of its 19 subjects were exclusively heterosexual in behavior four and a half years after treatment, but its outcomes were based on patient self-report and had no external validation. In Haldeman's view, those participants in the study who reported change were bisexual at the outset, and its authors wrongly interpreted capacity for heterosexual sex as change of sexual orientation.

The term "reparative therapy" has been used as a synonym for conversion therapy generally, but according to Jack Drescher it properly refers to a specific kind of therapy associated with the psychologists Elizabeth Moberly and Joseph Nicolosi. The term "reparative" refers to Nicolosi's postulate that same-sex attraction is a person's rational and unconscious attempt to "self-repair" feelings of inferiority.

Most mental health professionals and the American Psychological Association consider reparative therapy discredited, but it is still practiced by some. In 2014 the Republican Party of Texas endorsed "counseling, which offers reparative therapy and treatment" in their party platform. Exodus International regarded reparative therapy as a useful tool to eliminate "unwanted same-sex attraction" but ceased activities in June 2013 and issued a statement repudiating its aims and apologizing for the harm the organization had caused to LGBT people. Psychoanalysts critical of Nicolosi's theories have offered gay-affirmative approaches as an alternative to reparative therapy.

Haldeman has described William Masters' and Virginia Johnson's work on sexual orientation change as a form of conversion therapy.

In "Homosexuality in Perspective", published in 1979, Masters and Johnson viewed homosexuality as the result of blocks that prevented the learning that facilitated heterosexual responsiveness, and described a study of 54 gay men who were dissatisfied with their sexual orientation. The original study did not describe the treatment methodology used, but this was published five years later. John C. Gonsiorek criticized their study on several grounds in 1981, pointing out that while Masters and Johnson stated that their patients were screened for major psychopathology or severe neurosis, they did not explain how this screening was performed, or how the motivation of the patients to change was assessed. Nineteen of their subjects were described as uncooperative during therapy and refused to participate in a follow-up assessment, but all of them were assumed without justification to have successfully changed.

Haldeman writes that Masters and Johnson's study was founded upon heterosexist bias, and that it would be tremendously difficult to replicate. In his view, the distinction Masters and Johnson made between "conversion" (helping gay men with no previous heterosexual experience to learn heterosexual sex) and "reversion" (directing men with some previous heterosexual experience back to heterosexuality) was not well founded. Many of the subjects Masters and Johnson labelled homosexual may not have been homosexual, since, of their participants, only 17% identified themselves as exclusively homosexual, while 83% were in the predominantly heterosexual to bisexual range. Haldeman observed that since 30% of the sample was lost to the follow-up, it is possible that the outcome sample did not include any people attracted mainly or exclusively to the same sex. Haldeman concludes that it is likely that, rather than converting or reverting gay people to heterosexuality, Masters and Johnson only strengthened heterosexual responsiveness in people who were already bisexual.

In the 1940s and 1950s, neurologist Walter Freeman popularized the ice-pick lobotomy to treat homosexuality. He personally performed as many as 3,439 lobotomy surgeries in 23 states, of which 2,500 used his ice-pick procedure, despite the fact that he had no formal surgical training. Up to 40% of Freeman's patients were gay individuals subjected to a lobotomy in order to change their homosexual orientation, leaving most of these individuals severely disabled for the rest of their lives. While promoted at the time as a treatment for various psychoses, the effectiveness of lobotomy in changing sexual orientation was already the subject of critical research in 1948 when a single case was investigated by Joseph Friedlander and Ralph Banay. A video depicting the "ice-pick lobotomy" of a homosexual man was featured in the documentary film, "".

In May 2001, Robert Spitzer presented "Can Some Gay Men and Lesbians Change Their Sexual Orientation? 200 Participants Reporting a Change from Homosexual to Heterosexual Orientation", a study of attempts to change homosexual orientation through ex-gay ministries and conversion therapy, at the American Psychiatric Association's convention in New Orleans. The study was partly a response to the APA's 2000 statement cautioning against clinical attempts at changing homosexuality, and was aimed at determining whether such attempts were ever successful rather than how likely it was that change would occur for any given individual. Spitzer wrote that some earlier studies provided evidence for the effectiveness of therapy in changing sexual orientation, but that all of them suffered from methodological problems.

In 2012, Spitzer renounced and retracted this study, stating "I was quite wrong in the conclusions that I made from this study. The study does not provide evidence, really, that gays can change. And that's quite an admission on my part." He also apologized to the gay community for making unproven claims of the efficacy of reparative therapy, calling it his only professional regret. Spitzer has requested that all "ex-gay" therapy organizations such as NARTH, PFOX, American College of Pediatricians, and Focus on the Family stop citing his study as evidence for conversion therapy.

The study results were based solely on interviews with the patients and not on any objective observed results. This made it possible and likely that the report was reporting what the patients wanted their results to be rather than the actual results.

Spitzer reported that after intervention, 66% of the men and 44% of the women had achieved "Good Heterosexual Functioning", which he defined as requiring five criteria (being in a loving heterosexual relationship during the last year, overall satisfaction in emotional relationship with a partner, having heterosexual sex with the partner at least a few times a month, achieving physical satisfaction through heterosexual sex, and not thinking about having homosexual sex more than 15% of the time while having heterosexual sex). He found that the most common reasons for seeking change were lack of emotional satisfaction from gay life, conflict between same-sex feelings and behavior and religious beliefs, and desire to marry or remain married. This paper was widely reported in the international media and taken up by politicians in the United States, Germany, and Finland, and by conversion therapists.

In 2003, Spitzer published the paper in the "Archives of Sexual Behavior". Spitzer's study has been criticized on numerous ethical and methodological grounds, and "press releases from both NGLTF and HRC sought to undermine Spitzer's credibility by connecting him politically to right-wing groups that had backed the ex-gay movement". Gay activists argued that the study would be used by conservatives to undermine gay rights. Spitzer acknowledged that the study sample consisted of people who sought treatment primarily because of their religious beliefs (93% of the sample), served in various church-related functions, and who publicly spoke in favor of changing homosexual orientation (78%), and thus were strongly motivated to overreport success. Critics felt he dismissed this source of bias, without even attempting to measure deception or self-deception (a standard practice in self-reporting psychological tests like MMPI-2). That participants had to rely upon their memories of what their feelings were before treatment may have distorted the findings. It was impossible to determine whether any change that occurred was due to the treatment because it was not clear what it involved and there was no control group. Spitzer's own data showed that claims of change were reflected mostly in changes in self-labelling and behavior, less in attractions, and least in the homoerotic content during the masturbatory fantasies; this particular finding was consistent with other studies in this area. Participants may have been bisexual before treatment. Follow-up studies were not conducted. Spitzer stressed the limitations of his study. Spitzer said that the number of gay people who could successfully become heterosexual was likely to be "pretty low", and conceded that his subjects were "unusually religious".

Ariel Shidlo and Michael Schroeder found in "Changing Sexual Orientation: A Consumer's Report", a peer-reviewed study of 202 respondents published in 2002, that 88% of participants failed to achieve a sustained change in their sexual behavior and 3% reported changing their orientation to heterosexual. The remainder reported either losing all sexual drive or attempting to remain celibate, with no change in attraction. Some of the participants who failed felt a sense of shame and had gone through conversion therapy programs for many years. Others who failed believed that therapy was worthwhile and valuable. Many respondents felt harmed by the attempt to change, and reported depression, suicidal ideation and attempts, hypervigilance of gender-deviant mannerisms, social isolation, fear of being a child abuser and poor self-esteem. Of the 8 respondents (out of a sample of 202) who reported a change in sexual orientation, 7 worked as ex-gay counselors or group leaders.

Although no national ban exists, several US states and individual counties ban therapy attempting to change sexual orientation as shown in the map below.

Many health organizations around the world have denounced and criticized sexual orientation change efforts. National health organizations in the United States have announced that there has been no scientific demonstration of conversion therapy's efficacy in the last forty years.
They find that conversion therapy is ineffective, risky and can be harmful. Anecdotal claims of cures are counterbalanced by assertions of harm, and the American Psychiatric Association, for example, cautions ethical practitioners under the Hippocratic oath to do no harm and to refrain from attempts at conversion therapy.

Mainstream medical bodies state that conversion therapy can be harmful because it may exploit guilt and anxiety, thereby damaging self-esteem and leading to depression and even suicide. There is also concern in the mental health community that the advancement of conversion therapy can cause social harm by disseminating inaccurate views about sexual orientation and the ability of gay and bisexual people to lead happy, healthy lives.

Major health organizations critical of conversion therapy include:






The American Psychological Association undertook a study of the peer-reviewed literature in the area of sexual orientation change efforts (SOCE) and found a myriad of issues with the procedures used in conducting the research. The taskforce did find that some participants experienced a lessening of same sex attraction and arousal, but that these instances were "rare" and "uncommon". The taskforce concluded that, "given the limited amount of methodically sound research, claims that recent SOCE is effective are not supported". Two issues with SOCE claims are that conversion therapists falsely assume that homosexuality is a mental disorder and that their research focuses almost exclusively on gay men and rarely includes lesbians.

The American Psychological Association's code of conduct states that "Psychologists respect the dignity and worth of all people, and the rights of individuals to privacy, confidentiality, and self-determination", but also that "Psychologists are aware that special safeguards may be necessary to protect the rights and welfare of persons or communities whose vulnerabilities impair autonomous decision making." The American Counseling Association says that "it is of primary importance to respect a client's autonomy to request a referral for a service not offered by a counselor". They said that no one should be forced to attempt to change their sexual orientation against their will, including children being forced by their parents.

Supporters of SOCE focus on patient self-determination when discussing whether therapy should be available. Mark Yarhouse, of Pat Robertson's Regent University, wrote that "psychologists have an ethical responsibility to allow individuals to pursue treatment aimed at curbing experiences of same-sex attraction or modifying same-sex behaviors, not only because it affirms the client's rights to dignity, autonomy, and agency, as persons presumed capable of freely choosing among treatment modalities and behavior, but also because it demonstrates regard for diversity". Yarhouse and Throckmorton, of the private Christian school Grove City College, argue that the procedure should be available out of respect for a patient's values system and because they find evidence that it can be effective. Haldeman similarly argues for a client's right to access to therapy if requested from a fully informed position: "For some, religious identity is so important that it is more realistic to consider changing sexual orientation than abandoning one's religion of origin ... and if there are those who seek to resolve the conflict between sexual orientation and spirituality with conversion therapy, they must not be discouraged."

In response to Yarhouse's paper, Jack Drescher argued that "any putative ethical obligation to refer a patient for reparative therapy is outweighed by a stronger ethical obligation to keep patients away from mental health practitioners who engage in questionable clinical practices". Chuck Bright wrote that refusing to endorse a procedure that "has been deemed unethical and potentially harmful by most medical and nearly every professional psychotherapy regulating body cannot be justifiably identified as prohibiting client self-determination". Some commentators, recommending a hard stand against the practice, have found therapy inconsistent with a psychologist's ethical duties because "it is more ethical to let a client continue to struggle honestly with her or his identity than to collude, even peripherally, with a practice that is discriminatory, oppressive, and ultimately ineffective in its own stated ends". They argue that clients who request it do so out of social pressure and internalized homophobia, pointing to evidence that rates of depression, anxiety, alcohol and drug abuse and suicidal feelings are roughly doubled in those who undergo therapy.

Haldeman argues that, due to concern for people whose "spiritual or religious concerns" may assume priority over their sexual orientation, mental health organizations do not ban conversion therapy outright.

In 1998, the American Psychiatric Association issued a statement opposing any treatment which is based upon the assumption that homosexuality is a mental disorder or that a person should change their orientation, but did not have a formal position on other treatments that attempt to change a person's sexual orientation. In 2000, they augmented that statement by saying that as a general principle, a therapist should not determine the goal of treatment, but recommends that ethical practitioners refrain from attempts to change clients' sexual orientation until more research is available.

The American Counseling Association has stated that they do not condone any training to educate and prepare a counselor to practice conversion therapy. Counselors who do offer training in conversion therapy must inform students that the techniques are unproven. They suggest counselors do not refer clients to a conversion therapist or to proceed cautiously once they know the counselor fully informs clients of the unproven nature of the treatment and the potential risks. However, "it is of primary importance to respect a client's autonomy to request a referral for a service not offered by a counselor". A counselor performing conversion therapy must provide complete information about the treatment, offer referrals to gay-affirmative counselors, discuss the right of clients, understand the client's request within a cultural context, and only practice within their level of expertise.

NARTH stated in 2012 that refusing to offer therapy aimed at change to a client who requests it, and telling them that their only option is to claim a gay identity, could also be considered ethically unacceptable. In 2012 the British Psychological Society issued a position statement opposing any treatments that are based on an assumption that non-heterosexual orientations are pathological.

A 2013 article by the Committee on Adolescence of the American Academy of Pediatrics stated "Referral for 'conversion' or 'reparative therapy' is never indicated; therapy is not effective and may be harmful to LGBTQ individuals by increasing internalized stigma, distress, and depression."

In 2014, the American Association of Christian Counselors amended its code of ethics to eliminate the promotion of conversion therapy for homosexuals and encouraged them to be celibate instead. An article in the American Medical Association's Journal of Ethics argues that if a pediatrician learns that parents of a 12-year-old patient seek conversion therapy, the pediatrician can advise against "the ineffective and potentially harmful intervention" while being culturally sensitive of their religious objections to homosexuality. The authors argue that the doctor's medical ethics means they should place the interests of the patient above the cultural sensitivities of the parents, and confidentially counsel the patient about resources for LGBT youth facing bullying, and advise the parents about resources for parents of LGBT children. In 2014, major therapy professional bodies in the United Kingdom issued a joint consensus statement opposing conversion therapy. Professional bodies supporting the statement included the UK Council for Psychotherapy, the British Psychoanalytic Council, the Royal College of Psychiatrists, the British Association for Counselling and Psychotherapy, the British Psychological Society and the National Counselling Society.

In 2015, with support of the UK Government's Department of Health, a wide range of UK organisations signed a memorandum of understanding (MoU) setting out an agreed framework for activities by parties concerned to help address the issues raised by the practice of conversion therapy in the UK. In addition to many of the professional bodies that previously issued the consensus statement, signatories included the UK Association of Christian Counsellors, the Royal College of General Practitioners, NHS England and NHS Scotland. The signatory organisations recognised a shared commitment to protecting the public from the risks of conversion therapy. They committed to raise awareness among healthcare professionals and psychological therapists of ethical issues involved in conversion therapy and to provide training to enable therapists to support clients in distress in an appropriate way.

The World Health Organization's ICD-10, which along with the DSM-IV is widely used internationally, states that "sexual orientation by itself is not to be regarded as a disorder". It lists ego-dystonic sexual orientation as a disorder instead, which it defines as occurring where "the gender identity or sexual preference (heterosexual, homosexual, bisexual, or prepubertal) is not in doubt, but the individual wishes it were different because of associated psychological and behavioural disorders, and may seek treatment in order to change it".

In 2012, the Pan American Health Organization (the North and South American branch of the World Health Organization) released a statement cautioning against services that purport to "cure" people with non-heterosexual sexual orientations as they lack medical justification and represent a serious threat to the health and well-being of affected people, and noted that the global scientific and professional consensus is that homosexuality is a normal and natural variation of human sexuality and cannot be regarded as a pathological condition. The Pan American Health Organization further called on governments, academic institutions, professional associations and the media to expose these practices and to promote respect for diversity. The World Health Organization affiliate further noted that gay minors have sometimes been forced to attend these "therapies" involuntarily, being deprived of their liberty and sometimes kept in isolation for several months, and that these findings were reported by several United Nations bodies. Additionally, the Pan American Health Organization recommended that such practices be denounced and subject to sanctions and penalties under national legislation, as they constitute a violation of the ethical principles of health care and violate human rights that are protected by international and regional agreements.

The development of theoretical models of sexual orientation in countries outside the United States that have established mental health professions often follows the history within the U.S. (although often at a slower pace), shifting from pathological to non-pathological conceptions of homosexuality.

Major medical and psychological bodies in Australia uniformly prohibit conversion therapy practices, with published statements having come from peak bodies representing psychologists, psychiatrists, and medical practitioners. In a statement issued jointly with the College of Psychiatrists, Royal Australasian College of Physicians President Catherine Yelland summarised the view of the Australian medical community: "[g]ay conversion therapy is unethical, harmful and not supported by medical evidence." The approaches taken by peak medical bodies is exemplified by the 2015 Australian Psychological Society Position Statement, which declares (emphasis in original) that the:
The Position Statement supports this position by reference to the Society's Code of Ethics, which were adopted in 2007 and mandated as the Code of Ethics for Australian psychologists in 2010 by the Psychology Board of Australia. Under the Code, psychologists are required to "avoid discriminating unfairly against people on the basis of age, religion, sexuality, ethnicity, gender, disability, or any other basis proscribed by law” and mandates that they
The Position Statement explicitly states that this ethical "requirement not to discriminate and to respect clients' moral rights does not equate to a justification to treat homosexuality or bisexuality as a disorder requiring treatment," relying on the Code of Ethics' section on propriety: "psychologists only provide psychological services within the boundaries of their professional competence [which] includes but is not restricted to ... basing their service on established knowledge of the discipline and profession of psychology". Regarding the knowledge base relating to conversion therapy, the statement is unequivocal (emphasis in original):
The Society's position concludes by noting that it "is, of course, appropriate for psychologists to provide clinical services to clients who experience distress in regards to their sexual orientation ... [but this practice] should seek to understand "the reasons for" distress and how it may be alleviated. Evidence-based strategies to alleviate distress do not include attempts at changing sexual orientation, but could include challenging negative stereotypes, seeking social support, and self-acceptance, among others."

The Government of Victoria announced in 2016 that it would be legislating to ban all LGBTQI conversion therapy. The new law began operating in February 2017 and allows the Health Complaints Commissioner to act against any health professional engaged in practices that are "found to be making false claims and to be acting in a manner that puts people's physical, mental or psychological health, safety or welfare at risk" – and in a world-first, this law applies to conversion therapy for adults as well as for minors. Western Australia and the Australian Capital Territory announced in September 2017 that they are investigating similar laws. Advocates for a ban on conversion therapy argued that reviews need to go beyond the practices of health professionals and into activities of religious groups and the unregulated (non-medical) counselling sector.

A Fairfax Media investigation in 2018 reported that "across Australia, organisations who believe that LGBTI people can or should change are hard at work. Conversion practices are hidden in evangelical churches and ministries, taking the form of exorcisms, prayer groups or counselling disguised as pastoral care. They're also present in some religious schools or practised in the private offices of health professionals. They're pushed out through a thriving network of courses and mentors in the borderless world of cyberspace, cloaked in the terminology of 'self improvement' or 'spiritual healing.'" A study of Pentecostal-Charismatic Churches found that LGBTI parishioners were faced with four options: remain closeted, come out but commit to remaining celibate, undergo conversion therapy, or leave the church... the majority took the last option, though typically only after "agonising attempts to reconcile their faith and their sexuality." The study provides corroboration that conversion therapy remains practiced within religious communities. 

Following the Fairfax investigation, Victorian Premier Daniel Andrews called on Prime Minister Malcolm Turnbull to support outlawing conversion therapy as part of the national mental health strategy. Federal Health Minister Greg Hunt declared that the issue is one for the states as no Commonwealth funding goes to sexual orientation change efforts – though "gay conversion ideology has been quietly pushed in schools as part of the federal government's chaplaincy program." The report noted that the Victorian law applies only to people offering health services and so does not catch religious groups and charities "who say they are helping same-sex attracted people to live in accordance with their faith." 

Chris, a survivor of conversion therapy joined Andrews in calling for the Federal Government to outlaw conversion therapy, declaring that "praying the gay away nearly killed me." He established a petition calling on Turnbull and Hunt to act to outlaw conversion therapy, declaring: "I prayed to God asking him to either heal me, or kill me. I was so depressed, I wanted to die." In April 2018, Shadow Health Minister Catherine King wrote a response to the petition: “I’m writing to let you know that Labor stands with you, Chris Csabs and the medical experts in opposing gay conversion therapy...two Turnbull Government ministers – the Acting Prime Minister and the Health Minister – have now failed to condemn the practice when given the chance.” Shortly after Catherine King’s response, Health Minister for Queensland Dr Steven Miles voiced his concerns over the practise and stated that the Federal Health Minister should be working with the states to enact change. In May 2018, Health Minister Jill Hennessy called for an inquiry into gay conversion therapies. In an unprecedented move, the state government indicated it would not only investigate health professionals but will focus on religious and faith-based ministries propagating Gay Conversion ideologies. The following day, Health Minister for the Australian Capital Territory Meegan Fitzharris followed Catherine King’s lead by also responding to the petition, stating that, “The ACT government will ban gay conversion therapy. It is abhorrent and completely inconsistent with the inclusive values of Canberrans.”

On June 25, 2015, a New Jersey jury found the Jewish conversion therapy organization JONAH guilty of consumer fraud in the case Ferguson v JONAH for promising to be able to change its client's sexual urges and determined its commercial practices to be unconscionable.

In a 1997 U.S. case, the Ninth Circuit addressed conversion therapy in the context of an asylum application. A Russian citizen "had been apprehended by the Russian militia, registered at a clinic as a 'suspected lesbian', and forced to undergo treatment for lesbianism, such as 'sedative drugs' and hypnosis. ... The Ninth Circuit held that the conversion treatments to which Pitcherskaia had been subjected constituted mental and physical torture." The court rejected the argument that the treatments to which Pitcherskaia had been subjected did not constitute persecution because they had been intended to help her, not harm her, and stated "human rights laws cannot be sidestepped by simply couching actions that torture mentally or physically in benevolent terms such as 'curing' or 'treating' the victims".

In 1993, the Superior Court of San Francisco – Family Court placed 15-year old lesbian Lyn Duff under the guardianship of a foster couple after her mother committed her to Rivendell Psychiatric Center in West Jordan, Utah, where she allegedly endured physical abuse under the guise of conversion therapy. Lyn Duff's petition to leave her mother was granted without court opinion.




</doc>
<doc id="26411" url="https://en.wikipedia.org/wiki?curid=26411" title="Ring homomorphism">
Ring homomorphism

In ring theory or abstract algebra, a ring homomorphism is a function between two rings which respects the structure.

More explicitly, if "R" and "S" are rings, then a ring homomorphism is a function such that f is

If "R" and "S" are rngs (also known as "pseudo-rings", or "non-unital rings"), then the natural notion is that of a rng homomorphism, defined as above except without the third condition "f"(1) = 1. It is possible to have a rng homomorphism between (unital) rings that is not a ring homomorphism.

The composition of two ring homomorphisms is a ring homomorphism. It follows that the class of all rings forms a category with ring homomorphisms as the morphisms (cf. the category of rings).
In particular, one obtains the notions of ring endomorphism, ring isomorphism, and ring automorphism.

Let be a ring homomorphism. Then, directly from these definitions, one can deduce:

Moreover,



Injective ring homomorphisms are identical to monomorphisms in the category of rings: If is a monomorphism that is not injective, then it sends some "r" and "r" to the same element of "S". Consider the two maps "g" and "g" from Z["x"] to "R" that map "x" to "r" and "r", respectively; and are identical, but since "f" is a monomorphism this is impossible.

However, surjective ring homomorphisms are vastly different from epimorphisms in the category of rings. For example, the inclusion is a ring epimorphism, but not a surjection. However, they are exactly the same as the strong epimorphisms.




</doc>
<doc id="26413" url="https://en.wikipedia.org/wiki?curid=26413" title="Real Madrid C.F.">
Real Madrid C.F.

Real Madrid Club de Fútbol (; "Royal Madrid Football Club"), commonly known as Real Madrid, or simply as Real, is a professional football club based in Madrid, Spain.

Founded on 6 March 1902 as the Madrid Football Club, the club has traditionally worn a white home kit since inception. The word "real" is Spanish for "royal" and was bestowed to the club by King Alfonso XIII in 1920 together with the royal crown in the emblem. The team has played its home matches in the 81,044-capacity Santiago Bernabéu Stadium in downtown Madrid since 1947. Unlike most European sporting entities, Real Madrid's members ("socios") have owned and operated the club throughout its history.

The club was estimated to be worth €3.47 billion ($4.1 billion) in 2018, and in the 2016–17 season it was the second highest-earning football club in the world, with an annual revenue of €674.6 million. The club is one of the most widely supported teams in the world. Real Madrid is one of three founding members of La Liga that have never been relegated from the top division since its inception in 1929, along with Athletic Bilbao and Barcelona. The club holds many long-standing rivalries, most notably "El Clásico" with Barcelona and "El Derbi" with Atlético Madrid.

Real Madrid established itself as a major force in both Spanish and European football during the 1950s, winning five consecutive European Cups and reaching the final seven times. This success was replicated in the league, where the club won five times in the space of seven years. This team, which consisted of players such as Alfredo Di Stéfano, Ferenc Puskás, Francisco Gento and Raymond Kopa, is considered by some in the sport to be the greatest team of all time. In domestic football, the club has won 64 trophies; a record 33 La Liga titles, 19 Copa del Rey, 10 Supercopa de España, a Copa Eva Duarte, and a Copa de la Liga. In European and worldwide competitions, the club has won a record 25 trophies; a record 13 European Cup/UEFA Champions League titles, two UEFA Cups and four UEFA Super Cups. In international football, they have achieved a record six club world championships.

Real Madrid was recognised as the FIFA Club of the 20th Century on 11 December 2000, and received the FIFA Centennial Order of Merit on 20 May 2004. The club was also awarded Best European Club of the 20th Century by the IFFHS on 11 May 2010. In June 2017, the team succeeded in becoming the first club to win back to back Champions Leagues, then made it three in a row in May 2018, extending their lead atop the UEFA club rankings.

Real Madrid's origins go back to when football was introduced to Madrid by the academics and students of the "Institución Libre de Enseñanza", which included several Cambridge and Oxford University graduates. They founded "(Sociedad) Sky Football" in 1897, commonly known as "La Sociedad" (The Society) as it was the only one based in Madrid, playing on Sunday mornings at Moncloa. In 1900, conflict between members caused some of them to leave and create a new club, "Nueva Sociedad de Football" (New Society of Football), to distinguish themselves from "Sky Football". Among the dissenters were Julián Palacios, recognized as the first Real Madrid president, Juan Padrós and Carlos Padrós, the latter two being brothers and future presidents of Real Madrid. In 1901 this new club was renamed as Madrid Football Club. Later, following a restructuring in 1902, "Sky" was renamed as "New Foot-Ball Club". On 6 March 1902, after a new Board presided by Juan Padrós had been elected, Madrid Football Club was officially founded.

Three years after its foundation, in 1905, "Madrid FC" won its first title after defeating Athletic Bilbao in the Spanish Cup final. The club became one of the founding sides of the Royal Spanish Football Federation on 4 January 1909, when club president Adolfo Meléndez signed the foundation agreement of the Spanish FA. After moving between grounds the team moved to the "Campo de O'Donnell" in 1912. In 1920, the club's name was changed to Real Madrid after King Alfonso XIII granted the title of Real (Royal) to the club.

In 1929, the first Spanish football league was founded. Real Madrid led the first league season until the last match, a loss to Athletic Bilbao, meant they finished runners-up to Barcelona. Real Madrid won its first League title in the 1931–32 season and retained the title the following year, becoming the first team to win the championship twice.

On 14 April 1931, the arrival of the Second Spanish Republic caused the club to lose the title Real and went back to being named Madrid Football Club. Football continued during the Second World War, and on 13 June 1943 Madrid beat Barcelona 11–1 in the second leg of a semi-final of the Copa del Generalísimo, the Copa del Rey having been renamed in honour of General Franco. It has been suggested Barcelona players were intimidated by police, including by the director of state security who "allegedly told the team that some of them were only playing because of the regime's generosity in permitting them to remain in the country." The Barcelona chairman, Enric Piñeyro, was assaulted by Madrid fans. However, none of these allegations have been proven and FIFA and UEFA still consider the result as legitimate. According to Spanish journalist and writer, Juan Carlos Pasamontes, Barcelona player Josep Valle denied that the Spanish security forces came before the match. Instead, at the end of the first half, Barcelona coach Juan José Nogués and all of his players were angry with the hard-style of play Real Madrid was using and with the aggressiveness of the home crowd. When they refused to take the field, the Superior Chief of Police of Madrid appeared, identified himself, and ordered the team to take the field.

Santiago Bernabéu Yeste became president of Real Madrid in 1945. Under his presidency, the club, its stadium Santiago Bernabéu and its training facilities Ciudad Deportiva were rebuilt after the Spanish Civil War damages. Additionally, during the 1950s former "Real Madrid Amateurs" player Miguel Malbo founded Real Madrid's youth academy, or ""cantera"," known today as La Fábrica. Beginning in 1953, he embarked upon a strategy of signing world-class players from abroad, the most prominent being Alfredo Di Stéfano.

In 1955, acting upon the idea proposed by Gabriel Hanot, a French sports journalist and editor of "L'Équipe", Bernabéu, Bedrignan and Gusztáv Sebes created a tournament for the champions teams around Europe, under invitation, that would eventually become what today is known as the UEFA Champions League. It was under Bernabéu's guidance that Real Madrid established itself as a major force in both Spanish and European football. The club won the European Cup five times in a row between 1956 and 1960, which included the 7–3 Hampden Park final against Eintracht Frankfurt in 1960. After these five consecutive successes, Real was permanently awarded the original cup and earning the right to wear the UEFA badge of honour.

The club won the European Cup for a sixth time in 1966 defeating Partizan Belgrade 2–1 in the final with a team composed entirely of same nationality players, a first in the competition. This team became known as the Yé-yé. The name "Yé-yé" came from the "Yeah, yeah, yeah" chorus in The Beatles' song "She Loves You" after four members of the team posed for "Marca" and impersonated the Beatles. The Yé-yé generation was also European Cup runner-up in 1962 and 1964. In the 1970s, Real Madrid won five league championships and three Spanish Cups. The club played its first UEFA Cup Winners' Cup final in 1971 and lost to English side Chelsea 2–1. On 2 July 1978, club president Santiago Bernabéu died while the World Cup was being played in Argentina. FIFA decreed three days of mourning to honour him during the tournament. The following year, the club organized the first edition of the Trofeo Santiago Bernabéu in memory of its former president.

By the early 1980s, Real Madrid had lost its grasp on the Liga title until a new cohort of home-grown stars brought domestic success back to the club. Spanish sport journalist Julio César Iglesias gave to this generation the name "La Quinta del Buitre" ("Vulture's Cohort"), which was derived from the nickname given to one of its members, Emilio Butragueño. The other four members were Manuel Sanchís, Martín Vázquez, Míchel and Miguel Pardeza; all five footballers were graduates of Real Madrid's youth academy. With "La Quinta del Buitre" (reduced to four members when Pardeza left for Zaragoza in 1986) and notable players like goalkeeper Francisco Buyo, right-back Miguel Porlán "Chendo" and Mexican striker Hugo Sánchez, Real Madrid had one of the best teams in Spain and Europe during the second half of the 1980s, winning two UEFA Cups, five Spanish championships in a row, one Spanish cup and three Spanish Super Cups. In the early 1990s, "La Quinta del Buitre" split up after Martín Vázquez, Emilio Butragueño and Míchel left the club.

In 1996, President Lorenzo Sanz appointed Fabio Capello as coach. Although his tenure lasted only one season, Real Madrid was proclaimed league champion and players like Roberto Carlos, Predrag Mijatović, Davor Šuker and Clarence Seedorf arrived at the club to strengthen a squad that already boasted the likes of Raúl, Fernando Hierro, Iván Zamorano, and Fernando Redondo. As a result, Real Madrid (with the addition of Fernando Morientes in 1997) finally ended its 32-year wait for its seventh European Cup: in 1998, under manager Jupp Heynckes, they defeated Juventus 1–0 in the final with a goal from Predrag Mijatović.

In 1999, lack of popularity with the fans and a fall out with Lorenzo Sanz resulted in Capello being sacked, and Vicente del Bosque eventually taking over in November of that year. The squad was also largely different from the previous squad: the budding young talent of Raúl, Iker Casillas, Fernando Morientes and Guti being supported with the arrival of Steve McManaman and Nicolas Anelka from the English Premier League, alongside local talents Míchel Salgado, and Iván Helguera and the older veterans such as Fernando Hierro and Roberto Carlos. In Del Bosque's first season in charge, Real won the European Cup/Champions League for the eight time, following a 3–0 victory over Valencia in the final with goals from Morientes, McManaman and Raúl. This victory marked the beginning of a successful period in Real Madrid's history.

In July 2000, Florentino Pérez was elected club president. He vowed in his campaign to erase the club's €270 million debt and modernize the club's facilities. However, the primary electoral promise that propelled Pérez to victory was the signing of Luís Figo from arch-rivals Barcelona. The following year, the club had its training ground rezoned and used the money to begin assembling the "Galácticos" team by signing a global star every summer, which included Zinedine Zidane, Ronaldo, Luís Figo, David Beckham and Fabio Cannavaro. It is debatable whether the gamble paid off, as despite winning the UEFA Champions League and an Intercontinental Cup in 2002, followed by La Liga in 2003, the club failed to win a major trophy for the next three seasons.

The few days after the capturing of the 2003 Liga title were surrounded with controversy. The first controversial decision came when Pérez sacked winning coach Vicente del Bosque. Over a dozen players left the club, including Madrid captain Fernando Hierro, while defensive midfielder Claude Makélélé refused to take part in training in protest at being one of the lowest-paid players at the club and subsequently moved to Chelsea. "That's a lot [of players leaving] when the normal rule is: never change a winning team," stated Zidane. Real Madrid, with newly appointed coach Carlos Queiroz, started their domestic league slowly after a hard win over Real Betis.

The 2005–06 season began with the promise of several new signings: Júlio Baptista (€24 million), Robinho (€30 million) and Sergio Ramos (€27 million). However, Real Madrid suffered from some poor results, including a 0–3 loss at the hands of Barcelona at the Santiago Bernabéu in November 2005. Madrid's coach Wanderley Luxemburgo was sacked the following month and his replacement was Juan Ramón López Caro. A brief return to form came to an abrupt halt after losing the first leg of the Copa del Rey quarterfinal, 6–1 to Real Zaragoza. Shortly after, Real Madrid were eliminated from the Champions League for a fourth successive year, this time at the hands of Arsenal. On 27 February 2006, Florentino Pérez resigned.

Ramón Calderón was elected as club president on 2 July 2006 and subsequently appointed Fabio Capello as the new coach and Predrag Mijatović as the new sporting director. Real Madrid won the Liga title in 2007 for the first time in four years, but Capello was nonetheless sacked at the end of the campaign. The title was won on 17 June, where Real faced Mallorca at the Bernabéu while Barcelona and Sevilla, the other title challengers, faced Gimnàstic de Tarragona and Villarreal, respectively. At half-time, Real were 0–1 down, while Barcelona had surged ahead into a 0–3 lead in Tarragona. However, three goals in the last half-hour secured Madrid a 3–1 win and their first league title since 2003.

On 1 June 2009, Florentino Pérez regained Real Madrid's presidency. Pérez continued with the "Galácticos" policy pursued in his first term, buying Kaká from Milan for a record-breaking (in pound sterling) sum of £56 million, and then breaking the record again by purchasing Cristiano Ronaldo from Manchester United for £80 million.

José Mourinho took over as manager in May 2010. In April 2011, a strange occurrence happened when, for the first time ever, four "Clásicos" were to be played in a span of just 18 days. The first fixture was for the Liga campaign on 17 April (which ended 1–1 with penalty goals for both sides), the Copa del Rey final (which ended 1–0 to Madrid) and the controversial two-legged Champions League semifinal on 27 April and 2 May (3–1 loss on aggregate) to Barcelona.

In the 2011–12 La Liga season, Real Madrid won La Liga for a record 32nd time in the league's history, also finishing the season with numerous club-level records set, including 100 points reached in a single season, a total of 121 goals scored, a goal difference of +89 and 16 away wins, with 32 wins overall. In the same season, Cristiano Ronaldo become the fastest player to reach 100 goals scored in Spanish league history. In reaching 101 goals in 92 games, Ronaldo surpassed Real Madrid legend Ferenc Puskás, who scored 100 goals in 105 matches. Ronaldo set a new club mark for individual goals scored in one year (60), and became the first player ever to score against all 19 opposition teams in a single season.

Real Madrid began the 2012–13 season winning the Supercopa de España, defeating Barcelona on away goals, but finished as second in the league competition. A major transfer of the season was signing from Tottenham Hotspur of Luka Modrić for a fee in the region of £33 million. After a disappointing extra time loss to Atlético Madrid in the 2013 Copa del Rey Final, Pérez announced the departure of José Mourinho at the end of the season by "mutual agreement".

On 25 June 2013, Carlo Ancelotti succeeded Mourinho to become the manager of Real Madrid on a three-year deal, with Zinedine Zidane named as one of his assistants. On 1 September 2013, the long-awaited transfer of Gareth Bale from Tottenham Hotspur was announced. The transfer of the Welshman was reportedly the new world record signing, with the transfer price approximated at €100 million. In Ancelotti's first season at the club, Real Madrid won the Copa del Rey, with Bale scoring the winner in the final against Barcelona. On 24 May, Real Madrid defeated city rivals Atlético Madrid in the 2014 Champions League Final, winning their first European title since 2002, and becoming the first team to win ten European Cups/Champions League titles, an achievement known as ""La Décima"".

After winning the 2014 Champions League, Real Madrid signed goalkeeper Keylor Navas, midfielder Toni Kroos and attacking midfielder James Rodríguez. The club won the 2014 UEFA Super Cup against Sevilla, the club's 79th official trophy. During the last week of the 2014 summer transfer window, Real Madrid sold two players key in the previous season's successes: Xabi Alonso to Bayern Munich and Ángel Di María to Manchester United. This decision from the club was surrounded by controversy, with Cristiano Ronaldo stating, "If I was in charge, maybe I would have done things differently," while Carlo Ancelotti admitted, "We must start again from zero."

After a slow start to the 2014–15 La Liga season, Real Madrid went on a record-breaking 22-match winning streak, which included wins against Barcelona and Liverpool, surpassing the previous Spanish record of 18 successive wins set by Frank Rijkaard's "Barça" in the 2005–06 season. The streak came to an end in their opening match of 2015 with a loss to Valencia, leaving the club two short of equalling the world record of 24 consecutive wins. The club failed to retain the Champions League (losing to Juventus in the semi-finals) and the Copa del Rey, and also failed to land the league title (finishing two points and a place behind champions Barcelona), shortcomings that all preceded Ancelotti's dismissing on 25 May 2015. 

On 3 June 2015, Rafael Benítez was confirmed as the new Real Madrid manager, signing a three-year contract. Real Madrid remained unbeaten in the league until a 3–2 loss at Sevilla in the 11th matchday. This was followed by a 0–4 home loss in the first "Clásico" of the season against Barcelona. In the Copa del Rey Round of 32, Real fielded an ineligible player in Denis Cheryshev in a 1–3 first leg win away against Cádiz, resulting in the second leg being cancelled and Real being disqualified. Benítez was relieved of his duties on 4 January 2016 following allegations of unpopularity with supporters, displeasure with players and a failure to get good results against top teams. 

On 4 January 2016, Benítez's departure was announced along with the promotion of Zinedine Zidane to his first head coaching role. Under Zidane, Real ended up finishing in second place, just one point behind champions Barcelona, in the 2015–16 La Liga. On 28 May, Real Madrid won their 11th Champions League title, extending their record for most successes in the competition, with the achievement being termed ""La Undécima"".

Real Madrid began their 2016–17 campaign, which was to be Zidane's first full season in charge of the club, with victory in the 2016 UEFA Super Cup. On 10 December 2016, Madrid won their 35th-straight match without a loss, which set a new club record. On 18 December 2016, Madrid defeated Japanese club Kashima Antlers 4–2 in the final of the 2016 FIFA Club World Cup. With a 3–3 draw at Sevilla on 12 January 2017, Madrid's unbeaten run extended to 40, breaking Barcelona's Spanish record of 39 matches unbeaten in all competitions from the previous season. Their unbeaten streak ended after a 1–2 away loss against Sevilla in La Liga three days later. In May that year, Madrid won the 2016–17 La Liga for a record 33rd time, their first title in five years. On 3 June, the club’s Champions League Final win against Juventus resulted in Real Madrid being the first team to successfully defend their title in the UEFA Champions League era, and the first to win consecutive titles in the competition since Milan in 1989 and 1990, when the tournament was known as the European Cup. Real Madrid's title was its 12th, extending its record, and its third in four years. The achievement is also known as ""La Duodécima"". The 2016–17 season was the greatest campaign in terms of trophies won in the history of Real Madrid.

Real Madrid won the 2017 UEFA Super Cup 2–1 against Manchester United. Five days later, Real Madrid beat Barcelona at the Camp Nou in the first leg of the 2017 Supercopa de España, before winning the second leg 2–0, ending a 24 consecutive scoring record of Barcelona in "El Clásico" matches, and with a 5–1 aggregate score. Real Madrid also won their third successive UEFA Champions League in 2018, becoming the first club to win three straight UEFA Champions League titles since the tournament's inception, as well as the first team to win three straight in European competition since Bayern Munich in the 1970s. On 31 May, only five days after winning the final, Zidane announced his resignation as Real Madrid manager, citing the club's "need for change" as his rationale for departing. 

On 12 June, Real Madrid named Julen Lopetegui, the head coach of the Spanish national team, as their new manager. It was announced that he would officially become manager after the 2018 FIFA World Cup, however, the Spanish national team sacked Lopetegui a day prior to the tournament, stating that he negotiated terms with the club without informing them. The club then began aggressively re-shaping the squad in the summer of 2018, which began by sanctioning the sale of Ronaldo to Juventus for a reported €100 million.

The first crest had a simple design consisting of a decorative interlacing of the three initials of the club, "MCF" for Madrid Club de Fútbol, in dark blue on a white shirt.
The first change in the crest occurred in 1908 when the letters adopted a more streamlined form and appeared inside a circle. The next change in the configuration of the crest did not occur until the presidency of Pedro Parages in 1920. At that time, King Alfonso XIII granted the club his royal patronage which came in the form of the title "Real Madrid," meaning "Royal." Thus, Alfonso's crown was added to the crest and the club styled itself "Real Madrid Club de Fútbol".

With the dissolution of the monarchy in 1931, all the royal symbols (the crown on the crest and the title of Real) were eliminated. The crown was replaced by the dark mulberry band of the Region of Castile. In 1941, two years after the end of the Civil War, the crest's "Real Corona", or "Royal Crown", was restored while the mulberry stripe of Castile was retained as well. In addition, the whole crest was made full color, with gold being the most prominent, and the club was again called Real Madrid Club de Fútbol. The most recent modification to the crest occurred in 2001 when the club wanted to better situate itself for the 21st century and further standardize its crest. One of the modifications made was changing the mulberry stripe to a more bluish shade.

Real Madrid has maintained the white shirt for its home kit throughout the history of the club. There was, however, one season that the shirt and shorts were not both white. It was an initiative undertaken by Escobal and Quesada in 1925; the two were traveling through England when they noticed the kit worn by London-based team Corinthian F.C., one of the most famous teams at the time known for its elegance and sportsmanship. It was decided that Real Madrid would wear black shorts in an attempt to replicate the English team, but the initiative lasted just one year. After being eliminated from the cup by Barcelona with a 1–5 defeat in Madrid and a 2–0 defeat in Catalonia, President Parages decided to return to an all-white kit, claiming that the other kit brought bad luck. By the early 1940s, the manager changed the kit again by adding buttons to the shirt and the club's crest on the left breast, which has remained ever since. On 23 November 1947, in a game against Atlético Madrid at the Metropolitano Stadium, Real Madrid became the first Spanish team to wear numbered shirts. English club Leeds United permanently switched their blue shirt for a white one in the 1960s, to emulate the dominant Real Madrid of the era.

Real's traditional away colours are all blue or all purple. Since the advent of the replica kit market, the club has also released various other one colour designs, including red, green, orange and black. The club's kit is manufactured by Adidas, whose contract extends from 1998. Real Madrid's first shirt sponsor, Zanussi, agreed for the 1982–83, 1983–84 and 1984–85 seasons. Following that, the club was sponsored by Parmalat and Otaysa before a long-term deal was signed with Teka in 1992. In 2001, Real Madrid ended their contract with Teka and for one season and used the Realmadrid.com logo to promote the club's website. Then, in 2002, a deal was signed with Siemens Mobile and in 2006, the BenQ Siemens logo appeared on the club's shirt. Real Madrid's shirt sponsor from 2007 until 2013 was bwin.com following the economic problems of BenQ Siemens. It is currently Fly Emirates which is set to expire in 2018. In 2015, Madrid signed a new 10-year contract believed to be worth a total of £850 million (€1 billion), earning £59 million (€64 million) per season.

After moving between grounds, the team moved to the Campo de O'Donnell in 1912, which remained its home ground for 11 years. After this period, the club moved for one year to the Campo de Ciudad Lineal, a small ground with a capacity of 8,000 spectators. After that, Real Madrid moved its home matches to Estadio Chamartín, which was inaugurated on 17 May 1923 with a match against Newcastle United. In this stadium, which hosted 22,500 spectators, Real Madrid celebrated its first Spanish league title. After some successes, the 1943 elected president Santiago Bernabéu decided that the Estadio Chamartín was not big enough for the ambitions of the club, and thus a new stadium was built and was inaugurated on 14 December 1947. This was the Santiago Bernabéu Stadium as it is known today, although it did not acquire the present name until 1955. The first match at the Bernabéu was played between Real Madrid and the Portuguese club Belenenses and won by "Los Blancos", 3–1, the first goal being scored by Sabino Barinaga.

The capacity has changed frequently, peaking at 120,000 after a 1953 expansion. Since then, there have been a number of reductions due to modernizations (the last standing places went away in 1998–99 in response to UEFA regulations which forbids standing at matches in the UEFA competition), countered to some extent by expansions. The latest capacity is 81,044 spectators. A plan to add a retractable roof has been announced. Real Madrid has the fourth-highest of the average attendances of European football clubs, behind only Borussia Dortmund, Barcelona and Manchester United.

The Bernabéu has hosted the 1964 UEFA European Championship final, the 1982 FIFA World Cup final, the 1957, 1969 and 1980 European Cup finals and the 2010 UEFA Champions League Final. The stadium has its own Madrid Metro station along the 10 line called "Santiago Bernabéu". On 14 November 2007, the Bernabéu has been upgraded to Elite Football Stadium status by UEFA.

On 9 May 2006, the Alfredo Di Stéfano Stadium was inaugurated in the City of Madrid, where Real Madrid usually trains. The inaugural match was played between Real Madrid and Stade de Reims, a rematch of the 1956 European Cup final. Real Madrid won the match 6–1 with goals from Sergio Ramos, Antonio Cassano (2), Roberto Soldado (2) and José Manuel Jurado. The venue is now part of the Ciudad Real Madrid, the club's new training facilities located outside Madrid in Valdebebas. The stadium holds 5,000 people and is Real Madrid Castilla's home ground. It is named after former Real legend Alfredo Di Stéfano.

Raúl holds the record for most Real Madrid appearances, having played 741 first-team matches from 1994 to 2010. Iker Casillas comes second with 725 appearances, followed by Manuel Sanchis, Jr., having played 710 times. The record for a goalkeeper is held by Iker Casillas, with 725 appearances. With 166 caps (162 while at the club), he is also Real's most capped international player while with 127 caps (47 while at the club).
Cristiano Ronaldo is Real Madrid's all-time top goalscorer, with 450 goals. Five other players have also scored over 200 goals for Real: Alfredo Di Stéfano (1953–64), Santillana (1971–88), Ferenc Puskás (1958–66), Hugo Sánchez (1985–92) and the previous goalscoring record-holder Raúl (1994–2010). Cristiano Ronaldo also holds the record for the most league goals scored in one season (48 in 2014–15), alongside being Real's top goalscorer of all time in La Liga history with 311 goals. Di Stéfano's 49 goals in 58 matches was for decades the all-time highest tally in the European Cup, until it was surpassed by Raúl in 2005, which now is held by Cristiano Ronaldo with 105 goals. The fastest goal in the history of the club (12 seconds) was scored by the Brazilian Ronaldo on 3 December 2003 during a league match against Atlético Madrid.

Officially, the highest home attendance figure for a Real Madrid match is 83,329, which was for a football cup competition, the Copa del Rey, in 2006. The current legal capacity of the Santiago Bernabéu is 81,044. The club's average attendance in 2007–08 season was 76,234, the highest in European Leagues. Real has also set records in Spanish football, most notably the most domestic titles (33 as of 2016–17) and the most seasons won in a row (five, during 1960–65 and 1985–90). With 121 matches (from 17 February 1957 to 7 March 1965), the club holds the record for longest unbeaten run at home in La Liga.

The club also hold the record for winning the European Cup/UEFA Champions League thirteen times and for the most semi-final appearances (28). As of April 2016, Cristiano Ronaldo is the all-time top scorer in the UEFA Champions League, with 120 (121 including qualifiers)  goals in total, 105 while playing for Real Madrid. The team has the record number of consecutive participations in the European Cup (before it became the Champions League) with 15, from 1955–56 to 1969–70. Among the club's on-field records is a 22-game winning streak in all competitions during the 2014–15 season, a Spanish record and fourth worldwide. The same season the team tied the win-streak for games in the Champions League, with ten. In September 2017, the club equalled the record of the Brazilian club Santos, starring Pelé, by scoring in their 73rd consecutive game.

In June 2009, the club broke its own record for the highest transfer fee ever paid in the history of football by agreeing to pay Manchester United €94 million (£80 million) for the services of Cristiano Ronaldo. The fee of €77.5 million (100 billion lire) for Zinedine Zidane's transfer from Juventus to Real Madrid in 2001 was the previous highest transfer fee ever paid. This record (in pound sterling) had been broken previously in June 2009, for a few days, when Real Madrid agreed to buy Kaká from Milan for €67m (£65 million). The transfer of Tottenham Hotspur's Gareth Bale in 2013 was reportedly the new world record signing, with the transfer price expected at around €100 million. In January 2016, documents pertaining to Bale's transfer were leaked which confirmed a world record transfer fee of €100,759,418. The club's sale record came on 10 July 2018, when Juventus signed Cristiano Ronaldo for €100 million.

During most home matches the majority of the seats in the stadium are occupied by season ticket holders, of which the figure is capped at 65,000. To become a season ticket holder one must first be a "socio", or club member. In addition to members, the club has more than 1,800 "peñas" (official, club-affiliated supporters' groups) in Spain and around the world. Real Madrid has the second highest average all-time attendance in Spanish football and regularly attracts over 74,000 fans to the Bernabéu. One of the best supported teams globally, Real Madrid was the first sports team (and first brand) to reach 100 million fans on Facebook in April 2017.

Real Madrid's hardcore supporters are the so-called "Ultras Sur" supporters, or simply Ultras. They are known for their extreme right-wing politics, akin to Barcelona's hardcore supporters group Boixos Nois. The Ultras Surs have developed an alliance with other right wing groups, most notably Lazio "Irriducibili" fans, and have also developed an alliance with left-wing groups. On several occasions, they have racially abused opposing players and have been investigated by UEFA for doing so. Florentino Pérez took it upon himself to ban the Ultras from the Bernabéu and assign their seats to the general public. This decision was controversial with some of the Bernabéu faithful, however, as the lively atmosphere of games would suffer as a result. The Ultras have since held protests outside the Bernabéu and have demanded to be reinstated and allowed to enter the grounds.

There is often a fierce rivalry between the two strongest teams in a national league, and this is particularly the case in La Liga, where the game between Real Madrid and Barcelona is known as "The Classic" ("El Clásico"). From the start of national competitions, the clubs were seen as representatives of two rival regions in Spain, Catalonia and Castile, as well as of the two cities. The rivalry reflects what many regard as the political and cultural tensions felt between Catalans and the Castilians, seen by one author as a re-enactment of the Spanish Civil War.
Over the years, the record from Real Madrid and Barcelona is 81 victories for Madrid, 76 victories for Barcelona, and 39 draws.

During the dictatorships of Primo de Rivera and especially of Francisco Franco (1939–1975), all regional cultures were suppressed. All of the languages spoken in Spanish territory, except Spanish (Castilian) itself, were officially banned. Symbolising the Catalan people's desire for freedom, Barcelona became "More than a club" (""Més que un club"") for the Catalans. According to Manuel Vázquez Montalbán, the best way for the Catalans to demonstrate their identity was by joining Barcelona. It was less risky than joining a clandestine anti-Franco movement, and allowed them to express their dissidence.

On the other hand, Real Madrid was widely seen as the embodiment of the sovereign oppressive centralism and the fascist regime at management level and beyond– Santiago Bernabéu, the former club president for whom Real Madrid's stadium is named, fought on the Nationalist side during the Spanish Civil War. During the war, however, members of both clubs, such as Josep Sunyol and Rafael Sánchez Guerra, suffered at the hands of Francoists.

During the 1950s, the rivalry was exacerbated further when there was a controversy surrounding the transfer of Alfredo Di Stéfano, who eventually played for Real Madrid and was key to their subsequent success. The 1960s saw the rivalry reach the European stage when they met twice in a controversial knock-out round of the European Cup, with Madrid receiving unfavourable treatment from the referee. In 2002, the European encounter between the clubs was dubbed the "Match of The Century" by Spanish media, and Madrid's win was watched by more than 500 million people.

The club's nearest neighbour is Atlético Madrid, a rivalry being shared between fans of both football teams. Although Atlético was founded by three Basque students in 1903, it was joined in 1904 by dissident members of "Madrid FC". Tensions escalated further after Atlético were merged with the football team of the Spanish airforce (and thus renamed Atlético Aviación), and in the 1940s, Atlético was perceived as the preferred team of Franco's regime before he revelled in Real's European success in the 1950s. Furthermore, Real supporters initially came from the middle and upper classes while the Atlético supporters were drawn from the working class. Today, however, these distinctions are largely blurred. They met for the first time on 21 February 1929 in matchday three of the first League Championship at the former Chamartín. It was the first official derby of the new tournament, and Real won 2–1.

The rivalry first gained international attention in 1959 during the European Cup when the two clubs met in the semi-final. Real won the first leg 2–1 at the Bernabéu while Atlético won 1–0 at the "Metropolitano". The tie went to a replay, which Real won 2–1. Atlético, however, gained some revenge when, led by former Real Madrid coach José Villalonga, it defeated its city rivals in two successive "Copa del Generalísimo" finals in 1960 and 1961.

Between 1961 and 1989, when Real dominated La Liga, only Atlético offered it any serious challenge, winning Liga titles in 1966, 1970, 1973 and 1977. In 1965, Atlético became the first team to beat Real at the Bernabéu in eight years. Real Madrid's record against Atlético in more recent times is very favorable. A high point coming in the 2002–03 season, when Real clinched the La Liga title after a 0–4 victory at Atlético at the Vicente Calderón Stadium. Atlético's first win over its city rivals since 1999 came with the Copa del Rey win in May 2013. In 2013–14, Real and Atlético were finalists of UEFA Champions League, the first final which hosted two clubs from same city. Real Madrid triumphed with 4–1 in extra time. On 7 February 2015, Real suffered their first defeat in 14 years at the Vicente Calderón, a 4–0 loss. On 28 May 2016, Real and Atlético met again for the Champions League title in Milan, which resulted in a win for Real after a penalty shootout.

A further minor rivalry exists between Real Madrid and Athletic Bilbao. This is known as El Viejo Clásico ("the old classic"), so named as the two clubs were dominant in the first half of the 20th century, meeting in nine Copa del Rey finals including the first in 1903. Until 10 December 2011, this fixture was the most played in the history of Spanish football, when it was surpassed by "El Clásico".

Athletic Bilbao, who operate a policy of only using local players, have long since ceased to be a competitive rival to clubs such as Real Madrid who scour the globe for the best talent; the "Lions" have collected no major trophies since 1984 and won only two of the 26 matches between the teams from 2005–06 to 2016–17. However, the matches remain keenly fought due to their historical and cultural significance, with some parallels to the political aspect of the Barcelona/Catalonia rivalry as Athletic are the largest club in the Basque region.

Real Madrid and Germany's Bayern Munich are two of the most successful clubs in the UEFA Champions League/European Cup competition, Real winning thirteen times and Bayern winning five times. Although they have never met in a final, Real Madrid versus Bayern is the match that has historically been played most often in the Champions League/European Cup with 26 matches (12 wins for Madrid, 11 wins for Bayern, with 3 draws), Real's biggest loss at home in the Champions League came at the hands of Bayern on 29 February 2000, 2–4. Real Madrid supporters often refer to Bayern as the ""Bestia negra"" ("Black Beast").

During the 2010s, the two teams met in the 2011–12 Champions League semi-finals, which ended 3–3 on aggregate (Bayern won 3–1 on penalties after extra time, but lost the final at their own stadium), and then at the same stage in the 2013–14 edition with Real Madrid winning 5–0 on aggregate on their way to winning the competition. They were also drawn together in the 2016–17 quarter-finals; Real Madrid won 6–3 on aggregate and subsequently lifted the trophy. The following year, they met in the semi-finals, with Real Madrid again progressing 4–3.

Arjen Robben, Xabi Alonso, Toni Kroos and James Rodríguez are among the players to appear for both clubs in the early 21st century.

It was under Florentino Pérez's first presidency (2000–2006) that Real Madrid started its ambition of becoming the world's richest professional football club. The club ceded part of its training grounds to the city of Madrid in 2001, and sold the rest to four corporations: Repsol YPF, Mutua Automovilística de Madrid, Sacyr Vallehermoso and OHL. The sale eradicated the club's debts, paving the way for it to buy the world's most expensive players, such as Zinedine Zidane, Luís Figo, Ronaldo and David Beckham. The city had previously rezoned the training grounds for development, a move which in turn increased their value, and then bought the site. The European Commission started an investigation into whether the city overpaid for the property, to be considered a form of state subsidy.

The sale of the training ground for office buildings cleared Real Madrid's debts of €270 million and enabled the club to embark upon an unprecedented spending spree which brought big-name players to the club. In addition, profit from the sale was spent on a state-of-the-art training complex on the city's outskirts. Although Pérez's policy resulted in increased financial success from the exploitation of the club's high marketing potential around the world, especially in Asia, it came under increasing criticism for being too focused on marketing the Real Madrid brand and not enough on the performances of the team.

By September 2007, Real Madrid was considered the most valuable football brand in Europe by BBDO. In 2008, it was ranked the second-most valuable club in football, with a value of €951 million (£640 million / $1.285 billion), only beaten by Manchester United, which was valued at €1.333 billion (£900 million). In 2010, Real Madrid had the highest turnover in football worldwide. In September 2009, Real Madrid's management announced plans to open its own dedicated theme park by 2013.

A study at Harvard University concluded that Real Madrid "is one of the 20 most important brand names and the only one in which its executives, the players, are well-known. We have some spectacular figures in regard to worldwide support of the club. There are an estimated 287 million people worldwide who follow Real Madrid." In 2010, "Forbes" evaluated Real Madrid's worth to be around €992 million (US$1.323 billion), ranking them second after Manchester United, based on figures from the 2008–09 season. According to Deloitte, Real Madrid had a recorded revenue of €401 million in the same period, ranking first.

Along with Barcelona, Athletic Bilbao and Osasuna, Real Madrid is organised as a registered association. This means that Real Madrid is owned by its supporters who elect the club president. The club president cannot invest his own money into the club and the club can only spend what it earns, which is mainly derived through merchandise sales, television rights and ticket sales. Unlike a limited company, it is not possible to purchase shares in the club, but only membership. The members of Real Madrid, called "socios", form an assembly of delegates which is the highest governing body of the club. As of 2010, the club has 60,000 "socios". At the end of the 2009–10 season, the club's board of directors stated that Real Madrid had a net debt of €244.6 million, €82.1 million lower than the previous fiscal year. Real Madrid announced that it had a net debt of €170 million after the 2010–11 season. From 2007 to 2011, the club made a net profit of €190 million.

During the 2009–10 season, Real Madrid made €150 million through ticket sales, which was the highest in top-flight football. The club has the highest number of shirt sales a season, around 1.5 million. For the 2010–11 season its wage bill totalled €169 million, which was second-highest in Europe behind Barcelona. However, its wage bill to turnover ratio was the best in Europe at 43 percent, ahead of Manchester United and Arsenal at 46 percent and 50 percent, respectively. In 2013, "Forbes" listed the club as the world's most valuable sports team, worth $3.3 billion.

Real Madrid was the featured club in the second installment of the "Goal!" football movie trilogy, "" (2007). The film follows former Newcastle United star Santiago Muñez as he is first scouted, and then signed by Real Madrid for the 2005–06 season. The film's creators wanted to put emphasis on the changes in Muñez's life after his move to Madrid. Production was done with the full support of UEFA, allowing the film crew to use many real life players in cameo roles. Real Madrid squad members featured in the film included Iker Casillas, Zinedine Zidane, David Beckham, Ronaldo, Roberto Carlos, Raúl, Sergio Ramos, Robinho, Michael Owen, Míchel Salgado, Júlio Baptista, Steve McManaman and Iván Helguera. Non-Real Madrid players to make cameo appearances included Ronaldinho, Thierry Henry, Lionel Messi, Samuel Eto'o, Andrés Iniesta, Pablo Aimar, Freddie Ljungberg, Cesc Fàbregas and Santiago Cañizares. In the film, both Florentino Pérez and Alfredo Di Stéfano presented the fictional player Muñez to the club after his signing.

"Real, The Movie" is a 2005 part feature, part documentary film that showcases the worldwide passion for Real Madrid. Produced by the club and directed by Borja Manso, it follows five sub-stories of fans from around the world and their love for the club. Along with the fictional portion of the film, it also contains real footage of the squad, during training at Ciudad Real Madrid, matches, and interviews. Although the film mentions all of the squad, it mainly focuses on "galácticos" such as David Beckham, Zinedine Zidane, Raúl, Luís Figo, Ronaldo, Iker Casillas, and Roberto Carlos, among others. The film was originally produced in Spanish, but has been dubbed for their worldwide fanbase.

The book "White Storm: 100 years of Real Madrid" by Phil Ball was the first English-language history of Real Madrid. Published in 2002, it talks about the most successful moments of the club during its first centenary, having been translated into various languages. In late 2011, Real Madrid released a digital music album, entitled "Legends", and a remix of the club's anthem, "Himno del Real Madrid," was released as the first single from the album.

Real Madrid TV is an encrypted digital television channel, operated by Real Madrid and specialising in the club. The channel is available in Spanish and English. It is located at Ciudad Real Madrid in Valdebebas (Madrid), Real Madrid's training centre.

"Hala Madrid" is a magazine published quarterly for the Real Madrid club members and the "Madridistas Fan Club" card holders. The phrase Hala Madrid, meaning "Forward Madrid" or "Go Madrid", is also the title of the club's official anthem, which is often sung by the Madridistas (the club's fans). The magazine includes reports on the club's matches in the previous month, as well as information about the reserve and youth teams. Features often include interviews with players, both past and present, and the club's historic matches.

Real Madrid has appeared in many football-based video games, namely in the FIFA and Pro Evolution Soccer series. A Real Madrid player has graced the cover of both titles a combined seven times. 

In 2007, Spanish game publisher Virgin Play signed a deal with the club to make officially licensed Real Madrid video games. The only one released under the deal (due to Virgin Play's liquidation in September 2009) would end up being "Real Madrid: The Game", which was developed by Atomic Planet Entertainment and was published under Virgin Play's publishing division V.2 Play in May 2009 for the PlayStation 2, PlayStation Portable, Windows, Wii and Nintendo DS exclusively in European territories Virgin Play released their products in. The game featured a career mode with a mixture of role-playing and simulation as well as arcade-styled Football gameplay.


Spanish teams are limited to three players without EU citizenship. The squad list includes only the principal nationality of each player; several non-European players on the squad have dual citizenship with an EU country. Also, players from the ACP countries in Africa, the Caribbean, and the Pacific that are signatories to the Cotonou Agreement are not counted against non-EU quotas due to the Kolpak ruling.





</doc>
<doc id="26414" url="https://en.wikipedia.org/wiki?curid=26414" title="Resurrection of Jesus">
Resurrection of Jesus

The resurrection of Jesus or resurrection of Christ is the Christian religious belief that, after being put to death, Jesus rose again from the dead: as the Nicene Creed expresses it, "On the third day he rose again in accordance with the Scriptures".

According to the New Testament, after the Romans crucified Jesus, he was anointed and buried in a new tomb by Joseph of Arimathea but God raised him from the dead and he appeared to witnesses before he ascended into heaven, to sit at the right hand of Father.

Paul the Apostle declared that "Christ died for our sins according to the scriptures; And that he was buried, and that he rose again the third day according to the scriptures".() The chapter states that such a belief in both the death and resurrection of Christ is of central importance to the Christian faith: "And if Christ has not been raised, then our preaching is in vain and your faith is in vain."() Paul further asserted "And if Christ has not been raised, your faith is futile and you are still in your sins. Then those also who have fallen asleep in Christ have perished. If in Christ we have hope in this life only, we are of all people most to be pitied."()

Christians celebrate the resurrection of Jesus on Easter Sunday, two days after Good Friday, the day of his crucifixion. Easter's date corresponds roughly with Passover, the Jewish observance associated with the Exodus, that is fixed for the night of the full moon near the time of the spring equinox.

Belief in a bodily resurrection of the dead became well established within some segments of Jewish society in the centuries leading up to the time of Christ, as recorded by Daniel , from the mid-2nd century BC: "Many of those sleeping in the dust shall awaken, some to everlasting life, and some to everlasting peril". Josephus (1st century AD) gives the following outline: The Pharisees believed in resurrection of the dead, and the Sadducees did not. The Sadducees, politically powerful religious leaders, rejected the afterlife, angels, and demons as well as the Pharisees' oral law. The Pharisees, whose views became Rabbinic Judaism, eventually won (or at least survived) this debate. The promise of a future resurrection appears in the Torah as well as in certain Jewish works, such as the Life of Adam and Eve, "c" 100 BC, and the Pharisaic book 2 Maccabees, "c" 124 BC.

The resurrection is mentioned in several locations in the Bible. There are several places in the Four Gospels in which Jesus foretells his coming death and resurrection, which he states is the plan of God the Father. Christians view the resurrection of Jesus as part of the plan of salvation and redemption by atonement for man's sin.

The letters of Paul, which began to appear around two decades after the death of Jesus, record widespread belief in the resurrection of Jesus among his earliest followers. Their many references include:

Paul's proof of the resurrection is the appearances of the risen Lord to others and himself.
In his First Epistle to the Corinthians he lists these appearances, first to the Apostle Peter, then to "the Twelve," then to five hundred at one time, then to James (generally believed to be the member of Jesus' family with that name), then to "all the Apostles," and finally to Paul himself. In contrast to the Gospels, Paul doesn't mention appearances to women such as Mary Magdalene, but does mention an appearance to a group of 500 which is not mentioned in other New Testament sources.

Just before sunrise on the day after the regular weekly Sabbath three women, Mary Magdalene, Mary the mother of James, and Salome, came to anoint Jesus' body, wondering how they would be able to roll the large rock away from the tomb; but they found the rock already rolled aside and a young man in white inside; he told them that Jesus had risen, and that they should tell Peter and the disciples that he will meet them in Galilee, "just as he told you". Then the women "fled from the tomb".

Just before sunrise on the day after the regular weekly Sabbath two women, Mary Magdalene and "the other Mary", went to visit the tomb. An angel had come down from Heaven, accompanied by an earthquake, and rolled the rock aside from the tomb. The angel waited for them and told them not to be afraid, but to tell the disciples that Jesus had risen and will meet them in Galilee. The women were joyful and set out to tell the disciples the good news, then soon afterward Jesus appeared and told them not to be afraid, and told them that He had risen and that they should tell the disciples that they will see Him in Galilee. The disciples went to Galilee, where they then saw Jesus in the flesh.

The soldiers guarding the tomb were terrified by the angel, and informed the chief priests; the priests and elders bribed them to spread a lie that the disciples stole the body, "[a]nd this story has been widely circulated among the Jews to this very day".

Just after sunrise on the day after the Sabbath, "the women" came to anoint Jesus's body. They found the stone rolled away and the tomb empty. Suddenly two men in clothing glowing "like lightning" stood beside them and said, "[Jesus] is not here; he has risen". The women told the disciples, but the disciples did not believe them, except for Peter who ran to the tomb. Peter found the grave-clothes in the empty tomb and went away, wondering.

The same day Jesus appeared to two of his followers on the road to Emmaus. They failed to recognize him until he broke bread and gave thanks, and he then vanished. The two go at once to Jerusalem where they found the disciples describing Jesus' appearance to Peter. As they told their story Jesus appeared to them all. They were afraid, but he invited them to touch his body, then ate with them, and explained the prophecies which are fulfilled through him.

(The Acts of the Apostles is presented as a continuation of the Gospel of Luke.)<br>
Jesus appeared to the apostles for forty days, giving many proofs that he was alive, and instructing them not to leave Jerusalem until they were baptised with the Holy Spirit.

Early on the day after the Sabbath, before sunrise, Mary Magdalene visited the tomb and found the large stone had already been rolled away. She told Peter and "the beloved disciple", who then ran to the tomb to only find the grave-clothes, then went home. They assumed his body had been stolen. Mary wept, then saw two angels who spoke to her, and then saw Jesus, whom she did not recognize. Jesus told her to tell the disciples that he is ascending to God, and Mary then told the disciples she had seen the Lord.

That evening Jesus appeared among them inside a locked room, and gave them power over sin and forgiveness of sin. A week later he appeared to doubting Thomas, who had not believed, but when Thomas was instructed to touch the wounds of Jesus he said, "My Lord and my God!" Jesus replied: "Because you have seen me, you have believed; blessed are those who have not seen and yet have believed".

In the New Testament all four gospels conclude with an extended narrative of Jesus's arrest, trial, crucifixion, burial, and his resurrection. In each gospel these five events in the life of Jesus are treated with more intense detail than any other portion of that gospel's narrative. Scholars note that the reader receives an almost hour-by-hour account of what is happening. The death and resurrection of Jesus are treated as the climax of the story, the point to which everything else has been moving all the while.

After his death by crucifixion, Jesus was placed in a new tomb which was discovered early Sunday morning to be empty. The New Testament does not include an account of the "moment of resurrection". In the Eastern Church icons do not depict that moment, but show the myrrhbearers and depict scenes of salvation. The major resurrection appearances of Jesus in the canonical gospels (and to a lesser extent other books of the New Testament) are reported to have occurred after his death, burial and resurrection, but prior to his ascension.

The synoptic gospels agree that, as the evening came after the crucifixion, Joseph of Arimathea asked Pilate for the body of Jesus, and that, after Pilate granted his request, wrapped it in linen cloth and laid it in a tomb. This was in accordance with Mosaic Law, which stated that a person hanged on a tree must not be allowed to remain there at night, but should be buried before sundown.

In Matthew, Joseph was identified as "also a disciple of Jesus;" in Mark he was identified as "a respected member of the council (Sanhedrin) who was also himself looking for the Kingdom of God;" in Luke he was identified as "a member of the council, good and righteous, who did not consent to their purpose or deed, and who was looking for the Kingdom of God'" and in John he was identified as "a disciple of Jesus".

The Gospel of Mark states that when Joseph of Arimathea asked for Jesus's body, Pilate marveled that Jesus was already dead, and he summoned the centurion to confirm this before releasing the body to Joseph. In the Gospel of John, it is recorded that Joseph of Arimathea was assisted in the burial process by Nicodemus, who brought a mixture of myrrh and aloes and included these spices in the burial clothes per Jewish customs.

Although no single gospel gives an inclusive or definitive account of the resurrection of Jesus or his appearances, there are four points at which all four gospels converge:


All four gospels report that women were the ones to find the tomb of Jesus empty. According to Mark and Luke, the "announcement" of Jesus' resurrection was made to several women. According to Mark and John, Jesus "appeared first" (in and ) to Mary Magdalene.

In the gospels, especially the synoptics, women play a central role as eyewitnesses at Jesus' death, entombment, and in the discovery of the empty tomb. All three synoptics repeatedly make women the subject of verbs of seeing, clearly presenting them as eyewitnesses.

After the tomb was found empty, the gospels indicate that Jesus made a series of appearances to the disciples. He was not immediately recognizable, according to Luke. E. P. Sanders concluded that although he could appear and disappear, he was not a ghost. Writing that Luke was very insistent about that, Sanders pointed out that "the risen Lord could be touched, and he could eat". He first appeared to Mary Magdalene, but she did not recognize him at first. The first two disciples to whom he appeared, walked and talked with him for quite a while without knowing who he was, (the road to Emmaus appearance). He was made known "in the breaking of the bread". When he first appeared to the disciples in the upper room, Thomas was not present and would not believe until a later appearance where he was invited to put his finger into the holes in Jesus' hands and side. Beside the Sea of Galilee he encouraged Peter to serve his followers. His final gospel appearance is reported as being forty days after the resurrection when he was "carried up" into heaven where he sits on the right hand of God. 

At a later time, on the road to Damascus, Saul of Tarsus, then the arch-persecutor of the early disciples, was converted to Christ following an extraordinary vision and discourse with Jesus which left him blind for three days. (Saul later became known as Paul the Apostle.) 
He became one of Christianity's foremost missionaries and theologians. 

New Testament scholar and theologian E. P. Sanders argues that a concerted plot to foster belief in the Resurrection would probably have resulted in a more consistent story, and that some of those who were involved in the events gave their lives for their belief. Sanders offers his own hypothesis, saying "there seems to have been a competition: 'I saw him,' 'so did I,' 'the women saw him first,' 'no, I did; they didn't see him at all,' and so on." In defending the historicity of the resurrection, Sanders goes so far as to state, "That Jesus’ followers (and later Paul) had resurrection experiences is, in my judgment, a fact. What the reality was that gave rise to the experiences I do not know."

James D.G. Dunn writes that, whereas the apostle Paul's resurrection experience was "visionary in character" and "non-physical, non-material," the accounts in the Gospels describe physical appearances to the other apostles and women. He contends that the "massive realism'...of the [Gospel] appearances themselves can only be described as visionary with great difficulty—and Luke would certainly reject the description as inappropriate," and that the earliest conception of resurrection in the Jerusalem Christian community was physical. Conversely, Helmut Koester writes that the descriptions of the resurrection were originally epiphanies in which the disciples are called to a ministry by the risen Jesus and were interpreted as physical proof of the event at a secondary stage. He contends that the more detailed accounts of the resurrection are also secondary and do not come from historically trustworthy sources, but instead belong to the genre of the narrative types.

N.T. Wright argues that the account of the empty tomb and the visionary experiences point towards the historical reality of the resurrection. He suggests that multiple lines of evidence from the New Testament and the early Christian beliefs it reflects shows that it would be highly unlikely that belief in the empty tomb would simply appear without a clear basis in the memory of early Christians. In tandem with the historically certain visionary experiences of the early disciples and apostles, Jesus' resurrection as a historical reality becomes far more plausible. Wright treats the resurrection as a historical and accessible event, rather than as a 'supernatural' or 'metaphysical' event.

Summarizing its traditional analysis, the Catholic Church stated in its Catechism: "Although the Resurrection was an historical event that could be verified by the sign of the empty tomb and by the reality of the apostles' encounters with the risen Christ, still it remains at the very heart of the mystery of faith as something that transcends and surpasses history."

In his book "The First Coming: How the Kingdom of God Became Christianity", Thomas Sheehan argues that even Paul's account of the resurrection is not meant to be taken as referring to a literal, physical rising from the grave, and that stories of a bodily resurrection did not appear until as much as half a century following the crucifixion. Instead, Sheehan believes that Paul's understanding of the resurrection, and perhaps Peter's as well, is a metaphysical one, with the stories of Jesus's (figurative) resurrection reflecting his triumphant "entry into God's eschatological presence," and that Paul's reference to Jesus having risen "on the third day" () "is not a chronological designation but an apocalyptic symbol for God's eschatological saving act, which strictly speaking has no date in history. Thus the 'third day' does not refer to Sunday, April 9, 30 C.E., or to any other moment in time. And as regards the 'place' where the resurrection occurred, the formula in First Corinthians does not assert that Jesus was raised from the tomb, as if the raising were a physical and therefore temporal resuscitation. Without being committed to any preternatural physics of resurrection, the phrase 'he was raised on the third day' simply expresses the belief that Jesus was rescued from the fate of utter absence from God (death) and was admitted to the saving presence of God (the eschatological future)."

An early alternative interpretation was provided by George Bush, Professor of Hebrew at New York City University, in 1845 in a book entitled "The Resurrection of Christ". He reviews in detail the post-crucifixion appearances of Jesus and demonstrates how they can be better understood as visions of a spiritual or celestial body rather than as appearances of a material body using, in many cases, a careful analysis of the original Greek or Hebrew words.

Peter Kirby, the founder of EarlyChristianWritings.com, states that, "Many scholars doubt the historicity of the empty tomb." According to Robert M. Price, Christian "apologists love to make the claims ... that the resurrection of Jesus is the best attested event in history", but "probabilistic arguments" show that "the resurrection is anything but an open-and-shut case". Robert Greg Cavin, a professor of Philosophy and Religious Studies at Cypress College, states that, "our only sources of potential evidence, the New Testament Easter traditions, fall far short of providing the kind of information necessary for establishing the resurrection hypothesis."

Biblical scholar Géza Vermes analyzes this subject in his book, "The Resurrection". He concludes that there are eight possible theories to explain the resurrection of Jesus. Vermes outlines his boundaries as follows, I have discounted the two extremes that are not susceptible to rational judgment, the blind faith of the fundamentalist believer and the out-of-hand rejection of the inveterate skeptic. The fundamentalists accept the story, not as written down in the New Testament texts, but as reshaped, transmitted, and interpreted by Church tradition. They smooth down the rough edges and abstain from asking tiresome questions. The unbelievers, in turn, treat the whole Resurrection story as the figment of early Christian imagination. Most inquirers with a smattering of knowledge of the history of religions will find themselves between these two poles.

From his analysis, Vermes presents the remaining six possibilities to explain the resurrection of Jesus account, (1) "The body was removed by someone unconnected with Jesus", (2) "The body of Jesus was stolen by his disciples", (3) "The empty tomb was not the tomb of Jesus", (4) Buried alive, Jesus later left the tomb", (5) Jesus recovered from a coma and departed Judea, and (6) the possibility that there was a "spiritual, not bodily, resurrection". Vermes states that none of these six possibilities are likely to be historical.

According to N. T. Wright in his book "The Resurrection of the Son of God", "There can be no question: Paul is a firm believer in bodily resurrection. He stands with his fellow Jews against the massed ranks of pagans; with his fellow Pharisees against other Jews." And according to Gary Habermas, "Many other scholars have spoken in support of a bodily notion of Jesus’ resurrection."

Habermas also argues three facts in support of Paul's belief in a physical resurrection body. (1) Paul is a Pharisee and therefore (unlike the Sadducees) believes in a physical resurrection. (2) In Philippians 3:11 Paul says "That I may attain to the "ek anastasis" (out-resurrection)" from the dead, which according to Habermas means that "What goes down is what comes up". And (3) In Philippians 3:20–21 "We look from heaven for Jesus who will change our vile "soma" (body) to be like unto his "soma" (body)". According to Habermas, if Paul meant that we would change into a spiritual body then Paul would have used the Greek "pneuma" instead of "soma". Although others argue that a "body" (or "soma") can be a "spirit" "body", not necessarily "flesh", in order for it to be a body, according to Paul's own words to the Corinthians, regarding "spiritual body". But they say that it was a true resurrection nonetheless.

Flavius Josephus ("c". 37–"c". 100), a Jew and Roman citizen who worked under the patronage of the Emperor Vespasian and his son Titus, wrote the "Antiquities of the Jews c". 93 which contains a passage known as the "Testimonium Flavianum". This passage mentions John the Baptist and Jesus as two holy men among the Jews. The text describes the death and resurrection of Jesus as follows: "When Pilate, upon the accusation of the first men amongst us, condemned [Jesus] to be crucified, those who had formerly loved him did not cease [to follow him], for he appeared to them on the third day, living again, as the divine prophets foretold, along with a myriad of other marvelous things concerning him."

There are various other arguments against the historicity of the resurrection story. For example, the number of other historical figures and gods with similar death and resurrection accounts has been pointed out. However the majority consensus among biblical scholars is that the genre of the Gospels is a kind of ancient biography and not myth. Robert M. Price claims that if the resurrection could, in fact, be proven through science or historical evidence, the event would lose its miraculous qualities. In a more focused argument, Carrier asserts that, "The surviving evidence legal and historical, suggests that Jesus was not formally buried Friday night," but that "it had to have been placed Saturday night in a special public graveyard reserved for convicts. On this theory, the women who visited the tomb Sunday morning mistook its vacancy."
New Testament historian Bart D. Ehrman recognizes that "Some scholars have argued that it's more plausible that in fact Jesus was placed in a common burial plot, which sometimes happened, or was, as many other crucified people, simply left to be eaten by scavenging animals." He further elaborates by saying: "[T]he accounts are fairly unanimous in saying (the earliest accounts we have are unanimous in saying) that Jesus was in fact buried by this fellow, Joseph of Arimathea, and so it's relatively reliable that that's what happened." Analyzing all ancient reports of crucifixion, he later changed his mind to Jesus having been eaten by scavengers.

In Christian theology, the resurrection of Jesus is a foundation of the Christian faith. Christians, through faith in the working of God are spiritually resurrected with Jesus, and are redeemed so that they may walk in a new way of life. As Paul the Apostle stated: "If Christ was not raised, then all our preaching is useless, and your trust in God is useless". The death and resurrection of Jesus are the most important events in Christian Theology. They form the point in scripture where Jesus gives his ultimate demonstration that he has power over life and death, thus he has the ability to give people eternal life. Terry Miethe, a Christian philosopher at Oxford University, stated, " 'Did Jesus rise from the dead?' is the most important question regarding the claims of the Christian faith.' " According to the Bible, "God raised him from the dead", he ascended to heaven, to the "right hand of God", and will return again to fulfill the rest of Messianic prophecy such as the resurrection of the dead, the Last Judgment and establishment of the Kingdom of God; see also Messianism and Messianic Age.

Some modern scholars use the belief of Jesus' followers in the resurrection as a point of departure for establishing the continuity of the historical Jesus and the proclamation of the early church. Carl Jung suggests that the crucifixion-resurrection account was the forceful spiritual symbol of, literally, God-as-Yahweh becoming God-as-Job.

The apostle Paul wrote that: "If there is no resurrection of the dead, then Christ has not been raised; if Christ has not been raised, then our preaching is in vain and your faith is in vain... If Christ has not been raised, your faith is futile". Many scholars have contended that in discussion on the resurrection, the apostle Paul refers to a rabbinic style transmission of an early authoritative tradition that he received and has passed on to the church at Corinth. For this and other reasons, it is widely believed that this creed is of pre-Pauline origin. Geza Vermes writes that the creed is "a tradition he [Paul] has inherited from his seniors in the faith concerning the death, burial and resurrection of Jesus". The creed's ultimate origins are within the Jerusalem apostolic community having been formalised and passed on within a few years of the resurrection. Paul Barnett writes that this creedal formula, and others, were variants of the "one basic early tradition that Paul "received" in Damascus from Ananias in about 34 [AD]" after his conversion.

But Christ really has been raised from the dead. He is the first of all those who will rise. Death came because of what a man did. Rising from the dead also comes because of what a man did. Because of Adam, all people die. So because of Christ, all will be made alive.

Paul's views went against the thoughts of the Greek philosophers to whom a bodily resurrection meant a new imprisonment in a corporeal body, which was what they wanted to avoid—given that for them the corporeal and the material fettered the spirit. At the same time, Paul believed that the newly resurrected body would be a heavenly body; immortal, glorified, powerful and spiritual in contrast to an earthly body, which is mortal, dishonored, weak and natural. According to theologian Peter Carnley, the resurrection of Jesus was different from the resurrection of Lazarus as: "In the case of Lazarus, the stone was rolled away so that he could walk out... the raised Christ didn't have to have the stone rolled away, because he is transformed and can appear anywhere, at any time".

According to international scholar Thorwald Lorenzen, the first Easter led to a shift in emphasis from faith "in God" to faith "in Christ". Today, Lorenzen finds "a strange silence about the resurrection in many pulpits". He writes that among some Christians, ministers and professors, it seems to have become "a cause for embarrassment or the topic of apologetics". It has been argued that many Christians neglect the resurrection because of their understandable preoccupation with the Cross. However, the belief in Jesus' physical resurrection remains the single doctrine most accepted by Christians of all denominational backgrounds.

In the teachings of the apostolic Church, the resurrection was seen as heralding a new era. Forming a theology of the resurrection fell to the apostle Paul. It was not enough for Paul to simply repeat elementary teachings, but as states, "go beyond the initial teachings about Christ and advance to maturity". Fundamental to Pauline theology is the connection between Christ's Resurrection and redemption. Paul explained the importance of the resurrection of Jesus as the cause and basis of the hope of Christians to share a similar experience.

The teachings of the apostle Paul formed a key element of the Christian tradition and theology. If his death stands at the center of Paul's theology, so does the resurrection: unless the one died the death of "all", the "all" would have little to celebrate in the resurrection of the one. Paul taught that, just as Christians share in Jesus' death in baptism, so they will share in his resurrection for Jesus was designated the Son of God by his resurrection. In Paul states:
But Christ really has been raised from the dead. He is the first of all those who will rise. Death came because of what a man did. Rising from the dead also comes because of what a man did. Because of Adam, all people die. So because of Christ, all will be made alive.
The Apostolic Fathers, discussed the death and resurrection of Jesus, including Ignatius (50–115), Polycarp (69–155), and Justin Martyr (100–165). Following the conversion of Constantine and the liberating Edict of Milan in 313, the ecumenical councils of the 4th, 5th and 6th centuries, that focused on Christology helped shape the Christian understanding of the redemptive nature of resurrection, and influenced both the development of its iconography, and its use within Liturgy.

Belief in bodily resurrection was a constant note of the Christian church in antiquity. And nowhere was it argued for more strongly than in North Africa. Saint Augustine accepted it at the time of his conversion in 386. Augustine defended resurrection, and argued that given that Christ has risen, there is resurrection of the dead. Moreover, he argued that the death and resurrection of Jesus was for the salvation of man, stating: "to achieve each resurrection of ours, the savior paid with his single life, and he pre-enacted and presented his one and only one by way of sacrament and by way of model."

The 5th century theology of Theodore of Mopsuestia provides an insight into the development of the Christian understanding of the redemptive nature of resurrection. The crucial role of the sacraments in the mediation of salvation was well accepted at the time. In Theodore's representation of the Eucharist, the sacrificial and salvific elements are combined in the "One who saved us and delivered us by the sacrifice of Himself". Theodore's interpretation of the Eucharistic rite is directed towards the triumph over the power of death brought about by the resurrection.

The emphasis on the salvific nature of the resurrection continued in Christian theology in the next centuries, e.g., in the 8th century Saint John of Damascus wrote that: "... When he had freed those who were bound from the beginning of time, Christ returned again from among the dead, having opened for us the way to resurrection" and Christian iconography of the ensuing years represented that concept.

Groups such as Jews, Muslims, Bahá'ís, and other non-Christians, as well as some liberal Christians, dispute whether Jesus actually rose from the dead. Arguments over death and resurrection claims occur at many religious debates and interfaith dialogues.

`Abdu'l-Bahá taught that Christ's resurrection was a spiritual resurrection and that the accounts in the Gospels are parables. `Abdu'l-Bahá wrote: "We explain, therefore, the meaning of Christ's resurrection in the following way: After the martyrdom of Christ the Apostles were perplexed and dismayed. The reality of Christ, which consists in His teachings, His bounties, His perfections and His spiritual power, was hidden and concealed for two or three days after His martyrdom, and had no outward appearance or manifestation—indeed, it was as though it entirely lost. For those who truly believed were few in number and even those few were perplexed and dismayed. The Cause of Christ was thus as a lifeless body. After three days the Apostles became firm and steadfast, arose to aid the Cause of Christ, resolved to promote the divine teachings and practice their Lord's admonitions, and endeavoured to serve Him. Then did the reality of Christ become resplendent, His grace shine forth, His religion find new life, and His teachings and admonitions become manifest and visible. In other words the Cause of Christ, which was like unto a lifeless body, was quickened to life and surrounded by the grace of the Holy Spirit."

Baha'is believe the Quran's statement: "And because of their saying, "We killed Messiah ʿĪsā, son of Mariam, the Messenger of Allāh",–—but they killed him not, nor crucified him, but it appeared so to them, and those who differ therein are full of doubts". means that Jesus's Spirit didn't die on the cross, however Baha'is uphold that Jesus was actually crucified in the flesh.

Some Gnostics did not believe in a literal physical resurrection. "For the gnostic any resurrection of the dead was excluded from the outset; the flesh or substance is destined to perish. 'There is no resurrection of the flesh, but only of the soul', say the so-called Archontics, a late gnostic group in Palestine".

Muslims believe that ʿĪsā (Jesus) son of Mariam (Mary) was a holy prophet with a divine message. The Islamic perspective is that Jesus was not crucified and will return to the world at the end of times. "But Allāh raised him up to Himself. And Allāh is Ever All-Powerful, All-Wise". The Quran says in Surah An-Nisa [Ch004:Verse157] "And because of their saying, "We killed Messiah ʿĪsā, son of Mariam, the Messenger of Allāh",—but they killed him not, nor crucified him, but it appeared so to them, and those who differ therein are full of doubts".

Christianity split from Judaism in the 1st century AD, and the two faiths have differed in their theology since. According to the "Toledot Yeshu", the body of Jesus was removed in the same night by a gardener named Juda, after hearing the disciples planned to steal the body of Jesus. However, "Toledot Yeshu" is not considered either canonical or normative within rabbinic literature. Van Voorst states that "Toledot Yeshu" is a medieval document set without a fixed form which is "most unlikely" to have reliable information about Jesus. The Blackwell Companion to Jesus states that the "Toledot Yeshu" has no historical facts as such, and was perhaps created as a tool for warding off conversions to Christianity.

The resurrection of Jesus has long been central to Christian faith and appears within diverse elements of the Christian tradition, from feasts to artistic depictions to religious relics. In Christian teachings, the sacraments derive their saving power from the passion and resurrection of Christ, upon which the salvation of the world entirely depends.

An example of the interweaving of the teachings on the resurrection with Christian relics is the application of the concept of "miraculous image formation" at the moment of resurrection to the Shroud of Turin. Christian authors have stated the belief that the body around whom the shroud was wrapped was not merely human, but divine, and that the image on the shroud was miraculously produced at the moment of resurrection. Quoting Pope Paul VI's statement that the shroud is "the wonderful document of His Passion, Death and Resurrection, written for us in letters of blood" author Antonio Cassanelli argues that the shroud is a deliberate divine record of the five stages of the Passion of Christ, created at the moment of resurrection.

Easter, the preeminent feast that celebrates the resurrection of Jesus, is clearly the earliest Christian festival. Since the earliest Christian times, it has focused on the redemptive act of God in the death and resurrection of Christ.

Easter is linked to the Passover and Exodus from Egypt recorded in the Old Testament through the Last Supper and crucifixion that preceded the resurrection. According to the New Testament, Jesus gave the Passover meal a new meaning, as he prepared himself and his disciples for his death in the upper room during the Last Supper. He identified the loaf of bread and cup of wine as his body soon to be sacrificed and his blood soon to be shed. states, "Get rid of the old yeast that you may be a new batch without yeast—as you really are. For Christ, our Passover lamb, has been sacrificed"; this refers to the Passover requirement to have no yeast in the house and to the allegory of Jesus as the Paschal lamb.

In the Catacombs of Rome, artists indirectly hinted at the resurrection by using images from the Old Testament such as the fiery furnace and Daniel in the Lion's den. Depictions prior to the 7th century generally showed secondary events such as the Myrrhbearers at the tomb of Jesus to convey the concept of the resurrection. An early symbol of the resurrection was the wreathed Chi Rho (Greek letters representing the word "Khristos" or "Christ"), whose origin traces to the victory of emperor Constantine I at the Battle of the Milvian Bridge in 312, which he attributed to the use of a cross on the shields of his soldiers. Constantine used the Chi Rho on his standard and his coins showed a labarum with the Chi Rho killing a serpent.

The use of a wreath around the Chi Rho symbolizes the victory of the resurrection over death, and is an early visual representation of the connection between the Crucifixion of Jesus and his triumphal resurrection, as seen in the 4th century sarcophagus of Domitilla. in Rome. Here, in the wreathed Chi Rho the death and Resurrection of Christ are shown as inseparable, and the Resurrection is not merely a happy ending tucked at the end of the life of Christ on earth. Given the use of similar symbols on the Roman military banner, this depiction also conveyed another victory, namely that of the Christian faith: the Roman soldiers who had once arrested Jesus and marched him to Calvary now walked under the banner of a resurrected Christ.

The cosmic significance of the resurrection in Western theology goes back to Saint Ambrose, who in the 4th century said that "The universe rose again in Him, the heaven rose again in Him, the earth rose again in Him, for there shall be a new heaven and a new earth". This theme developed gradually in the West, later than in the East where the resurrection had been linked from an earlier date to redemption and the renewal and rebirth of the whole world. In art this was symbolized by combining the depictions of the resurrection with the Harrowing of Hell in icons and paintings. A good example is from the Chora Church in Istanbul, where John the Baptist, Solomon and other figures are also present, depicting that Christ was not alone in the resurrection. The depiction sequence at the 10th century Hosios Loukas shows Christ as he pulls Adam from his tomb, followed by Eve, signifying the salvation of humanity after the resurrection.




</doc>
<doc id="26415" url="https://en.wikipedia.org/wiki?curid=26415" title="Rube Foster">
Rube Foster

Andrew "Rube" Foster (September 17, 1879 – December 9, 1930) was an American baseball player, manager, and executive in the Negro leagues. He was elected to the Baseball Hall of Fame in 1981.

Foster, considered by historians to have been perhaps the best African-American pitcher of the first decade of the 1900s, also founded and managed the Chicago American Giants, one of the most successful black baseball teams of the pre-integration era. Most notably, he organized the Negro National League, the first long-lasting professional league for African-American ballplayers, which operated from 1920 to 1931. He is known as the "father of Black Baseball."

Foster adopted his longtime nickname, "Rube", as his official middle name later in life.

Foster was born in Calvert, Texas on September 17, 1879. His father, also named Andrew, was a reverend and elder of the local American Methodist Episcopal Church. Foster started his professional career with the Waco Yellow Jackets, an independent black team, in 1897. Over the next few years he gradually built up a reputation among white and black fans alike, until he was signed by Frank Leland's Chicago Union Giants, a team in the top ranks of black baseball, in 1902. After a slump, he was released, and signed with a white semipro team based in Otsego, Michigan – Bardeen's Otsego Independents. According to Phil Dixon's American Baseball Chronicles: Great Teams, The 1905 Philadelphia Giants, Volume III "In completing the summer of 1902 with Otsego's multi-ethnic team––the only multi-race team with which he would ever regularly perform––Foster is reported to have pitched twelve games. He finished with a documented record of eight wins and four loses along with eighty-two documented strikeouts. Ironically, strikeout totals for five games which he appeared were not recorded. If found the totals would likely show that Foster struck out more than one-hundred batters for Otsego. In the seven games where details exist, Foster average eleven strikeouts per outing." Toward the end of the season he joined the Cuban X-Giants of Philadelphia, perhaps the best team in black baseball. The 1903 season saw Foster establish himself as the X-Giants' pitching star. In a post-season series for the eastern black championship, the X-Giants defeated Sol White's Philadelphia Giants five games to two, with Foster himself winning four games.

According to various accounts, including his own, Foster acquired the nickname "Rube" after defeating star Philadelphia Athletics left-hander Rube Waddell in a postseason exhibition game played sometime between 1902 and 1905. A newspaper story in the Trenton (NJ) "Times" from July 26, 1904, contains the earliest known example of Foster being referred to as "Rube", indicating that the supposed meeting with Waddell must have taken place earlier than that. Recent research has uncovered a game played on August 2, 1903, in which Foster met and defeated Waddell while the latter was playing under an assumed name for a semi-pro team in New York City.

Now a star, Foster jumped to the Philadelphia Giants for the 1904 season. Legend has it that John McGraw, manager of the New York Giants, hired Foster to teach the young Christy Mathewson the "fadeaway", or screwball, though historians have cast this story in doubt. During the 1904 season, Foster won 20 games against all competition (including two no-hitters) and lost six. In a rematch with Foster's old team, the Cuban X-Giants, he won two games and batted .400 in leading the Philadelphia Giants to the black championship.

In 1905, Foster (by his own account several years later) compiled a fantastic record of 51–4, though recent research has confirmed only a 25–3 record. He led the Giants to another championship series victory, this time over the Brooklyn Royal Giants. The "Philadelphia Telegraph" wrote that "Foster has never been equalled in a pitcher's box." The following season, the Philadelphia Giants helped form the International League of Independent Professional Ball Players, composed of both all-black and all-white teams in the Philadelphia and Wilmington, Delaware, areas.

In 1907, Foster's manager Sol White published his "Official Baseball Guide: History of Colored Baseball", with Foster contributing an article on "How to Pitch." However, before the season began, he and several other stars (including, most importantly, the outfielder Pete Hill) left the Philadelphia Giants for the Chicago Leland Giants, with Foster named playing manager. Under his leadership, the Lelands won 110 games (including 48 straight) and lost only ten, and took the Chicago City League pennant. The following season the Lelands tied a national championship series with the Philadelphia Giants, each team winning three games.

Foster suffered a broken leg in July 1909, but rushed himself back into the lineup in time for an October exhibition series against the Chicago Cubs. Foster, pitching the second game, squandered a 5–2 lead in the ninth inning, then lost the game on a controversial play when a Cubs runner stole home while Foster was arguing with the umpire. The Lelands lost the series, three games to nothing. The Lelands also lost the unofficial western black championship to the St. Paul Colored Gophers.

In 1910, Foster wrested legal control of the team from its founder, Frank Leland. He proceeded to put together the team he later considered his finest. He signed John Henry Lloyd away from the Philadelphia Giants; along with Hill, second baseman Grant Johnson, catcher Bruce Petway, and pitchers Frank Wickware and Pat Dougherty, Lloyd sparked the Lelands to a 123–6 record (with Foster himself contributing a 13–2 record on the mound).

The following season, Foster established a partnership with John Schorling, the son-in-law of Chicago White Sox owner Charlie Comiskey. The White Sox had just moved into Comiskey Park, and Schorling arranged for Foster's team to use the vacated South Side Park, at 39th and Wentworth. Settling into their new home (now called Schorling's Park), the Lelands became the Chicago American Giants. For the next four seasons, the American Giants claimed the western black baseball championship, though they lost a 1913 series to the Lincoln Giants for the national championship.

By 1915 Foster's first serious rival in the midwest ha emerged: C. I. Taylor's Indianapolis ABCs, who claimed the western championship after defeating the American Giants four games to none in July. One of the victories was a forfeit called after a brawl between the two teams broke out. After the series, Foster and Taylor engaged in a public dispute about that game and the championship. In 1916, both teams again claimed the western title. The continued wrangling led to calls for a black baseball league to be formed, but Foster, Taylor, and the other major clubs in the midwest were unable to come to any agreement.

By this time, Foster was pitching very little, compiling only a 2–2 record in 1915. His last recorded outing on the mound was in 1917; from this time he became purely a bench manager. As a manager and team owner, Foster was a disciplinarian. He asserted control over every aspect of the game, and set a high standard for personal conduct, appearance, and professionalism among his players. Given Schorling Park's huge dimensions, Foster developed a style of play that emphasized speed, bunting, place hitting, power pitching, and defense. He was also considered a great teacher, and many of his players themselves eventually became managers, including Pete Hill, Bruce Petway, Bingo DeMoss, Dave Malarcher, Sam Crawford, Poindexter Williams, and many others.

In 1919, Foster helped Tenny Blount finance a new club in Detroit, the Stars. He also transferred several of his veteran players there, including Hill, who was to manage the new team, and Petway. He may have been preparing the way for the formation, the following year, of the Negro National League (NNL).

In 1920, Foster, Taylor, and the owners of six other midwestern clubs met in the spring to form a professional baseball circuit for African-American teams. Foster, as president, controlled league operations, while remaining owner and manager of the American Giants. He was periodically accused of favoring his own team, especially in matters of scheduling (the Giants in the early years tended to have a disproportionate number of home games) and personnel: Foster seemed able to acquire whatever talent he needed from other clubs, such as Jimmie Lyons, the Detroit Stars' best player in 1920, who was transferred to the American Giants for 1921, or Foster's own younger brother, Bill, who joined the American Giants unwillingly when Rube forced the Memphis Red Sox to give him up in 1926. His critics believed he had organized the league primarily for purposes of booking games for the American Giants. With a stable schedule and reasonably solvent opponents, Foster was able to improve receipts at the gate. It is also true that when opposing clubs lost money, he was known to help them meet payroll, sometimes out of his own pocket. 

His American Giants won the new league's first three pennants, before being overtaken by the Kansas City Monarchs in 1923. In the same year the Hilldale Club and Bacharach Giants, the most important eastern clubs, pulled out of an agreement with the NNL and founded their own league, the Eastern Colored League (ECL). The ECL raided the older circuit for players, Foster's own ace pitcher Dave Brown among them. Eventually the two leagues reached an agreement to respect one another's contracts, and to play a world series.

After two years of finishing behind the Monarchs, Foster "cleaned house" in spring, 1925, releasing several veterans (including Lyons and pitchers Dick Whitworth and Tom Williams). On May 26, Foster was nearly asphyxiated by a gas leak in Indianapolis. Though he recovered and returned to his team, his behavior grew erratic from then on. Foster had instituted a split-season format, and his American Giants finished third in both halves.

1926 saw him complete his team's reshaping, leaving only a handful of veterans from the championship squads of 1920-22. The club finished third in the season's first half; but Foster would never finish the second. Over the years "Foster grew increasingly paranoid. Took to carrying a revolver with him everywhere he went." Suffering from serious delusions he was institutionalized midway through the 1926 season at an asylum in Kankakee, Illinois.

The American Giants and the NNL lived on—in fact, led by Dave Malarcher, the Giants won the pennant and World Series in both 1926 and 1927—but the league clearly suffered in the absence of Foster's leadership. Foster died in 1930, never having recovered his sanity, and a year later the league he had founded fell apart.

Foster is interred in Lincoln Cemetery in Blue Island, Illinois.

In 1981, Foster was elected to the National Baseball Hall of Fame. He was the first representative of the Negro leagues elected as a pioneer or executive.

On December 30, 2009, the U.S. Postal Service announced that it planned to issue a pair of postage stamps in June honoring Negro leagues Baseball. On July 17, 2010, the Postal Service issued a se-tenant pair of 44-cent, first-class, U.S. commemorative postage stamps, to honor the all-black professional baseball leagues that operated from 1920 to about 1960. One of the stamps depicts Foster, along with his name and the words "NEGRO LEAGUES BASEBALL". The stamps were formally issued at the Negro Leagues Baseball Museum, during the celebration of the museum's twentieth anniversary.

The Negro Leagues Baseball Museum hosts the annual Andrew "Rube" Foster Lecture, in September.





</doc>
<doc id="26416" url="https://en.wikipedia.org/wiki?curid=26416" title="Ring Lardner">
Ring Lardner

Ringgold Wilmer "Ring" Lardner (March 5, 1885 – September 25, 1933) was an American sports columnist and short-story writer best known for his satirical writings on sports, marriage, and the theatre. His contemporaries Ernest Hemingway, Virginia Woolf, and F. Scott Fitzgerald all professed strong admiration for his writing.

Born in Niles, Michigan, Ring Lardner was the son of wealthy parents, Henry and Lena Phillips Lardner. He was the youngest of nine children. Lardner's name came from a cousin of the same name. The cousin had been named by Lardner's uncle, Rear Admiral James L. Lardner, who had decided to name his son after a friend, Rear Admiral Cadwalader Ringgold, who was from a distinguished military family. Lardner never liked his given name and abbreviated it to Ring, naming one of his sons Ring Jr.

In childhood he wore a brace for his deformed foot until he was eleven. He also had a passion for baseball, stage, and music . He later attended the Armour Institute in Chicago. 

Lardner married Ellis Abbott of Goshen, Indiana, in 1911. They had four sons, John, James, Ring Jr., and David. 

Lardner died on September 25, 1933, at the age of 48 in East Hampton, New York, of a heart attack due to complications from tuberculosis.

Lardner started his writing career as a sports columnist, finding work with the newspaper "South Bend Times" in 1905. In 1907, he relocated to Chicago, where he got a job with the "Inter-Ocean", but within a year, he quit to work for the "Chicago Examiner", and then for the "Tribune". Two years later, Lardner was in St. Louis, writing the humorous baseball column "Pullman Pastimes" for Taylor Spink and the "Sporting News". Some of this work was the basis for his book "You Know Me Al". Within three months, he was an employee of the "Boston American".

In 1913, Lardner returned to the "Chicago Tribune", which became the home newspaper for his syndicated column "In the Wake of the News" (started by Hugh Keough, who had died in 1912). The column appeared in more than 100 newspapers, and is still published in the "Tribune". Lardner's Tribune and syndicated writing was not exclusively sports related: his dispatches from/near the World War One front were collected in the book "My Four Weeks in France", and his immersive coverage of the 1920 Democratic Convention resulted in Lardner receiving 0.5 votes on the 23rd ballot.

In 1916, Lardner published his first successful book, "You Know Me Al", an epistolary novel written in the form of letters by "Jack Keefe", a bush-league baseball player, to a friend back home. The letters made much use of the fictional author's idiosyncratic vernacular. It had initially been published as six separate but interrelated short stories in "The Saturday Evening Post", causing some to classify the book as a collection of stories, others as a novel. Like most of Lardner's stories, "You Know Me Al" employs satire, in this case to show the stupidity and avarice of a certain type of athlete. The journalist Andrew Ferguson wrote that "Ring Lardner thought of himself as primarily a sports columnist whose stuff wasn't destined to last, and he held to that absurd belief even after his first masterpiece, "You Know Me Al", was published in 1916 and earned the awed appreciation of Virginia Woolf, among other very serious, unfunny people." Ferguson termed the book one of the top five pieces of American humor writing.

Sarah Bembrey has written about a singular event in Lardner's sportswriting experience: "In 1919 something happened that changed his way of reporting about sports and changed his love for baseball. This was the Black Sox scandal when the Chicago White Sox sold out the World Series to the Cincinnati Reds. Ring was exceptionally close to the White Sox and felt he was betrayed by the team. After the scandal, Ring always wrote about sports as if there were some kink to the outcome." Lardner's last fictional baseball writing was collected in the book "Lose with a Smile" (1933).

Lardner later published such stories as "Haircut", "Some Like Them Cold", "The Golden Honeymoon", "Alibi Ike", and "A Day with Conrad Green". He also continued to write follow-up stories to "You Know Me Al", with the protagonist of that book, the headstrong but gullible Jack Keefe, experiencing various ups and downs in his major league career and in his personal life. Private Keefe's World War I training camp letters home to his friend Al were collected in the book "Treat 'Em Rough: Letters From Jack the Kaiser Killer". The sequel, "The Real Dope", followed Keefe overseas to the trenches in France.

Lardner also had a lifelong fascination with the theatre, although his only Broadway three-act successes were the thrice-filmed "Elmer The Great", co-written with George M. Cohan, and "June Moon", a comedy authored with Broadway veteran George S. Kaufman. Lardner also wrote skits for the Ziegfeld Follies. and a series of brief nonsense plays that ridiculed the conventions of the theatre using zany humor and outrageous, impossible stage directions, such as "The curtain is lowered for seven days to denote the lapse of a week."

He was a dedicated composer and lyricist: both his first – "Zanzibar" (1903) – and last – "June Moon" (1920) – published stage works included several Lardner tunes. He wrote at least one recorded song for Bert Williams, and provided the lyrics for the song "That Old Quartet" (1913) by Nathaniel D. Mann. Other collaborators of note included Aubrey Stauffer, Jerome Kern, and Vincent Youmans – with whom he toiled on the Ziegfeld Astaires musical, "Smiles" (1930).

Lardner was a good friend of F. Scott Fitzgerald and other authors of the Jazz Age. His books were published by Maxwell Perkins, who also served as Fitzgerald's editor. To create his first book of short stories Lardner had to get copies from the magazines who bought the stories — he held his own short stories in low regard and did not save copies.

Lardner was in some respects the model for the tragic character Abe North of Fitzgerald's last completed novel, "Tender Is the Night".

Lardner influenced Ernest Hemingway, who sometimes wrote articles for his high school newspaper using the pseudonym Ring Lardner, Jr. The two met during December 1928, thanks to Max Perkins, but did not become friends. Lardner's gift for dialogue heavily influenced the writer John O'Hara, who said he learned from reading Lardner "that if you wrote down speech as it is spoken truly, you produce true characters, and the opposite is also true: if your characters don't talk like people they aren't good characters" and added, "it's the attribute most lacking in American writers and almost totally lacking in the British."

J.D. Salinger referred to Lardner in two of his works, " The Catcher in the Rye" and "Franny and Zooey". The protagonist says "My favorite author is my brother D.B. and my next favourite is Ring Lardner". Wayne C. Booth mentioned Lardner's famous short story "The Haircut" in his essay "Telling and Showing."

In his movie "Eight Men Out" (1988) about the Black Sox scandal, writer-director John Sayles portrayed Lardner as one of the clear-eyed observers who was not taken in by the conspiracy. In one scene, Lardner strolls through the White Sox train, singing a parody of the song "I'm Forever Blowing Bubbles", changed to "I'm Forever Throwing Ballgames".

In 2016, Lardner was inducted into the Chicago Literary Hall of Fame.

John Lardner was a newspaperman, sports columnist, and magazine writer.

James Lardner, also a newspaperman, was killed in the Spanish Civil War fighting with the International Brigades.

Ring Lardner, Jr. was a screenwriter who was blacklisted after the Second World War as one of the Hollywood Ten, screenwriters who were incarcerated for contempt of Congress after refusing to answer questions posed by the House Un-American Activities Committee (HUAC). He won two Academy Awards for his screenplays—one before his imprisonment and blacklisting (for Woman of the Year in 1942), and one after (for M*A*S*H in 1970). His book, "The Lardners, My Family Remembered" (), is a source of information on his father.

David Lardner worked for "The New Yorker" as a general reporter and war correspondent before he was killed by a landmine near Aachen, Germany in October 1944, less than one month after his arrival in Europe.

Ring Lardner was a great-uncle to 1993 Pulitzer Prize winner George Lardner, Jr., a journalist at "The Washington Post" since 1963.




</doc>
<doc id="26417" url="https://en.wikipedia.org/wiki?curid=26417" title="River Clyde">
River Clyde

The River Clyde (, , ) is a river that flows into the Firth of Clyde in Scotland. It is the eighth-longest river in the United Kingdom, and the second-longest in Scotland. Traveling through the major city of Glasgow, it was an important river for shipbuilding and trade in the British Empire. To the Romans, it was "Clota", and in the early medieval Cumbric language, it was known as "Clud" or "Clut", and was central to the Kingdom of Strathclyde ("Teyrnas Ystrad Clut").

The Clyde is formed by the confluence of two streams, the Daer Water (the headwaters of which are dammed to form the Daer Reservoir) and the Potrail Water. The Southern Upland Way crosses both streams before they meet at Watermeetings () to form the River Clyde proper. At this point, the Clyde is only from Tweed's Well, the source of the River Tweed, and is also near Annanhead Hill, the source of the River Annan.

From there, it meanders northeastward before turning to the west, its flood plain used for many major roads in the area, until it reaches the town of Lanark. On the banks of the Clyde, the industrialists David Dale and Robert Owen built their mills and the model settlement of New Lanark. The mills harness the power of the Falls of Clyde, the most spectacular of which is Cora Linn. A hydroelectric power station still generates electricity here, although the mills are now a museum and World Heritage Site.
Between the towns of Motherwell and Hamilton, the course of the river has been altered to create an artificial loch within Strathclyde Park. Part of the original course can still be seen, and lies between the island and the east shore of the loch. The river then flows through Blantyre and Bothwell, where the ruined Bothwell Castle stands on a defensible promontory.
Past Uddingston and into the southeast of Glasgow, the river begins to widen, meandering a course through Cambuslang, Rutherglen, and Dalmarnock. Flowing past Glasgow Green, the river is artificially straightened and widened through the centre, and although the new Clyde Arc now hinders access to the traditional Broomielaw dockland area, seagoing ships can still come upriver as far as Finnieston, where the PS "Waverley" docks. From there, it flows past the shipbuilding heartlands, through Govan, Partick, Whiteinch, Scotstoun, and Clydebank, all of which housed major shipyards, of which only two remain.

The river flows out west of Glasgow, past Renfrew, and under the Erskine Bridge past Dumbarton on the north shore to the sandbank at Ardmore Point between Cardross and Helensburgh. Opposite, on the south shore, the river continues past the last Lower Clyde shipyard at Port Glasgow to Greenock, where it reaches the Tail of the Bank as the river merges into the Firth of Clyde. A significant issue of oxygen depletion in the water column has occurred at the mouth of the River Clyde.

The valley of the Clyde was the focus for the G-BASE project from the British Geological Survey in the summer of 2010.

The success of the Clyde at the beginning of the Industrial Revolution was driven by the location of Glasgow, being a port facing the Americas. Tobacco and cotton trade began the drive in the early 18th century. However, the shallow Clyde was not navigable for the largest ocean-going ships, so cargo had to be transferred at Greenock or Port Glasgow to smaller ships to sail upstream into Glasgow itself.

In 1768, John Golborne advised the narrowing of the river and the increasing of the scour by the construction of rubble jetties and the dredging of sandbanks and shoals. A particular problem was the division of the river into two shallow channels by the Dumbuck shoal near Dumbarton. After James Watt's report on this in 1769, a jetty was constructed at Longhaugh Point to block off the southern channel. This being insufficient, a training wall called the Lang Dyke was built in 1773 on the Dumbuck shoal to stop water flowing over into the southern channel. In the late 18th and early 19th centuries, hundreds of jetties were built out from the banks between Dumbuck and the Broomielaw quay in Glasgow itself. In some cases, this resulted in an immediate deepening as the constrained water flow washed away the river bottom; in others, dredging was required.

In the mid-19th century, engineers took on a much greater dredging of the Clyde, removing millions of cubic feet of silt to deepen and widen the channel. The major stumbling block in the project was a massive geological intrusion known as Elderslie Rock. As a result, the work was not completed until the 1880s. At this time, the Clyde also became an important source of inspiration for artists, such as John Atkinson Grimshaw and James Kay, willing to depict the new industrial era and the modern world.

The completion of the dredging was well-timed; as steelworking grew in the city, the channel finally became navigable all the way up to Glasgow. Shipbuilding replaced trade as the major activity on the river, and shipbuilding companies were rapidly establishing themselves on the river. Soon, the Clyde gained a reputation for being the best location for shipbuilding in the British Empire, and grew to become the world's pre-eminent shipbuilding centre. "Clydebuilt" became an industry benchmark of quality, and the river's shipyards were given contracts for prestigious ocean-going liners, as well as warships, including the "Queen Mary" and the "Queen Elizabeth 2" in later years, all built in the town of Clydebank.

From the founding of the Scott family's shipyard at Greenock in 1712 to the present day, over 25,000 ships have been built on the River Clyde and its Firth and on the tributary River Kelvin and River Cart together with boatyards at Maryhill and Kirkintilloch on the Forth & Clyde Canal and Blackhill on the Monkland Canal. In the same time, an estimated over 300 firms have engaged in shipbuilding on Clydeside, although probably a peak of 30 to 40 at any one time.

The shipbuilding firms became household names on Clydeside, but also known around the world – Denny of Dumbarton, Scott of Greenock, Lithgows of Port Glasgow, Simon and Lobnitz of Renfrew, Alexander Stephen and Sons of Linthouse, Fairfield of Govan, Inglis of Pointhouse, Barclay Curle of Whiteinch, Connell and Yarrow of Scotstoun, to name but a few. Almost as famous were the engineering firms that supplied the machinery to drive these vessels, the boilers and pumps, and steering gear – Rankin & Blackmore, Hastie's and Kincaid's of Greenock, Rowan's of Finnieston, Weir's of Cathcart, Howden's of Tradeston, and Babcock & Wilcox of Renfrew.

One 'Clyde' shipyard was not even located on one of these waterways: Alley & MacLellan's Sentinel Works in Jessie Street at Polmadie is around half a mile from the Clyde, yet it is reputed to have constructed over 500 vessels, many of which were assembled then 'knocked down' to kit form for despatch to some far distant and remote location – one such vessel being the SS "Chauncy Maples", still in service on Lake Malawi. Clyde shipbuilding reached its peak in the years just before World War I, and over 370 ships were thought to have been completed in 1913.

The first recorded Clyde racing yacht, a 46-ton cutter, was built by Scotts of Greenock in 1803, and the great Scottish yacht designer William Fyfe did not start designing yachts until 1807. The first yacht club on the Clyde was the Northern Yacht Club, which appeared in 1824 and received its Royal Charter in 1831. The club was founded to organise and encourage the sport, and by 1825, Scottish and Irish clubs were racing against each other on the Clyde. However, yachting and yacht building did not really take off until the middle of the 19th century.

The Clyde became famous worldwide for its significant contribution to yachting and yachtbuilding, and was the home of many notable designers: William Fife III, Alfred Mylne, G. L. Watson, E. McGruer, and David Boyd. It was also the home of many famous yacht yards.

Robertson's Yard started repairing boats in a small workshop at Sandbank in 1876, and went on to become one of the foremost wooden boat builders on the Clyde. The 'golden years' of Robertson's yard were in the early 20th century, when they started building classic 12- and 15-m racing yachts. More than 55 boats were built by Robertson's in preparation for the First World War, and the yard remained busy even during the Great Depression in the 1930s, as many wealthy businessmen developed a passion for yacht racing on the Clyde. During World War II, the yard was devoted to Admiralty work, producing large, high-speed Fairmile Marine motor boats (motor torpedo boats and motor gun boats). After the war, the yard built the successful one-class Loch Longs and two David Boyd-designed 12-m challengers for the America's Cup: "Sceptre" (1958) and "Sovereign" (1964). Due to difficult business conditions in 1965, the yard was turned over to GRP production work (mainly Pipers and Etchells) until it closed in 1980. During its 104-year history, Robertson's Yard built 500 boats, many of which are still sailing today.

The downfall of the Clyde as a major industrial centre came during and after World War II. Clydebank in particular was targeted by the Luftwaffe and sustained heavy damage. The immediate postwar period had a severe reduction in warship orders, which was balanced by a prolonged boom in merchant shipbuilding. By the end of the 1950s, however, the rise of other shipbuilding nations, recapitalised and highly productive, made many European yards uncompetitive. Several Clydeside yards booked a series of loss-making contracts in the hope of weathering the storm. However, by the mid-1960s, shipbuilding on the Clyde was becoming increasingly uneconomical and potentially faced collapse. This culminated in the closure of Harland and Wolff's Linthouse yard and a bankruptcy crisis facing Fairfields of Govan. The government responded by creating the Upper Clyde Shipbuilders consortium. After the consortium's controversial collapse in 1971, the Labour government of James Callaghan later passed the Aircraft and Shipbuilding Industries Act, which nationalised most of the Clyde's shipyards and grouped them with other major British shipyards such as British Shipbuilders.

Today, two major shipyards remain in operation on the Upper Clyde; they are owned by a naval defence contractor, BAE Systems Surface Ships, which specialises in the design and construction of technologically advanced warships for the Royal Navy and other navies around the world. The two yards are the former Yarrow yard at Scotstoun and Fairfields at Govan. Also, the King George V Dock is operated by the Clyde Port Authority. On the Lower Clyde, the privately owned Ferguson Shipbuilders at Port Glasgow is the last survivor of the many shipyards that once dominated Port Glasgow and Greenock – its mainstay being the construction of car ferries.

The Clyde Waterfront Regeneration project is expected to attract investment of up to £5.6bn in the area from Glasgow Green to Dumbarton. Market gardens and garden centres have grown up on the fertile plains of the Clyde Valley. Tourism has also brought many back to the riverside, especially in Glasgow, where former docklands have given way to housing and amenities on the banks in the city, for instance, the Glasgow Harbour project, the Glasgow Science Centre, and the creation of the Scottish Exhibition and Conference Centre. With the migration of the commercial Port of Glasgow downstream to the deeper waters of the Firth of Clyde, the river has been extensively cleaned up, once having a very poor reputation for pollution and sewage, to make it suitable for recreational use.

Organic chemical pollution contained in sediments of the Clyde estuary have been evaluated by British Geological Survey.<ref name="Chemical Signatures of the Anthropocene in the Clyde Estuary, UK: Sediment hosted Pb, 207/206Pb, Polyaromatic Hydrocarbon (PAH) and Polychlorinated Biphenyl (PCB) Pollution Records."></ref> Surface sediments from the Glasgow reaches of the Clyde, Cunningar to Milton, contained polyaromatic hydrocarbons (PAH) from 630 µg/kg to 23,711 µg/kg and polychlorinated biphenyl (PCB) in the range of 5 to 130.5 µg/kg, which classifies these sediments as non-toxic. However, a later study showed PCB concentrations as high as 5,797 µg/kg, which is above published threshold levels for the chlorinated compounds. Comparison of individual PAH compounds with different thermal stabilities shows that the source of PAH pollution in the Clyde changes downriver. PAH in the inner Clyde (Cunningar to Milton) are from combustion sources (vehicle exhaust, coal burning), whereas PAH in the outer Clyde are from petroleum spills 

The amount and type of sedimentary pollution contained in the Clyde is closely related to the area's industrial history. Assessment of the changing type of pollution through time (1750 to 2002) has been gained through the collection of seven sediment cores of one metre's depth which were dated using lead concentrations and changing lead isotope ratios. The sediments showed a long but declining history of coal usage and an increasing reliance on petroleum fuels from about the 1950s. Thereafter, declining hydrocarbon pollution was followed by the onset (1950s), peak (1965–1977), and decline (after 1980s) in total PCB concentrations.

Although pollution from heavy industry and power generation is falling, evidence indicates that man-made pollution from new synthetic compounds in electrical products and textiles is on the increase. The amounts of 16 polybrominated diphenyl ether (PBDE) compounds used as flame retardants in televisions, computers, and furniture upholstery were measured in sediment cores collected from six sites between Princes Dock and Greenock. Comparison of the amounts of PBDE compounds revealed a decline in certain compounds in line with the European ban on production of mixtures containing environmentally harmful PBDE with eight and nine bromine atoms and a rise in the less harmful mixture composed of ten bromine atoms.

The Clyde is central to the "Para Handy" novels of Neil Munro, and subsequent adaptations, and features in novels by Alasdair Gray, Matthew Fitt, and Robin Jenkins. It appears in the "Ossian" poetry of James Macpherson, as well as the works of John Wilson, William McGonagall, Edwin Morgan, Norman McCaig, Douglas Dunn and W.S. Graham. It features in artworks by, amongst others, William McTaggart, J.M.W. Turner, Robert Salmon, John Atkinson Grimshaw, Stanley Spencer, and George Wyllie. 

The Clyde appears prominently in the films "Young Adam", "Sweet Sixteen", "Just a Boys' Game", and "Down Where The Buffalo Go", and was the subject of the Academy Award-winning film documentary "Seawards the Great Ships". It is referenced in the traditional folk song "Black Is the Color (of My True Love's Hair)", as well as "Song of the Clyde", popularised by Kenneth McKellar.




</doc>
<doc id="26418" url="https://en.wikipedia.org/wiki?curid=26418" title="Reactor">
Reactor




</doc>
<doc id="26421" url="https://en.wikipedia.org/wiki?curid=26421" title="List of reproductive issues">
List of reproductive issues

List of reproductive issues may refer to:


</doc>
<doc id="26426" url="https://en.wikipedia.org/wiki?curid=26426" title="Rational root theorem">
Rational root theorem

In algebra, the rational root theorem (or rational root test, rational zero theorem, rational zero test or "p"/"q" theorem) states a constraint on rational solutions of a polynomial equation
with integer coefficients. Solutions of the equation are roots (equivalently, zeroes) of the polynomial on the left side of the equation.

If "a" and "a" are nonzero,
then each rational solution "x",
when written as a fraction "x" = "p"/"q" in lowest terms (i.e., the greatest common divisor of "p" and "q" is 1), satisfies

The rational root theorem is a special case (for a single linear factor) of Gauss's lemma on the factorization of polynomials. The integral root theorem is a special case of the rational root theorem if the leading coefficient "a" = 1.

The theorem is used in order to determine whether a polynomial has any rational roots, and if so to find them. Since the theorem gives constraints on the numerator and denominator of the fully reduced rational roots as being divisors of certain numbers, all possible combinations of divisors can be checked and either the rational roots will be found or it will be determined that there are none. If one or more are found, they can be factored out of the polynomial, resulting in a polynomial of lower degree whose roots are also roots of the original polynomial.

The general cubic equation

with integer coefficients has three solutions in the complex plane. If it is found by the rational root test that there are no rational solutions, then the only way to express the solutions algebraically is to use cube roots. But if the test finds three rational solutions, then the cube roots are avoided. And if precisely one rational solution "r" is found to exist, then ("x" – "r") can be factored out of the cubic polynomial using polynomial long division, leaving a quadratic polynomial whose two roots are the remaining two roots of the cubic; and these can be found using the quadratic formula, again avoiding the use of cube roots.

Let "P"("x") = "a""x" + "a""x" + ... + "a""x" + "a" for some "a", ..., "a" ∈ Z, and suppose "P"("p"/"q") = 0 for some coprime "p", "q" ∈ Z:

If we multiply both sides by "q", shift the constant term to the right hand side, and factor out "p" on the left hand side, we get

We see that "p" times the integer quantity in parentheses equals −"a""q", so "p" divides "a""q". But "p" is coprime to "q" and therefore to "q", so by (the generalized form of) Euclid's lemma it must divide the remaining factor "a" of the product.

If we instead shift the leading term to the right hand side and factor out "q" on the left hand side, we get

And for similar reasons, we can conclude that "q" divides "a".

Should there be a nontrivial factor dividing all the coefficients of the polynomial, then one can divide by the greatest common divisor of the coefficients so as to obtain a primitive polynomial in the sense of Gauss's lemma; this does not alter the set of rational roots and only strengthens the divisibility conditions. That lemma says that if the polynomial factors in , then it also factors in as a product of primitive polynomials. Now any rational root corresponds to a factor of degree 1 in of the polynomial, and its primitive representative is then , assuming that "p" and "q" are coprime. But any multiple in of has leading term divisible by "q" and constant term divisible by "p", which proves the statement. This argument shows that more generally, any irreducible factor of "P" can be supposed to have integer coefficients, and leading and constant coefficients dividing the corresponding coefficients of "P".

In the polynomial

any rational root fully reduced would have to have a numerator that divides evenly into 1 and a denominator that divides evenly into 2. Hence the only possible rational roots are ±1/2 and ±1; since neither of these equates the polynomial to zero, it has no rational roots.

In the polynomial

the only possible rational roots would have a numerator that divides 6 and a denominator that divides 1, limiting the possibilities to ±1, ±2, ±3, and ±6. Of these, 1, 2, and –3 equate the polynomial to zero, and hence are its rational roots. (In fact these are its only roots since a cubic has only three roots; in general, a polynomial could have some rational and some irrational roots.)

Every rational root of the polynomial
must be among the numbers symbolically indicated by
which gives the list of 8 possible answers:

These root candidates can be tested using Horner's method (for instance). In this particular case there is exactly one rational root. If a root candidate does not cause the polynomial to equal zero, it can be used to shorten the list of remaining candidates. For example, "x" = 1 does not work, as the polynomial then equals 1. This means that substituting "x" = 1 + "t" yields a polynomial in "t" with constant term 1, while the coefficient of "t" remains the same as the coefficient of "x". Applying the rational root theorem thus yields the following possible roots for "t":

Therefore,

Root candidates that do not occur on both lists are ruled out. The list of rational root candidates has thus shrunk to just "x" = 2 and "x" = 2/3.

If "k" rational roots are found ("k" ≥ 1), Horner's method will also yield a polynomial of degree "n" − "k" whose roots, together with the rational roots, are exactly the roots of the original polynomial. It may also be the case that none of the candidates is a solution; in this case the equation setting the polynomial equal to 0 has no rational solution. If the equation lacks a constant term "a", then 0 is one of the rational solutions of the equation.





</doc>
<doc id="26427" url="https://en.wikipedia.org/wiki?curid=26427" title="Round Table">
Round Table

The Round Table is King Arthur's famed table in the Arthurian legend, around which he and his knights congregate. As its name suggests, it has no head, implying that everyone who sits there has equal status. The table was first described in 1155 by Wace, who relied on previous depictions of Arthur's fabulous retinue. The symbolism of the Round Table developed over time; by the close of the 12th century it had come to represent the chivalric order associated with Arthur's court, the Knights of the Round Table.

Though the Round Table itself is not mentioned, the concept of Arthur having a marvelous court made up of many prominent warriors is much older. Geoffrey of Monmouth says that after establishing peace throughout Britain, Arthur "increased his personal entourage by inviting very distinguished men from far-distant kingdoms to join it." The code of chivalry so important in later romance figures in as well, as Geoffrey says Arthur established "such a code of courtliness in his household that he inspired peoples living far away to imitate him." 

Long before Geoffrey, Arthur's court was well known to Welsh storytellers; in the romance "Culhwch and Olwen", written around 1100, the protagonist Culhwch invokes the names of 225 individuals affiliated with Arthur. In fact, the fame of Arthur's entourage became so prominent in Welsh tradition that in the later additions to the Welsh Triads, the formula tying named individuals to "Arthur's Court" in the triad titles began to supersede the older "Island of Britain" formula. Though the code of chivalry crucial to later continental romances dealing with the Round Table is mostly absent from the earlier Welsh material, some passages of "Culhwch and Olwen" seem to prefigure it, for instance when Arthur explains the ethos of his court, saying "[w]e are nobles as long as we are sought out: the greater the bounty we may give, the greater our nobility, fame and honour."

Though no Round Table appears in the early Welsh texts, Arthur is associated with various items of household furniture. The earliest of these is Saint Carannog's mystical floating altar in that saint's 12th century "Vita"; in the story Arthur has found the altar and attempts unsuccessfully to use it for a table, and returns it to Carannog in exchange for the saint ridding the land of a meddlesome dragon. Arthur's household furniture figures into local topographical folklore throughout Britain as early as the early 12th century, with various landmarks being named "Arthur's Seat", "Arthur's Oven", and "Arthur's Bed-chamber". 

A henge at Eamont Bridge near Penrith, Cumbria is known as "King Arthur's Round Table". The still-visible Roman amphitheatre at Caerleon has been associated with the Round Table and has been suggested as a possible source for the legend. Following archaeological discoveries at the Roman ruins in Chester, some writers suggested that the Chester Roman Amphitheatre was the true prototype of the Round Table, but the English Heritage Commission, acting as consultants to a History Channel documentary in which the claim was made, declared that there was no archaeological basis to the story.

The Round Table first appeared in Wace's "Roman de Brut", a Norman language adaptation of Geoffrey of Monmouth's "Historia Regum Britanniae" finished in 1155. Wace says Arthur created the Round Table to prevent quarrels among his barons, none of whom would accept a lower place than the others. Layamon added to the story when he adapted Wace's work into the Middle English "Brut" in the early 13th century, saying that the quarrel between Arthur's vassals led to violence at a Yuletide feast. In response a Cornish carpenter built an enormous but easily transportable Round Table to prevent further dispute. Wace claims he was not the source of the Round Table; both he and Layamon credited it instead to the Bretons. Some scholars have doubted this claim, while others believe it may be true. There is some similarity between the chroniclers' description of the Round Table and a custom recorded in Celtic stories, in which warriors sit in a circle around the king or lead warrior, in some cases feuding over the order of precedence as in Layamon. There is a possibility that Wace, contrary to his own claims, derived Arthur's round table not from any Breton source, but rather from medieval biographies of Charlemagne—notably Einhard's "Vita Caroli" and Notker the Stammerer's "De Carolo Magno"—in which the king is said to have possessed a round table decorated with a map of Rome.

The Round Table takes on new dimensions in the romances of the late 12th and early 13th century, where it becomes a symbol of the famed order of chivalry which flourishes under Arthur. In Robert de Boron's "Merlin", written around the 1190s, the wizard Merlin creates the Round Table in imitation of the table of the Last Supper and of Joseph of Arimathea's Holy Grail table. This table, here made for Arthur's father Uther Pendragon rather than Arthur himself, has twelve seats and one empty place to mark the betrayal of Judas. This seat must remain empty until the coming of the knight who will achieve the Grail. The Didot "Perceval", a prose continuation of Robert's work, takes up the story, and the knight Percival sits in the seat and initiates the Grail quest.

The prose cycles of the 13th century, the Lancelot-Grail cycle and the Post-Vulgate Cycle, further adapt the chivalric attributes of the Round Table. Here it is the perfect knight Galahad, rather than Percival, who assumes the empty seat, now called the Siege Perilous. Galahad's arrival marks the start of the Grail quest as well as the end of the Arthurian era. In these works the Round Table is kept by King Leodegrance of Cameliard after Uther's death; Arthur inherits it when he marries Leodegrance's daughter Guinevere. Other versions treat the Round Table differently, for instance Arthurian works from Italy often distinguish between the "Old Table" of Uther's time and Arthur's "New Table."

During the Middle Ages, festivals called Round Tables were celebrated throughout Europe in imitation of Arthur's court. These events featured jousting, dancing, and feasting, and in some cases attending knights assumed the identities of Arthur's entourage. The earliest of these was held in Cyprus in 1223 to celebrate a knighting. Round Tables were popular in various European countries through the rest of the Middle Ages and were at times very elaborate; René of Anjou even erected an Arthurian castle for his 1446 Round Table. On December 19, 1566, Mary, Queen of Scots gave a feast in Stirling Castle with 30 guests at an imagined replica of Arthur's table during the masque-themed celebrations of the baptism of the future James VI.

The artifact known as the "Winchester Round Table", a large tabletop hanging in Winchester Castle bearing the names of various knights of Arthur's court, was probably created for a Round Table tournament. The current paintwork is late; it was done by order of Henry VIII of England for Holy Roman Emperor Charles V's 1522 state visit, and depicts Henry himself sitting in Arthur's seat above a Tudor rose. The table itself is considerably older; dendrochronology calculates the date of construction to 1250–1280—during the reign of Edward I—using timber from store felled over a period of years. Edward was an Arthurian enthusiast who attended at least five Round Tables and hosted one himself in 1299, which may have been the occasion for the creation of the Winchester Round Table. Martin Biddle, from an examination of Edward's financial accounts, links it instead with a tournament Edward held near Winchester on April 20, 1290, to mark the betrothal of one of his daughters.





</doc>
<doc id="26428" url="https://en.wikipedia.org/wiki?curid=26428" title="Rosetta Stone">
Rosetta Stone

The Rosetta Stone is a granodiorite stele, found in 1799, inscribed with three versions of a decree issued at Memphis, Egypt in 196 BC during the Ptolemaic dynasty on behalf of King Ptolemy V. The top and middle texts are in Ancient Egyptian using hieroglyphic script and Demotic script, respectively, while the bottom is in Ancient Greek. As the decree has only minor differences between the three versions, the Rosetta Stone proved to be the key to deciphering Egyptian hieroglyphs.

The stone, carved in black granodiorite during the Hellenistic period, is believed to have originally been displayed within a temple, possibly at nearby Sais. It was probably moved during the early Christian or medieval period, and was eventually used as building material in the construction of Fort Julien near the town of Rashid (Rosetta) in the Nile Delta. It was rediscovered there in July 1799 by a French soldier named Pierre-François Bouchard during the Napoleonic campaign in Egypt. It was the first Ancient Egyptian bilingual text recovered in modern times, and it aroused widespread public interest with its potential to decipher this previously untranslated hieroglyphic language. Lithographic copies and plaster casts began circulating among European museums and scholars. Meanwhile, British troops defeated the French in Egypt in 1801, and the original stone came into British possession under the Capitulation of Alexandria and was transported to London. It has been on public display at the British Museum almost continuously since 1802, and is the most-visited object there.

Study of the decree was already under way when the first full translation of the Greek text appeared in 1803. It was 20 years, however, before the transliteration of the Egyptian scripts was announced by Jean-François Champollion in Paris in 1822; it took longer still before scholars were able to read Ancient Egyptian inscriptions and literature confidently. Major advances in the decoding were recognition that the stone offered three versions of the same text (1799); that the demotic text used phonetic characters to spell foreign names (1802); that the hieroglyphic text did so as well, and had pervasive similarities to the demotic (Thomas Young, 1814); and that, in addition to being used for foreign names, phonetic characters were also used to spell native Egyptian words (Champollion, 1822–1824).

Ever since its rediscovery, the stone has been the focus of nationalist rivalries, including its transfer from French to British possession during the Napoleonic Wars, a long-running dispute over the relative value of Young and Champollion's contributions to the decipherment and, since 2003, demands for the stone's return to Egypt.

Two other fragmentary copies of the same decree were discovered later, and several similar Egyptian bilingual or trilingual inscriptions are now known, including two slightly earlier Ptolemaic decrees (the Decree of Canopus in 238 BC, and the Memphis decree of Ptolemy IV, c. 218 BC). The Rosetta Stone is, therefore, no longer unique, but it was the essential key to modern understanding of Ancient Egyptian literature and civilisation. The term "Rosetta Stone" is now used in other contexts as the name for the essential clue to a new field of knowledge.

The Rosetta Stone is listed as "a stone of black granodiorite, bearing three inscriptions ... found at Rosetta" in a contemporary catalogue of the artefacts discovered by the French expedition and surrendered to British troops in 1801. At some period after its arrival in London, the inscriptions on the stone were coloured in white chalk to make them more legible, and the remaining surface was covered with a layer of carnauba wax designed to protect the Rosetta Stone from visitors' fingers. This gave a dark colour to the stone that led to its mistaken identification as black basalt. These additions were removed when the stone was cleaned in 1999, revealing the original dark grey tint of the rock, the sparkle of its crystalline structure, and a pink vein running across the top left corner. Comparisons with the Klemm collection of Egyptian rock samples showed a close resemblance to rock from a small granodiorite quarry at Gebel Tingar on the west bank of the Nile, west of Elephantine in the region of Aswan; the pink vein is typical of granodiorite from this region.

The Rosetta Stone is high at its highest point, wide, and thick. It weighs approximately . It bears three inscriptions: the top register in Ancient Egyptian hieroglyphs, the second in the Egyptian Demotic script, and the third in Ancient Greek. The front surface is polished and the inscriptions lightly incised on it; the sides of the stone are smoothed, but the back is only roughly worked, presumably because this would have not been visible when it was erected.

The Rosetta Stone is a fragment of a larger stele. No additional fragments were found in later searches of the Rosetta site. Owing to its damaged state, none of the three texts is absolutely complete. The top register, composed of Egyptian hieroglyphs, suffered the most damage. Only the last 14 lines of the hieroglyphic text can be seen; all of them are broken on the right side, and 12 of them on the left. The following register of demotic text has survived best; it has 32 lines, of which the first 14 are slightly damaged on the right side. The final register of Greek text contains 54 lines, of which the first 27 survive in full; the rest are increasingly fragmentary due to a diagonal break at the bottom right of the stone.

The stele was erected after the coronation of King Ptolemy V and was inscribed with a decree that established the divine cult of the new ruler. The decree was issued by a congress of priests who gathered at Memphis. The date is given as "4 Xandicus" in the Macedonian calendar and "18 Meshir" in the Egyptian calendar, which corresponds to March 27, 196 BC. The year is stated as the ninth year of Ptolemy V's reign (equated with 197/196 BC), which is confirmed by four priests named who officiated in that same year: Aëtus son of Aëtus was priest of the divine cults of Alexander the Great and the five Ptolemies down to Ptolemy V himself; his three colleagues, named in turn in the inscription, led the worship of Berenice Euergetis (wife of Ptolemy III), Arsinoe Philadelphos (wife and sister of Ptolemy II), and Arsinoe Philopator, mother of Ptolemy V. However, a second date is also given in the Greek and hieroglyphic texts, corresponding to , the official anniversary of Ptolemy's coronation. The inscription in demotic conflicts with this, listing consecutive days in March for the decree and the anniversary. It is uncertain why such discrepancies exist, but it is clear that the decree was issued in 196 BC and that it was designed to re-establish the rule of the Ptolemaic kings over Egypt.

The decree was issued during a turbulent period in Egyptian history. Ptolemy V Epiphanes reigned from 204 to 181 BC, the son of Ptolemy IV Philopator and his wife and sister Arsinoe. He had become ruler at the age of five after the sudden death of both of his parents, who were murdered in a conspiracy that involved Ptolemy IV's mistress Agathoclea, according to contemporary sources. The conspirators effectively ruled Egypt as Ptolemy V's guardians until a revolt broke out two years later under general Tlepolemus, when Agathoclea and her family were lynched by a mob in Alexandria. Tlepolemus, in turn, was replaced as guardian in 201 BC by Aristomenes of Alyzia, who was chief minister at the time of the Memphis decree.

Political forces beyond the borders of Egypt exacerbated the internal problems of the Ptolemaic kingdom. Antiochus III the Great and Philip V of Macedon had made a pact to divide Egypt's overseas possessions. Philip had seized several islands and cities in Caria and Thrace, while the Battle of Panium (198 BC) had resulted in the transfer of Coele-Syria, including Judaea, from the Ptolemies to the Seleucids. Meanwhile, in the south of Egypt, there was a long-standing revolt that had begun during the reign of Ptolemy IV, led by Horwennefer and by his successor Ankhwennefer. Both the war and the internal revolt were still ongoing when the young Ptolemy V was officially crowned at Memphis at the age of 12 (seven years after the start of his reign), and the Memphis decree issued.

The stele is a late example of a class of donation stelae, which depicts the reigning monarch granting a tax exemption to the resident priesthood. Pharaohs had erected these stelae over the previous 2,000 years, the earliest examples dating from the Egyptian Old Kingdom. In earlier periods, all such decrees were issued by the king himself, but the Memphis decree was issued by the priests, as the maintainers of traditional Egyptian culture. The decree records that Ptolemy V gave a gift of silver and grain to the temples. It also records that there was particularly high flooding of the Nile in the eighth year of his reign, and he had the excess waters dammed for the benefit of the farmers. In return for these concessions, the priesthood pledged that the king's birthday and coronation days would be celebrated annually, and that all the priests of Egypt would serve him alongside the other gods. The decree concludes with the instruction that a copy was to be placed in every temple, inscribed in the "language of the gods" (hieroglyphs), the "language of documents" (demotic), and the "language of the Greeks" as used by the Ptolemaic government.

Securing the favour of the priesthood was essential for the Ptolemaic kings to retain effective rule over the populace. The High Priests of Memphis—where the king was crowned—were particularly important, as they were the highest religious authorities of the time and had influence throughout the kingdom. Given that the decree was issued at Memphis, the ancient capital of Egypt, rather than Alexandria, the centre of government of the ruling Ptolemies, it is evident that the young king was anxious to gain their active support. Thus, although the government of Egypt had been Greek-speaking ever since the conquests of Alexander the Great, the Memphis decree, like the two preceding decrees in the series, included texts in Egyptian to show its connection to the general populace by way of the literate Egyptian priesthood.

There exists no one definitive English translation of the decree because of the minor differences between the three original texts, and because modern understanding of the ancient languages continues to develop. An up-to-date translation by R. S. Simpson appears on the British Museum website, based on the demotic text. It can be compared with Edwyn R. Bevan's full translation in "The House of Ptolemy" (1927), based on the Greek text with footnote comments on variations between this and the two Egyptian texts.

The stele almost certainly did not originate in the town of Rashid (Rosetta) where it was found, but more likely came from a temple site farther inland, possibly the royal town of Sais. The temple from which it originally came was probably closed around AD 392 when Eastern Roman emperor Theodosius I ordered the closing of all non-Christian temples of worship. The original stele broke at some point, its largest piece becoming what we now know as the Rosetta Stone. Ancient Egyptian temples were later used as quarries for new construction, and the Rosetta Stone probably was re-used in this manner. Later it was incorporated in the foundations of a fortress constructed by the Mameluke Sultan Qaitbay (c. 1416/18–1496) to defend the Bolbitine branch of the Nile at Rashid. There it lay for at least another three centuries until its rediscovery.

Two other inscriptions containing the same Memphis decree have been found since the discovery of the Rosetta Stone: the Nubayrah Stele, and an inscription found at the Temple of Philae (on the Philae obelisk). Unlike the Rosetta Stone, the hieroglyphic texts of these other copies of the decree were relatively intact. The Rosetta Stone had been deciphered long before they were found, but later Egyptologists, including Wallis Budge, used these other copies to refine the reconstruction of the hieroglyphs that must have been used in the lost portions of the hieroglyphic text on the Rosetta Stone.

Napoleon's 1798 campaign in Egypt came at (and helped cause) the beginning of a burst of Egyptomania in Europe, and especially France. A corps of 167 technical experts ("savants"), known as the "Commission des Sciences et des Arts", accompanied the French expeditionary army to Egypt. On 1799, French soldiers under the command of Colonel d'Hautpoul were strengthening the defences of Fort Julien, a couple of miles north-east of the Egyptian port city of Rosetta (modern-day Rashid). Lieutenant Pierre-François Bouchard spotted a slab with inscriptions on one side that the soldiers had uncovered. He and d'Hautpoul saw at once that it might be important and informed General Jacques-François Menou, who happened to be at Rosetta. The find was announced to Napoleon's newly founded scientific association in Cairo, the Institut d'Égypte, in a report by Commission member Michel Ange Lancret noting that it contained three inscriptions, the first in hieroglyphs and the third in Greek, and rightly suggesting that the three inscriptions were versions of the same text. Lancret's report, dated 1799, was read to a meeting of the Institute soon after . Bouchard, meanwhile, transported the stone to Cairo for examination by scholars. Napoleon himself inspected what had already begun to be called "la Pierre de Rosette", the Rosetta Stone, shortly before his return to France in August 1799.

The discovery was reported in September in "Courrier de l'Égypte", the official newspaper of the French expedition. The anonymous reporter expressed a hope that the stone might one day be the key to deciphering hieroglyphs. In 1800, three of the Commission's technical experts devised ways to make copies of the texts on the stone. One of these experts was Jean-Joseph Marcel, a printer and gifted linguist, who is credited as the first to recognise that the middle text was written in the Egyptian Demotic script, rarely used for stone inscriptions and seldom seen by scholars at that time, rather than Syriac as had originally been thought. It was artist and inventor Nicolas-Jacques Conté who found a way to use the stone itself as a printing block to reproduce the inscription. A slightly different method was adopted by Antoine Galland. The prints that resulted were taken to Paris by General Charles Dugua. Scholars in Europe were now able to see the inscriptions and attempt to read them.

After Napoleon's departure, French troops held off British and Ottoman attacks for another 18 months. In March 1801, the British landed at Aboukir Bay. Menou was now in command of the French expedition. His troops, including the Commission, marched north towards the Mediterranean coast to meet the enemy, transporting the stone along with many other antiquities. He was defeated in battle, and the remnant of his army retreated to Alexandria where they were surrounded and besieged, the stone now inside the city. Menou surrendered on August 30.

After the surrender, a dispute arose over the fate of the French archaeological and scientific discoveries in Egypt, including the artefacts, biological specimens, notes, plans, and drawings collected by the members of the commission. Menou refused to hand them over, claiming that they belonged to the Institute. British General John Hely-Hutchinson refused to relieve the city until Menou gave in. Scholars Edward Daniel Clarke and William Richard Hamilton, newly arrived from England, agreed to examine the collections in Alexandria and claimed to have found many artefacts that the French had not revealed. In a letter home, Clarke said that "we found much more in their possession than was represented or imagined".

Hutchinson claimed that all materials were property of the British Crown, but French scholar Étienne Geoffroy Saint-Hilaire told Clarke and Hamilton that the French would rather burn all their discoveries than turn them over, referring ominously to the destruction of the Library of Alexandria. Clarke and Hamilton pleaded the French scholars' case to Hutchinson, who finally agreed that items such as natural history specimens would be the scholars' private property. Menou quickly claimed the stone, too, as his private property. Hutchinson was equally aware of the stone's unique value and rejected Menou's claim. Eventually an agreement was reached, and the transfer of the objects was incorporated into the Capitulation of Alexandria signed by representatives of the British, French, and Ottoman forces.

It is not clear exactly how the stone was transferred into British hands, as contemporary accounts differ. Colonel Tomkyns Hilgrove Turner was to escort it to England, but he claimed later that he had personally seized it from Menou and carried it away on a gun-carriage. In a much more detailed account, Edward Daniel Clarke stated that a French "officer and member of the Institute" had taken him, his student John Cripps, and Hamilton secretly into the back streets behind Menou's residence and revealed the stone hidden under protective carpets among Menou's baggage. According to Clarke, their informant feared that the stone might be stolen if French soldiers saw it. Hutchinson was informed at once and the stone was taken away—possibly by Turner and his gun-carriage.

Turner brought the stone to England aboard the captured French frigate HMS "Egyptienne", landing in Portsmouth in February 1802. His orders were to present it and the other antiquities to King George III. The King, represented by War Secretary Lord Hobart, directed that it should be placed in the British Museum. According to Turner's narrative, he and Hobart agreed that the stone should be presented to scholars at the Society of Antiquaries of London, of which Turner was a member, before its final deposit in the museum. It was first seen and discussed there at a meeting on 1802.

In 1802 the Society created four plaster casts of the inscriptions, which were given to the universities of Oxford, Cambridge, and Edinburgh and to Trinity College Dublin. Soon afterwards, prints of the inscriptions were made and circulated to European scholars. Before the end of 1802, the stone was transferred to the British Museum, where it is located today. New inscriptions painted in white on the left and right edges of the slab stated that it was "Captured in Egypt by the British Army in 1801" and "Presented by King George III".

The stone has been exhibited almost continuously in the British Museum since June 1802. During the middle of the 19th century, it was given the inventory number "EA 24", "EA" standing for "Egyptian Antiquities". It was part of a collection of ancient Egyptian monuments captured from the French expedition, including a sarcophagus of Nectanebo II (EA 10), the statue of a high priest of Amun (EA 81), and a large granite fist (EA 9). The objects were soon discovered to be too heavy for the floors of Montagu House (the original building of The British Museum), and they were transferred to a new extension that was built onto the mansion. The Rosetta Stone was transferred to the sculpture gallery in 1834 shortly after Montagu House was demolished and replaced by the building that now houses the British Museum. According to the museum's records, the Rosetta Stone is its most-visited single object, and a simple image of it has been the museum's best selling postcard for several decades.
The Rosetta Stone was originally displayed at a slight angle from the horizontal, and rested within a metal cradle that was made for it, which involved shaving off very small portions of its sides to ensure that the cradle fitted securely. It originally had no protective covering, and it was found necessary by 1847 to place it in a protective frame, despite the presence of attendants to ensure that it was not touched by visitors. Since 2004, the conserved stone has been on display in a specially built case in the centre of the Egyptian Sculpture Gallery. A replica of the Rosetta Stone is now available in the King's Library of the British Museum, without a case and free to touch, as it would have appeared to early 19th-century visitors.

The museum was concerned about heavy bombing in London towards the end of the First World War in 1917, and the Rosetta Stone was moved to safety, along with other portable objects of value. The stone spent the next two years below ground level in a station of the Postal Tube Railway at Mount Pleasant near Holborn. Other than during wartime, the Rosetta Stone has left the British Museum only once: for one month in October 1972, to be displayed alongside Champollion's "Lettre" at the Louvre in Paris on the 150th anniversary of the letter's publication. Even when the Rosetta Stone was undergoing conservation measures in 1999, the work was done in the gallery so that it could remain visible to the public.

Prior to the discovery of the Rosetta Stone and its eventual decipherment, the ancient Egyptian language and script had not been understood since shortly before the fall of the Roman Empire. The usage of the hieroglyphic script had become increasingly specialised even in the later Pharaonic period; by the 4th century AD, few Egyptians were capable of reading them. Monumental use of hieroglyphs ceased after the closing of all non-Christian temples in 391 by Roman Emperor Theodosius I; the last known inscription is dated to , found at Philae and known as the Graffito of Esmet-Akhom.

Hieroglyphs retained their pictorial appearance, and classical authors emphasised this aspect, in sharp contrast to the Greek and Roman alphabets. In the 5th century, the priest Horapollo wrote "Hieroglyphica", an explanation of almost 200 glyphs. His work was believed to be authoritative, yet it was misleading in many ways, and this and other works were a lasting impediment to the understanding of Egyptian writing. Later attempts at decipherment were made by Arab historians in medieval Egypt during the 9th and 10th centuries. Dhul-Nun al-Misri and Ibn Wahshiyya were the first historians to study hieroglyphs, by comparing them to the contemporary Coptic language used by Coptic priests in their time. The study of hieroglyphs continued with fruitless attempts at decipherment by European scholars, notably Johannes Goropius Becanus in the 16th century, Athanasius Kircher in the 17th, and Georg Zoëga in the 18th. The discovery of the Rosetta Stone in 1799 provided critical missing information, gradually revealed by a succession of scholars, that eventually allowed Jean-François Champollion to solve the puzzle that Kircher had called the riddle of the Sphinx.

The Greek text on the Rosetta Stone provided the starting point. Ancient Greek was widely known to scholars, but they were not familiar with details of its use in the Hellenistic period as a government language in Ptolemaic Egypt; large-scale discoveries of Greek papyri were a long way in the future. Thus, the earliest translations of the Greek text of the stone show the translators still struggling with the historical context and with administrative and religious jargon. Stephen Weston verbally presented an English translation of the Greek text at a Society of Antiquaries meeting in April 1802.

Meanwhile, two of the lithographic copies made in Egypt had reached the Institut de France in Paris in 1801. There, librarian and antiquarian Gabriel de La Porte du Theil set to work on a translation of the Greek, but he was dispatched elsewhere on Napoleon's orders almost immediately, and he left his unfinished work in the hands of colleague Hubert-Pascal Ameilhon. Ameilhon produced the first published translations of the Greek text in 1803, in both Latin and French to ensure that they would circulate widely. At Cambridge, Richard Porson worked on the missing lower right corner of the Greek text. He produced a skilful suggested reconstruction, which was soon being circulated by the Society of Antiquaries alongside its prints of the inscription. At almost the same moment, Christian Gottlob Heyne in Göttingen was making a new Latin translation of the Greek text that was more reliable than Ameilhon's, which was first published in 1803. It was reprinted by the Society of Antiquaries in a special issue of its journal "Archaeologia" in 1811, alongside Weston's previously unpublished English translation, Colonel Turner's narrative, and other documents.

At the time of the stone's discovery, Swedish diplomat and scholar Johan David Åkerblad was working on a little-known script of which some examples had recently been found in Egypt, which came to be known as Demotic. He called it "cursive Coptic" because he was convinced that it was used to record some form of the Coptic language (the direct descendant of Ancient Egyptian), although it had few similarities with the later Coptic script. French Orientalist Antoine-Isaac Silvestre de Sacy had been discussing this work with Åkerblad when he received one of the early lithographic prints of the Rosetta Stone in 1801 from Jean-Antoine Chaptal, French minister of the interior. He realised that the middle text was in this same script. He and Åkerblad set to work, both focusing on the middle text and assuming that the script was alphabetical. They attempted to identify the points where Greek names ought to occur within this unknown text, by comparing it with the Greek. In 1802, Silvestre de Sacy reported to Chaptal that he had successfully identified five names (""Alexandros"", ""Alexandreia"", ""Ptolemaios"", ""Arsinoe"", and Ptolemy's title ""Epiphanes""), while Åkerblad published an alphabet of 29 letters (more than half of which were correct) that he had identified from the Greek names in the demotic text. They could not, however, identify the remaining characters in the Demotic text, which, as is now known, included ideographic and other symbols alongside the phonetic ones.

Silvestre de Sacy eventually gave up work on the stone, but he was to make another contribution. In 1811, prompted by discussions with a Chinese student about Chinese script, Silvestre de Sacy considered a suggestion made by Georg Zoëga in 1797 that the foreign names in Egyptian hieroglyphic inscriptions might be written phonetically; he also recalled that as early as 1761, Jean-Jacques Barthélemy had suggested that the characters enclosed in cartouches in hieroglyphic inscriptions were proper names. Thus, when Thomas Young, foreign secretary of the Royal Society of London, wrote to him about the stone in 1814, Silvestre de Sacy suggested in reply that in attempting to read the hieroglyphic text, Young might look for cartouches that ought to contain Greek names and try to identify phonetic characters in them.

Young did so, with two results that together paved the way for the final decipherment. In the hieroglyphic text, he discovered the phonetic characters ""p t o l m e s"" (in today's transliteration ""p t w l m y s"") that were used to write the Greek name ""Ptolemaios"". He also noticed that these characters resembled the equivalent ones in the Demotic script, and went on to note as many as 80 similarities between the hieroglyphic and Demotic texts on the stone, an important discovery because the two scripts were previously thought to be entirely different from one another. This led him to deduce correctly that the Demotic script was only partly phonetic, also consisting of ideographic characters imitated from hieroglyphs. Young's new insights were prominent in the long article "Egypt" that he contributed to the "Encyclopædia Britannica" in 1819. He could make no further progress, however.

In 1814 Young first exchanged correspondence about the stone with Jean-François Champollion, a teacher at Grenoble who had produced a scholarly work on ancient Egypt. Champollion saw copies of the brief hieroglyphic and Greek inscriptions of the Philae obelisk in 1822, on which William John Bankes had tentatively noted the names "Ptolemaios" and "Kleopatra" in both languages. From this, Champollion identified the phonetic characters "k l e o p a t r a" (in today's transliteration "q l i҆ w p ꜣ d r ꜣ.t"). On the basis of this and the foreign names on the Rosetta Stone, he quickly constructed an alphabet of phonetic hieroglyphic characters, which appears in his famous 1822 ""Lettre à M. Dacier"" sent to Bon-Joseph Dacier, secretary of the Paris Académie des Inscriptions et Belles-Lettres and immediately published by the Académie. In the postscript Champollion notes that similar phonetic characters seemed to occur in both Greek and Egyptian names, a hypothesis confirmed in 1823, when he identified the names of pharaohs Ramesses and Thutmose written in cartouches at Abu Simbel. These far older hieroglyphic inscriptions had been copied by Bankes and sent to Champollion by Jean-Nicolas Huyot. From this point, the stories of the Rosetta Stone and the decipherment of Egyptian hieroglyphs diverge, as Champollion drew on many other texts to develop an Ancient Egyptian grammar and a hieroglyphic dictionary which were published after his death in 1832.

Work on the stone now focused on fuller understanding of the texts and their contexts by comparing the three versions with one another. In 1824 Classical scholar Antoine-Jean Letronne promised to prepare a new literal translation of the Greek text for Champollion's use. Champollion in return promised an analysis of all the points at which the three texts seemed to differ. Following Champollion's sudden death in 1832, his draft of this analysis could not be found, and Letronne's work stalled. François Salvolini, Champollion's former student and assistant, died in 1838, and this analysis and other missing drafts were found among his papers. This discovery incidentally demonstrated that Salvolini's own publication on the stone, published in 1837, was plagiarism. Letronne was at last able to complete his commentary on the Greek text and his new French translation of it, which appeared in 1841. During the early 1850s, German Egyptologists Heinrich Brugsch and Max Uhlemann produced revised Latin translations based on the demotic and hieroglyphic texts. The first English translation followed in 1858, the work of three members of the Philomathean Society at the University of Pennsylvania.

Whether one of the three texts was the standard version, from which the other two were originally translated, is a question that has remained controversial. Letronne attempted to show in 1841 that the Greek version, the product of the Egyptian government under the Macedonian Ptolemies, was the original. Among recent authors, John Ray has stated that "the hieroglyphs were the most important of the scripts on the stone: they were there for the gods to read, and the more learned of their priesthood". Philippe Derchain and Heinz Josef Thissen have argued that all three versions were composed simultaneously, while Stephen Quirke sees in the decree "an intricate coalescence of three vital textual traditions". Richard Parkinson points out that the hieroglyphic version strays from archaic formalism and occasionally lapses into language closer to that of the demotic register that the priests more commonly used in everyday life. The fact that the three versions cannot be matched word for word helps to explain why its decipherment has been more difficult than originally expected, especially for those original scholars who were expecting an exact bilingual key to Egyptian hieroglyphs.

Even before the Salvolini affair, disputes over precedence and plagiarism punctuated the decipherment story. Thomas Young's work is acknowledged in Champollion's 1822 "Lettre à M. Dacier", but incompletely, according to British critics: for example, James Browne, a sub-editor on the "Encyclopædia Britannica" (which had published Young's 1819 article), anonymously contributed a series of review articles to the "Edinburgh Review" in 1823, praising Young's work highly and alleging that the "unscrupulous" Champollion plagiarised it. These articles were translated into French by Julius Klaproth and published in book form in 1827. Young's own 1823 publication reasserted the contribution that he had made. The early deaths of Young (1829) and Champollion (1832) did not put an end to these disputes. The authoritative work on the stone by British Museum curator E. A. Wallis Budge (1904) gives special emphasis to Young's contribution compared with Champollion's. In the early 1970s, French visitors complained that the portrait of Champollion was smaller than one of Young on an adjacent information panel; English visitors complained that the opposite was true. The portraits were in fact the same size.

Egypt first requested the return of the Rosetta Stone in July 2003, on the British Museum's 250th anniversary. Zahi Hawass, the chief of Egypt's Supreme Council of Antiquities, asked that the stele be repatriated to Egypt, commenting that it was the "icon of our Egyptian identity". He repeated the proposal two years later in Paris, listing the stone as one of several key items belonging to Egypt's cultural heritage, a list which also included: the iconic bust of Nefertiti in the Egyptian Museum of Berlin; a statue of the Great Pyramid architect Hemiunu in the Roemer-und-Pelizaeus-Museum in Hildesheim, Germany; the Dendara Temple Zodiac in the Louvre in Paris; and the bust of Ankhhaf from the Museum of Fine Arts, Boston.

During 2005, the British Museum presented Egypt with a full-sized replica of the stele. This was initially displayed in the renovated Rashid National Museum, close to the site where the stone was found. In November 2005, Hawass suggested a three-month loan of the Rosetta Stone, while reiterating the eventual goal of a permanent return. In December 2009, he proposed to drop his claim for the permanent return of the Rosetta Stone if the British Museum lent the stone to Egypt for three months for the opening of the Grand Egyptian Museum at Giza in 2013. These requests were refused.

As John Ray has observed, "the day may come when the stone has spent longer in the British Museum than it ever did in Rosetta." There is strong opposition among national museums to the repatriation of objects of international cultural significance such as the Rosetta Stone. In response to repeated Greek requests for return of the Elgin Marbles from the Parthenon and similar requests to other museums around the world, in 2002 over 30 of the world's leading museums — including the British Museum, the Louvre, the Pergamon Museum in Berlin and the Metropolitan Museum in New York City — issued a joint statement declaring that "objects acquired in earlier times must be viewed in the light of different sensitivities and values reflective of that earlier era" and that "museums serve not just the citizens of one nation but the people of every nation".

The Digital Rosetta Stone Project is a collaboration between the University of Leipzig's Digital Humanities Chair and the Institute of Egyptology; and the British Museum and the Digital Epigraphy and Archaeology Project, to make available an annotated transcription of each of the three versions of the text, as well as providing them in XML format. High-resolution images and a high-resolution 3D model of the stone will also be produced.

The term "Rosetta stone" has been used idiomatically to represent a crucial key in the process of decryption of encoded information, especially when a small but representative sample is recognised as the clue to understanding a larger whole. According to the "Oxford English Dictionary", the first figurative use of the term appeared in the 1902 edition of the "Encyclopædia Britannica" relating to an entry on the chemical analysis of glucose. Another use of the phrase is found in H. G. Wells' 1933 novel "The Shape of Things to Come", where the protagonist finds a manuscript written in shorthand that provides a key to understanding additional scattered material that is sketched out in both longhand and on typewriter.

Since then, the term has been widely used in other contexts. For example, Nobel laureate Theodor W. Hänsch in a 1979 Scientific American article on spectroscopy wrote that "the spectrum of the hydrogen atoms has proved to be the Rosetta stone of modern physics: once this pattern of lines had been deciphered much else could also be understood". Fully understanding the key set of genes to the human leucocyte antigen has been described as "the Rosetta Stone of immunology". The flowering plant "Arabidopsis thaliana" has been called the "Rosetta Stone of flowering time". A Gamma ray burst (GRB) found in conjunction with a supernova has been called a Rosetta Stone for understanding the origin of GRBs. The technique of Doppler echocardiography has been called a Rosetta Stone for clinicians trying to understand the complex process by which the left ventricle of the human heart can be filled during various forms of diastolic dysfunction.

The name has also become used in various forms of translation software. Rosetta Stone is a brand of language-learning software published by Rosetta Stone Ltd., headquartered in Arlington County, Virginia, US. "Rosetta" is the name of a "lightweight dynamic translator" that enables applications compiled for PowerPC processors to run on Apple systems using an x86 processor. "Rosetta" is an online language translation tool to help localisation of software, developed and maintained by Canonical as part of the Launchpad project. Similarly, Rosetta@home is a distributed computing project for predicting protein structures from amino acid sequences (or "translating" sequence into structure). The Rosetta Project brings language specialists and native speakers together to develop a meaningful survey and near-permanent archive of 1,500 languages, intended to last from AD 2000 to 12,000. The European Space Agency's "Rosetta" spacecraft was launched to study the comet 67P/Churyumov–Gerasimenko in the hope that determining its composition will reveal the origin of the Solar System.




</doc>
<doc id="26429" url="https://en.wikipedia.org/wiki?curid=26429" title="Redshirt (stock character)">
Redshirt (stock character)

A "redshirt" is a stock character in fiction who dies soon after being introduced. The term originates from the original "" (NBC, 1966–69) television series in which the red-shirted security personnel frequently die during episodes. Redshirt deaths are often used to dramatize the potential peril that the main characters face.

In "Star Trek", red-uniformed security officers and engineers who accompany the main characters on landing parties often suffer quick deaths. The trope first appears in the episode "What Are Little Girls Made Of?" (1966). Of the 59 crew members killed in the series, 43 (73%) were wearing red shirts. The "" book "Legends of the Ferengi" says Starfleet security personnel "rarely survive beyond the second act break". An episode of "" titled "" (1998) also references red as a sort of bad luck omen, in which the plot centers around a group of cadets calling themselves "Red Squad", almost all of whom die in the episode. The cinematic reboot of the franchise features a character named Olson (portrayed by Greg Ellis) who dies early on during a mission; he wears a red uniform in homage to the trope from the original series.

In other media, the term "redshirt" and images of characters wearing red shirts represent characters destined for suffering or death.



</doc>
<doc id="26431" url="https://en.wikipedia.org/wiki?curid=26431" title="Receptor">
Receptor

Receptor may refer to:



</doc>
<doc id="26432" url="https://en.wikipedia.org/wiki?curid=26432" title="Resolution-class submarine">
Resolution-class submarine

The "Resolution" class was a class of four nuclear ballistic missile submarines (SSBN) built for the Royal Navy as part of the UK Polaris programme. Each submarine was armed with up to 16 UGM-27 Polaris A-3 nuclear missiles.

The class comprised , , and . They were built by Vickers Armstrong in Barrow-in-Furness and Cammell Laird in Birkenhead between 1964 and 1968. All four boats were based at HM Naval Base Clyde (HMS "Neptune"), west of Glasgow, Scotland.

The "Resolution" class was the launch platform for the United Kingdom's strategic nuclear deterrent from the late 1960s until 1994, when it was replaced by the carrying the Trident II.

During the 1950s and early 1960s, the United Kingdom's nuclear deterrent was based on the RAF's V-bombers. But in the early 1960s developments in radar and surface-to-air missiles made it clear that bombers were becoming vulnerable, and would be unlikely to penetrate Soviet airspace. Free-fall nuclear weapons would no longer be a credible deterrent.

To address this problem, in May 1960 the British Prime Minister, Harold Macmillan arranged a deal with US President Eisenhower to equip the V bombers with the US-designed AGM-48 Skybolt. The Skybolt was a range ballistic missile that allowed the launching bombers to remain well away from Soviet defences and launch attacks that would be basically invulnerable. With this range, the V bombers would have to fly only a few hundred miles from their bases before being in range for an attack on Moscow.

Under the agreement the UK's contribution to the programme was limited to developing suitable mounting points on the Avro Vulcan bomber, installing the required guidance systems that fed the missiles updated positioning information, and development of a British version of the US W47 warhead to arm it, the RE.179 .

The incoming Kennedy administration expressed serious doubts of both Skybolt and the US deterrent force in general. Robert McNamara was highly critical of the US bomber fleet, which he saw as obsolete in an age of ICBMs. Skybolt was seen simply as a means of continuing the existence of a system he no longer considered credible, and given the rapidly improving capabilities of ICBM inertial guidance systems, a precision strike capability with free-fall bombs would no longer be needed. McNamara was equally concerned about the UK also having its own nuclear force, and worried that the US could be drawn into a war by the UK. He wanted to bring the UK into a dual-key arrangement.

McNamara first broached the idea of cancelling Skybolt with the British in November 1962. When this was reported in the House of Commons, a storm of protest broke out. A meeting was arranged to settle the issue, and Macmillan stated in no uncertain terms that the UK would be retaining their independent deterrent capability, no matter what the cost. With development of their Polaris-derived warheads well along, a suitable launch platform would be developed, if need be.

Faced with a clear failure in policy terms, Kennedy gave up on the idea of strong-arming Britain into accepting a dual-key arrangement. By the end of the series of meetings, the UK had gained the much more impressive Polaris system, and would start development of a new submarine to launch it. The SSBNs would then take over the nuclear deterrent role from the RAF's V bombers from 1968 onwards.

Two pairs of the boats were ordered in May 1963 from Vickers Shipbuilding Ltd, Barrow in Furness and from Cammell Laird and Co. Ltd, Birkenhead. The option of buying a fifth unit, planned as , was cancelled in February 1965. Traditional battleship names were used, signifying that they were the capital ships of their time.

Vickers Armstrong in Barrow-in-Furness constructed "Resolution" and "Repulse" and Cammell Laird in Birkenhead constructed "Renown" and "Revenge". The construction was unusual in that the bow and stern were constructed separately before being assembled together with the American-designed missile compartment.

The design was a modification of the fleet submarine, but greatly extended to incorporate the missile compartment between the fin and the nuclear reactor. The length was , breadth , height and the displacement submerged and surfaced. A Rolls-Royce pressurised water reactor (PWR1) and English Electric Company turbines gave them a speed of and they could dive to depths of . Sixteen Polaris A3 missiles were carried, in two rows of eight. For emergencies there was a diesel generator and six torpedo tubes located at the bow, firing the Tigerfish wire-guided homing torpedoes. The submarines put to sea with a crew of 143.

According to former head of the Royal Corps of Naval Constructors R.J. Daniel, the "Resolution"-class SSBNs possessed five features that were envied by the United States Navy: the machinery loading hatch, automated hovering system, welded hull valves, standardised valves, and raft-mounted propulsion machinery.

The first to be completed was "Resolution", laid down in February 1964 and launched in September 1966. After commissioning in 1967 she underwent a long period of sea trials, culminating in the test firing of a Polaris missile from the USAF Eastern Test Range off Cape Kennedy at 11:15 on 15 February 1968. "Resolution" commenced her first operational patrol on 15 June 1968, beginning 28 years of Polaris patrols. The class were part of the 10th Submarine Squadron, all based at Faslane Naval Base, Scotland.

All four of the class underwent conversion during the 1980s so that they could be fitted with the Polaris A3TK missile which was fitted with the British-developed Chevaline MRV system.

As the newer "Vanguard"-class submarines entered service, the "Resolution" class was eventually retired and all boats laid up at Rosyth dockyard with their used nuclear fuel removed. All four will eventually be disposed of via MOD's Submarine Dismantling Project (SDP). This project will begin in 2016 with as the first submarine to prove the technique. The selected method will first remove all Low-level radioactive waste from the vessel, followed by the more radioactive intermediate-level waste. All non-radioactive material in the remainder of the vessel will be recycled for re-use by conventional ship-breaking techniques.

New methods of project management were used in the refits of the "Resolution" class, including:






</doc>
<doc id="26435" url="https://en.wikipedia.org/wiki?curid=26435" title="Roger Angell">
Roger Angell

Roger Angell (born September 19, 1920) is an American essayist known for his writing on sports, especially baseball. He has been a regular contributor to "The New Yorker" and was its chief fiction editor for many years. He has written numerous works of fiction, non-fiction, and criticism, and for many years wrote an annual Christmas poem for "The New Yorker".

He received a number of awards for his writing, including the George Polk Award for Commentary in 1980, the "Kenyon Review" Award for Literary Achievement in 2005 along with Umberto Eco, and the inaugural PEN/ESPN Lifetime Achievement Award for Literary Sports Writing in 2011.

He was elected a Fellow of the American Academy of Arts and Sciences in 2007 and is a long-time ex-officio member of the council of the Authors Guild.

He was named the 2014 recipient of the J. G. Taylor Spink Award by the Baseball Writers' Association of America on December 10, 2013.

Angell is the son of Katharine Sergeant Angell White, "The New Yorker"’s first fiction editor, and the stepson of renowned essayist E. B. White, but was raised for the most part by his father, Ernest Angell, an attorney who became head of the American Civil Liberties Union.

Angell is a 1938 graduate of the Pomfret School and attended Harvard University. He served in the United States Army Air Forces during World War II.

Angell's earliest published works were pieces of short fiction and personal narratives. Several of these pieces were collected in "The Stone Arbor and Other Stories" (1960) and "A Day in the Life of Roger Angell" (1970).

He first contributed to "The New Yorker" in March, 1944. His contributions have continued into 2018.

In 1948, Angell was employed at Holiday Magazine, a travel magazine that featured literary writers. 

He first wrote professionally about baseball in 1962, when William Shawn, editor of "The New Yorker", had him travel to Florida to write about spring training.

Angell has been called the "Poet Laureate of baseball" but dislikes the term. In a review of "Once More Around the Park" for the "Journal of Sport History", Richard C. Crepeau wrote that "Gone for Good", Angell's essay on the career of Steve Blass, "may be the best piece that anyone has ever written on baseball or any other sport". Angell was one of several personalities who gave commentaries throughout the Ken Burns series, "Baseball", in 1994.

One of the most striking items from Angell's essays is one ultimately published in "Season Ticket", involving a spring training trip to see the Baltimore Orioles, where he interviews Earl Weaver, then the manager of the Orioles, about Cal Ripken, Jr., who was about to start his rookie season. Angell quotes Weaver as saying about Ripken that, at whichever position the team decides (between shortstop and third base), "his manager can just write his name into the lineup every day for the next fifteen years; that's how good he is". Starting that year, Ripken in fact was written into lineups every day for more than fifteen years, setting the all-time consecutive-games-played streak of 2,632 games. Angell's quotation of Weaver stands as one of the most incredibly prescient (and well-documented) "first-guesses" in recorded literature.

Angell has two children, Alice and John Henry. He had Alice and another daughter, Callie, with his first wife Evelyn, and John Henry with Carol. Callie Angell, who was an authority on the films of Andy Warhol, committed suicide on May 5, 2010, in Manhattan, where she worked as a curator at the Whitney Museum of American Art; she was 62. In a 2014 essay, Angell mentioned her death -- "the oceanic force and mystery of that event"—and his struggle to comprehend that "a beautiful daughter of mine, my oldest child, had ended her life." Alice Angell lives in Portland, Maine, and John Henry Angell lives in Portland, Oregon.

His second wife, Carol Rogge Angell, to whom he was married for 48 years, died on April 10, 2012, of metastatic breast cancer at the age of 73. In 2014, he married Margaret Moorman, a writer and teacher.

In 2019, University of Nebraska Press will publish "No Place I Would Rather Be: Roger Angell and a Life in Baseball Writing", a book about Angell's career written by Joe Bonomo.


</doc>
