<doc id="27737" url="https://en.wikipedia.org/wiki?curid=27737" title="Saint Kitts">
Saint Kitts

Saint Kitts, also known more formally as Saint Christopher Island, is an island in the West Indies. The west side of the island borders the Caribbean Sea, and the eastern coast faces the Atlantic Ocean. Saint Kitts and the neighbouring island of Nevis constitute one country: the Federation of Saint Kitts and Nevis. Saint Kitts and Nevis are separated by a shallow channel known as "The Narrows".

Saint Kitts became home to the first Caribbean British and French colonies in the mid-1620s. Along with the island nation of Nevis, Saint Kitts was a member of the British West Indies until gaining independence on September 19, 1983.

The island is one of the Leeward Islands in the Lesser Antilles. It is situated about southeast of Miami, Florida. The land area of St. Kitts is about , being approximately long and on average about across.

Saint Kitts has a population of around 40,000, the majority of whom are mainly of African descent. The primary language is English, with a literacy rate of approximately 98%. Residents call themselves Kittitians.

Brimstone Hill Fortress National Park, a UNESCO World Heritage Site, is the largest fortress ever built in the Eastern Caribbean. The island of Saint Kitts is home to the Warner Park Cricket Stadium, which was used to host 2007 Cricket World Cup matches. This made St. Kitts and Nevis the smallest nation to ever host a World Cup event. Saint Kitts is also home to several institutions of higher education, including Ross University School of Veterinary Medicine, Windsor University School of Medicine, and the University of Medicine and Health Sciences.

The capital of the two-island nation, and also its largest port, is the town of Basseterre on Saint Kitts. There is a modern facility for handling large cruise ships there. A ring road goes around the perimeter of the island with smaller roads branching off it; the interior of the island is too steep for habitation.

Saint Kitts is away from Sint Eustatius to the north and from Nevis to the south. St. Kitts has three distinct groups of volcanic peaks: the North West or Mount Misery Range; the Middle or Verchilds Range and the South East or Olivees Range. The highest peak is Mount Liamuiga, formerly Mount Misery, a dormant volcano 1,156 m high.

The youngest volcanic center is Mt. Liamuiga, 5 km in diameter and rising to an elevation of 1155 m. Its last eruption was 1620 years ago, corresponding with the Steel Dust series of pyroclastic deposits on the western flank. The Mansion Series of pyroclastic deposits and andesite with basalt layers occur on the northern flank, along with mudflows. This volcano has a crater 900 m wide and 244 m deep, plus two distinct parasitic domes consisting primarily of andesite, Brimstone Hill and Sandy Point Hill which is coalesced with Farm Flat. Brimstone Hill is noted for having limestone on its flanks, which was dragged upward with the formation of the dome 44,400 years ago. Mt. Liamuiga partially overlays the Middle Range to the southeast. This Middle Range is another stratovolcano 976 m in height with a small summit crater containing a lake. Next in line is the 900 m South East Range, 1 Myr in age, consisting of four peaks. Ottley's dome and Monkey Hill dome are on the flanks, while the older volcanoes represented by Canada Hills, and Conaree Hills lie past the airport and Bassaterre on the southeast flank. The Salt Dome Peninsula contains the oldest volcanic deposits, 2.3-2.77 Myr in age, consisting of at least nine Pelean domes rising up to 319 m in height, which includes Williams Hill and St. Anthony's Peaks.

During the last Ice Age, the sea level was up to lower and St. Kitts and Nevis were one island along with Saba and Sint Eustatius (also known as Statia).

St. Kitts was originally settled by pre-agricultural, pre-ceramic "Archaic people", who migrated south down the archipelago from Florida. In a few hundred years they disappeared, to be replaced by the ceramic-using and agriculturalist Saladoid people around 100 BC, who migrated to St. Kitts north up the archipelago from the banks of the Orinoco River in Venezuela. Around 800 AD, they were replaced by the Igneri people, members of the Arawak group.

Around 1300, the Kalinago, or Carib people arrived on the islands. These war-like people quickly dispersed the Igneri, and forced them northwards to the Greater Antilles. They named Saint Kitts "Liamuiga" meaning "fertile island", and would likely have expanded further north if not for the arrival of Europeans.

A Spanish expedition under Christopher Columbus arrived and claimed the island for Spain in 1493. 

The first English colony was established in 1623, followed by a French colony in 1625. The English and French briefly united to massacre the local Kalinago (preempting a Kalinago plan to massacre the Europeans), and then partitioned the island, with the English colonists in the middle and the French on either end. In 1629, a Spanish force sent to clear the islands of foreign settlement seized St. Kitts. The English settlement was rebuilt following the 1630 peace between England and Spain.

The island alternated repeatedly between English (then British) and French control during the 17th and 18th centuries, as one power took the whole island, only to have it switch hands due to treaties or military action. Parts of the island were heavily fortified, as exemplified by the UNESCO World Heritage Site at Brimstone Hill and the now-crumbling Fort Charles.

Since 1783, St. Kitts has been affiliated with the Kingdom of Great Britain, which became the United Kingdom.

The island originally produced tobacco; but it changed to sugar cane in 1640, due to stiff competition from the colony of Virginia. The labour-intensive cultivation of sugar cane was the reason for the large-scale importation of African slaves. The importation began almost immediately upon the arrival of Europeans to the region.

The purchasing of enslaved Africans was outlawed in the British Empire by an Act of Parliament in 1807. Slavery was abolished by an Act of Parliament which became law on 1 August 1834. This emancipation was followed by four years of apprenticeship, put in place to protect the planters from losing their labour force.

August the 1st is now celebrated as a public holiday and is called Emancipation Day. In 1883, St. Kitts, Nevis, and Anguilla were all linked under one presidency, located on St. Kitts, to the dismay of the Nevisians and Anguillans. Anguilla eventually separated out of this arrangement, in 1971, after an armed raid on St. Kitts.

Sugar production continued to dominate the local economy until 2005, when, after 365 years of having a mono-culture, the government closed the sugar industry. This was due to huge losses and European Union plans to greatly cut sugar prices.

For purposes of governing, the island is divided into nine parishes:

St. Kitts & Nevis uses the Eastern Caribbean dollar, which maintains a fixed exchange rate of 2.7-to-one with the United States dollar. The US dollar is almost as widely accepted as the Eastern Caribbean dollar.

For hundreds of years, St. Kitts operated as a sugar monoculture, but due to decreasing profitability, the government closed the industry in 2005. Tourism is a major and growing source of income to the island, although the number and density of resorts is less than on many other Caribbean islands. Transportation, non-sugar agriculture, manufacturing and construction are the other growing sectors of the economy.

St. Kitts is dependent on tourism to drive its economy. Tourism has been increasing since 1978. In 2009, there were 587,479 arrivals to Saint Kitts compared to 379,473 in 2007, which represents an increase of just under 40% growth in a two-year period. As tourism grows, the demand for vacation property increases in conjunction.

St. Kitts & Nevis also acquires foreign direct investment from their unique citizenship by investment program, outlined in their Citizenship Act of 1984. Interested parties can acquire citizenship if they pass the government's strict background checks and make an investment into an approved real estate development. Purchasers who pass government due diligence and make a minimum investment of US$400,000, into qualifying government approved real estate, are entitled to apply for citizenship of the Federation of St. Kitts and Nevis. Many projects are approved under the citizenship by investment program, and the main qualifying projects of interest can be found within the Henley Estates market overview .

The country hosts an annual St. Kitts Music Festival.

Robert L. Bradshaw International Airport serves St. Kitts. British Airways (BA) flies in twice a week from London and daily connections from Charlotte, Miami and New York are available.

The Basseterre Ferry Terminal facilitates travel between St. Kitts and sister island Nevis.

The narrow-gauge (30 inches) St Kitts Scenic Railway circles the island and offers passenger service from its headquarters near the airport, although the service is geared more for tourists than as day-to-day transportation for residents. Built between 1912 and 1926 to haul sugar cane from farms to the sugar factory in Basseterre, since 2003 the railway has offered a 3.5 hour, 30-mile circle tour of the island on specially designed double-decker open-air coaches, with 12 miles of the trip being by bus.

Saint Kitts is or was the residence of:





</doc>
<doc id="27739" url="https://en.wikipedia.org/wiki?curid=27739" title="Shogi">
Shogi

The earliest predecessor of the game, chaturanga, originated in India in the 6th century. Shogi in its present form was played as early as the 16th century, while a direct ancestor without the drop rule was recorded from 1210 in a historical document "Nichūreki", which is an edited copy of "Shōchūreki" and "Kaichūreki" from the late Heian period (c. 1120).

Shogi was the earliest chess variant to allow captured pieces to be returned to the board by the capturing player. This rule is speculated to have been invented in the 15th century and possibly connected to the practice of 15th century mercenaries switching loyalties when captured instead of being killed.

Two players face each other across a board composed of rectangles in a grid of 9 "ranks" (rows) by 9 "files" (columns). In Japanese they are called "Sente" (first player) and "Gote" (second player), but in English are conventionally referred to as Black and White, with Black the first player.
The board is nearly always rectangular, and the rectangles are undifferentiated by marking or color. Pairs of dots mark the players' promotion zones.

Each player has a set of 20 wedge-shaped pieces of slightly different sizes. Except for the kings, opposing pieces are undifferentiated by marking or color. Pieces face "forward" (toward the opponent's side); this shows who controls the piece during play. The pieces from largest (most important) to smallest (least important) are:

Several of these names were chosen to correspond to their rough equivalents in international chess, and not as literal translations of the Japanese names.

Each piece has its name written on its surface in the form of two "kanji" (Chinese characters used in Japanese), usually in black ink. On the reverse side of each piece, other than the king and gold general, are one or two other characters, in amateur sets often in a different color (usually red); this side is turned face up during play to indicate that the piece has been promoted.

Following is a table of the pieces with their Japanese representations and English equivalents. The abbreviations are used for game notation and often when referring to the pieces in speech in Japanese.

English speakers sometimes refer to promoted bishops as "horses" and promoted rooks as "dragons", after their Japanese names, and generally use the Japanese term "tokin" for promoted pawns. Silver generals and gold generals are commonly referred to simply as "silvers" and "golds".

The characters inscribed on the reverse sides of the pieces to indicate promotion may be in red ink, and are usually cursive. The characters on the backs of the pieces that promote to gold generals are cursive variants of 'gold', becoming more cursive (more abbreviated) as the value of the original piece decreases. These cursive forms have these equivalents in print: for promoted silver, for promoted knight, for promoted lance, and for promoted pawn (tokin). Another typographic convention has abbreviated versions of the original values, with a reduced number of strokes: for a promoted knight , for a promoted lance , and the as above for a promoted silver, but for "tokin".

The suggestion that the Japanese characters have deterred Western players from learning shogi has led to "Westernized" or "international" pieces which use iconic symbols instead of characters. Most players soon learn to recognize the characters, however, partially because the traditional pieces are already iconic by size, with more powerful pieces being larger. As a result, Westernized pieces have never become popular. Bilingual pieces with both Japanese characters and English captions have been developed as have pieces with animal cartoons.

Each player sets up friendly pieces facing forward (toward the opponent).


Traditionally, the order of placing the pieces on the board is determined. There are two commonly used orders, the "Ōhashi" order 大橋流 and the "Itō" order 伊藤流. Placement sets pieces with multiples (generals, knights, lances) from left to right in all cases, and follows the order:


A "furigoma" 振り駒 'piece toss' is used to decide who moves first. One of the players tosses five pawns. If the number of tokins (promoted pawns, と) facing up is higher than unpromoted pawns (歩), then the player who tossed the pawns plays "gote" 後手 'white' (that is, getting the second move). Among amateur tournaments, the higher-ranked player or defending champion performs the piece toss. In professional games, the furigoma is done on the behalf of the higher-ranked player/champion by the timekeeper who kneels by the side of the higher-ranked player and tosses the pawn pieces onto a silk cloth. In friendly amateur games, a player will ask the opponent to toss the pawns out of politeness. Otherwise, the person who tosses the pawns can be determined by Rock–paper–scissors.

After the piece toss "furigoma," the game proceeds. If multiple games are played, then players alternate turns for who goes first in subsequent games. (The terms "Black" and "White" are used to differentiate sides although there is no difference in the color of the pieces.) For each turn, a player may either move a piece that is currently on the board (and potentially promote it, capture an opposing piece, or both) or else drop a piece that has been previously captured onto a square of the board. These options are explained below.

Professional games are timed as in international chess, but professional shogi players are almost never expected to keep time in their games. Instead a timekeeper is assigned, typically an apprentice professional. Time limits are much longer than in international chess (9 hours a side plus extra time in the prestigious "Meijin" title match), and in addition "byōyomi" (literally "second counting") is employed. This means that when the ordinary time has run out, the player will from that point on have a certain amount of time to complete every move (a "byōyomi" period), typically upwards of one minute. The final ten seconds are counted down, and if the time expires the player to move loses the game immediately. Amateurs often play with electronic clocks that beep out the final ten seconds of a "byōyomi" period, with a prolonged beep for the last five.

The usual goal of a game is for one player to checkmate the other player's king, winning the game.

Most shogi pieces can move only to an adjacent square. A few may move across the board, and one jumps over intervening pieces.

The lance, bishop, and rook are "ranging" pieces: They can move any number of squares along a straight line limited only by intervening pieces and the edge of the board. If an opposing piece intervenes, it may be captured by removing it from the board and replacing it with the moving piece. If a friendly piece intervenes, the moving piece must stop short of that square; if the friendly piece is adjacent, the moving piece may not move in that direction at all.

A king (玉/王) moves one square in any direction, orthogonal or diagonal.

A rook (飛) moves any number of squares in an orthogonal direction.

A bishop (角) moves any number of squares in a diagonal direction. Because they cannot move orthogonally, the players' unpromoted bishops can reach only half the squares of the board, unless one is captured and then dropped.

A gold general (金) moves one square orthogonally, or one square diagonally forward, giving it six possible destinations. It cannot move diagonally backwards.

A silver general (銀) moves one square diagonally, or one square straight forward, giving it five possible destinations. Because an unpromoted silver can retreat more easily than a promoted one, it is common to leave a silver unpromoted at the far side of the board. (See Promotion).

A knight (桂) jumps at an angle intermediate to orthogonal and diagonal, amounting to one square straight forward plus one square diagonally forward, in a single move. Thus the knight has two possible forward destinations. Unlike international chess knights, shogi knights cannot move to the sides or in a backwards direction. The knight is the only piece that ignores intervening pieces on the way to its destination. It is not blocked from moving if the square in front of it is occupied, but neither can it capture a piece on that square. It is often useful to leave a knight unpromoted at the far side of the board. A knight "must" promote, however, if it reaches either of the two furthest ranks. (See Promotion.)

A lance (香) moves just like the rook except it cannot move backwards or to the sides. It is often useful to leave a lance unpromoted at the far side of the board. A lance "must" promote, however, if it reaches the furthest rank. (See Promotion.)

A pawn (歩) moves one square straight forward. It cannot retreat. Unlike international chess pawns, shogi pawns capture the same as they move. A pawn "must" promote if it arrives at the furthest rank. (See Promotion.) In practice, however, a pawn is usually promoted whenever possible. There are two restrictions on where a pawn may be dropped. (See Drops.)

All pieces but the knight move either horizontally, vertically, or diagonally. These directions cannot be combined in a single move; one direction must be chosen.

Every piece blocks the movement of all other non-jumping pieces through the square it occupies.

If a piece occupies a legal destination for an opposing piece, it may be "captured" by removing it from the board and replacing it with the opposing piece. The capturing piece may not continue beyond that square on that turn. Shogi pieces capture the same as they move.

Normally when moving a piece, a player snaps it to the board with the ends of the fingers of the same hand. This makes a sudden sound effect, bringing the piece to the attention of the opponent. This is also true for capturing and dropping pieces. On a traditional "shogi-ban", the pitch of the snap is deeper, delivering a subtler effect.
A player's "promotion zone" consists of the furthest one-third of the board – the three ranks occupied by the opponent's pieces at setup. The zone is typically delineated on shogi boards by two inscribed dots. When a piece is moved, if part of the piece's path lies within the promotion zone (that is, if the piece moves into, out of, or wholly within the zone; but "not" if it is dropped into the zone – see Drops), then the player has the option to "promote" the piece at the end of the turn. Promotion is indicated by turning the piece over after it moves, revealing the character of the promoted piece.

If a pawn or lance is moved to the furthest rank, or a knight is moved to either of the two furthest ranks, that piece "must" promote (otherwise, it would have no legal move on subsequent turns). A silver general is never required to promote, and it is often advantageous to keep a silver general unpromoted. (It is easier, for example, to extract an unpromoted silver from behind enemy lines; whereas a promoted silver, with only one line of retreat, can be easily blocked.)

Promoting a piece changes the way it moves. The various pieces promote as follows:

When captured, a piece loses its promoted status. Otherwise promotion is permanent.

A promoted rook ("dragon king", 龍王 "ryūō") moves as a rook and as a king. Alternate forms: 龍, 竜.

A promoted bishop ("dragon horse", 龍馬 "ryūma") moves as a bishop and as a king. Alternate form: 馬.

A promoted silver (成銀 "narigin") moves the same as a gold general. Alternate forms: 全, cursive 金.

A promoted knight (成桂 "narikei") moves the same as a gold general. Alternate forms: 圭, 今, cursive 金.

A promoted lance (成香 "narikyō") moves the same as a gold general. Alternate forms: 杏, 仝, cursive 金.

A promoted pawn (と金 "tokin") moves the same as a gold general. Alternate forms: と, 个.

Captured pieces are retained in hand and can be brought back into play under the capturing player's control. The Japanese term for "piece(s) in hand" is either 持ち駒 "mochigoma" or 手駒 "tegoma." On any turn, instead of moving a piece on the board, a player may select a piece in hand and place it – unpromoted side up and facing the opposing side – on any empty square. The piece is then one of that player's active pieces on the board and can be moved accordingly. This is called "dropping" the piece, or simply, a "drop". A drop counts as a complete move.

A drop cannot capture a piece, nor does dropping within the promotion zone result in immediate promotion. Capture and/or promotion may occur normally, however, on subsequent moves of the piece.

A pawn, knight, or lance may not be dropped on the furthest rank, since those pieces would have no legal moves on subsequent turns. For the same reason, a knight may not be dropped on the penultimate (player's 8th) rank.

There are two additional restrictions when dropping pawns:


It is common to keep captured pieces on a wooden stand (駒台 "komadai)" which is traditionally placed so that its bottom left corner aligns with the bottom right corner of the board from the perspective of each player. It is not permissible to hide pieces from full view.

It is common for players to swap bishops, which oppose each other across the board, early in the game. This leaves each player with a bishop in hand to be dropped later. The ability for drops in shogi give the game tactical richness and complexity. The fact that no piece ever goes entirely out of play accounts for the rarity of draws.

When a player's move threatens to capture the opposing king on the next turn, the move is said to "give check" to the king and the king is said to be "in check." If a player's king is in check, that player's responding move must remove the check if possible. Ways to remove a check include moving the king away from the threat, capturing the threatening piece, or placing another interposing piece between the king and the threatening piece.

To announce check in Japanese, one can say "ōte" (). However, this is an influence of international chess and is not required, even as a courtesy. Announcing a check vocally is unheard of in serious play.

The usual way for shogi games to end is for one side to checkmate the other side's king, after which the losing player will be given the opportunity to admit defeat. However, there are three other possible ways for a game to end: "repetition" ( "sennichite"), "impasse" ( "jishōgi"), and an "illegal move" (反則手). The first two – repetition and impasse – are particularly uncommon. Illegal moves are also uncommon in professional games although this may not be true with amateur players (especially beginners).

The losing player will resign at this point, although in practice play up to the checkmate point rarely occurs, as players normally resign as soon as a loss is deemed inevitable. In traditional tournament play, a formal resignation is required—that is, a checkmate is not a sufficient condition for winning. The resignation is indicated by bowing and/or saying 'I lost' (負けました "makemashita") and/or placing the right hand over the piece stands. Placing the hand over the piece stand is a vestige of the older practice of gently dropping one's pieces in hand over the board in order to indicate resignation. In western practice, a handshake may be used.

If the king is in check and there is no possible move which could protect the king, the move is said to "checkmate" ("tsumi" 詰み) the king. Checkmate effectively means that the opponent wins the game as the player would have no remaining legal moves. At this point, the checkmated player usually resigns. However, in practice, the defeated player will resign when the situation is hopeless. (See also: tsumeshogi, hisshi.)

As a practical matter, when an opponent's king has entered a player's own territory especially with supporting defending pieces, the opponent's king is often very difficult to mate given the forward attacking nature of most shogi pieces. This state is referred to as entering king (入玉 "nyū gyoku"). If both players' kings are in entering king states, the game becomes more likely to result in an impasse. (See §Impasse below.)

In the adjacent diagram example, although White's king is in a strong Anaguma castle, Black's king has entered White's territory making it very difficult to mate. Therefore, this position favors Black.
In professional and serious (tournament) amateur games, a player who makes an illegal move loses immediately. The loss stands even if play continued and the move was discovered later in game. However, if neither the opponent nor a third party points out the illegal move and the opponent later resigned, the resignation stands as the result. 

Illegal moves include:


In friendly amateur games, this rule is sometimes relaxed, and the player is able to take back the illegal move and replay a new legal move.

If the same game position occurs four times with the same player to move and the same pieces in hand for each player, then the game ends in a repetition draw or "sennichite" (lit. "moves for a thousand days"), as long as the positions are not due to perpetual check. (Perpetual check is an illegal move, which ends the game in a loss in tournament play.)

In professional shogi, a repetition draw outcome is not a final result as draws essentially do not count. There can be only one victorious through wins. This is a significant difference from western chess, in which a player can play specifically to obtain draws for gaining points. In the case of a repetition draw, professional shogi players will have to immediately play a subsequent game (or as many games as necessary) with sides reversed in order to obtain a true win outcome. (That is, the player who was White becomes Black, and vice versa.) Also, depending on the tournament, professional players play the subsequent game in the remainder of the allowed game time.

Thus, aiming for a repetition draw may be a possible professional strategy for the White player in order to play the second game as Black, which has a slight statistical advantage and/or greater initiative. For instance, Bishop Exchange Fourth File Rook is a passive strategy for White with the goal of a repetition draw (as it requires two tempo losses – swinging the rook and trading the bishops) while it is a very aggressive strategy if played by Black.

Repetition draws are rare in professional shogi occurring in about 1–2% of games and even rarer in amateur games. In professional shogi, repetition draws usually occur in the opening as certain positions are reached that are theoretically disadvantaged for both sides (reciprocal zugzwang). In amateur shogi, repetition draws tend to occur in the middle or endgame as a result of player errors.

The game reaches an Impasse or Deadlock ("jishōgi") if both kings have advanced into their respective promotion zones – a situation known as 相入玉 ("ai-nyū gyoku" "double entering kings") – and neither player can hope to mate the other or to gain any further material. An Impasse can result in either a win or a draw. If an Impasse happens, the winner is decided as follows: each player agrees to an Impasse, then each rook or bishop, promoted or not, scores 5 points for the owning player, and all other pieces except kings score 1 point each. A player scoring fewer than 24 points loses. (If neither player has fewer than 24, the game is no contest—a draw.) "Jishōgi" is considered an outcome in its own right rather than no contest, but there is no practical difference.

As an Impasse needs to be agreed on for the rule to be invoked, a player may refuse to do so and attempt to win the game in future moves. If that happens, there is no official rule about the verdict of the game. 

Amateur resolutions. However, for amateur games there are various guidances. Fairbairn reports a practice (considered a rule by the Shogi Association for The West) where the dispute is resolved by either player moving all friendly pieces into the promotion zone and then the game ends with points tallied. 

Another resolution is the 27-Point (27点法) rule used for some amateur tournaments. One version of this is simply the player who has 27 or more points is the winner of the Impasse. Another version is a 27-Point Declaration rule. For instance, the Declaration rule on the online shogi site, 81Dojo, is that the player who wants to declare an Impasse win must (i) declare an intention win via Impasse, (ii) have the king in the enemy camp (the promotion zone for that player), (iii) 10 other pieces must be in the promotion zone, (iv) not be in check, (v) have time remaining, and (vi) must have 28 points if Black or 27 points if White. If all of these conditions are met, then the Impasse declarer will win the game regardless of whether the opponent objects. 

Yet another resolution to a refusal to agree to Impasse is the so-called Try Rule (トライルール). In this case, after both kings have entered their corresponding promotion zones, then the player who first moves the king to the opponent's king's start square (51 for Black, 59 for White) first will be the winner.

In professional tournaments, the rules typically require drawn games to be replayed with sides reversed, possibly with reduced time limits. This is rare compared to chess and xiangqi, occurring at a rate of 1–2% even in amateur games. 

The 1982 "Meijin" title match between Makoto Nakahara and Hifumi Katoh was unusual in this regard with an impasse draw in the first (Double Yagura) game on April 13–14 (only the fifth draw in the then 40-year history of the tournament). This game (with Katoh as Black) lasted for 223 moves with 114 minutes spent pondering a single move. One of the reasons for the length of this game was that White (Nakahara) was very close to falling below the minimum of 24 points required for a draw. Thus, the end of the endgame was strategically about trying to keep White's points above the 24-point threshold. In this match, "sennichite" occurred in the sixth and eighth games. Thus, this best-of-seven match lasted eight games and took over three months to finish; Black did not lose a single game and the eventual victor was Katoh at 4–3.

Amateur players are ranked from 15 "kyū" to 1 kyū and then from 1 "dan" to 8 dan. Amateur 8 dan was previously only honorarily given to famous people. While it is now possible to win amateur 8 dan by actual strength (winning amateur Ryu-oh 3 times), this has yet to be achieved.

Professional players operate with their own scale, from 6 kyū to 3 dan for pro-aspiring players and professional 4 dan to 9 dan for formal professional players. Amateur and professional ranks are offset (with amateur 4 dan being equivalent to professional 6 kyū).

Shogi has a handicap system (like go) in which games between players of disparate strengths are adjusted so that the stronger player is put in a more disadvantageous position in order to compensate for the difference in playing levels. In a handicap game, one or more of White's pieces are removed from the setup, and instead White plays first.

There are two commons systems used to notate piece movements in shogi game records. One is used in Japanese language texts while a second was created for western players by George Hodges and Glyndon Townhill in the English language. Other systems are used to notate shogi board positions.

In western piece movement notation, the format is the piece initial followed by the type of movement and finally the file and rank where the piece moved to. The piece initials are K (King), R (Rook), B (Bishop), G (Gold), S (Silver), N (Knight), L (Lance), and P (Pawn). Simple movement is indicated with -, captures with x, and piece drops with *. The files are indicated with numerals 1–9 as are the ranks 1–9. Thus, Rx24 indicates 'rook captures on 24'. Promoted pieces are notated with + prefixed to the piece initial (e.g. +Rx24). Piece promotion is also indicated with + (e.g. S-21+) while unpromotion is indicated with = (e.g. S-21=). Piece ambiguity is resolved by notating which square a piece is moving from (e.g. N65-53+ means 'knight from 65 moves to 53 and promotes').

The Japanese notation system uses Japanese characters for pieces and promotion indication and uses Japanese numerals instead of letters for ranks. Movement type aside from drops is not indicated, and the conventions for resolving ambiguity are quite different from the western system. As examples, the western Rx24 would be 2四飛 in Japanese notation, +Rx24 would be 2四龍, S-21+ 2一銀成, S-21= 2一銀不成, and N65-53+ could be either 5三桂左成 or 5三桂右成 depending on whether the knight moved from the left or right.

Although not strictly part of the notational calculus for games, game results are indicated in Japanese newspapers, websites, etc. with wins indicated by a white circle and losses indicated by a black circle.

Shogi is similar to chess but has a much larger game tree complexity because of the use of drops, greater number of pieces, and larger board size. In comparison, shogi games average about 140 moves per game where as western chess games average about 80 moves per game and minishogi averages about 40 moves per game. 

Like chess, however, the game can be divided into the opening, middle game and endgame, each requiring a different strategy. The opening consists of arranging one's defenses usually in a castle and positioning for attack, the mid game consists of attempting to break through the opposing defenses while maintaining one's own, and the endgame starts when one side's defenses have been compromised.

In the adjacent diagram, Black has chosen a Ranging Rook position (specifically Fourth File Rook) where the rook has been moved leftward away from its starting position. Additionally, Black is utilizing a Silver Crown castle, which is a type of fortification structure constructed with one silver and two gold pieces and the king moved inside of the fortification – the "silver crown" name comes from the silver being positioned directly above the king's head on the 27 square as if it were a crown. In the diagram, White has chosen a Static Rook position, in which the rook remains on its starting square. This Static Rook position is specifically a type of Counter-Ranging Rook position known as Static Rook Anaguma that uses an Anaguma castle. The Anaguma fortification has the king moved all the way into very edge corner of the board on the 11 square as if it were a badger in a hole with a silver moved to the 22 square in order to close up the hole and additional reinforcing golds on 31 and 32 squares. This board position required 33 moves (or 12 move pairs as counted in western chess) to construct.

Shogi players are expected to follow etiquette in addition to rules explicitly described. Commonly accepted etiquette include following:

Shogi piece sets may contain two types of king pieces, (king) and (jewel). In this case, the higher classed player, in either social or genuine shogi player rank, may take the king piece. For example, in titleholder system games, the current titleholder takes the king piece as the higher.

The higher-ranked (or older) player also sits facing the door of the room and is the person who takes the pieces out of the piece box.

Shogi does not have a touch-move rule as in western chess tournament play or chu shogi. However, in professional games, a piece is considered to be moved when the piece has been let go of. In both amateur and professional play, any piece may be touched in order to adjust its centralization within its square (to look tidy).

Taking back moves (待った "matta") in professional games is prohibited. However, in friendly amateur games in Japan, it is often permitted.

Professional players are required to follow several ritualistic etiquette prescriptions such as kneeling exactly 15 centimeters from the shogi board, sitting in the formal seiza position, etc.

From "The Chess Variant Pages":
It is not clear when chess was brought to Japan. The earliest generally accepted mention of shogi is (1058–1064) by Fujiwara Akihira. The oldest archaeological evidence is a group of 16 shogi pieces excavated from the grounds of Kōfuku-ji in Nara Prefecture. As it was physically associated with a wooden tablet written on in the sixth year of Tenki (1058), the pieces are thought to date from that period. These simple pieces were cut from a writing plaque in the same five-sided shape as modern pieces, with the names of the pieces written on them.

The dictionary of common folk culture, (c. 1210–1221), a collection based on the two works and , describes two forms of shogi, large "(dai)" shogi and small "(shō)" shogi. These are now called Heian shogi (or Heian small shogi) and Heian dai shogi. Heian small shogi is the version on which modern shogi is based, but the "Nichūreki" states that one wins if one's opponent is reduced to a single king, indicating that drops had not yet been introduced. According to Kōji Shimizu, chief researcher at the Archaeological Institute of Kashihara, Nara Prefecture, the names of the Heian shogi pieces keep those of chaturanga (general, elephant, horse, chariot and soldier), and add to them the five treasures of Buddhism (jade, gold, silver, katsura tree, and incense).

Around the 13th century the game of dai shogi developed, created by increasing the number of pieces in Heian shogi, as was sho shogi, which added the rook, bishop, and drunken elephant from dai shogi to Heian shogi. The drunken elephant steps one square in any direction except directly backward, and promotes to the prince, which acts as a second king and must also be captured along with the original king for the other player to win. Around the 15th century, the rules of dai shogi were simplified, creating the game of chu shogi. Chu shogi, like its parent dai shogi, contains many distinct pieces, such as the queen (identical with Western chess) and the lion (which moves like a king, but twice per turn, potentially being able to capture twice, among other idiosyncrasies). The popularity of dai shogi soon waned in favour of chu shogi, until it stopped being played commonly. Chu shogi rivalled sho shogi in popularity until the introduction of drops in the latter, upon which standard shogi became ascendant, although chu shogi was still commonly played until about World War II, especially in Kyoto. Dai shogi was much less often played, but must have been remembered somewhat, as it is depicted in a woodcut by Kobayashi Kiyochika from around 1904 or 1905.

It is thought that the rules of standard shogi were fixed in the 16th century, when the drunken elephant was removed from the set of pieces present in sho shogi. There is no clear record of when drops were introduced, however.

In the Edo period, shogi variants were greatly expanded: tenjiku shogi, dai dai shogi, maka dai dai shogi, tai shogi, and taikyoku shogi were all invented. It is thought that these were played to only a very limited extent, however. Both standard shogi and Go were promoted by the Tokugawa shogunate. In 1612, the shogunate passed a law giving endowments to top shogi players (). During the reign of the eighth shogun, Tokugawa Yoshimune, castle shogi tournaments were held once a year on the 17th day of Kannazuki, corresponding to November 17, which is Shogi Day on the modern calendar.

The title of "meijin" became hereditary in the Ōhashi and Itō families until the fall of the shogunate, when it came to be passed by recommendation. Today the title is used for the winner of the Meijin-sen competition, the first modern title match. From around 1899, newspapers began to publish records of shogi matches, and high-ranking players formed alliances with the aim of having their games published. In 1909, the was formed, and in 1924, the was formed. This was an early incarnation of the modern , or JSA, and 1924 is considered by the JSA to be the date it was founded.

In 1935, "meijin" Kinjirō Sekine stepped down, and the rank of meijin came to be awarded to the winner of a . became the first Meijin under this system in 1937. This was the start of the (see titleholder system). After the war other tournaments were promoted to title matches, culminating with the in 1988 for the modern line-up of seven. About 200 professional shogi players compete. Each year, the title holder defends the title against a challenger chosen from knockout or round matches.

After the Second World War, SCAP (occupational government mainly led by US) tried to eliminate all "feudal" factors from Japanese society and shogi was included in the possible list of items to be banned along with Bushido (philosophy of samurai) and other things. The reason for banning shogi for SCAP was its exceptional character as a board game seen in the usage of captured pieces. SCAP insisted that this could lead to the idea of prisoner abuse. But Kozo Masuda, then one of the top professional shogi players, when summoned to the SCAP headquarters for an investigation, criticized such understanding of shogi and insisted that it is not shogi but western chess that potentially contains the idea of prisoner abuse because it just kills the pieces of the opponent while shogi is rather democratic for giving prisoners the chance to get back into the game. Masuda also said that chess contradicts the ideal of gender equality in western society because the king shields itself behind the queen and runs away. Masuda’s assertion is said to have eventually led to the exemption of shogi from the list of items to be banned.

The closest cousin of shogi in the chaturanga family is makruk of Thailand. Not only the similarity in distribution and movements of the pieces but also the names of shogi pieces suggest intimacy between shogi and makruk by its Buddhist symbolism (gold, silver, Cassia and Incense), which is not recognized in Chinese chess at all. In fact, Chinese chess and its East Asian variants are far remoter relatives than makruk. Though some early variants of chaturanga more similar to shogi and makruk are known to have been played in Tang dynasty China, they are thought to have been extinguished in Song dynasty China and in East Asia except in Japan probably owing to the popularity of Chinese chess.

There are two organizations for shogi professional players in Japan: the JSA, and the , or LPSA. The JSA is the primary organization for men and women's professional shogi while the LPSA is a group of women professionals who broke away from the JSA in 2007 to establish their own independent organization. Both organize tournaments for their members and have reached an agreement to cooperate with each other to promote shogi through events and other activities. Top professional players are fairly well-paid from tournament earnings. In 2016, the highest tournament earners were Yoshiharu Habu and Akira Watanabe who earned ¥91,500,000 and ¥73,900,000. (The tenth highest earner, Kouichi Fukaura, won ¥18,490,000.)

The JSA recognizes two categories of shogi professionals: , and . Sometimes "kishi" are addressed as , a term from Go used to distinguish "kishi" from other classes of players. JSA professional ranks and female professional ranks are not equivalent and each has their own promotion criteria and ranking system. In 2006, the JSA officially granted women "professional status". This is not equivalent, however, to the more traditional way of "gaining professional status", i.e., being promoted from the : leagues of strong amateur players aspiring to become a professional. Rather, it is a separate system especially designed for female professionals. Qualified amateurs, regardless of gender, may apply for the "Shoreikai System" and all those who successfully "graduate" are granted "kishi" status; however, no woman has yet to accomplish this feat (the highest women have reached is "Shoreikai 3 "dan" league" by Kana Satomi and Tomoka Nishiyama), so "kishi" is de facto only used to refer to male shogi professionals.

The JSA is the only body which can organize tournaments for professionals, e.g., the eight major tournaments in the titleholder system and other professional tournaments. In 1996, Yoshiharu Habu became the only "kishi" to hold seven major titles at the same time. For female professionals, both the JSA and LPSA organize tournaments, either jointly or separately. Tournaments for amateurs may be organized by the JSA and LPSA as well as local clubs, newspapers, private corporations, educational institutions or municipal governments for cities or prefectures under the guidance of the JSA or LPSA.

Since the 1990s, shogi has grown in popularity outside Japan, particularly in the People's Republic of China, and especially in Shanghai. The January 2006 edition of stated that there were 120,000 shogi players in Shanghai. The spread of the game to countries where Chinese characters are not in common use, however, has been slower.

In Europe there are currently (November 2017) over 1,200 active players.

Shogi has the highest game complexity of all popular chess variants. Computers have steadily improved in playing shogi since the 1970s. In 2007, champion Yoshiharu Habu estimated the strength of the 2006 world computer shogi champion Bonanza at the level of two-dan shoreikai.

The JSA prohibits its professionals from playing computers in public without prior permission, with the reason of promoting shogi and monetizing the computer–human events.

On October 12, 2010, after some 35 years of development, a computer finally beat a professional player, when the top ranked female champion Ichiyo Shimizu was beaten by the Akara2010 system in a game lasting just over 6 hours.

On July 24, 2011, computer shogi programs Bonanza and Akara crushed the amateur team of Kosaku and Shinoda in two games. The allotted time for the amateurs was one hour and then three minutes per move. The allotted time for the computer was 25 minutes and then 10 seconds per move.

On April 20, 2013, GPS Shogi defeated 8-dan professional shogi player Hiroyuki Miura in a 102-move game which lasted over 8 hours.

The highest rated player on Shogi Club 24 is computer program Ponanza, rated 3455 on December 13, 2015.

On April 10, 2016, Ponanza defeated Takayuki Yamasaki, 8-dan in 85 moves. Takayuki used 7 hours 9 minutes.

From a computational complexity point of view, generalized shogi is EXPTIME-complete.

Hundreds of video games were released exclusively in Japan for several consoles.

The backwards "uma" (shogi horse symbol) is often featured on merchandise (such as on large decorative shogi piece sculptures, keychains, and other keepsakes) available for sale in Tendō. It is also serves as a symbol of good luck. (Cf. Rabbit's foot.) There are multiple theories on its origin. One is that "uma" (うま ) spelled in the Japanese syllabary backwards is まう "mau" (舞う), which means "(to) dance" and dancing horses are a good luck omen.

In the manga series "Naruto", shogi plays an essential part in Shikamaru Nara's character development. He often plays it with his sensei, Asuma Sarutobi, apparently always beating him. When Asuma is fatally injured in battle, he reminds Shikamaru that the shogi king must always be protected, and draws a parallel between the king in shogi and his yet-unborn daughter, Mirai, whom he wanted Shikamaru to guide.

Shogi has been a central plot point in the manga and anime "Shion no Ō", the manga and anime "March Comes in Like a Lion", and the manga and television drama "81diver".

In the manga and anime "Durarara!!", the information broker Izaya Orihara plays a twisted version of chess, go and shogi, where he mixes all three games into one as a representation of the battles in Ikebukuro.

In the video game "Persona 5", the Star confidant is a high school shogi player looking to break into the ranks of the professionals. The player character will gain knowledge stat when spending time with the confidant, supposedly from learning to play shogi. The abilities learned from ranking up the confidant comes from Japanese shogi terms.

In the light novel, manga, and anime "The Ryuo's Work is Never Done!", protagonist Yaichi Kuzuryū is a prodigy shogi player who won the title of Ryūō at the age of 16. He is approached by Ai Hinatsuru, a 9-year-old girl who begs him to make her his disciple. Astonished by Ai's potential, Yaichi agrees to become her master, and the two then brave themselves together in the world of shogi with their friends and rivals.




Rules

Online play

Online tools


</doc>
<doc id="27743" url="https://en.wikipedia.org/wiki?curid=27743" title="Solar energy">
Solar energy

Solar energy is radiant light and heat from the Sun that is harnessed using a range of ever-evolving technologies such as solar heating, photovoltaics, solar thermal energy, solar architecture, molten salt power plants and artificial photosynthesis.

It is an important source of renewable energy and its technologies are broadly characterized as either passive solar or active solar depending on how they capture and distribute solar energy or convert it into solar power. Active solar techniques include the use of photovoltaic systems, concentrated solar power and solar water heating to harness the energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light-dispersing properties, and designing spaces that naturally circulate air.

The large magnitude of solar energy available makes it a highly appealing source of electricity. The United Nations Development Programme in its 2000 World Energy Assessment found that the annual potential of solar energy was 1,575–49,837 exajoules (EJ). This is several times larger than the total world energy consumption, which was 559.8 EJ in 2012.

In 2011, the International Energy Agency said that "the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries’ energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating global warming, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared".

The Earth receives 174 petawatts (PW) of incoming solar radiation (insolation) at the upper atmosphere. Approximately 30% is reflected back to space while the rest is absorbed by clouds, oceans and land masses. The spectrum of solar light at the Earth's surface is mostly spread across the visible and near-infrared ranges with a small part in the near-ultraviolet. Most of the world's population live in areas with insolation levels of 150–300 watts/m², or 3.5–7.0 kWh/m² per day.

Solar radiation is absorbed by the Earth's land surface, oceans – which cover about 71% of the globe – and atmosphere. Warm air containing evaporated water from the oceans rises, causing atmospheric circulation or convection. When the air reaches a high altitude, where the temperature is low, water vapor condenses into clouds, which rain onto the Earth's surface, completing the water cycle. The latent heat of water condensation amplifies convection, producing atmospheric phenomena such as wind, cyclones and anti-cyclones. Sunlight absorbed by the oceans and land masses keeps the surface at an average temperature of 14 °C. By photosynthesis, green plants convert solar energy into chemically stored energy, which produces food, wood and the biomass from which fossil fuels are derived.

The total solar energy absorbed by Earth's atmosphere, oceans and land masses is approximately 3,850,000 exajoules (EJ) per year. In 2002, this was more energy in one hour than the world used in one year. Photosynthesis captures approximately 3,000 EJ per year in biomass. The amount of solar energy reaching the surface of the planet is so vast that in one year it is about twice as much as will ever be obtained from all of the Earth's non-renewable resources of coal, oil, natural gas, and mined uranium combined,

The potential solar energy that could be used by humans differs from the amount of solar energy present near the surface of the planet because factors such as geography, time variation, cloud cover, and the land available to humans limit the amount of solar energy that we can acquire.

Geography affects solar energy potential because areas that are closer to the equator have a greater amount of solar radiation. However, the use of photovoltaics that can follow the position of the sun can significantly increase the solar energy potential in areas that are farther from the equator. Time variation effects the potential of solar energy because during the nighttime there is little solar radiation on the surface of the Earth for solar panels to absorb. This limits the amount of energy that solar panels can absorb in one day. Cloud cover can affect the potential of solar panels because clouds block incoming light from the sun and reduce the light available for solar cells.

In addition, land availability has a large effect on the available solar energy because solar panels can only be set up on land that is otherwise unused and suitable for solar panels. Roofs have been found to be a suitable place for solar cells, as many people have discovered that they can collect energy directly from their homes this way. Other areas that are suitable for solar cells are lands that are not being used for businesses where solar plants can be established.

Solar technologies are characterized as either passive or active depending on the way they capture, convert and distribute sunlight and enable solar energy to be harnessed at different levels around the world, mostly depending on distance from the equator. Although solar energy refers primarily to the use of solar radiation for practical ends, all renewable energies, other than Geothermal power and Tidal power, derive their energy either directly or indirectly from the Sun.

Active solar techniques use photovoltaics, concentrated solar power, solar thermal collectors, pumps, and fans to convert sunlight into useful outputs. Passive solar techniques include selecting materials with favorable thermal properties, designing spaces that naturally circulate air, and referencing the position of a building to the Sun. Active solar technologies increase the supply of energy and are considered supply side technologies, while passive solar technologies reduce the need for alternate resources and are generally considered demand side technologies.

In 2000, the United Nations Development Programme, UN Department of Economic and Social Affairs, and World Energy Council published an estimate of the potential solar energy that could be used by humans each year that took into account factors such as insolation, cloud cover, and the land that is usable by humans. The estimate found that solar energy has a global potential of 1,575–49,837 EJ per year "(see table below)".

Solar thermal technologies can be used for water heating, space heating, space cooling and process heat generation.

In 1878, at the Universal Exposition in Paris, Augustin Mouchot successfully demonstrated a solar steam engine, but couldn't continue development because of cheap coal and other factors. 

In 1897, Frank Shuman, a U.S. inventor, engineer and solar energy pioneer, built a small demonstration solar engine that worked by reflecting solar energy onto square boxes filled with ether, which has a lower boiling point than water, and were fitted internally with black pipes which in turn powered a steam engine. In 1908 Shuman formed the Sun Power Company with the intent of building larger solar power plants. He, along with his technical advisor A.S.E. Ackermann and British physicist Sir Charles Vernon Boys, developed an improved system using mirrors to reflect solar energy upon collector boxes, increasing heating capacity to the extent that water could now be used instead of ether. Shuman then constructed a full-scale steam engine powered by low-pressure water, enabling him to patent the entire solar engine system by 1912.

Shuman built the world's first solar thermal power station in Maadi, Egypt, between 1912 and 1913. His plant used parabolic troughs to power a engine that pumped more than of water per minute from the Nile River to adjacent cotton fields. Although the outbreak of World War I and the discovery of cheap oil in the 1930s discouraged the advancement of solar energy, Shuman's vision and basic design were resurrected in the 1970s with a new wave of interest in solar thermal energy. In 1916 Shuman was quoted in the media advocating solar energy's utilization, saying:
Solar hot water systems use sunlight to heat water. In low geographical latitudes (below 40 degrees) from 60 to 70% of the domestic hot water use with temperatures up to 60 °C can be provided by solar heating systems. The most common types of solar water heaters are evacuated tube collectors (44%) and glazed flat plate collectors (34%) generally used for domestic hot water; and unglazed plastic collectors (21%) used mainly to heat swimming pools.

As of 2007, the total installed capacity of solar hot water systems was approximately 154 thermal gigawatt (GW). China is the world leader in their deployment with 70 GW installed as of 2006 and a long-term goal of 210 GW by 2020. Israel and Cyprus are the per capita leaders in the use of solar hot water systems with over 90% of homes using them. In the United States, Canada, and Australia, heating swimming pools is the dominant application of solar hot water with an installed capacity of 18 GW as of 2005.

In the United States, heating, ventilation and air conditioning (HVAC) systems account for 30% (4.65 EJ/yr) of the energy used in commercial buildings and nearly 50% (10.1 EJ/yr) of the energy used in residential buildings. Solar heating, cooling and ventilation technologies can be used to offset a portion of this energy.
Thermal mass is any material that can be used to store heat—heat from the Sun in the case of solar energy. Common thermal mass materials include stone, cement and water. Historically they have been used in arid climates or warm temperate regions to keep buildings cool by absorbing solar energy during the day and radiating stored heat to the cooler atmosphere at night. However, they can be used in cold temperate areas to maintain warmth as well. The size and placement of thermal mass depend on several factors such as climate, daylighting and shading conditions. When properly incorporated, thermal mass maintains space temperatures in a comfortable range and reduces the need for auxiliary heating and cooling equipment.

A solar chimney (or thermal chimney, in this context) is a passive solar ventilation system composed of a vertical shaft connecting the interior and exterior of a building. As the chimney warms, the air inside is heated causing an updraft that pulls air through the building. Performance can be improved by using glazing and thermal mass materials in a way that mimics greenhouses.

Deciduous trees and plants have been promoted as a means of controlling solar heating and cooling. When planted on the southern side of a building in the northern hemisphere or the northern side in the southern hemisphere, their leaves provide shade during the summer, while the bare limbs allow light to pass during the winter. Since bare, leafless trees shade 1/3 to 1/2 of incident solar radiation, there is a balance between the benefits of summer shading and the corresponding loss of winter heating. In climates with significant heating loads, deciduous trees should not be planted on the Equator-facing side of a building because they will interfere with winter solar availability. They can, however, be used on the east and west sides to provide a degree of summer shading without appreciably affecting winter solar gain.

Solar cookers use sunlight for cooking, drying and pasteurization. They can be grouped into three broad categories: box cookers, panel cookers and reflector cookers. The simplest solar cooker is the box cooker first built by Horace de Saussure in 1767. A basic box cooker consists of an insulated container with a transparent lid. It can be used effectively with partially overcast skies and will typically reach temperatures of . Panel cookers use a reflective panel to direct sunlight onto an insulated container and reach temperatures comparable to box cookers. Reflector cookers use various concentrating geometries (dish, trough, Fresnel mirrors) to focus light on a cooking container. These cookers reach temperatures of and above but require direct light to function properly and must be repositioned to track the Sun.

Solar concentrating technologies such as parabolic dish, trough and Scheffler reflectors can provide process heat for commercial and industrial applications. The first commercial system was the Solar Total Energy Project (STEP) in Shenandoah, Georgia, USA where a field of 114 parabolic dishes provided 50% of the process heating, air conditioning and electrical requirements for a clothing factory. This grid-connected cogeneration system provided 400 kW of electricity plus thermal energy in the form of 401 kW steam and 468 kW chilled water, and had a one-hour peak load thermal storage. Evaporation ponds are shallow pools that concentrate dissolved solids through evaporation. The use of evaporation ponds to obtain salt from seawater is one of the oldest applications of solar energy. Modern uses include concentrating brine solutions used in leach mining and removing dissolved solids from waste streams. Clothes lines, clotheshorses, and clothes racks dry clothes through evaporation by wind and sunlight without consuming electricity or gas. In some states of the United States legislation protects the "right to dry" clothes. Unglazed transpired collectors (UTC) are perforated sun-facing walls used for preheating ventilation air. UTCs can raise the incoming air temperature up to and deliver outlet temperatures of . The short payback period of transpired collectors (3 to 12 years) makes them a more cost-effective alternative than glazed collection systems. As of 2003, over 80 systems with a combined collector area of had been installed worldwide, including an collector in Costa Rica used for drying coffee beans and a collector in Coimbatore, India, used for drying marigolds.

Solar distillation can be used to make saline or brackish water potable. The first recorded instance of this was by 16th-century Arab alchemists. A large-scale solar distillation project was first constructed in 1872 in the Chilean mining town of Las Salinas. The plant, which had solar collection area of , could produce up to per day and operate for 40 years. Individual still designs include single-slope, double-slope (or greenhouse type), vertical, conical, inverted absorber, multi-wick, and multiple effect. These stills can operate in passive, active, or hybrid modes. Double-slope stills are the most economical for decentralized domestic purposes, while active multiple effect units are more suitable for large-scale applications.

Solar water disinfection (SODIS) involves exposing water-filled plastic polyethylene terephthalate (PET) bottles to sunlight for several hours. Exposure times vary depending on weather and climate from a minimum of six hours to two days during fully overcast conditions. It is recommended by the World Health Organization as a viable method for household water treatment and safe storage. Over two million people in developing countries use this method for their daily drinking water.

Solar energy may be used in a water stabilization pond to treat waste water without chemicals or electricity. A further environmental advantage is that algae grow in such ponds and consume carbon dioxide in photosynthesis, although algae may produce toxic chemicals that make the water unusable.

Molten salt can be employed as a thermal energy storage method to retain thermal energy collected by a solar tower or solar trough of a concentrated solar power plant, so that it can be used to generate electricity in bad weather or at night. It was demonstrated in the Solar Two project from 1995–1999. The system is predicted to have an annual efficiency of 99%, a reference to the energy retained by storing heat before turning it into electricity, versus converting heat directly into electricity. The molten salt mixtures vary. The most extended mixture contains sodium nitrate, potassium nitrate and calcium nitrate. It is non-flammable and nontoxic, and has already been used in the chemical and metals industries as a heat-transport fluid, so experience with such systems exists in non-solar applications.

The salt melts at . It is kept liquid at in an insulated "cold" storage tank. The liquid salt is pumped through panels in a solar collector where the focused sun heats it to . It is then sent to a hot storage tank. This is so well insulated that the thermal energy can be usefully stored for up to a week.

When electricity is needed, the hot salt is pumped to a conventional steam-generator to produce superheated steam for a turbine/generator as used in any conventional coal, oil, or nuclear power plant. A 100-megawatt turbine would need a tank about tall and in diameter to drive it for four hours by this design.

Several parabolic trough power plants in Spain and solar power tower developer SolarReserve use this thermal energy storage concept. The Solana Generating Station in the U.S. has six hours of storage by molten salt. The María Elena plant is a 400 MW thermo-solar complex in the northern Chilean region of Antofagasta employing molten salt technology.

Solar power is the conversion of sunlight into electricity, either directly using photovoltaics (PV), or indirectly using concentrated solar power (CSP). CSP systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. PV converts light into electric current using the photoelectric effect.

Solar power is anticipated to become the world's largest source of electricity by 2050, with solar photovoltaics and concentrated solar power contributing 16 and 11 percent to the global overall consumption, respectively. In 2016, after another year of rapid growth, solar generated 1.3% of global power.

Commercial concentrated solar power plants were first developed in the 1980s. The 392 MW Ivanpah Solar Power Facility, in the Mojave Desert of California, is the largest solar power plant in the world. Other large concentrated solar power plants include the 150 MW Solnova Solar Power Station and the 100 MW Andasol solar power station, both in Spain. The 250 MW Agua Caliente Solar Project, in the United States, and the 221 MW Charanka Solar Park in India, are the world's largest photovoltaic plants. Solar projects exceeding 1 GW are being developed, but most of the deployed photovoltaics are in small rooftop arrays of less than 5 kW, which are connected to the grid using net metering and/or a feed-in tariff.

In the last two decades, photovoltaics (PV), also known as solar PV, has evolved from a pure niche market of small scale applications towards becoming a mainstream electricity source. A solar cell is a device that converts light directly into electricity using the photoelectric effect. The first solar cell was constructed by Charles Fritts in the 1880s. In 1931 a German engineer, Dr Bruno Lange, developed a photo cell using silver selenide in place of copper oxide. Although the prototype selenium cells converted less than 1% of incident light into electricity, both Ernst Werner von Siemens and James Clerk Maxwell recognized the importance of this discovery. Following the work of Russell Ohl in the 1940s, researchers Gerald Pearson, Calvin Fuller and Daryl Chapin created the crystalline silicon solar cell in 1954. These early solar cells cost 286 USD/watt and reached efficiencies of 4.5–6%. By 2012 available efficiencies exceeded 20%, and the maximum efficiency of research photovoltaics was in excess of 40%.

Concentrating Solar Power (CSP) systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. The concentrated heat is then used as a heat source for a conventional power plant. A wide range of concentrating technologies exists; the most developed are the parabolic trough, the concentrating linear fresnel reflector, the Stirling dish and the solar power tower. Various techniques are used to track the Sun and focus light. In all of these systems a working fluid is heated by the concentrated sunlight, and is then used for power generation or energy storage.

Sunlight has influenced building design since the beginning of architectural history. Advanced solar architecture and urban planning methods were first employed by the Greeks and Chinese, who oriented their buildings toward the south to provide light and warmth.

The common features of passive solar architecture are orientation relative to the Sun, compact proportion (a low surface area to volume ratio), selective shading (overhangs) and thermal mass. When these features are tailored to the local climate and environment they can produce well-lit spaces that stay in a comfortable temperature range. Socrates' Megaron House is a classic example of passive solar design. The most recent approaches to solar design use computer modeling tying together solar lighting, heating and ventilation systems in an integrated solar design package. Active solar equipment such as pumps, fans and switchable windows can complement passive design and improve system performance.

Urban heat islands (UHI) are metropolitan areas with higher temperatures than that of the surrounding environment. The higher temperatures result from increased absorption of solar energy by urban materials such as asphalt and concrete, which have lower albedos and higher heat capacities than those in the natural environment. A straightforward method of counteracting the UHI effect is to paint buildings and roads white, and to plant trees in the area. Using these methods, a hypothetical "cool communities" program in Los Angeles has projected that urban temperatures could be reduced by approximately 3 °C at an estimated cost of US$1 billion, giving estimated total annual benefits of US$530 million from reduced air-conditioning costs and healthcare savings.

Agriculture and horticulture seek to optimize the capture of solar energy in order to optimize the productivity of plants. Techniques such as timed planting cycles, tailored row orientation, staggered heights between rows and the mixing of plant varieties can improve crop yields. While sunlight is generally considered a plentiful resource, the exceptions highlight the importance of solar energy to agriculture. During the short growing seasons of the Little Ice Age, French and English farmers employed fruit walls to maximize the collection of solar energy. These walls acted as thermal masses and accelerated ripening by keeping plants warm. Early fruit walls were built perpendicular to the ground and facing south, but over time, sloping walls were developed to make better use of sunlight. In 1699, Nicolas Fatio de Duillier even suggested using a tracking mechanism which could pivot to follow the Sun. Applications of solar energy in agriculture aside from growing crops include pumping water, drying crops, brooding chicks and drying chicken manure. More recently the technology has been embraced by vintners, who use the energy generated by solar panels to power grape presses.

Greenhouses convert solar light to heat, enabling year-round production and the growth (in enclosed environments) of specialty crops and other plants not naturally suited to the local climate. Primitive greenhouses were first used during Roman times to produce cucumbers year-round for the Roman emperor Tiberius. The first modern greenhouses were built in Europe in the 16th century to keep exotic plants brought back from explorations abroad. Greenhouses remain an important part of horticulture today, and plastic transparent materials have also been used to similar effect in polytunnels and row covers.

Development of a solar-powered car has been an engineering goal since the 1980s. The World Solar Challenge is a biannual solar-powered car race, where teams from universities and enterprises compete over across central Australia from Darwin to Adelaide. In 1987, when it was founded, the winner's average speed was and by 2007 the winner's average speed had improved to .
The North American Solar Challenge and the planned South African Solar Challenge are comparable competitions that reflect an international interest in the engineering and development of solar powered vehicles.

Some vehicles use solar panels for auxiliary power, such as for air conditioning, to keep the interior cool, thus reducing fuel consumption.

In 1975, the first practical solar boat was constructed in England. By 1995, passenger boats incorporating PV panels began appearing and are now used extensively. In 1996, Kenichi Horie made the first solar-powered crossing of the Pacific Ocean, and the "Sun21" catamaran made the first solar-powered crossing of the Atlantic Ocean in the winter of 2006–2007. There were plans to circumnavigate the globe in 2010.

In 1974, the unmanned AstroFlight Sunrise airplane made the first solar flight. On 29 April 1979, the "Solar Riser" made the first flight in a solar-powered, fully controlled, man-carrying flying machine, reaching an altitude of . In 1980, the "Gossamer Penguin" made the first piloted flights powered solely by photovoltaics. This was quickly followed by the "Solar Challenger" which crossed the English Channel in July 1981. In 1990 Eric Scott Raymond in 21 hops flew from California to North Carolina using solar power. Developments then turned back to unmanned aerial vehicles (UAV) with the "Pathfinder" (1997) and subsequent designs, culminating in the "Helios" which set the altitude record for a non-rocket-propelled aircraft at in 2001. The "Zephyr", developed by BAE Systems, is the latest in a line of record-breaking solar aircraft, making a 54-hour flight in 2007, and month-long flights were envisioned by 2010. As of 2016, Solar Impulse, an electric aircraft, is currently circumnavigating the globe. It is a single-seat plane powered by solar cells and capable of taking off under its own power. The design allows the aircraft to remain airborne for several days.

A solar balloon is a black balloon that is filled with ordinary air. As sunlight shines on the balloon, the air inside is heated and expands causing an upward buoyancy force, much like an artificially heated hot air balloon. Some solar balloons are large enough for human flight, but usage is generally limited to the toy market as the surface-area to payload-weight ratio is relatively high.

Solar chemical processes use solar energy to drive chemical reactions. These processes offset energy that would otherwise come from a fossil fuel source and can also convert solar energy into storable and transportable fuels. Solar induced chemical reactions can be divided into thermochemical or photochemical. A variety of fuels can be produced by artificial photosynthesis. The multielectron catalytic chemistry involved in making carbon-based fuels (such as methanol) from reduction of carbon dioxide is challenging; a feasible alternative is hydrogen production from protons, though use of water as the source of electrons (as plants do) requires mastering the multielectron oxidation of two water molecules to molecular oxygen. Some have envisaged working solar fuel plants in coastal metropolitan areas by 2050 the splitting of sea water providing hydrogen to be run through adjacent fuel-cell electric power plants and the pure water by-product going directly into the municipal water system. Another vision involves all human structures covering the earth's surface (i.e., roads, vehicles and buildings) doing photosynthesis more efficiently than plants.

Hydrogen production technologies have been a significant area of solar chemical research since the 1970s. Aside from electrolysis driven by photovoltaic or photochemical cells, several thermochemical processes have also been explored. One such route uses concentrators to split water into oxygen and hydrogen at high temperatures (). Another approach uses the heat from solar concentrators to drive the steam reformation of natural gas thereby increasing the overall hydrogen yield compared to conventional reforming methods. Thermochemical cycles characterized by the decomposition and regeneration of reactants present another avenue for hydrogen production. The Solzinc process under development at the Weizmann Institute of Science uses a 1 MW solar furnace to decompose zinc oxide (ZnO) at temperatures above . This initial reaction produces pure zinc, which can subsequently be reacted with water to produce hydrogen.

Thermal mass systems can store solar energy in the form of heat at domestically useful temperatures for daily or interseasonal durations. Thermal storage systems generally use readily available materials with high specific heat capacities such as water, earth and stone. Well-designed systems can lower peak demand, shift time-of-use to off-peak hours and reduce overall heating and cooling requirements.

Phase change materials such as paraffin wax and Glauber's salt are another thermal storage medium. These materials are inexpensive, readily available, and can deliver domestically useful temperatures (approximately ). The "Dover House" (in Dover, Massachusetts) was the first to use a Glauber's salt heating system, in 1948. Solar energy can also be stored at high temperatures using molten salts. Salts are an effective storage medium because they are low-cost, have a high specific heat capacity and can deliver heat at temperatures compatible with conventional power systems. The Solar Two project used this method of energy storage, allowing it to store in its 68 m³ storage tank with an annual storage efficiency of about 99%.

Off-grid PV systems have traditionally used rechargeable batteries to store excess electricity. With grid-tied systems, excess electricity can be sent to the transmission grid, while standard grid electricity can be used to meet shortfalls. Net metering programs give household systems a credit for any electricity they deliver to the grid. This is handled by 'rolling back' the meter whenever the home produces more electricity than it consumes. If the net electricity use is below zero, the utility then rolls over the kilowatt hour credit to the next month. Other approaches involve the use of two meters, to measure electricity consumed vs. electricity produced. This is less common due to the increased installation cost of the second meter. Most standard meters accurately measure in both directions, making a second meter unnecessary.

Pumped-storage hydroelectricity stores energy in the form of water pumped when energy is available from a lower elevation reservoir to a higher elevation one. The energy is recovered when demand is high by releasing the water, with the pump becoming a hydroelectric power generator.

Beginning with the surge in coal use which accompanied the Industrial Revolution, energy consumption has steadily transitioned from wood and biomass to fossil fuels. The early development of solar technologies starting in the 1860s was driven by an expectation that coal would soon become scarce. However, development of solar technologies stagnated in the early 20th century in the face of the increasing availability, economy, and utility of coal and petroleum.

The 1973 oil embargo and 1979 energy crisis caused a reorganization of energy policies around the world and brought renewed attention to developing solar technologies. Deployment strategies focused on incentive programs such as the Federal Photovoltaic Utilization Program in the U.S. and the Sunshine Program in Japan. Other efforts included the formation of research facilities in the U.S. (SERI, now NREL), Japan (NEDO), and Germany (Fraunhofer Institute for Solar Energy Systems ISE).

Commercial solar water heaters began appearing in the United States in the 1890s. These systems saw increasing use until the 1920s but were gradually replaced by cheaper and more reliable heating fuels. As with photovoltaics, solar water heating attracted renewed attention as a result of the oil crises in the 1970s but interest subsided in the 1980s due to falling petroleum prices. Development in the solar water heating sector progressed steadily throughout the 1990s and annual growth rates have averaged 20% since 1999. Although generally underestimated, solar water heating and cooling is by far the most widely deployed solar technology with an estimated capacity of 154 GW as of 2007.

The International Energy Agency has said that solar energy can make considerable contributions to solving some of the most urgent problems the world now faces:

The development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries’ energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating climate change, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared.
In 2011, a report by the International Energy Agency found that solar energy technologies such as photovoltaics, solar hot water and concentrated solar power could provide a third of the world's energy by 2060 if politicians commit to limiting climate change. The energy from the sun could play a key role in de-carbonizing the global economy alongside improvements in energy efficiency and imposing costs on greenhouse gas emitters. "The strength of solar is the incredible variety and flexibility of applications, from small scale to big scale".

The International Organization for Standardization has established several standards relating to solar energy equipment. For example, ISO 9050 relates to glass in building while ISO 10217 relates to the materials used in solar water heaters.




</doc>
<doc id="27745" url="https://en.wikipedia.org/wiki?curid=27745" title="Standard conditions for temperature and pressure">
Standard conditions for temperature and pressure

Standard conditions for temperature and pressure are standard sets of conditions for experimental measurements to be established to allow comparisons to be made between different sets of data. The most used standards are those of the International Union of Pure and Applied Chemistry (IUPAC) and the National Institute of Standards and Technology (NIST), although these are not universally accepted standards. Other organizations have established a variety of alternative definitions for their standard reference conditions.

In chemistry, IUPAC has changed the definition of standard temperature and pressure (STP) in 1982: 

STP should not be confused with the standard state commonly used in thermodynamic evaluations of the Gibbs energy of a reaction.

NIST uses a temperature of 20 °C (293.15 K, 68
 °F) and an absolute pressure of 1 atm (14.696 psi, 101.325 kPa). This standard is also called normal temperature and pressure (abbreviated as NTP).

The International Standard Metric Conditions for natural gas and similar fluids are and 101.325 kPa.

In industry and commerce, standard conditions for temperature and pressure are often necessary to define the standard reference conditions to express the volumes of gases and liquids and related quantities such as the rate of volumetric flow (the volumes of gases vary significantly with temperature and pressure) – standard cubic meters per second (sm3/s), and normal cubic meters per second (nm3/s). However, many technical publications (books, journals, advertisements for equipment and machinery) simply state "standard conditions" without specifying them, often leading to confusion and errors. Good practice always incorporates the reference conditions of temperature and pressure.

Before 1918, many professionals and scientists using the metric system of units defined the standard reference conditions of temperature and pressure for expressing gas volumes as being and . During those same years, the most commonly used standard reference conditions for people using the imperial or U.S. customary systems was and 14.696 psi (1 atm) because it was almost universally used by the oil and gas industries worldwide. The above definitions are no longer the most commonly used in either system of units.

Many different definitions of standard reference conditions are currently being used by organizations all over the world. The table below lists a few of them, but there are more. Some of these organizations used other standards in the past. For example, IUPAC has, since 1982, defined standard reference conditions as being 0 °C and 100 kPa (1 bar), in contrast to its old standard of 0 °C and 101.325 kPa (1 atm).

Natural gas companies in Europe, Australia, and South America have adopted 15 °C (59 °F) and 101.325 kPa (14.696 psi) as their standard gas volume reference conditions, used as the base values for defining the standard cubic meter. Also, the International Organization for Standardization (ISO), the United States Environmental Protection Agency (EPA) and National Institute of Standards and Technology (NIST) each have more than one definition of standard reference conditions in their various standards and regulations.

Notes:

In aeronautics and fluid dynamics the "International Standard Atmosphere" (ISA) is a specification of pressure, temperature, density, and speed of sound at each altitude. The International Standard Atmosphere is representative of atmospheric conditions at mid latitudes. In the USA this information is specified the U.S. Standard Atmosphere which is identical to the "International Standard Atmosphere" at all altitudes up to 65,000 feet above sea level.

Due to the fact that many definitions of standard temperature and pressure differ in temperature significantly from standard laboratory temperatures (e.g., 0 °C vs. ~25 °C), reference is often made to "standard laboratory conditions" (a term deliberately chosen to be different from the term "standard conditions for temperature and pressure", despite its semantic near identity when interpreted literally). However, what is a "standard" laboratory temperature and pressure is inevitably geography-bound, given that different parts of the world differ in climate, altitude and the degree of use of heat/cooling in the workplace. For example, schools in New South Wales, Australia use 25 °C at 100 kPa for standard laboratory conditions.
ASTM International has published Standard ASTM E41- Terminology Relating to Conditioning and hundreds of special conditions for particular materials and test methods. Other standards organizations also have specialized standard test conditions.

It is equally as important to indicate the applicable reference conditions of temperature and pressure when stating the molar volume of a gas as it is when expressing a gas volume or volumetric flow rate. Stating the molar volume of a gas without indicating the reference conditions of temperature and pressure has very little meaning and can cause confusion.

The molar volume of gases around STP and at atmospheric pressure can be calculated with an accuracy that is usually sufficient by using the ideal gas law. The molar volume of any ideal gas may be calculated at various standard reference conditions as shown below:

Technical literature can be confusing because many authors fail to explain whether they are using the ideal gas constant "R", or the specific gas constant "R". The relationship between the two constants is "R" = "R" / "m", where "m" is the molecular mass of the gas.

The US Standard Atmosphere (USSA) uses 8.31432 m·Pa/(mol·K) as the value of "R". However, the USSA,1976 does recognize that this value is not consistent with the values of the Avogadro constant and the Boltzmann constant.




</doc>
<doc id="27750" url="https://en.wikipedia.org/wiki?curid=27750" title="Script kiddie">
Script kiddie

In programming and hacking culture, a script kiddie or skiddie is an unskilled individual who uses scripts or programs developed by others to attack computer systems and networks and deface websites. It is generally assumed that most script kiddies are juveniles who lack the ability to write sophisticated programs or exploits on their own and that their objective is to try to impress their friends or gain credit in computer-enthusiast communities. However, the term does not relate to the actual age of the participant. The term is considered to somewhat offensive.

In a Carnegie Mellon report prepared for the U.S. Department of Defense in 2005, script kiddies are defined as The more immature but unfortunately often just as dangerous exploiter of security lapses on the Internet. The typical script kiddy uses existing and frequently well known and easy-to-find techniques and programs or scripts to search for and exploit weaknesses in other computers on the Internet—often randomly and with little regard or perhaps even understanding of the potentially harmful consequences.

Script kiddies have at their disposal a large number of effective, easily downloadable programs capable of breaching computers and networks. Such programs have included remote denial-of-service WinNuke, trojans, Back Orifice, NetBus and Sub7 vulnerability scanner/injector kit Metasploit and often software intended for legitimate security auditing.

Script kiddies vandalize websites both for the thrill of it and to increase their reputation among their peers. Some more malicious script kiddies have used virus toolkits to create and propagate the Anna Kournikova and Love Bug viruses.
Script kiddies lack, or are only developing, programming skills sufficient to understand the effects and side effects of their actions. As a result, they leave significant traces which lead to their detection, or directly attack companies which have detection and countermeasures already in place, or in recent cases, leave automatic crash reporting turned on.




</doc>
<doc id="27751" url="https://en.wikipedia.org/wiki?curid=27751" title="Scalable Vector Graphics">
Scalable Vector Graphics

Scalable Vector Graphics (SVG) is an XML-based vector image format for two-dimensional graphics with support for interactivity and animation. The SVG specification is an open standard developed by the World Wide Web Consortium (W3C) since 1999.

SVG images and their behaviors are defined in XML text files. This means that they can be searched, indexed, scripted, and compressed. As XML files, SVG images can be created and edited with any text editor, as well as with drawing software.

All major modern web browsers—including Mozilla Firefox, Internet Explorer, Google Chrome, Opera, Safari, and Microsoft Edge—have SVG rendering support.

SVG has been in development within the World Wide Web Consortium (W3C) since 1999, after six competing proposals for vector graphics languages had been submitted to the consortium during 1998. The early SVG Working Group decided not to develop any of the commercial submissions, but to create a new markup language that was informed by but not really based on any of them.

SVG allows three types of graphic objects: vector graphic shapes such as paths and outlines consisting of straight lines and curves, bitmap images, and text. Graphical objects can be grouped, styled, transformed and composited into previously rendered objects. The feature set includes nested transformations, clipping paths, alpha masks, filter effects and template objects. SVG drawings can be interactive and can include animation, defined in the SVG XML elements or via scripting that accesses the SVG Document Object Model (DOM). SVG uses CSS for styling and JavaScript for scripting. Text, including internationalization and localization, appearing in plain text within the SVG DOM enhances the accessibility of SVG graphics.

The SVG specification was updated to version 1.1 in 2011. There are two 'Mobile SVG Profiles,' SVG Tiny and SVG Basic, meant for mobile devices with reduced computational and display capabilities. Scalable Vector Graphics 2 became a W3C Candidate Recommendation on 15 September 2016. SVG 2 incorporates several new features in addition to those of SVG 1.1 and SVG Tiny 1.2.

Though the SVG Specification primarily focuses on vector graphics markup language, its design includes the basic capabilities of a page description language like Adobe's PDF. It contains provisions for rich graphics, and is compatible with CSS for styling purposes. SVG has the information needed to place each glyph and image in a chosen location on a printed page.

SVG drawings can be dynamic and interactive. Time-based modifications to the elements can be described in SMIL, or can be programmed in a scripting language (e.g. ECMAScript or JavaScript). The W3C explicitly recommends SMIL as the standard for animation in SVG.

A rich set of event handlers such as "onmouseover" and "onclick" can be assigned to any SVG graphical object.

SVG images, being XML, contain many repeated fragments of text, so they are well suited for lossless data compression algorithms. When an SVG image has been compressed with the industry standard gzip algorithm, it is referred to as an "SVGZ" image and uses the corresponding codice_1 filename extension. Conforming SVG 1.1 viewers will display compressed images. An SVGZ file is typically 20 to 50 percent of the original size. W3C provides SVGZ files to test for conformance.

SVG was developed by the W3C SVG Working Group starting in 1998, after six competing vector graphics submissions were received that year:
The working group was chaired at the time by Chris Lilley of the W3C.



Because of industry demand, two mobile profiles were introduced with SVG 1.1: "SVG Tiny" (SVGT) and "SVG Basic" (SVGB). 

These are subsets of the full SVG standard, mainly intended for user agents with limited capabilities. In particular, SVG Tiny was defined for highly restricted mobile devices such as cellphones; it does not support styling or scripting. SVG Basic was defined for higher-level mobile devices, such as smartphones.

In 2003, the 3GPP, an international telecommunications standards group, adopted SVG Tiny as the mandatory vector graphics media format for next-generation phones. SVGT is the required vector graphics format and support of SVGB is optional for Multimedia Messaging Service (MMS) and Packet-switched Streaming Service. It was later added as required format for vector graphics in 3GPP IP Multimedia Subsystem (IMS).

Neither mobile profile includes support for the full Document Object Model (DOM), while only SVG Basic has optional support for scripting, but because they are fully compatible subsets of the full standard, most SVG graphics can still be rendered by devices which only support the mobile profiles.

SVGT 1.2 adds a microDOM (μDOM), styling and scripting.

The MPEG-4 Part 20 standard - "Lightweight Application Scene Representation (LASeR) and Simple Aggregation Format (SAF)" is based on SVG Tiny. It was developed by MPEG (ISO/IEC JTC1/SC29/WG11) and published as ISO/IEC 14496-20:2006. SVG capabilities are enhanced in MPEG-4 Part 20 with key features for mobile services, such as dynamic updates, binary encoding, state-of-art font representation. SVG was also accommodated in MPEG-4 Part 11, in the Extensible MPEG-4 Textual (XMT) format - a textual representation of the MPEG-4 multimedia content using XML.

The SVG 1.1 specification defines 14 functional areas or feature sets:


An SVG document can define components including shapes, gradients etc., and use them repeatedly. SVG images can also contain raster graphics, such as PNG and JPEG images, and further SVG images.

This code will produce the shapes shown in the image (excluding the grid):
The use of SVG on the web was limited by the lack of support in older versions of Internet Explorer (IE). Many web sites that serve SVG images, such as Wikipedia, also provide the images in a raster format, either automatically by HTTP content negotiation or by allowing the user directly to choose the file.

Google announced on 31 August 2010 that it had started to index SVG content on the web, whether it is in standalone files or embedded in HTML, and that users would begin to see such content listed among their search results.
It was announced on 8 December 2010 that Google Image Search would also begin indexing SVG files. The site announced an option to restrict image searches to SVG files on 11 February 2011.

Konqueror was the first browser to support SVG in release version 3.2 in February 2004. As of 2011, all major desktop browsers, and many minor ones, have some level of SVG support. Other browsers' implementations are not yet complete; see comparison of layout engines for further details.

Some earlier versions of Firefox (e.g. versions between 1.5 and 3.6), as well as a smattering of other now-outdated web browsers capable of displaying SVG graphics, needed them embedded in codice_30 or codice_31 elements to display them integrated as parts of an HTML webpage instead of using the standard way of integrating images with codice_32. However, SVG images may be included in XHTML pages using XML namespaces.

Tim Berners-Lee, the inventor of the World Wide Web, has been critical of (earlier versions of) Internet Explorer for its failure to support SVG.

There are several advantages to native and full support: plugins are not needed, SVG can be freely mixed with other content in a single document, and rendering and scripting become considerably more reliable.

Internet Explorer, up to and including IE8, was the only major browser not to provide native SVG support. IE8 and older require a plug-in to render SVG content. There are a number of plug-ins available to assist, including:

On 5 January 2010, a senior manager of the Internet Explorer team at Microsoft announced on his official blog that Microsoft had just requested to join the SVG Working Group of the W3C in order to "take part in ensuring future versions of the SVG spec will meet the needs of developers and end users," although no plans for SVG support in Internet Explorer were mentioned at that time. Internet Explorer 9 beta supported a basic SVG feature set based on the SVG 1.1 W3C recommendation. Functionality has been implemented for most of the SVG document structure, interactivity through scripting and styling inline and through CSS. The presentation elements, attributes and DOM interfaces that have been implemented include basic shapes, colors, filling, gradients, patterns, paths and text.

SVG Tiny (SVGT) 1.1 and 1.2 are mobile profiles for SVG. SVGT 1.2 includes some features not found in SVG 1.1, including non-scaling strokes, which are supported by some SVG 1.1 implementations, such as Opera, Firefox and WebKit. As shared code bases between desktop and mobile browsers increased, the use of SVG 1.1 over SVGT 1.2 also increased.

Support for SVG may be limited to SVGT on older or more limited smart phones, or may be primarily limited by their respective operating system. Adobe Flash Lite has optionally supported SVG Tiny since version 1.1. At the SVG Open 2005 conference, Sun demonstrated a mobile implementation of SVG Tiny 1.1 for the Connected Limited Device Configuration (CLDC) platform.

Mobiles that use Opera Mobile, as well as the iPhone's built in browser, also include SVG support. However, even though it used the WebKit engine, the Android built-in browser did not support SVG prior to v3.0 (Honeycomb). Prior to v3.0, Firefox Mobile 4.0b2 (beta) for Android was the first browser running under Android to support SVG by default.

The level of SVG Tiny support available varies from mobile to mobile, depending on the SVG engine installed. Many newer mobile products support additional features beyond SVG Tiny 1.1, like gradient and opacity; this is sometimes referred as "SVGT 1.1+", though there is no such standard.

RIM's BlackBerry has built-in support for SVG Tiny 1.1 since version 5.0. Support continues for WebKit-based BlackBerry Torch browser in OS 6 and 7.

Nokia's S60 platform has built-in support for SVG. For example, icons are generally rendered using the platform's SVG engine. Nokia has also led the JSR 226: Scalable 2D Vector Graphics API expert group that defines Java ME API for SVG presentation and manipulation. This API has been implemented in S60 Platform 3rd Edition Feature Pack 1 and onward. Some Series 40 phones also support SVG (such as Nokia 6280).

Most Sony Ericsson phones beginning with K700 (by release date) support SVG Tiny 1.1. Phones beginning with K750 also support such features as opacity and gradients. Phones with Sony Ericsson Java Platform-8 have support for JSR 226.

Windows Phone has supported SVG since version 7.5

SVG is also supported on various mobile devices from Motorola, Samsung, LG, and Siemens mobile/BenQ-Siemens. eSVG, an SVG rendering library mainly written for embedded devices, is available on some mobile platforms.

This is an incomplete list of web applications that can convert SVG files to raster image formats (this process is known as rasterization), or raster images to SVG (this process is known as image tracing or vectorization) - without the need of installing a desktop software or browser plug-in.

SVG images can be produced by the use of a vector graphics editor, such as Inkscape, Adobe Illustrator, Adobe Flash Professional, or CorelDRAW, and rendered to common raster image formats such as PNG using the same software. Inkscape uses a (built-in) potrace to import raster image formats.

Software can be programmed to render SVG images by using a library such as librsvg used by GNOME since 2000, or Batik. SVG images can also be rendered to any desired popular image format by using ImageMagick, a free command-line utility (which also uses librsvg under the hood).

Other uses for SVG include embedding for use in word processing (e.g. with LibreOffice) and desktop publishing (e.g. Scribus), plotting graphs (e.g. gnuplot), and importing paths (e.g. for use in GIMP or Blender). Microsoft Office 2016 added support for importing and editing SVG images in January 2017. The Uniform Type Identifier for SVG used by Apple is public.svg-image and conforms to public.image and public.xml.

The DOCTYPE for SVG 1.0 is:

and that for SVG 1.1 is but for various reasons, a DOCTYPE should not be included in SVG files.



 


</doc>
<doc id="27752" url="https://en.wikipedia.org/wiki?curid=27752" title="Spectroscopy">
Spectroscopy

Spectroscopy is the study of the interaction between matter and electromagnetic radiation. Historically, spectroscopy originated through the study of visible light dispersed according to its wavelength, by a prism. Later the concept was expanded greatly to include any interaction with radiative energy as a function of its wavelength or frequency. Spectroscopic data are often represented by an emission spectrum, a plot of the response of interest as a function of wavelength or frequency.

Spectroscopy and spectrography are terms used to refer to the measurement of radiation intensity as a function of wavelength and are often used to describe experimental spectroscopic methods. Spectral measurement devices are referred to as spectrometers, spectrophotometers, spectrographs or spectral analyzers.

Daily observations of color can be related to spectroscopy. Neon lighting is a direct application of atomic spectroscopy. Neon and other noble gases have characteristic emission frequencies (colors). Neon lamps use collision of electrons with the gas to excite these emissions. Inks, dyes and paints include chemical compounds selected for their spectral characteristics in order to generate specific colors and hues. A commonly encountered molecular spectrum is that of nitrogen dioxide. Gaseous nitrogen dioxide has a characteristic red absorption feature, and this gives air polluted with nitrogen dioxide a reddish-brown color. Rayleigh scattering is a spectroscopic scattering phenomenon that accounts for the color of the sky.

Spectroscopic studies were central to the development of quantum mechanics and included Max Planck's explanation of blackbody radiation, Albert Einstein's explanation of the photoelectric effect and Niels Bohr's explanation of atomic structure and spectra. Spectroscopy is used in physical and analytical chemistry because atoms and molecules have unique spectra. As a result, these spectra can be used to detect, identify and quantify information about the atoms and molecules. Spectroscopy is also used in astronomy and remote sensing on Earth. Most research telescopes have spectrographs. The measured spectra are used to determine the chemical composition and physical properties of astronomical objects (such as their temperature and velocity).

One of the central concepts in spectroscopy is a resonance and its corresponding resonant frequency. Resonances were first characterized in mechanical systems such as pendulums. Mechanical systems that vibrate or oscillate will experience large amplitude oscillations when they are driven at their resonant frequency. A plot of amplitude vs. excitation frequency will have a peak centered at the resonance frequency. This plot is one type of spectrum, with the peak often referred to as a spectral line, and most spectral lines have a similar appearance.

In quantum mechanical systems, the analogous resonance is a coupling of two quantum mechanical stationary states of one system, such as an atom, via an oscillatory source of energy such as a photon. The coupling of the two states is strongest when the energy of the source matches the energy difference between the two states. The energy formula_1 of a photon is related to its frequency formula_2 by formula_3 where formula_4 is Planck's constant, and so a spectrum of the system response vs. photon frequency will peak at the resonant frequency or energy. Particles such as electrons and neutrons have a comparable relationship, the de Broglie relations, between their kinetic energy and their wavelength and frequency and therefore can also excite resonant interactions.

Spectra of atoms and molecules often consist of a series of spectral lines, each one representing a resonance between two different quantum states. The explanation of these series, and the spectral patterns associated with them, were one of the experimental enigmas that drove the development and acceptance of quantum mechanics. The hydrogen spectral series in particular was first successfully explained by the Rutherford-Bohr quantum model of the hydrogen atom. In some cases spectral lines are well separated and distinguishable, but spectral lines can also overlap and appear to be a single transition if the density of energy states is high enough. Named series of lines include the principal, sharp, diffuse and fundamental series.

Spectroscopy is a sufficiently broad field that many sub-disciplines exist, each with numerous implementations of specific spectroscopic techniques. The various implementations and techniques can be classified in several ways.

The types of spectroscopy are distinguished by the type of radiative energy involved in the interaction. In many applications, the spectrum is determined by measuring changes in the intensity or frequency of this energy. The types of radiative energy studied include:

The types of spectroscopy also can be distinguished by the nature of the interaction between the energy and the material. These interactions include:

Spectroscopic studies are designed so that the radiant energy interacts with specific types of matter.

Atomic spectroscopy was the first application of spectroscopy developed. Atomic absorption spectroscopy and atomic emission spectroscopy involve visible and ultraviolet light. These absorptions and emissions, often referred to as atomic spectral lines, are due to electronic transitions of outer shell electrons as they rise and fall from one electron orbit to another. Atoms also have distinct x-ray spectra that are attributable to the excitation of inner shell electrons to excited states.

Atoms of different elements have distinct spectra and therefore atomic spectroscopy allows for the identification and quantitation of a sample's elemental composition. Robert Bunsen and Gustav Kirchhoff discovered new elements by observing their emission spectra. Atomic absorption lines are observed in the solar spectrum and referred to as Fraunhofer lines after their discoverer. A comprehensive explanation of the hydrogen spectrum was an early success of quantum mechanics and explained the Lamb shift observed in the hydrogen spectrum, which further led to the development of quantum electrodynamics.

Modern implementations of atomic spectroscopy for studying visible and ultraviolet transitions include flame emission spectroscopy, inductively coupled plasma atomic emission spectroscopy, glow discharge spectroscopy, microwave induced plasma spectroscopy, and spark or arc emission spectroscopy. Techniques for studying x-ray spectra include X-ray spectroscopy and X-ray fluorescence.

The combination of atoms into molecules leads to the creation of unique types of energetic states and therefore unique spectra of the transitions between these states. Molecular spectra can be obtained due to electron spin states (electron paramagnetic resonance), molecular rotations, molecular vibration, and electronic states. Rotations are collective motions of the atomic nuclei and typically lead to spectra in the microwave and millimeter-wave spectral regions. Rotational spectroscopy and microwave spectroscopy are synonymous. Vibrations are relative motions of the atomic nuclei and are studied by both infrared and Raman spectroscopy. Electronic excitations are studied using visible and ultraviolet spectroscopy as well as fluorescence spectroscopy.

Studies in molecular spectroscopy led to the development of the first maser and contributed to the subsequent development of the laser.

The combination of atoms or molecules into crystals or other extended forms leads to the creation of additional energetic states. These states are numerous and therefore have a high density of states. This high density often makes the spectra weaker and less distinct, i.e., broader. For instance, blackbody radiation is due to the thermal motions of atoms and molecules within a material. Acoustic and mechanical responses are due to collective motions as well.
Pure crystals, though, can have distinct spectral transitions, and the crystal arrangement also has an effect on the observed molecular spectra. The regular lattice structure of crystals also scatters x-rays, electrons or neutrons allowing for crystallographic studies.

Nuclei also have distinct energy states that are widely separated and lead to gamma ray spectra. Distinct nuclear spin states can have their energy separated by a magnetic field, and this allows for nuclear magnetic resonance spectroscopy.

Other types of spectroscopy are distinguished by specific applications or implementations:


The history of spectroscopy began with Isaac Newton's optics experiments (1666–1672). Newton applied the word "spectrum" to describe the rainbow of colors that combine to form white light and that are revealed when the white light is passed through a prism. During the early 1800s, Joseph von Fraunhofer made experimental advances with dispersive spectrometers that enabled spectroscopy to become a more precise and quantitative scientific technique. Since then, spectroscopy has played and continues to play a significant role in chemistry, physics, and astronomy.






</doc>
<doc id="27753" url="https://en.wikipedia.org/wiki?curid=27753" title="List of science fiction themes">
List of science fiction themes

The following is a list of articles about recurring themes in science fiction.











</doc>
<doc id="27754" url="https://en.wikipedia.org/wiki?curid=27754" title="Samaritanism">
Samaritanism

Samaritan religion is the national religion of the Samaritans. The Samaritans follow the "Samaritan" Torah, which the Samaritans believe to be the original unchanged Torah, as opposed to the Torah used by Jews. In addition to the Samaritan Torah, Samaritans also rever their version of the Book of Joshua and recognize some of the Biblical figures such as Eli. 

Samaritanism is internally described as the religion that began with Moses, unchanged over the millennia that have since passed. Samaritans believe Judaism, as well as the Jewish Torah, has been corrupted by time and thus is no longer serving the duties God mandated on Mount Sinai. Additional differences with Judaism center on the place of worship which in Samaritanism is recognized as Mount Gerizim in Samaria, as opposed to Mount Moriya in Judea within Judaism.

Samaritanism holds that the summit of Mount Gerizim is the true location of God's Holy Place, as opposed to the Foundation Stone on the Temple Mount as Judaism teaches. As such, Samaritans trace their history as a separate entity from the Jews back to the time of Moses, where they believe Joshua laid the foundation for their temple. Samaritan historiography traces the schism itself to the High Priest Eli abandoning Moses' Tabernacle in favor of Mount Gerizim citation needed following Joshua's death.

Abu l-Fath, who in the 14th century wrote a major work of Samaritan history, comments on Samaritan origins as follows:

Further, the "Samaritan Chronicle Adler", or New Chronicle, believed to have been composed in the 18th century using earlier chronicles as sources states:

Samaritanism emerged as an independent ethnic culture following its survival of the Assyrian captivity in the 8th century BC. Jewish sources attest their own narrative of the origins of the Samaritans. From here there are conflicting proposals, including the Samaritans being the people of Kutha described in the Talmud. The traditional Jewish narrative, formed based off 2 Kings and Josephus, details the people of Israel were removed by the king of the Assyrians (Sargon II) to Halah, to Gozan on the Khabur River and to the towns of the Medes. The king of the Assyrians then brought people from Babylon, Kutha, Avah, Emath, and Sepharvaim to place in Samaria. Because God sent lions among them to kill them, the king of the Assyrians sent one of the priests from Bethel to teach the new settlers about God's ordinances. The eventual result was that the new settlers worshiped both the God of the land and their own gods from the countries from which they came. However, genetic studies showed the Samaritans are almost definitely descendants of the historical Israelite population, albeit isolated given the people's reclusive history. This casts doubt into, if not totally disproves, this historical theory that Samaritans originated from Assyria.

Furthermore, the Dead Sea scroll 4Q372, which recounts the hope that the northern tribes will return to the land of Joseph, remark that the current dwellers in the north are referred to as fools, an enemy people, however they are not referred to as foreigners. It goes on to say that these people, the Samaritans, mocked Jerusalem and built a temple on a high place (Gerizim) to provoke Israel.

Conflict between the Samaritans and the Jews were numerous between the end of the Assyrian diaspora and to the Bar Kokhba revolt. The Tanakh describes multiple instigations from the Samaritan population against the Jews and disparages them, Jesus' Parable of the Good Samaritan also gives evidence of conflict. The destruction of Mount Gerizim's Samaritan temple is attributed to the High Priest John Hyrcanus.

Following the failed revolts, Mount Gerizim was rededicated with a new temple, which was ultimately again destroyed during the Samaritan Revolts. Persecution of Samaritans was common in the following centuries.

The principle beliefs of Samaritanism are as follows:


The Samaritans have retained an offshoot of the Ancient Hebrew script, a High Priesthood, the slaughtering and eating of lambs on Passover eve, and the celebration of the first month's beginning around springtime as the New Year. Yom Teru'ah (the Biblical name for "Rosh Hashanah"), at the beginning of Tishrei, is not considered a New Year as it is in Rabbinic Judaism. The Samaritan Pentateuch differs from the Jewish Masoretic Text as well. Some differences are doctrinal: for example, the Samaritan Torah explicitly states that Mount Gerizim is "the place that God "has chosen"" to establish His name, as opposed to the Jewish Torah that refers to "the place that God "chooses"". Other differences are minor and seem more or less accidental.

Passover is particularly important in the Samaritan community, climaxing with sacrificing up to 40 sheep. The Counting of the Omer remains largely unchanged, however the week before Shavuot is a unique festival celebrating the continued commitment Samaratinism has maintained since the time of Moses. Shavuot is characterized by nearly day-long services of continuous prayer, especially over the stones on Gerizim tradition attributes to Joshua. During Sukkot, the sukkah is built inside houses as opposed to traditional outdoor settings. The restrictions of Yom Kippur are more universal in Samaritanism, with even breastfeeding and the feeding of children being disallowed, and the separation of gender during services is never enforced. 

Samaritan law differs from Halakha (Rabbinic Jewish law) and other Jewish movements. The Samaritans have several groups of religious texts, which correspond to Jewish Halakha. A few examples of such texts are:



</doc>
<doc id="27760" url="https://en.wikipedia.org/wiki?curid=27760" title="Statute of Anne">
Statute of Anne

The Statute of Anne, also known as the Copyright Act 1710 (cited either as 8 Ann. c. 21 or as 8 Ann. c. 19), is an act of the Parliament of Great Britain passed in 1710, which was the first statute to provide for copyright regulated by the government and courts, rather than by private parties.

Prior to the statute's enactment in 1710, copying restrictions were authorized by the Licensing of the Press Act 1662. These restrictions were enforced by the Stationers' Company, a guild of printers given the exclusive power to print—and the responsibility to censor—literary works. The censorship administered under the Licensing Act led to public protest; as the act had to be renewed at two-year intervals, authors and others sought to prevent its reauthorisation. In 1694, Parliament refused to renew the Licensing Act, ending the Stationers' monopoly and press restrictions.

Over the next 10 years the Stationers repeatedly advocated bills to re-authorize the old licensing system, but Parliament declined to enact them. Faced with this failure, the Stationers decided to emphasise the benefits of licensing to authors rather than publishers, and the Stationers succeeded in getting Parliament to consider a new bill. This bill, which after substantial amendments was granted Royal Assent on 5 April 1710, became known as the Statute of Anne due to its passage during the reign of Queen Anne. The new law prescribed a copyright term of 14 years, with a provision for renewal for a similar term, during which only the author and the printers to whom they chose to license their works could publish the author's creations. Following this, the work's copyright would expire, with the material falling into the public domain. Despite a period of instability known as the Battle of the Booksellers when the initial copyright terms under the Statute began to expire, the Statute of Anne remained in force until the Copyright Act 1842 replaced it.

The statute is considered a "watershed event in Anglo-American copyright history ... transforming what had been the publishers' private law copyright into a public law grant". Under the statute, copyright was for the first time vested in authors rather than publishers; it also included provisions for the public interest, such as a legal deposit scheme. The Statute was an influence on copyright law in several other nations, including the United States, and even in the 21st century is "frequently invoked by modern judges and academics as embodying the utilitarian underpinnings of copyright law".

With the introduction of the printing press to England by William Caxton in 1476, printed works became both more common and more economically important. As early as 1483, Richard III recognised the value of literary works by specifically exempting them from the government's protectionist legislation. Over the next fifty years, the government moved further towards economic regulation, abolishing the provision with the Printers and Binders Act 1534, which also banned the import of foreign works and empowered the Lord Chancellor to set maximum pricing for English books. This was followed by increasing degrees of censorship. A further proclamation of 1538, aiming to stop the spread of Lutheran doctrine, saw Henry VIII note that "sondry contentious and sinyster opiniones, have by wrong teachynge and naughtye bokes increaced and growen within this his realme of England", and declare that all authors and printers must allow the Privy Council or their agents to read and censor books before publication.

This censorship peaked on 4 May 1557, when Mary I issued a royal warrant formally incorporating the Stationers' Company. The old method of censorship had been limited by the Second Statute of Repeal, and with Mary's increasing unpopularity the existing system was unable to cope with the number of critical works being printed. Instead, the royal warrant devolved this power to the Company. This was done by decreeing that only the Company's publishers could print and distribute books. Their Wardens were given the power to enter any printing premises, destroy illegal works and imprison anyone found manufacturing them. In this way the government "harnessed the self interest of the publishers to the yoke of royal incentive", guaranteeing that the Company would follow the rules due to the economic monopoly it gave their members. With the abolition of the Star Chamber and Court of High Commission by the Long Parliament, the legal basis for this warrant was removed, but the Long Parliament chose to replace it with the Licensing Act 1662. This provided that the Company would retain their original powers, and imposed additional restrictions on printing; King's Messengers were permitted to enter any home or business in search of illegal presses. The legislation required renewal every two years, and was regularly reapproved.

This was not "copyright" as is normally understood; although there was a monopoly on the right to copy, this was available to publishers, not authors, and did not exist by default; it only applied to books which had been accepted and published by the Company. A member of the Company would register the book, and would then have a perpetual copyright over its printing, copying and publication, which could be leased, transferred to others or given to heirs upon the member's death. The only exception to this was that, if a book was out of print for more than 6 months and the publisher ignored a warning to make it available, the copyright would be released and other publishers would be permitted to copy it. Authors themselves were not particularly respected until the 18th century, and were not permitted to be members of the Company, playing no role in the development or use of its licences despite the Company's sovereign authority to decide what was published. There is evidence that some authors were recognised by the Company itself to have the right to copy and the right to alter their works; these authors were uniformly the writers of uneconomical books who were underwriting their publication.

The Company's monopoly, censorship and failure to protect authors made the system highly unpopular; John Milton wrote "Areopagitica" as a result of his experiences with the Company, accusing Parliament of being deceived by "the fraud of some old patentees and monopolisers in the trade of bookselling". He was not the first writer to criticise the system, with John Locke writing a formal memorandum to the MP Edward Clarke in 1693 while the Licensing Act was being renewed, complaining that the existing system restricted the free exchange of ideas and education while providing an unfair monopoly for Company members. Academic Mark Rose attributes the efforts of Milton to promote the "bourgeois public sphere", along with the Glorious Revolution's alterations to the political system and the rise of public coffee houses, as the source of growing public unhappiness with the system. At the same time, this was a period in which clearly defined political parties were taking shape, and with the promise of regular elections, an environment where the public were of increasing importance to the political process. The result was a "developing public sphere [which] provided the context that enabled the collapse of traditional press controls".

The result of this environment was the lapse of the Licensing Act. In November 1694, a committee was appointed by the Commons to see what laws were "lately expired and expiring [and] fit to be revived and continued". The Committee reported in January 1695, and suggested the renewal of the Licensing Act; this was included in the "Continuation Bill", but rejected by the House of Commons on 11 February. When it reached the House of Lords, the Lords re-included the Licensing Act, and returned the bill to the Commons. In response, a second committee was appointed - this one to produce a report indicating why the Commons disagreed with the inclusion of the Licensing Act, and chaired by Edward Clarke. This committee soon reported to the Commons, and Clarke was ordered to carry a message to the Lords requesting a conference over the Act. On 18 April 1695, Clarke met with representatives of the Lords, and they agreed to allow the Continuation Bill to pass without the renewal of the Licensing Act. With this, "the Lords' decision heralded an end to a relationship that had developed throughout the sixteenth and seventeenth centuries between the State and the Company of Stationers", ending both nascent publishers' copyright and the existing system of censorship.

John Locke's close relationship with Clarke, along with the respect he commanded, is seen by academics as what led to this decision. Locke had spent the early 1690s campaigning against the statute, considering it "ridiculous" that the works of dead authors were held perpetually in copyright. In letters to Clarke he wrote of the absurdity of the existing system, complaining primarily about the unfairness of it to authors, and "[t]he parallels between Locke's commentary and those reasons presented by the Commons to the Lords for refusing to renew the 1662 Act are striking". He was assisted by a number of independent printers and booksellers, who opposed the monopolistic aspects of the Act, and introduced a petition in February 1693 that the Act prevented them from conducting their business. The "developing public sphere", along with the harm the existing system had caused to both major political parties, is also seen as a factor.

The failure to renew the Licensing Act led to confusion and both positive and negative outcomes; while the government no longer played a part in censoring publications, and the monopoly of the Company over printing was broken, there was uncertainty as to whether or not copyright was a binding legal concept without the legislation. Economic chaos also resulted; with the Company now unable to enforce any monopoly, provincial towns began establishing printing presses, producing cheaper books than the London booksellers. The absence of the censorship provisions also opened Britain up as a market for internationally printed books, which were similarly cheaper than those British printers could produce.

The rejection of the existing system was not done with universal approval, and there were ultimately twelve unsuccessful attempts to replace it. The first was introduced to the House of Commons on 11 February 1695. A committee, again led by Clarke, was to write a "Bill for the Better Regulating of Printing and the Printing Presses". This bill was essentially a copy of the Licensing Act, but with a narrower jurisdiction; only books covering religion, history, the affairs of the state or the law would require official authorisation. Four days after its introduction, the Stationers' held an emergency meeting to agree to petition the Commons - this was because the bill did not contain any reference to books as property, eliminating their monopoly on copying. Clarke also had issues with the provisions, and the debate went on until the end of the Parliamentary session, with the bill failing to pass.

With the end of the Parliamentary session came the first general election under the Triennial Act 1694, which required the Monarch to dissolve Parliament every 3 years, causing a general election. This led to the "golden age" of the English electorate, and allowed for the forming of two major political parties - the Whigs and Tories. At the same time, with the failure to renew the Licensing Act, a political press developed. While the Act had been in force only one official newspaper existed; the "London Gazette", published by the government. After its demise, a string of newspapers sprang into being, including the "Flying Post", the "Evening Post" and the "Daily Courant". Newspapers had a strong bias towards particular parties, with the "Courant" and the "Flying Post" supporting the Whigs and the "Evening Post" in favour of the Tories, leading to politicians from both parties realising the importance of an efficient propaganda machine in influencing the electorate. This added a new dimension to the Commons' decision to reject two new renewals of the Licensing Act in the new Parliamentary session.

Authors, as well as Stationers, then joined the demand for a new system of licensing. Jonathan Swift was a strong advocate for licensing, and Daniel Defoe wrote on 8 November 1705 that with the absence of licensing, "One Man Studies Seven Year, to bring a finish'd Peice into the World, and a Pyrate Printer, Reprints his Copy immediately, and Sells it for a quarter of the Price ... these things call for an Act of Parliament". Seeing this, the Company took the opportunity to experiment with a change to their approach and argument. Instead of lobbying because of the effect the absence of legislation was having on their trade, they lobbied on behalf of the authors, but seeking the same things. The first indication of this change in approach comes from the 1706 pamphlet by John How, a stationer, titled "Reasons humbly Offer'd for a Bill for the Encouragement of Learning and the Improvement of Printing". This argued for a return to licensing, not with reference to the printers, but because without something to protect authors and guarantee them an income, "Learned men will be wholly discouraged from Propagating the most useful Parts of Knowledge and Literature". Using these new tactics and the support of authors, the Company petitioned Parliament again in both 1707 and 1709 to introduce a bill providing for copyright.

Although both bills failed, they led to media pressure that was exacerbated by both Defoe and How. Defoe's "A Review", published on 3 December 1709 and demanding "a Law in the present Parliament ... for the Encouragement of Learning, Arts, and Industry, by securing the Property of Books to the Authors or Editors of them", was followed by How's "Some Thoughts on the Present State of Printing and Bookselling", which hoped that Parliament "might think fit to secure Property in Books by a Law". This was followed by another review by Defoe on 6 December, in which he even went so far as to provide a draft text for the bill. On 12 December, the Stationers submitted yet another petition asking for legislation on the issue, and the House of Commons gave three MPs – Spencer Compton, Craven Peyton and Edward Wortley – permission to form a drafting committee. On 11 January 1710, Wortley introduced this bill, titling it "A Bill for the Encouragement of Learning and for Securing the Property of Copies of Books to the rightful Owners thereof".

The bill imposed fines on anyone who imported or traded in unlicensed or foreign books, required every book for which copyright protection was sought to be entered into the Stationers' Register, provided a legal deposit system centred around the King's Library, the University of Oxford and the University of Cambridge, but said nothing about limiting the term of copyright. It also specified that books were property; an emphasis on the idea that authors deserved copyright simply due to their efforts. The Stationers were enthusiastic, urging Parliament to pass the bill, and it received its second reading on 9 February. A Committee of the Whole met to amend it on 21 February, with further alterations made when it was passed back to the House of Commons on 25 February. Alterations during this period included minor changes, such as extending the legal deposit system to cover Sion College and the Faculty of Advocates, but also major ones, including the introduction of a limit on the length of time for which copyright would be granted.

Linguistic amendments were also included; the line in the preamble emphasising that authors possessed books as they would any other piece of property was dropped, and the bill moved from something designed "for Securing the Property of Copies of Books to the rightful Owners thereof" to a bill "for the Encouragement of Learning, by Vesting the Copies of Printed Books in the Authors or Purchasers of such Copies". Another amendment allowed anyone to own and trade in copies of books, undermining the Stationers. Other changes were made when the bill went to the House of Lords, and it was finally returned to the Commons on 5 April. The aims of the resulting statute are debated; Ronan Deazley suggests that the intent was to balance the rights of the author, publisher and public in such a way as to ensure the maximum dissemination of works, while other academics argue that the bill was intended to protect the Company's monopoly or, conversely, to weaken it. Oren Bracha, writing in the "Berkeley Technology Law Journal", says that when considering which of these options are correct, "the most probable answer [is] all of them". Whatever the motivations, the bill was passed on 5 April 1710, and is commonly known simply as the Statute of Anne due its passage during the reign of Queen Anne.

Consisting of 11 sections, the Statute of Anne is formally titled "An Act for the Encouragement of Learning, by Vesting the Copies of Printed Books in the Authors or Purchasers of Copies, during the Times therein mentioned". The preamble for the Statute indicates the purpose of the legislation - to bring order to the book trade - saying: 
The Statute then continued by stating the nature of copyright. The right granted was the right to copy; to have sole control over the printing and reprinting of books, with no provision to benefit the owner of this right after the sale. This right, previously held by the Stationers' Company's members, would automatically be given to the author as soon as it was published, although they had the ability to license these rights to another person. The copyright could be gained through two stages; first, the registration of the book's publication with the Company, to prevent unintentional infringement, and second, the deposit of copies of the book at the Stationers' Company, the royal library and various universities. One restriction on copyright was a "cumbersome system" designed to prohibit unreasonably high prices for books, which limited how much authors could charge for copies. There was also a prohibition on importing foreign works, with exceptions made for Latin and Greek classics.

Once registration had been completed and the deposits were made, the author was granted an exclusive right to control the copying of the book. Penalties for infringing this right were severe, with all infringing copies to be destroyed and large fines to be paid to both the copyright holder and the government; there was only a three-month statute of limitations on bringing a case, however. This exclusive right's length was dependent on when the book had been published. If it was published after 10 April 1710, the length of copyright was 14 years; if published before that date, 21 years. An author who survived until the copyright expired would be granted an additional 14-year term, and when that ran out, the works would enter the public domain. Copyright under the Statute applied to Scotland and England, as well as Ireland when that country joined the union in 1800.

The Statute was initially welcomed, ushering in "stability to an insecure book trade" while providing for a "pragmatic bargain" between the rights of the author, publisher and public intended to boost public learning and the availability of knowledge. The clause requiring book deposits, however, was not seen as a success. If the books were not deposited, the penalties would be severe, with a fine of £5. The number of deposits required, however, meant that it was a substantial burden; a print run might only be of 250 copies, and if they were particularly expensive to print, it could be cheaper to ignore the law. Some booksellers argued that the deposit provision only applied to registered books, and so deliberately avoided registration just to be able to minimise their liability. This was further undermined by the ruling in "Beckford v Hood", where the Court of King's Bench confirmed that, even without registration, copyright could be enforced against infringers.

Another failure, identified by Bracha, is not found in what the Statute covered, but in what it did not. The Statute did not provide any means for identifying authors, did not identify what constituted authored works, and covered only "books", even while discussing "property" as a whole. Moreover, the right provided was merely that of "making and selling ... exact reprints. To a large extent, the new regime was the old stationer's privilege, except it was universalised, capped in time, and formally conferred upon authors rather than publishers". The effect of the Statute on authors was also minimal. Previously, publishers would have bought the original manuscript from writers for a lump sum; with the passage of the Statute, they simply did the same thing, but with the manuscript's copyright as well. The remaining economic power of the Company also allowed them to pressure booksellers and distributors into continuing their past arrangements, meaning that even theoretically "public domain" works were, in practise, still treated as copyrighted.

When the copyrights granted to works published before the Statute began to expire in 1731, the Stationers' Company and their publishers again began to fight to preserve the status quo. Their first port of call was Parliament, where they lobbied for new legislation to extend the length of copyright, and when this failed, they turned to the courts. Their principal argument was that copyright had not been created by the Statute of Anne; it existed beforehand, in the common law, and was perpetual. As such, even though the Statute provided for a limited term, all works remained in copyright under the common law regardless of when statutory copyright expired. Starting in 1743, this began a thirty-year campaign known as the "Battle of the Booksellers". They first tried going to the Court of Chancery and applying for injunctions prohibiting other publishers from printing their works, and this was initially successful. A series of legal setbacks over the next few years, however, left the law ambiguous.

The first major action taken to clarify the situation was "Millar v Taylor". Andrew Millar, a British publisher, purchased the rights to James Thomson's "The Seasons" in 1729, and when the copyright term expired, a competing publisher named Robert Taylor began issuing his own reprints of the work. Millar sued, and went to the Court of King's Bench to obtain an injunction and advocate perpetual copyright at common law. The jury found that the facts submitted by Millar were accurate, and asked the judges to clarify whether common law copyright existed. The first arguments were delivered on 30 June 1767, with John Dunning representing Millar and Edward Thurlow representing Taylor. A second set of arguments were submitted for Millar by William Blackstone on 7 June, and judgment was given on 20 April 1769. The final decision, written by Lord Mansfield and endorsed by Aston and Willes JJ, confirmed that there existed copyright at common law that turned "upon Principles before and independent" of the Statute of Anne, something justified because it was right "that an Author should reap the pecuniary Profits of his own Ingenuity and Labour". In other words, regardless of the Statute, there existed a perpetual copyright under the common law. Yates J dissented, on the grounds that the focus on the author obscured the effect this decision would have on "the rest of mankind", which he felt would be to create a virtual monopoly, something that would harm the public and should certainly not be considered "an encouragement of the propagation of learning".

Although this decision was a boon to the Stationers, it was short-lived. Following "Millar", the right to print "The Seasons" was sold to a coalition of publishers including Thomas Becket. Two Scottish printers, Alexander and John Donaldson, began publishing an unlicensed edition, and Becket successfully obtained an injunction to stop them. This decision was appealed in "Donaldson v Beckett", and eventually went to the House of Lords. After consulting with the judges of the King's Bench, Common Pleas and Exchequer of Pleas, the Lords concluded that copyright was not perpetual, and that the term permitted by the Statute of Anne was the maximum length of legal protection for publishers and authors alike.

Until its repeal, most extensions to copyright law were based around provisions found in the Statute of Anne. The one successful bill from the lobbying in the 1730s, which came into force on 29 September 1739, extended the provision prohibiting the import of foreign books to also prohibit the import of books that, while originally published in Britain, were being reprinted in foreign nations and then shipped to England and Wales. This was intended to stop the influx of cheap books from Ireland, and also repealed the price restrictions in the Statute of Anne. Another alteration was over the legal deposit provisions of the Statute, which many booksellers found unfair. Despite an initial period of compliance, the principle of donating copies of books to certain libraries lapsed, partly due to the unwieldiness of the statute's provisions and partly because of a lack of cooperation by the publishers. In 1775 Lord North, who was Chancellor of the University of Oxford, succeeded in passing a bill that reiterated the legal deposit provisions and granted the universities perpetual copyright on their works.

Another range of extensions came in relation to what could be copyrighted. The Statute only referred to books, and being an Act of Parliament, it was necessary to pass further legislation to include various other types of intellectual property. The Engraving Copyright Act 1734 extended copyright to cover engravings, statutes in 1789 and 1792 involved cloth, sculptures were copyrighted in 1814 and the performance of plays and music were covered by copyright in 1833 and 1842 respectively. The length of copyright was also altered; the Copyright Act 1814 set a copyright term of either 28 years, or the natural life of the author if this was longer. Despite these expansions, some still felt copyright was not a strong enough regime. In 1837, Thomas Noon Talfourd introduced a bill into Parliament to expand the scope of copyright. A friend of many men of letters, Talfourd aimed to provide adequate rewards for authors and artists. He campaigned for copyright to exist for the life of the author, with an additional 60 years after that. He also proposed that existing statutes be codified under the bill, so that the case law that had arisen around the Statute of Anne was clarified.

Talfourd's proposals led to opposition, and he reintroduced modified versions of them year on year. Printers, publishers and booksellers were concerned about the cost implications for original works, and for reprinting works that had fallen out of copyright. Many within Parliament argued that the bill failed to take into account the public interest, including Lord Macaulay, who succeeded in defeating one of Talfourd's bills in 1841. The Copyright Act 1842 passed, but "fell far short of Talfourd's dream of a uniform, consistent, codified law of copyright". It extended copyright to life plus seven years, and, as part of the codification clauses, repealed the Statute of Anne.

The Statute of Anne is traditionally seen as "a historic moment in the development of copyright", and the first statute in the world to provide for copyright. Craig Joyce and Lyman Ray Patterson, writing in the "Emory Law Journal", call this a "too simple understanding [that] ignores the statute's source", arguing that it is at best a derivative of the Licensing Act. Even considering this, however, the Statute of Anne was "the watershed event in Anglo-American copyright history ... transforming what had been the publishers' private law copyright into a public law grant". Patterson, writing separately, does note the differences between the Licensing Act and the Statute of Anne; the question of censorship was, by 1710, out of the question, and in that regard the Statute is distinct, not providing for censorship.

It also marked the first time that copyright had been vested primarily in the author, rather than the publisher, and also the first time that the injurious treatment of authors by publishers was recognised; regardless of what authors signed away, the second 14-year term of copyright would automatically return to them. Even in the 21st century, the Statute of Anne is "frequently invoked by modern judges and academics as embodying the utilitarian underpinnings of copyright law". In "IceTV v Nine Network", for example, the High Court of Australia noted that the title of the Statute "echoed explicitly the emphasis on the practical or utilitarian importance that certain seventeenth-century philosophers attached to knowledge and its encouragement in the scheme of human progress". Despite "widely recognised flaws", the Act became a model copyright statute, both within the United Kingdom and internationally. Christophe Geiger notes that it is "a difficult, almost impossible task" to analyse the relationship between the Statute of Anne and early French copyright law, both because it is difficult to make a direct connection, and because the ongoing debate over both has led to radically different interpretations of each nation's law.

Similarly, Belgium took no direct influence from the Statute or English copyright theory, but Joris Deene of the University of Ghent identifies an indirect influence "at two levels"; the criteria for what constitutes copyrightable material, which comes from the work of English theorists such as Locke and Edward Young, and the underlying justification of copyright law. In Belgium, this justification is both that copyright serves the public interest, and that copyright is a "private right" that serves the interests of individual authors. Both theories were taken into account in "Donaldson v Beckett", as well as in the drafting of the Statute of Anne, and Deene infers that they subsequently affected the Belgian debates over their first copyright statute. In the United States, the Copyright Clause of the United States Constitution and the first Federal copyright statute, the Copyright Act of 1790, both draw on the Statute of Anne. The 1790 Act contains provisions for a 14-year term of copyright and sections that provide for authors who published their works before 1790, both of which mirror the protection offered by the Statute 80 years previously.





</doc>
<doc id="27761" url="https://en.wikipedia.org/wiki?curid=27761" title="School choice">
School choice

School choice is a term for K–12 public education options in the United States, describing a wide array of programs offering students and their families alternatives to publicly provided schools, to which students are generally assigned by the location of their family residence. In the United States, the most common—both by number of programs and by number of participating students—school choice programs are scholarship tax credit programs, which allow individuals or corporations to receive tax credits toward their state taxes in exchange for donations made to non-profit organizations that grant private school scholarships. In other cases, a similar subsidy may be provided by the state through a school voucher program. Other school choice options include open enrollment laws (which allow students to attend public schools outside the district in which the students live), charter schools, magnet schools, virtual schools, homeschooling, education savings accounts (ESAs), and individual tax credits or deductions for educational expenses.

Economist and Nobel laureate Milton Friedman proposed in 1955 using free market principles to improve the United States public school system. The practice had been that children were assigned a public school based on where their parents live, which public schools were funded by state and local taxes. Friedman proposed that parents should be able to receive those education funds in the form of vouchers, which would allow them to choose their children's schools, including both public and private, religious and non-religious options. In 1996, Friedman and his wife economist Rose Director Friedman founded the Friedman Foundation for Educational Choice, (now EdChoice). This American education reform organization headquartered in Indianapolis, Indiana seeks to advance “school choice for all children” nationwide.

The first use of school vouchers in the United States came in the form of state tuition grants provided by Virginia's 1956 Stanley Plan, which financed white-only private schools known as segregation academies. Other states followed until the practice was disallowed by "Griffin v. County School Board of Prince Edward County" (1964). While "school choice" has always implied school improvement, debates have regularly followed about the motivation and implementation.

States with scholarship tax credit programs grant individuals and/or businesses a credit, whether full or partial, toward their taxes for donations made to scholarship granting organizations (also called school tuition organizations). SGOs/STOs use the donations to create scholarships that are then given to help pay for the cost of tuition for students. These scholarships allow students to attend private schools or out-of-district public schools that would otherwise be prohibitively expensive for many families. These programs currently exist in fourteen states: Alabama, Arizona, Florida, Georgia, Illinois, Iowa, Kansas, Louisiana, Minnesota, New Hampshire, Oklahoma, Pennsylvania, Rhode Island, and Virginia in the United States.

Vouchers give students the opportunity to attend a private school of their choosing, secular or religious. This would be paid for by accessing all or part of the public funding set aside for their children’s education.

Charter schools are independent public schools which are exempt from many of the state and local regulations which govern most public schools. These exemptions grant charter schools some autonomy and flexibility with decision-making, such as teacher union contracts, hiring, and curriculum. In return, charter schools are subject to stricter accountability on spending and academic performance. The majority of states (and the District of Columbia) have charter school laws, though they vary in how charter schools are approved. Minnesota was the first state to have a charter school law and the first charter school in the United States, City Academy High School, opened in St. Paul, Minnesota in 1992. The prevalence of charter schools has increased with the support of the Obama Administration. Under the Administration, the Department of Education has provided funding incentives to states and school districts that increase the number of charter schools.

Somewhere between 22 and 26% of Dayton, Ohio children are in charter schools. This is the highest percentage in the nation. Other hotbeds for charter schools are Kansas City (24%), Washington, D.C. (20-24%), and Arizona. Almost one in four public schools in Arizona are charter schools, comprising about 8% of total enrollment.

Charter schools can also come in the form of cyber charters. Cyber charter schools deliver the majority of their instruction over the internet instead of in a school building. And, like all charter schools, cyber charters are public schools, but they are free from some of the rules and regulations that conventional public schools must follow.

Magnet schools are public schools that often have a specialized function like science, technology, or art. These magnet schools, unlike charter schools, are not open to all children. Much like many private schools, some (but not all) magnet schools require a test to get in. Magnet schools are an example of open enrollment programs. Open enrollment refers to district or statewide programs that allow families to choose public schools other than the ones they are assigned. Intradistrict open enrollment programs allow school choice within a district. Interdistrict open enrollment allows families to choose schools outside the district in other districts.

"Home education" or "home schooling" is instruction in a child's home, or provided primarily by a parent, or under direct parental control. Informal home education has always taken place, and formal instruction in the home has at times also been very popular. As public education grew in popularity during the 1900s, however, the number of people educated at home using a planned curriculum dropped. In the last 20 years, in contrast, the number of children being formally educated at home has grown tremendously, in particular in the United States. The laws relevant to home education differ throughout the country. In some states the parent simply needs to notify the state that the child will be educated at home. In other states the parents are not free to educate at home unless at least one parent is a certified teacher and yearly progress reports are reviewed by the state. Such laws are not always enforced however. According to the federal government, about 1.1 million children were home educated in 2003.

District of Choice

District of Choice is a program in California created during 1993, allowing any California public school district to enroll students outside district lines. To participate in the program, district governing boards only need to declare themselves a District of Choice and set a quota for how many transfer students to accept to participate in the program. School districts cannot discriminate among students to enroll, but can limit them through an unbiased lottery system. The program was created in response to several parent's concerns over the lack of choice of schools to enroll their children in. Currently 47 school districts and 10,000 students participate in the program, serving 5 percent of school districts and 0.2 percent of students in California.

This variant of school choice allows the parent to withdraw their child out of the public or charter school, and receive a direct deposit of public funds into a government-authorized savings account. These funds are often distributed in the form of a debit card that can be used to pay for various services, such as private school tuition and fees, online programs, private tutoring, community college costs, higher education services, and other approved learning materials and services. ESA’s also acquire the ability to pay for a combination of public school courses and private services.

Certain states allow parents to claim a tax credit or deduction as a means to provide relief for certain educational expenses. These can include private school tuition, textbooks, school supplies and equipment, tutoring, and transportation. Currently, Alabama, Illinois, Indiana, Iowa, Louisiana, Minnesota, and Wisconsin have such programs.


<nowiki>*</nowiki>Tax credits: lowers the total a person owes on taxes

<nowiki>*</nowiki>Tax deduction: reduces a persons total taxable income

Online learning permits students to work with teachers and their courses over the internet. This can be used in cooperation with, or in place of traditional classroom instruction. The online learning can be also paid for by accessing ESA’s and vouchers.

This form of tutelage is a student-tailored form of education. This form of instruction can have various combinations. For example, course choice programs, public school courses, and special education therapies can all be integrated into a students curriculum. There are a myriad of possibilities, especially as learning innovations continue to occur.

The goal of school choice programs is to give parents more control over their child's education and to allow parents to pursue the most appropriate learning environments for children. For example, school choice may enable parents to choose a school that provides religious instruction, stronger discipline, better foundational skills (including reading, writing, mathematics, and science), everyday skills (from handling money to farming), or other desirable foci.

Supporters of voucher models of school choice argue that choice creates competition between schools for students. Schools that fail to attract students can be closed. Advocates of school choice argue that this competition for students (and the dollars that come with them) create a catalyst for schools to create innovative programs, become more responsive to parental demands, and to increase student achievement. Caroline Hoxby suggests that this competition increases the productivity of a school. Hoxby describes a productive school as being one that produces high student achievement for each dollar spent. Others suggest that this competition gives parents more power to influence their child's school in the school marketplace. Parents and students become the consumers and schools must work to attract new students with new programs. Parents also have the ability to punish schools that they judge to be inferior by leaving the 'bad' school for a better, more highly ranked school. Parents look for schools that will advocate for the needs of their child and if the school does not meet the needs required for that child, parents have the choice to find a school that will be more suitable. This freedom to choose puts the consequences of good or bad choosing on the parents instead of the government.

Another argument in favor of school choice is based on cost-effectiveness. Studies undertaken by the Cato Institute and other libertarian and conservative think tanks conclude that privately run education both costs less and produces superior outcomes compared to public education.

Others argue that since children from impoverished families almost exclusively attend D or F ranked public schools, school choice programs would give parents the power to opt their children out of poorly-performing schools assigned by zip code and seek better education elsewhere. Supporters say this would level the playing field by broadening opportunities for low-income students—particularly minorities—to attend high-quality schools that would otherwise be accessible only to higher-income families.

The Organisation Internationale pour le Droit à l'Education et la Liberté d'Enseignement (OIDEL), an international non-profit organization for the development of freedom of education, maintains that the right to education is a fundamental human right which cannot exist without the presence of State benefits and the protection of individual liberties. According to the organization, freedom of education notably implies the freedom for parents to choose a school for their children without discrimination on the basis of finances. To advance freedom of education, OIDEL promotes a greater parity between public and private schooling systems.

Teachers' unions in the United States are very opposed to school choice. School choice measures are criticized as profiteering in an under-regulated environment. Charter authorization organizations have non-profit status; and contract with related for-profit entities with public funding. Some reports indicate that the New Markets Tax Credit allows double returns on charter school related investments. Reports indicate that charters create organizational arms that profit by charging high rent, and that while the facilities are used as schools, there are no property taxes. Other reports indicate bankers, hedge fund types and private equity investors gathered in New York to hear about opportunities at Capital Roundtable's conference on "private equity investing in for-profit education companies" which involve the collection of an individual's property taxes. Walton Foundation has also held charter school investment conferences featuring Standard & Poor's, Piper Jaffray, Bank of America, and Wells Capital Management.

Public school entities are chiefly concerned that these school choice measures are taking funding away from public schools and therefore depleting their already strained resources. Other opponents of certain school choice policies (particularly vouchers) have cited the Establishment Clause and individual state Blaine amendments, which forbid, to one degree or another, the use of direct government aid to religiously affiliated entities. This is of particular concern in the voucher debate because voucher dollars are often spent at parochial schools.

Some school choice measures are criticized by public school entities, organizations opposed to church-state entanglement, and self-identified liberal advocacy groups. Known plaintiffs who have filed suit to challenge the constitutionality of state sponsored school choice laws are as follows: School Boards Associations, Public School Districts, Federations for Teachers, Associations of School Business Officials, Education Associations/Associations of Educators (unions for public school teachers), the American Civil Liberties Union, Freedom From Religion Foundation, and People for the American Way.

There is evidence that school choice programs reduce housing prices, and that they do so in high-performing districts more than in low-performing districts.

School choice systems afford families the opportunity to choose where to enroll their child among all available schools in a district. The policy is an alternative to neighborhood schools, which often assign low-income families to lower-performing schools and high-income families to higher performing, better-funded schools. In theory, school choice should lead to less segregation by race and class than neighborhood school policies by allowing families to select schools outside their neighborhoods. However, a closer look at the reality of school choice presents several ways in which the policy does not provide equal opportunities to all students. A longitudinal interview study showed that families of lower socioeconomic status considered and eventually chose lower-performing schools than their higher-income peers. Low-income families lacked the social networks and access to information to learn which schools were higher-performing, and sometimes a family's own history with school achievement affected their choice for their child. Many of the current school choice models do not offer transportation to out-of-neighborhood schools, which discourages low-income families from selecting schools outside their neighborhoods.

For the California School District of Choice program, analysis from the California State Assembly demonstrates possible evidence that the program is excluding low-income students and increasing racial segregation. A report from the legislature lists several Districts of Choice accepting Black, Latino, and low-income students at lower percentages than the city or the student's original district demographics. The legislature found that Latino students consist of 66 percent of students enrolling in original districts, while only 32 percent transfer to Districts of Choice. During the program's inception, a budget was provided for transportation, but the budget currently does not exist and some Districts of Choice do not pick up that extra cost. Critics argue that the lack of transportation creates a barrier for low income students to enter the program. Several school districts have sued Districts of Choice over concerns of accepting and excluding specific demographics.

Charter schools are another example of school choice. Parents can opt out of their traditional public school to attend a charter, regardless of school district. Charters are significantly more segregated than their traditional public school counterparts. A descriptive study using three national data sources on schools in the United States examined the enrollment characteristics of charter schools as opposed to traditional public schools. The study found that Black students are over-enrolled in charters nationally, and the same is true for Latino students in metro areas. In a majority of states, at least half of the Latino and Black students in charters are in intensely segregated minority schools (90–100% minority.) Conversely, in diverse parts of the West and the South, charters serve racially isolated white populations. Charters also enroll a higher percent of low-income students than public schools nationally.

The next question we must answer is why school choice systems and charter schools are more racially segregated. One theory is that because choice systems allow parents to select their own school, any differing priorities between racial groups would cause some schools to be more appealing to black families, others to white families, et cetera. While no racial differences in school preferences have been found through self-report, to truly examine parental motivations, we must also look at behaviors. This idea was examined in a group of parents who had switched their children out of the traditional public school and into a charter. A survey was paired with an analysis of the characteristics of the school that each family left, and the characteristics of the charter school where the family then placed their child. These results showed that regardless of stated preferences, parents chose to place their students in schools where they would have more classmates of their own race, even if this meant placing the child in a school with lower test scores. This effect was the same across White, Black and Latino families. However, this trend was seen only in choosing elementary and middle schools, not at the high school level.
Although it appears that parents are moving to more racially segregated schools, these choices are not necessarily motivated by race. Instead, parents may be influenced by the location of the school. In a similar study, the proximity of the school to the family's home was found to be a significant predictor of school choice. The author suggests, then, that the segregation found in charter schools is not due to a parental bias towards racially homogeneous schools. Instead, segregation in school choice programs is a reflection of the deeply segregated neighborhoods in our country, and the fact that parents want their child's school to be close to home.

Finally, it is important to consider the ramifications of a system that is significantly more racially segregated than traditional public schools. In an ethnographic study of three California charters serving populations segregated by race and class, the majority Black and Latino schools suffered from fewer financial resources. These schools received less financial support from families, and had to fund services for their students that families from the majority white school paid for on their own, such as school supplies or emotional counseling. Additionally, teachers in the lower-income, minority segregated schools had less training and fewer resources. One result of this was that frustrated teachers were more likely to attribute difficulties to negative student characteristics and the values of the child's family. Black and Latino students have also been shown to have significantly lower achievement gains on standardized testing when in schools with high minority populations, based on a correlational study using No Child Left Behind data.
The broad effect of school choice and charter schools is to increase race and class segregation in schools. This isolation creates environments in schools that do not reflect the make-up of the United States, and prevent students from being exposed to peers of different races or cultures. More research is needed to examine how racially and economically segregated schools affect students' attitudes towards other races and classes. It is also important to remember that even some schools with diverse populations do not function as such: many highly "integrated" schools are in fact heavily tracked, with disproportionately fewer students of color in advanced tracks. The real work ahead is to devise a system to functionally integrate schools so that students of all races and classes have access to equally successful schools, advanced courses, and preparation for higher education.

The basic compulsory educational system in Finland is the nine-year comprehensive school (Finnish "peruskoulu", Swedish "grundskola", "basic school"), for which school attendance is mandatory (homeschooling is allowed, but extremely rare). There are no so-called "gifted" programs. The more able children are expected to help those who are slower to catch on.

The French government subsidizes most private primary and secondary schools, including those affiliated with religious denominations, under contracts stipulating that education must follow the same curriculum as public schools and that schools cannot discriminate on grounds of religion or force pupils to attend religion classes.

This system of "école libre" (Free Schooling) is mostly used not for religious reasons, but for practical reasons (private schools may offer more services, such as after-class tutoring) as well as the desire of parents living in disenfranchised areas to send their children away from the local schools, where they perceive that the youth are too prone to delinquency or have too many difficulties keeping up with schooling requirements that the educational content is bound to suffer. The threatened repealing of that status in the 1980s triggered mass street demonstrations in favor of the status. 

Sweden reformed its school system in 1992. Its system of school choice is one of the freest in the world, allowing students to use public funds for the publicly or privately run school of their choice, including religious and for-profit schools. Fifteen years after the reform, private school enrollment had increased from 1% to 10% of the student population.

In Chile, there is an extensive voucher system in which the state pays private and municipal schools directly, based on average attendance (90% of the country students utilize such a system). The result has been a steady increase in the number and recruitment of private schools that show consistently better results in standardized testing than municipal schools. The reduction of students in municipal schools has gone from 78% of all students in 1981, to 57% in 1990, and to less than 50% in 2005.

Regarding vouchers in Chile, researchers have found that when controls for the student's background (parental income and education) are introduced, the difference in performance between public and private subsectors is not significant. There is also greater variation within each subsector than between the two systems.

A variety of forms of school choice exist in the United States. It is a highly debatable subject because some people wish to use taxpayer dollars in order to allow low-income students the choice of a private schools by way of vouchers. Research has shown that the students who transfer do worse in math and reading than their public school counterparts; not until an average of four years in the voucher program do the transfer students show to be performing equal to their counterparts in the public education system.

Scholarship tax credit programs currently exist in Alabama, Arizona, Florida, Georgia, Illinois, Iowa, Kansas, Louisiana, Minnesota, New Hampshire, Oklahoma, Pennsylvania, Rhode Island, and Virginia.

Arizona has a well-known and fast-growing tax credit program. In the Arizona Individual Private School Tuition Tax Credit Program, in accordance with A.R.S. §43-1089 and §1089.03, individuals can claim up to $1,053 and couples filing joint returns can claim up to $2106 (for 2014, amounts are indexed annually). Nearly 24,000 children received scholarships in the 2011-2012 school year. Since the program has started in 1998, over 77,500 taxpayers have participated in the program, providing over $500 million in scholarship money for children at private schools across the state.

The Arizona program was challenged in court in "ACSTO v Winn" by a group of state taxpayers on the grounds that the tax credit violated the First Amendment because the tuition grants could go to students who attend private schools with religious affiliations. The suit was initially brought against the state until the Arizona Christian School Tuition Organization (ACSTO), one of the largest School Tuition Organizations in the state, voluntarily stepped in to represent the defense with the help of the Alliance Defending Freedom (formerly Alliance Defense Fund). Typically, taxpayers are not allowed to bring suit against the government regarding how taxes are spent because injury would be purely speculative. In addition, insomuch as a donation to a School Tuition Organization is still a charitable act, just like any donation to a charity, there would be no standing unless all charitable deduction programs nationwide were brought under scrutiny. The Court ruled 5-4 to let the tax credit program stand. In April 2011, a Fairleigh Dickinson University PublicMind poll found that a majority of American voters (60%) felt that the tax credits support school choice for parents whereas 26% felt as it the tax credits support religion.

In Iowa, the Educational Opportunities Act was signed into law in 2006, creating a pool of tax credits for eligible donors to student tuition organizations (STOs). At first, these tax caps were $5 million but in 2007, Governor Chet Culver increased the total amount to $7.5 million. The Iowa Alliance for Choice in Education (Iowa ACE) oversees the STOs and advocates for school choice in Iowa.

Greater Opportunities for Access to Learning (GOAL) is the Georgia program which offers a state income tax credit to donors of scholarships to private schools. Representative David Casas was responsible for passing the Georgia version of the school choice legislation.

Vouchers currently exist in Wisconsin, Ohio, Florida, Indiana and, most recently, the District of Columbia and Georgia.

The largest and oldest Voucher program is in Milwaukee. Started in 1990, and expanded in 1995, it currently allows no more than 15% of the district's public school enrollment to use vouchers. As of 2005 over 14,000 students use vouchers and they are nearing the 15% cap.

School vouchers are legally controversial in some states. In 2014 a lawsuit sought to challenge the legality of the Florida voucher program.

In the U.S., the legal and moral precedents for vouchers may have been set by the G.I. bill, which includes a voucher program for university-level education of veterans. The G.I. bill permits veterans to take their educational benefits at religious schools, an extremely divisive issue when applied to primary and secondary schools.

In "Zelman v. Simmons-Harris", 536 U.S. 639 (2002), the Supreme Court of the United States held that school vouchers could be used to pay for education in sectarian schools without violating the Establishment Clause of the First Amendment. As a result, states are basically free to enact voucher programs that provide funding for any school of the parent's choosing.

The Supreme Court has not decided, however, whether states can provide vouchers for secular schools only, excluding sectarian schools. Proponents of funding for parochial schools argue that such an exclusion would violate the free exercise clause. However, in "Locke v. Davey", 540 U.S. 712 (2004), the Court held that states could exclude majors in "devotional theology" from an otherwise generally available college scholarship. The Court has not indicated, however, whether this holding extends to the public school context, and it may well be limited to the context of individuals training to enter the ministry.

The majority of states (and the District of Columbia) have charter school laws. Minnesota was the first state to have a charter school law and the first charter school in the United States, City Academy, opened in St. Paul, Minnesota in 1992.

Dayton, Ohio has between 22–26% of all children in charter schools. This is the highest percentage in the nation. Other hotbeds for charter schools are Kansas City (24%), Washington, D.C. (20-24%) and the State of Arizona. Almost 1 in 4 public schools in Arizona are charter schools, comprising about 8% of total enrollment.

Charter schools can also come in the form of Cyber Charters. Cyber charter schools deliver the majority of their instruction over the internet instead of in a school building. And, like charter schools, they are public schools, but free of many of the rules and regulations that public schools must follow.

Magnet schools are public schools that often have a specialized function like science, technology or art. These magnet schools, unlike charter schools, are not open to all children. Much like many private schools, the students must test into the school.

The laws relevant to homeschooling differ between US states. In some states the parent simply needs to notify the state that the child will be educated at home. In other states the parents are not free to educate at home unless at least one parent is a certified teacher and yearly progress reports are reviewed by the state. Such laws are not always enforced however. According to the Federal Government, about 1.1 million children were Home Educated in 2003.

The United States has school choice at the university level. College students can get subsidized tuition by attending "any" public college or university within their state of residence. Furthermore, the U.S. federal government provides tuition assistance for both public and private colleges via the G.I. Bill and federally guaranteed student loans.




</doc>
<doc id="27762" url="https://en.wikipedia.org/wiki?curid=27762" title="Star Frontiers">
Star Frontiers

Star Frontiers is a science fiction role-playing game produced by TSR beginning in 1982. The game offered a space opera action-adventure setting.

"Star Frontiers" takes place near the center of a spiral galaxy (the setting does not specify whether the galaxy is our own Milky Way). 
A previously undiscovered quirk of the laws of physics allows starships to jump to "The Void", a hyperspatial realm that greatly shortens the travel times between inhabited worlds, once they reach 1% of the speed of light (3,000 km/s).

The basic game setting was an area known as "The Frontier Sector" where four sentient races (Dralasite, Humans, Vrusk, and Yazirian) had met and formed the United Planetary Federation (UPF). The original homeworlds of the Dralasites, Humans, and Vrusk were never detailed in the setting and it is possible that they no longer existed. A large number of the star systems shown on the map of the Frontier sector in the basic rulebook were unexplored and undetailed, allowing the Gamemaster (called the "referee" in the game) to put whatever they wished there.

Players could take on any number of possible roles in the setting but the default was to act as hired agents of the Pan Galactic corporation in exploring the Frontier and fighting the aggressive incursions of the alien and mysterious worm-like race known as the Sathar. Most published modules for the game followed these themes.


These races were altered heavily and reused in TSR's "Spelljammer", and were later loosely republished for "d20 Future" by Wizards of the Coast.


The game was a percentile-based system and used only 10-sided dice (d10). Characters had attributes rated from 1-100 (usually in the 25-75 range) which could be rolled against for raw-attribute actions such as lifting items or getting out of the way of falling rocks. There were eight attributes that were paired together (and shared the same rating to begin with)—Strength/Stamina, Dexterity/Reaction Speed, Intuition/Logic, and Personality/Leadership.

Characters also each had a Primary Skill Area (PSA—Military, Technological, or Biosocial) which allowed them to buy skills that fell into their PSA at a discount. Skills were rated from 1–6 and usually consisted of a set of subskills that gave a chance for accomplishing a particular action as a base percentage plus a 10% bonus for each skill level the character had in the skill. Weapon skills were based on the character's relevant attribute (Dexterity or Strength) but other skills had a base chance of success independent of the character's attributes. Many of the technological skills were penalized by the complexity of the robot, security system, or computer the character was attempting to manipulate (also rated from 1 to 6).

Characters were usually quite durable in combat—it would take several hits from normal weapons to kill an average character. Medical technology was also advanced—characters could recover quickly from wounds with appropriate medical attention and a dead character could be "frozen" and revived later.

Vehicle and robot rules were included in the "Alpha Dawn" basic set. A beneficial feature of the game was its seamless integration of personal, vehicle and aerial combat simulation. The "Knight Hawks" rules expansion set included detailed rules for starships.
The basic set also included a short "bestiary" of creatures native to the world of Volturnus (the setting for the introductory module included with the basic boxed set), along with rules for creating new creatures.

Character advancement consisted of spending experience points on improving skills and attributes.

The basic boxed set was renamed "Alpha Dawn" after the expansions began publication. It included two ten-sided dice, a large set of cardboard counters, and a folding map with a futuristic city on one side and various wilderness areas on the other for use with the included adventure, SF-0: "Crash on Volturnus".

A second boxed set called "Knight Hawks" followed shortly. It provided rules for using starships in the setting and also a set of wargame rules for fighting space battles between the UPF and Sathar. Included were counters for starships, two-ten sided dice, a large folding map with open space on one side and on the other a space station and starship (for use with the included adventure), and the adventure SFKH-0: "Warriors of White Light". This set was designed by Douglas Niles (who also designed the D&D wargame "Battlesystem", released two years later).

Adventures printed separately for the game included two more adventures set on Volturnus (SF-1: "Volturnus, Planet of Mystery" and SF-2: "Starspawn of Volturnus" continuing the adventure included in the basic set), SF-3: "Sundown on Starmist", SF-4: "Mission to Alcazzar", SF-5: "Bugs in the System" and SF-6: "Dark Side of the Moon". The last two modules (SF-5 and SF-6) were written by authors from TSR's UK division, and are distinctly different from the others in the series in tone and production style.

Adventures using the "Knight Hawks" rules included SFKH-1: "Dramune Run" and a trilogy set "Beyond the Frontier" in which the players learn more about the Sathar and foil their latest plot (SFKH-2: "Mutiny on the Eleanor Moraes", SFKH-3: "Face of the Enemy", and SFKH-4: "The War Machine").

Two modules also re-created the plot and setting of the movies "" and "".

A late addition to the line was "Zebulon's Guide to Frontier Space" which introduced several additional races and radical changes to the game's mechanics. Of the three planned volumes of the Guide, only the first was ever published (in 1985), leaving the game in a partially-overhauled state. Gamers were given little to no practical advice on how to convert their existing characters to the new rules, and TSR never published any further products using the "Zebulon's" concepts.

Wizards of the Coast published many of the races originally found in "Star Frontiers" in their "d20 Future" supplement for d20 Modern.

A version of the setting called "Star Law", which uses the d20 system rules was published as an alternate campaign setting in the d20 Future book. It uses the species names of Vrusk, Dralasite, Sathar, and Yazirian, but is not actually the "Star Frontiers" setting.



</doc>
<doc id="27763" url="https://en.wikipedia.org/wiki?curid=27763" title="Structuralism">
Structuralism

In sociology, anthropology, and linguistics, structuralism is the methodology that implies elements of human culture must be understood by way of their relationship to a broader, overarching system or structure. It works to uncover the structures that underlie all the things that humans do, think, perceive, and feel. Alternatively, as summarized by philosopher Simon Blackburn, structuralism is "the belief that phenomena of human life are not intelligible except through their interrelations. These relations constitute a structure, and behind local variations in the surface phenomena there are constant laws of abstract culture".

Structuralism in Europe developed in the early 1900s, in the structural linguistics of Ferdinand de Saussure and the subsequent Prague, Moscow and Copenhagen schools of linguistics. In the late 1950s and early 1960s, when structural linguistics was facing serious challenges from the likes of Noam Chomsky and thus fading in importance, an array of scholars in the humanities borrowed Saussure's concepts for use in their respective fields of study. French anthropologist Claude Lévi-Strauss was arguably the first such scholar, sparking a widespread interest in structuralism.

The structuralist mode of reasoning has been applied in a diverse range of fields, including anthropology, sociology, psychology, literary criticism, economics and architecture. The most prominent thinkers associated with structuralism include Claude Lévi-Strauss, linguist Roman Jakobson, and psychoanalyst Jacques Lacan. As an intellectual movement, structuralism was initially presumed to be the heir apparent to existentialism. However, by the late 1960s, many of structuralism's basic tenets came under attack from a new wave of predominantly French intellectuals such as the philosopher and historian Michel Foucault, the philosopher and linguist Jacques Derrida, the Marxist philosopher Louis Althusser, and the literary critic Roland Barthes. Though elements of their work necessarily relate to structuralism and are informed by it, these theorists have generally been referred to as post-structuralists. In the 1970s, structuralism was criticized for its rigidity and ahistoricism. Despite this, many of structuralism's proponents, such as Lacan, continue to assert an influence on continental philosophy and many of the fundamental assumptions of some of structuralism's post-structuralist critics are a continuation of structuralism.

The term "structuralism" is a related term that describes a particular philosophical/literary movement or moment. The term appeared in the works of French anthropologist Claude Lévi-Strauss and gave rise in France to the "structuralist movement," which influenced the thinking of other writers such as Louis Althusser, the psychoanalyst Jacques Lacan, as well as the structural Marxism of Nicos Poulantzas, most of whom disavowed themselves as being a part of this movement.
The origins of structuralism connect with the work of Ferdinand de Saussure on linguistics, along with the linguistics of the Prague and Moscow schools. In brief, Saussure's structural linguistics propounded three related concepts.

Proponents of structuralism would argue that a specific domain of culture may be understood by means of a structure—modelled on language—that is distinct both from the organizations of reality and those of ideas or the imagination—the "third order". In Lacan's psychoanalytic theory, for example, the structural order of "the Symbolic" is distinguished both from "the Real" and "the Imaginary"; similarly, in Althusser's Marxist theory, the structural order of the capitalist mode of production is distinct both from the actual, real agents involved in its relations and from the ideological forms in which those relations are understood.

Blending Freud and Saussure, the French (post)structuralist Jacques Lacan applied structuralism to psychoanalysis and, in a different way, Jean Piaget applied structuralism to the study of psychology. But Jean Piaget, who would better define himself as constructivist, considers structuralism as "a method and not a doctrine" because for him "there exists no structure without a construction, abstract or genetic".

Although the French theorist Louis Althusser is often associated with a brand of structural social analysis which helped give rise to "structural Marxism", such association was contested by Althusser himself in the Italian foreword to the second edition of "Reading Capital". In this foreword Althusser states the following: 

Despite the precautions we took to distinguish ourselves from the 'structuralist' ideology ..., despite the decisive intervention of categories foreign to 'structuralism' ..., the terminology we employed was too close in many respects to the 'structuralist' terminology not to give rise to an ambiguity. With a very few exceptions ... our interpretation of Marx has generally been recognized and judged, in homage to the current fashion, as 'structuralist'... We believe that despite the terminological ambiguity, the profound tendency of our texts was not attached to the 'structuralist' ideology.
In a later development, feminist theorist Alison Assiter enumerated four ideas that she says are common to the various forms of structuralism. First, that a structure determines the position of each element of a whole. Second, that every system has a structure. Third, structural laws deal with co-existence rather than change. Fourth, structures are the "real things" that lie beneath the surface or the appearance of meaning.

In "Course in General Linguistics" the analysis focuses not on the "use" of language (called ""parole"", or speech), but rather on the underlying system of language (called ""langue""). This approach examines how the elements of language relate to each other in the present, synchronically rather than diachronically. Saussure argued that linguistic signs were composed of two parts:


This was quite different from previous approaches that focused on the relationship between words and the things in the world that they designate. Other key notions in structural linguistics include paradigm, syntagm, and value (though these notions were not fully developed in Saussure's thought). A structural "idealism" is a class of linguistic units (lexemes, morphemes or even constructions) that are possible in a certain position in a given linguistic environment (such as a given sentence), which is called the "syntagm". The different functional role of each of these members of the paradigm is called "value" ("" in French).

Saussure's "Course" influenced many linguists between World War I and World War II. In the United States, for instance, Leonard Bloomfield developed his own version of structural linguistics, as did Louis Hjelmslev in Denmark and Alf Sommerfelt in Norway. In France Antoine Meillet and Émile Benveniste continued Saussure's project, and members of the Prague school of linguistics such as Roman Jakobson and Nikolai Trubetzkoy conducted research that would be greatly influential. However, by the 1950s Saussure's linguistic concepts were under heavy criticism and were soon largely abandoned by practicing linguists: 

Saussure's views are not held, so far as I know, by modern linguists, only by literary critics and the occasional philosopher. [Strict adherence to Saussure] has elicited wrong film and literary theory on a grand scale. One can find dozens of books of literary theory bogged down in signifiers and signifieds, but only a handful that refer to Chomsky.
The clearest and most important example of Prague school structuralism lies in phonemics. Rather than simply compiling a list of which sounds occur in a language, the Prague school sought to examine how they were related. They determined that the inventory of sounds in a language could be analysed in terms of a series of contrasts. Thus in English the sounds /p/ and /b/ represent distinct phonemes because there are cases (minimal pairs) where the contrast between the two is the only difference between two distinct words (e.g. 'pat' and 'bat'). Analyzing sounds in terms of contrastive features also opens up comparative scope—it makes clear, for instance, that the difficulty Japanese speakers have differentiating /r/ and /l/ in English is because these sounds are not contrastive in Japanese. Phonology would become the paradigmatic basis for structuralism in a number of different fields.

According to structural theory in anthropology and social anthropology, meaning is produced and reproduced within a culture through various practices, phenomena and activities that serve as systems of signification. A structuralist approach may study activities as diverse as food-preparation and serving rituals, religious rites, games, literary and non-literary texts, and other forms of entertainment to discover the deep structures by which meaning is produced and reproduced within the culture. For example, Lévi-Strauss analysed in the 1950s cultural phenomena including mythology, kinship (the alliance theory and the incest taboo), and food preparation. In addition to these studies, he produced more linguistically focused writings in which he applied Saussure's distinction between "langue" and "parole" in his search for the fundamental structures of the human mind, arguing that the structures that form the "deep grammar" of society originate in the mind and operate in people unconsciously. Lévi-Strauss took inspiration from mathematics.

Another concept used in structural anthropology came from the Prague school of linguistics, where Roman Jakobson and others analysed sounds based on the presence or absence of certain features (such as voiceless vs. voiced). Lévi-Strauss included this in his conceptualization of the universal structures of the mind, which he held to operate based on pairs of binary oppositions such as hot-cold, male-female, culture-nature, cooked-raw, or marriageable vs. tabooed women.

A third influence came from Marcel Mauss (1872–1950), who had written on gift-exchange systems. Based on Mauss, for instance, Lévi-Strauss argued that kinship systems are based on the exchange of women between groups (a position known as 'alliance theory') as opposed to the 'descent'-based theory described by Edward Evans-Pritchard and Meyer Fortes. While replacing Marcel Mauss at his "Ecole Pratique des Hautes Etudes" chair, Lévi-Strauss' writing became widely popular in the 1960s and 1970s and gave rise to the term "structuralism" itself.

In Britain, authors such as Rodney Needham and Edmund Leach were highly influenced by structuralism. Authors such as Maurice Godelier and Emmanuel Terray combined Marxism with structural anthropology in France. In the United States, authors such as Marshall Sahlins and James Boon built on structuralism to provide their own analysis of human society. Structural anthropology fell out of favour in the early 1980s for a number of reasons. D'Andrade suggests that this was because it made unverifiable assumptions about the universal structures of the human mind. Authors such as Eric Wolf argued that political economy and colonialism should be at the forefront of anthropology. More generally, criticisms of structuralism by Pierre Bourdieu led to a concern with how cultural and social structures were changed by human agency and practice, a trend which Sherry Ortner has referred to as 'practice theory'.

Some anthropological theorists, however, while finding considerable fault with Lévi-Strauss's version of structuralism, did not turn away from a fundamental structural basis for human culture. The Biogenetic Structuralism group for instance argued that some kind of structural foundation for culture must exist because all humans inherit the same system of brain structures. They proposed a kind of neuroanthropology which would lay the foundations for a more complete scientific account of cultural similarity and variation by requiring an integration of cultural anthropology and neuroscience—a program that theorists such as Victor Turner also embraced.

In literary theory, structuralist criticism relates literary texts to a larger structure, which may be a particular genre, a range of intertextual connections, a model of a universal narrative structure, or a system of recurrent patterns or motifs.Structuralism argues that there must be a structure in every text, which explains why it is easier for experienced readers than for non-experienced readers to interpret a text. Hence, everything that is written seems to be governed by specific rules, or a "grammar of literature", that one learns in educational institutions and that are to be unmasked.

A potential problem of structuralist interpretation is that it can be highly reductive, as scholar Catherine Belsey puts it: "the structuralist danger of collapsing all difference." An example of such a reading might be if a student concludes the authors of "West Side Story" did not write anything "really" new, because their work has the same structure as Shakespeare's "Romeo and Juliet". In both texts a girl and a boy fall in love (a "formula" with a symbolic operator between them would be "Boy + Girl") despite the fact that they belong to two groups that hate each other ("Boy's Group - Girl's Group" or "Opposing forces") and conflict is resolved by their death. Structuralist readings focus on how the structures of the single text resolve inherent narrative tensions. If a structuralist reading focuses on multiple texts, there must be some way in which those texts unify themselves into a coherent system. The versatility of structuralism is such that a literary critic could make the same claim about a story of two "friendly" families ("Boy's Family + Girl's Family") that arrange a marriage between their children despite the fact that the children hate each other ("Boy - Girl") and then the children commit suicide to escape the arranged marriage; the justification is that the second story's structure is an 'inversion' of the first story's structure: the relationship between the values of love and the two pairs of parties involved have been reversed.

Structuralistic literary criticism argues that the "literary banter of a text" can lie only in new structure, rather than in the specifics of character development and voice in which that structure is expressed. Literary structuralism often follows the lead of Vladimir Propp, Algirdas Julien Greimas, and Claude Lévi-Strauss in seeking out basic deep elements in stories, myths, and more recently, anecdotes, which are combined in various ways to produce the many versions of the ur-story or ur-myth.

There is considerable similarity between structural literary theory and Northrop Frye's archetypal criticism, which is also indebted to the anthropological study of myths. Some critics have also tried to apply the theory to individual works, but the effort to find unique structures in individual literary works runs counter to the structuralist program and has an affinity with New Criticism.

Throughout the 1940s and 1950s, existentialism, such as that propounded by Jean-Paul Sartre, was the dominant European intellectual movement. Structuralism rose to prominence in France in the wake of existentialism, particularly in the 1960s. The initial popularity of structuralism in France led to its spread across the globe.

Structuralism rejected the concept of human freedom and choice and focused instead on the way that human experience and thus, behaviour, is determined by various structures. The most important initial work on this score was Claude Lévi-Strauss's 1949 volume "The Elementary Structures of Kinship". Lévi-Strauss had known Jakobson during their time together at the New School in New York during WWII and was influenced by both Jakobson's structuralism as well as the American anthropological tradition. In "Elementary Structures" he examined kinship systems from a structural point of view and demonstrated how apparently different social organizations were in fact different permutations of a few basic kinship structures. In the late 1950s he published "Structural Anthropology", a collection of essays outlining his program for structuralism.

By the early 1960s structuralism as a movement was coming into its own and some believed that it offered a single unified approach to human life that would embrace all disciplines. Roland Barthes and Jacques Derrida focused on how structuralism could be applied to literature.

The so-called "Gang of Four" of structuralism was Lévi-Strauss, Lacan, Barthes, and Foucault.

Structuralism is less popular today than other approaches, such as post-structuralism and deconstruction. Structuralism has often been criticized for being ahistorical and for favouring deterministic structural forces over the ability of people to act. As the political turbulence of the 1960s and 1970s (and particularly the student uprisings of May 1968) began affecting academia, issues of power and political struggle moved to the center of people's attention.

In the 1980s, deconstruction—and its emphasis on the fundamental ambiguity of language rather than its crystalline logical structure—became popular. By the end of the century structuralism was seen as an historically important school of thought, but the movements that it spawned, rather than structuralism itself, commanded attention.

Several social thinkers and academics have strongly criticized structuralism or even dismissed it "in toto". The French hermeneutic philosopher Paul Ricœur (1969) criticized Lévi-Strauss for constantly overstepping the limits of validity of the structuralist approach, ending up in what Ricœur described as "a Kantianism without a transcendental subject". Anthropologist Adam Kuper (1973) argued that "'Structuralism' came to have something of the momentum of a millennial movement and some of its adherents felt that they formed a secret society of the seeing in a world of the blind. Conversion was not just a matter of accepting a new paradigm. It was, almost, a question of salvation." Philip Noel Pettit (1975) called for an abandoning of "the positivist dream which Lévi-Strauss dreamed for semiology" arguing that semiology is not to be placed among the natural sciences. Cornelius Castoriadis (1975) criticized structuralism as failing to explain symbolic mediation in the social world; he viewed structuralism as a variation on the "logicist" theme, and he argued that, contrary to what structuralists advocate, language—and symbolic systems in general—cannot be reduced to logical organizations on the basis of the binary logic of oppositions. Critical theorist Jürgen Habermas (1985) accused structuralists, such as Foucault, of being positivists; he remarked that while Foucault is not an ordinary positivist, he nevertheless paradoxically uses the tools of science to criticize science (see "Performative contradiction" and "Foucault–Habermas debate"). Sociologist Anthony Giddens (1993) is another notable critic; while Giddens draws on a range of structuralist themes in his theorizing, he dismisses the structuralist view that the reproduction of social systems is merely "a mechanical outcome".





</doc>
<doc id="27764" url="https://en.wikipedia.org/wiki?curid=27764" title="Systems engineering">
Systems engineering

Systems engineering is an interdisciplinary field of engineering and engineering management that focuses on how to design and manage complex systems over their life cycles. At its core, systems engineering utilizes systems thinking principles to organize this body of knowledge. Issues such as requirements engineering, reliability, logistics, coordination of different teams, testing and evaluation, maintainability and many other disciplines necessary for successful system development, design, implementation, and ultimate decommission become more difficult when dealing with large or complex projects. Systems engineering deals with work-processes, optimization methods, and risk management tools in such projects. It overlaps technical and human-centered disciplines such as industrial engineering, mechanical engineering, manufacturing engineering, control engineering, software engineering, electrical engineering, cybernetics, organizational studies, civil engineering and project management. Systems engineering ensures that all likely aspects of a project or system are considered, and integrated into a whole.

The systems engineering process is a discovery process that is quite unlike a manufacturing process. A manufacturing process is focused on repetitive activities that achieve high quality outputs with minimum cost and time. The systems engineering process must begin by discovering the real problems that need to be resolved, and identify the most probable or highest impact failures that can occur – systems engineering involves finding solutions to these problems.

The term "systems engineering" can be traced back to Bell Telephone Laboratories in the 1940s. The need to identify and manipulate the properties of a system as a whole, which in complex engineering projects may greatly differ from the sum of the parts' properties, motivated various industries, especially those developing systems for the U.S. Military, to apply the discipline.

When it was no longer possible to rely on design evolution to improve upon a system and the existing tools were not sufficient to meet growing demands, new methods began to be developed that addressed the complexity directly. The continuing evolution of systems engineering comprises the development and identification of new methods and modeling techniques. These methods aid in a better comprehension of the design and developmental control of engineering systems as they grow more complex. Popular tools that are often used in the systems engineering context were developed during these times, including USL, UML, QFD, and IDEF0.

In 1990, a professional society for systems engineering, the "National Council on Systems Engineering" (NCOSE), was founded by representatives from a number of U.S. corporations and organizations. NCOSE was created to address the need for improvements in systems engineering practices and education. As a result of growing involvement from systems engineers outside of the U.S., the name of the organization was changed to the International Council on Systems Engineering (INCOSE) in 1995. Schools in several countries offer graduate programs in systems engineering, and continuing education options are also available for practicing engineers.

Systems engineering signifies only an approach and, more recently, a discipline in engineering. The aim of education in systems engineering is to formalize various approaches simply and in doing so, identify new methods and research opportunities similar to that which occurs in other fields of engineering. As an approach, systems engineering is holistic and interdisciplinary in flavour.

The traditional scope of engineering embraces the conception, design, development, production and operation of physical systems. Systems engineering, as originally conceived, falls within this scope. "Systems engineering", in this sense of the term, refers to the distinctive set of concepts, methodologies, organizational structures (and so on) that have been developed to meet the challenges of engineering effective functional systems of unprecedented size and complexity within time, budget, and other constraints. The Apollo program is a leading example of a systems engineering project.

The use of the term "systems engineer" has evolved over time to embrace a wider, more holistic concept of "systems" and of engineering processes. This evolution of the definition has been a subject of ongoing controversy, and the term continues to apply to both the narrower and broader scope.

Traditional systems engineering was seen as a branch of engineering in the classical sense, that is, as applied only to physical systems, such as spacecraft and aircraft. More recently, systems engineering has evolved to a take on a broader meaning especially when humans were seen as an essential component of a system. Checkland, for example, captures the broader meaning of systems engineering by stating that 'engineering' "can be read in its general sense; you can engineer a meeting or a political agreement."

Consistent with the broader scope of systems engineering, the Systems Engineering Body of Knowledge (SEBoK) has defined three types of systems engineering: (1) Product Systems Engineering (PSE) is the traditional systems engineering focused on the design of physical systems consisting of hardware and software. (2) Enterprise Systems Engineering (ESE) pertains to the view of enterprises, that is, organizations or combinations of organizations, as systems. (3) Service Systems Engineering (SSE) has to do with the engineering of service systems. Checkland defines a service system as a system which is conceived as serving another system. Most civil infrastructure systems are service systems.

Systems engineering focuses on analyzing and eliciting customer needs and required functionality early in the development cycle, documenting requirements, then proceeding with design synthesis and system validation while considering the complete problem, the system lifecycle. This includes fully understanding all of the stakeholders involved. Oliver et al. claim that the systems engineering process can be decomposed into
Within Oliver's model, the goal of the Management Process is to organize the technical effort in the lifecycle, while the Technical Process includes "assessing available information", "defining effectiveness measures", to "create a behavior model", "create a structure model", "perform trade-off analysis", and "create sequential build & test plan".

Depending on their application, although there are several models that are used in the industry, all of them aim to identify the relation between the various stages mentioned above and incorporate feedback. Examples of such models include the Waterfall model and the VEE model.

System development often requires contribution from diverse technical disciplines. By providing a systems (holistic) view of the development effort, systems engineering helps mold all the technical contributors into a unified team effort, forming a structured development process that proceeds from concept to production to operation and, in some cases, to termination and disposal. In an acquisition, the holistic integrative discipline combines contributions and balances tradeoffs among cost, schedule, and performance while maintaining an acceptable level of risk covering the entire life cycle of the item.

This perspective is often replicated in educational programs, in that systems engineering courses are taught by faculty from other engineering departments, which helps create an interdisciplinary environment.

The need for systems engineering arose with the increase in complexity of systems and projects, in turn exponentially increasing the possibility of component friction, and therefore the unreliability of the design. When speaking in this context, complexity incorporates not only engineering systems, but also the logical human organization of data. At the same time, a system can become more complex due to an increase in size as well as with an increase in the amount of data, variables, or the number of fields that are involved in the design. The International Space Station is an example of such a system.

The development of smarter control algorithms, microprocessor design, and analysis of environmental systems also come within the purview of systems engineering. Systems engineering encourages the use of tools and methods to better comprehend and manage complexity in systems. Some examples of these tools can be seen here:

Taking an interdisciplinary approach to engineering systems is inherently complex since the behavior of and interaction among system components is not always immediately well defined or understood. Defining and characterizing such systems and subsystems and the interactions among them is one of the goals of systems engineering. In doing so, the gap that exists between informal requirements from users, operators, marketing organizations, and technical specifications is successfully bridged.

One way to understand the motivation behind systems engineering is to see it as a method, or practice, to identify and improve common rules that exist within a wide variety of systems. Keeping this in mind, the principles of systems engineering – holism, emergent behavior, boundary, et al. – can be applied to any system, complex or otherwise, provided systems thinking is employed at all levels. Besides defense and aerospace, many information and technology based companies, software development firms, and industries in the field of electronics & communications require systems engineers as part of their team.

An analysis by the INCOSE Systems Engineering center of excellence (SECOE) indicates that optimal effort spent on systems engineering is about 15-20% of the total project effort. At the same time, studies have shown that systems engineering essentially leads to reduction in costs among other benefits. However, no quantitative survey at a larger scale encompassing a wide variety of industries has been conducted until recently. Such studies are underway to determine the effectiveness and quantify the benefits of systems engineering.

Systems engineering encourages the use of modeling and simulation to validate assumptions or theories on systems and the interactions within them.

Use of methods that allow early detection of possible failures, in safety engineering, are integrated into the design process. At the same time, decisions made at the beginning of a project whose consequences are not clearly understood can have enormous implications later in the life of a system, and it is the task of the modern systems engineer to explore these issues and make critical decisions. No method guarantees today's decisions will still be valid when a system goes into service years or decades after first conceived. However, there are techniques that support the process of systems engineering. Examples include soft systems methodology, Jay Wright Forrester's System dynamics method, and the Unified Modeling Language (UML)—all currently being explored, evaluated, and developed to support the engineering decision process.

Education in systems engineering is often seen as an extension to the regular engineering courses, reflecting the industry attitude that engineering students need a foundational background in one of the traditional engineering disciplines (e.g., aerospace engineering, civil engineering, electrical engineering, mechanical engineering, manufacturing engineering, industrial engineering)—plus practical, real-world experience to be effective as systems engineers. Undergraduate university programs in systems engineering are rare. Typically, systems engineering is offered at the graduate level in combination with interdisciplinary study.

INCOSE maintains a continuously updated Directory of Systems Engineering Academic Programs worldwide. As of 2009, there are about 80 institutions in United States that offer 165 undergraduate and graduate programs in systems engineering. Education in systems engineering can be taken as "Systems-centric" or "Domain-centric".
Both of these patterns strive to educate the systems engineer who is able to oversee interdisciplinary projects with the depth required of a core-engineer.

Systems engineering tools are strategies, procedures, and techniques that aid in performing systems engineering on a project or product. The purpose of these tools vary from database management, graphical browsing, simulation, and reasoning, to document production, neutral import/export and more.

There are many definitions of what a system is in the field of systems engineering. Below are a few authoritative definitions:


Depending on their application, tools are used for various stages of the systems engineering process:

Models play important and diverse roles in systems engineering. A model can be defined in several
ways, including:
Together, these definitions are broad enough to encompass physical engineering models used in the verification of a system design, as well as schematic models like a functional flow block diagram and mathematical (i.e., quantitative) models used in the trade study process. This section focuses on the last.

The main reason for using mathematical models and diagrams in trade studies is to provide estimates of system effectiveness, performance or technical attributes, and cost from a set of known or estimable quantities. Typically, a collection of separate models is needed to provide all of these outcome variables. The heart of any mathematical model is a set of meaningful quantitative relationships among its inputs and outputs. These relationships can be as simple as adding up constituent quantities to obtain a total, or as complex as a set of differential equations describing the trajectory of a spacecraft in a gravitational field. Ideally, the relationships express causality, not just correlation. Furthermore, key to successful systems engineering activities are also the methods with which these models are efficiently and effectively managed and used to simulate the systems. However, diverse domains often present recurring problems of modeling and simulation for systems engineering, and new advancements are aiming to crossfertilize methods among distinct scientific and engineering communities, under the title of 'Modeling & Simulation-based Systems Engineering'.

Initially, when the primary purpose of a systems engineer is to comprehend a complex problem, graphic representations of a system are used to communicate a system's functional and data requirements. Common graphical representations include:

A graphical representation relates the various subsystems or parts of a system through functions, data, or interfaces. Any or each of the above methods are used in an industry based on its requirements. For instance, the N2 chart may be used where interfaces between systems is important. Part of the design phase is to create structural and behavioral models of the system.

Once the requirements are understood, it is now the responsibility of a systems engineer to refine them, and to determine, along with other engineers, the best technology for a job. At this point starting with a trade study, systems engineering encourages the use of weighted choices to determine the best option. A decision matrix, or Pugh method, is one way (QFD is another) to make this choice while considering all criteria that are important. The trade study in turn informs the design, which again affects graphic representations of the system (without changing the requirements). In an SE process, this stage represents the iterative step that is carried out until a feasible solution is found. A decision matrix is often populated using techniques such as statistical analysis, reliability analysis, system dynamics (feedback control), and optimization methods.

Systems Modeling Language (SysML), a modeling language used for systems engineering applications, supports the specification, analysis, design, verification and validation of a broad range of complex systems.

Lifecycle Modeling Language (LML), is an open-standard modeling language designed for systems engineering that supports the full lifecycle: conceptual, utilization, support and retirement stages.

Many related fields may be considered tightly coupled to systems engineering. The following areas have contributed to the development of systems engineering as a distinct entity:






</doc>
<doc id="27765" url="https://en.wikipedia.org/wiki?curid=27765" title="September 4">
September 4






</doc>
<doc id="27766" url="https://en.wikipedia.org/wiki?curid=27766" title="Sam &amp; Max">
Sam &amp; Max

Sam & Max is a media franchise focusing on the titular fictional characters, the Freelance Police. The characters, who occupy a universe that parodies American popular culture, were created by Steve Purcell in his youth, and later debuted in a 1987 comic book series. The characters have since been the subject of a graphic adventure video game developed by LucasArts, a produced for Fox in cooperation with Nelvana Limited, and a series of episodic adventure games developed by Telltale Games. In addition, a variety of machinima and a webcomic have been produced for the series.

The characters are a pair of anthropomorphic, vigilante private investigators based in a dilapidated office block in New York City. Sam is a six-foot-tall dog who wears a suit and a fedora, while Max is a short and aggressive "hyperkinetic rabbity thing". Both enjoy solving problems and cases as maniacally as possible, often with complete disregard for the law. Driving a seemingly indestructible black-and-white 1960 DeSoto Adventurer, the pair travel to many contemporary and historical locations to fight crime, including the Moon, Ancient Egypt, the White House and the Philippines, as well as several fictitious locations.

The series has been very successful despite its relatively limited amount of media, and has gathered a significant fan base. However, the franchise did not gain more widespread recognition until after the 1993 release of LucasArts' "Sam & Max Hit the Road", which cultivated interest in Purcell's original comics. "Sam & Max Hit the Road" is regarded as an exceptional adventure game and an iconic classic of computer gaming in the 1990s. Subsequent video games and the television series have also fared well with both critics and fans; critics consider the episodic video games to be the first successful application of the episodic distribution model.

The idea of "Sam & Max" originated with Steve Purcell's younger brother, Dave, who invented the concept of a comic about a detective team consisting of a dog and a rabbit in his youth. Dave would often leave the comics around the house, so Steve, in a case of sibling rivalry, often finished the incomplete stories in parodies of their original form, deliberately making the characters mix up each other's names, over-explain things, shoot at each other and mock the way in which they had been drawn, as "kind of a parody of the way a kid talks when he's writing comics". Over time, this developed from Steve merely mocking his brother's work to him creating his own full stories with the characters. Ultimately, in the late 1970s, Dave Purcell gave Steve the rights to the characters, signing them over in a contract on Steve's birthday and allowing him to develop the characters in his own way. In 1980, Purcell began to produce "Sam & Max" comic strips for the weekly newsletter of the California College of Arts and Crafts. While the visual appearance of the characters had not yet been fully developed, the stories were similar in style to those that would follow when Purcell was offered by "Fish Police" author Steven Moncuse the chance to publish his work properly in 1987.

Many aspects of the "Sam & Max" comics were influenced by Purcell's own experiences. Rats and cockroaches are common throughout the franchise, the former inspired by Purcell's pet rat. In another example, Sam and Max are occasionally shown playing a game called "fizzball", in which the object of the exercise is to hit a can of beer in mid-air with a solid axe handle. Purcell had previously invented the game with his friends, including fellow comic book writers Art Adams and Mike Mignola.

Sam is a laid-back but enthusiastic, brown-coated anthropomorphic Irish Wolfhound, described as a "canine shamus". He wears either a gray or blue suit with a matching fedora, to make people more cooperative when conversing with a six-foot talking dog. A warped sense of justice makes Sam the more passionate of the pair for their police work, only held back from taking his job seriously by Max. Nevertheless, he enjoys the mannerisms and dress that come with their line of work. Sam possesses near encyclopedic amounts of knowledge, particularly on obscure topics, and is prone to long-winded sentences filled with elaborate terminology. Although he is always keen to display this information—regardless of its accuracy—Sam can be capable of total ignorance towards more practical matters; for instance, despite his regard for his DeSoto Adventurer, he is severely negligent with the car's maintenance. Sam still retains various doglike qualities: he is excitable, enthusiastic but also susceptible to emotions of embarrassment and guilt. Nevertheless, Sam is "not above sticking his head out the car window and letting his tongue flap in the breeze". Sam rarely loses his temper, and is able to react to panic-inducing situations with extreme calm. When he does get angry, Sam tends to react in a violent, uncharacteristically savage manner, in which case it is usually Max that calms him down and prevents him from acting upon his anger. Sam usually is armed with an oversized .44 revolver.

Max is an anthropomorphic "hyperkinetic, three-foot rabbity thing" with white fur, but prefers being called a lagomorph. Max retains few characteristics consistent with a rabbit, with permanently rigid ears set in an excited posture and a huge jaw normally stuck in a crazed grin. Unhinged, uninhibited and near psychotic, Max enjoys violence and tends to prefer the aggressive way of solving problems, seeing the world as little more than a vessel for his "pinball-like stream of consciousness". This creates a seeming disregard for self-preservation; Max will revel in dangerous situations with little impression that he understands the risks he faces. As a result, Max is usually enthusiastic to engage in any activity, including being used by Sam as a cable cutter or an impromptu bludgeon. Despite this, Max possesses a sharp mind and an observational nature, and enjoys interpreting new experiences in as unpredictable manner as possible. However, Max has a distaste for long stories and occasionally loses focus during lengthy scenes of plot exposition; by his own admission, Max possesses a particularly short attention span. Despite his seemingly heartless personality, he believes strongly in protecting Sam. However, Max can still act violently towards his friend, stating that when he dies he will take Sam with him. Moreover, Max is extremely possessive of Sam and their status as partners and best friends. Max traditionally carries a Luger pistol, but as he wears no clothes, other characters often make comments as to where Max keeps it on his person. Purcell considers Max to be representative of pure id, the uncoordinated instinctual trends of the human psyche.

Sam and Max debuted in the 1987 comic book series "Sam & Max: Freelance Police", published by Fishwrap Productions, also the publisher of "Fish Police". The first comic, "Monkeys Violating the Heavenly Temple", was Steve Purcell's first full story. The comic came about after Purcell agreed to create a full "Sam & Max" story for publication alongside Steve Moncuse's "Fish Police" series. "Monkeys Violating the Heavenly Temple" established many of the key features in the series; the main story of the comic saw the Freelance Police journey to the Philippines to stop a volcano god cult. "Night of the Gilded Heron-Shark" and "Night of the Cringing Wildebeest" accompanied the main story, focusing on a stand-off with a group of gangsters in Sam and Max's office and an investigation into a carnival refreshment booth respectively.

Over the subsequent years, several other comics were published, often by different publishers, including and Epic Comics. "Fair Wind to Java" was originally published in 1988 as a Munden's Bar story in the pages of First Comics' "Grimjack", featuring the Freelance Police fighting pyramid-building aliens in Ancient Egypt, and was followed in 1989 by "On the Road", a three chapter story showing what Sam and Max do on vacation. In 1990, a Christmas themed story, "The Damned Don't Dance" was released. 1992 saw the release of a further two comics; "Bad Day On The Moon" took the Freelance Police to deal with a roach infestation bothering giant rats on the Moon, and was later adapted as a story for the animated TV series, whilst "Beast From The Cereal Aisle" focused on the duo conducting an exorcism at the local supermarket. Two more comics were produced in 1997, "The Kids Take Over" and "Belly Of The Beast". The former has Sam and Max wake up from cryogenic sleep to discover that the entire world is now ruled by children while the latter sees the Freelance Police confronting a vampire abducting children at Halloween.

Purcell joined LucasArts in 1988 as an artist and game designer, where he was approached about contributing to LucasArts' new quarterly newsletter, "The Adventurer", a publication designed to inform customers about upcoming LucasArts games and company news. From its debut issue in 1990 to 1996, Purcell created twelve comic strips for the newsletter. The strips portrayed a variety of stories, from similar plots as in the comic books to parodies of LucasArts games such as "Monkey Island" and "Full Throttle" and the Lucasfilm franchises "Star Wars" and "Indiana Jones".

In 1995, all of the comics and "The Adventurer" strips published to that date were released in a compilation, "Sam & Max: Surfin' the Highway". Published by Marlowe & Company, the 154 page book was updated and republished in 1996. This original version of "Surfin' the Highway" went out of print in 1997, becoming a high priced collectors item sold through services such as eBay. In 2007, a 197-page twenty-year anniversary edition, containing all printed comics and strips as well as a variety of other artwork, was co-designed by Steve Purcell and Jake Rodkin and published by Telltale Games. This second publication received an Eisner Award nomination for "Best Graphic Album – Reprint" in 2009.

In December 2005, Purcell started a "Sam & Max" webcomic, hosted on the website of Telltale Games. Entitled "The Big Sleep", the webcomic began with Sam and Max bursting out of their graves at Kilpeck Church in England, symbolizing the Freelance Police's return after nearly a decade. In the twelve page story, Max has to save Sam after earwigs start a colony in Sam's brain. The webcomic concluded in April 2007, and was later awarded the Eisner Award for "Best Digital Comic" of 2007.

Following LucasArts' employment of Purcell in 1988, the characters of Sam and Max appeared in internal testing material for new SCUMM engine programmers; Purcell created animated versions of the characters and an office backdrop for the programmers to practice on. In 1992, LucasArts offered Purcell the chance to create a video game out of the characters, out of a wish to use new characters after the success of its two other main adventure titles, "Monkey Island" and "Maniac Mansion", and after a positive reaction from fans to the "Sam & Max" comic strips featured in LucasArts' "The Adventurer" newsletter. Consequently, development on a graphic adventure game, "Sam & Max Hit the Road", began shortly after. Based on the SCUMM engine and designed by Sean Clark, Michael Stemmle, Steve Purcell and his future wife Collette Michaud, the game was partially based on the 1989 comic "On The Road", and featured the Freelance Police travelling across America in search of an escaped bigfoot. Sam was voiced in the game by comedian Bill Farmer, while actor Nick Jameson voiced Max. "Sam & Max Hit the Road" was originally released for DOS in November 1993. Soon after "Sam & Max Hit the Road", another "Sam & Max" game using SCUMM entered planning under Purcell and Dave Grossman, but was abandoned. In a later interview Grossman described this sequel's highlight as "a giant spaceship shaped like Max's head".

In September 2001 development began on a new project, "Sam & Max Plunge Through Space". The game was to be an Xbox exclusive title, developed by Infinite Machine, a small company consisting of a number of former LucasArts employees. The story of the game was developed by Purcell and fellow designer Chuck Jordan and involved the Freelance Police travelling the galaxy to find a stolen Statue of Liberty. However, Infinite Machine went bankrupt within a year, partially due to the failure of their first game, "New Legends", and the project was abandoned.

At the 2002 Electronic Entertainment Expo convention, nearly a decade after the release of "Sam & Max Hit the Road", LucasArts announced the production of a PC sequel, entitled "Sam & Max: Freelance Police". "Freelance Police", like "Hit the Road", was to be a point-and-click graphic adventure game, using a new 3D game engine. Development of "Freelance Police" was led by Michael Stemmle. Steve Purcell contributed to the project by writing the story and producing concept art. Farmer and Jameson were also set to reprise their voice acting roles. In March 2004, however, quite far into the game's development, "Sam & Max: Freelance Police" was abruptly cancelled by LucasArts, citing "current market place realities and underlying economic considerations" in a short press release. The fan reaction to the cancellation was strong; a petition of 32,000 signatures stating the disappointment of fans was later presented to LucasArts.

After LucasArts' license with Steve Purcell expired in 2005, the "Sam & Max" franchise moved to Telltale Games, a company of former LucasArts employees who had worked on a number of LucasArts adventure games, including on the development of "Freelance Police". Under Telltale Games, a new episodic series of "Sam & Max" video games was announced. Like both "Sam & Max Hit the Road" and "Freelance Police", "Sam & Max Save the World" was in a point-and-click graphic adventure game format. The game used a new 3D game engine, different from the one used in "Freelance Police". The first season ran for six episodes, each with a self-contained storyline but with an overall story arc involving hypnotism running through the series. The first episode was released on GameTap in October 2006, with episodes following regularly until April 2007. Sam is voiced by David Nowlin, while Max is voiced by William Kasten in all episodes except the first one, where Andrew Chaikin voices the character. In addition, Telltale Games produced fifteen machinima shorts to accompany the main episodes. These shorts were released in groups of three in between the release of each episode, showing the activities of the Freelance Police in between each story.

A second season of episodic video games developed by Telltale Games was announced in July 2007. "Sam & Max Beyond Time and Space" followed the same overall format as "Save the World", with each episode having an overarching storyline involving time travel and laundering of the souls of the dead. As with "Save the World", episodes were originally published on GameTap before being made available for general release. The season consisted of five episodes and ran from November 2007 to April 2008. Nowlin and Kasten both returned to reprise their voice roles. In addition to the main games, a twenty-minute machinima video was produced, taking the form of a "Sam & Max" Christmas special.

A third game entitled "Sam & Max: The Devil's Playhouse" was confirmed in May 2008 for release in 2009; the title was later pushed back to 2010, with concept art emerging after Telltale's completion of "Tales of Monkey Island". The season again ran for five episodes, released monthly from April to August 2010. "The Devil's Playhouse" followed a structure similar to "'Tales of Monkey Island", with each episode forming a part of an ongoing narrative, involving psychic powers and forces that would use them for world domination. A two-minute Flash cartoon also accompanied the game, dealing with the origin story of General Skun-ka'pe, one of the game's antagonists. Max also appears in Telltale's 2010 casual game "Poker Night at the Inventory" alongside Tycho Brahe from "Penny Arcade", the Heavy from "Team Fortress 2" and Strong Bad from "Homestar Runner". Sam and Max (now voiced by Dave Boat) also appear in the game's sequel alongside Claptrap from "Borderlands", Brock Samson from "The Venture Bros.", Ash Williams from "Evil Dead" and GLaDOS from "Portal".

"Sam & Max" were adapted into a cartoon series for Fox in 1997. Produced by Canadian studio Nelvana, the series ran for 24 episodes. Each episode was approximately ten minutes, and were often aired in pairs. Broadcast on Fox Kids in the United States, YTV in Canada, and Channel 4 in the United Kingdom, the first episode was aired on October 4, 1997; the series concluded on April 25, 1998. As opposed to the more adult humor in the rest of the series, "The Adventures of Sam & Max: Freelance Police" was aimed more at children, even though some humor in it was often directed at adults. As such, the violence inherent in the franchise is toned down, including removing Sam and Max's guns, and the characters do not use the moderate profanity that they use in their other appearances. As in most "Sam & Max" stories, the series revolves around the Freelance Police accepting missions from their mysterious superior, the commissioner, and embarking on cases to a large variety of implausible locations. Sam is voiced by Harvey Atkin, while Max is voiced by Robert Tinkler. The series performed well and was considered a success, and in 1998 received the Gemini Award for "Best Animated Program or Series". Despite the series' success, a second series was never commissioned. In June 2007, it was reported that Shout! Factory were preparing a DVD release of the series. In October 2007, as part of their marketing for "Sam & Max Save the World", GameTap hosted the series on their website. The DVD release of the series was later published in March 2008.

The "Sam & Max" franchise features a variety of soundtracks that accompany its video game products. This music is mostly grounded in film noir jazz, incorporating various other styles at certain points, such as Dixieland, waltz and mariachi, usually to support the cartoon nature of the series. The first "Sam & Max" game, "Sam & Max Hit the Road", was one of the first games to feature a fully scored music soundtrack, written by LucasArts' composers Clint Bajakian, Michael Land and Peter McConnell. The music was incorporated into the game using Land and McConnell's iMUSE engine, which allowed for audio to be synchronized with the visuals. Although the full soundtrack was never released, audio renders of four of the game's MIDI tracks were included on the CD version of the game.

For "Sam & Max Save the World", "Beyond Time and Space", and "The Devil's Playhouse", Telltale Games contracted composer Jared Emerson-Johnson, a musician whose previous work included composition and sound editing for LucasArts, to write the scores. The soundtracks for the first two games were released in two disc sets after the release of the games themselves; the "Season One Soundtrack" was published in July 2007, whilst the "Season Two Soundtrack" was released in September 2008. Emerson-Johnson's scores use live performances as opposed to synthesized music often used elsewhere in the video games industry. Critics reacted positively to Emerson-Johnson's scores, IGN described Emerson-Johnson's work as a "breath of fresh air", while 1UP.com praised his work as "top-caliber" and Music4Games stated that the "whimsical nature of [the classical jazz approach] is well suited to the "Sam & Max" universe, which approaches American popular culture with a level of irreverence". Purcell later commented that Emerson-Johnson had seamlessly blended a "huge palette of genres and styles", whilst in September 2008, Brendan Q. Ferguson, one of the lead designers on "Save the World" and "Beyond Time and Space", stated that he believed that it was Emerson-Johnson's scores that created the vital atmosphere in the games, noting that prior to the implementation of the soundtracks, playing the games was an "unrelenting horror".

The "Sam & Max" franchise has been highly successful critically, and is considered an iconic and influential aspect of the video game industry in the 1990s and the adventure game genre. In 2007, Steve Purcell wrote that he was somewhat surprised at the success of his creation, noting that the series had gained a large fan gathering despite the small size of the franchise. As the series contains only a small amount of comics, video games and a short TV series, Purcell commented that there was "certainly not enough material to build that relentless traction of an endlessly renewed sitcom or a syndicated comic that has existed since the Korean Conflict". The comics were well received by critics, many praising the humor and style of the stories and characters. However, later commentators have noted that the comic book series did not gain much popularity or recognition until after the release of "Sam & Max Hit the Road" in 1993; the later episodic video games are seen to have revived interest in the comics again, resulting in the creation of the webcomic "The Big Sleep" and publication of an anniversary edition of "Surfin' The Highway".

Upon its release in 1993, "Sam & Max Hit the Road" was met with near universal acclaim. Critics praised the title for its humor, voice acting, graphics, music and gameplay. It has since come to be regarded as a classic graphic adventure game, one of the most critically successful projects by LucasArts to date. "Sam & Max Hit the Road" is regularly featured in lists of top games, and was nominated for the 1994 Annie Award for "Best Animated CD-ROM", although the award instead went to LucasArts' "". The abrupt cancellation of the sequel to "Sam & Max Hit the Road" in 2004 garnered substantial criticism of LucasArts. In addition to a petition of 32,000 signatures objecting to the termination of development on "Sam & Max: Freelance Police", both Steve Purcell and the media were critical of LucasArts' decision. Purcell stated that he failed to understand quite why the game was cancelled, as he believed the development of the game was proceeding without hindrance, while the media put forward the view that LucasArts was moving to consolidate its position with low business risk "Star Wars" video games instead of pursuing the adventure games that had brought them success in earlier years. The cancellation of "Freelance Police" is often cited as the culmination in a perceived decline in the overall adventure game genre, and LucasArts later dismissed many of the designers involved with developing their adventure games, effectively ending their adventure game era.

Although "Sam & Max Save the World" did not receive the critical acclaim that "Sam & Max Hit the Road" acquired, it still received a favorable response from critics across its release in 2006 and 2007. Critics praised the game's humor, graphics and gameplay, although concerns were voiced over the low difficulty of the puzzles and the effectiveness of the story. "Save the World" is considered by journalists in the video game industry to be the first successful application of episodic gaming, as Telltale Games had managed to release a steady stream content with only small time gaps. Previous attempts by Valve Software with the "Half-Life" series, Ritual Entertainment with "SiN Episodes" and Telltale Games themselves with "" were for a variety of reasons not considered successful implementations of the distribution model. "Beyond Time and Space" was considered similar to "Save the World" and reviewers equally praised and faulted the game on this, although overall "Beyond Time and Space" received a good reception from critics.

The success of the franchise has spawned a selection of merchandise, including posters and prints, items of clothing and sketchbooks of Purcell's work during various stages of the series' development. Collectable statues of the characters have also been created. However, despite references in Purcell's sketchbooks and demand from both fans and journalists alike, plush toys of the characters have not been produced. However, a plush toy of Max has been created and sold, albeit limited edition, as a collaboration between internet shop Hashtag Collectibles and Steve Purcell.



</doc>
<doc id="27767" url="https://en.wikipedia.org/wiki?curid=27767" title="Standard-definition television">
Standard-definition television

Standard-definition television (SDTV or SD) is a television system which uses a resolution that is not considered to be either high- or enhanced-definition. The two common SDTV signal types are 576i, with 576 interlaced lines of resolution, derived from the European-developed PAL and SECAM systems; and 480i based on the American NTSC system. SDTV and high definition television (HDTV) are the two categories of display formats for digital television (DTV) transmissions.

In North America, digital SDTV is broadcast in the same aspect ratio as NTSC signals with widescreen content being center cut. However, in other parts of the world that used the PAL or SECAM color systems, standard-definition television is now usually shown with a aspect ratio, with the transition occurring between the mid-1990s and mid-2000s. Older programs with a 4:3 aspect ratio are broadcast with a flag that switches the display to 4:3.

Standards that support digital SDTV broadcast include DVB, ATSC, and ISDB. The last two were originally developed for HDTV, but are more often used for their ability to deliver multiple SD video and audio streams via multiplexing, than for using the entire bitstream for one HD channel.

For SMPTE 259M-C compliance, a SDTV broadcast image is scaled to 720 pixels wide for every 480 NTSC (or 576 PAL) lines of the image with the amount of non-proportional line scaling dependent on either the display or pixel aspect ratio. The display ratio for broadcast widescreen is commonly 16:9, the display ratio for a traditional or letterboxed broadcast is 4:3.

An SDTV image outside the constraints of the SMPTE standards requires no non-proportional scaling with 640 pixels for every line of the image. The display and pixel aspect ratio is generally not required with the line height defining the aspect. For widescreen 16:9, 360 lines define a widescreen image and for traditional 4:3, 480 lines define an image.

SDTV refresh rates can be 24, 25, 30, 50 or 60 frames per second with a possible rate multiplier of 1000/1001 for NTSC. 50 and 60 rates are generally frame doubled versions of 25 and 30 rates for jitter issues when using non-interlaced lines.

Digital SDTV in 4:3 aspect ratio has the same appearance as regular analog TV (NTSC, PAL, SECAM) without the ghosting, snowy images and white noise. However, if the reception has interference or is poor, where the error correction cannot compensate one will encounter various other artifacts such as image freezing, stuttering or dropouts from missing intra-frames or blockiness from missing macroblocks. The audio encoding is the last to suffer loss due to the lower bandwidth requirements.

Television signals are transmitted in digitally encoded form, and the lines are scaled to fit SMPTE SDI bandwidth requirements, as opposed to unrestricted uses such as when lines are rendered or overlaid to a modern computer monitor and modern SMPTE implementations of HDTV. The table below summarizes pixel aspect ratios for the scaling of various kinds of SDTV video lines. Note that the actual image (be it 4:3 or 16:9) is always contained in the center 704 horizontal pixels of the digital frame, regardless of how many horizontal pixels (704 or 720) are used. In case of digital video line having 720 horizontal pixels, only the center 704 pixels contain actual 4:3 or 16:9 image, and the 8 pixel wide stripes from either side are called nominal analogue blanking for horizontal blanking and should be discarded before displaying the image. Nominal analogue blanking should not be confused with overscan, as overscan areas are part of the actual 4:3 or 16:9 image.

The pixel aspect ratio is always the same for corresponding 720 and 704 pixel resolutions because the center part of a 720 pixels wide image is equal to the corresponding 704 pixels wide image.




</doc>
<doc id="27772" url="https://en.wikipedia.org/wiki?curid=27772" title="Sandstone">
Sandstone

Sandstone is a clastic sedimentary rock composed mainly of sand-sized (0.0625 to 2 mm) mineral particles or rock fragments.

Most sandstone is composed of quartz or feldspar (both silicates) because they are the most resistant minerals to weathering processes at the Earth's surface, as seen in Bowen's reaction series. Like uncemented sand, sandstone may be any color due to impurities within the minerals, but the most common colors are tan, brown, yellow, red, grey, pink, white, and black. Since sandstone beds often form highly visible cliffs and other topographic features, certain colors of sandstone have been strongly identified with certain regions.

Rock formations that are primarily composed of sandstone usually allow the percolation of water and other fluids and are porous enough to store large quantities, making them valuable aquifers and petroleum reservoirs. Fine-grained aquifers, such as sandstones, are better able to filter out pollutants from the surface than are rocks with cracks and crevices, such as limestone or other rocks fractured by seismic activity.

Quartz-bearing sandstone can be converted into quartzite through metamorphism, usually related to tectonic compression within orogenic belts.

Sandstones are "clastic" in origin (as opposed to either "organic", like chalk and coal, or "chemical", like gypsum and jasper).
They are formed from cemented grains that may either be fragments of a pre-existing rock or be mono-minerallic crystals. The cements binding these grains together are typically calcite, clays, and silica. Grain sizes in sands are defined (in geology) within the range of 0.0625 mm to 2 mm (0.002–0.079 inches). Clays and sediments with smaller grain sizes not visible with the naked eye, including siltstones and shales, are typically called "argillaceous" sediments; rocks with larger grain sizes, including breccias and conglomerates, are termed "rudaceous" sediments.

The formation of sandstone involves two principal stages. First, a layer or layers of sand accumulates as the result of sedimentation, either from water (as in a stream, lake, or sea) or from air (as in a desert). Typically, sedimentation occurs by the sand settling out from suspension; i.e., ceasing to be rolled or bounced along the bottom of a body of water or ground surface (e.g., in a desert or erg). Finally, once it has accumulated, the sand becomes sandstone when it is compacted by the pressure of overlying deposits and cemented by the precipitation of minerals within the pore spaces between sand grains.

The most common cementing materials are silica and calcium carbonate, which are often derived either from dissolution or from alteration of the sand after it was buried. Colours will usually be tan or yellow (from a blend of the clear quartz with the dark amber feldspar content of the sand). A predominant additional colourant in the southwestern United States is iron oxide, which imparts reddish tints ranging from pink to dark red (terracotta), with additional manganese imparting a purplish hue. Red sandstones are also seen in the Southwest and West of Britain, as well as central Europe and Mongolia. The regularity of the latter favours use as a source for masonry, either as a primary building material or as a facing stone, over other construction.

The environment where it is deposited is crucial in determining the characteristics of the resulting sandstone, which, in finer detail, include its "grain size", "sorting", and "composition" and, in more general detail, include the rock geometry and sedimentary structures. Principal environments of deposition may be split between terrestrial and marine, as illustrated by the following broad groupings:

Framework grains are sand-sized ( diameter) detrital fragments that make up the bulk of a sandstone. These grains can be classified into several different categories based on their mineral composition:


Matrix is very fine material, which is present within interstitial pore space between the framework grains. The interstitial pore space can be classified into two varieties. One is to call the sandstone an arenite, and the other is to call it a wacke. Below is a definition of the differences between the two matrices:

Cement is what binds the siliciclastic framework grains together. Cement is a secondary mineral that forms after deposition and during burial of the sandstone. These cementing materials may be either silicate minerals or non-silicate minerals, such as calcite.

Pore space includes the open spaces within a rock or a soil. The pore space in a rock has a direct relationship to the porosity and permeability of the rock. The porosity and permeability are directly influenced by the way the sand grains are packed together.

All sandstones are composed of the same general minerals. These minerals make up the framework components of the sandstones. Such components are quartz, feldspars, and lithic fragments. Matrix may also be present in the interstitial spaces between the framework grains. Below is a list of several major groups of sandstones. These groups are divided based on mineralogy and texture. Even though sandstones have very simple compositions which are based on framework grains, geologists have not been able to agree on a specific, right way, to classify sandstones. Sandstone classifications are typically done by point-counting a thin section using a method like the Gazzi-Dickinson Method. The composition of a sandstone can have important information regarding the genesis of the sediment when used with a triangular "Q"uartz, "F"eldspar, "L"ithic fragment (QFL diagrams). Many geologists, however, do not agree on how to separate the triangle parts into the single components so that the framework grains can be plotted. Therefore, there have been many published ways to classify sandstones, all of which are similar in their general format.

Visual aids are diagrams that allow geologists to interpret different characteristics about a sandstone. The following QFL chart and the sandstone provenance model correspond with each other therefore, when the QFL chart is plotted those points can then be plotted on the sandstone provenance model. The stage of textural maturity chart illustrates the different stages that a sandstone goes through.

Dott's (1964) sandstone classification scheme is one of many such schemes used by geologists for classifying sandstones. Dott's scheme is a modification of Gilbert's classification of silicate sandstones, and it incorporates R.L. Folk's dual textural and compositional maturity concepts into one classification system. The philosophy behind combining Gilbert's and R. L. Folk's schemes is that it is better able to "portray the continuous nature of textural variation from mudstone to arenite and from stable to unstable grain composition". Dott's classification scheme is based on the mineralogy of framework grains, and on the type of matrix present in between the framework grains.

In this specific classification scheme, Dott has set the boundary between arenite and wackes at 15% matrix. In addition, Dott also breaks up the different types of framework grains that can be present in a sandstone into three major categories: quartz, feldspar, and lithic grains.

Sandstone has been used for domestic construction and housewares since prehistoric times, and continues to be used.

Sandstone was a popular building material from ancient times. It is relatively soft, making it easy to carve. It has been widely used around the world in constructing temples, homes, and other buildings. It has also been used for artistic purposes to create ornamental fountains and statues.

Some sandstones are resistant to weathering, yet are easy to work. This makes sandstone a common building and paving material including in asphalt concrete. However, some that have been used in the past, such as the Collyhurst sandstone used in North West England, have been found less resistant, necessitating repair and replacement in older buildings. Because of the hardness of individual grains, uniformity of grain size and friability of their structure, some types of sandstone are excellent materials from which to make grindstones, for sharpening blades and other implements. Non-friable sandstone can be used to make grindstones for grinding grain, e.g., gritstone.

A type of pure quartz sandstone, the orthoquartzite, with more of 90–95 percent of quartz, has been proposed for nomination to the Global Heritage Stone Resource. In some regions of Argentina, the orthoquartzite-stoned facade is one of the main features of the Mar del Plata style bungalows.




</doc>
<doc id="27773" url="https://en.wikipedia.org/wiki?curid=27773" title="Sophia of Hanover">
Sophia of Hanover

Sophia of Hanover (born Sophia of the Palatinate; 14 October 1630 – 8 June 1714) was the Electress of Hanover from 1692 to 1698. As a granddaughter of James VI and I, she became heir presumptive to the crowns of the Kingdom of England and the Kingdom of Ireland under the Act of Settlement 1701. After the Acts of Union 1707, she became heir presumptive to the unified throne of the Kingdom of Great Britain. She died less than two months before she would have become queen succeeding her first cousin once removed, Queen Anne, and her claim to the throne passed on to her eldest son, George Louis, Elector of Hanover, who ascended as George I on 1 August 1714 (Old Style).

Born to Frederick V of the Palatinate, a member of the House of Wittelsbach, and Elizabeth Stuart, in 1630, Sophia grew up in the Dutch Republic, where her family had sought refuge after the sequestration of their Electorate during the Thirty Years' War. Sophia's brother Charles Louis was restored to the Palatinate as part of the Peace of Westphalia. Sophia married Ernest Augustus of Brunswick-Lüneburg in 1658. Despite his jealous temper and frequent absences, Sophia loved him, and bore him seven children who survived to adulthood. Initially a landless cadet, Ernest Augustus succeeded in having the House of Hanover raised to electoral dignity in 1692. Therefore, Sophia became Electress of Hanover, the title by which she is best remembered. A patron of the arts, Sophia commissioned the palace and gardens of Herrenhausen and sponsored philosophers, such as Gottfried Leibniz and John Toland.

A daughter of Frederick V of the Palatinate by Elizabeth Stuart, also known as the "Winter King and Queen of Bohemia" for their short rule in that country, Sophia was born in The Wassenaer Hof, The Hague, Dutch Republic, where her parents had fled into exile after the Battle of White Mountain. Through her mother, she was the granddaughter of James VI and I, king of Scotland and England. At birth, Sophia was granted an annuity of 40 thalers by the Estates of Friesland. Sophia was courted by her first cousin, Charles II of England, but she rebuffed his advances as she thought he was using her in order to get money from her mother's supporter, Lord William Craven.

Before her marriage, Sophia, as the daughter of Frederick V, Elector Palatine of the Rhine, was referred to as Sophie, Princess Palatine of the Rhine, or as Sophia of the Palatinate. The Electors of the Palatinate were the Calvinist senior branch of House of Wittelsbach, whose Catholic branch ruled the Electorate of Bavaria.

On 30 September 1658, she married Ernest Augustus, Elector of Brunswick-Lüneburg, at Heidelberg, who in 1692 became the first Elector of Hanover. Ernst August was a second cousin of Sophia's mother Elizabeth Stuart, as they were both great-grandchildren of Christian III of Denmark.

Sophia became a friend and admirer of Gottfried Leibniz while he was librarian at the Court of Hanover. Their friendship lasted from 1676 until her death in 1714. This friendship resulted in a substantial correspondence, first published in the nineteenth century (Klopp 1973), that reveals Sophia to have been a woman of exceptional intellectual ability and curiosity. She was well-read in the works of René Descartes and Baruch Spinoza. Together with Ernest Augustus, she greatly improved the Summer Palace of Herrenhausen and she was the guiding spirit in the creation of the gardens (which still exist) surrounding the palace, where she died.

Sophia had seven children who reached adulthood. They were:


Sophia was absent for almost a year, 1664–65, during a long holiday with Ernest Augustus in Italy but she corresponded regularly with her sons' governess and took a great interest in her sons' upbringing, even more so on her return. After Sophia's tour, she bore Ernest Augustus another four sons and a daughter. In her letters, Sophia describes her eldest son as a responsible, conscientious child who set an example to his younger brothers and sisters.

Sophia was, at first, against the marriage of her son and Sophia Dorothea of Celle, looking down on Sophia Dorothea's mother (who was not of royal birth and who Sophia referred to as "mouse dirt mixed among the pepper") and concerned by Sophia Dorothea's legitimated status, but was eventually won over by the advantages inherent in the marriage.

In September 1700, Sophia met her cousin, William III of England, at Loo. This happened just two months after the death of Prince William, Duke of Gloucester, nephew of William III and son of the future Queen Anne. By this time, given the ailing William III's reluctance to remarry, the inclusion of Sophia in the line of succession was becoming more likely, because she was a Protestant, as was her husband. Her candidature was also aided by the fact that, because she had grown up in the Netherlands so she was well known to her cousin, William III, and was able to converse fluently with him in Dutch, his native tongue.

A year after their meeting, the Parliament of England passed the Act of Settlement 1701 declaring that, in the event of no legitimate issue from Anne or William III, the crowns of England and Scotland were to settle upon "the most excellent princess Sophia, electress and duchess-dowager of Hanover" and "the heirs of her body, being Protestant". The key excerpt from the Settlement, naming Sophia as heir presumptive reads:

Therefore for a further Provision of the Succession of the Crown in the Protestant Line We Your Majesties most dutifull and Loyall Subjects the Lords Spirituall and Temporall and Commons in this present Parliament assembled do beseech Your Majesty that it may be enacted and declared and be it enacted and declared by the Kings most Excellent Majesty by and with the Advice and Consent of the Lords Spirituall and Temporall and Commons in this present Parliament assembled and by the Authority of the same That the most Excellent Princess Sophia Electress and Dutchess Dowager of Hannover Daughter of the most Excellent Princess Elizabeth late Queen of Bohemia Daughter of our late Sovereign Lord King James the First of happy Memory be and is hereby declared to be the next in Succession in the Protestant Line to the Imperiall Crown and Dignity of the forsaid Realms of England France and Ireland with the Dominions and Territories thereunto belonging after His Majesty and the Princess Anne of Denmark and in Default of Issue of the said Princess Anne and of His Majesty respectively.

Sophia was made heir presumptive to cut off not just a claim by the Roman Catholic James Francis Edward Stuart, who would have become James III and VIII, but also to deny the throne to the many other Roman Catholics and spouses of Roman Catholics who held a claim. The act restricts the British throne to the "Protestant heirs" of Sophia of Hanover who had never been Roman Catholic or married a Roman Catholic. Some British politicians attempted several times to bring Sophia to England in order to enable her to assume government immediately in the event of Anne's death. It was also argued that such a course was necessary to ensure Sophia's succession, for Anne's Roman Catholic half-brother was significantly closer to London than was Sophia. The Electress was eager to move to London, but the proposal was denied, as such action would mortally offend Anne who was strongly opposed to a rival court in her kingdom. Anne might have been aware that Sophia, who was active and lively despite her old age, could cut a better figure than herself. Sophia was completely uncertain of what would happen after Anne's death, saying: "What Parliament does one day, it undoes the next."

When the law was passed in mid-1701, Sophia (age 70), five of her children (ages 35 to 41), and three legitimate grandchildren (ages 14 to 18) were alive. Although Sophia was in her seventy-first year, older than Anne by thirty-five years, she was very fit and healthy, and invested time and energy in securing the succession either for herself or her son. Currently, there are more than 5,000 legitimate descendants of Sophia, although not all are in the line of succession. The Sophia Naturalization Act 1705 granted the right of British nationality to Sophia's non-Roman Catholic descendants; those who had obtained the right to British citizenship via this Act at any time before its repeal by the British Nationality Act 1948 retain this lawful right today.

Although considerably older than Queen Anne, Sophia enjoyed much better health. According to the Countess of Bückeburg in a letter to Sophia's niece, the Raugravine Luise, on 5 June 1714 Sophia felt ill after receiving an angry letter from Queen Anne. Two days later she was walking in the gardens of Herrenhausen when she ran to shelter from a sudden downpour of rain and collapsed and died, aged 83—a very advanced age for the era. Just over a month later, in August, Queen Anne died at the age of 49. Had Anne predeceased Sophia, Sophia would have been the oldest person to ascend the British throne.

Upon Sophia's death, her eldest son Elector George Louis of Brunswick-Lüneburg (1660–1727) became heir presumptive in her place, and weeks later, succeeded Anne as George I. Sophia's daughter Sophia Charlotte of Hanover (1668–1705) married Frederick I of Prussia, from whom the later Prussian and German monarchs descend.

Sophia was buried in the chapel of Leine Palace, as were her husband and, later, their son George I. After destruction of the palace and its chapel during World War II by British air raids, their remains were moved into the mausoleum of King Ernest Augustus I in the Berggarten of Herrenhausen Gardens in 1957.




</doc>
<doc id="27774" url="https://en.wikipedia.org/wiki?curid=27774" title="Scanning tunneling microscope">
Scanning tunneling microscope

A scanning tunneling microscope (STM) is an instrument for imaging surfaces at the atomic level. Its development in 1981 earned its inventors, Gerd Binnig and Heinrich Rohrer (at IBM Zürich), the Nobel Prize in Physics in 1986. For a STM, good resolution is considered to be 0.1 nm lateral resolution and 0.01 nm (10 pm) depth resolution. With this resolution, individual atoms within materials are routinely imaged and manipulated. The STM can be used not only in ultra-high vacuum but also in air, water, and various other liquid or gas ambients, and at temperatures ranging from near zero kelvin to over 1000 °C.

STM is based on the concept of quantum tunneling. When a conducting tip is brought very near to the surface to be examined, a bias (voltage difference) applied between the two can allow electrons to tunnel through the vacuum between them. The resulting "tunneling current" is a function of tip position, applied voltage, and the local density of states (LDOS) of the sample. Information is acquired by monitoring the current as the tip's position scans across the surface, and is usually displayed in image form. STM can be a challenging technique, as it requires extremely clean and stable surfaces, sharp tips, excellent vibration control, and sophisticated electronics, but nonetheless many hobbyists have built their own.

First, a voltage bias is applied and the tip is brought close to the sample by coarse sample-to-tip control, which is turned off when the tip and sample are sufficiently close. At close range, fine control of the tip in all three dimensions when near the sample is typically piezoelectric, maintaining tip-sample separation W typically in the 4-7 Å (0.4-0.7 nm) range, which is the equilibrium position between attractive (3<W<10Å) and repulsive (W<3Å) interactions. In this situation, the voltage bias will cause electrons to tunnel between the tip and sample, creating a current that can be measured. Once tunneling is established, the tip's bias and position with respect to the sample can be varied (with the details of this variation depending on the experiment) and data are obtained from the resulting changes in current.

If the tip is moved across the sample in the x-y plane, the changes in surface height and density of states causes changes in current. These changes are mapped in images. This change in current with respect to position can be measured itself, or the height, z, of the tip corresponding to a constant current can be measured. These two modes are called constant height mode and constant current mode, respectively. In constant current mode, feedback electronics adjust the height by a voltage to the piezoelectric height control mechanism. This leads to a height variation and thus the image comes from the tip topography across the sample and gives a constant charge density surface; this means contrast on the image is due to variations in charge density. In constant height mode, the voltage and height are both held constant while the current changes to keep the voltage from changing; this leads to an image made of current changes over the surface, which can be related to charge density. The benefit to using a constant height mode is that it is faster, as the piezoelectric movements require more time to register the height change in constant current mode than the current change in constant height mode. All images produced by STM are grayscale, with color optionally added in post-processing in order to visually emphasize important features.

In addition to scanning across the sample, information on the electronic structure at a given location in the sample can be obtained by sweeping voltage and measuring current at a specific location. This type of measurement is called scanning tunneling spectroscopy (STS) and typically results in a plot of the local density of states as a function of energy within the sample. The advantage of STM over other measurements of the density of states lies in its ability to make extremely local measurements: for example, the density of states at an impurity site can be compared to the density of states far from impurities.

Framerates of at least 25 Hz enable so called video-rate STM. Framerates up to 80 Hz are possible with fully working feedback that adjusts the height of the tip. Due to the line-by-line scanning motion, a proper comparison on the speed requires not only the framerate, but also the number of pixels in an image: with a framerate of 10 Hz and 100x100 pixels the tip moves with a line frequency of 1 kHz, whereas it moves with only with 500 Hz, when measuring with a faster framerate of 50 Hz but only 10x10 pixels. Video-rate STM can be used to scan surface diffusion.

The components of an STM include scanning tip, piezoelectric controlled height and x,y scanner, coarse sample-to-tip control, vibration isolation system, and computer.

The resolution of an image is limited by the radius of curvature of the scanning tip of the STM. Additionally, image artifacts can occur if the tip has two tips at the end rather than a single atom; this leads to “double-tip imaging,” a situation in which both tips contribute to the tunneling. Therefore, it has been essential to develop processes for consistently obtaining sharp, usable tips. Recently, carbon nanotubes have been used in this instance.

The tip is often made of tungsten or platinum-iridium, though gold is also used. Tungsten tips are usually made by electrochemical etching, and platinum-iridium tips by mechanical shearing.

Due to the extreme sensitivity of tunnel current to height, proper vibration insulation or an extremely rigid STM body is imperative for obtaining usable results. In the first STM by Binnig and Rohrer, magnetic levitation was used to keep the STM free from vibrations; now mechanical spring or gas spring systems are often used. Additionally, mechanisms for reducing eddy currents are sometimes implemented.

Maintaining the tip position with respect to the sample, scanning the sample and acquiring the data is computer controlled. The computer may also be used for enhancing the image with the help of image processing as well as performing quantitative measurements.

Many other microscopy techniques have been developed based upon STM. These include photon scanning microscopy (PSTM), which uses an optical tip to tunnel photons; scanning tunneling potentiometry (STP), which measures electric potential across a surface; spin polarized scanning tunneling microscopy (SPSTM), which uses a ferromagnetic tip to tunnel spin-polarized electrons into a magnetic sample, and atomic force microscopy (AFM), in which the force caused by interaction between the tip and sample is measured.

Other STM methods involve manipulating the tip in order to change the topography of the sample. This is attractive for several reasons. Firstly the STM has an atomically precise positioning system which allows very accurate atomic scale manipulation. Furthermore, after the surface is modified by the tip, it is a simple matter to then image with the same tip, without changing the instrument. IBM researchers developed a way to manipulate xenon atoms adsorbed on a nickel surface. This technique has been used to create electron "corrals" with a small number of adsorbed atoms, which allows the STM to be used to observe electron Friedel oscillations on the surface of the material. Aside from modifying the actual sample surface, one can also use the STM to tunnel electrons into a layer of electron beam photoresist on a sample, in order to do lithography. This has the advantage of offering more control of the exposure than traditional electron beam lithography. Another practical application of STM is atomic deposition of metals (gold, silver, tungsten, etc.) with any desired (pre-programmed) pattern, which can be used as contacts to nanodevices or as nanodevices themselves.

Variable temperature STM was used to investigate temperature dependency of molecular rotations on single crystalline surfaces.
Rotating molecules appear blurred compared to non-rotating ones.

Recently groups have found they can use the STM tip to rotate individual bonds within single molecules. The electrical resistance of the molecule depends on the orientation of the bond, so the molecule effectively becomes a molecular switch.

Tunneling is a functioning concept that arises from quantum mechanics. Classically, an object hitting an impenetrable barrier will not pass through. In contrast, objects with a very small mass, such as the electron, have wavelike characteristics which permit such an event, referred to as tunneling.

Electrons behave as beams of energy, and in the presence of a potential "U"("z"), assuming 1-dimensional case, the energy levels "ψ"("z") of the electrons are given by solutions to Schrödinger’s equation,

where "ħ" is the reduced Planck’s constant, "z" is the position, and "m" is the mass of an electron. If an electron of energy "E" is incident upon an energy barrier of height "U"("z"), the electron wave function is a traveling wave solution,

where

if "E" > "U"("z"), which is true for a wave function inside the tip or inside the sample. Inside a barrier, "E" < "U"("z") so the wave functions which satisfy this are decaying waves,

where

quantifies the decay of the wave inside the barrier, with the barrier in the +"z" direction for formula_6.
Knowing the wave function allows one to calculate the probability density for that electron to be found at some location. In the case of tunneling, the tip and sample wave functions overlap such that when under a bias, there is some finite probability to find the electron in the barrier region and even on the other side of the barrier. Let us assume the bias is "V" and the barrier width is "W". This probability, "P", that an electron at "z"=0 (left edge of barrier) can be found at "z"="W" (right edge of barrier) is proportional to the wave function squared,

If the bias is small, we can let "U" − "E" ≈ "φM" in the expression for "κ", where "φM", the work function, gives the minimum energy needed to bring an electron from an occupied level, the highest of which is at the Fermi level (for metals at "T"=0 kelvins), to vacuum level. When a small bias "V" is applied to the system, only electronic states very near the Fermi level, within "eV" (a product of electron charge and voltage, not to be confused here with electronvolt unit), are excited. These excited electrons can tunnel across the barrier. In other words, tunneling occurs mainly with electrons of energies near the Fermi level.

However, tunneling does require that there be an empty level of the same energy as the electron for the electron to tunnel into on the other side of the barrier. It is because of this restriction that the tunneling current can be related to the density of available or filled states in the sample. The current due to an applied voltage "V" (assume tunneling occurs sample to tip) depends on two factors: 1) the number of electrons between "E" and "eV" in the sample, and 2) the number among them which have corresponding free states to tunnel into on the other side of the barrier at the tip. The higher the density of available states the greater the tunneling current. When "V" is positive, electrons in the tip tunnel into empty states in the sample; for a negative bias, electrons tunnel out of occupied states in the sample into the tip.

Mathematically, this tunneling current is given by

One can sum the probability over energies between "E" − "eV" and "E" to get the number of states available in this energy range per unit volume, thereby finding the local density of states (LDOS) near the Fermi level. The LDOS near some energy "E" in an interval "ε" is given by

and the tunnel current at a small bias V is proportional to the LDOS near the Fermi level, which gives important information about the sample. It is desirable to use LDOS to express the current because this value does not change as the volume changes, while probability density does. Thus the tunneling current is given by

where ρ(0,"E") is the LDOS near the Fermi level of the sample at the sample surface. This current can also be expressed in terms of the LDOS near the Fermi level of the sample at the tip surface,

The exponential term in the above equations means that small variations in W greatly influence the tunnel current. If the separation is decreased by 1 Å, the current increases by an order of magnitude, and vice versa.

This approach fails to account for the "rate" at which electrons can pass the barrier. This rate should affect the tunnel current, so it can be treated using the Fermi's golden rule with the appropriate tunneling matrix element. John Bardeen solved this problem in his study of the metal-insulator-metal junction. He found that if he solved Schrödinger’s equation for each side of the junction separately to obtain the wave functions ψ and χ for each electrode, he could obtain the tunnel matrix, M, from the overlap of these two wave functions. This can be applied to STM by making the electrodes the tip and sample, assigning ψ and χ as sample and tip wave functions, respectively, and evaluating M at some surface S between the metal electrodes, where z=0 at the sample surface and z=W at the tip surface.

Now, Fermi’s Golden Rule gives the rate for electron transfer across the barrier, and is written

where δ(E–E) restricts tunneling to occur only between electron levels with the same energy. The tunnel matrix element, given by

is a description of the lower energy associated with the interaction of wave functions at the overlap, also called the resonance energy.

Summing over all the states gives the tunneling current as

where "f" is the Fermi function, ρ and ρ are the density of states in the sample and tip, respectively. The Fermi distribution function describes the filling of electron levels at a given temperature T.

An earlier, similar invention, the "Topografiner" of R. Young, J. Ward, and F. Scire from the NIST, relied on field emission. However, Young is credited by the Nobel Committee as the person who realized that it should be possible to achieve better resolution by using the tunnel effect.




</doc>
<doc id="27775" url="https://en.wikipedia.org/wiki?curid=27775" title="STM">
STM

STM may refer to:







</doc>
<doc id="27778" url="https://en.wikipedia.org/wiki?curid=27778" title="Svenska Akademiens ordbok">
Svenska Akademiens ordbok

Svenska Akademiens ordbok (), abbreviated SAOB, is a dictionary published by the Swedish Academy, with the official title "Ordbok över svenska språket utgiven av Svenska Akademien". This dictionary is the Swedish counterpart of the "Oxford English Dictionary" (OED) or the "Deutsches Wörterbuch" (DWB). 

Work on the dictionary started in 1787 and the first volume was published in 1898 and as of 2017, when the latest volume appeared, work has progressed to the word VRET. The dictionary has approximately 450,000 main entries. The searchable web version has been available since 1997.




</doc>
<doc id="27781" url="https://en.wikipedia.org/wiki?curid=27781" title="Shirehorses">
Shirehorses

The Shirehorses are a spoof band comprising two BBC Radio DJs from Manchester, Mark Radcliffe and Marc Riley, known collectively as Mark and Lard.

As part of their BBC Radio 1 shows, the pair produced pastiches of chart songs, such as "You're Gormless", a parody of Babybird's "You're Gorgeous", "Lardy Boy", a parody of Placebo's "Nancy Boy", and "Why Is It Always Dairylea", spoofing Travis's "Why Does It Always Rain on Me?", using the band names 'Baby Bloke', 'Gazebo' and 'Dave Lee Travisty' respectively. When they rewrote The Seahorses' "Love Is the Law" as "(Now) I Know (Where I'm Going) Our Kid", they chose the stage-name Shirehorses, which they then retained for future recordings and performances. Other parodies include "I Want a Roll with It" (spoofing "Roll with It" by Oasis), "Feel Like Shite" ("Alright" by Supergrass), and "Country Spouse" ("Country House" by Blur). 

The band toured extensively, playing many small, university gigs to exploit their popularity with students. However, they also performed at larger venues, supporting Blur on a 1997 UK tour, taking in several stadia, and appearing at Glastonbury Festival in 1997.

Marc Riley was formerly a member of British Manchester band the Fall and later the Creepers before embarking on a radio presentation career alongside Mark Radcliffe. Formerly a double act on BBC Radio 1, in March 2004 they went their separate ways, Radcliffe initially to BBC Radio 2, Riley to BBC Radio 6 Music and later joined at the station by Mark Radcliffe as part of the afternoon Radcliffe and Maconie show.

The Shirehorses have released two albums to date:


</doc>
<doc id="27783" url="https://en.wikipedia.org/wiki?curid=27783" title="Stem cell">
Stem cell

Stem cells are biological cells that can differentiate into other types of cells and can divide to produce more of the same type of stem cells. They are found in multicellular organisms.

In mammals, there are two broad types of stem cells: embryonic stem cells, which are isolated from the inner cell mass of blastocysts, and adult stem cells, which are found in various tissues. In adult organisms, stem cells and progenitor cells act as a repair system for the body, replenishing adult tissues. In a developing embryo, stem cells can differentiate into all the specialized cells—ectoderm, endoderm and mesoderm (see induced pluripotent stem cells)—but also maintain the normal turnover of regenerative organs, such as blood, skin, or intestinal tissues.

There are three known accessible sources of autologous adult stem cells in humans:
Stem cells can also be taken from umbilical cord blood just after birth. Of all stem cell types, autologous harvesting involves the least risk. By definition, autologous cells are obtained from one's own body, just as one may bank his or her own blood for elective surgical procedures.

Adult stem cells are frequently used in various medical therapies (e.g., bone marrow transplantation). Stem cells can now be artificially grown and transformed (differentiated) into specialized cell types with characteristics consistent with cells of various tissues such as muscles or nerves. Embryonic cell lines and autologous embryonic stem cells generated through somatic cell nuclear transfer or dedifferentiation have also been proposed as promising candidates for future therapies. Research into stem cells grew out of findings by Ernest A. McCulloch and James E. Till at the University of Toronto in the 1960s.

The classical definition of a stem cell requires that it possesses two properties:

Two mechanisms exist to ensure that a stem cell population is maintained:

1. Obligatory asymmetric replication: a stem cell divides into one mother cell that is identical to the original stem cell, and another daughter cell that is differentiated.

When a stem cell self-renews it divides and does not disrupt the undifferentiated state. This self-renewal demands control of cell cycle as well as upkeep of multipotency or pluripotency, which all depends on the stem cell.

2. Stochastic differentiation: when one stem cell develops into two differentiated daughter cells, another stem cell undergoes mitosis and produces two stem cells identical to the original.

"Potency" specifies the differentiation potential (the potential to differentiate into different cell types) of the stem cell.

In practice, stem cells are identified by whether they can regenerate tissue. For example, the defining test for bone marrow or hematopoietic stem cells (HSCs) is the ability to transplant the cells and save an individual without HSCs. This demonstrates that the cells can produce new blood cells over a long term. It should also be possible to isolate stem cells from the transplanted individual, which can themselves be transplanted into another individual without HSCs, demonstrating that the stem cell was able to self-renew.

Properties of stem cells can be illustrated "in vitro", using methods such as clonogenic assays, in which single cells are assessed for their ability to differentiate and self-renew. Stem cells can also be isolated by their possession of a distinctive set of cell surface markers. However, "in vitro" culture conditions can alter the behavior of cells, making it unclear whether the cells shall behave in a similar manner "in vivo". There is considerable debate as to whether some proposed adult cell populations are truly stem cells.

Embryonic stem (ES) cells are the cells of the inner cell mass of a blastocyst, an early-stage embryo. Human embryos reach the blastocyst stage 4–5 days post fertilization, at which time they consist of 50–150 cells. ES cells are pluripotent and give rise during development to all derivatives of the three primary germ layers: ectoderm, endoderm and mesoderm. In other words, they can develop into each of the more than 200 cell types of the adult body when given sufficient and necessary stimulation for a specific cell type. They do not contribute to the extra-embryonic membranes or the placenta.

During embryonic development these inner cell mass cells continuously divide and become more specialized. For example, a portion of the ectoderm in the dorsal part of the embryo specializes as 'neurectoderm', which will become the future central nervous system. Later in development, neurulation causes the neurectoderm to form the neural tube. At the neural tube stage, the anterior portion undergoes encephalization to generate or 'pattern' the basic form of the brain. At this stage of development, the principal cell type of the CNS is considered a neural stem cell. These neural stem cells are pluripotent, as they can generate a large diversity of many different neuron types, each with unique gene expression, morphological, and functional characteristics. The process of generating neurons from stem cells is called neurogenesis. One prominent example of a neural stem cell is the radial glial cell, so named because it has a distinctive bipolar morphology with highly elongated processes spanning the thickness of the neural tube wall, and because historically it shared some glial characteristics, most notably the expression of glial fibrillary acidic protein (GFAP). The radial glial cell is the primary neural stem cell of the developing vertebrate CNS, and its cell body resides in the ventricular zone, adjacent to the developing ventricular system. Neural stem cells are committed to the neuronal lineages (neurons, astrocytes, and oligodendrocytes), and thus their potency is restricted.

Nearly all research to date has made use of mouse embryonic stem cells (mES) or human embryonic stem cells (hES) derived from the early inner cell mass. Both have the essential stem cell characteristics, yet they require very different environments in order to maintain an undifferentiated state. Mouse ES cells are grown on a layer of gelatin as an extracellular matrix (for support) and require the presence of leukemia inhibitory factor (LIF) in serum media. A drug cocktail containing inhibitors to GSK3B and the MAPK/ERK pathway, called 2i, has also been shown to maintain pluripotency in stem cell culture. Human ES cells are grown on a feeder layer of mouse embryonic fibroblasts (MEFs) and require the presence of basic fibroblast growth factor (bFGF or FGF-2). Without optimal culture conditions or genetic manipulation, embryonic stem cells will rapidly differentiate.

A human embryonic stem cell is also defined by the expression of several transcription factors and cell surface proteins. The transcription factors Oct-4, Nanog, and Sox2 form the core regulatory network that ensures the suppression of genes that lead to differentiation and the maintenance of pluripotency. The cell surface antigens most commonly used to identify hES cells are the glycolipids stage specific embryonic antigen 3 and 4 and the keratan sulfate antigens Tra-1-60 and Tra-1-81. By using human embryonic stem cells to produce specialized cells like nerve cells or heart cells in the lab, scientists can gain access to adult human cells without taking tissue from patients. They can then study these specialized adult cells in detail to try and catch complications of diseases, or to study cells reactions to potentially new drugs. The molecular definition of a stem cell includes many more proteins and continues to be a topic of research.

There are currently no approved treatments using embryonic stem cells. The first human trial was approved by the US Food and Drug Administration in January 2009. However, the human trial was not initiated until October 13, 2010 in Atlanta for spinal cord injury research. On November 14, 2011 the company conducting the trial (Geron Corporation) announced that it will discontinue further development of its stem cell programs. ES cells, being pluripotent cells, require specific signals for correct differentiation—if injected directly into another body, ES cells will differentiate into many different types of cells, causing a teratoma. Differentiating ES cells into usable cells while avoiding transplant rejection are just a few of the hurdles that embryonic stem cell researchers still face. Due to ethical considerations, many nations currently have moratoria or limitations on either human ES cell research or the production of new human ES cell lines. Because of their combined abilities of unlimited expansion and pluripotency, embryonic stem cells remain a theoretically potential source for regenerative medicine and tissue replacement after injury or disease.

The primitive stem cells located in the organs of fetuses are referred to as fetal stem cells.
There are two types of fetal stem cells:


Adult stem cells, also called somatic (from Greek σωματικóς, "of the body") stem cells, are stem cells which maintain and repair the tissue in which they are found. They can be found in children, as well as adults.

Pluripotent adult stem cells are rare and generally small in number, but they can be found in umbilical cord blood and other tissues. Bone marrow is a rich source of adult stem cells, which have been used in treating several conditions including liver cirrhosis, chronic limb ischemia and endstage heart failure. The quantity of bone marrow stem cells declines with age and is greater in males than females during reproductive years. Much adult stem cell research to date has aimed to characterize their potency and self-renewal capabilities. DNA damage accumulates with age in both stem cells and the cells that comprise the stem cell environment. This accumulation is considered to be responsible, at least in part, for increasing stem cell dysfunction with aging (see DNA damage theory of aging).

Most adult stem cells are lineage-restricted (multipotent) and are generally referred to by their tissue origin (mesenchymal stem cell, adipose-derived stem cell, endothelial stem cell, dental pulp stem cell, etc.). Muse cells (multi-lineage differentiating stress enduring cells) are a recently discovered pluripotent stem cell type found in multiple adult tissues, including adipose, dermal fibroblasts, and bone marrow. While rare, muse cells are identifiable by their expression of SSEA-3, a marker for undifferentiated stem cells, and general mesenchymal stem cells markers such as CD105. When subjected to single cell suspension culture, the cells will generate clusters that are similar to embryoid bodies in morphology as well as gene expression, including canonical pluripotency markers Oct4, Sox2, and Nanog.

Adult stem cell treatments have been successfully used for many years to treat leukemia and related bone/blood cancers through bone marrow transplants. Adult stem cells are also used in veterinary medicine to treat tendon and ligament injuries in horses.

The use of adult stem cells in research and therapy is not as controversial as the use of embryonic stem cells, because the production of adult stem cells does not require the destruction of an embryo. Additionally, in instances where adult stem cells are obtained from the intended recipient (an autograft), the risk of rejection is essentially non-existent. Consequently, more US government funding is being provided for adult stem cell research.

Multipotent stem cells are also found in amniotic fluid. These stem cells are very active, expand extensively without feeders and are not tumorigenic. Amniotic stem cells are multipotent and can differentiate in cells of adipogenic, osteogenic, myogenic, endothelial, hepatic and also neuronal lines.
Amniotic stem cells are a topic of active research.

Use of stem cells from amniotic fluid overcomes the ethical objections to using human embryos as a source of cells. Roman Catholic teaching forbids the use of embryonic stem cells in experimentation; accordingly, the Vatican newspaper "Osservatore Romano" called amniotic stem cells "the future of medicine".

It is possible to collect amniotic stem cells for donors or for autologuous use: the first US amniotic stem cells bank was opened in 2009 in Medford, MA, by Biocell Center Corporation and collaborates with various hospitals and universities all over the world.

Adult stem cells have limitations with their potency; unlike ESCs, they are not able to differentiate into cells from all three germ layers. As such, they are deemed multipotent.

However, reprogramming allows for the creation of pluripotent cells, induced pluripotent stem cells, from adult cells. It is important to note that these are not adult stem cells, but adult cells (e.g. epithelial cells) reprogrammed to give rise to cells with pluripotent capabilities. Using genetic reprogramming with protein transcription factors, pluripotent stem cells with ESC-like capabilities have been derived. The first demonstration of Induced Pluripotent Stem Cells was conducted by Shinya Yamanaka and his colleagues at Kyoto University. They used the transcription factors Oct3/4, Sox2, c-Myc, and Klf4 to reprogram mouse fibroblast cells into pluripotent cells. Subsequent work used these factors to induce pluripotency in human fibroblast cells. Junying Yu, James Thomson, and their colleagues at the University of Wisconsin–Madison used a different set of factors, Oct4, Sox2, Nanog and Lin28, and carried out their experiments using cells from human foreskin. However, they were able to replicate Yamanaka's finding that inducing pluripotency in human cells was possible.

It is important to note that iPSCs and ESCs are not equivalent. They have many similar properties, such as pluripotency and differentiation potential, the expression of pluripotency genes, epigenetic patterns, embryoid body and teratoma formation, and viable chimera formation. However, similar does not mean they are the same. In fact, there are many differences within these properties. Importantly, the chromatin of iPSCs appears to be more "closed" or methylated than that of ESCs. Similarly, the gene expression pattern between ESCs and iPSCs, or even iPSCs sourced from different origins. There are thus questions about the "completeness" of reprogramming and the somatic memory of induced pluripotent stem cells. Despite this, inducing adult cells to be pluripotent appears to be viable.

As a result of the success of these experiments, Ian Wilmut, who helped create the first cloned animal Dolly the Sheep, has announced that he will abandon somatic cell nuclear transfer as an avenue of research.

Furthermore, induced pluripotent stem cells provide several therapeutic advantages. Like ESCs, they are pluripotent. They thus have great differentiation potential; theoretically, they could produce any cell within the human body (if reprogramming to pluripotency was "complete"). Moreover, unlike ESCs, they potentially could allow doctors to create a pluripotent stem cell line for each individual patient. In fact, frozen blood samples can be used as a source of induced pluripotent stem cells, opening a new avenue for obtaining the valued cells. Patient specific stem cells allow for the screening for side effects before drug treatment, as well as the reduced risk of transplantation rejection. Despite their current limited use therapeutically, iPSCs hold create potential for future use in medical treatment and research.

To ensure self-renewal, stem cells undergo two types of cell division (see "Stem cell division and differentiation" diagram). Symmetric division gives rise to two identical daughter cells both endowed with stem cell properties. Asymmetric division, on the other hand, produces only one stem cell and a progenitor cell with limited self-renewal potential. Progenitors can go through several rounds of cell division before terminally differentiating into a mature cell. It is possible that the molecular distinction between symmetric and asymmetric divisions lies in differential segregation of cell membrane proteins (such as receptors) between the daughter cells.

An alternative theory is that stem cells remain undifferentiated due to environmental cues in their particular niche. Stem cells differentiate when they leave that niche or no longer receive those signals. Studies in "Drosophila" germarium have identified the signals decapentaplegic and adherens junctions that prevent germarium stem cells from differentiating.

Stem cell therapy is the use of stem cells to treat or prevent a disease or condition. Bone marrow transplant is a form of stem cell therapy that has been used for many years without controversy.

Stem cell treatments may lower symptoms of the disease or condition that is being treated. The lowering of symptoms may allow patients to reduce the drug intake of the disease or condition. Stem cell treatment may also provide knowledge for society to further stem cell understanding and future treatments.

Stem cell treatments may require immunosuppression because of a requirement for radiation before the transplant to remove the person's previous cells, or because the patient's immune system may target the stem cells. One approach to avoid the second possibility is to use stem cells from the same patient who is being treated.

Pluripotency in certain stem cells could also make it difficult to obtain a specific cell type. It is also difficult to obtain the exact cell type needed, because not all cells in a population differentiate uniformly. Undifferentiated cells can create tissues other than desired types.

Some stem cells form tumors after transplantation; pluripotency is linked to tumor formation especially in embryonic stem cells, fetal proper stem cells, induced pluripotent stem cells. Fetal proper stem cells form tumors despite multipotency.

Some of the fundamental patents covering human embryonic stem cells are owned by the Wisconsin Alumni Research Foundation (WARF) – they are patents 5,843,780, 6,200,806, and 7,029,913 invented by James A. Thomson. WARF does not enforce these patents against academic scientists, but does enforce them against companies.

In 2006, a request for the US Patent and Trademark Office (USPTO) to re-examine the three patents was filed by the Public Patent Foundation on behalf of its client, the non-profit patent-watchdog group Consumer Watchdog (formerly the Foundation for Taxpayer and Consumer Rights). In the re-examination process, which involves several rounds of discussion between the USPTO and the parties, the USPTO initially agreed with Consumer Watchdog and rejected all the claims in all three patents, however in response, WARF amended the claims of all three patents to make them more narrow, and in 2008 the USPTO found the amended claims in all three patents to be patentable. The decision on one of the patents (7,029,913) was appealable, while the decisions on the other two were not. Consumer Watchdog appealed the granting of the '913 patent to the USPTO's Board of Patent Appeals and Interferences (BPAI) which granted the appeal, and in 2010 the BPAI decided that the amended claims of the '913 patent were not patentable. However, WARF was able to re-open prosecution of the case and did so, amending the claims of the '913 patent again to make them more narrow, and in January 2013 the amended claims were allowed.

In July 2013, Consumer Watchdog announced that it would appeal the decision to allow the claims of the '913 patent to the US Court of Appeals for the Federal Circuit (CAFC), the federal appeals court that hears patent cases. At a hearing in December 2013, the CAFC raised the question of whether Consumer Watchdog had legal standing to appeal; the case could not proceed until that issue was resolved.

Diseases and conditions where stem cell treatment is being investigated include:

Research is underway to develop various sources for stem cells, and to apply stem cell treatments for neurodegenerative diseases and conditions, diabetes, heart disease, and other conditions. Research is also underway in generating organoids using stem cells, which would allow for further understanding of human development, organogenesis, and modeling of human diseases.

In more recent years, with the ability of scientists to isolate and culture embryonic stem cells, and with scientists' growing ability to create stem cells using somatic cell nuclear transfer and techniques to create induced pluripotent stem cells, controversy has crept in, both related to abortion politics and to human cloning.

Hepatotoxicity and drug-induced liver injury account for a substantial number of failures of new drugs in development and market withdrawal, highlighting the need for screening assays such as stem cell-derived hepatocyte-like cells, that are capable of detecting toxicity early in the drug development process.



</doc>
<doc id="27784" url="https://en.wikipedia.org/wiki?curid=27784" title="Sappho">
Sappho

Sappho (; Aeolic Greek , "Psappho" ; c. 630 – c. 570 BC) was an archaic Greek poet from the island of Lesbos. Sappho is known for her lyric poetry, written to be sung and accompanied by a lyre. Most of Sappho's poetry is now lost, and what is extant has survived only in fragmentary form, except for one complete poem – the "Ode to Aphrodite". As well as lyric poetry, ancient commentators claimed that Sappho wrote elegiac and iambic poetry. Three epigrams attributed to Sappho are extant, but these are actually Hellenistic imitations of Sappho's style.

Little is known of Sappho's life. She was from a wealthy family from Lesbos, though the names of both of her parents are uncertain. Ancient sources say that she had three brothers; the names of two of them are mentioned in the Brothers Poem discovered in 2014. She was exiled to Sicily around 600 BC, and may have continued to work until around 570. Later legends surrounding Sappho's love for the ferryman Phaon and her death are unreliable.

Sappho was a prolific poet, probably composing around 10,000 lines. Her poetry was well-known and greatly admired through much of antiquity, and she was among the canon of nine lyric poets most highly esteemed by scholars of Hellenistic Alexandria. Sappho's poetry is still considered extraordinary and her works continue to influence other writers. Beyond her poetry, she is well known as a symbol of love and desire between women.

There are three major sources of information about Sappho's life: her own poetry, though scholars are cautious of reading poetry as a biographical source; the "testimonia", a term that refers to biographical and literary references to Sappho from other classical authors, that however do not date from Sappho's lifetime; and anything that is known of the history in which Sappho lived.

The "testimonia" were written by those who had access to more of Sappho's poetry than modern readers do, and are a valuable source on how Sappho's poetry was received in antiquity, but it is difficult to assess how accurate their representations of Sappho's life are. Inferences made by ancient scholars and reported in the "testimonia" are in many cases known to be wrong. Nonetheless, some details mentioned in the "testimonia" may be derived from Sappho's own poetry, and may therefore possess some historical value.

Little is known about Sappho's life for certain. She was from Mytilene on the island of Lesbos and was probably born around 630 BC.
Tradition names her mother as Cleïs, though ancient scholars may simply have guessed this name, assuming that Sappho's daughter Cleïs was named after her. Sappho's father's name is less certain. Ten names are known for Sappho's father from the ancient "testimonia"; this proliferation of possible names suggests that he was not explicitly named in any of Sappho's poetry. The earliest and most commonly attested name for Sappho's father is Scamandronymus. In Ovid's "Heroides", Sappho's father died when she was seven. Sappho's father is not mentioned in any of her surviving works, but Campbell suggests that this detail may have been based on a now-lost poem. Sappho's own name is found in numerous variant spellings, even in her own Aeolian dialect; the form that appears in her own extant poetry is Psappho.

No reliable portrait of Sappho's physical appearance has survived; all extant representations, ancient and modern, are artists' conceptions. In the Tithonus poem she describes her hair as now white but formerly "melaina", i.e. black. A literary papyrus of the second century A.D. describes her as "pantelos mikra", quite tiny. Alcaeus possibly describes Sappho as "violet-haired", which was a common Greek poetic way of describing dark hair. Some scholars dismiss this tradition as unreliable.

Sappho was said to have three brothers: Erigyius, Larichus, and Charaxus. According to Athenaeus, Sappho often praised Larichus for pouring wine in the town hall of Mytilene, an office held by boys of the best families. This indication that Sappho was born into an aristocratic family is consistent with the sometimes rarefied environments that her verses record. One ancient tradition tells of a relation between Charaxus and the Egyptian courtesan Rhodopis. Herodotus, the oldest source of the story, reports that Charaxus ransomed Rhodopis for a large sum and that Sappho wrote a poem rebuking him for this.

Sappho may have had a daughter named Cleïs, who is referred to in two fragments. Not all scholars accept that Cleïs was Sappho's daughter. Fragment 132 describes Cleïs as "παῖς" ("pais"), which, as well as meaning "child", can also refer to the "youthful beloved in a male homosexual liaison". It has been suggested that Cleïs was one of Sappho's younger lovers, rather than her daughter, though Judith Hallett argues that the language used in fragment 132 suggests that Sappho was referring to Cleïs as her daughter.

According to the Suda, Sappho was married to Kerkylas of Andros. However, the name appears to have been invented by a comic poet: the name "Kerkylas" comes from the word "κέρκος" ("kerkos"), a possible meaning of which is "penis", and is not otherwise attested as a name, while "Andros", as well as being the name of a Greek island, is a form of the Greek word "ἀνήρ" ("aner"), which means man. Thus, the name may be a joke name, and as such could be rendered as "Dick Allcock from the Isle of Man".

Sappho and her family were exiled from Lesbos to Syracuse, Sicily around 600 BC. The Parian Chronicle records Sappho going into exile some time between 604 and 591. This may have been as a result of her family's involvement with the conflicts between political elites on Lesbos in this period, the same reason for Sappho's contemporary Alcaeus' exile from Mytilene around the same time. Later the exiles were allowed to return.

A tradition going back at least to Menander (Fr. 258 K) suggested that Sappho killed herself by jumping off the Leucadian cliffs for love of Phaon, a ferryman. This is regarded as unhistorical by modern scholars, perhaps invented by the comic poets or originating from a misreading of a first-person reference in a non-biographical poem. The legend may have resulted in part from a desire to assert Sappho as heterosexual.

Today Sappho, for many, is a symbol of female homosexuality; the common term "lesbian" is an allusion to Sappho. However, she has not always been so considered. In classical Athenian comedy (from the Old Comedy of the fifth century to Menander in the late fourth and early third centuries BC), Sappho was caricatured as a promiscuous heterosexual woman, and it is not until the Hellenistic period that the first "testimonia" which explicitly discuss Sappho's homoeroticism are preserved. The earliest of these is a fragmentary biography written on papyrus in the late third or early second century BC, which states that Sappho was "accused by some of being irregular in her ways and a woman-lover". Denys Page comments that the phrase "by some" implies that even the full corpus of Sappho's poetry did not provide conclusive evidence of whether she described herself as having sex with women. These ancient authors do not appear to have believed that Sappho did, in fact, have sexual relationships with other women, and as late as the tenth century the Suda records that Sappho was "slanderously accused" of having sexual relationships with her "female pupils".

Among modern scholars, Sappho's sexuality is still debated – André Lardinois has described it as the "Great Sappho Question". Early translators of Sappho sometimes heterosexualised her poetry. Ambrose Philips' 1711 translation of the Ode to Aphrodite portrayed the object of Sappho's desire as male, a reading that was followed by virtually every other translator of the poem until the twentieth century, while in 1781 Alessandro Verri interpreted fragment 31 as being about Sappho's love for Phaon. Friedrich Gottlieb Welcker argued that Sappho's feelings for other women were "entirely idealistic and non-sensual", while Karl Otfried Müller wrote that fragment 31 described "nothing but a friendly affection": Glenn Most comments that "one wonders what language Sappho would have used to describe her feelings if they had been ones of sexual excitement" if this theory were correct. By 1970, it would be argued that the same poem contained "proof positive of [Sappho's] lesbianism". 

Today, it is generally accepted that Sappho's poetry portrays homoerotic feelings: as Sandra Boehringer puts it, her works "clearly celebrate eros between women". Toward the end of the twentieth century, though, some scholars began to reject the question of whether or not Sappho was a lesbian – Glenn Most wrote that Sappho herself "would have had no idea what people mean when they call her nowadays a homosexual", André Lardinois stated that it is "nonsensical" to ask whether Sappho was a lesbian, and Page duBois calls the question a "particularly obfuscating debate".

One of the major focuses of scholars studying Sappho has been to attempt to determine the cultural context in which Sappho's poems were composed and performed. Various cultural contexts and social roles played by Sappho have been suggested, including teacher, cult-leader, and poet performing for a circle of female friends. However, the performance contexts of many of Sappho's fragments are not easy to determine, and for many more than one possible context is conceivable.

One longstanding suggestion of a social role for Sappho is that of "Sappho as schoolmistress". At the beginning of the twentieth century, the German classicist Ulrich von Wilamowitz-Moellendorff posited that Sappho was a sort of schoolteacher, in order to "explain away Sappho's passion for her 'girls and defend her from accusations of homosexuality. The view continues to be influential, both among scholars and the general public, though more recently the idea has been criticised by historians as anachronistic and has been rejected by several prominent classicists as unjustified by the evidence. In 1959, Denys Page, for example, stated that Sappho's extant fragments portray "the loves and jealousies, the pleasures and pains, of Sappho and her companions"; and he adds, "We have found, and shall find, no trace of any formal or official or professional relationship between them, ... no trace of Sappho the principal of an academy." David A. Campbell in 1967 judged that Sappho may have "presided over a literary coterie", but that "evidence for a formal appointment as priestess or teacher is hard to find". None of Sappho's own poetry mentions her teaching, and the earliest "testimonium" to support the idea of Sappho as a teacher comes from Ovid, six centuries after Sappho's lifetime. Despite these problems, many newer interpretations of Sappho's social role are still based on this idea. In these interpretations, Sappho was involved in the ritual education of girls, for instance as a trainer of choruses of girls.

Even if Sappho did compose songs for training choruses of young girls, not all of her poems can be interpreted in this light, and despite scholars' best attempts to find one, Yatromanolakis argues that there is no single performance context to which all of Sappho's poems can be attributed. Parker argues that Sappho should be considered as part of a group of female friends for whom she would have performed, just as her contemporary Alcaeus is. Some of her poetry appears to have been composed for identifiable formal occasions, but many of her songs are about – and possibly were to be performed at – banquets.

Sappho probably wrote around 10,000 lines of poetry; today, only about 650 survive. She is best known for her lyric poetry, written to be accompanied by music. The Suda also attributes to Sappho epigrams, elegiacs, and iambics, but the only epigrams attributed to Sappho to survive are in fact later works, and the iambic and elegiac poems attributed to her in antiquity were probably also actually later imitations. Ancient authors claim that Sappho primarily wrote love poetry, and the indirect transmission of Sappho's work supports this notion. However, the papyrus tradition suggests that this may not have been the case: a series of papyri published in 2014 contains fragments of ten consecutive poems from Book I of the Alexandrian edition of Sappho, of which only two are certainly love poems, while at least three and possibly four are primarily concerned with family.

Sappho's poetry was probably first written down on Lesbos, either in her lifetime or shortly afterwards, initially probably in the form of a score for performers of Sappho's work. In the fifth century, Athenian book publishers probably began to produce copies of Lesbian lyric poetry, some including explanatory material and glosses as well as the poems themselves. Some time in the second or third century, Alexandrian scholars produced a critical edition of Sappho's poetry. There may have been more than one Alexandrian edition – John J. Winkler argues for two, one edited by Aristophanes of Byzantium and another by his pupil Aristarchus of Samothrace. This is not certain – ancient sources tell us that Aristarchus' edition of Alcaeus replaced the edition by Aristophanes, but are silent on whether Sappho's work, too, went through multiple editions. 

The Alexandrian edition of Sappho's poetry was based on the existing Athenian collections, and was divided into at least eight books, though the exact number is uncertain. Many modern scholars have followed Denys Page, who conjectured a ninth book in the standard edition; Yatromanolakis doubts this, noting that though "testimonia" refer to an eighth book of Sappho's poetry, none mention a ninth. Whatever its make-up, the Alexandrian edition of Sappho probably grouped her poems by their metre: ancient sources tell us that each of the first three books contained poems in a single specific metre. Ancient editions of Sappho, possibly starting with the Alexandrian edition, seem to have ordered the poems in at least the first book of Sappho's poetry – which contained works composed in Sapphic stanzas – alphabetically.

Even after the publication of the standard Alexandrian edition, Sappho's poetry continued to circulate in other poetry collections. For instance, the Cologne Papyrus on which the Tithonus poem is preserved was part of a Hellenistic anthology of poetry, which contained poetry arranged by theme, rather than by metre and incipit, as it was in the Alexandrian edition.

The earliest surviving manuscripts of Sappho, including the potsherd on which fragment 2 is preserved, date to the third century BC, and thus predate the Alexandrian edition. The latest surviving copies of Sappho’s poems transmitted directly from ancient times are written on parchment codex pages from the sixth and seventh centuries AD, and were surely reproduced from ancient papyri now lost. Manuscript copies of Sappho's works may have survived a few centuries longer, but around the 9th century her poetry appears to have disappeared, and by the twelfth century, John Tzetzes could write that "the passage of time has destroyed Sappho and her works".

According to legend, Sappho's poetry was lost because the church disapproved of her morals. These legends appear to have originated in the renaissance – around 1550, Jerome Cardan wrote that Gregory Nazianzen had Sappho's work publicly destroyed, and at the end of the sixteenth century Joseph Justus Scaliger claimed that Sappho's works were burned in Rome and Constantinople in 1073 on the orders of Pope Gregory VII. In reality, Sappho's work was probably lost as the demand for it was insufficiently great for it to be copied onto parchment when codices superseded papyrus scrolls as the predominant form of book. Another contributing factor to the loss of Sappho's poems may have been the perceived obscurity of her Aeolic dialect, which contains many archaisms and innovations absent from other ancient Greek dialects. During the Roman period, by which time the Attic dialect had become the standard for literary compositions, many readers found Sappho's dialect difficult to understand and, in the second century AD, the Roman author Apuleius specifically remarks on its "strangeness".

Only approximately 650 lines of Sappho's poetry still survive, of which just one poem – the "Ode to Aphrodite" – is complete, and more than half of the original lines survive in around ten more fragments. Many of the surviving fragments of Sappho contain only a single word – for example, fragment 169A is simply a word meaning "wedding gifts", and survives as part of a dictionary of rare words. The two major sources of surviving fragments of Sappho are quotations in other ancient works, from a whole poem to as little as a single word, and fragments of papyrus, many of which were discovered at Oxyrhynchus in Egypt. Other fragments survive on other materials, including parchment and potsherds. The oldest surviving fragment of Sappho currently known is the Cologne papyrus which contains the Tithonus poem, dating to the third century BC.

Until the last quarter of the nineteenth century, only the ancient quotations of Sappho survived. In 1879, the first new discovery of a fragment of Sappho was made at Fayum. By the end of the nineteenth century, Grenfell and Hunt had begun to excavate an ancient rubbish dump at Oxyrhynchus, leading to the discoveries of many previously unknown fragments of Sappho. Fragments of Sappho continue to be rediscovered. Most recently, major discoveries in 2004 (the "Tithonus poem" and a new, previously unknown fragment) and 2014 (fragments of nine poems: five already known but with new readings, four, including the "Brothers Poem", not previously known) have been reported in the media around the world.

Sappho clearly worked within a well-developed tradition of Lesbian poetry, which had evolved its own poetic diction, meters, and conventions. Among her famous poetic forebears were Arion and Terpander. Nonetheless, her work is innovative; it is some of the earliest Greek poetry to adopt the "lyric 'I'" – to write poetry adopting the viewpoint of a specific person, in contrast to the earlier epic poets Homer and Hesiod, who present themselves more as "conduits of divine inspiration". Her poetry explores individual identity and personal emotions – desire, jealousy, and love; it also adopts and reinterprets the existing imagery epic poetry in exploring these themes.

Sappho's poetry is known for its clear language and simple thoughts, sharply-drawn images, and use of direct quotation which brings a sense of immediacy. Unexpected word-play is a characteristic feature of her style. An example is from fragment 96: "now she stands out among Lydian women as after sunset the rose-fingered moon exceeds all stars", a variation of the Homeric epithet "rosy-fingered Dawn". Sappho's poetry often uses hyperbole, according to ancient critics "because of its charm". An example is found in fragment 111, where Sappho writes that "The groom approaches like Ares[...] Much bigger than a big man".

Leslie Kurke groups Sappho with those archaic Greek poets from what has been called the "élite" ideological tradition, which valued luxury ("habrosyne") and high birth. These elite poets tended to identify themselves with the worlds of Greek myths, gods, and heroes, as well as the wealthy East, especially Lydia. Thus in fragment 2 Sappho describes Aphrodite "pour into golden cups nectar lavishly mingled with joys", while in the Tithonus poem she explicitly states that "I love the finer things ["habrosyne"]". According to Page DuBois, the language, as well as the content, of Sappho's poetry evokes an aristocratic sphere. She contrasts Sappho's "flowery,[...] adorned" style with the "austere, decorous, restrained" style embodied in the works of later classical authors such as Sophocles, Demosthenes, and Pindar.

Traditional modern literary critics of Sappho's poetry have tended to see her poetry as a vivid and skilled but spontaneous and naive expression of emotion: typical of this view are the remarks of H. J. Rose that "Sappho wrote as she spoke, owing practically nothing to any literary influence," and that her verse displays "the charm of absolute naturalness." Against this essentially romantic view, one school of more recent critics argues that, on the contrary, Sappho's poetry displays and depends for its effect on a sophisticated deployment of the strategies of traditional Greek rhetorical genres.

In antiquity Sappho's poetry was highly admired, and several ancient sources refer to her as the "tenth Muse". The earliest surviving poem to do so is a third-century BC epigram by Dioscorides, but poems are preserved in the "Greek Anthology" by Antipater of Sidon and attributed to Plato on the same theme. She was sometimes referred to as "The Poetess", just as Homer was "The Poet". The scholars of Alexandria included Sappho in the canon of nine lyric poets. According to Aelian, the Athenian lawmaker and poet Solon asked to be taught a song by Sappho "so that I may learn it and then die". This story may well be apocryphal, especially as Ammianus Marcellinus tells a similar story about Socrates and a song of Stesichorus, but it is indicative of how highly Sappho's poetry was considered in the ancient world.

Sappho's poetry also influenced other ancient authors. In Greek, the Hellenistic poet Nossis was described by Marylin B. Skinner as an imitator of Sappho, and Kathryn Gutzwiller argues that Nossis explicitly positioned herself as an inheritor of Sappho's position as a woman poet. Beyond poetry, Plato cites Sappho in his "Phaedrus", and Socrates' second speech on love in that dialogue appears to echo Sappho's descriptions of the physical effects of desire in fragment 31. In the first century BC, Catullus established the themes and metres of Sappho's poetry as a part of Latin literature, adopting the Sapphic stanza, believed in antiquity to have been invented by Sappho, giving his lover in his poetry the name "Lesbia" in reference to Sappho, and adapting and translating Sappho's 31st fragment in his poem 51. 

Other ancient poets wrote about Sappho's life. She was a popular character in ancient Athenian comedy, and at least six separate comedies called "Sappho" are known. The earliest known ancient comedy to take Sappho as its main subject was the early-fifth or late-fourth century BC "Sappho" by Ameipsias, though nothing is known of it apart from its name. Sappho was also a favourite subject in the visual arts, the most commonly depicted poet on sixth and fifth-century Attic red-figure vase paintings, and the subject of a sculpture by Silanion.

From the fourth century BC, ancient works portray Sappho as a tragic heroine, driven to suicide by her unrequited love for Phaon. For instance, a fragment of a play by Menander says that Sappho threw herself off of the cliff at Leucas out of her love for Phaon. Ovid's "Heroides" 15 is written as a letter from Sappho to her supposed love Phaon, and when it was first rediscovered in the 15th century was thought to be a translation of an authentic letter of Sappho's. Sappho's suicide was also depicted in classical art, for instance on a first-century BC basilica in Rome near the Porta Maggiore.

While Sappho's poetry was admired in the ancient world, her character was not always so well considered. In the Roman period, critics found her lustful and perhaps even homosexual. Horace called her "mascula Sappho" in his "Epistles", which the later Porphyrio commented was "either because she is famous for her poetry, in which men more often excel, or because she is maligned for having been a tribad". By the third century AD, the difference between Sappho's literary reputation as a poet and her moral reputation as a woman had become so significant that the suggestion that there were in fact two Sapphos began to develop. In his "Historical Miscellanies", Aelian wrote that there was "another Sappho, a courtesan, not a poetess".

By the medieval period, Sappho's works had been lost, though she was still known through later ancient authors such as Ovid. Her works began to become accessible again in the sixteenth century, first in early printed editions of authors who had quoted her. In 1508 Aldus Manutius printed an edition of Dionysius of Hallicarnassus, which contained Sappho 1, the "Ode to Aphrodite", and the first printed edition of Longinus' "On the Sublime", complete with his quotation of Sappho 31, appeared in 1554. In 1566, the French printer Robert Estienne produced an edition of the Greek lyric poets which contained around 40 fragments attributed to Sappho. In 1652, the first English translation of a poem by Sappho was published, in John Hall's translation of "On the Sublime". In 1681 Anne Le Fèvre's French edition of Sappho made her work even more widely known. Theodor Bergk's 1854 edition became the standard edition of Sappho in the second half of the 19th century; in the first part of the 20th, the papyrus discoveries of new poems by Sappho led to editions and translations by Edwin Marion Cox and John Maxwell Edmonds, and culminated in the 1955 publication of Edgar Lobel's and Denys Page's "Poetarum Lesbiorum Fragmenta".

Like the ancients, modern critics have tended to consider Sappho's poetry "extraordinary". As early as the 9th century, Sappho was referred to as a talented woman poet, and in works such as Boccaccio's "De Claris Mulieribus" and Christine de Pisan's "Book of the City of Ladies" she gained a reputation as a learned lady. Even after Sappho's works had been lost, the Sapphic stanza continued to be used in medieval lyric poetry, and with the rediscovery of her work in the Renaissance, she began to increasingly influence European poetry. In the 16th century, members of La Pléiade, a circle of French poets, were influenced by her to experiment with Sapphic stanzas and with writing love-poetry with a first-person female voice. From the Romantic era, Sappho's work – especially her "Ode to Aphrodite" – has been a key influence of conceptions of what lyric poetry should be. Such influential poets as Alfred Lord Tennyson in the nineteenth century, and A. E. Housman in the twentieth, have been influenced by her poetry. Tennyson based poems including "Eleanore" and "Fatima" on Sappho's fragment 31, while three of Housman's works are adaptations of the Midnight poem, long thought to be by Sappho though the authorship is now disputed. At the beginning of the twentieth century, the Imagists – especially Ezra Pound, H. D., and Richard Aldington – were influenced by Sappho's fragments; a number of Pound's poems in his early collection "Lustra" were adaptations of Sapphic poems, while H. D.'s poetry was frequently Sapphic in "style, theme or content", and in some cases, such as "Fragment 40" more specifically invoke Sappho's writing.

It was not long after the rediscovery of Sappho that her sexuality once again became the focus of critical attention. In the early seventeenth century, John Donne wrote "Sapho to Philaenis", returning to the idea of Sappho as a hypersexual lover of women. The modern debate on Sappho's sexuality began in the 19th century, with Welcker publishing, in 1816, an article defending Sappho from charges of prostitution and lesbianism, arguing that she was chaste – a position which would later be taken up by Wilamowitz at the end of the 19th and Henry Thornton Wharton at the beginning of the 20th centuries. Despite attempts to defend her good name, in the nineteenth century Sappho was co-opted by the Decadent Movement as a lesbian "daughter of de Sade", by Charles Baudelaire in France and later Algernon Charles Swinburne in England. By the late 19th century, lesbian writers such as Michael Field and Amy Levy became interested in Sappho for her sexuality, and by the turn of the twentieth century she was a sort of "patron saint of lesbians".

From the 19th century, Sappho began to be regarded as a role model for campaigners for women's rights, beginning with works such as Caroline Norton's "The Picture of Sappho". Later in that century, she would become a model for the so-called New Woman – independent and educated women who desired social and sexual autonomy – and by the 1960s, the feminist Sappho was – along with the hypersexual, often but not exclusively lesbian Sappho – one of the two most important cultural perceptions of Sappho.

The discoveries of new poems by Sappho in 2004 and 2014 excited both scholarly and media attention. The announcement of the Tithonus poem was the subject of international news coverage, and was described by Marylin Skinner as "the "trouvaille" of a lifetime". The publication in 2014 of the Brothers poem was described as "more exciting than a new album by David Bowie" by the "Daily Telegraph".





</doc>
<doc id="27786" url="https://en.wikipedia.org/wiki?curid=27786" title="Simon bar Kokhba">
Simon bar Kokhba

Simon bar Kokhba (; died 135 CE), born Simon ben Kosevah, was the leader of what is known as the Bar Kokhba revolt against the Roman Empire in 132 CE, establishing an independent Jewish state which he ruled for three years as "Nasi" ("Prince"). His state was conquered by the Romans in 135 following a two and half-year war.

Documents discovered in the 20th century in the Cave of Letters give his original name, with variations: Simeon bar Kosevah (), Bar Koseva () or Ben Koseva (). It is probable that his original name was Bar Koseva. The name may indicate that his father or his place of origin was named Koseva(h), but might as well be a general family name.

The Jewish sage Rabbi Akiva indulged the possibility that Simon could be the Jewish messiah, and gave him the surname "Bar Kokhba" meaning "Son of the Star" in Aramaic, from the Star Prophecy verse from Numbers : "There shall come a star out of Jacob". The name Bar Kokhba does not appear in the Talmud but in ecclesiastical sources. Rabbinical writers subsequent to Rabbi Akiva did not share Rabbi Akiva's estimation of ben Kosiva. Akiva's disciple, Yose ben Halaphta, in the Seder 'Olam (chapter 30) called him "bar Koziba" (), meaning, "son of the lie". The judgment of Bar Koseba that is implied by this change of name was carried on by later rabbinic scholarship at least to the time of the codification of the Talmud, where the name is always rendered "Simon bar Koziba" () or Bar Kozevah.

Despite the devastation wrought by the Romans during the First Jewish–Roman War (66–73 CE), which left the population and countryside in ruins, a series of laws passed by Roman Emperors provided the incentive for the second rebellion. Based on the delineation of years in Eusebius' "Chronicon" (now Chronicle of Jerome), it was only in the 16th year of Hadrian's reign, or what was equivalent to the 4th year of the 227th Olympiad, that the Jewish revolt began, under the Roman governor Tineius (Tynius) Rufus, whereas Hadrian sent an army to crush the resistance. Bar Kokhba, the leader of this resistance at the time, punished any Jew who refused to join his ranks. Two and a half years later, the war had ended. The Roman Emperor Hadrian at this time barred Jews from entering Jerusalem; a new Roman city, Aelia Capitolina, was to be built in its place.

The second Jewish rebellion took place 60 years after the first and established an independent state lasting three years. For many Jews of the time, this turn of events was heralded as the long hoped for Messianic Age. The excitement was short-lived, however, and after a brief span of glory, the revolt was crushed by the Roman legions.

The Romans fared very poorly during the initial revolt facing a unified Jewish force, in contrast to First Jewish-Roman War, where Flavius Josephus records three separate Jewish armies fighting each other for control of the Temple Mount during the three weeks after the Romans had breached Jerusalem's walls and were fighting their way to the center. Being outnumbered and taking heavy casualties, the Romans adopted a scorched earth policy which reduced and demoralized the Judean populace, slowly grinding away at the will of the Judeans to sustain the war.

Bar Kokhba took up refuge in the fortress of Betar. The Romans eventually captured it after laying siege to the city for three and a half years, and they killed all the defenders except for one Jewish youth whose life was spared, "viz." Simeon ben Gamliel. Rabbi Yohanan has related the following account of the massacre: “The brains of three-hundred children were found upon one stone, along with three-hundred baskets of what remained of phylacteries ("tefillin") were found in Betar, each and every one of which had the capacity to hold three measures (three "seahs", or what is equivalent to about 28 liters). If you should come to take [all of them] into account, you would find that they amounted to three-hundred measures.” Rabban [Shimon] Gamliel said: “Five-hundred schools were in Betar, while the smallest of them wasn't less than three-hundred children. They used to say, ‘If the enemy should ever come upon us, with these metal pointers [used in pointing at the letters of sacred writ] we'll go forth and stab them.’ But since iniquities had caused [their fall], the enemy came in and wrapped up each and every child in his own book and burnt them together, and no one remained except me.” According to Cassius Dio, 580,000 Jews were killed in overall war operations across the country, and some 50 fortified towns and 985 villages razed to the ground, while those who perished by famine, disease and fire was past finding out. So costly was the Roman victory that the Emperor Hadrian, when reporting to the Roman Senate, did not see fit to begin with the customary greeting “If you and your children are healthy, it is well; I and the legions are healthy.”

In the aftermath of the war, Hadrian consolidated the older political units of Judaea, Galilee and Samaria into the new province of Syria Palaestina, which is commonly interpreted as an attempt to complete the disassociation with Judaea.

Over the past few decades, new information about the revolt has come to light, from the discovery of several collections of letters, some possibly by Bar Kokhba himself, in the Cave of Letters overlooking the Dead Sea. These letters can now be seen at the Israel Museum.

According to Israeli archaeologist Yigael Yadin, Bar Kokhba tried to revive Hebrew and make Hebrew the official language of the Jews as part of his messianic ideology. In "A Roadmap to the Heavens: An Anthropological Study of Hegemony among Priests, Sages, and Laymen (Judaism and Jewish Life)" by Sigalit Ben-Zion (page 155), Yadin remarked: "it seems that this change came as a result of the order that was given by Bar Kokhba, who wanted to revive the Hebrew language and make it the official language of the state."

Simon bar Kokhba is portrayed in rabbinic literature as being somewhat irrational and irascible in conduct. The Talmud says that he presided over an army of Jewish insurgents numbering some 200,000, but had compelled its young recruits to prove their valor by each man chopping off one of his own fingers. The Sages of Israel complained to him why he marred the people of Israel with such blemishes. Whenever he would go forth into battle, he was reported as saying: "O Master of the universe, there is no need for you to assist us [against our enemies], but do not embarrass us either!" It is also said of him that he killed his maternal uncle, Rabbi Elazar Hamudaʻi, after suspecting him of collaborating with the enemy, thereby forfeiting Divine protection, which led to the destruction of Betar in which Bar Kokhba himself also perished.

Bar Kokhba was a ruthless leader, punishing any Jew who refused to join his ranks. According to Eusebius' "Chronicon" (now Chronicle of Jerome), he severely punished the sect of Christians with death by different means of torture for their refusal to fight against the Romans.

Since the end of the nineteenth century, Bar-Kochba has been the subject of numerous works of art (dramas, operas, novels, etc.), including:

Another operetta on the subject of Bar Kokhba was written by the Russian-Jewish emigre composer Yaacov Bilansky Levanon in Palestine in the 1920s.

John Zorn's Masada Chamber Ensemble recorded an album called "Bar Kokhba", showing a photograph of the Letter of Bar Kokhba to Yeshua, son of Galgola on the cover.

According to a legend, during his reign, Bar Kokhba was once presented a mutilated man, who had his tongue ripped out and hands cut off. Unable to talk or write, the victim was incapable of telling who his attackers were. Thus, Bar Kokhba decided to ask simple questions to which the dying man was able to nod or shake his head with his last movements; the murderers were consequently apprehended.

In Hungary, this legend spawned the "Bar Kokhba game", in which one of two players comes up with a word or object, while the other must figure it out by asking questions only to be answered with "yes" or "no". The questioner usually asks first if it is a living being, if not, if it is an object, if not, it is surely an abstraction. The verb "kibarkochbázni" ("to Bar Kochba out") became a common language verb meaning "retrieving information in an extremely tedious way".




</doc>
<doc id="27790" url="https://en.wikipedia.org/wiki?curid=27790" title="Schizophrenia">
Schizophrenia

Schizophrenia is a mental disorder characterized by abnormal social behavior and failure to understand reality. Common symptoms include false beliefs, unclear or confused thinking, hearing voices that others do not, reduced social engagement and emotional expression, and a lack of motivation. People with schizophrenia often have additional mental health problems such as anxiety, depressive, or substance-use disorders. Symptoms typically come on gradually, begin in young adulthood, and last a long time.
The causes of schizophrenia include environmental and genetic factors. Possible environmental factors include being raised in a city, cannabis use during adolescence, certain infections, parental age and poor nutrition during pregnancy. Genetic factors include a variety of common and rare genetic variants. Diagnosis is based on observed behavior, the person's reported experiences and reports of others familiar with the person. During diagnosis a person's culture must also be taken into account. As of 2013 there is no objective test. Schizophrenia does not imply a "split personality" or "dissociative identity disorder" – conditions with which it is often confused in public perception.
The mainstay of treatment is antipsychotic medication, along with counselling, job training and social rehabilitation. It is unclear whether typical or atypical antipsychotics are better. In those who do not improve with other antipsychotics clozapine may be tried. In more serious situations where there is risk to self or others involuntary hospitalization may be necessary, although hospital stays are now shorter and less frequent than they once were.
About 0.3 to 0.7% of people are affected by schizophrenia during their lifetimes. In 2013 there were an estimated 23.6 million cases globally. Males are more often affected, and on average experience more severe symptoms. About 20% of people eventually do well and a few recover completely; while about 50% have lifelong impairment. Social problems, such as long-term unemployment, poverty and homelessness are common. The average life expectancy of people with the disorder is ten to twenty five years less than for the general population. This is the result of increased physical health problems and a higher suicide rate (about 5%). In 2015 an estimated 17,000 people worldwide died from behavior related to, or caused by, schizophrenia.

Individuals with schizophrenia may experience hallucinations (most reported are hearing voices), delusions (often bizarre or persecutory in nature), and disorganized thinking and speech. The last may range from loss of train of thought, to sentences only loosely connected in meaning, to speech that is not understandable known as word salad. Social withdrawal, sloppiness of dress and hygiene, and loss of motivation and judgment are all common in schizophrenia.

Distortions of self-experience such as feeling as if one's thoughts or feelings are not really one's own to believing thoughts are being inserted into one's mind, sometimes termed passivity phenomena, are also common. There is often an observable pattern of emotional difficulty, for example lack of responsiveness. Impairment in social cognition is associated with schizophrenia, as are symptoms of paranoia. Social isolation commonly occurs. Difficulties in working and long-term memory, attention, executive functioning, and speed of processing also commonly occur. In one uncommon subtype, the person may be largely mute, remain motionless in bizarre postures, or exhibit purposeless agitation, all signs of catatonia. People with schizophrenia often find facial emotion perception to be difficult. It is unclear if the phenomenon called "thought blocking", where a talking person suddenly becomes silent for a few seconds to minutes, occurs in schizophrenia.

About 30 to 50 percent of people with schizophrenia fail to accept that they have an illness or comply with their recommended treatment. Treatment may have some effect on insight.

People with schizophrenia may have a high rate of irritable bowel syndrome but they often do not mention it unless specifically asked. Psychogenic polydipsia, or excessive fluid intake in the absence of physiological reasons to drink, is relatively common in people with schizophrenia.
Schizophrenia is often described in terms of positive and negative (or deficit) symptoms. "Positive symptoms" are those that most individuals do not normally experience, but are present in people with schizophrenia. They can include delusions, disordered thoughts and speech, and tactile, auditory, visual, olfactory and gustatory hallucinations, typically regarded as manifestations of psychosis. Hallucinations are also typically related to the content of the delusional theme. Positive symptoms generally respond well to medication.

"Negative symptoms" are deficits of normal emotional responses or of other thought processes, and are less responsive to medication. They commonly include flat expressions or little emotion, poverty of speech, inability to experience pleasure, lack of desire to form relationships, and lack of motivation. Negative symptoms appear to contribute more to poor quality of life, functional ability, and the burden on others than positive symptoms do. People with greater negative symptoms often have a history of poor adjustment before the onset of illness, and response to medication is often limited.

The validity of the positive and negative construct has been challenged by factor analysis studies observing a three dimension grouping of symptoms. While different terminology is used, a dimension for hallucinations, a dimension for disorganization, and a dimension for negative symptoms are usually described.

Deficits in cognitive abilities are widely recognized as a core feature of schizophrenia. The extent of the cognitive deficits an individual experiences is a predictor of how functional an individual will be, the quality of occupational performance, and how successful the individual will be in maintaining treatment. The presence and degree of cognitive dysfunction in individuals with schizophrenia has been reported to be a better indicator of functionality than the presentation of positive or negative symptoms. The deficits impacting the cognitive function are found in a large number of areas: working memory, long-term memory, verbal declarative memory, semantic processing, episodic memory, attention, learning (particularly verbal learning). Deficits in verbal memory are the most pronounced in individuals with schizophrenia, and are not accounted for by deficit in attention. Verbal memory impairment has been linked to a decreased ability in individuals with schizophrenia to semantically encode (process information relating to meaning), which is cited as a cause for another known deficit in long-term memory. When given a list of words, healthy individuals remember positive words more frequently (known as the Pollyanna principle); however, individuals with schizophrenia tend to remember all words equally regardless of their connotations, suggesting that the experience of anhedonia impairs the semantic encoding of the words. These deficits have been found in individuals before the onset of the illness to some extent. First-degree family members of individuals with schizophrenia and other high-risk individuals also show a degree of deficit in cognitive abilities, and specifically in working memory. A review of the literature on cognitive deficits in individuals with schizophrenia shows that the deficits may be present in early adolescence, or as early as childhood. The deficits which an individual with schizophrenia presents tend to remain the same over time in most patients, or follow an identifiable course based upon environmental variables.

Although the evidence that cognitive deficits remain stable over time is reliable and abundant, much of the research in this domain focuses on methods to improve attention and working memory. Efforts to improve learning ability in individuals with schizophrenia using a high- versus low-reward condition and an instruction-absent or instruction-present condition revealed that increasing reward leads to poorer performance while providing instruction leads to improved performance, highlighting that some treatments may exist to increase cognitive performance. Training individuals with schizophrenia to alter their thinking, attention, and language behaviors by verbalizing tasks, engaging in cognitive rehearsal, giving self-instructions, giving coping statements to the self to handle failure, and providing self-reinforcement for success, significantly improves performance on recall tasks. This type of training, known as self-instructional (SI) training, produced benefits such as lower number of nonsense verbalizations and improved recall while distracted.

Late adolescence and early adulthood are peak periods for the onset of schizophrenia, critical years in a young adult's social and vocational development. In 40% of men and 23% of women diagnosed with schizophrenia, the condition manifested itself before the age of 19. The onset of the disease is usually later in women than in men. To minimize the developmental disruption associated with schizophrenia, much work has recently been done to identify and treat the prodromal (pre-onset) phase of the illness, which has been detected up to 30 months before the onset of symptoms. Those who go on to develop schizophrenia may experience transient or self-limiting psychotic symptoms and the non-specific symptoms of social withdrawal, irritability, dysphoria, and clumsiness before the onset of the disease. Children who go on to develop schizophrenia may also demonstrate decreased intelligence, decreased motor development (reaching milestones such as walking slowly), isolated play preference, social anxiety, and poor school performance.

A combination of genetic and environmental factors play a role in the development of schizophrenia. People with a family history of schizophrenia who have a transient psychosis have a 20–40% chance of being diagnosed one year later.

Estimates of the heritability of schizophrenia is around 80%, which implies that 80% of the individual differences in risk to schizophrenia is explained by individual differences in genetics. These estimates vary because of the difficulty in separating genetic and environmental influences. The greatest single risk factor for developing schizophrenia is having a first-degree relative with the disease (risk is 6.5%); more than 40% of monozygotic twins of those with schizophrenia are also affected. If one parent is affected the risk is about 13% and if both are affected the risk is nearly 50%.

Many genes are known to be involved in schizophrenia, each of small effect and unknown transmission and expression. The summation of these effect sizes into a polygenic risk score can explain at least 7% of the variability in liability for schizophrenia. Around 5% of cases of schizophrenia are understood to be at least partially attributable to rare copy number variants (CNVs), including 22q11, 1q21 and 16p11. These rare CNVs increase the risk of an individual developing the disorder by as much as 20-fold, and are frequently comorbid with autism and intellectual disabilities. There is a genetic relation between the common variants which cause schizophrenia and bipolar disorder, an inverse genetic correlation with intelligence and no genetic correlation with immune disorders.

Environmental factors associated with the development of schizophrenia include the living environment, drug use, and prenatal stressors.

Maternal stress has been associated with an increased risk of schizophrenia, possibly in association with reelin. Maternal Stress has been observed to lead to hypermethylation and therefore under-expression of reelin, which in animal models leads to reduction in GABAergic neurons, a common finding in schizophrenia. Maternal nutritional deficiencies, such as those observed during a famine, as well as maternal obesity have also been identified as possible risk factors for schizophrenia. Both maternal stress and infection have been demonstrated to alter fetal neurodevelopment through pro-inflammatory proteins such as IL-8 and TNF.

Parenting style seems to have no major effect, although people with supportive parents do better than those with critical or hostile parents. Childhood trauma, death of a parent, and being bullied or abused increase the risk of psychosis. Living in an urban environment during childhood or as an adult has consistently been found to increase the risk of schizophrenia by a factor of two, even after taking into account drug use, ethnic group, and size of social group. Other factors that play an important role include social isolation and immigration related to social adversity, racial discrimination, family dysfunction, unemployment, and poor housing conditions.

It has been hypothesized that in some people, development of schizophrenia is related to intestinal tract dysfunction such as seen with non-celiac gluten sensitivity or abnormalities in the intestinal flora. A subgroup of persons with schizophrenia present an immune response to gluten different from that found in people with celiac, with elevated levels of certain serum biomarkers of gluten sensitivity such as anti-gliadin IgG or anti-gliadin IgA antibodies.

About half of those with schizophrenia use drugs or alcohol excessively.
Amphetamine, cocaine, and to a lesser extent alcohol, can result in a transient stimulant psychosis or alcohol-related psychosis that presents very similarly to schizophrenia. Although it is not generally believed to be a cause of the illness, people with schizophrenia use nicotine at much higher rates than the general population.

Alcohol abuse can occasionally cause the development of a chronic, substance-induced psychotic disorder via a kindling mechanism. Alcohol use is not associated with an earlier onset of psychosis.

Cannabis can be a contributory factor in schizophrenia, potentially causing the disease in those who are already at risk. The increased risk may require the presence of certain genes within an individual or may be related to preexisting psychopathology. Early exposure is strongly associated with an increased risk. The size of the increased risk is not clear, but appears to be in the range of two to three times greater for psychosis. Higher dosage and greater frequency of use are indicators of increased risk of chronic psychoses.

Other drugs may be used only as coping mechanisms by individuals who have schizophrenia, to deal with depression, anxiety, boredom, and loneliness.

Factors such as hypoxia and infection, or stress and malnutrition in the mother during fetal development, may result in a slight increase in the risk of schizophrenia later in life. People diagnosed with schizophrenia are more likely to have been born in winter or spring (at least in the northern hemisphere), which may be a result of increased rates of viral exposures in utero. The increased risk is about five to eight percent. Other infections during pregnancy or around the time of birth that may increase the risk include "Toxoplasma gondi" and "Chlamydia".

A number of attempts have been made to explain the link between altered brain function and schizophrenia. One of the most common is the dopamine hypothesis, which attributes psychosis to the mind's faulty interpretation of the misfiring of dopaminergic neurons.

Many psychological mechanisms have been implicated in the development and maintenance of schizophrenia. Cognitive biases have been identified in those with the diagnosis or those at risk, especially when under stress or in confusing situations. Some cognitive features may reflect global neurocognitive deficits such as memory loss, while others may be related to particular issues and experiences.

Despite a demonstrated appearance of blunted affect, recent findings indicate that many individuals diagnosed with schizophrenia are emotionally responsive, particularly to stressful or negative stimuli, and that such sensitivity may cause vulnerability to symptoms or to the disorder. Some evidence suggests that the content of delusional beliefs and psychotic experiences can reflect emotional causes of the disorder, and that how a person interprets such experiences can influence symptomatology. The use of "safety behaviors" (acts such as gestures or the use of words in specific contexts) to avoid or neutralize imagined threats may actually contribute to the chronicity of delusions. Further evidence for the role of psychological mechanisms comes from the effects of psychotherapies on symptoms of schizophrenia.

Schizophrenia is associated with subtle differences in brain structures, found in forty to fifty percent of cases, and in brain chemistry during acute psychotic states. Studies using neuropsychological tests and brain imaging technologies such as fMRI and PET to examine functional differences in brain activity have shown that differences seem to occur most commonly in the frontal lobes, hippocampus, and temporal lobes. Reductions in brain volume are most pronounced in grey matter structures, and correlate with duration of illness, although white matter abnormalities have also been found. A progressive increase in ventricular volume as well as a progressive reduction in grey matter in the frontal, parietal, and temporal lobes has also been observed. These differences have been linked to the neurocognitive deficits often associated with schizophrenia. Because neural circuits are altered, it has alternatively been suggested that schizophrenia could be thought of as a neurodevelopmental disorder with psychosis occurring as a possibly preventable late stage. There has been debate on whether treatment with antipsychotics can itself cause reduction of brain volume.

Particular attention has been paid to the function of dopamine in the mesolimbic pathway of the brain. This focus largely resulted from the accidental finding that phenothiazine drugs, which block dopamine function, could reduce psychotic symptoms. It is also supported by the fact that amphetamines, which trigger the release of dopamine, may exacerbate the psychotic symptoms in schizophrenia. The influential dopamine hypothesis of schizophrenia proposed that excessive activation of D receptors was the cause of (the positive symptoms of) schizophrenia. Although postulated for about 20 years based on the D blockade effect common to all antipsychotics, it was not until the mid-1990s that PET and SPET imaging studies provided supporting evidence. While dopamine D2/D3 receptors are elevated in schizophrenia, the effect size is small, and only evident in medication naive schizophrenics. On the other hand, presynaptic dopamine metabolism and release is elevated despite no difference in dopamine transporter. The altered synthesis of dopamine in the nigrostriatal system have been confirmed in several human studies. Hypoactivity of dopamine D1 receptor activation in the prefrontal cortex has also been observed. The hyperactivity of D2 receptor stimulation and relative hypoactivity of D1 receptor stimulation is thought to contribute to cognitive dysfunction by disrupting signal to noise ratio in cortical microcircuits. The dopamine hypothesis is now thought to be simplistic, partly because newer antipsychotic medication (atypical antipsychotic medication) can be just as effective as older medication (typical antipsychotic medication), but also affects serotonin function and may have slightly less of a dopamine blocking effect.

Interest has also focused on the neurotransmitter glutamate and the reduced function of the NMDA glutamate receptor in schizophrenia, largely because of the abnormally low levels of glutamate receptors found in the postmortem brains of those diagnosed with schizophrenia, and the discovery that glutamate-blocking drugs such as phencyclidine and ketamine can mimic the symptoms and cognitive problems associated with the condition. Reduced glutamate function is linked to poor performance on tests requiring frontal lobe and hippocampal function, and glutamate can affect dopamine function, both of which have been implicated in schizophrenia; this has suggested an important mediating (and possibly causal) role of glutamate pathways in the condition. But positive symptoms fail to respond to glutamatergic medication. Closely related to evidence of glutamate dysfunction in schizophrenia is the observed changes GABAergic transmission. Post-Mortem studies demonstrate decreased expression of GAD67, GAT-1 and GABA receptor subunits in the prefrontal cortex, although this appears to be restricted to a certain subsets of parvalbumin containing GABAergic neurons. While in vivo imaging of GABAergic signaling appears to be moderately reduced, this may be dependent upon treatment and disease stage.

Schizophrenia is diagnosed based on criteria in either the American Psychiatric Association's (APA) fifth edition of the "Diagnostic and Statistical Manual of Mental Disorders" (DSM 5), or the World Health Organization's International Statistical Classification of Diseases and Related Health Problems (ICD-10). These criteria use the self-reported experiences of the person and reported abnormalities in behavior, followed by a clinical assessment by a mental health professional. Symptoms associated with schizophrenia occur along a continuum in the population and must reach a certain severity and level of impairment, before a diagnosis is made. As of 2013 there is no objective test.

In 2013, the American Psychiatric Association released the fifth edition of the DSM (DSM-5). To be diagnosed with schizophrenia, two diagnostic criteria have to be met over much of the time of a period of at least one month, with a significant impact on social or occupational functioning for at least six months. The person had to be suffering from delusions, hallucinations, or disorganized speech. A second symptom could be negative symptoms, or severely disorganized or catatonic behaviour. The definition of schizophrenia remained essentially the same as that specified by the 2000 version of DSM (DSM-IV-TR), but DSM-5 makes a number of changes.

The ICD-10 criteria are typically used in European countries, while the DSM criteria are used in the United States and to varying degrees around the world, and are prevailing in research studies. The ICD-10 criteria put more emphasis on Schneiderian first-rank symptoms. In practice, agreement between the two systems is high. The current proposal for the ICD-11 criteria for schizophrenia recommends adding self-disorder as a symptom.

If signs of disturbance are present for more than a month but less than six months, the diagnosis of schizophreniform disorder is applied. Psychotic symptoms lasting less than a month may be diagnosed as brief psychotic disorder, and various conditions may be classed as psychotic disorder not otherwise specified, while schizoaffective disorder is diagnosed if symptoms of mood disorder are substantially present alongside psychotic symptoms. If the psychotic symptoms are the direct physiological result of a general medical condition or a substance, then the diagnosis is one of a psychosis secondary to that condition. Schizophrenia is not diagnosed if symptoms of pervasive developmental disorder are present unless prominent delusions or hallucinations are also present.

With the publication of DSM-5, the APA removed all sub-classifications of schizophrenia. The five sub-classifications included in DSM-IV-TR were:

The ICD-10 defines additional subtypes:

Psychotic symptoms may be present in several other mental disorders, including bipolar disorder, borderline personality disorder, drug intoxication, and drug-induced psychosis. Delusions ("non-bizarre") are also present in delusional disorder, and social withdrawal in social anxiety disorder, avoidant personality disorder and schizotypal personality disorder. Schizotypal personality disorder has symptoms that are similar but less severe than those of schizophrenia. Schizophrenia occurs along with obsessive-compulsive disorder (OCD) considerably more often than could be explained by chance, although it can be difficult to distinguish obsessions that occur in OCD from the delusions of schizophrenia. A few people withdrawing from benzodiazepines experience a severe withdrawal syndrome which may last a long time. It can resemble schizophrenia and be misdiagnosed as such.

A more general medical and neurological examination may be needed to rule out medical illnesses which may rarely produce psychotic schizophrenia-like symptoms, such as metabolic disturbance, systemic infection, syphilis, AIDS dementia complex, epilepsy, limbic encephalitis, and brain lesions. Stroke, multiple sclerosis, hyperthyroidism, hypothyroidism, and dementias such as Alzheimer's disease, Huntington's disease, frontotemporal dementia, and the Lewy body dementias may also be associated with schizophrenia-like psychotic symptoms. It may be necessary to rule out a delirium, which can be distinguished by visual hallucinations, acute onset and fluctuating level of consciousness, and indicates an underlying medical illness. Investigations are not generally repeated for relapse unless there is a specific "medical" indication or possible adverse effects from antipsychotic medication. In children hallucinations must be separated from typical childhood fantasies.

Prevention of schizophrenia is difficult as there are no reliable markers for the later development of the disorder. There is tentative evidence for the effectiveness of early interventions to prevent schizophrenia. While there is some evidence that early intervention in those with a psychotic episode may improve short-term outcomes, there is little benefit from these measures after five years. Attempting to prevent schizophrenia in the prodrome phase is of uncertain benefit and therefore as of 2009 is not recommended. Cognitive behavioral therapy may reduce the risk of psychosis in those at high risk after a year and is recommended in this group, by the National Institute for Health and Care Excellence (NICE). Another preventative measure is to avoid drugs that have been associated with development of the disorder, including cannabis, cocaine, and amphetamines.

The primary treatment of schizophrenia is antipsychotic medications, often in combination with psychological and social supports. Hospitalization may occur for severe episodes either voluntarily or (if mental health legislation allows it) involuntarily. Long-term hospitalization is uncommon since deinstitutionalization beginning in the 1950s, although it still occurs. Community support services including drop-in centers, visits by members of a community mental health team, supported employment and support groups are common. Some evidence indicates that regular exercise has a positive effect on the physical and mental health of those with schizophrenia. As of 2015 it is unclear if transcranial magnetic stimulation (TMS) is useful for schizophrenia.

The first-line psychiatric treatment for schizophrenia is antipsychotic medication, which can reduce the positive symptoms of psychosis in about 7 to 14 days. Antipsychotics, however, fail to significantly improve the negative symptoms and cognitive dysfunction. In those on antipsychotics, continued use decreases the risk of relapse. There is little evidence regarding effects from their use beyond two or three years. However use of anti-psychotics can lead to dopamine hypersensitivity increasing the risk of symptoms if antipsychotics are stopped.

The choice of which antipsychotic to use is based on benefits, risks, and costs. It is debatable whether, as a class, typical or atypical antipsychotics are better. Amisulpride, olanzapine, risperidone, and clozapine may be more effective but are associated with greater side effects. Typical antipsychotics have equal drop-out and symptom relapse rates to atypicals when used at low to moderate dosages. There is a good response in 40–50%, a partial response in 30–40%, and treatment resistance (failure of symptoms to respond satisfactorily after six weeks to two or three different antipsychotics) in 20% of people. Clozapine is an effective treatment for those who respond poorly to other drugs ("treatment-resistant" or "refractory" schizophrenia), but it has the potentially serious side effect of agranulocytosis (lowered white blood cell count) in less than 4% of people.

Most people on antipsychotics have side effects. People on typical antipsychotics tend to have a higher rate of extrapyramidal side effects, while some atypicals are associated with considerable weight gain, diabetes and risk of metabolic syndrome; this is most pronounced with olanzapine, while risperidone and quetiapine are also associated with weight gain. Risperidone has a similar rate of extrapyramidal symptoms to haloperidol. It remains unclear whether the newer antipsychotics reduce the chances of developing neuroleptic malignant syndrome or tardive dyskinesia, a rare but serious neurological disorder.

For people who are unwilling or unable to take medication regularly, long-acting depot preparations of antipsychotics may be used to achieve control. They reduce the risk of relapse to a greater degree than oral medications. When used in combination with psychosocial interventions, they may improve long-term adherence to treatment. The American Psychiatric Association suggests considering stopping antipsychotics in some people if there are no symptoms for more than a year.

A number of psychosocial interventions may be useful in the treatment of schizophrenia including: family therapy, assertive community treatment, supported employment, cognitive remediation, skills training, token economic interventions, and psychosocial interventions for substance use and weight management. Family therapy or education, which addresses the whole family system of an individual, may reduce relapses and hospitalizations. Evidence for the effectiveness of cognitive-behavioral therapy (CBT) in either reducing symptoms or preventing relapse is minimal. Evidence for metacognitive training is mixed with some reviews finding benefit and another not. Art or drama therapy have not been well-researched.

Schizophrenia has great human and economic costs. It results in a decreased life expectancy by 10–25 years. This is primarily because of its association with obesity, poor diet, sedentary lifestyles, and smoking, with an increased rate of suicide playing a lesser role. Antipsychotic medications may also increase the risk. These differences in life expectancy increased between the 1970s and 1990s.
Schizophrenia is a major cause of disability, with active psychosis ranked as the third-most-disabling condition after quadriplegia and dementia and ahead of paraplegia and blindness. Approximately three-fourths of people with schizophrenia have ongoing disability with relapses and 16.7 million people globally are deemed to have moderate or severe disability from the condition. Some people do recover completely and others function well in society. Most people with schizophrenia live independently with community support. About 85% are unemployed. Some evidence suggests that paranoid schizophrenia may have a better prospect than other types of schizophrenia for independent living and occupational functioning. In people with a first episode of psychosis a good long-term outcome occurs in 42%, an intermediate outcome in 35% and a poor outcome in 27%. Outcomes for schizophrenia appear better in the developing than the developed world. These conclusions, however, have been questioned.
There is a higher than average suicide rate associated with schizophrenia. This has been cited at 10%, but a more recent analysis revises the estimate to 4.9%, most often occurring in the period following onset or first hospital admission. Several times more (20 to 40%) attempt suicide at least once. There are a variety of risk factors, including male gender, depression, and a high intelligence quotient.
Schizophrenia and smoking have shown a strong association in studies worldwide. Use of cigarettes is especially high in individuals diagnosed with schizophrenia, with estimates ranging from 80 to 90% being regular smokers, as compared to 20% of the general population. Those who smoke tend to smoke heavily, and additionally smoke cigarettes with high nicotine content. Among people with schizophrenia use of cannabis is also common.

Schizophrenia affects around 0.3–0.7% of people at some point in their life, or 24 million people worldwide as of 2011. It occurs 1.4 times more frequently in males than females and typically appears earlier in men—the peak ages of onset are 25 years for males and 27 years for females. Onset in childhood is much rarer, as is onset in middle or old age.

Despite the prior belief that schizophrenia occurs at similar rates worldwide, its frequency varies across the world, within countries, and at the local and neighborhood level. This variation has been estimated to be fivefold. It causes approximately one percent of worldwide disability adjusted life years and resulted in 20,000 deaths in 2010. The rate of schizophrenia varies up to threefold depending on how it is defined.

In 2000, the World Health Organization found the percentage of people affected and the number of new cases that develop each year is roughly similar around the world, with age-standardized prevalence per 100,000 ranging from 343 in Africa to 544 in Japan and Oceania for men, and from 378 in Africa to 527 in Southeastern Europe for women. About 1.1% of adults have schizophrenia in the United States.

In the early 20th century, the psychiatrist Kurt Schneider listed the forms of psychotic symptoms that he thought distinguished schizophrenia from other psychotic disorders. These are called "first-rank symptoms" or Schneider's first-rank symptoms. They include delusions of being controlled by an external force, the belief that thoughts are being inserted into or withdrawn from one's conscious mind, the belief that one's thoughts are being broadcast to other people, and hearing hallucinatory voices that comment on one's thoughts or actions or that have a conversation with other hallucinated voices. Although they have significantly contributed to the current diagnostic criteria, the specificity of first-rank symptoms has been questioned. A review of the diagnostic studies conducted between 1970 and 2005 found that they allow neither a reconfirmation nor a rejection of Schneider's claims, and suggested that first-rank symptoms should be de-emphasized in future revisions of diagnostic systems. The absence of first-rank symptoms should raise suspicion of a medical disorder, however.

The history of schizophrenia is complex and does not lend itself easily to a linear narrative. Accounts of a schizophrenia-like syndrome are thought to be rare in historical records before the 19th century, although reports of irrational, unintelligible, or uncontrolled behavior were common. A detailed case report in 1797 concerning James Tilly Matthews, and accounts by Philippe Pinel published in 1809, are often regarded as the earliest cases of the illness in the medical and psychiatric literature. The Latinized term "dementia praecox" was first used by German alienist Heinrich Schule in 1886 and then in 1891 by Arnold Pick in a case report of a psychotic disorder (hebephrenia). In 1893 Emil Kraepelin borrowed the term from Schule and Pick and in 1899 introduced a broad new distinction in the classification of mental disorders between "dementia praecox" and mood disorder (termed manic depression and including both unipolar and bipolar depression). Kraepelin believed that "dementia praecox" was probably caused by a long-term, smouldering systemic or "whole body" disease process that affected many organs and peripheral nerves in the body but which affected the brain after puberty in a final decisive cascade. His use of the term "praecox" distinguished it from other forms of dementia such as Alzheimer's disease which typically occur later in life. It is sometimes argued that the use of the term "démence précoce" in 1852 by the French physician Bénédict Morel constitutes the medical discovery of schizophrenia. However, this account ignores the fact that there is little to connect Morel's descriptive use of the term and the independent development of the "dementia praecox" disease concept at the end of the nineteenth century.
The word "schizophrenia"—which translates roughly as "splitting of the mind" and comes from the Greek roots "schizein" (σχίζειν, "to split") and "phrēn", "phren-" (φρήν, φρεν-, "mind")—was coined by Eugen Bleuler in 1908 and was intended to describe the separation of function between personality, thinking, memory, and perception. American and British interpretations of Bleuler led to the claim that he described its main symptoms as four "A"s: flattened "affect", "autism", impaired "association" of ideas, and "ambivalence". Bleuler realized that the illness was "not" a dementia, as some of his patients improved rather than deteriorated, and thus proposed the term schizophrenia instead. Treatment was revolutionized in the mid-1950s with the development and introduction of chlorpromazine.

In the early 1970s, the diagnostic criteria for schizophrenia were the subject of a number of controversies which eventually led to the operational criteria used today. It became clear after the 1971 US–UK Diagnostic Study that schizophrenia was diagnosed to a far greater extent in America than in Europe. This was partly due to looser diagnostic criteria in the US, which used the DSM-II manual, contrasting with Europe and its ICD-9. David Rosenhan's 1972 study, published in the journal "Science" under the title "On being sane in insane places", concluded that the diagnosis of schizophrenia in the US was often subjective and unreliable. These were some of the factors leading to the revision not only of the diagnosis of schizophrenia, but the revision of the whole DSM manual, resulting in the publication of the DSM-III in 1980.

The term schizophrenia is commonly misunderstood to mean that affected persons have a "split personality". Although some people diagnosed with schizophrenia may hear voices and may experience the voices as distinct personalities, schizophrenia does not involve a person changing among distinct, multiple personalities; the confusion arises in part due to the literal interpretation of Bleuler's term "schizophrenia" (Bleuler originally associated schizophrenia with dissociation, and included split personality in his category of schizophrenia). Dissociative identity disorder (having a "split personality") was also often misdiagnosed as schizophrenia based on the loose criteria in the DSM-II. The first known misuse of the term to mean "split personality" was in an article by the poet T. S. Eliot in 1933. Other scholars have traced earlier roots. Rather, the term means a "splitting of mental functions", reflecting the presentation of the illness.

In 2002, the term for schizophrenia in Japan was changed from to to reduce stigma. The new name was inspired by the biopsychosocial model; it increased the percentage of people who were informed of the diagnosis from 37 to 70% over three years. A similar change was made in South Korea in 2012. A professor of psychiatry, Jim van Os, has proposed changing the English term to "psychosis spectrum syndrome".

In the United States, the cost of schizophrenia—including direct costs (outpatient, inpatient, drugs, and long-term care) and non-health care costs (law enforcement, reduced workplace productivity, and unemployment)—was estimated to be $62.7 billion in 2002. The book and film "A Beautiful Mind" chronicles the life of John Forbes Nash, a mathematician who won the Nobel Prize for Economics and was diagnosed with schizophrenia.

Individuals with severe mental illness, including schizophrenia, are at a significantly greater risk of being "victims" of both violent and non-violent crime. Schizophrenia has been associated with a higher rate of violent acts, but most appear to be related to associated substance abuse. Rates of homicide linked to psychosis are similar to those linked to substance misuse, and parallel the overall rate in a region. What role schizophrenia has on violence independent of drug misuse is controversial, but certain aspects of individual histories or mental states may be factors. About 11% of people in prison for homicide have schizophrenia while 21% have mood disorders. Another study found about 8-10% of people with schizophrenia had committed a violent act in the past year compared to 2% of the general population.

Media coverage relating to violent acts by individuals with schizophrenia reinforces public perception of an association between schizophrenia and violence. In a large, representative sample from a 1999 study, 12.8% of Americans believed that individuals with schizophrenia were "very likely" to do something violent against others, and 48.1% said that they were "somewhat likely" to. Over 74% said that people with schizophrenia were either "not very able" or "not able at all" to make decisions concerning their treatment, and 70.2% said the same of money-management decisions. The perception of individuals with psychosis as violent has more than doubled in prevalence since the 1950s, according to one meta-analysis.

Research has found a tentative benefit in using minocycline to treat schizophrenia. Nidotherapy or efforts to change the environment of people with schizophrenia to improve their ability to function, is also being studied; however, there is not enough evidence yet to make conclusions about its effectiveness. Negative symptoms have proven a challenge to treat, as they are generally not made better by medication. Various agents have been explored for possible benefits in this area. There have been trials on drugs with anti-inflammatory activity, based on the premise that inflammation might play a role in the pathology of schizophrenia.



</doc>
<doc id="27791" url="https://en.wikipedia.org/wiki?curid=27791" title="Sophie Germain">
Sophie Germain

Marie-Sophie Germain (; 1 April 1776 – 27 June 1831) was a French mathematician, physicist, and philosopher. Despite initial opposition from her parents and difficulties presented by society, she gained education from books in her father's library including ones by Leonhard Euler and from correspondence with famous mathematicians such as Lagrange, Legendre, and Gauss. One of the pioneers of elasticity theory, she won the grand prize from the Paris Academy of Sciences for her essay on the subject. Her work on Fermat's Last Theorem provided a foundation for mathematicians exploring the subject for hundreds of years after. Because of prejudice against her sex, she was unable to make a career out of mathematics, but she worked independently throughout her life. Before her death Gauss had recommended that she be awarded an honorary degree, but that never occurred. On June 27, 1831, she died from breast cancer . At the centenary of her life, a street and a girls’ school were named after her. The Academy of Sciences established the Sophie Germain Prize in her honor.

Marie-Sophie Germain was born on April 1, 1776, in Paris, France, in a house on Rue Saint-Denis. According to most sources, her father, Ambroise-François, was a wealthy silk merchant, though some believe he was a goldsmith. In 1789, he was elected as a representative of the bourgeoisie to the États-Généraux, which he saw change into the Constitutional Assembly. It is therefore assumed that Sophie witnessed many discussions between her father and his friends on politics and philosophy. Gray proposes that after his political career, Ambroise-François became the director of a bank; at least, the family remained well-off enough to support Germain throughout her adult life.

Marie-Sophie had one younger sister, named Angélique-Ambroise, and one older sister, named Marie-Madeline. Her mother was also named Marie-Madeline, and this plethora of "Maries" may have been the reason she went by Sophie. Germain's nephew Armand-Jacques Lherbette, Marie-Madeline's son, published some of Germain's work after she died (see Work in Philosophy).

When Germain was 13, the Bastille fell, and the revolutionary atmosphere of the city forced her to stay inside. For entertainment she turned to her father's library. Here she found J. E. Montucla's "L'Histoire des Mathématiques", and his story of the death of Archimedes intrigued her.

Sophie Germain thought that if the geometry method, which at that time referred to all of pure mathematics, could hold such fascination for Archimedes, it was a subject worthy of study. So she pored over every book on mathematics in her father's library, even teaching herself Latin and Greek so she could read works like those of Sir Isaac Newton and Leonhard Euler. She also enjoyed "Traité d'Arithmétique" by Étienne Bézout and "Le Calcul Différentiel" by Jacques Antoine-Joseph Cousin. Later, Cousin visited her in her house, encouraging her in her studies.

Germain's parents did not at all approve of her sudden fascination with mathematics, which was then thought inappropriate for a woman. When night came, they would deny her warm clothes and a fire for her bedroom to try to keep her from studying, but after they left she would take out candles, wrap herself in quilts and do mathematics. As Lynn Osen describes, when her parents found Sophie "asleep at her desk in the morning, the ink frozen in the ink horn and her slate covered with calculations," they realized that their daughter was serious and relented. After some time, her mother even secretly supported her.

In 1794, when Germain was 18, the École Polytechnique opened. As a woman, Germain was barred from attending, but the new system of education made the "lecture notes available to all who asked." The new method also required the students to "submit written observations." Germain obtained the lecture notes and began sending her work to Joseph Louis Lagrange, a faculty member. She used the name of a former student Monsieur Antoine-August Le Blanc, "fearing," as she later explained to Gauss, "the ridicule attached to a female scientist." When Lagrange saw the intelligence of M. LeBlanc, he requested a meeting, and thus Sophie was forced to disclose her true identity. Fortunately, Lagrange did not mind that Germain was a woman, and he became her mentor. He visited her in her home, giving her moral support.

Germain first became interested in number theory in 1798 when Adrien-Marie Legendre published "Essai sur la théorie des nombres". After studying the work, she opened correspondence with him on number theory, and later, elasticity. Legendre showed some of Germain's work in the "Supplément" to his second edition of the "Théorie des Nombres", where he calls it "très ingénieuse" ["very ingenious"] (See Her work on Fermat's Last Theorem).

Germain's interest in number theory was renewed when she read Carl Friedrich Gauss' monumental work "Disquisitiones Arithmeticae". After three years of working through the exercises and trying her own proofs for some of the theorems, she wrote, again under the pseudonym of M. LeBlanc, to the author himself, who was one year younger than she. The first letter, dated 21 November 1804, discussed Gauss' "Disquisitiones" and presented some of Germain's work on Fermat's Last Theorem. In the letter, Germain claimed to have proved the theorem for "n" = "p" – 1, where "p" is a prime number of the form "p" = 8"k" + 7. However, her proof contained a weak assumption, and Gauss' reply did not comment on Germain's proof.

Around 1807 (sources differ), during the Napoleonic wars, the French were occupying the German town of Braunschweig, where Gauss lived. Germain, concerned that he might suffer the fate of Archimedes, wrote to General Pernety, a family friend, requesting that he ensure Gauss' safety. General Pernety sent a chief of a battalion to meet with Gauss personally to see that he was safe. As it turned out, Gauss was fine, but he was confused by the mention of Sophie's name.

Three months after the incident, Germain disclosed her true identity to Gauss. He replied,
How can I describe my astonishment and admiration on seeing my esteemed correspondent M leBlanc metamorphosed into this celebrated person. . . when a woman, because of her sex, our customs and prejudices, encounters infinitely more obstacles than men in familiarising herself with [number theory's] knotty problems, yet overcomes these fetters and penetrates that which is most hidden, she doubtless has the most noble courage, extraordinary talent, and superior genius.
Gauss' letters to Olbers show that his praise for Germain was sincere. In the same 1807 letter, Sophie claimed that if formula_1 is of the form formula_2 then formula_3 is also of that form. Gauss replied with a counterexample: formula_4 can be written as formula_2 but formula_6 cannot.

Although Gauss thought well of Germain, his replies to her letters were often delayed, and he generally did not review her work. Eventually his interests turned away from number theory, and in 1809 the letters ceased. Despite the friendship of Germain and Gauss, they never met.

When Germain's correspondence with Gauss ceased, she took interest in a contest sponsored by the Paris Academy of Sciences concerning Ernst Chladni's experiments with vibrating metal plates. The object of the competition, as stated by the Academy, was "to give the mathematical theory of the vibration of an elastic surface and to compare the theory to experimental evidence." Lagrange's comment that a solution to the problem would require the invention of a new branch of analysis deterred all but two contestants, Denis Poisson and Germain. Then Poisson was elected to the Academy, thus becoming a judge instead of a contestant, and leaving Germain as the only entrant to the competition.

In 1809 Germain began work. Legendre assisted by giving her equations, references, and current research. She submitted her paper early in the fall of 1811, and did not win the prize. The judging commission felt that "the true equations of the movement were not established," even though "the experiments presented ingenious results." Lagrange was able to use Germain's work to derive an equation that was "correct under special assumptions."

The contest was extended by two years, and Germain decided to try again for the prize. At first Legendre continued to offer support, but then he refused all help. Germain's anonymous 1813 submission was still littered with mathematical errors, especially involving double integrals, and it received only an honorable mention because "the fundamental base of the theory [of elastic surfaces] was not established." The contest was extended once more, and Germain began work on her third attempt. This time she consulted with Poisson. In 1814 he published his own work on elasticity, and did not acknowledge Germain's help (although he had worked with her on the subject and, as a judge on the Academy commission, had had access to her work).

Germain submitted her third paper, "Recherches sur la théorie des surfaces élastiques" under her own name, and on 8 January 1816 she became the first woman to win a prize from the Paris Academy of Sciences. She did not appear at the ceremony to receive her award. Although Germain had at last been awarded the "prix extraordinaire", the Academy was still not fully satisfied. Sophie had derived the correct differential equation, but her method did not predict experimental results with great accuracy, as she had relied on an incorrect equation from Euler, which led to incorrect boundary conditions. Here is Germain's final equation:

where "N" is a constant.

After winning the Academy contest, she was still not able to attend its sessions because of the Academy's tradition of excluding women other than the wives of members. Seven years later this situation was transformed when she made friends with Joseph Fourier, a secretary of the Academy, who obtained tickets to the sessions for her.

Germain published her prize-winning essay at her own expense in 1821, mostly because she wanted to present her work in opposition to that of Poisson. In the essay she pointed out some of the errors in her method.

In 1826 she submitted a revised version of her 1821 essay to the Academy. According to Andrea Del Centina, the revision included attempts to clarify her work by "introducing certain simplifying hypotheses." This put the Academy in an awkward position, as they felt the paper to be "inadequate and trivial," but they did not want to "treat her as a professional colleague, as they would any man, by simply rejecting the work." So Augustin-Louis Cauchy, who had been appointed to review her work, recommended she publish it, and she followed his advice.

One further work of Germain's on elasticity was published posthumously in 1831: her "Mémoire sur la courbure des surfaces." She used the mean curvature in her research (see Honors in Number Theory).

Germain's best work was in number theory, and her most significant contribution to number theory dealt with Fermat's Last Theorem. In 1815, after the elasticity contest, the Academy offered a prize for a proof of Fermat's Last Theorem. It reawakened Germain's interest in number theory, and she wrote to Gauss again after ten years of no correspondence.

In the letter, Germain said that number theory was her preferred field, and that it was in her mind all the time she was studying elasticity. She outlined a strategy for a general proof of Fermat's Last Theorem, including a proof for a special case. Germain's letter to Gauss contained her substantial progress toward a proof. She asked Gauss if her approach to the theorem was worth pursuing. Gauss never answered.

Fermat's Last Theorem can be divided into two cases. Case 1 involves all "p" that do not divide any of "x", "y", or "z". Case 2 includes all "p" that divide at least one of "x", "y", or "z". Germain proposed the following, commonly called "Sophie Germain's theorem":
Let "p" be an odd prime. If there exists an auxiliary prime "P" = 2"Np" + 1 (N any positive integer not divisible by 3) such that:
Then the first case of Fermat's Last Theorem holds true for "p".
Germain used this result to prove the first case of Fermat's Last Theorem for all odd primes "p"<100, but according to Andrea Del Centina, "she had actually shown that it holds for every exponent "p"<197." L. E. Dickson later used Germain's theorem to prove Fermat's Last Theorem for odd primes less than 1700.

In an unpublished manuscript entitled "Remarque sur l'impossibilité de satisfaire en nombres entiers a l'équation x + y = z", Germain showed that any counterexamples to Fermat's theorem for "p">5 must be numbers "whose size frightens the imagination," around 40 digits long. Sophie did not publish this work. Her brilliant theorem is known only because of the footnote in Legendre's treatise on number theory, where he used it to prove Fermat's Last Theorem for "p" = 5 (see Correspondence with Legendre). Germain also proved or nearly proved several results that were attributed to Lagrange or were rediscovered years later. Del Centina states that "after almost two hundred years her ideas were still central", but ultimately her method did not work.

In addition to mathematics, Germain studied philosophy and psychology. She wanted to classify facts and generalize them into laws that could form a system of psychology and sociology, which were then just coming into existence. Her philosophy was highly praised by Auguste Comte.

Two of her philosophical works, "Pensées diverses" and "Considérations générales sur l'état des sciences et des lettres, aux différentes époques de leur culture", were published, both posthumously. This was due in part to the efforts of Lherbette, her nephew, who collected her philosophical writings and published them. "Pensées" is a history of science and mathematics with Sophie's commentary. In "Considérations", the work admired by Comte, Sophie argues that there are no differences between the sciences and the humanities.

In 1829 Germain learned she had breast cancer. Despite the pain, she continued to work. In 1831 "Crelle's Journal" published her paper on the curvature of elastic surfaces and "a note about finding and in formula_8." Mary Gray records, "She also published in "Annales de chimie et de physique" an examination of principles which led to the discovery of the laws of equilibrium and movement of elastic solids." On 27 June 1831, she died in the house at 13 rue de Savoie.

Despite Germain's intellectual achievements, her death certificate lists her as a "rentière – annuitant" (property holder), not a "mathématicienne." But her work was not unappreciated by everyone. When the matter of honorary degrees came up at the University of Göttingen in 1837—six years after Germain's death—Gauss lamented, "she [Germain] proved to the world that even a woman can accomplish something worthwhile in the most rigorous and abstract of the sciences and for that reason would well have deserved an honorary degree."

Germain's resting place in the Père Lachaise Cemetery in Paris is marked by a gravestone. At the centennial celebration of her life, a street and a girls' school were named after her, and a plaque was placed at the house where she died. The school houses a bust commissioned by the Paris City Council.

E. Dubouis defined a "sophien" of a prime to be a prime where , for such that yield such that has no solutions when and are prime to .

A Sophie Germain prime is a prime such that is also prime.

The "Germain curvature" (also called mean curvature) is formula_9, when and are the maximum and minimum values of the normal curvature.

"Sophie Germain's Identity" states that for any }, then,

Vesna Petrovich found that the educated world's response to the publication in 1821 of Germain's prize-winning essay "ranged from polite to indifferent". Yet, some critics had high praise for it. Of her essay in 1821, Cauchy said, "[it] was a work for which the name of its author and the importance of the subject both deserved the attention of mathematicians." Germain was also included in H. J. Mozans' book "Woman in Science", although Marilyn Bailey Ogilvie claims that the biography "is inaccurate and the notes and bibliography are unreliable". Nevertheless, it quotes the mathematician Claude-Louis Navier as saying, "it is a work which few men are able to read and which only one woman was able to write."

Germain's contemporaries also had good things to say relating to her work in mathematics. Osen relates that "Baron de Prony called her the Hypatia of the nineteenth century," and "J.J Biot wrote, in the "Journal de Savants", that she had probably penetrated the science of mathematics more deeply than any other of her sex." Gauss certainly thought highly of her, and he recognized that European culture presented special difficulties to a woman in mathematics (see Correspondence with Gauss).

The modern view generally acknowledges that although Germain had great talent as a mathematician, her haphazard education had left her without the strong base she needed to truly excel. As explained by Gray, "Germain's work in elasticity suffered generally from an absence of rigor, which might be attributed to her lack of formal training in the rudiments of analysis." Petrovich adds, "This proved to be a major handicap when she could no longer be regarded as a young prodigy to be admired but was judged by her peer mathematicians."

Not withstanding the problems with Germain's theory of vibrations, Gray states that "Germain's work was fundamental in the development of a general theory of elasticity." Mozans writes, however, that when the Eiffel tower was built and the architects inscribed the names of 72 great French scientists, Germain's name was not among them: despite the salience of her work to the tower's construction. Mozans asked, "Was she excluded from this list... because she was a woman? It would seem so."

Concerning her early work in number theory, J. H. Sampson states, "She was clever with formal algebraic manipulations; but there is little evidence that she really understood the "Disquisitiones", and her work of that period that has come down to us seems to touch only on rather superficial matters." Gray adds that "The inclination of sympathetic mathematicians to praise her work rather than to provide substantive criticism from which she might learn was crippling to her mathematical development." Yet Marilyn Bailey Ogilvie recognizes that "Sophie Germain's creativity manifested itself in pure and applied mathematics...[she] provided imaginative and provocative solutions to several important problems," and, as Petrovich proposes, it may have been her very lack of training that gave her unique insights and approaches. Louis Bucciarelli and Nancy Dworsky, Germain's biographers, summarize as follows: "All the evidence argues that Sophie Germain had a mathematical brilliance that never reached fruition due to a lack of rigorous training available only to men."

Germain was referenced and quoted in David Auburn's 2001 play "Proof." The protagonist is a young struggling female mathematician, Catherine, who found great inspiration in the work of Germain. Germain was also mentioned in John Madden's film adaptation of the same play in a conversation between Catherine (Gwyneth Paltrow) and Hal (Jake Gyllenhaal).

In the fictional work "The Last Theorem" by Arthur C. Clarke and Frederik Pohl, Sophie Germain was credited with inspiring the central character, Ranjit Subramanian, to solve Fermat's Last Theorem.

The Sophie Germain Prize (Prix Sophie Germain), awarded annually by the Foundation Sophie Germain, is conferred by the Academy of Sciences in Paris. Its purpose is to honour a French mathematician for research in the foundations of mathematics. This award, in the amount of €8,000, was established in 2003, under the auspices of the Institut de France.





</doc>
<doc id="27793" url="https://en.wikipedia.org/wiki?curid=27793" title="Shoa">
Shoa

Shoa may refer to:




</doc>
<doc id="27794" url="https://en.wikipedia.org/wiki?curid=27794" title="Small arms">
Small arms

Small arms include handguns (revolvers and pistols) and long guns, such as rifles, carbines, shotguns, submachine guns, assault rifles, personal defense weapons, and light machine guns. 

The world's top small arms manufacturing companies are Browning, Remington, Colt, Ruger, Smith & Wesson, Savage, Mossberg (USA), Heckler & Koch, SIG Sauer, Walther (Germany), Glock, Steyr-Mannlicher (Austria), FN Herstal (Belgium), Beretta (Italy), Norinco (China), Tula Arms and Kalashnikov (Russia), while former top producers were Mauser & Springfield Armory.

In 2018, Small Arms Survey reported that there are over one billion small arms distributed globally, of which 857 million (about 85 percent) are in civilian hands. U.S. civilians alone account for 393 million (about 46 percent) of the worldwide total of civilian held firearms. This amounts to "120.5 firearms for every 100 residents." The worlds armed forces control about 133 million (about 13 percent) of the global total of small arms, of which over 43 percent belonging to two countries the Russian Federation (30.3 million) and China (27.5 million). Law enforcement agencies control about 23 million (about 2 percent) of the global total of small arms.





</doc>
<doc id="27796" url="https://en.wikipedia.org/wiki?curid=27796" title="Succubus">
Succubus

A succubus is a demon in female form, or supernatural entity in folklore (traced back to medieval legend), that appears in dreams and takes the form of a woman in order to seduce men, usually through sexual activity. The male counterpart is the incubus. Religious traditions hold that repeated sexual activity with a succubus may result in the deterioration of health or mental state, or even death.

In modern representations, a succubus may or may not appear in dreams and is often depicted as a highly attractive seductress or enchantress; whereas, in the past, succubi were generally depicted as frightening and demonic.

The word is derived from Late Latin "succuba" "paramour"; from "succubare" "to lie beneath" ("sub-" "under" and "cubare" "to lie in bed"), used to describe the sleeper's position to the supernatural being as well. The word "succubus" originates from the late 14th century.

According to Zohar and the Alphabet of Ben Sira, Lilith was Adam's first wife, who later became a succubus. She left Adam and refused to return to the Garden of Eden after she mated with archangel Samael. In Zoharistic Kabbalah, there were four succubi who mated with the archangel Samael. There were four original queens of the demons: Lilith, Eisheth, Agrat bat Mahlat, and Naamah. A succubus may take a form of a beautiful young girl but closer inspection may reveal deformities of her body, such as bird-like claws or serpentine tails. Folklore also describes the act of sexually penetrating a succubus as akin to entering a cavern of ice, and there are reports of succubi forcing men to perform cunnilingus on their vulvas that drip with urine and other fluids. In later folklore, a succubus took the form of a siren.

Throughout history, priests and rabbis, including Hanina Ben Dosa and Abaye, tried to curb the power of succubi over humans. However, not all succubi were malevolent. According to Walter Map in the satire "De Nugis Curialium" ("Trifles of Courtiers"), Pope Sylvester II (999–1003) was allegedly involved with a succubus named Meridiana, who helped him achieve his high rank in the Catholic Church. Before his death, he confessed of his sins and died repentant.

According to the Kabbalah and the school of Rashba, the original three queens of the demons, Agrat Bat Mahlat, Naamah, Eisheth Zenunim, and all their cohorts give birth to children, except Lilith. According to other legends, the children of Lilith are called Lilin.

According to the "Malleus Maleficarum", or "Witches' Hammer", written by Heinrich Kramer (Institoris) in 1486, succubi collect semen from men they seduce. Incubi, or male demons, then use the semen to impregnate human females, thus explaining how demons could apparently sire children despite the traditional belief that they were incapable of reproduction. Children so begotten – cambions – were supposed to be those that were born deformed, or more susceptible to supernatural influences. While the book does not address why a human female impregnated with the semen of a human male would not produce regular human offspring, an explanation could be that the semen is altered before being transferred to the female host. However in some lore, the child is born deformed because the conception was unnatural.

King James in his dissertation titled Dæmonologie refutes the possibility for angelic entities to reproduce and instead offered a suggestion that a devil would carry out two methods of impregnating women: the first, to steal the sperm out of a dead man and deliver it into a woman. If a demon could extract the semen quickly, the transportation of the substance could not be instantly transported to a female host, causing it to go cold. This explains his view that succubae and incubi were the same demonic entity only to be described differently based on the tormented sexes being conversed with. The second method was the idea that a dead body could be possessed by a devil, causing it to rise and have sexual relations with others. However, there is no mention of a female corpse being possessed to elicit sex from men.

In Arabian mythology, the "qarînah" () is a spirit similar to the succubus, with origins possibly in ancient Egyptian religion or in the animistic beliefs of pre-Islamic Arabia. A qarînah "sleeps with the person and has relations during sleep as is known by the dreams." They are said to be invisible, but a person with "second sight" can see them, often in the form of a cat, dog, or other household pet. "In Omdurman it is a spirit which possesses. ... Only certain people are possessed and such people cannot marry or the qarina will harm them." To date, many African myths claim that men who have similar experience with such principality (succubus) in dreams (usually in form of a beautiful woman) find themselves exhausted as soon as they awaken; often claiming spiritual attack upon them. Local rituals/divination are often invoked in order to appeal the god for divine protection and intervention.

In the field of medicine, there is some belief that the stories relating to encounters with succubi bear resemblance to the contemporary phenomenon of people reporting alien abductions, which has been ascribed to the condition known as sleep paralysis. It is therefore suggested that historical accounts of people experiencing encounters with succubi may rather have been symptoms of sleep paralysis, with the hallucination of the said creatures coming from their contemporary culture.

Throughout history, succubi have been popular characters in music, literature, film, television, and especially as video game and manga/anime characters.


</doc>
<doc id="27797" url="https://en.wikipedia.org/wiki?curid=27797" title="Suzanne Vega">
Suzanne Vega

Suzanne Nadine Vega (born July 11, 1959) is an American singer-songwriter, musician and record producer, best known for her eclectic folk-inspired music.

Vega's music career spans more than 30 years. She came to prominence in the mid 1980s, releasing four singles that entered the Top 40 charts in the UK during the 1980s and 1990s, including "Marlene on the Wall", "Left of Center", "Luka" and "No Cheap Thrill". "Tom's Diner," which was originally released as an a cappella recording on Vega's second album, "Solitude Standing", was remixed in 1990 as a dance track by English electronic duo DNA with Vega as featured artist, and it became a Top 10 hit in over five countries. The song was used as a test during the creation of the MP3 format.

Vega has released nine studio albums to date, the latest of which is "", released in 2016.

Suzanne Nadine Vega was born on July 11, 1959, in Santa Monica, California. Her mother, Pat Vega (née Schumacher), is a computer systems analyst of German-Swedish heritage. Her father, Richard Peck, is of Scottish-English-Irish origin. They divorced soon after her birth. Her stepfather, Edgardo Vega Yunqué, also known as Ed Vega, was a writer and teacher from Puerto Rico. When Vega was two and a half, her family moved to New York City. She grew up in Spanish Harlem and the Upper West Side.

She was not aware of having a different biological father, Richard Peck, until she was nine years old. They met for the first time in her late 20s, and they remain in contact.

She attended the High School of Performing Arts, where she studied modern dance and graduated in 1977.

While majoring in English literature at Barnard College, she performed in small venues in Greenwich Village, where she was a regular contributor to Jack Hardy's Monday night songwriters' group at the Cornelia Street Cafe and had some of her first songs published on "Fast Folk" anthology albums. In 1984, she received a major label recording contract, making her one of the first "Fast Folk" artists to break out on a major label.

Vega's self-titled debut album was released in 1985 and was well received by critics in the U.S.; it reached platinum status in the United Kingdom. Produced by Lenny Kaye and Steve Addabbo, the songs feature Vega's acoustic guitar in straightforward arrangements. A video was released for the album's song "Marlene on the Wall", which went into MTV and VH1's rotations. During this period Vega also wrote lyrics for two songs ("Lightning" and "Freezing") on "Songs from Liquid Days" by composer Philip Glass.

Vega's song "Left of Center" co-written with Steve Addabbo for the 1986 John Hughes film "Pretty in Pink" reached No. 32 on the UK Singles Chart in 1986.

Her next effort, "Solitude Standing" (1987), garnered critical and commercial success, selling over a million copies in the U.S. It includes the international hit single "Luka", which is written about, and from the point of view of, an abused child—at the time an uncommon subject for a pop hit. While continuing a focus on Vega's acoustic guitar, the music is more strongly pop-oriented and features fuller arrangements. The a cappella "Tom's Diner" from this album was later a hit, remixed by two British dance producers under the name DNA, in 1990. The track was originally a bootleg, until Vega allowed DNA to release it through her record company, and it became her all-time biggest hit.

Vega's third album, "Days of Open Hand" (1990), continued in the style of her first two albums.

In 1992 she released the album "99.9F°". It consists of a mixture of folk music, dance beats and industrial music. This record was awarded Gold status by the RIAA in recognition of selling over 500,000 copies in the U.S. The single "Blood Makes Noise" from this album peaked at number-one on Billboard's Modern Rock Tracks. Vega later married the album's producer Mitchell Froom.

Her fifth album, "Nine Objects of Desire", was released in 1996. The music varies between a frugal, simple style and the industrial production of "99.9F°". This album contains "Caramel", featured in the movie "The Truth About Cats & Dogs", and later the trailer for the movie "Closer". A song not included on that album, "Woman on the Tier," was featured on the soundtrack of the movie "Dead Man Walking".

In 1997 she took a singing part on the concept album "Heaven and Hell", a musical interpretation of the seven deadly sins by her colleague Joe Jackson, with whom she had already collaborated in 1986 on "Left of Center" from the "Pretty in Pink" soundtrack (with Vega singing and Jackson playing piano).

In 1999, Avon Books published Vega's book "The Passionate Eye: The Collected Writings of Suzanne Vega", a volume of poems, lyrics, essays and journalistic pieces.

In September 2001, Vega released a new album entitled "Songs in Red and Gray". Three songs deal with Vega's divorce from her first husband, Mitchell Froom.

At the memorial concert for her brother Tim Vega in December 2002, Vega began her role as the subject of the direct-cinema documentary, "Some Journey", directed by Christopher Seufert of Mooncusser Films. The documentary has not been completed.

Underground hip hop duo Felt named a track on their album "" released in 2002 "Suzanne Vega".

In 2003, the twenty-one-song greatest hits compilation "Retrospective: The Best of Suzanne Vega" was released. (The UK version of "Retrospective" included an eight-song bonus CD as well as a DVD containing twelve songs.) In the same year she was invited by Grammy Award-winning jazz guitarist Bill Frisell, to play at the "Century of Song" concerts at the famed "Ruhrtriennale" in Germany.

In 2003, she hosted the American Public Media radio series "American Mavericks", about 20th century American composers, which received the Peabody Award for Excellence in Broadcasting.

On August 3, 2006, Vega became the first major recording artist to perform live in the Internet-based virtual world, "Second Life". The event was hosted by John Hockenberry of public radio's "The Infinite Mind".

On September 17, 2006, she performed in Central Park, as part of a benefit concert for the Save Darfur Coalition. During the concert she highlighted her support for Amnesty International, of which she has been a member since 1988.
In early October 2006, Vega took part in the Academia Film Olomouc (AFO) in Olomouc, the Czech Republic, the oldest festival of documentary films in Europe, in which she appeared as a main guest. She was invited there as the subject of the documentary film by director Christopher Seufert, that had a test screening at the festival. At the end of the festival she performed her classic songs and added one brand new piece called "New York Is a Woman".

Vega is also interviewed in the book "Everything Is Just a Bet" which was published in Czech in October 2006. The book contains twelve interview transcriptions from the talk show called "Stage Talks" that regularly runs in the Švandovo divadlo (Švandovo Theatre) in Prague. Vega introduced the book to the audience of the Švandovo divadlo (Švandovo Theatre), and together with some other Czech celebrities gave a signing session.

She signed a new recording contract with Blue Note Records in the spring of 2006, and released "Beauty & Crime" on July 17, 2007. The album, produced by Jimmy Hogarth, won a Grammy Award for Best Engineered Album, Non-Classical. Her contract was not renewed and she was dropped in June 2008.

In 2007, Vega followed the lead of numerous other mainstream artists and released her track "Pornographer's Dream" as podsafe. The song spent two weeks at number-one during 2007 and finished as the No. 11 hit of the year on the PMC Top10's annual countdown. In 2015, Vega joined The 14th Annual Independent Music Awards judging panel to assist independent musicians' careers.

A partial cover version of her song "Tom's Diner" was used to introduce the 2010 British movie "4.3.2.1", with its lyrics largely rewritten to echo the plot. This musical hybrid was released as "Keep Moving". Vega participated in the Danger Mouse/Sparklehorse/David Lynch collaboration "Dark Night of the Soul". She wrote both melody and lyrics for her song, which is titled "The Man Who Played God", inspired by a biography of Pablo Picasso. Vega sang lead vocals on the song "Now I Am an Arsonist" with singer-songwriter Jonathan Coulton on his 2011 album, "Artificial Heart".

Vega has re-recorded her back-catalogue, both for artistic and commercial (and control) reasons, in the "Close-up" series. Vol. 1 ("Love Songs") and Vol. 2 ("People & Places") appeared in 2010 while Vol. 3 ("States of Being") was released in July 2011 followed by Vol. 4 ("Songs of Family") in September 2012. Volumes 2, 3 and 4 of the "Close-Up" albums included previously unrecorded material; Volumes 2 and 3 each included one new collaboratively written song, while Volume 4 included three songs that Vega had written years earlier, but had not previously gotten around to recording. In all, Vega's "Close-Up" series features 60 re-recorded songs and five new compositions, representing about three-quarters of her lifetime songwriting output.

While performing live, Vega and long-term collaborator Gerry Leonard began to introduce a number of new songs into the setlist, including the live favorite "I Never Wear White". Over the course of a year, the songs were completed and recorded in a live-studio setting with the help of a number of guests. Produced by Leonard, "Tales from the Realm of the Queen of Pentacles" was released in February 2014. It was her first album of new material in seven years and became Vega's first studio album to reach the UK Top 40 since 1992, peaking at No. 37.

New album "" was released on October 14, 2016.

At the age of nine she began to write poetry. She was encouraged to do so by her stepfather. It took her 3 years to write her first song, "Brother Mine", which was finished at the age of 14. It was first published on "Close-Up Vol. 4, Songs of Family", along with her other early song, "The Silver Lady".

Vega has not learned to read musical notes; she sees the melody as a shape and chords as colors. She focuses on lyrics and melodic ideas; for advanced features – like intros or bridges – she relies on other artists she works with. Most of her albums, except the first one, were made in such cooperation.

She states that 80% of the songs she started writing get finished.

The most important artistic influences on her work come from Lou Reed, Bob Dylan and Leonard Cohen. Some other important artists for her are Paul Simon and Laura Nyro.

Vega and Duncan Sheik wrote a play "Carson McCullers Talks About Love", about the life of the writer Carson McCullers. In the play directed by Kay Matschullat, which premiered in 2011, Vega alternates between monologue and songs. Vega and Sheik were nominated for Outstanding Music in a Play for the 57th annual Drama Desk awards.

The album "", based on this play, was released in 2016. Vega considers it to be a third version, because it's rewritten, and she made the first version in college.

Vega has established her own recording label after the 2008 economic crisis. From that point, she stopped working for Blue Note Records and started thinking about re-recording her back catalog with new arrangements and gaining control over her works (which she eventually did with the "Close-Up Series").

The name "Amanuensis Productions" was meant as a private joke about "servant" (amanuensis) owning the "masters" (recording masters), also a pun at A&M still legally owning her previous master tapes.

Running the label proved to be harder than she expected. In 2015 it just "broke even", but new licenses were coming for "Tom's Diner".

On March 17, 1995, Vega married Mitchell Froom, a musician and a record producer (who played on and produced "99.9F°" and "Nine Objects of Desire"). They have a daughter, Ruby Froom (born July 8, 1994). The band Soul Coughing's "Ruby Vroom" album was named after her, with Vega's approval. Beginning in 2010, Ruby has occasionally performed with her mother.
Vega and Froom separated and divorced in 1998.

On February 11, 2006, Vega married Paul Mills, a lawyer and poet, "22 years after he first proposed to her".

Vega practices Nichiren Buddhism and is a member of the US branch of the worldwide Buddhist association Soka Gakkai International.

! Year !! Awards !! Work !! Category !! Result

Studio albums

Books



</doc>
<doc id="27799" url="https://en.wikipedia.org/wiki?curid=27799" title="Semigroup">
Semigroup

In mathematics, a semigroup is an algebraic structure consisting of a set together with an associative binary operation.

The binary operation of a semigroup is most often denoted multiplicatively: "x"·"y", or simply "xy", denotes the result of applying the semigroup operation to the ordered pair . Associativity is formally expressed as that for all "x", "y" and "z" in the semigroup.

The name "semigroup" originates in the fact that a semigroup generalizes a group by preserving only associativity and closure under the binary operation from the axioms defining a group while omitting the requirement for an identity element and inverses. From the opposite point of view (of adding rather than removing axioms), a semigroup is an associative magma. As in the case of groups or magmas, the semigroup operation need not be commutative, so "x"·"y" is not necessarily equal to "y"·"x"; a typical example of associative but non-commutative operation is matrix multiplication. If the semigroup operation is commutative, then the semigroup is called a "commutative semigroup" or (less often than in the analogous case of groups) it may be called an "abelian semigroup".

A monoid is an algebraic structure intermediate between groups and semigroups, and is a semigroup having an identity element, thus obeying all but one of the axioms of a group; existence of inverses is not required of a monoid. A natural example is strings with concatenation as the binary operation, and the empty string as the identity element. Restricting to non-empty strings gives an example of a semigroup that is not a monoid. Positive integers with addition form a commutative semigroup that is not a monoid, whereas the non-negative integers do form a monoid. A semigroup without an identity element can be easily turned into a monoid by just adding an identity element. Consequently, monoids are studied in the theory of semigroups rather than in group theory. Semigroups should not be confused with quasigroups, which are a generalization of groups in a different direction; the operation in a quasigroup need not be associative but quasigroups preserve from groups a notion of division. Division in semigroups (or in monoids) is not possible in general.

The formal study of semigroups began in the early 20th century. Early results include a Cayley theorem for semigroups realizing any semigroup as transformation semigroup, in which arbitrary functions replace the role of bijections from group theory. Other fundamental techniques of studying semigroups like Green's relations do not imitate anything in group theory though. A deep result in the classification of finite semigroups is Krohn–Rhodes theory. The theory of finite semigroups has been of particular importance in theoretical computer science since the 1950s because of the natural link between finite semigroups and finite automata via the syntactic monoid. In probability theory, semigroups are associated with Markov processes. In other areas of applied mathematics, semigroups are fundamental models for linear time-invariant systems. In partial differential equations, a semigroup is associated to any equation whose spatial evolution is independent of time. There are numerous special classes of semigroups, semigroups with additional properties, which appear in particular applications. Some of these classes are even closer to groups by exhibiting some additional but not all properties of a group. Of these we mention: regular semigroups, orthodox semigroups, semigroups with involution, inverse semigroups and cancellative semigroups. There also interesting classes of semigroups that do not contain any groups except the trivial group; examples of the latter kind are bands and their commutative subclass—semilattices, which are also s.

A semigroup is a set formula_1 together with a binary operation "formula_2" (that is, a function formula_3) that satisfies the associative property:

More succinctly, a semigroup is an associative magma.


A left identity of a semigroup formula_1 (or more generally, magma) is an element formula_7 such that for all formula_8 in formula_1, formula_10. Similarly, a right identity is an element formula_11 such that for all formula_8 in formula_1, formula_14. Left and right identities are both called one-sided identities. A semigroup may have one or more left identities but no right identity, and vice versa.

A two-sided identity (or just identity) is an element that is both a left and right identity. Semigroups with a two-sided identity are called monoids. A semigroup may have at most one two-sided identity. If a semigroup has a two-sided identity, then the two-sided identity is the only one-sided identity in the semigroup. If a semigroup has both a left identity and a right identity, then it has a two-sided identity (which is therefore the unique one-sided identity).

A semigroup formula_1 without identity may be embedded in a monoid formed by adjoining an element formula_16 to formula_1 and defining formula_18 for all formula_19. The notation formula_20 denotes a monoid obtained from formula_1 by adjoining an identity "if necessary" (formula_22 for a monoid).

Similarly, every magma has at most one absorbing element, which in semigroup theory is called a zero. Analogous to the above construction, for every semigroup formula_1, one can define formula_24, a semigroup with 0 that embeds formula_1.

The semigroup operation induces an operation on the collection of its subsets: given subsets "A" and "B" of a semigroup "S", their product , written commonly as "AB", is the set (This notion is defined identically as it is for groups.) In terms of this operation, a subset "A" is called

If "A" is both a left ideal and a right ideal then it is called an ideal (or a two-sided ideal).

If "S" is a semigroup, then the intersection of any collection of subsemigroups of "S" is also a subsemigroup of "S".
So the subsemigroups of "S" form a complete lattice.

An example of semigroup with no minimal ideal is the set of positive integers under addition. The minimal ideal of a commutative semigroup, when it exists, is a group.

Green's relations, a set of five equivalence relations that characterise the elements in terms of the principal ideals they generate, are important tools for analysing the ideals of a semigroup and related notions of structure.

The subset with the property that its every element commutes with any other element of the semigroup is called the center of the semigroup. The center of a semigroup is actually a subsemigroup.

A semigroup homomorphism is a function that preserves semigroup structure. A function between two semigroups is a homomorphism if the equation
holds for all elements "a", "b" in "S", i.e. the result is the same when performing the semigroup operation after or before applying the map "f".

A semigroup homomorphism between monoids preserves identity if it is a monoid homomorphism. But there are semigroup homomorphisms which are not monoid homomorphisms, e.g. the canonical embedding of a semigroup formula_1 without identity into formula_20. Conditions characterizing monoid homomorphisms are discussed further. Let formula_28 be a semigroup homomorphism. The image of formula_11 is also a semigroup. If formula_30 is a monoid with an identity element formula_31, then formula_32 is the identity element in the image of formula_11. If formula_34 is also a monoid with an identity element formula_35 and formula_35 belongs to the image of formula_11, then formula_38, i.e. formula_11 is a monoid homomorphism. Particularly, if formula_11 is surjective, then it is a monoid homomorphism.

Two semigroups "S" and "T" are said to be isomorphic if there is a bijection with the property that, for any elements "a", "b" in "S", . Isomorphic semigroups have the same structure.

A semigroup congruence formula_41 is an equivalence relation that is compatible with the semigroup operation. That is, a subset formula_42 that is an equivalence relation and formula_43 and formula_44 implies formula_45 for every formula_46 in "S". Like any equivalence relation, a semigroup congruence formula_41 induces congruence classes

and the semigroup operation induces a binary operation formula_49 on the congruence classes:

Because formula_41 is a congruence, the set of all congruence classes of formula_41 forms a semigroup with formula_49, called the quotient semigroup or factor semigroup, and denoted formula_54. The mapping formula_55 is a semigroup homomorphism, called the quotient map, canonical surjection or projection; if S is a monoid then quotient semigroup is a monoid with identity formula_56. Conversely, the kernel of any semigroup homomorphism is a semigroup congruence. These results are nothing more than a particularization of the first isomorphism theorem in universal algebra. Congruence classes and factor monoids are the objects of study in string rewriting systems.

A nuclear congruence on "S" is one which is the kernel of an endomorphism of "S".

A semigroup "S" satisfies the maximal condition on congruences if any family of congruences on "S", ordered by inclusion, has a maximal element. By Zorn's lemma, this is equivalent to saying that the ascending chain condition holds: there is no infinite strictly ascending chain of congruences on "S".

Every ideal "I" of a semigroup induces a subsemigroup, the Rees factor semigroup via the congruence   ⇔   either or both "x" and "y" are in "I".

The following notions introduce the idea that a semigroup is contained in another one.

A semigroup T is a quotient of a semigroup S if there is a surjective semigroup morphism from T to S. For example, formula_57 is a quotient of formula_58, using the morphism consisting of taking the remainder modulo p of an integer.

A semigroup T divides a semigroup S, noted formula_59 if T is a quotient of a subsemigroup S. In particular, subsemigroups of S divides T, while it is not necessarily the case that there are a quotient of S.

Both of those relation are transitive.

For any subset "A" of "S" there is a smallest subsemigroup "T" of "S" which contains "A", and we say that "A" generates "T". A single element "x" of "S" generates the subsemigroup 
If this is finite, then "x" is said to be of finite order, otherwise it is of infinite order.
A semigroup is said to be periodic if all of its elements are of finite order.
A semigroup generated by a single element is said to be monogenic (or cyclic). If a monogenic semigroup is infinite then it is isomorphic to the semigroup of positive integers with the operation of addition.
If it is finite and nonempty, then it must contain at least one idempotent.
It follows that every nonempty periodic semigroup has at least one idempotent.

A subsemigroup which is also a group is called a subgroup. There is a close relationship between the subgroups of a semigroup and its idempotents. Each subgroup contains exactly one idempotent, namely the identity element of the subgroup. For each idempotent "e" of the semigroup there is a unique maximal subgroup containing "e". Each maximal subgroup arises in this way, so there is a one-to-one correspondence between idempotents and maximal subgroups. Here the term "maximal subgroup" differs from its standard use in group theory.

More can often be said when the order is finite. For example, every nonempty finite semigroup is periodic, and has a minimal ideal and at least one idempotent. The number of finite semigroups of a given size (greater than 1) is (obviously) larger than the number of groups of the same size. For example, of the sixteen possible "multiplication tables" for a set of two elements eight form semigroups whereas only four of these are monoids and only two form groups. For more on the structure of finite semigroups, see Krohn–Rhodes theory.


There is a structure theorem for commutative semigroups in terms of semilattices. A semilattice (or more precisely a meet-semilattice) formula_60 is a partially ordered set where every pair of elements formula_61 has a greatest lower bound, denoted formula_62. The operation formula_63 makes formula_64 into a semigroup satisfying the additional idempotence law formula_65.

Given a homomorphism formula_66 from an arbitrary semigroup to a semilattice, each inverse image formula_67 is a (possibly empty) semigroup. Moreover, formula_68 becomes graded by formula_64, in the sense that

formula_70

If formula_71 is onto, the semilattice formula_64 is isomorphic to the quotient of formula_1 by the equivalence relation formula_74 such that formula_75 iff formula_76. This equivalence relation is a semigroup congruence, as defined above.

Whenever we take the quotient of a commutative semigroup by a congruence, we get another commutative semigroup. The structure theorem says that for any commutative semigroup formula_1, there is a finest congruence formula_74 such that the quotient of formula_68 by this equivalence relation is a semilattice. Denoting this semilattice by formula_80, we get a homomorphism formula_71 from formula_1 onto formula_80. As mentioned, formula_68 becomes graded by this semilattice.

Furthermore, the components formula_85 are all Archimedean semigroups. An Archimedean semigroup is one where given any pair of elements formula_86, there exists an element formula_87 and formula_88 such that formula_89.

The Archimedean property follows immediately from the ordering in the semilattice formula_64, since with this ordering we have formula_91 if and only if formula_89 for some formula_87 and formula_88.

The group of fractions or group completion of a semigroup "S" is the group generated by the elements of "S" as generators and all equations which hold true in "S" as relations. There is an obvious semigroup homomorphism which sends each element of "S" to the corresponding generator. This has a universal property for morphisms from "S" to a group: given any group "H" and any semigroup homomorphism , there exists a unique group homomorphism with "k"="fj". We may think of "G" as the "most general" group that contains a homomorphic image of "S".

An important question is to characterize those semigroups for which this map is an embedding. This need not always be the case: for example, take "S" to be the semigroup of subsets of some set "X" with set-theoretic intersection as the binary operation (this is an example of a semilattice). Since holds for all elements of "S", this must be true for all generators of "G"("S") as well: which is therefore the trivial group. It is clearly necessary for embeddability that "S" have the cancellation property. When "S" is commutative this condition is also sufficient and the Grothendieck group of the semigroup provides a construction of the group of fractions. The problem for non-commutative semigroups can be traced to the first substantial paper on semigroups. Anatoly Maltsev gave necessary and sufficient conditions for embeddability in 1937.

Semigroup theory can be used to study some problems in the field of partial differential equations. Roughly speaking, the semigroup approach is to regard a time-dependent partial differential equation as an ordinary differential equation on a function space. For example, consider the following initial/boundary value problem for the heat equation on the spatial interval and times :

Let be the "L" space of square-integrable real-valued functions with domain the interval and let "A" be the second-derivative operator with domain

where "H" is a Sobolev space. Then the above initial/boundary value problem can be interpreted as an initial value problem for an ordinary differential equation on the space "X":

On an heuristic level, the solution to this problem "ought" to be . However, for a rigorous treatment, a meaning must be given to the exponential of "tA". As a function of "t", exp("tA") is a semigroup of operators from "X" to itself, taking the initial state "u" at time to the state at time "t". The operator "A" is said to be the infinitesimal generator of the semigroup.

The study of semigroups trailed behind that of other algebraic structures with more complex axioms such as groups or rings. A number of sources attribute the first use of the term (in French) to J.-A. de Séguier in "Élements de la Théorie des Groupes Abstraits" (Elements of the Theory of Abstract Groups) in 1904. The term is used in English in 1908 in Harold Hinton's "Theory of Groups of Finite Order".

Anton Suschkewitsch obtained the first non-trivial results about semigroups. His 1928 paper "Über die endlichen Gruppen ohne das Gesetz der eindeutigen Umkehrbarkeit" ("On finite groups without the rule of unique invertibility") determined the structure of finite simple semigroups and showed that the minimal ideal (or Green's relations J-class) of a finite semigroup is simple. From that point on, the foundations of semigroup theory were further laid by David Rees, James Alexander Green, Evgenii Sergeevich Lyapin, Alfred H. Clifford and Gordon Preston. The latter two published a two-volume monograph on semigroup theory in 1961 and 1967 respectively. In 1970, a new periodical called "Semigroup Forum" (currently edited by Springer Verlag) became one of the few mathematical journals devoted entirely to semigroup theory.

In recent years researchers in the field have become more specialized with dedicated monographs appearing on important classes of semigroups, like inverse semigroups, as well as monographs focusing on applications in algebraic automata theory, particularly for finite automata, and also in functional analysis.

If the associativity axiom of a semigroup is dropped, the result is a magma, which is nothing more than a set "M" equipped with a binary operation .

Generalizing in a different direction, an n"-ary semigroup (also n"-semigroup, polyadic semigroup or multiary semigroup) is a generalization of a semigroup to a set "G" with a "n"-ary operation instead of a binary operation. The associative law is generalized as follows: ternary associativity is , i.e. the string "abcde" with any three adjacent elements bracketed. "N"-ary associativity is a string of length with any "n" adjacent elements bracketed. A 2-ary semigroup is just a semigroup. Further axioms lead to an "n"-ary group.

A third generalization is the semigroupoid, in which the requirement that the binary relation be total is lifted. As categories generalize monoids in the same way, a semigroupoid behaves much like a category but lacks identities.

Infinitary generalizations of commutative semigroups have sometimes been considered by various authors.





</doc>
<doc id="27801" url="https://en.wikipedia.org/wiki?curid=27801" title="Super Mario Kart">
Super Mario Kart

Super Mario Kart is a 1992 kart racing video game developed and published by Nintendo for the Super Nintendo Entertainment System video game console. The first game of the "Mario Kart" series, it was released in Japan and North America in 1992, and in Europe the following year. Selling nine million copies worldwide, the game went on to become the fourth best selling SNES game of all time. "Super Mario Kart" was re-released on the Wii's Virtual Console in 2009, and on the Wii U's Virtual Console in 2013. Nintendo re-released "Super Mario Kart" in the United States in September 2017 as part of the company's Super NES Classic Edition.

In "Super Mario Kart", the player takes control of one of eight "Mario" series characters, each with differing capabilities. In single player mode players can race against computer-controlled characters in multi-race cups over three difficulty levels. During the races, offensive and speed boosting power-ups can be used to gain an advantage. Alternatively players can race against the clock in a Time Trial mode. In multi-player mode two players can simultaneously take part in the cups or can race against each other one-on-one in Match Race mode. In a third multiplayer mode – Battle Mode – the aim is to defeat the other players by attacking them with power-ups, destroying balloons which surround each kart.

"Super Mario Kart" received positive reviews and was praised for its presentation, innovation and use of Mode 7 graphics. It has been ranked among the best games of all time by several organizations including "Edge", IGN, "The Age" and GameSpot, while "Guinness World Records" has named it as the top console game ever. It is often credited with creating the kart-racing subgenre of video games, leading other developers to try to duplicate its success. The game is also seen as having been key to expanding the "Mario" series into non-platforming games. This diversity has led to it becoming the best-selling game franchise of all time. Several sequels to "Super Mario Kart" have been released, for consoles, handhelds and in arcades, each enjoying critical and commercial success. While some elements have developed throughout the series, the core experience from "Super Mario Kart" has remained intact.

"Super Mario Kart" is a kart racing game featuring several single and multiplayer modes. During the game, players take control of one of eight "Mario" franchise characters and drive karts around tracks with a "Mario" franchise theme. In order for them to begin driving, Lakitu will appear with a traffic light hanging on his fishing pole, which starts the countdown. When the light turns green, the race or battle officially begins. During a race, the player's viewpoint is from behind his or her kart. The goal of the game is to either finish a race ahead of other racers, who are controlled by the computer and other players, or complete a circuit in the fastest time. There is also a battle mode in which the aim is to attack the karts of the other human players.

Tiles marked with question marks are arrayed on the race tracks; they give special abilities (power-ups) to a player's kart if the vehicle passes over them. Power-ups, such as the ability to throw shells and bananas, allow racers to hit others with the objects, causing them to spin and lose control. A kart that obtains the star power-up is temporarily invulnerable to attack. Computer players have specific special powers associated with each character, that they are able to use throughout the race. Lines of coins are found on the tracks in competitive race modes. By running over these coins, a kart collects them and increases its top speed. Having coins also helps players when their kart is hit by another: instead of spinning and losing control, they lose a coin. Coins are also lost when karts are struck by power-ups or fall off the tracks.

The game features advanced maneuvers such as power sliding and hopping. Power sliding allows a kart to maintain its speed while turning, although executing the maneuver for too long causes the kart to spin. Hopping helps a kart execute tighter turns: the kart makes a short hop and turns in the air, speeding off in the new direction when it lands. Reviewers praised "Super Mario Kart"s gameplay, describing the battle mode as "addictive" and the single player gameplay as "incredible". IGN stated that the gameplay mechanics defined the genre.

"Super Mario Kart" has two single-player modes, Mario Kart GP (which stands for Grand Prix) and Time Trial. In Mario Kart GP, one player is required to race against seven computer-controlled characters in a series of five races which are called cups. Initially there are three cups available – the Mushroom Cup, Flower Cup and Star Cup – at two difficulty levels, 50cc and 100cc. By winning all three of the cups at the 100cc level, a fourth cup – the Special Cup – is unlocked. Winning all four cups at 100cc unlocks a new difficulty level, 150cc. Each cup consists of five five-lap races, each taking place on a distinct track. In order to continue through a cup, a position of fourth or higher must be achieved in each race. If a player finishes in fifth to eighth position, they are "ranked out" and the race must be replayed – at the cost of one of a limited number of lives – until a placing of fourth or above is achieved. If the player has no lives when they rank out, the game is over. Points are accrued by finishing in the top four positions in a race; first to fourth place receive nine, six, three and one points. If a player finished in the same position three times in a row, then an extra life is awarded. The racer with the highest number of points after all five races have been completed wins the cup. In time trial mode, players race against the clock through the same tracks that are present in Mario Kart GP mode, attempting to set the fastest time possible.

"Super Mario Kart" also has three multiplayer modes; Mario Kart GP, Match Race and Battle Mode. The multiplayer modes support two players and the second player uses the bottom half of the screen which is used as a map in the single player modes. Mario Kart GP is the same as in single player, the only difference being that there are now two human controlled and six computer-controlled drivers. Match Race involves the two players going head to head on a track of their choice without any opponents. In Battle Mode, the two players again go head to head, but this time in one of four dedicated Battle Mode courses. Each player starts with three balloons around their kart which can be popped by power-ups fired by the other player. The first player to have all three of their balloons popped loses.

"Super Mario Kart" features eight playable characters from the "Mario" series – Mario, Luigi, Princess Peach, Yoshi, Bowser, Donkey Kong Jr., Koopa Troopa and Toad. Each character's kart has different capabilities with differing levels of top speed, acceleration and handling. Mario, Luigi, Peach, Yoshi, Bowser and Toad returned in all of the subsequent "Mario Kart" games starting with "Mario Kart 64". During races, computer-controlled characters have special items, or superpowers, which they are able to use. These powers are specific to each character; for example, Yoshi drops eggs which cause players who hit them to lose coins and spin, while Donkey Kong Jr. throws bananas.

The characters are rendered as sprites portrayed from sixteen different angles. The sprites were described as "detailed" by Nintendo Magazine System when the game was first reviewed and were thought to contribute to the "spectacular" graphics of the game as a whole. More recently, Nintendojo called the sprites "not-so-pretty" when they are rendered at a distance, and IGN has commented on the dated look of the game. "Super Mario Kart" was the first game to feature playable characters from the "Mario" series other than Mario or Luigi in a non-platforming game and the selection and different attributes of the characters is regarded as one of the game's strengths, IGN describing a well-balanced "all-star cast". All of the characters present in "Super Mario Kart" have gone on to appear in all of the later games in the series, except for Koopa Troopa, who has only appeared intermittently after being replaced by Wario in "Mario Kart 64". Donkey Kong Jr. was replaced by Donkey Kong, who has appeared in every "Mario Kart" game since. This was Donkey Kong Jr.'s last appearance as a playable character, except for the "Mario Tennis" sub-series, including installments on the Nintendo 64 and Virtual Boy.

The tracks in "Super Mario Kart" are based on locations in "Super Mario World" such as Donut Plains. Each of the four cups contains five different tracks for a total of twenty unique tracks, additionally there are four unique Battle Mode courses. The course outlines are marked out by impassable barriers and feature a variety of bends ranging from sharp hairpins to wide curves which players can power slide around. Numerous obstacles themed from the "Mario" series appear, such as Thwomps in the Bowser's Castle tracks, the Cheep-Cheeps from "Super Mario World" in Koopa Beach and pipe barriers which are found in the Mario Circuit tracks. Other features include off-road sections which slow down the karts such as the mud bogs in the Choco Island tracks. Each single player track is littered with coins and power up tiles, as well as turbo tiles which give the karts a boost of speed and jumps which launch the karts into the air.

The tracks have received positive commentary with GameSpy describing them as wonderfully designed and IGN calling them perfect. When naming its top five "Mario Kart" tracks of all time in 2008, 1UP.com named Battle Mode Course 4 at number three and Rainbow Road – along with its subsequent versions in the series – at number one. The track themes in "Super Mario Kart" influenced later games in the series; recurring themes that first appeared in "Super Mario Kart" include haunted tracks, Bowser's castle and Rainbow Road. Some of the tracks from "Super Mario Kart" have been duplicated in later games. All twenty of the original tracks are unlockable as an extra feature in the Game Boy Advance sequel "". Remakes of Mario Circuit 1, Donut Plains 1, Koopa Beach 2 and Choco Island 2 appear as part of the Retro Grand Prix series in "Mario Kart DS", remakes of Ghost Valley 2, Mario Circuit 3, and Battle Course 4 appear as part of the Retro Grand Prix and battles in "Mario Kart Wii", remakes of Mario Circuit 2 and Rainbow Road appear as part of the Retro Grand Prix in "Mario Kart 7", and a remake of Donut Plains 3 appears as part of the Retro Grand Prix and battles in "Mario Kart 8", and a remake of Battle Course 1 appear as a Retro Battle Course in "Mario Kart 8 Deluxe".

"Super Mario Kart" was produced by Shigeru Miyamoto and directed by Tadashi Sugiyama and Hideki Konno. In an interview Miyamoto has said that the development team originally set out to produce a game capable of displaying two players on the same game screen simultaneously. In the same interview Konno stated that development started with a desire to create a two player racing game in contrast to the single player gameplay of SNES launch title "F-Zero". The team found that due to limitations of the SNES hardware, the strong focus on multiplayer prevented the inclusions of tracks as elaborate as those found in "F-Zero". Computer and Video Games suggest that this initial emphasis on creating a two player experience is the reason for the game's horizontal split-screen during single player modes.

The intention to create the racing modes of the game had been present from the start of the project and Battle Mode was developed from the desire to create a one-on-one mode where victory was not determined simply by competing for rank. The game did not start out as a "Mario" series game and the first prototype featured a generic man in overalls in the kart; the team decided that characters three heads tall would best suit the design of the karts. They did not decide on incorporating "Mario" series characters into the game until two or three months after the start of development. The choice was made after the development team, when observing how one kart looked to another driving past it, decided to see what it would look like with Mario in the kart. Thinking that having Mario in the kart looked better than previous designs, the idea of a Mario themed racing game was born.

Notable in the development of "Super Mario Kart" was its use of Mode 7 graphics. First seen in "F-Zero", Mode 7 is a form of texture mapping available on the SNES which allows a plane to be rotated and scaled freely, achieving a pseudo-three-dimensional appearance. 1UP.com have credited the use of Mode 7 with giving the game graphics which at the time of release were considered to be "breathtaking". Retrospective reflection on the Mode 7 visuals was mixed, with IGN stating that the once revolutionary technology now looks "crude and flickery". "Super Mario Kart" featured a DSP (Digital Signal Processor) chip; DSPs were used in SNES games as they provided a better handling of floating point calculations to assist with three-dimensional maths. The DSP-1 chip that was used in "Super Mario Kart" went on to be the most popular DSP chip to be used in SNES games. The music for the title was created by composer Soyo Oka.

"Super Mario Kart" received critical acclaim and proved to be a commercial success; it received a Player's Choice release after selling one million copies and eventually went on to sell nine million copies to become the third best selling game ever for the SNES. Aggregate scoring sites GameRankings and MobyGames both give an average of more than 90 percent. Critics praised the game's Mode 7 graphics; in 1992 "Nintendo Magazine System" described them as superb and the graphics have since been described as among the best ever seen on the SNES. Another aspect of the game to have been praised is its gameplay, which Thunderbolt has described as the "deepest [and] most addictive... to be found on the SNES console". "Nintendo Magazine System" showed a preference for the multiplayer modes of the game and stated that while the "single player mode becomes dull quickly" the "two-player mode won't lose appeal". Retrospective reviews of the game have been positive with perfect scores given by review sites including Thunderbolt and HonestGamers. The use of the style and characters from the "Mario" franchise was also praised as well as the individual characteristics of each racer. Mean Machines describes the game as having "struck gold" in a way that no other – not even its sequels – has matched and GameSpot named the game as one of the greatest games of all time for its innovation, gameplay and visual style.

Since being released "Super Mario Kart" has been listed among the best games ever made several times. In 1996, "Next Generation" listed it as number 37 on their "Top 100 Games of All Time", commenting that the controls are elegantly designed to offer "supreme fun." IGN ranked it as the 15th best game ever in 2005, describing it as "the original karting masterpiece" and as the 23rd best game ever in 2007, discussing its originality at time of release. "The Age" placed it at number 19 on their list of the 50 best games in 2005 and in 2007 "Edge" ranked "Super Mario Kart" at number 14 on a list of their 100 best games, noting its continued influence on video game design. The game is also included in Yahoo! Games UK's list of the hundred greatest games of all time which praises the appealing characters and power ups and 1UP.com's "Essential 50", a list of the fifty most important games ever made. The game placed 13th in "Official Nintendo Magazine"'s 100 greatest Nintendo games of all time. "Guinness World Records" ranked it at number 1 on a list of the top 50 console games of all time based on initial impact and lasting legacy.

"Super Mario Kart" has been credited with inventing the "kart racing" subgenre of video gaming and soon after its release several other developers attempted to duplicate its success. In 1994, less than two years after the release of "Super Mario Kart", Sega released "Sonic Drift"; a kart racing game featuring characters from the "Sonic the Hedgehog" series. Also in 1994 Ubisoft released "Street Racer", a kart racing game for the SNES and Mega Drive/Genesis which included a four player mode not present in "Super Mario Kart". Apogee Software released "Wacky Wheels" for PC. Future games that followed in the mould of "Super Mario Kart" include "South Park Rally", "Konami Krazy Racers", "Diddy Kong Racing", "Sonic & Sega All-Stars Racing" and several racing games in the "Crash Bandicoot" series. Response to the karting games released since "Super Mario Kart" has been mixed, with GameSpot describing them as tending to be bad while 1UP.com notes that countless developers have tried to improve upon the Mario Kart formula without success.

"Super Mario Kart" is also credited as being the first non-platforming game to feature multiple playable characters from the "Mario" franchise. As well as several sequels Nintendo has released numerous other sporting and non-sporting Mario spin-offs since "Super Mario Kart"; a trend in part accredited to the commercial and critical success of the game. The "Mario" characters have appeared in many sports games including those relating to basketball, baseball, golf, tennis and soccer. Non-sporting franchises using the "Mario" characters have also been created, including the "Super Smash Bros." series of fighting games and the "Mario Party" series of board game based, party games. "Mario" series characters have also made cameos in games from other series such as "SSX on Tour" and "NBA Street V3", both published by EA Sports. The genre spanning nature of the Mario series that was sparked off by the success of "Super Mario Kart" has been described as key to the success and longevity of the franchise; keeping fans interested despite the infrequency of traditional Mario platforming games. Following this model the "Mario" series has gone on to become the best selling video game franchise of all time with 193 million units sold as of January 2007, almost 40 million units ahead of second ranked franchise ("Pokémon", also by Nintendo).

"Super Mario Kart" was re-released on the Japanese Virtual Console on June 9, 2009, and later in North America on November 23, 2009. Previously, when naming it as one of the most wanted games for the platform in November 2008, Eurogamer stated that problems emulating the Mode 7 graphics were responsible for its absence.

The game was also released for the Wii U Virtual Console in Japan during June 2013, and in Europe on March 27, 2014. In addition, North America users was able to get the game starting from August 6, 2014 to celebrate the 22nd anniversary of the game, which also includes the new game update of "Mario Kart 8" on August 27, 2014.

"Super Mario 3D World" has a stage with a look based on the Mario Circuit racetracks from "Super Mario Kart". A remixed version of the music can also be heard. "Super Mario Odyssey" also has a remix, when racing a RC car around a track in New Donk City in the Metro Kingdom.

Several sequels to "Super Mario Kart" have been brought out for successive generations of Nintendo consoles, each receiving commercial success and critical acclaim. The first of these, "Mario Kart 64" was released in 1996 for the Nintendo 64 and was the first "Mario Kart" game to feature fully 3D graphics. Although reviewers including IGN and GameSpot felt that the single player gameplay was lacking compared to its predecessor, the simultaneous four-person multiplayer modes – a first for the Nintendo 64 – were praised. The second sequel, "", was released for the Game Boy Advance in 2001. It was described by GameSpot as more of a remake of "Super Mario Kart" than a sequel to "Mario Kart 64" and featured a return to the graphical style of the original. As well as featuring all new tracks, players are able to unlock the original SNES tracks if certain achievements are completed. "" was released for the GameCube in 2003. Unlike any other "Mario Kart" game before or since, it features two riders in each kart, allowing for a new form of cooperative multiplayer where one player controls the kart's movement and the other fires weapons. "Mario Kart DS", released for the Nintendo DS in 2005, was the first "Mario Kart" game to include online play via the Nintendo Wi-Fi Connection. It went on to become the best selling hand-held racing game of all time, selling 7.83 million units. The game also marks the debut of tracks appearing in previous games. "Mario Kart Wii" was released for the Wii in 2008 and incorporates motion controls and 12-player racing. Like "Mario Kart DS", it includes on-line play; it also allows racers to play as user-created Miis (after unlocking the Mii character) as well as "Mario" series characters and comes packaged with the Wii Wheel peripheral, which can act as the game's primary control mechanism when coupled with a Wii Remote. "Mario Kart Wii" went on to be the worldwide best-selling game of 2008 ahead of another Nintendo game – "Wii Fit" – and the critically acclaimed "Grand Theft Auto IV". "Mario Kart 7" for the Nintendo 3DS was released in 2011, which features racing on land, sea, and air. Also in "Mario Kart 7" is the ability to customize your kart and to race in first-person mode. Three "Mario Kart" arcade games have also been released, "Mario Kart Arcade GP" in 2005, "Mario Kart Arcade GP 2" in 2007, and "Mario Kart Arcade GP DX" in 2013. All of them were developed jointly by Nintendo and Namco and feature classic Namco characters including Pac-Man and Blinky. The most recent entry in the series is "Mario Kart 8" for the Wii U, which was released at the end of May 2014, which brings back gliders and propellers from "Mario Kart 7" as well as 12-player racing in "Mario Kart Wii". "Mario Kart 8" also includes a new feature called Mario Kart TV, where players can watch highlights of previous races and uploading them to YouTube. Another new feature is anti-gravity racing, where players can race on walls and ceilings.

As the series has progressed, many aspects included in "Super Mario Kart" have been developed and altered. The power up boxes which are flat against the track in "Super Mario Kart" due to the technical limitations of the SNES became floating boxes in later games. The roster of racers has expanded in recent games to include a greater selection of Nintendo characters including some which had not been created at the time of "Super Mario Kart's" release – such as Petey Piranha from "Super Mario Sunshine" who appeared in "Mario Kart: Double Dash!!". Multiplayer has remained a key feature of the series and has expanded from the two-player modes available in "Super Mario Kart"; first to allow up to four simultaneous players in "Mario Kart 64" and eventually up to twelve simultaneous online players in "Mario Kart Wii". Many of the track themes have been retained throughout the series, including Rainbow Road – the final track of the Special Cup – which has appeared in every "Mario Kart" console game. Other features present in "Super Mario Kart" have disappeared from the series. These include the "super-powers" of the computer characters, the feather power up which allows players to jump high into the air and having a restricted number of lives. The only other "Mario Kart" games to feature the coin collecting of the original are "Mario Kart: Super Circuit", "Mario Kart 7", and "Mario Kart 8". The aspects of style and gameplay from "Super Mario Kart" that have been retained throughout the series have led Nintendo to face criticism for a lack of originality but the franchise is still considered to be a beloved household name by many, known for its familiar core gameplay.



</doc>
<doc id="27802" url="https://en.wikipedia.org/wiki?curid=27802" title="Seymour Papert">
Seymour Papert

Seymour Aubrey Papert (; February 29, 1928 – July 31, 2016) was a South African-born American mathematician, computer scientist, and educator, who spent most of his career teaching and researching at MIT. He was one of the pioneers of artificial intelligence, and of the constructionist movement in education. He was co-inventor, with Wally Feurzeig and Cynthia Solomon, of the Logo programming language.

Papert attended the University of the Witwatersrand, receiving a Bachelor of Arts degree in philosophy in 1949 followed by a PhD in mathematics in 1952. He then went on to receive a second doctorate, also in mathematics, at the University of Cambridge (1959), supervised by Frank Smithies.

Papert worked as a researcher in a variety of places, including St. John's College, Cambridge, the Henri Poincaré Institute at the University of Paris, the University of Geneva, and the National Physical Laboratory in London before becoming a research associate at MIT in 1963. He held this position until 1967, when he became professor of applied math and was made co-director of the MIT Artificial Intelligence Laboratory by its founding director Professor Marvin Minsky, until 1981; he also served as Cecil and Ida Green professor of education at MIT from 1974 to 1981.

Papert worked on learning theories, and was known for focusing on the impact of new technologies on learning in general, and in schools as learning organizations in particular.

At MIT, Papert went on to create the Epistemology and Learning Research Group at the MIT Architecture Machine Group which later became the MIT Media Lab. Here, he was the developer of a theory on learning called constructionism, built upon the work of Jean Piaget in constructivist learning theories. Papert had worked with Piaget at the University of Geneva from 1958 to 1963 and was one of Piaget's protégés; Piaget himself once said that "no one understands my ideas as well as Papert". Papert has rethought how schools should work, based on these theories of learning.

Papert used Piaget's work in his development of the Logo programming language while at MIT. He created Logo as a tool to improve the way children think and solve problems. A small mobile robot called the "Logo Turtle" was developed, and children were shown how to use it to solve simple problems in an environment of play. A main purpose of the Logo Foundation research group is to strengthen the ability to learn knowledge. Papert insisted a simple language or program that children can learn—like Logo—can also have advanced functionality for expert users.


As part of his work with technology, Papert has been a proponent of the Knowledge Machine. He was one of the principals for the One Laptop Per Child initiative to manufacture and distribute The Children's Machine in developing nations.

Papert also collaborated with the construction toy manufacturer Lego on their Logo-programmable Lego Mindstorms robotics kits, which were named after his groundbreaking 1980 book.

He was a leading figure in the revolutionary socialist circle around "Socialist Review" while living in London in the 1950s. Papert was also a prominent activist against South African apartheid policies during his university education.

Papert was married to Dona Strauss, and later to Androula Christofides Henriques .

Papert's third wife was MIT professor Sherry Turkle, and together they wrote the influential paper "Epistemological Pluralism and the Revaluation of the Concrete".

In his final 24 years, Papert was married to Suzanne Massie, who is a Russian scholar and author of "Pavlovsk, Life of a Russian Palace" and "Land of the Firebird".

Papert (then aged 78), received a serious brain injury when struck by a motor scooter on 5 December 2006 while crossing the street with colleague Uri Wilensky when they were both attending the 17th International Commission on Mathematical Instruction (ICMI) Study conference in Hanoi, Vietnam. He underwent emergency surgery to remove a blood clot at the French Hospital of Hanoi before being transferred in a complex operation by Swiss Air Ambulance Bombardier Challenger Jet to Boston, Massachusetts. He was moved to a hospital closer to his home in January 2007, but then contracted septicemia which damaged a heart valve, which was later replaced. By 2008 he had returned home, could think and communicate clearly and walk "almost unaided", but still had "some complicated speech problems" and was in receipt of extensive rehabilitation support. His rehabilitation team used some of the very principles of experiential, hands-on learning that he had pioneered.

Papert died at his home in Blue Hill, Maine, on July 31, 2016.

Papert's work has been used by other researchers in the fields of education and computer science. He influenced the work of Uri Wilensky in the design of NetLogo and collaborated with him on the study of knowledge restructurations, as well as the work of Andrea diSessa and the development of "dynaturtles". In 1981, Papert along with several others in the Logo group at MIT, started Logo Computer Systems Inc. (LCSI), of which he was Board Chair for over 20 years. Working with LCSI, Papert designed a number of award-winning programs, including LogoWriter and Lego/Logo (marketed as Lego Mindstorms). He also influenced the research of Idit Harel Caperton, coauthoring articles and the book "Constructionism", and chairing the advisory board of the company MaMaMedia. He also influenced Alan Kay and the Dynabook concept, and worked with Kay on various projects.

Papert won a Guggenheim fellowship in 1980, a Marconi International fellowship in 1981, the Software Publishers Association Lifetime Achievement Award in 1994, and the Smithsonian Award from "Computerworld" in 1997. Papert has been called by Marvin Minsky "the greatest living mathematics educator".

MIT President L. Rafael Reif summarized Papert's lifetime of accomplishments: “With a mind of extraordinary range and creativity, Seymour Papert helped revolutionize at least three fields, from the study of how children make sense of the world, to the development of artificial intelligence, to the rich intersection of technology and learning. The stamp he left on MIT is profound. Today, as MIT continues to expand its reach and deepen its work in digital learning, I am particularly grateful for Seymour’s groundbreaking vision, and we hope to build on his ideas to open doors to learners of all ages, around the world.”


</doc>
<doc id="27804" url="https://en.wikipedia.org/wiki?curid=27804" title="Search engine (computing)">
Search engine (computing)

A search engine is an information retrieval system designed to help find information stored on a computer system. The search results are usually presented in a list and are commonly called "hits". Search engines help to minimize the time required to find information and the amount of information which must be consulted, akin to other techniques for managing information overload. 

The most public, visible form of a search engine is a Web search engine which searches for information on the World Wide Web.

Search engines provide an interface to a group of items that enables users to specify criteria about an item of interest and have the engine find the matching items. The criteria are referred to as a search query. In the case of text search engines, the search query is typically expressed as a set of words that identify the desired concept that one or more documents may contain. There are several styles of search query syntax that vary in strictness. It can also switch names within the search engines from previous sites. Whereas some text search engines require users to enter two or three words separated by white space, other search engines may enable users to specify entire documents, pictures, sounds, and various forms of natural language. Some search engines apply improvements to search queries to increase the likelihood of providing a quality set of items through a process known as query expansion. Query understanding methods can be used as standardize query language. 

The list of items that meet the criteria specified by the query is typically sorted, or ranked. Ranking items by relevance (from highest to lowest) reduces the time required to find the desired information. Probabilistic search engines rank items based on measures of similarity (between each item and the query, typically on a scale of 1 to 0, 1 being most similar) and sometimes popularity or authority (see Bibliometrics) or use relevance feedback. Boolean search engines typically only return items which match exactly without regard to order, although the term "boolean search engine" may simply refer to the use of boolean-style syntax (the use of operators AND, OR, NOT, and XOR) in a probabilistic context.

To provide a set of matching items that are sorted according to some criteria quickly, a search engine will typically collect metadata about the group of items under consideration beforehand through a process referred to as indexing. The index typically requires a smaller amount of computer storage, which is why some search engines only store the indexed information and not the full content of each item, and instead provide a method of navigating to the items in the search engine result page. Alternatively, the search engine may store a copy of each item in a cache so that users can see the state of the item at the time it was indexed or for archive purposes or to make repetitive processes work more efficiently and quickly.

Other types of search engines do not store an index. Crawler, or spider type search engines (a.k.a. real-time search engines) may collect and assess items at the time of the search query, dynamically considering additional items based on the contents of a starting item (known as a seed, or seed URL in the case of an Internet crawler). Meta search engines store neither an index nor a cache and instead simply reuse the index or results of one or more other search engine to provide an aggregated, final set of results.










</doc>
<doc id="27805" url="https://en.wikipedia.org/wiki?curid=27805" title="Spaced repetition">
Spaced repetition

Spaced repetition is a learning technique that incorporates increasing intervals of time between subsequent review of previously learned material in order to exploit the psychological spacing effect. Alternative names include "spaced rehearsal", "expanding rehearsal", "graduated intervals", "repetition spacing", "repetition scheduling", "spaced retrieval" and "expanded retrieval".

Although the principle is useful in many contexts, spaced repetition is commonly applied in contexts in which a learner must acquire a large number of items and retain them indefinitely in memory. It is, therefore, well suited for the problem of vocabulary acquisition in the course of second language learning, due to the size of the target language's inventory of open-class words.

The notion that spaced repetition could be used for improving learning was first proposed in the book "Psychology of Study" by Prof. C. A. Mace in 1932: "Perhaps the most important discoveries are those which relate to the appropriate distribution of the periods of study...Acts of revision should be spaced in gradually increasing intervals, roughly intervals of one day, two days, four days, eight days, and so on."

In 1939, H. F. Spitzer tested the effects of a type of spaced repetition on sixth-grade students in Iowa learning science facts. Spitzer tested over 3600 students in Iowa and showed that spaced repetition was effective. This early work went unnoticed, and the field was relatively quiet until the late 1960s when cognitive psychologists, including Melton and Landauer & Bjork, explored manipulation of repetition timing as a means to improve recall. Around the same time, Pimsleur language courses pioneered the practical application of spaced repetition theory to language learning, and in 1973 Sebastian Leitner devised his "Leitner system", an all-purpose spaced repetition learning system based on flashcards.

With the increase in access to personal computers in the 1980s, spaced repetition began to be implemented with computer-assisted language learning software-based solutions, enabling automated scheduling and statistic gathering, scaling to thousands of cards scheduled individually. To enable the user to reach a target level of achievement (e.g. 90% of all material correctly recalled at any given time point), the software adjusts the repetition spacing interval. Material that is hard appears more often and material that is easy less often, with difficulty defined according to the ease with which the user is able to produce a correct response.

There are several families of algorithms for scheduling spaced repetition:

Some have theorized that the precise length of intervals does not have a great impact on algorithm effectiveness, although it has been suggested by others that the interval (expanded vs. fixed interval, etc.) is quite important. The experimental results regarding this point are mixed.

Graduated-interval recall is a type of spaced repetition published by Paul Pimsleur in 1967. It is used in the Pimsleur language learning system and is particularly suited to programmed audio instruction due to the very short times (measured in seconds or minutes) between the first few repetitions, as compared to other forms of spaced repetition which may not require such precise timings.

The intervals published in Pimsleur's paper were: 5 seconds, 25 seconds, 2 minutes, 10 minutes, 1 hour, 5 hours, 1 day, 5 days, 25 days, 4 months, and 2 years.

By timing a Pimsleur language program with a stopwatch, it is possible to verify that the intervals are not followed exactly but have upper and lower bounds. A similar principle (graduated intervals with upper and lower bounds) is used in at least one open source software project (Gradint) to schedule audio-only lessons.

Most spaced repetition software (SRS) programs are modeled after the manual style of learning with physical flashcards: items to memorize are entered into the program as question-answer pairs. When a pair is due to be reviewed, the question is displayed on screen, and the user must attempt to answer. After answering, the user manually reveals the answer and then tells the program (subjectively) how difficult answering was. The program schedules pairs based on spaced repetition algorithms.
Without a program, the user has to schedule physical flashcards; this is time-intensive and limits users to simple algorithms like the Leitner system.

Further refinements with regard to software:

Notable implementations include Anki, Brainscape, Cerego, Course Hero, Duolingo, Lingvist, Memrise, Mnemosyne, Quizlet, Skritter, SuperMemo, Synap and WaniKani.



</doc>
<doc id="27806" url="https://en.wikipedia.org/wiki?curid=27806" title="SuperMemo">
SuperMemo

SuperMemo (from "Super Memory") is a learning method and software package developed by SuperMemo World and SuperMemo R&D with Piotr Woźniak in Poland from 1985 to the present. It is based on research into long-term memory, and is a practical application of the spaced repetition learning method that has been proposed for efficient instruction by a number of psychologists as early as in the 1930s. 

According to the developers of SuperMemo and some other proponents of spaced repetition learning, the process can optimize long-term knowledge acquisition.

The method is available as a computer program for Windows, Windows CE, Windows Mobile, (Pocket PC), iPhone, iPad, iPod Touch, (iTunes), Palm OS (PalmPilot), etc. It can also be used in a web browser or even without a computer.

The desktop version of SuperMemo (since v. 2002) supports incremental reading.

The SuperMemo program stores a database of questions and answers constructed by the user. When reviewing information saved in the database, the program uses the SuperMemo algorithm to decide what questions to show the user. The user then answers the question and rates their recall - did they answer the question easily, with hesitation, not at all, and so on - and their rating is used to calculate how soon they should be shown the question again. While the exact algorithm varies with the version of SuperMemo, in general, items that are harder to remember show up more frequently.

Besides simple text questions and answers, the latest version of SuperMemo supports images, video, and HTML questions and answers.

The specific algorithms SuperMemo uses have been published, and re-implemented in other programs.

Different algorithms have been used; SM–0 refers to the original (non-computer-based) algorithm, while SM-2 refers to the original computer-based algorithm released in the 1987 (used in SuperMemo versions 1.0 through 3.0, referred to as SM-2 because SuperMemo version 2 was the most popular of these). Subsequent versions of the software have further optimized the algorithm.

As of June 2016, the latest version of the SuperMemo algorithm is SM-17, released in 2016.

The SM-2 algorithm uses the performance on a card to schedule only that card, while SM-3 and newer algorithms use card performance to schedule that card and similar cards. The additional optimizations sometimes yield perverse results – answering "hard" on a card may yield an interval longer than answering "easy" on a card – and are criticized as reducing the robustness of the algorithm, making it more sensitive to variations – non-uniform difficulty of cards (a problem in versions 4 to 6, according to Woźniak), inconsistencies in studying, and so forth.

Woźniak disagreed with the criticism, but noted that in practice the other factors affecting study make it not very important.

Some of the algorithms have been reimplemented in other, often free programs such as Anki, Mnemosyne, and Emacs Org-mode's Org-drill. See full list of flashcard software.

The SM-2 algorithm has proven most popular in other applications, and is used (in modified form) in Anki and Mnemosyne, among others. Org-drill implements SM-5 by default, and optionally other algorithms such as SM-2.



</doc>
<doc id="27808" url="https://en.wikipedia.org/wiki?curid=27808" title="Samuel Pepys">
Samuel Pepys

Samuel Pepys ( ; 23 February 1633 – 26 May 1703) was an administrator of the navy of England and Member of Parliament who is most famous for the diary he kept for a decade while still a relatively young man. Pepys had no maritime experience, but he rose to be the Chief Secretary to the Admiralty under both King Charles II and King James II through patronage, hard work, and his talent for administration. His influence and reforms at the Admiralty were important in the early professionalisation of the Royal Navy.

The detailed private diary that Pepys kept from 1660 until 1669 was first published in the 19th century and is one of the most important primary sources for the English Restoration period. It provides a combination of personal revelation and eyewitness accounts of great events, such as the Great Plague of London, the Second Dutch War, and the Great Fire of London.

Pepys was born in Salisbury Court, Fleet Street, London on 23 February 1633, the son of John Pepys (1601–1680), a tailor, and Margaret Pepys ("née" Kite; died 1667), daughter of a Whitechapel butcher. His great uncle Talbot Pepys was Recorder and briefly Member of Parliament (MP) for Cambridge in 1625. His father's first cousin Sir Richard Pepys was elected MP for Sudbury in 1640, appointed Baron of the Exchequer on 30 May 1654, and appointed Lord Chief Justice of Ireland on 25 September 1655.

Pepys was the fifth of eleven children, but child mortality was high and he was soon the oldest survivor. He was baptised at St Bride's Church on 3 March 1633. Pepys did not spend all of his infancy in London; for a while, he was sent to live with nurse Goody Lawrence at Kingsland, just north of the city. In about 1644, Pepys attended Huntingdon Grammar School before being educated at St Paul's School, London, c. 1646–1650. He attended the execution of Charles I in 1649.

In 1650, he went to the University of Cambridge, having received two exhibitions from St Paul's School (perhaps owing to the influence of Sir George Downing, who was chairman of the judges and for whom he later worked at the Exchequer) and a grant from the Mercers' Company. In October, he was admitted as a sizar to Magdalene College; he moved there in March 1651 and took his Bachelor of Arts degree in 1654.

Later in 1654 or early in 1655, he entered the household of another of his father's cousins, Sir Edward Montagu, who was later created 1st Earl of Sandwich.

Pepys married fourteen-year-old Elisabeth de St Michel, a descendant of French Huguenot immigrants, first in a religious ceremony on 10 October 1655 and later in a civil ceremony on 1 December 1655 at St Margaret's, Westminster.

From a young age, Pepys suffered from bladder stones in his urinary tract—a condition from which his mother and brother John also later suffered. He was almost never without pain, as well as other symptoms, including "blood in the urine" (hematuria). By the time of his marriage, the condition was very severe.

In 1657 Pepys decided to undergo surgery; not an easy option, as the operation was known to be especially painful and hazardous. Nevertheless, Pepys consulted surgeon Thomas Hollier and, on 26 March 1658, the operation took place in a bedroom in the house of Pepys' cousin Jane Turner. Pepys' stone was successfully removed and he resolved to hold a celebration on every anniversary of the operation, which he did for several years. However, there were long-term effects from the operation. The incision on his bladder broke open again late in his life. The procedure may have left him sterile, though there is no direct evidence for this, as he was childless before the operation. In mid-1658 Pepys moved to Axe Yard, near the modern Downing Street. He worked as a teller in the Exchequer under George Downing.

On 1 January 1660 ("1 January 1659/1660" in contemporary terms), Pepys began to keep a diary. He recorded his daily life for almost ten years. This record of a decade of Pepys' life is more than a million words long and is often regarded as Britain’s most celebrated diary. Pepys has been called the greatest diarist of all time due to his frankness in writing concerning his own weaknesses and the accuracy with which he records events of daily British life and major events in the 17th century. Pepys wrote about the contemporary court and theatre (including his amorous affairs with the actresses), his household, and major political and social occurrences.

Historians have been using his diary to gain greater insight and understanding of life in London in the 17th century. Pepys wrote consistently on subjects such as personal finances, the time he got up in the morning, the weather, and what he ate. He talked at length about his new watch which he was very proud of (and which had an alarm, a new thing at the time), a country visitor who did not enjoy his time in London because he felt that it was too crowded, and his cat waking him up at one in the morning. Pepys's diary is one of the only known sources which provides such length in details of everyday life of an upper-middle-class man during the seventeenth century.

Aside from day-to-day activities, Pepys also commented on the significant and turbulent events of his nation. England was in disarray when he began writing his diary. Oliver Cromwell had died just a few years before, creating a period of civil unrest and a large power vacuum to be filled. Pepys had been a strong supporter of Cromwell, but he converted to the Royalist cause upon the Protector’s death. He was on the ship that brought Charles II home to England. He gave a firsthand account of events, such as the coronation of King Charles II and the Restoration of the British Monarchy to the throne, the Anglo-Dutch war, the Great Plague, and the Great Fire of London.

Pepys did not plan on his contemporaries ever seeing his diary, which is evident from the fact that he wrote in shorthand and sometimes in a "code" of various Spanish, French, and Italian words (especially when describing his illicit affairs). However, Pepys often juxtaposed profanities in his native English amidst his "code" of foreign words, a practice which would reveal the details to any casual reader. He did intend future generations to see the diary, as evidenced by its inclusion in his library and its catalogue before his death along with the shorthand guide he used and the elaborate planning by which he ensured his library survived intact after his death. 
The women whom he pursued, his friends, and his dealings are all laid out. His diary reveals his jealousies, insecurities, trivial concerns, and his fractious relationship with his wife. It has been an important account of London in the 1660s. The juxtaposition of his commentary on politics and national events, alongside the very personal, can be seen from the beginning. His opening paragraphs, written in January 1660, begin:

The entries from the first few months were filled with news of General George Monck's march on London. In April and May of that year, he was encountering problems with his wife, and he accompanied Montagu's fleet to the Netherlands to bring Charles II back from exile. Montagu was made Earl of Sandwich on 18 June, and Pepys secured the position of Clerk of the Acts to the Navy Board on 13 July. As secretary to the board, Pepys was entitled to a £350 annual salary plus the various gratuities and benefits that came with the job–including bribes. He rejected an offer of £1,000 for the position from a rival and soon afterwards moved to official accommodation in Seething Lane in the City of London.

Pepys stopped writing his diary in 1669. His eyesight began to trouble him and he feared that writing in dim light was damaging his eyes. He did imply in his last entries that he might have others write his diary for him, but doing so would result in a loss of privacy and it seems that he never went through with those plans. In the end, Pepys' fears were unjustified and he lived another 34 years without going blind, but he never took to writing his diary again.

However, Pepys dictated a journal for two months in 1669–70 as a record of his dealings with the Commissioners of Accounts at that period. He also kept a diary for a few months in 1683 when he was sent to Tangier, Morocco as the most senior civil servant in the navy, during the English evacuation. The diary mostly covers work-related matters.

On the Navy Board, Pepys proved to be a more able and efficient worker than colleagues in higher positions. This often annoyed Pepys and provoked much harsh criticism in his diary. Among his colleagues were Admiral Sir William Penn, Sir George Carteret, Sir John Mennes and Sir William Batten.

Pepys learned arithmetic from a private tutor and used models of ships to make up for his lack of first-hand nautical experience, and ultimately came to play a significant role in the board's activities. In September 1660, he was made a Justice of the Peace; on 15 February 1662, Pepys was admitted as a Younger Brother of Trinity House; and on 30 April, he received the freedom of Portsmouth. Through Sandwich, he was involved in the administration of the short-lived English colony at Tangier. He joined the Tangier committee in August 1662 when the colony was first founded and became its treasurer in 1665. In 1663, he independently negotiated a £3,000 contract for Norwegian masts, demonstrating the freedom of action that his superior abilities allowed. He was appointed to a commission of the royal fishery on 8 April 1664.

Pepys' job required him to meet many people to dispense money and make contracts. He often laments how he "lost his labour" having gone to some appointment at a coffee house or tavern, only to discover that the person was not there whom he was seeking. These occasions were a constant source of frustration to Pepys.

Pepys' diary provides a first-hand account of the Restoration, and it is also notable for its detailed accounts of several major events of the 1660s, along with the lesser known diary of John Evelyn. In particular, it is an invaluable source for the study of the Second Anglo-Dutch War of 1665–7, the Great Plague of 1665, and the Great Fire of London in 1666. In relation to the Plague and Fire, C. S. Knighton has written: "From its reporting of these two disasters to the metropolis in which he thrived, Pepys's diary has become a national monument." Robert Latham, editor of the definitive edition of the diary, remarks concerning the Plague and Fire: "His descriptions of both—agonisingly vivid—achieve their effect by being something more than superlative reporting; they are written with compassion. As always with Pepys it is people, not literary effects, that matter."

In early 1665, the start of the Second Anglo-Dutch War placed great pressure on Pepys. His colleagues were either engaged elsewhere or incompetent, and Pepys had to conduct a great deal of business himself. He excelled under the pressure, which was extreme due to the complexity and under-funding of the Royal Navy. At the outset, he proposed a centralised approach to supplying the fleet. His idea was accepted, and he was made surveyor-general of victualling in October 1665. The position brought a further £300 a year.

Pepys wrote about the Second Anglo-Dutch War: "In all things, in wisdom, courage, force and success, the Dutch have the best of us and do end the war with victory on their side". And King Charles II said: "Don't fight the Dutch, imitate them".

In 1667, with the war lost, Pepys helped to discharge the navy. The Dutch had defeated England on open water and now began to threaten the mainland itself. In June 1667, they conducted their Raid on the Medway, broke the defensive chain at Gillingham, and towed away the , one of the Royal Navy's most important ships. As he had done during the Fire and the Plague, Pepys again removed his wife and his gold from London.

The Dutch raid was a major concern in itself, but Pepys was personally placed under a different kind of pressure: the Navy Board and his role as Clerk of the Acts came under scrutiny from the public and from Parliament. The war ended in August and, on 17 October, the House of Commons created a committee of "miscarriages". On 20 October, a list was demanded from Pepys of ships and commanders at the time of the division of the fleet in 1666. However, these demands were actually quite desirable for him, as tactical and strategic mistakes were not the responsibility of the Navy Board.

The Board did face some allegations regarding the Medway raid, but they could exploit the criticism already attracted by commissioner of Chatham Peter Pett to deflect criticism from themselves. The committee accepted this tactic when they reported in February 1668. The Board was, however, criticised for its use of tickets to pay seamen. These tickets could only be exchanged for cash at the Navy's treasury in London. Pepys made a long speech at the bar of the Commons on 5 March 1668 defending this practice. It was, in the words of C. S. Knighton, a "virtuoso performance".

The commission was followed by an investigation led by a more powerful authority, the commissioners of accounts. They met at Brooke House, Holborn and spent two years scrutinising how the war had been financed. In 1669, Pepys had to prepare detailed answers to the committee's eight "Observations" on the Navy Board's conduct. In 1670, he was forced to defend his own role. A seaman's ticket with Pepys' name on it was produced as incontrovertible evidence of his corrupt dealings but, thanks to the intervention of the king, Pepys emerged from the sustained investigation relatively unscathed.

Outbreaks of plague were not particularly unusual events in London; major epidemics had occurred in 1592, 1603, 1625 and 1636. Furthermore, Pepys was not among the group of people who were most at risk. He did not live in cramped housing, he did not routinely mix with the poor, and he was not required to keep his family in London in the event of a crisis. It was not until June 1665 that the unusual seriousness of the plague became apparent, so Pepys's activities in the first five months of 1665 were not significantly affected by it. Indeed, Claire Tomalin writes that "the most notable fact about Pepys's plague year is that to him it was one of the happiest of his life." In 1665, he worked very hard, and the outcome was that he quadrupled his fortune. In his annual summary on 31 December, he wrote, "I have never lived so merrily (besides that I never got so much) as I have done this plague time". Nonetheless, Pepys was certainly concerned about the plague. On 16 August he wrote:

He also chewed tobacco as a protection against infection, and worried that wig-makers might be using hair from the corpses as a raw material. Furthermore, it was Pepys who suggested that the Navy Office should evacuate to Greenwich, although he did offer to remain in town himself. He later took great pride in his stoicism. Meanwhile, Elisabeth Pepys was sent to Woolwich. She did not return to Seething Lane until January 1666, and was shocked by the sight of St Olave's churchyard, where 300 people had been buried.

In the early hours of 2 September 1666, Pepys was awakened by his servant who had spotted a fire in the Billingsgate area. He decided that the fire was not particularly serious and returned to bed. Shortly after waking, his servant returned and reported that 300 houses had been destroyed and that London Bridge was threatened. Pepys went to the Tower to get a better view. Without returning home, he took a boat and observed the fire for over an hour. In his diary, Pepys recorded his observations as follows:

The wind was driving the fire westward, so he ordered the boat to go to Whitehall and became the first person to inform the king of the fire. According to his entry of 2 September 1666, Pepys recommended to the king that homes be pulled down in the path of the fire in order to stem its progress. Accepting this advice, the king told him to go to Lord Mayor Thomas Bloodworth and tell him to start pulling down houses. Pepys took a coach back as far as St Paul's Cathedral before setting off on foot through the burning city. He found the Lord Mayor, who said, "Lord! what can I do? I am spent: people will not obey me. I have been pulling down houses; but the fire overtakes us faster than we can do it." At noon, he returned home and "had an extraordinary good dinner, and as merry, as at this time we could be", before returning to watch the fire in the city once more. Later, he returned to Whitehall, then met his wife in St. James's Park. In the evening, they watched the fire from the safety of Bankside. Pepys writes that "it made me weep to see it". Returning home, Pepys met his clerk Tom Hayter who had lost everything. Hearing news that the fire was advancing, he started to pack up his possessions by moonlight.

A cart arrived at 4 a.m. on 3 September and Pepys spent much of the day arranging the removal of his possessions. Many of his valuables, including his diary, were sent to a friend from the Navy Office at Bethnal Green. At night, he "fed upon the remains of yesterday's dinner, having no fire nor dishes, nor any opportunity of dressing any thing." The next day, Pepys continued to arrange the removal of his possessions. By then, he believed that Seething Lane was in grave danger, so he suggested calling men from Deptford to help pull down houses and defend the king's property. He described the chaos in the city and his curious attempt at saving his own goods:

Pepys had taken to sleeping on his office floor; on Wednesday, 5 September, he was awakened by his wife at 2 a.m. She told him that the fire had almost reached All Hallows-by-the-Tower and that it was at the foot of Seething Lane. He decided to send her and his gold—about £2,350—to Woolwich. In the following days, Pepys witnessed looting, disorder, and disruption. On 7 September, he went to Paul's Wharf and saw the ruins of St Paul's Cathedral, of his old school, of his father's house, and of the house in which he had had his stone removed. Despite all this destruction, Pepys's house, office, and diary were saved.

The diary gives a detailed account of Pepys' personal life. He liked wine, plays, and the company of other people. He also spent time evaluating his fortune and his place in the world. He was always curious and often acted on that curiosity, as he acted upon almost all his impulses. Periodically, he would resolve to devote more time to hard work instead of leisure. For example, in his entry for New Year's Eve, 1661, he writes: "I have newly taken a solemn oath about abstaining from plays and wine…" The following months reveal his lapses to the reader; by 17 February, it is recorded, "Here I drank wine upon necessity, being ill for the want of it."

Pepys was one of the most important civil servants of his age, and was also a widely cultivated man, taking an interest in books, music, the theatre and science. He was passionately interested in music; he composed, sang, and played for pleasure, and even arranged music lessons for his servants. He played the lute, viol, violin, flageolet, recorder and spinet to varying degrees of proficiency. He was also a keen singer, performing at home, in coffee houses, and even in Westminster Abbey. He and his wife took flageolet lessons from master Thomas Greeting. He also taught his wife to sing and paid for dancing lessons for her (although these stopped when he became jealous of the dancing master).

Pepys was known to be brutal to his servants, once beating a servant Jane with a broom until she cried. He kept a boy servant whom he frequently beat with a cane, a birch rod, a whip or a rope’s end.

Propriety did not prevent him from engaging in a number of extramarital liaisons with various women that were chronicled in his diary, often in some detail, and generally using a cocktail of languages (English, French, Spanish and Latin) when relating the intimate details. The most dramatic of these encounters was with Deborah Willet, a young woman engaged as a companion for Elisabeth Pepys. On 25 October 1668, Pepys was surprised by his wife as he embraced Deb Willet; he writes that his wife "coming up suddenly, did find me imbracing the girl con "[with]" my hand sub "[under]" su "[her]" coats; and endeed I was with my main "[hand]" in her cunny. I was at a wonderful loss upon it and the girl also..." Following this event, he was characteristically filled with remorse, but (equally characteristically) continued to pursue Willet after she had been dismissed from the Pepys household.

"Mrs Knep was the wife of a Smithfield horsedealer, and the mistress of Pepys"—or at least "she granted him a share of her favours". Scholars disagree on the full extent of the Pepys/Knep relationship, but much of later generations' knowledge of Knep comes from the diary. Pepys first met Knep on 6 December 1665. He described her as "pretty enough, but the most excellent, mad-humoured thing, and sings the noblest that I ever heard in my life." He called her husband "an ill, melancholy, jealous-looking fellow" and suspected him of abusing his wife. Knep provided Pepys with backstage access and was a conduit for theatrical and social gossip. When they wrote notes to each other, Pepys signed himself "Dapper Dickey," while Knep was "Barbry Allen" (that popular song was an item in her musical repertory).

Pepys was known for sexually assaulting his female servants, often fondling his maid Mary Mercer while she dressed him in the morning.

The diary was written in one of the many standard forms of shorthand used in Pepys' time, in this case called tachygraphy and devised by Thomas Shelton. It is clear from its content that it was written as a purely personal record of his life and not for publication, yet there are indications that Pepys took steps to preserve the bound manuscripts of his diary. He wrote it out in fair copy from rough notes, and he also had the loose pages bound into six volumes, catalogued them in his library with all his other books, and is likely to have suspected that eventually someone would find them interesting.

This tree resumes, in a more compact form and with a few additional details, trees published elsewhere in a box-like form. It is meant to help the reader of the "Diary" and also integrates some biographical informations found in the same sources.


Pepys' health suffered from the long hours that he worked throughout the period of the diary. Specifically, he believed that his eyesight had been affected by his work. He reluctantly concluded in his last entry, dated 31 May 1669, that he should completely stop writing for the sake of his eyes, and only dictate to his clerks from then on, which meant that he could no longer keep his diary.

Pepys and his wife took a holiday to France and the Low Countries in June–October 1669; on their return, Elisabeth fell ill and died on 10 November 1669. Pepys erected a monument to her in the church of St Olave's, Hart Street, London. Pepys never remarried, but he did have a long-term housekeeper named Mary Skinner who was assumed by many of his contemporaries to be his mistress and sometimes referred to as Mrs. Pepys. In his will, he left her an annuity of £200 and many of his possessions.

In 1672 he became an Elder Brother of Trinity House and served in this capacity until 1689; he was Master of Trinity House in 1676–1677 and again in 1685–1686. In 1673 he was promoted to Secretary to the Admiralty Commission and elected MP for Castle Rising in Norfolk.

In 1673 he was involved with the establishment of the Royal Mathematical School at Christ's Hospital, which was to train 40 boys annually in navigation, for the benefit of the Royal Navy and the English Merchant Navy. In 1675 he was appointed a Governor of Christ's Hospital and for many years he took a close interest in its affairs. Among his papers are two detailed memoranda on the administration of the school. In 1699, after the successful conclusion of a seven-year campaign to get the master of the Mathematical School replaced by a man who knew more about the sea, he was rewarded for his service as a Governor by being made a Freeman of the City of London. He also served as Master (without ever hanving been a Freeman or Liveryman) of the Clothworkers' Company (1677-8).

At the beginning of 1679 Pepys was elected MP for Harwich in Charles II's third parliament which formed part of the Cavalier Parliament. He was elected along with Sir Anthony Deane, a Harwich alderman and leading naval architect, to whom Pepys had been patron since 1662. By May of that year, they were under attack from their political enemies. Pepys resigned as Secretary to the Admiralty. They were imprisoned in the Tower of London on suspicion of treasonable correspondence with France, specifically leaking naval intelligence. The charges are believed to have been fabricated under the direction of Anthony Ashley-Cooper, 1st Earl of Shaftesbury. Pepys was accused, among other things, of being a papist. They were released in July, but proceedings against them were not dropped until June 1680.
Though he had resigned from the Tangier committee in 1679, in 1683 he was sent to Tangier to assist Lord Dartmouth with the evacuation and abandonment of the English colony. After six months' service, he travelled back through Spain accompanied by the naval engineer Edmund Dummer, returning to England after a particularly rough passage on 30 March 1684. In June 1684, once more in favour, he was appointed King's Secretary for the affairs of the Admiralty, a post that he retained after the death of Charles II (February 1685) and the accession of James II. The phantom Pepys Island, alleged to be near South Georgia, was named after him in 1684, having been first "discovered" during his tenure at the Admiralty.

From 1685 to 1688, he was active not only as Secretary for the Admiralty, but also as MP for Harwich. He had been elected MP for Sandwich, but this election was contested and he immediately withdrew to Harwich. When James fled the country at the end of 1688, Pepys's career also came to an end. In January 1689, he was defeated in the parliamentary election at Harwich; in February, one week after the accession of William III and Mary II, he resigned his secretaryship.

He was elected a Fellow of the Royal Society in 1665 and served as its President from 1 December 1684 to 30 November 1686. Isaac Newton's "Principia Mathematica" was published during this period, and its title page bears Pepys' name. There is a probability problem called the "Newton–Pepys problem" that arose out of correspondence between Newton and Pepys about whether one is more likely to roll at least one six with six dice or at least two sixes with twelve dice. It has only recently been noted that the gambling advice which Newton gave Pepys was correct, while the logical argument with which Newton accompanied it was unsound.

He was imprisoned on suspicion of Jacobitism from May to July 1689 and again in June 1690, but no charges were ever successfully brought against him. After his release, he retired from public life at age 57. He moved out of London ten years later (1701) to a house in Clapham owned by his friend William Hewer, who had begun his career working for Pepys in the admiralty. Clapham was in the country at the time; it is now part of inner London.

Pepys lived there until his death on 26 May 1703. He had no children and bequeathed his estate to his unmarried nephew John Jackson. Pepys had disinherited his nephew Samuel Jackson for marrying contrary to his wishes. When John Jackson died in 1724, Pepys' estate reverted to Anne, daughter of Archdeacon Samuel Edgeley, niece of Will Hewer and sister of Hewer Edgeley, nephew and godson of Pepys' old Admiralty employee and friend Will Hewer. Hewer was also childless and left his immense estate to his nephew Hewer Edgeley (consisting mostly of the Clapham property, as well as lands in Clapham, London, Westminster and Norfolk) on condition that the nephew (and godson) would adopt the surname Hewer. So Will Hewer's heir became Hewer Edgeley-Hewer, and he adopted the old Will Hewer home in Clapham as his residence. That is how the Edgeley family acquired the estates of both Samuel Pepys and Will Hewer, sister Anne inheriting Pepys' estate, and brother Hewer inheriting that of Will Hewer. On the death of Hewer Edgeley-Hewer in 1728, the old Hewer estate went to Edgeley-Hewer's widow Elizabeth, who left the estate to Levett Blackborne, the son of Abraham Blackborne, merchant of Clapham, and other family members, who later sold it off in lots. Lincoln's Inn barrister Levett Blackborne also later acted as attorney in legal scuffles for the heirs who had inherited the Pepys estate. 

Pepys' former protégé and friend Hewer acted as the executor of Pepys' estate.

Pepys was buried along with his wife in St Olave Hart Street in London.

Pepys was a lifelong bibliophile and carefully nurtured his large collection of books, manuscripts, and prints. At his death, there were more than 3,000 volumes, including the diary, all carefully catalogued and indexed; they form one of the most important surviving 17th-century private libraries. The most important items in the Library are the six original bound manuscripts of Pepys's diary, but there are other remarkable holdings, including:


Pepys made detailed provisions in his will for the preservation of his book collection. His nephew and heir John Jackson died in 1723, when it was transferred intact to Magdalene College, Cambridge, where it can be seen in the Pepys Building. The bequest included all the original bookcases and his elaborate instructions that placement of the books "be strictly reviewed and, where found requiring it, more nicely adjusted".

Motivated by the publication of Evelyn's Diary, Lord Granville deciphered a few pages. John Smith (later the Rector of St Mary the Virgin in Baldock) was then engaged to transcribe the diaries into plain English. He laboured at this task for three years, from 1819 to 1822, unaware until nearly finished that a key to the shorthand system was stored in Pepys' library a few shelves above the diary volumes. Others had apparently succeeded in reading the diary earlier, perhaps knowing about the key, because a work of 1812 quotes from a passage of it. Smith's transcription, which is also kept in the Pepys Library, was the basis for the first published edition of the diary, edited by Lord Braybrooke, released in two volumes in 1825.

A second transcription, done with the benefit of the key, but often less accurately, was completed in 1875 by Mynors Bright and published in 1875–1879. This added about a third to the previously published text, but still left only about 80% of the diary in print. Henry B. Wheatley, drawing on both his predecessors, produced a new edition in 1893–1899, revised in 1926, with extensive notes and an index.

All of these editions omitted passages (chiefly about Pepys' sexual adventures) which the editors thought too obscene ever to be printed. Wheatley, in the preface to his edition noted, "a few passages which cannot possibly be printed. It may be thought by some that these omissions are due to an unnecessary squeamishness, but it is not really so, and readers are therefore asked to have faith in the judgement of the editor."

The complete, unexpurgated, and definitive edition, edited and transcribed by Robert Latham and William Matthews, was published by Bell & Hyman, London, and the University of California Press, Berkeley, in nine volumes, along with separate Companion and Index volumes, over the years 1970–1983. Various single-volume abridgements of this text are also available.

The Introduction in volume I provides a scholarly but readable account of "The Diarist", "The Diary" ("The Manuscript", "The Shorthand", and "The Text"), "History of Previous Editions", "The Diary as Literature", and "The Diary as History". The Companion provides a long series of detailed essays about Pepys and his world.

The first unabridged recording of the diary as an audiobook was published in 2015 by "Naxos AudioBooks".

On 1 January 2003 Phil Gyford started a weblog, pepysdiary.com, that serialised the diary one day each evening together with annotations from public and experts alike. In December 2003 the blog won the best specialist blog award in "The Guardian"<nowiki>'</nowiki>s Best of British Blogs.

In 1958 the BBC produced a serial called "Samuel Pepys!", in which Peter Sallis played the title role. In 2003 a television film "The Private Life of Samuel Pepys" aired on BBC2. Steve Coogan played Pepys. The 2004 film "Stage Beauty" concerns London theatre in the 17th century and is based on Jeffrey Hatcher's play "Compleat Female Stage Beauty", which in turn was inspired by a reference in Pepys's diary to the actor Edward Kynaston, who played female roles in the days when women were forbidden to appear on stage. Pepys is a character in the film and is portrayed as an ardent devotee of the theatre. Hugh Bonneville plays Pepys. Daniel Mays portrays Pepys in "The Great Fire", a 2014 BBC television miniseries. Pepys has also been portrayed in various other film and television productions, played by diverse actors including Mervyn Johns, Michael Palin, Michael Graham Cox and Philip Jackson.

BBC Radio 4 has broadcast serialised radio dramatisations of the diary. In the 1990s it was performed as a "Classic Serial" starring Bill Nighy, and in the 2010s it was serialised as part of the "Woman's Hour" radio magazine programme. One audiobook edition of Pepys' diary selections is narrated by Kenneth Branagh. A fictionalised Pepys narrates the second chapter of Harry Turtledove's science fiction novel "A Different Flesh" (serialised 1985–1988, book form 1988). This chapter is entitled "And So to Bed" and written in the form of entries from the Pepys diary. The entries detail Pepys' encounter with American "Homo erectus" specimens (imported to London as beasts of burden) and his formation of the "transformational theory of life," thus causing evolutionary theory to gain a foothold in scientific thought in the 17th century rather than the 19th. Deborah Swift's 2017 novel "Pleasing Mr Pepys" is described as a "re-imagining of the events in Samuel Pepys's Diary".

Several detailed studies of Pepys' life are available. Arthur Bryant published his three-volume study in 1933–1938, long before the definitive edition of the diary, but, thanks to Bryant's lively style, it is still of interest. In 1974 Richard Ollard produced a new biography that drew on Latham's and Matthew's work on the text, benefitting from the author's deep knowledge of Restoration politics. Other biographies include: "Samuel Pepys: A Life", by Stephen Coote (London: Hodder & Stoughton, 2000) and, "Samuel Pepys and His World", by Geoffrey Trease (London: Thames and Hudson, 1972).

The most recent general study is by Claire Tomalin, which won the 2002 Whitbread Book of the Year award, the judges calling it a "rich, thoughtful and deeply satisfying" account that unearths "a wealth of material about the uncharted life of Samuel Pepys".



Editions of letters and other publications by Pepys

" The Diary".





 


</doc>
<doc id="27809" url="https://en.wikipedia.org/wiki?curid=27809" title="Chemical synapse">
Chemical synapse

Chemical synapses are biological junctions through which neurons' signals can be exchanged to each other and to non-neuronal cells such as those in muscles or glands. Chemical synapses allow neurons to form circuits within the central nervous system. They are crucial to the biological computations that underlie perception and thought. They allow the nervous system to connect to and control other systems of the body.

At a chemical synapse, one neuron releases neurotransmitter molecules into a small space (the synaptic cleft) that is adjacent to another neuron. The neurotransmitters are kept within small sacs called synaptic vesicles, and are released into the synaptic cleft by exocytosis. These molecules then bind to neurotransmitter receptors on the postsynaptic cell's side of the synaptic cleft. Finally, the neurotransmitters must be cleared from the synapse through one of several potential mechanisms including enzymatic degradation or re-uptake by specific transporters either on the presynaptic cell or possibly by neuroglia to terminate the action of the transmitter.

The adult human brain is estimated to contain from 10 to 5 × 10 (100–500 trillion) synapses. Every cubic millimeter of cerebral cortex contains roughly a billion (short scale, i.e. 10) of them. The number of synapses in the human cerebral cortex has separately been estimated at 0.15 quadrillion (150 trillion)

The word "synapse" comes from "synaptein", which Sir Charles Scott Sherrington and colleagues coined from the Greek "syn-" ("together") and "haptein" ("to clasp"). Chemical synapses are not the only type of biological synapse: electrical and immunological synapses also exist. Without a qualifier, however, "synapse" commonly means chemical synapse.

Synapses are functional connections between neurons, or between neurons and other types of cells. A typical neuron gives rise to several thousand synapses, although there are some types that make far fewer. Most synapses connect axons to dendrites, but there are also other types of connections, including axon-to-cell-body, axon-to-axon, and dendrite-to-dendrite. Synapses are generally too small to be recognizable using a light microscope except as points where the membranes of two cells appear to touch, but their cellular elements can be visualized clearly using an electron microscope.

Chemical synapses pass information directionally from a presynaptic cell to a postsynaptic cell and are therefore asymmetric in structure and function. The presynaptic axon terminal, or synaptic bouton, is a specialized area within the axon of the presynaptic cell that contains neurotransmitters enclosed in small membrane-bound spheres called synaptic vesicles (as well as a number of other supporting structures and organelles, such as mitochondria and endoplasmic reticulum). Synaptic vesicles are docked at the presynaptic plasma membrane at regions called active zones.

Immediately opposite is a region of the postsynaptic cell containing neurotransmitter receptors; for synapses between two neurons the postsynaptic region may be found on the dendrites or cell body. Immediately behind the postsynaptic membrane is an elaborate complex of interlinked proteins called the postsynaptic density (PSD).

Proteins in the PSD are involved in anchoring and trafficking neurotransmitter receptors and modulating the activity of these receptors. The receptors and PSDs are often found in specialized protrusions from the main dendritic shaft called dendritic spines.

Synapses may be described as symmetric or asymmetric. When examined under an electron microscope, asymmetric synapses are characterized by rounded vesicles in the presynaptic cell, and a prominent postsynaptic density. Asymmetric synapses are typically excitatory. Symmetric synapses in contrast have flattened or elongated vesicles, and do not contain a prominent postsynaptic density. Symmetric synapses are typically inhibitory.

The synaptic cleft —also called synaptic gap— is a gap between the pre- and postsynaptic cells that is about 20 nm wide. The small volume of the cleft allows neurotransmitter concentration to be raised and lowered rapidly.

An autapse is a chemical (or electrical) synapse formed when the axon of one neuron synapses with its own dendrites.

Here is a summary of the sequence of events that take place in synaptic transmission from a presynaptic neuron to a postsynaptic cell. Each step is explained in more detail below. Note that with the exception of the final step, the entire process may run only a few hundred microseconds, in the fastest synapses.

The release of a neurotransmitter is triggered by the arrival of a nerve impulse (or action potential) and occurs through an unusually rapid process of cellular secretion (exocytosis). Within the presynaptic nerve terminal, vesicles containing neurotransmitter are localized near the synaptic membrane. The arriving action potential produces an influx of calcium ions through voltage-dependent, calcium-selective ion channels at the down stroke of the action potential (tail current). Calcium ions then bind to synaptotagmin proteins found within the membranes of the synaptic vesicles, allowing the vesicles to fuse with the presynaptic membrane. The fusion of a vesicle is a stochastic process, leading to frequent failure of synaptic transmission at the very small synapses that are typical for the central nervous system. Large chemical synapses (e.g. the neuromuscular junction), on the other hand, have a synaptic release probability of 1. Vesicle fusion is driven by the action of a set of proteins in the presynaptic terminal known as SNAREs. As a whole, the protein complex or structure that mediates the docking and fusion of presynaptic vesicles is called the active zone. The membrane added by the fusion process is later retrieved by endocytosis and recycled for the formation of fresh neurotransmitter-filled vesicles.

An exception to the general trend of neurotransmitter release by vesicular fusion is found in the type II receptor cells of mammalian taste buds. Here the neurotransmitter ATP is released directly from the cytoplasm into the synaptic cleft via voltage gated channels.

Receptors on the opposite side of the synaptic gap bind neurotransmitter molecules. Receptors can respond in either of two general ways. First, the receptors may directly open ligand-gated ion channels in the postsynaptic cell membrane, causing ions to enter or exit the cell and changing the local transmembrane potential. The resulting change in voltage is called a postsynaptic potential. In general, the result is "excitatory" in the case of depolarizing currents, and "inhibitory" in the case of hyperpolarizing currents. Whether a synapse is excitatory or inhibitory depends on what type(s) of ion channel conduct the postsynaptic current(s), which in turn is a function of the type of receptors and neurotransmitter employed at the synapse. The second way a receptor can affect membrane potential is by modulating the production of chemical messengers inside the postsynaptic neuron. These second messengers can then amplify the inhibitory or excitatory response to neurotransmitters.

After a neurotransmitter molecule binds to a receptor molecule, it must be removed to allow for the postsynaptic membrane to continue to relay subsequent EPSPs and/or IPSPs. This removal can happen through one or more processes:


The strength of a synapse has been defined by Sir Bernard Katz as the product of (presynaptic) release probability "pr", quantal size "q" (the postsynaptic response to the release of a single neurotransmitter vesicle, a 'quantum'), and "n", the number of release sites. "Unitary connection" usually refers to an unknown number of individual synapses connecting a presynaptic neuron to a postsynaptic neuron. 
The amplitude of postsynaptic potentials (PSPs) can be as low as 0.4mV to as high as 20mV. The amplitude of a PSP can be modulated by neuromodulators or can change as a result of previous activity. Changes in the synaptic strength can be short-term, lasting seconds to minutes, or long-term (long-term potentiation, or LTP), lasting hours. Learning and memory are believed to result from long-term changes in synaptic strength, via a mechanism known as synaptic plasticity.

Desensitization of the postsynaptic receptors is a decrease in response to the same neurotransmitter stimulus. It means that the strength of a synapse may in effect diminish as a train of action potentials arrive in rapid succession – a phenomenon that gives rise to the so-called frequency dependence of synapses. The nervous system exploits this property for computational purposes, and can tune its synapses through such means as phosphorylation of the proteins involved.

Synaptic transmission can be changed by previous activity. These changes are called synaptic plasticity and may result in either a decrease in the efficacy of the synapse, called depression, or an increase in efficacy, called potentiation. These changes can either be long-term or short-term. Forms of short-term plasticity include synaptic fatigue or depression and synaptic augmentation. Forms of long-term plasticity include long-term depression and long-term potentiation. Synaptic plasticity can be either homosynaptic (occurring at a single synapse) or heterosynaptic (occurring at multiple synapses).

Homosynaptic Plasticity (or also homotropic modulation) is a change in the synaptic strength that results from the history of activity at a particular synapse. This can result from changes in presynaptic calcium as well as feedback onto presynaptic receptors, i.e. a form of autocrine signaling. Homosynaptic plasticity can affect the number and replenishment rate of vesicles or it can affect the relationship between calcium and vesicle release. Homosynaptic plasticity can also be postsynaptic in nature. It can result in either an increase or decrease in synaptic strength.

One example is neurons of the sympathetic nervous system (SNS), which release noradrenaline, which, besides affecting postsynaptic receptors, also affects presynaptic α2-adrenergic receptors, inhibiting further release of noradrenaline. This effect is utilized with clonidine to perform inhibitory effects on the SNS.

Heterosynaptic Plasticity (or also heterotropic modulation) is a change in synaptic strength that results from the activity of other neurons. Again, the plasticity can alter the number of vesicles or their replenishment rate or the relationship between calcium and vesicle release. Additionally, it could directly affect calcium influx. Heterosynaptic plasticity can also be postsynaptic in nature, affecting receptor sensitivity.

One example is again neurons of the sympathetic nervous system, which release noradrenaline, which, in addition, generates an inhibitory effect on presynaptic terminals of neurons of the parasympathetic nervous system.

In general, if an excitatory synapse is strong enough, an action potential in the presynaptic neuron will trigger an action potential in the postsynaptic cell. In many cases the excitatory postsynaptic potential (EPSP) will not reach the threshold for eliciting an action potential. When action potentials from multiple presynaptic neurons fire simultaneously, or if a single presynaptic neuron fires at a high enough frequency, the EPSPs can overlap and summate. If enough EPSPs overlap, the summated EPSP can reach the threshold for initiating an action potential. This process is known as summation, and can serve as a high pass filter for neurons.

On the other hand, a presynaptic neuron releasing an inhibitory neurotransmitter, such as GABA, can cause an inhibitory postsynaptic potential (IPSP) in the postsynaptic neuron, bringing the membrane potential farther away from the threshold, decreasing its excitability and making it more difficult for the neuron to initiate an action potential. If an IPSP overlaps with an EPSP, the IPSP can in many cases prevent the neuron from firing an action potential. In this way, the output of a neuron may depend on the input of many different neurons, each of which may have a different degree of influence, depending on the strength and type of synapse with that neuron. John Carew Eccles performed some of the important early experiments on synaptic integration, for which he received the Nobel Prize for Physiology or Medicine in 1963. Complex input/output relationships form the basis of transistor-based computations in computers, and are thought to figure similarly in neural circuits.

When a neurotransmitter is released at a synapse, it reaches its highest concentration inside the narrow space of the synaptic cleft, but some of it is certain to diffuse away before being reabsorbed or broken down. If it diffuses away, it has the potential to activate receptors that are located either at other synapses or on the membrane away from any synapse. The extrasynaptic activity of a neurotransmitter is known as "volume transmission". It is well established that such effects occur to some degree, but their functional importance has long been a matter of controversy.

Recent work indicates that volume transmission may be the predominant mode of interaction for some special types of neurons. In the mammalian cerebral cortex, a class of neurons called neurogliaform cells can inhibit other nearby cortical neurons by releasing the neurotransmitter GABA into the extracellular space. Along the same vein, GABA released from neurogliaform cells into the extracellular space also acts on surrounding astrocytes, assigning a role for volume transmission in the control of ionic and neurotransmitter homeostasis. Approximately 78% of neurogliaform cell boutons do not form classical synapses. This may be the first definitive example of neurons communicating chemically where classical synapses are not present."

An electrical synapse is an electrically conductive link between two abutting neurons that is formed at a narrow gap between the pre- and postsynaptic cells, known as a gap junction. At gap junctions, cells approach within about 3.5 nm of each other, rather than the 20 to 40 nm distance that separates cells at chemical synapses. As opposed to chemical synapses, the postsynaptic potential in electrical synapses is not caused by the opening of ion channels by chemical transmitters, but rather by direct electrical coupling between both neurons. Electrical synapses are faster than chemical synapses. Electrical synapses are found throughout the nervous system, including in the retina, the reticular nucleus of the thalamus, the neocortex, and in the hippocampus. While chemical synapses are found between both excitatory and inhibitory neurons, electrical synapses are most commonly found between smaller local inhibitory neurons. Electrical synapses can exist between two axons, two dendrites, or between an axon and a dendrite. In some fish and amphibians, electrical synapses can be found within the same terminal of a chemical synapse, as in Mauthner cells.

One of the most important features of chemical synapses is that they are the site of action for the majority of psychoactive drugs. Synapses are affected by drugs such as curare, strychnine, cocaine, morphine, alcohol, LSD, and countless others. These drugs have different effects on synaptic function, and often are restricted to synapses that use a specific neurotransmitter. For example, curare is a poison that stops acetylcholine from depolarizing the postsynaptic membrane, causing paralysis. Strychnine blocks the inhibitory effects of the neurotransmitter glycine, which causes the body to pick up and react to weaker and previously ignored stimuli, resulting in uncontrollable muscle spasms. Morphine acts on synapses that use endorphin neurotransmitters, and alcohol increases the inhibitory effects of the neurotransmitter GABA. LSD interferes with synapses that use the neurotransmitter serotonin. Cocaine blocks reuptake of dopamine and therefore increases its effects.

During the 1950s, Bernard Katz and Paul Fatt observed spontaneous miniature synaptic currents at the frog neuromuscular junction. Based on these observations, they developed the 'quantal hypothesis' that is the basis for our current understanding of neurotransmitter release as exocytosis and for which Katz received the Nobel Prize in Physiology or Medicine in 1970. In the late 1960s, Ricardo Miledi and Katz advanced the hypothesis that depolarization-induced influx of calcium ions triggers exocytosis.





</doc>
<doc id="27811" url="https://en.wikipedia.org/wiki?curid=27811" title="Sleep and learning">
Sleep and learning

Multiple hypotheses explain the possible connections between sleep and learning in humans. Research indicates that sleep does more than allow the brain to rest. It may also aid the consolidation of long-term memories.

REM sleep and slow-wave sleep play different roles in memory consolidation. REM is associated with the consolidation of nondeclarative (implicit) memories. An example of a nondeclarative memory would be a task that we can do without consciously thinking about it, such as riding a bike. Slow-wave, or non-REM (NREM) sleep, is associated with the consolidation of declarative (explicit) memories. These are facts that need to be consciously remembered, such as dates for a history class.

Popular sayings can reflect the notion that remolded memories produce new creative associations in the morning, and that performance often improves after a time-interval that includes sleep. Current studies demonstrate that a healthy sleep produces a significant learning-dependent performance boost. The idea is that sleep helps the brain to edit its memory, looking for important patterns and extracting overarching rules which could be described as 'the gist', and integrating this with existing memory. The 'synaptic scaling' hypothesis suggests that sleep plays an important role in regulating learning that has taken place while awake, enabling more efficient and effective storage in the brain, making better use of space and energy.

Healthy sleep must include the appropriate sequence and proportion of NREM and REM phases, which play different roles in the memory consolidation-optimization process. During a normal night of sleep, a person will alternate between periods of NREM and REM sleep. Each cycle is approximately 90 minutes long, containing a 20-30 minute bout of REM sleep. NREM sleep consists of sleep stages 1–4, and is where movement can be observed. A person can still move their body when they are in NREM sleep. If someone sleeping turns, tosses, or rolls over, this indicates that they are in NREM sleep. REM sleep is characterized by the lack of muscle activity. Physiological studies have shown that aside from the occasional twitch, a person actually becomes paralyzed during REM sleep. In motor skill learning, an interval of sleep may be critical for the expression of performance gains; without sleep these gains will be delayed (Korman et al., 2003).

Procedural memories are a form of nondeclarative memory, so they would most benefit from the fast-wave REM sleep. In a study, procedural memories have been shown to benefit from sleep (Walker et al., 2002, as cited in Walker, 2009). Subjects were tested using a tapping task, where they used their fingers to tap a specific sequence of numbers on a keyboard, and their performances were measured by accuracy and speed. This finger-tapping task was used to simulate learning a motor skill. The first group was tested, retested 12 hours later while awake, and finally tested another 12 hours later with sleep in between. The other group was tested, retested 12 hours later with sleep in between, and then retested 12 hours later while awake. The results showed that in both groups, there was only a slight improvement after a 12-hour wake session, but a significant increase in performance after each group slept. This study gives evidence that REM sleep is a significant factor in consolidating motor skill procedural memories, therefore sleep deprivation can impair performance on a motor learning task. This memory decrement results specifically from the loss of stage 2, REM sleep.

Declarative memory has also been shown to benefit from sleep, but not in the same way as procedural memory. Declarative memories benefit from the slow-waves nREM sleep. A study was conducted where the subjects learned word pairs, and the results showed that sleep not only prevents the decay of memory, but also actively fixates declarative memories (Payne et al., 2006). Two of the groups learned word pairs, then either slept or stayed awake, and were tested again. The other two groups did the same thing, except they also learned interference pairs right before being retested to try to disrupt the previously learned word pairs. The results showed that sleep was of "some" help in retaining the word pair associations, while against the interference pair, sleep helped "significantly".

After sleep, there is increased insight. This is because sleep helps people to reanalyze their memories. The same patterns of brain activity that occur during learning have been found to occur again during sleep, only faster. One way that sleep strengthens memories is by weeding out the less successful connections between neurons in the brain. This weeding out is essential to prevent overactivity. The brain compensates for strengthening some synapses (connections) between neurons, by weakening others. The weakening process occurs mostly during sleep. This weakening during sleep allows for strengthening of other connections while we are awake. Learning is the process of strengthening connections, therefore this process could be a major explanation for the benefits that sleep has on memory.

Research has shown that taking an afternoon nap increases learning capacity. A study (Mednick et al. 2009) tested two groups of subjects on a nondeclarative memory task. One group engaged in REM sleep, and one group did not (meaning that they engaged in NREM sleep). The investigators found that the subjects who engaged only in NREM sleep did not show much improvement. The subjects who engaged in REM sleep performed significantly better, indicating that REM sleep facilitated the consolidation of nondeclarative memories. More recently Holtz et al. (2012) demonstrated that a procedural task was learned and retained better if it was encountered immediately before going to sleep, while a declarative task was learned better in the afternoon 

A 2009 study based on electrophysiological recordings of large ensembles of isolated cells in the prefrontal cortex of rats revealed that cell assemblies that formed upon learning were more preferentially active during subsequent sleep episodes. More specifically, those replay events were more prominent during slow wave sleep and were concomittant with hippocampal reactivation events. This study has shown that neuronal patterns in large brain networks are tagged during learning so that they are replayed, and supposedly consolidated, during subsequent sleep. There have been other studies that have shown similar reactivation of learning pattern during motor skill and neuroprosthetic learning. Notably, new evidence is showing that reactivation and rescaling may be co-occurring during sleep .

Sleep has been directly linked to the grades of students. One in four U.S. high school students admit to falling asleep in class at least once a week. Consequently, results have shown that those who sleep less do poorly. In the United States sleep deprivation is common with students because almost all schools begin early in the morning and many of these students either choose to stay awake late into the night or cannot do otherwise due to delayed sleep phase syndrome. As a result, students that should be getting between 8.5 and 9.25 hours of sleep are getting only 7 hours. Perhaps because of this sleep deprivation, their grades lower and their concentration is impaired. As a result of studies showing the effects of sleep deprivation on grades, and the different sleep patterns for teenagers, a school in New Zealand, changed its start time to 10:30 a.m., in 2006, to allow students to keep to a schedule that allowed more sleep. In 2009, Monkseaton High School, in North Tyneside, had 800 pupils aged 13–19 starting lessons at 10 a.m. instead of the normal 9 a.m. and has reported that general absence has dropped by 8% and persistent absenteeism by 27%. Similarly, a high school in Copenhagen has committed to providing at least one class per year for students who will start at 10 a.m. or later.

College students represent one of the most sleep-deprived segments of our population. Only 11% of American college students sleep well, and 40% of students feel well rested only two days per week. About 73% have experienced at least some occasional sleep issues. This poor sleep is thought to have a severe impact on their ability to learn and remember information because the brain is being deprived of time that it needs to consolidate information which is essential to the learning process.




</doc>
<doc id="27812" url="https://en.wikipedia.org/wiki?curid=27812" title="Specie">
Specie

Specie may refer to:



</doc>
<doc id="27813" url="https://en.wikipedia.org/wiki?curid=27813" title="Systematics">
Systematics

Biological systematics is the study of the diversification of living forms, both past and present, and the relationships among living things through time. Relationships are visualized as evolutionary trees (synonyms: cladograms, phylogenetic trees, phylogenies). Phylogenies have two components: branching order (showing group relationships) and branch length (showing amount of evolution). Phylogenetic trees of species and higher taxa are used to study the evolution of traits (e.g., anatomical or molecular characteristics) and the distribution of organisms (biogeography). Systematics, in other words, is used to understand the evolutionary history of life on Earth.

In the study of biological systematics, researchers use the different branches to further understand the relationships between differing organisms. These branches are used to determine the applications and uses for modern day systematics.

Biological systematics classifies species by using three specific branches. "Numerical systematics", or "biometry", uses biological statistics to identify and classify animals. "Biochemical systematics" classifies and identifies animals based on the analysis of the material that makes up the living part of a cell—such as the nucleus, organelles, and cytoplasm. "Experimental systematics" identifies and classifies animals based on the evolutionary units that comprise a species, as well as their importance in evolution itself. Factors such as mutations, genetic divergence, and hybridization all are considered evolutionary units.

With the specific branches, researchers are able to determine the applications and uses for modern-day systematics. These applications include: 

John Lindley provided an early definition of systematics in 1830, although he wrote of "systematic botany" rather than using the term "systematics".

In 1970 Michener "et al." defined "systematic biology" and "taxonomy" (terms that are often confused and used interchangeably) in relationship to one another as follows:
Systematic biology (hereafter called simply systematics) is the field that (a) provides scientific names for organisms, (b) describes them, (c) preserves collections of them, (d) provides classifications for the organisms, keys for their identification, and data on their distributions, (e) investigates their evolutionary histories, and (f) considers their environmental adaptations. This is a field with a long history that in recent years has experienced a notable renaissance, principally with respect to theoretical content. Part of the theoretical material has to do with evolutionary areas (topics e and f above), the rest relates especially to the problem of classification. Taxonomy is that part of Systematics concerned with topics (a) to (d) above.
Taxonomy, systematic biology, systematics, biosystematics, scientific classification, biological classification, phylogenetics: At various times in history, all these words have had overlapping, related meanings. However, in modern usage, they can all be considered synonyms of each other.

For example, Webster's 9th New Collegiate Dictionary of 1987 treats "classification", "taxonomy", and "systematics" as synonyms. According to this work, the terms originated in 1790, c. 1828, and in 1888 respectively. Some claim systematics alone deals specifically with relationships through time, and that it can be synonymous with phylogenetics, broadly dealing with the inferred hierarchy of organisms. This means it would be a subset of taxonomy as it is sometimes regarded, but the inverse is claimed by others.

Europeans tend to use the terms "systematics" and "biosystematics" for the study of biodiversity as a whole, whereas North Americans tend to use "taxonomy" more frequently. However, taxonomy, and in particular alpha taxonomy, is more specifically the identification, description, and naming (i.e. nomenclature) of organisms,
while "classification" focuses on placing organisms within hierarchical groups that show their relationships to other organisms. All of these biological disciplines can deal with both extinct and extant organisms.

Systematics uses taxonomy as a primary tool in understanding, as nothing about an organism's relationships with other living things can be understood without it first being properly studied and described in sufficient detail to identify and classify it correctly. Scientific classifications are aids in recording and reporting information to other scientists and to laymen. The systematist, a scientist who specializes in systematics, must, therefore, be able to use existing classification systems, or at least know them well enough to skilfully justify not using them.

Phenetics was an attempt to determine the relationships of organisms through a measure of overall similarity, making no distinction between plesiomorphies (shared ancestral traits) and apomorphies (derived traits). From the late-20th century onwards, it was superseded by cladistics, which rejects plesiomorphies in attempting to resolve the phylogeny of Earth's various organisms through time. systematists generally make extensive use of molecular biology and of computer programs to study organisms.

Taxonomic characters are the taxonomic attributes that can be used to provide the evidence from which relationships (the phylogeny) between taxa are inferred. Kinds of taxonomic characters:





</doc>
<doc id="27814" url="https://en.wikipedia.org/wiki?curid=27814" title="Sine (disambiguation)">
Sine (disambiguation)

Sine is a trigonometric function.

Sine may also refer to:





</doc>
<doc id="27834" url="https://en.wikipedia.org/wiki?curid=27834" title="Sleep">
Sleep

Sleep is a naturally recurring state of mind and body, characterized by altered consciousness, relatively inhibited sensory activity, inhibition of nearly all voluntary muscles, and reduced interactions with surroundings. It is distinguished from wakefulness by a decreased ability to react to stimuli, but is more easily reversed than the state of being comatose. 

Sleep occurs in repeating periods, in which the body alternates between two distinct modes: REM sleep and non-REM sleep. Although REM stands for "rapid eye movement", this mode of sleep has many other aspects, including virtual paralysis of the body. A well-known feature of sleep is the dream, an experience typically recounted in narrative form, which resembles waking life while in progress, but which usually can later be distinguished as fantasy.

During sleep, most of the body's systems are in an anabolic state, helping to restore the immune, nervous, skeletal, and muscular systems; these are vital processes that maintain mood, memory, and cognitive function, and play a large role in the function of the endocrine and immune systems. The internal circadian clock promotes sleep daily at night. The diverse purposes and mechanisms of sleep are the subject of substantial ongoing research. 

Humans may suffer from various sleep disorders, including dyssomnias such as insomnia, hypersomnia, narcolepsy, and sleep apnea; parasomnias such as sleepwalking and REM behavior disorder; bruxism; and circadian rhythm sleep disorders. The advent of artificial light has substantially altered sleep timing in industrialized countries.

The most pronounced physiological changes in sleep occur in the brain. The brain uses significantly less energy during sleep than it does when awake, especially during non-REM sleep. In areas with reduced activity, the brain restores its supply of adenosine triphosphate (ATP), the molecule used for short-term storage and transport of energy. In quiet waking, the brain is responsible for 20% of the body's energy use, thus this reduction has an independently noticeable effect on overall energy consumption.

Sleep increases the sensory threshold. In other words, sleeping persons perceive fewer stimuli. However, they can generally still respond to loud noises and other salient sensory events.

During slow-wave sleep, humans secrete bursts of growth hormone. All sleep, even during the day, is associated with secretion of prolactin.

Key physiological methods for monitoring and measuring changes during sleep include electroencephalography (EEG) of brain waves, electrooculography (EOG) of eye movements, and electromyography (EMG) of skeletal muscle activity. Simultaneous collection of these measurements is called polysomnography, and can be performed in a specialized sleep laboratory. Sleep researchers also use simplified electrocardiography (EKG) for cardiac activity and actigraphy for motor movements.

Sleep is divided into two broad types: non-rapid eye movement (non-REM or NREM) sleep and rapid eye movement (REM) sleep. Non-REM and REM sleep are so different that physiologists identify them as distinct behavioral states. Non-REM sleep occurs first and after a transitional period is called slow-wave sleep or deep sleep. During this phase, body temperature and heart rate fall, and the brain uses less energy. REM sleep, also known as paradoxical sleep, represents a smaller portion of total sleep time. It is the main occasion for dreams (or nightmares), and is associated with desynchronized and fast brain waves, eye movements, loss of muscle tone, and suspension of homeostasis.

The sleep cycle of alternate NREM and REM sleep takes an average of 90 minutes, occurring 4–6 times in a good night's sleep. The American Academy of Sleep Medicine (AASM) divides NREM into three stages: N1, N2, and N3, the last of which is also called delta sleep or slow-wave sleep. The whole period normally proceeds in the order: N1 → N2 → N3 → N2 → REM. REM sleep occurs as a person returns to stage 2 or 1 from a deep sleep. There is a greater amount of deep sleep (stage N3) earlier in the night, while the proportion of REM sleep increases in the two cycles just before natural awakening.

Awakening can mean the end of sleep, or simply a moment to survey the environment and readjust body position before falling back asleep. Sleepers typically awaken soon after the end of a REM phase or sometimes in the middle of REM. Internal circadian indicators, along with successful reduction of homeostatic sleep need, typically bring about awakening and the end of the sleep cycle. Awakening involves heightened electrical activation in the brain, beginning with the thalamus and spreading throughout the cortex.

During a night's sleep, a small portion of time is usually spent in a waking state. As measured by electroencephalography, young females are awake for 0–1% of the larger sleeping period; young males are awake for 0–2%. In adults, wakefulness increases, especially in later cycles. One study found 3% awake time in the first ninety-minute sleep cycle, 8% in the second, 10% in the third, 12% in the fourth, and 13–14% in the fifth. Most of this awake time occurred shortly after REM sleep.

Today, many humans wake up with an alarm clock; however, some people can reliably wake themselves up at a specific time with no need for an alarm. Many sleep quite differently on workdays versus days off, a pattern which can lead to chronic circadian desynchronization. Many people regularly look at television and other screens before going to bed, a factor which may exacerbate disruption of the circadian cycle. Scientific studies on sleep have shown that sleep stage at awakening is an important factor in amplifying sleep inertia.

Sleep timing is controlled by the circadian clock (Process C), sleep-wake homeostasis (Process S), and to some extent by individual will.

Sleep timing depends greatly on hormonal signals from the circadian clock, or Process C, a complex neurochemical system which uses signals from an organism's environment to recreate an internal day–night rhythm. Process C counteracts the homeostatic drive for sleep during the day (in diurnal animals) and augments it at night. The suprachiasmatic nucleus (SCN), a brain area directly above the optic chiasm, is presently considered the most important nexus for this process; however, secondary clock systems have been found throughout the body.

An organism whose circadian clock exhibits a regular rhythm corresponding to outside signals is said to be "entrained"; an entrained rhythm persists even if the outside signals suddenly disappear. If an entrained human is isolated in a bunker with constant light or darkness, he or she will continue to experience rhythmic increases and decreases of body temperature and melatonin, on a period which slightly exceeds 24 hours. Scientists refer to such conditions as free-running of the circadian rhythm. Under natural conditions, light signals regularly adjust this period downward, so that it corresponds better with the exact 24 hours of an Earth day.

The clock exerts constant influence on the body, effecting sinusoidal oscillation of body temperature between roughly 36.2 °C and 37.2 °C. The suprachiasmatic nucleus itself shows conspicuous oscillation activity, which intensifies during subjective day (i.e., the part of the rhythm corresponding with daytime, whether accurately or not) and drops to almost nothing during subjective night. The circadian pacemaker in the suprachiasmatic nucleus has a direct neural connection to the pineal gland, which releases the hormone melatonin at night. Cortisol levels typically rise throughout the night, peak in the awakening hours, and diminish during the day. Circadian prolactin secretion begins in the late afternoon, especially in women, and is subsequently augmented by sleep-induced secretion, to peak in the middle of the night. Circadian rhythm exerts some influence on the nighttime secretion of growth hormone.

The circadian rhythm influences the ideal timing of a restorative sleep episode. Sleepiness increases during the night. REM sleep occurs more during body temperature minimum within the circadian cycle, whereas slow-wave sleep can occur more independently of circadian time.

The internal circadian clock is profoundly influenced by changes in light, since these are its main clues about what time it is. Exposure to even small amounts of light during the night can suppress melatonin secretion, and increase body temperature and wakefulness. Short pulses of light, at the right moment in the circadian cycle, can significantly 'reset' the internal clock. Blue light, in particular, exerts the strongest effect, leading to concerns that electronic media use before bed may interfere with sleep.

Modern humans often find themselves desynchronized from their internal circadian clock, due to the requirements of work (especially night shifts), long-distance travel, and the influence of universal indoor lighting. Even if they have sleep debt, or feel sleepy, people can have difficulty staying asleep at the peak of their circadian cycle. Conversely they can have difficulty waking up in the trough of the cycle. A healthy young adult entrained to the sun will (during most of the year) fall asleep a few hours after sunset, experience body temperature minimum at 6AM, and wake up a few hours after sunrise.

Generally speaking, the longer an organism is awake, the more it feels a need to sleep ("sleep debt"). This driver of sleep is referred to as Process S. The balance between sleeping and waking is regulated by a process called homeostasis. Induced or perceived lack of sleep is commonly called sleep deprivation.

Process S is driven by the depletion of glycogen and accumulation of adenosine in the forebrain that disinhibits the Ventrolateral preoptic nucleus, allowing for inhibition of the ascending reticular activating system.

Sleep deprivation tends to cause slower brain waves in the frontal cortex, shortened attention span, higher anxiety, impaired memory, and a grouchy mood. Conversely, a well-rested organism tends to have improved memory and mood. Neurophysiological and functional imaging studies have demonstrated that frontal regions of the brain are particularly responsive to homeostatic sleep pressure.

There is disagreement on how much sleep debt it is possible to accumulate, and whether sleep debt is accumulated against an individual's average sleep or some other benchmark. It is also unclear whether the prevalence of sleep debt among adults has changed appreciably in the industrialized world in recent decades. Sleep debt does show some evidence of being cumulative. Subjectively, however, humans seem to reach maximum sleepiness after 30 hours of waking. It is likely that in Western societies, children are sleeping less than they previously have.

One neurochemical indicator of sleep debt is adenosine, a neurotransmitter that inhibits many of the bodily processes associated with wakefulness. Adenosine levels increase in the cortex and basal forebrain during prolonged wakefulness, and decrease during the sleep-recovery period, potentially acting as a homeostatic regulator of sleep. Coffee and caffeine temporarily block the effect of adenosine, prolong sleep latency, and reduce total sleep time and quality.

Humans are also influenced by aspects of "social time", such as the hours when other people are awake, the hours when work is required, the time on the clock, etc. Time zones, standard times used to unify the timing for people in the same area, correspond only approximately to the natural rising and setting of the sun. The approximate nature of the timezone can be shown with China, a country which used to span five time zones and now uses only one (UTC +8).

In polyphasic sleep, an organism sleeps several times in a 24-hour cycle. Monophasic sleep occurs all at once. Under experimental conditions, humans tend to alternate more frequently between sleep and wakefulness (i.e., exhibit more polyphasic sleep) if they have nothing better to do. Given a 14-hour period of darkness in experimental conditions, humans tended towards bimodal sleep, with two sleep periods concentrated at the beginning and at the end of the dark time. Bimodal sleep in humans was more common before the industrial revolution.

Different characteristic sleep patterns, such as the familiarly so-called "early bird" and "night owl", are called "chronotypes". Genetics and sex have some influence on chronotype, but so do habits. Chronotype is also liable to change over the course of a person's lifetime. Seven-year-olds are better disposed to wake up early in the morning than are fifteen-year-olds. Chronotypes far outside the normal range are called circadian rhythm sleep disorders.

The siesta habit has recently been associated with a 37% lower coronary mortality, possibly due to reduced cardiovascular stress mediated by daytime sleep. Short naps at mid-day and mild evening exercise were found to be effective for improved sleep, cognitive tasks, and mental health in elderly people.

Many people experience a temporary drop in alertness in the early afternoon, commonly known as the "post-lunch dip." While a large meal can make a person feel sleepy, the post-lunch dip is mostly an effect of the circadian clock. People naturally feel most sleepy at two times of the day about 12 hours apart—for example, at 2:00 a.m. and 2:00 p.m. At those two times, the body clock "kicks in." At about 2 p.m. (14:00), it overrides the homeostatic buildup of sleep debt, allowing several more hours of wakefulness. At about 2 a.m. (02:00), with the daily sleep debt paid off, it "kicks in" again to ensure a few more hours of sleep.

It is hypothesized that a considerable amount of sleep-related behavior, such as when and how long a person needs to sleep, is regulated by genetics. Researchers have discovered some evidence that seems to support this assumption. Monozygotic (identical) but not dizygotic (fraternal) twins tend to have similar sleep habits. Neurotransmitters, molecules whose production can be traced to specific genes, are one genetic influence on sleep which can be analyzed. And the circadian clock has its own set of genes. Genes which may influence sleep include ABCC9, DEC2, and variants near PAX 8 and VRK2.

The quality of sleep may be evaluated from an objective and a subjective point of view. Objective sleep quality refers to how difficult it is for a person to fall asleep and remain in a sleeping state, and how many times they wake up during a single night. Poor sleep quality disrupts the cycle of transition between the different stages of sleep. Subjective sleep quality in turn refers to a sense of being rested and regenerated after awaking from sleep. A study by A. Harvey et al. (2002) found that insomniacs were more demanding in their evaluations of sleep quality than individuals who had no sleep problems.

Homeostatic sleep propensity (the need for sleep as a function of the amount of time elapsed since the last adequate sleep episode) must be balanced against the circadian element for satisfactory sleep. Along with corresponding messages from the circadian clock, this tells the body it needs to sleep. A person who regularly awakens at an early hour will generally not be able to sleep much later than his or her normal waking time, even if moderately sleep-deprived. The timing is correct when the following two circadian markers occur after the middle of the sleep episode and before awakening: maximum concentration of the hormone melatonin, and minimum core body temperature.

Human sleep needs vary by age and amongst individuals, and sleep is considered to be adequate when there is no daytime sleepiness or dysfunction. Moreover, self-reported sleep duration is only moderately correlated with actual sleep time as measured by actigraphy, and those affected with sleep state misperception may typically report having slept only four hours despite having slept a full eight hours.

Researchers have found that sleeping 6–7 hours each night correlates with longevity and cardiac health in humans, though many underlying factors may be involved in the causality behind this relationship.

Sleep difficulties are furthermore associated with psychiatric disorders such as depression, alcoholism, and bipolar disorder. Up to 90% of adults with depression are found to have sleep difficulties. Dysregulation detected by EEG includes disturbances in sleep continuity, decreased delta sleep and altered REM patterns with regard to latency, distribution across the night and density of eye movements.

By the time infants reach the age of two, their brain size has reached 90 percent of an adult-sized brain; a majority of this brain growth has occurred during the period of life with the highest rate of sleep. The hours that children spend asleep influence their ability to perform on cognitive tasks. Children who sleep through the night and have few night waking episodes have higher cognitive attainments and easier temperaments than other children.

Sleep also influences language development. To test this, researchers taught infants a faux language and observed their recollection of the rules for that language. Infants who slept within four hours of learning the language could remember the language rules better, while infants who stayed awake longer did not recall those rules as well. There is also a relationship between infants' vocabulary and sleeping: infants who sleep longer at night at 12 months have better vocabularies at 26 months.

Children need many hours of sleep per day in order to develop and function properly: up to 18 hours for newborn babies, with a declining rate as a child ages. Early in 2015, after a two-year study, the National Sleep Foundation in the US announced newly revised recommendations as shown in the table below.

The human organism physically restores itself during sleep, healing itself and removing metabolic wastes which build up during periods of activity. This restoration takes place mostly during slow-wave sleep, during which body temperature, heart rate, and brain oxygen consumption decrease. The brain, especially, requires sleep for restoration, whereas in the rest of the body these processes can take place during quiescent waking. In both cases, the reduced rate of metabolism enables countervailing restorative processes.

While awake, metabolism generates reactive oxygen species, which are damaging to cells. In sleep, metabolic rates decrease and reactive oxygen species generation is reduced allowing restorative processes to take over. The sleeping brain has been shown to remove metabolic waste products at a faster rate than during an awake state. It is further theorized that sleep helps facilitate the synthesis of molecules that help repair and protect the brain from these harmful elements generated during waking. Anabolic hormones such as growth hormones are secreted preferentially during sleep. Sleep has also been theorized to effectively combat the accumulation of free radicals in the brain, by increasing the efficiency of endogenous antioxidant mechanisms. The concentration of the sugar compound glycogen in the brain increases during sleep, and is depleted through metabolism during wakefulness.

Wound healing has been shown to be affected by sleep.

It has been shown that sleep deprivation affects the immune system. It is now possible to state that "sleep loss impairs immune function and immune challenge alters sleep," and it has been suggested that sleep increases white blood cell counts. A 2014 study found that depriving mice of sleep increased cancer growth and dampened the immune system's ability to control cancers.

The effect of sleep duration on somatic growth is not completely known. One study recorded growth, height, and weight, as correlated to parent-reported time in bed in 305 children over a period of nine years (age 1–10). It was found that "the variation of sleep duration among children does not seem to have an effect on growth." It is well established that slow-wave sleep affects growth hormone levels in adult men. During eight hours' sleep, Van Cauter, Leproult, and Plat found that the men with a high percentage of SWS (average 24%) also had high growth hormone secretion, while subjects with a low percentage of SWS (average 9%) had low growth hormone secretion.

Sleep enhances memory, with procedural memory benefiting from late, REM-rich sleep, and explicit memory benefiting from early, slow wave-rich sleep.

During sleep, especially REM sleep, people tend to have dreams: elusive first-person experiences, which, despite their frequently bizarre qualities, seem realistic while in progress. Dreams can seamlessly incorporate elements within a person's mind that would not normally go together. They can include apparent sensations of all types, especially vision and movement.

People have proposed many hypotheses about the functions of dreaming. Sigmund Freud postulated that dreams are the symbolic expression of frustrated desires that have been relegated to the unconscious mind, and he used dream interpretation in the form of psychoanalysis in attempting to uncover these desires.

Counterintuitively, penile erections during sleep are not more frequent during sexual dreams than during other dreams. The parasympathetic nervous system experiences increased activity during REM sleep which may cause erection of the penis or clitoris. In males, 80% to 95% of REM sleep is normally accompanied by partial to full penile erection, while only about 12% of men's dreams contain sexual content.

John Allan Hobson and Robert McCarley propose that dreams are caused by the random firing of neurons in the cerebral cortex during the REM period. Neatly, this theory helps explain the irrationality of the mind during REM periods, as, according to this theory, the forebrain then creates a story in an attempt to reconcile and make sense of the nonsensical sensory information presented to it. This would explain the odd nature of many dreams.

Using antidepressants, acetaminophen, ibuprofen, or alcoholic beverages is thought to potentially suppress dreams, whereas melatonin may have the ability to encourage them.

Insomnia is a general term for difficulty falling asleep and/or staying asleep. Insomnia is the most common sleep problem, with many adults reporting occasional insomnia, and 10–15% reporting a chronic condition. Insomnia can have many different causes, including psychological stress, a poor sleep environment, an inconsistent sleep schedule, or excessive mental or physical stimulation in the hours before bedtime. Insomnia is often treated through behavioral changes like keeping a regular sleep schedule, avoiding stimulating or stressful activities before bedtime, and cutting down on stimulants such as caffeine. The sleep environment may be improved by installing heavy drapes to shut out all sunlight, and keeping computers, televisions and work materials out of the sleeping area.

A 2010 review of published scientific research suggested that exercise generally improves sleep for most people, and helps sleep disorders such as insomnia. The optimum time to exercise "may" be 4 to 8 hours before bedtime, though exercise at any time of day is beneficial, with the exception of heavy exercise taken shortly before bedtime, which may disturb sleep. However, there is insufficient evidence to draw detailed conclusions about the relationship between exercise and sleep. Sleeping medications such as Ambien and Lunesta are an increasingly popular treatment for insomnia. Although these nonbenzodiazepine medications are generally believed to be better and safer than earlier generations of sedatives, they have still generated some controversy and discussion regarding side-effects. White noise appears to be a promising treatment for insomnia.

Obstructive sleep apnea is a condition in which major pauses in breathing occur during sleep, disrupting the normal progression of sleep and often causing other more severe health problems. Apneas occur when the muscles around the patient's airway relax during sleep, causing the airway to collapse and block the intake of oxygen. Obstructive sleep apnea is more common than central sleep apnea. As oxygen levels in the blood drop, the patient then comes out of deep sleep in order to resume breathing. When several of these episodes occur per hour, sleep apnea rises to a level of seriousness that may require treatment.

Diagnosing sleep apnea usually requires a professional sleep study performed in a sleep clinic, because the episodes of wakefulness caused by the disorder are extremely brief and patients usually do not remember experiencing them. Instead, many patients simply feel tired after getting several hours of sleep and have no idea why. Major risk factors for sleep apnea include chronic fatigue, old age, obesity and snoring.

Sleep disorders include narcolepsy, periodic limb movement disorder (PLMD), restless leg syndrome (RLS), upper airway resistance syndrome (UARS), and the circadian rhythm sleep disorders. Fatal familial insomnia, or FFI, an extremely rare genetic disease with no known treatment or cure, is characterized by increasing insomnia as one of its symptoms; ultimately sufferers of the disease stop sleeping entirely, before dying of the disease.

Somnambulism, known as sleep walking, is also a common sleeping disorder, especially among children. In somnambulism the individual gets up from his/her sleep and wanders around while still sleeping.

Older people may be more easily awakened by disturbances in the environment and may to some degree lose the ability to consolidate sleep.

Drugs which induce sleep, known as hypnotics, include benzodiazepines, although these interfere with REM; Nonbenzodiazepine hypnotics such as eszopiclone (Lunesta), zaleplon (Sonata), and zolpidem (Ambien); Antihistamines, such as diphenhydramine (Benadryl) and doxylamine; Alcohol (ethanol), despite its rebound effect later in the night and interference with REM; barbiturates, which have the same problem; melatonin, a component of the circadian clock, and released naturally at night by the pineal gland; and cannabis, which may also interfere with REM.

Stimulants, which inhibit sleep, include caffeine, an adenosine antagonist; amphetamine, MDMA, empathogen-entactogens, and related drugs; cocaine, which can alter the circadian rhythm, and methylphenidate, which acts similarly; and other analeptic drugs like modafinil and armodafinil with poorly understood mechanisms.

Dietary and nutritional choices may affect sleep duration and quality. One 2016 review indicated that a high carbohydrate diet promoted shorter onset to sleep and longer duration sleep than a high fat diet. A 2012 investigation indicated that mixed micronutrients and macronutrients are needed to promote quality sleep. A varied diet containing fresh fruits and vegetables, low saturated fat, and whole grains may be optimal for individuals seeking to improve sleep quality. High-quality clinical trials on long-term dietary practices are needed to better define the influence of diet on sleep quality.

Research suggests that sleep patterns vary significantly across cultures. The most striking differences are between societies that have plentiful sources of artificial light and ones that do not. The primary difference appears to be that pre-light cultures have more broken-up sleep patterns. For example, people without artificial light might go to sleep far sooner after the sun sets, but then wake up several times throughout the night, punctuating their sleep with periods of wakefulness, perhaps lasting several hours.

The boundaries between sleeping and waking are blurred in these societies. Some observers believe that nighttime sleep in these societies is most often split into two main periods, the first characterized primarily by deep sleep and the second by REM sleep.

Some societies display a fragmented sleep pattern in which people sleep at all times of the day and night for shorter periods. In many nomadic or hunter-gatherer societies, people will sleep on and off throughout the day or night depending on what is happening. Plentiful artificial light has been available in the industrialized West since at least the mid-19th century, and sleep patterns have changed significantly everywhere that lighting has been introduced. In general, people sleep in a more concentrated burst through the night, going to sleep much later, although this is not always the case.

Historian A. Roger Ekirch thinks that the traditional pattern of "segmented sleep," as it is called, began to disappear among the urban upper class in Europe in the late 17th century and the change spread over the next 200 years; by the 1920s "the idea of a first and second sleep had receded entirely from our social consciousness." Ekirch attributes the change to increases in "street lighting, domestic lighting and a surge in coffee houses," which slowly made nighttime a legitimate time for activity, decreasing the time available for rest. Today in most societies people sleep during the night, but in very hot climates they may sleep during the day. During Ramadan, many Muslims sleep during the day rather than at night.

In some societies, people sleep with at least one other person (sometimes many) or with animals. In other cultures, people rarely sleep with anyone except for an intimate partner. In almost all societies, sleeping partners are strongly regulated by social standards. For example, a person might only sleep with the immediate family, the extended family, a spouse or romantic partner, children, children of a certain age, children of specific gender, peers of a certain gender, friends, peers of equal social rank, or with no one at all. Sleep may be an actively social time, depending on the sleep groupings, with no constraints on noise or activity.

People sleep in a variety of locations. Some sleep directly on the ground; others on a skin or blanket; others sleep on platforms or beds. Some sleep with blankets, some with pillows, some with simple headrests, some with no head support. These choices are shaped by a variety of factors, such as climate, protection from predators, housing type, technology, personal preference, and the incidence of pests.

Sleep has been seen in culture as similar to death since antiquity; in Greek mythology, Hypnos (the god of sleep) and Thanatos (the god of death) were both said to be the children of Nyx (the goddess of night). John Donne, Samuel Taylor Coleridge, Percy Bysshe Shelley, and other poets have all written poems about the relationship between sleep and death. Shelley describes them as "both so passing, strange and wonderful!" Many people consider dying in one's sleep the most peaceful way to die. Phrases such as "big sleep" and "rest in peace" are often used in reference to death, possibly in effort to lessen its finality. 

Many cultural stories have been told about people falling asleep for extended periods of time. The earliest of these stories is the ancient Greek legend of Epimenides of Knossos. According to the biographer Diogenes Laërtius, Epimenides was a shepherd on the Greek island of Crete. One day, one of his sheep went missing and he went out to look for it, but became tired and fell asleep in a cave under Mount Ida. When he awoke, he continued searching for the sheep, but could not find it, so he returned to his old farm, only to discover that it was now under new ownership. He went to his hometown, but discovered that nobody there knew him. Finally, he met his younger brother, who was now an old man, and learned that he had been asleep in the cave for fifty-seven years.

A far more famous instance of a "long sleep" today is the Christian legend of the Seven Sleepers of Ephesus, in which seven Christians flee into a cave during pagan times in order to escape persecution, but fall asleep and wake up 360 years later to discover, to their astonishment, that the Roman Empire is now predominately Christian. The American author Washington Irving's short story "Rip Van Winkle", first published in 1819 in his collection of short stories "The Sketch Book of Geoffrey Crayon, Gent.", is about a man in colonial America named Rip Van Winkle who falls asleep on one of the Catskill Mountains and wakes up twenty years later after the American Revolution. The story is now considered one of the greatest classics of American literature.

Writing about the thematical representations of sleep in art, physician and sleep researcher Meir Kryger noted: "[Artists] have intense fascination with mythology, dreams, religious themes, the parallel between sleep and death, reward, abandonment of conscious control, healing, a depiction of innocence and serenity, and the erotic."




</doc>
<doc id="27837" url="https://en.wikipedia.org/wiki?curid=27837" title="Superoxide dismutase">
Superoxide dismutase

Superoxide dismutase (SOD, ) is an enzyme that alternately catalyzes the dismutation (or partitioning) of the superoxide (O) radical into either ordinary molecular oxygen (O) or hydrogen peroxide (HO). Superoxide is produced as a by-product of oxygen metabolism and, if not regulated, causes many types of cell damage. Hydrogen peroxide is also damaging and is degraded by other enzymes such as catalase. Thus, SOD is an important antioxidant defense in nearly all living cells exposed to oxygen. One exception is "Lactobacillus plantarum" and related lactobacilli, which use a different mechanism to prevent damage from reactive (O).

SODs catalyze the disproportionation of superoxide:
In this way, O is converted into two less damaging species.

The pathway by which SOD-catalyzed dismutation of superoxide may be written, for Cu,Zn SOD, with the following reactions :

The general form, applicable to all the different metal-coordinated forms of SOD, can be written as follows:

where M = Cu (n=1) ; Mn (n=2) ; Fe (n=2) ; Ni (n=2).

In a series of such reactions, the oxidation state and the charge of the metal cation oscillates between n and n+1: +1 and +2 for Cu, or +2 and +3 for the other metals .

Irwin Fridovich and Joe McCord at Duke University discovered the enzymatic activity of superoxide dismutase in 1968. SODs were previously known as a group of metalloproteins with unknown function; for example, CuZnSOD was known as erythrocuprein (or hemocuprein, or cytocuprein) or as the veterinary anti-inflammatory drug "Orgotein". Likewise, Brewer (1967) identified a protein that later became known as superoxide dismutase as an indophenol oxidase by protein analysis of starch gels using the phenazine-tetrazolium technique.

There are three major families of superoxide dismutase, depending on the protein fold and the metal cofactor: the Cu/Zn type (which binds both copper and zinc), Fe and Mn types (which bind either iron or manganese), and the Ni type (which binds nickel).

In higher plants, SOD isozymes have been localized in different cell compartments. Mn-SOD is present in mitochondria and peroxisomes. Fe-SOD has been found mainly in chloroplasts but has also been detected in peroxisomes, and CuZn-SOD has been localized in cytosol, chloroplasts, peroxisomes, and apoplast.

Three forms of superoxide dismutase are present in humans, in all other mammals, and most chordates. SOD1 is located in the cytoplasm, SOD2 in the mitochondria, and SOD3 is extracellular. The first is a dimer (consists of two units), whereas the others are tetramers (four subunits). SOD1 and SOD3 contain copper and zinc, whereas SOD2, the mitochondrial enzyme, has manganese in its reactive centre. The genes are located on chromosomes 21, 6, and 4, respectively (21q22.1, 6q25.3 and 4p15.3-p15.1).

In higher plants, superoxide dismutase enzymes (SODs) act as antioxidants and protect cellular components from being oxidized by reactive oxygen species (ROS). ROS can form as a result of drought, injury, herbicides and pesticides, ozone, plant metabolic activity, nutrient deficiencies, photoinhibition, temperature above and below ground, toxic metals, and UV or gamma rays. To be specific, molecular O is reduced to O (a ROS called superoxide) when it absorbs an excited electron released from compounds of the electron transport chain. Superoxide is known to denature enzymes, oxidize lipids, and fragment DNA. SODs catalyze the production of O and HO from superoxide (O), which results in less harmful reactants.

When acclimating to increased levels of oxidative stress, SOD concentrations typically increase with the degree of stress conditions. The compartmentalization of different forms of SOD throughout the plant makes them counteract stress very effectively. There are three well-known and -studied classes of SOD metallic coenzymes that exist in plants. First, Fe SODs consist of two species, one homodimer (containing 1-2 g Fe) and one tetramer (containing 2-4 g Fe). They are thought to be the most ancient SOD metalloenzymes and are found within both prokaryotes and eukaryotes. Fe SODs are most abundantly localized inside plant chloroplasts, where they are indigenous. Second, Mn SODs consist of a homodimer and homotetramer species each containing a single Mn(III) atom per subunit. They are found predominantly in mitochondrion and peroxisomes. Third, Cu-Zn SODs have electrical properties very different from those of the other two classes. These are concentrated in the chloroplast, cytosol, and in some cases the extracellular space. Note that Cu-Zn SODs provide less protection than Fe SODs when localized in the chloroplast.

Human white blood cells use enzymes such as NADPH oxidase to generate superoxide and other reactive oxygen species to kill bacteria. During infection, some bacteria (e.g., "Burkholderia pseudomallei") therefore produce superoxide dismutase to protect themselves from being killed.

SOD out-competes damaging reactions of superoxide, thus protecting the cell from superoxide toxicity.
The reaction of superoxide with non-radicals is spin-forbidden. In biological systems, this means that its main reactions are with itself (dismutation) or with another biological radical such as nitric oxide (NO) or with a transition-series metal. The superoxide anion radical (O) spontaneously dismutes to O and hydrogen peroxide (HO) quite rapidly (~10 Ms at pH 7). SOD is necessary because superoxide reacts with sensitive and critical cellular targets. For example, it reacts with the NO radical, and makes toxic peroxynitrite.

Because the uncatalysed dismutation reaction for superoxide requires two superoxide molecules to react with each other, the dismutation rate is second-order with respect to initial superoxide concentration. Thus, the half-life of superoxide, although very short at high concentrations (e.g., 0.05 seconds at 0.1mM) is actually quite long at low concentrations (e.g., 14 hours at 0.1 nM). In contrast, the reaction of superoxide with SOD is first order with respect to superoxide concentration. Moreover, superoxide dismutase has the largest k/K (an approximation of catalytic efficiency) of any known enzyme (~7 x 10 Ms), this reaction being limited only by the frequency of collision between itself and superoxide. That is, the reaction rate is "diffusion-limited".

The high efficiency of superoxide dismutase seems necessary: even at the subnanomolar concentrations achieved by the high concentrations of SOD within cells, superoxide inactivates the citric acid cycle enzyme aconitase, can poison energy metabolism, and releases potentially toxic iron. Aconitase is one of several iron-sulfur-containing (de)hydratases in metabolic pathways shown to be inactivated by superoxide.

SOD1 is an extremely stable protein. In the holo form (both copper and zinc bound) the melting point is > 90°C. In the apo form (no copper or zinc bound) the melting point is ~ 60°C. By differential scanning calorimetry (DSC), holo SOD1 unfolds by a two-state mechanism: from dimer to two unfolded monomers. In chemical denaturation experiments, holo SOD1 unfolds by a three-state mechanism with observation of a folded monomeric intermediate.

Superoxide is one of the main reactive oxygen species in the cell. As a consequence, SOD serves a key antioxidant role. The physiological importance of SODs is illustrated by the severe pathologies evident in mice genetically engineered to lack these enzymes. Mice lacking SOD2 die several days after birth, amid massive oxidative stress. Mice lacking SOD1 develop a wide range of pathologies, including hepatocellular carcinoma, an acceleration of age-related muscle mass loss, an earlier incidence of cataracts, and a reduced lifespan. Mice lacking SOD3 do not show any obvious defects and exhibit a normal lifespan, though they are more sensitive to hyperoxic injury. Knockout mice of any SOD enzyme are more sensitive to the lethal effects of superoxide-generating compounds, such as paraquat and diquat (herbicides).

"Drosophila" lacking SOD1 have a dramatically shortened lifespan, whereas flies lacking SOD2 die before birth.

SOD knockdowns in the worm "C. elegans" do not cause major physiological disruptions. However, the lifespan of "C. elegans" can be extended by superoxide/catalase mimetics suggesting that oxidative stress is a major determinant of the rate of aging.

Knockout or null mutations in SOD1 are highly detrimental to aerobic growth in the budding yeast "Saccharomyces cerevisiae" and result in a dramatic reduction in post-diauxic lifespan. In wild-type "S. cerevisiae", DNA damage rates increased 3-fold with age, but more than 5-fold in mutants deleted for either the "SOD1" or "SOD2" genes. Reactive oxygen species levels increase with age in these mutant strains and show a similar pattern to the pattern of DNA damage increase with age. Thus it appears that superoxide dismutase plays a substantial role in preserving genome integrity during aging in "S. cerevisiae".
SOD2 knockout or null mutations cause growth inhibition on respiratory carbon sources in addition to decreased post-diauxic lifespan.

In the fission yeast "Schizosaccharomyces pombe", deficiency of mitochondrial superoxide dismutase SOD2 accelerates chronological aging.

Several prokaryotic SOD null mutants have been generated, including "E. coli". The loss of periplasmic CuZnSOD causes loss of virulence and might be an attractive target for new antibiotics.

Mutations in the first SOD enzyme (SOD1) can cause familial amyotrophic lateral sclerosis (ALS, a form of motor neuron disease). The most common mutation in the U.S. is A4V, while the most intensely studied is G93A. The other two isoforms of SOD have not been linked to any human diseases, however, in mice inactivation of SOD2 causes perinatal lethality and inactivation of SOD1 causes hepatocellular carcinoma. Mutations in SOD1 can cause familial ALS (several pieces of evidence also show that wild-type SOD1, under conditions of cellular stress, is implicated in a significant fraction of sporadic ALS cases, which represent 90% of ALS patients.), by a mechanism that is presently not understood, but not due to loss of enzymatic activity or a decrease in the conformational stability of the SOD1 protein. Overexpression of SOD1 has been linked to the neural disorders seen in Down syndrome. In patients with thalassemia, SOD will increase as a form of compensation mechanism. However, in the chronic stage, SOD does not seem to be sufficient and tends to decrease due to the destruction of proteins from the massive reaction of oxidant-antioxidant.

In mice, the extracellular superoxide dismutase (SOD3, ecSOD) contributes to the development of hypertension. Diminished SOD3 activity has been linked to lung diseases such as Acute Respiratory Distress Syndrome (ARDS) or Chronic obstructive pulmonary disease (COPD).

Superoxide dismutase is also not expressed in neural crest cells in the developing fetus. Hence, high levels of free radicals can cause damage to them and induce dysraphic anomalies (neural tube defects).

A cross-sectional study in humans suggests that serum SOD could be a marker of cardiovascular alterations in hypertensive and diabetic patients, since changes in its serum levels are correlated with alterations in vascular structure and function.

SOD has powerful antinflammatory activity. For example, SOD is a highly effective experimental treatment of chronic inflammation in colitis. Treatment with SOD decreases reactive oxygen species generation and oxidative stress and, thus, inhibits endothelial activation. Therefore, such antioxidants may be important new therapies for the treatment of inflammatory bowel disease.

Likewise, SOD has multiple pharmacological activities. E.g., it ameliorates cis-platinum-induced nephrotoxicity in rodents. As "Orgotein" or "ontosein", a pharmacologically-active purified bovine liver SOD, it is also effective in the treatment of urinary tract inflammatory disease in man. For a time, bovine liver SOD even had regulatory approval in several European countries for such use. This was cut short by concerns about prion disease.

An SOD-mimetic agent, TEMPOL, is currently in clinical trials for radioprotection and to prevent radiation-induced dermatitis. TEMPOL and similar SOD-mimetic nitroxides exhibit a multiplicity of actions in diseases involving oxidative stress.

SOD may reduce free radical damage to skin—for example, to reduce fibrosis following radiation for breast cancer. Studies of this kind must be regarded as tentative, however, as there were not adequate controls in the study including a lack of randomization, double-blinding, or placebo. Superoxide dismutase is known to reverse fibrosis, possibly through de-differentiation of myofibroblasts back to fibroblasts.

SOD is commercially obtained from marine phytoplankton, bovine liver, horseradish, cantaloupe, and certain bacteria. For therapeutic purpose, SOD is usually injected locally. There is no evidence that ingestion of unprotected SOD or SOD-rich foods can have any physiological effects, as all ingested SOD is broken down into amino acids before being absorbed. However, ingestion of SOD bound to wheat proteins could improve its therapeutic activity, at least in theory.




</doc>
<doc id="27838" url="https://en.wikipedia.org/wiki?curid=27838" title="Sequence">
Sequence

In mathematics, a sequence is an enumerated collection of objects in which repetitions are allowed. Like a set, it contains members (also called "elements", or "terms"). The number of elements (possibly infinite) is called the "length" of the sequence. Unlike a set, the same elements can appear multiple times at different positions in a sequence, and order matters. Formally, a sequence can be defined as a function whose domain is either the set of the natural numbers (for infinite sequences) or the set of the first "n" natural numbers (for a sequence of finite length "n"). The position of an element in a sequence is its "rank" or "index"; it is the integer from which the element is the image. It depends on the context or of a specific convention, if the first element has index 0 or 1. When a symbol has been chosen for denoting a sequence, the "n"th element of the sequence is denoted by this symbol with "n" as subscript; for example, the "n"th element of the Fibonacci sequence is generally denoted "F".

For example, (M, A, R, Y) is a sequence of letters with the letter 'M' first and 'Y' last. This sequence differs from (A, R, M, Y). Also, the sequence (1, 1, 2, 3, 5, 8), which contains the number 1 at two different positions, is a valid sequence. Sequences can be "finite", as in these examples, or "infinite", such as the sequence of all even positive integers (2, 4, 6, ...). In computing and computer science, finite sequences are sometimes called strings, words or lists, the different names commonly corresponding to different ways to represent them in computer memory; infinite sequences are called streams. The empty sequence ( ) is included in most notions of sequence, but may be excluded depending on the context.
A sequence can be thought of as a list of elements with a particular order. Sequences are useful in a number of mathematical disciplines for studying functions, spaces, and other mathematical structures using the convergence properties of sequences. In particular, sequences are the basis for series, which are important in differential equations and analysis. Sequences are also of interest in their own right and can be studied as patterns or puzzles, such as in the study of prime numbers.

There are a number of ways to denote a sequence, some of which are more useful for specific types of sequences. One way to specify a sequence is to list the elements. For example, the first four odd numbers form the sequence (1, 3, 5, 7). This notation can be used for infinite sequences as well. For instance, the infinite sequence of positive odd integers can be written (1, 3, 5, 7, ...). Listing is most useful for infinite sequences with a pattern that can be easily discerned from the first few elements. Other ways to denote a sequence are discussed after the examples.

The prime numbers are the natural numbers bigger than 1 that have no divisors but 1 and themselves. Taking these in their natural order gives the sequence (2, 3, 5, 7, 11, 13, 17, ...). The prime numbers are widely used in mathematics and specifically in number theory.

The Fibonacci numbers are the integer sequence whose elements are the sum of the previous two elements. The first two elements are either 0 and 1 or 1 and 1 so that the sequence is (0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...).

For a large list of examples of integer sequences, see On-Line Encyclopedia of Integer Sequences.

Other examples of sequences include ones made up of rational numbers, real numbers, and complex numbers. The sequence (.9, .99, .999, .9999, ...) approaches the number 1. In fact, every real number can be written as the limit of a sequence of rational numbers, e.g. via its decimal expansion. For instance, π is the limit of the sequence (3, 3.1, 3.14, 3.141, 3.1415, ...). A related sequence is the sequence of decimal digits of π, i.e. (3, 1, 4, 1, 5, 9, ...). This sequence does not have any pattern that is easily discernible by eye, unlike the preceding sequence, which is increasing.

Other notations can be useful for sequences whose pattern cannot be easily guessed, or for sequences that do not have a pattern such as the digits of π. One such notation is to write down a general formula for computing the "n"th term as a function of "n", enclose it in parentheses, and include a subscript indicating the range of values that "n" can take. For example, in this notation the sequence of even numbers could be written as formula_1. The sequence of squares could be written as formula_2. The variable "n" is called an index, and the set of values that it can take is called the index set.

It is often useful to combine this notation with the technique of treating the elements of a sequence as variables. This yields expressions like formula_3, which denotes a sequence whose "n"th element is given by the variable formula_4. For example:
Note that we can consider multiple sequences at the same time by using different variables; e.g. formula_6 could be a different sequence than formula_3. We can even consider a sequence of sequences: formula_8 denotes a sequence whose "m"th term is the sequence formula_9.

An alternative to writing the domain of a sequence in the subscript is to indicate the range of values that the index can take by listing its highest and lowest legal values. For example, the notation formula_10 denotes the ten-term sequence of squares formula_11. The limits formula_12 and formula_13 are allowed, but they do not represent valid values for the index, only the supremum or infimum of such values, respectively. For example, the sequence formula_14 is the same as the sequence formula_3, and does not contain an additional term "at infinity". The sequence formula_16 is a bi-infinite sequence, and can also be written as formula_17.

In cases where the set of indexing numbers is understood, the subscripts and superscripts are often left off. That is, one simply writes formula_18 for an arbitrary sequence. Often, the index "k" is understood to run from 1 to ∞. However, sequences are frequently indexed starting from zero, as in
In some cases the elements of the sequence are related naturally to a sequence of integers whose pattern can be easily inferred. In these cases the index set may be implied by a listing of the first few abstract elements. For instance, the sequence of squares of odd numbers could be denoted in any of the following ways.


Moreover, the subscripts and superscripts could have been left off in the third, fourth, and fifth notations, if the indexing set was understood to be the natural numbers. Note that in the second and third bullets, there is a well-defined sequence formula_25, but it is not the same as the sequence denoted by the expression.

Sequences whose elements are related to the previous elements in a straightforward way are often defined using recursion. This is in contrast to the definition of sequence elements as a function of their position.

To define a sequence by recursion, one needs a rule to construct each element in terms of the ones before it. In addition, enough initial elements must be provided so that all subsequent elements of the sequence can be computed by the rule. The principle of mathematical induction can be used to prove that in this case, there is exactly one sequence that satisfies both the recursion rule and the initial conditions. Induction can also be used to prove properties about a sequence, especially for sequences whose most natural description is recursive.

The Fibonacci sequence can be defined using a recursive rule along with two initial elements. The rule is that each element is the sum of the previous two elements, and the first two elements are 0 and 1.
The first ten terms of this sequence are 0, 1, 1, 2, 3, 5, 8, 13, 21, and 34. A more complicated example of a sequence that is defined recursively is Recaman's sequence. We can define Recaman's sequence by

Not all sequences can be specified by a rule in the form of an equation, recursive or not, and some can be quite complicated. For example, the sequence of prime numbers is the set of prime numbers in their natural order, i.e. (2, 3, 5, 7, 11, 13, 17, ...).

Many sequences have the property that each element of a sequence can be computed from the previous element. In this case, there is some function "f" such that for all "n", formula_32.

There are many different notions of sequences in mathematics, some of which ("e.g.", exact sequence) are not covered by the definitions and notations introduced below.

For the purposes of this article, we define a sequence to be a function whose domain is an interval of integers. This definition covers several different uses of the word "sequence", including one-sided infinite sequences, bi-infinite sequences, and finite sequences (see below for definitions). However, many authors use a narrower definition by requiring the domain of a sequence to be the set of natural numbers. The narrower definition has the disadvantage that it rules out finite sequences and bi-infinite sequences, both of which are usually called sequences in standard mathematical practice. In some contexts, to shorten exposition, the codomain of the sequence is fixed by context, for example by requiring it to be the set R of real numbers, the set C of complex numbers, or a topological space.

Although sequences are a type of function, they are usually distinguished notationally from functions in that the input is written as a subscript rather than in parentheses, i.e. "a" rather than "f"("n"). There are terminological differences as well: the value of a sequence at the input 1 is called the "first element" of the sequence, the value at 2 is called the "second element", etc. Also, while a function abstracted from its input is usually denoted by a single letter, e.g. "f", a sequence abstracted from its input is usually written by a notation such as formula_33, or just as formula_34. Here "A" is the domain, or index set, of the sequence.

Sequences and their limits (see below) are important concepts for studying topological spaces. An important generalization of sequences is the concept of nets. A net is a function from a (possibly uncountable) directed set to a topological space. The notational conventions for sequences normally apply to nets as well.

The length of a sequence is defined as the number of terms in the sequence.

A sequence of a finite length "n" is also called an "n"-tuple. Finite sequences include the empty sequence ( ) that has no elements.
Normally, the term "infinite sequence" refers to a sequence that is infinite in one direction, and finite in the other—the sequence has a first element, but no final element. Such a sequence is called a singly infinite sequence or a one-sided infinite sequence when disambiguation is necessary. In contrast, a sequence that is infinite in both directions—i.e. that has neither a first nor a final element—is called a bi-infinite sequence, two-way infinite sequence, or doubly infinite sequence. A function from the set Z of "all" integers into a set, such as for instance the sequence of all even integers ( …, −4, −2, 0, 2, 4, 6, 8… ), is bi-infinite. This sequence could be denoted formula_35.

A sequence is said to be "monotonically increasing", if each term is greater than or equal to the one before it. For example, the sequence formula_36 is monotonically increasing if and only if "a" formula_37 "a" for all "n" ∈ N. If each consecutive term is strictly greater than (>) the previous term then the sequence is called strictly monotonically increasing. A sequence is monotonically decreasing, if each consecutive term is less than or equal to the previous one, and strictly monotonically decreasing, if each is strictly less than the previous. If a sequence is either increasing or decreasing it is called a monotone sequence. This is a special case of the more general notion of a monotonic function.

The terms nondecreasing and nonincreasing are often used in place of "increasing" and "decreasing" in order to avoid any possible confusion with "strictly increasing" and "strictly decreasing", respectively.

If the sequence of real numbers ("a") is such that all the terms are less than some real number "M", then the sequence is said to be bounded from above. In other words, this means that there exists "M" such that for all "n", "a" ≤ "M". Any such "M" is called an "upper bound". Likewise, if, for some real "m", "a" ≥ "m" for all "n" greater than some "N", then the sequence is bounded from below and any such "m" is called a "lower bound". If a sequence is both bounded from above and bounded from below, then the sequence is said to be bounded.

A subsequence of a given sequence is a sequence formed from the given sequence by deleting some of the elements without disturbing the relative positions of the remaining elements. For instance, the sequence of positive even integers (2, 4, 6, ...) is a subsequence of the positive integers (1, 2, 3, ...). The positions of some elements change when other elements are deleted. However, the relative positions are preserved.

Formally, a subsequence of the sequence formula_3 is any sequence of the form formula_39, where formula_40 is a strictly increasing sequence of positive integers.

Some other types of sequences that are easy to define include:

An important property of a sequence is "convergence". If a sequence converges, it converges to a particular value known as the "limit". If a sequence converges to some limit, then it is convergent. A sequence that does not converge is divergent.

Informally, a sequence has a limit if the elements of the sequence become closer and closer to some value formula_41 (called the limit of the sequence), and they become and remain "arbitrarily" close to formula_41, meaning that given a real number formula_43 greater than zero, all but a finite number of the elements of the sequence have a distance from formula_41 less than formula_43. 

For example, the sequence formula_46 shown to the right converges to the value 0. On the other hand, the sequences formula_47 (which begins 1, 8, 27, …) and formula_48 (which begins -1, 1, -1, 1, …) are both divergent. 

If a sequence converges, then the value it converges to is unique. This value is called the limit of the sequence. The limit of a convergent sequence formula_34 is normally denoted formula_50. If formula_34 is a divergent sequence, then the expression formula_50 is meaningless.

A sequence of real numbers formula_34 converges to a real number formula_41 if, for all formula_55, there exists a natural number formula_56 such that for all formula_57 we have formula_58

If formula_34 is a sequence of complex numbers rather than a sequence of real numbers, this last formula can still be used to define convergence, with the provision that formula_60 denotes the complex modulus, i.e. formula_61. If formula_34 is a sequence of points in a metric space, then the formula can be used to define convergence, if the expression formula_63 is replaced by the expression formula_64, which denotes the distance between formula_4 and formula_41.

If formula_34 and formula_68 are convergent sequences, then the following limits exist, and can be computed as follows:


Moreover:

A Cauchy sequence is a sequence whose terms become arbitrarily close together as n gets very large. The notion of a Cauchy sequence is important in the study of sequences in metric spaces, and, in particular, in real analysis. One particularly important result in real analysis is "Cauchy characterization of convergence for sequences":
In contrast, there are Cauchy sequences of rational numbers that are not convergent in the rationals, e.g. the sequence defined by
"x" = 1 and "x" = 
is Cauchy, but has no rational limit, cf. . More generally, any sequence of rational numbers that converges to an irrational number is Cauchy, but not convergent when interpreted as a sequence in the set of rational numbers.

Metric spaces that satisfy the Cauchy characterization of convergence for sequences are called complete metric spaces and are particularly nice for analysis.

In calculus, it is common to define notation for sequences which do not converge in the sense discussed above, but which instead become and remain arbitrarily large, or become and remain arbitrarily negative. If formula_4 becomes arbitrarily large as formula_87, we write
In this case we say that the sequence diverges, or that it converges to infinity. An example of such a sequence is .

If formula_4 becomes arbitrarily negative (i.e. negative and large in magnitude) as formula_87, we write
and say that the sequence diverges or converges to negative infinity.

A series is, informally speaking, the sum of the terms of a sequence. That is, it is an expression of the form formula_92 or formula_93, where formula_34 is a sequence of real or complex numbers. The partial sums of a series are the expressions resulting from replacing the infinity symbol with a finite number, i.e. the "N"th partial sum of the series formula_92 is the number
The partial sums themselves form a sequence formula_97, which is called the sequence of partial sums of the series formula_92. If the sequence of partial sums converges, then we say that the series formula_92 is convergent, and the limit formula_100 is called the value of the series. The same notation is used to denote a series and its value, i.e. we write formula_101.

Sequences play an important role in topology, especially in the study of metric spaces. For instance:

Sequences can be generalized to nets or filters. These generalizations allow one to extend some of the above theorems to spaces without metrics.

The topological product of a sequence of topological spaces is the cartesian product of those spaces, equipped with a natural topology called the product topology.

More formally, given a sequence of spaces formula_102, the product space

is defined as the set of all sequences formula_104 such that for each "i", formula_105 is an element of formula_106. The canonical projections are the maps "p" : "X" → "X" defined by the equation formula_107. Then the product topology on "X" is defined to be the coarsest topology (i.e. the topology with the fewest open sets) for which all the projections "p" are continuous. The product topology is sometimes called the Tychonoff topology.

In analysis, when talking about sequences, one will generally consider sequences of the form
which is to say, infinite sequences of elements indexed by natural numbers.

It may be convenient to have the sequence start with an index different from 1 or 0. For example, the sequence defined by "x" = 1/log("n") would be defined only for "n" ≥ 2. When talking about such infinite sequences, it is usually sufficient (and does not change much for most considerations) to assume that the members of the sequence are defined at least for all indices large enough, that is, greater than some given "N".

The most elementary type of sequences are numerical ones, that is, sequences of real or complex numbers. This type can be generalized to sequences of elements of some vector space. In analysis, the vector spaces considered are often function spaces. Even more generally, one can study sequences with elements in some topological space.

A sequence space is a vector space whose elements are infinite sequences of real or complex numbers. Equivalently, it is a function space whose elements are functions from the natural numbers to the field K, where K is either the field of real numbers or the field of complex numbers. The set of all such functions is naturally identified with the set of all possible infinite sequences with elements in K, and can be turned into a vector space under the operations of pointwise addition of functions and pointwise scalar multiplication. All sequence spaces are linear subspaces of this space. Sequence spaces are typically equipped with a norm, or at least the structure of a topological vector space.

The most important sequences spaces in analysis are the ℓ spaces, consisting of the "p"-power summable sequences, with the "p"-norm. These are special cases of L spaces for the counting measure on the set of natural numbers. Other important classes of sequences like convergent sequences or null sequences form sequence spaces, respectively denoted "c" and "c", with the sup norm. Any sequence space can also be equipped with the topology of pointwise convergence, under which it becomes a special kind of Fréchet space called an FK-space.

Sequences over a field may also be viewed as vectors in a vector space. Specifically, the set of "F"-valued sequences (where "F" is a field) is a function space (in fact, a product space) of "F"-valued functions over the set of natural numbers.

Abstract algebra employs several types of sequences, including sequences of mathematical objects such as groups or rings.

If "A" is a set, the free monoid over "A" (denoted "A", also called Kleene star of "A") is a monoid containing all the finite sequences (or strings) of zero or more elements of "A", with the binary operation of concatenation. The free semigroup "A" is the subsemigroup of "A" containing all elements except the empty sequence.

In the context of group theory, a sequence
of groups and group homomorphisms is called exact, if the image (or range) of each homomorphism is equal to the kernel of the next:

Note that the sequence of groups and homomorphisms may be either finite or infinite.

A similar definition can be made for certain other algebraic structures. For example, one could have an exact sequence of vector spaces and linear maps, or of modules and module homomorphisms.

In homological algebra and algebraic topology, a spectral sequence is a means of computing homology groups by taking successive approximations. Spectral sequences are a generalization of exact sequences, and since their introduction by , they have become an important research tool, particularly in homotopy theory.

An ordinal-indexed sequence is a generalization of a sequence. If α is a limit ordinal and "X" is a set, an α-indexed sequence of elements of "X" is a function from α to "X". In this terminology an ω-indexed sequence is an ordinary sequence.

Automata or finite state machines can typically be thought of as directed graphs, with edges labeled using some specific alphabet, Σ. Most familiar types of automata transition from state to state by reading input letters from Σ, following edges with matching labels; the ordered input for such an automaton forms a sequence called a "word" (or input word). The sequence of states encountered by the automaton when processing a word is called a "run". A nondeterministic automaton may have unlabeled or duplicate out-edges for any state, giving more than one successor for some input letter. This is typically thought of as producing multiple possible runs for a given word, each being a sequence of single states, rather than producing a single run that is a sequence of sets of states; however, 'run' is occasionally used to mean the latter.

Infinite sequences of digits (or characters) drawn from a finite alphabet are of particular interest in theoretical computer science. They are often referred to simply as "sequences" or "streams", as opposed to finite "strings". Infinite binary sequences, for instance, are infinite sequences of bits (characters drawn from the alphabet {0, 1}). The set "C" = {0, 1} of all infinite binary sequences is sometimes called the Cantor space.

An infinite binary sequence can represent a formal language (a set of strings) by setting the "n" th bit of the sequence to 1 if and only if the "n" th string (in shortlex order) is in the language. This representation is useful in the diagonalization method for proofs.








</doc>
<doc id="27840" url="https://en.wikipedia.org/wiki?curid=27840" title="Senryū">
Senryū

Senryū is named after Edo period haikai poet Senryū Karai (柄井川柳, 1718–1790), whose collection launched the genre into the public consciousness. A typical example from the collection:
This senryū, which can also be translated "Catching him / you see the robber / is your son," is not so much a personal experience of the author as an example of a type of situation (provided by a short comment called a maeku or fore-verse, which usually prefaces a number of examples) and/or a brief or witty rendition of an incident from history or the arts (plays, songs, tales, poetry, etc.). In this case, there was a historical incident of legendary proportion.

Some senryū skirt the line between haiku and senryū. The following senryū by Shūji Terayama copies the haiku structure faithfully, down to a blatantly obvious kigo, but on closer inspection is absurd in its content:
Terayama, who wrote about playing hide-and-seek in the graveyard as a child, thought of himself as the odd one out, the one who was always "it" in hide-and-seek. Indeed, the original haiku included the theme "oni" (the "it" in Japanese is a demon, though in some parts a very young child forced to play "it" was called a "sea slug" (namako)). To him, seeing a game of hide-and-seek, or recalling it as it grew cold would be a chilling experience. Terayama might also have recalled opening his eyes and finding himself all alone, feeling the cold more intensely than he did a minute before among other children. Either way, any genuinely personal experience would be haiku and not senryū in the classic sense. If you think Terayama's poem uses a child's game to express in hyperbolic metaphor how, in retrospect, life is short, and nothing more, then this would indeed work as a senryū. Otherwise, it is a bona-fide haiku. There is also the possibility that it is a joke about playing hide and seek, only to realize (winter having arrived during the months spent hiding) that no one wants to find you.

In the 1970s, Michael McClintock edited "Seer Ox: American Senryu Magazine". In 1993, Michael Dylan Welch edited and published "Fig Newtons: Senryū to Go", the first anthology of English-language senryū.
Additionally, one can regularly find senryū and related articles in some haiku publications. For example:

Senryū regularly appear in the pages of "Modern Haiku", "Frogpond", "Bottle Rockets", "Woodnotes", "Tundra", and other haiku journals, often unsegregated from haiku.

The Haiku Society of America holds the annual Gerald Brady Memorial Award for best unpublished senryū.

Since about 1990, the Haiku Poets of Northern California has been running a senryū contest, as part of its San Francisco International Haiku and Senryu Contest.





</doc>
<doc id="27843" url="https://en.wikipedia.org/wiki?curid=27843" title="September 30">
September 30

It is not only the last day of the third quarter of the year, but also the midway point of the second half of the year.





</doc>
<doc id="27846" url="https://en.wikipedia.org/wiki?curid=27846" title="Sorious Samura">
Sorious Samura

Sorious Samura (born 27 October 1963) is a Sierra Leonean journalist. He is best known for two CNN documentary films: "Cry Freetown" (2000) and "Exodus from Africa" (2001). The self-funded "Cry Freetown" depicts the most brutal period of the civil war in Sierra Leone with RUF rebels capturing the capital city (January 1999). The film won, among other awards, an Emmy Award and a Peabody. "Exodus from Africa" shows the harrowing effort by the best of young African male blood to break through to Europe via death- and danger-ridden paths from Sierra Leone and Nigeria, via Mali, the Sahara desert, Algeria, and Morocco through the Strait of Gibraltar to Spain.

In his recent two projects "Living with Hunger" and "Living with Refugees" (nominated for an Emmy award), he takes reality television to its extreme, becoming the central character in the films by living the lifestyle of an Ethiopian villager and Sudanese refugee respectively; in doing this, he tries to break the boundary between "us" (the people watching on TV) and "them" (those before the camera) by becoming one of them (albeit for just a month). "Living with corruption", his latest documentary shown on CNN, describes the shocking reality of how corruption is spread across society both in Sierra Leone and Kenya, affecting mostly the poor.

In 2010, Samura investigated attitudes to homosexuality in Africa in the Dispatches documentary "Africa's Last Taboo", produced for Channel 4.

Samura is also one of the directors of 'Insight News TV', an independent television production company in the UK focused on international current affairs programming.

Samura attended the Methodist Boys High School in the east end of Freetown. , he works in London, UK, and considers both London and Freetown his hometowns.



</doc>
<doc id="27847" url="https://en.wikipedia.org/wiki?curid=27847" title="Sandpit">
Sandpit

A sandpit (most Commonwealth countries) or sandbox (US/Canada) is a low, wide container or shallow depression filled with soft (beach) sand in which children can play. Sharp sand (as used in the building industry) is not suitable. Many homeowners with children build sandpits in their backyards because, unlike much playground equipment, they can be easily and cheaply constructed. A "sandpit" may also denote an open pit sand mine.

German sand gardens were the first organization of children's play in public spaces. The German “sand gardens” were an 1850 offshoot of Friedrich Froebel’s work on kindergartens. In 1886, Dr. Marie Zakrsewska, who while visiting in Berlin, in summer of 1885, observed sand gardens. Boston introduced sand gardens to America. Joseph Lee (recreation advocate) is considered the "founder of the playground movement.

The "pit" or "box" itself is simply a container for storing the sand so that it does not spread outward across lawns or other surrounding surfaces. Boxes of various shapes are often constructed from planks, logs, or other large wooden frames that allow children easy access to the sand and also provide a convenient place to sit. Small sandpits are also available commercially. These are usually made from plastic or wood and are often shaped like an animal or other familiar objects.

They sometimes also have lids to cover the sand when not in use, so that passing animals cannot contaminate the sand by urinating or defecating in it. Having lids also prevents the sand in outdoor sandpits from getting wet when it rains, although some dampness is often desirable as it helps the sand hold together. Prefabricated sandpits may also be used indoors, especially in day care facilities. Materials other than sand are also often used, such as oatmeal, which are necessarily non-toxic and light enough to easily vacuum up.

Sandpits can have a solid bottom or they can be built directly onto the soil. The latter allows free drainage (which is useful if the top is open) but can lead to contamination of the sand with soil if the children dig down to the ground.

The sand gets dirty over time and is eventually replaced. Many schools and playgrounds in North America have replaced sand around play structures with a wood chip mixture, as it is cheaper.




</doc>
<doc id="27848" url="https://en.wikipedia.org/wiki?curid=27848" title="Steve Wozniak">
Steve Wozniak

Stephen Gary Wozniak (; born on August 11, 1950), often referred to by the nickname Woz, is an American inventor, electronics engineer, programmer, philanthropist, and technology entrepreneur who co-founded Apple Computer, Inc. He and Apple co-founder Steve Jobs are widely recognized as pioneers of the personal computer revolution of the 1970s and 1980s.

Wozniak designed and developed the Apple I in 1976, which became the computer that launched Apple when he and Jobs marketed it that same year. He primarily designed the Apple II in 1977, known as one of the first highly successful mass-produced microcomputers, while Jobs oversaw the development of its foam-molded plastic case and early Apple employee Rod Holt developed the switching power supply. He also had major influence until 1981, along with computer scientist Jef Raskin, over the initial development of the original Apple Macintosh, which Jobs then took over following Wozniak's brief departure from the company due to a traumatic airplane accident. After stepping away from Apple in 1985 for good, Wozniak founded CL 9 and created the first universal remote, released in 1987. He then pursued several other business and philanthropic ventures throughout his career, focusing largely on tech in K–12 schools.

Wozniak is currently Chief Scientist at the data virtualization company Primary Data, and has remained an employee of Apple over the years in a ceremonial capacity.

Steve Wozniak was born in San Jose, California, the son of Francis Jacob "Jerry" Wozniak (1925–1994) from Michigan and Margaret Louise Wozniak (née Kern) (1923–2014) from Washington (state). He graduated from Homestead High School in 1968.
The name on Wozniak's birth certificate is "Stephan Gary Wozniak", but Steve's mother said that she intended it to be spelled "Stephen", which is what he uses. Wozniak has mentioned his surname being Polish and Ukrainian and spoken of his Polish descent, but stated that he does not know the origin of some other people with the Wozniak surname because he is "no heritage expert".

In the early 1970s, Wozniak was known as "Berkeley Blue" in the phone phreak community, after he made a blue box.

Wozniak has credited watching "Star Trek" and attending "Star Trek" conventions while in his youth as a source of inspiration for his starting Apple Inc.

In 1969, Wozniak returned to the Bay Area after being expelled from University of Colorado Boulder in his first year for sending prank messages on the university's computer system. During this time, as a self-taught project, Wozniak designed and built a "Cream Soda" computer with his friend Bill Fernandez. He later re-enrolled at De Anza College and transferred to University of California, Berkeley in 1971. Before focusing his attention on Apple, he was employed at Hewlett-Packard (HP) where he designed calculators. It was during this time that he befriended Steve Jobs.

Wozniak was introduced to Jobs by Fernandez, who attended Homestead High School with Jobs in 1971. Jobs and Wozniak became friends when Jobs worked for the summer at HP, where Wozniak too was employed, working on a mainframe computer. This was recounted by Wozniak in a 2007 interview with ABC News, of how and when he first met Jobs: 

In 1973, Jobs was working for arcade game company Atari, Inc. in Los Gatos, California. He was assigned to create a circuit board for the arcade video game "Breakout". According to Atari co-founder Nolan Bushnell, Atari offered $100 () for each chip that was eliminated in the machine. Jobs had little knowledge of circuit board design and made a deal with Wozniak to split the fee evenly between them if Wozniak could minimize the number of chips. Wozniak reduced the number of chips by 50, by using RAM for the brick representation. Too complex to be fully comprehended at the time, the fact that this prototype also had no scoring or coin mechanisms meant Woz's prototype could not be used. Jobs was paid the full bonus regardless. Jobs told Wozniak that Atari gave them only $700 and that Wozniak's share was thus $350 (). Wozniak did not learn about the actual $5,000 bonus () until ten years later, but said that if Jobs had told him about it and had said he needed the money, Wozniak would have given it to him.

On June 29, 1975 Wozniak tested his first working prototype, displaying a few letters and running sample programs. It was the first time in history that a character displayed on a TV screen was generated by a home computer. With the Apple I design, he and Jobs were largely working to impress other members of the Palo Alto-based Homebrew Computer Club, a local group of electronics hobbyists interested in computing. The Club was one of several key centers which established the home hobbyist era, essentially creating the microcomputer industry over the next few decades. Unlike other Homebrew designs, the Apple had an easy-to-achieve video capability that drew a crowd when it was unveiled.

In 1976, Wozniak developed the computer that eventually made him famous. He alone designed the hardware, circuit board designs, and operating system for the Apple I. Wozniak originally offered the design to HP while working there, but was denied by the company on five different occasions. Jobs instead had the idea to sell the Apple I with Wozniak as a fully assembled printed circuit board. Wozniak, at first skeptical, was later convinced by Jobs that even if they were not successful they could at least say to their grandkids they had had their own company. Together they sold some of their possessions (such as Wozniak's HP scientific calculator and Jobs' Volkswagen van), raised $1,300, and assembled the first boards in Jobs' bedroom and later (when there was no space left) in Jobs' garage. Wozniak's apartment in San Jose was filled with monitors, electronic devices, and some computer games Wozniak had developed. The Apple I sold for $666.66. (Wozniak later said he had no idea about the relation between the number and the mark of the beast, and "I came up with [it] because I like repeating digits.") Jobs and Wozniak sold their first 50 system boards to Paul Terrell, who was starting a new computer shop, called the Byte Shop, in Mountain View, California.

On April 1, 1976, Jobs and Wozniak formed Apple Computer (now called Apple Inc.) along with administrative supervisor Ronald Wayne, whose participation in the new venture was short lived. Wozniak resigned from his job at Hewlett-Packard and became the vice president in charge of research and development at Apple. He and Jobs decided on the name "Apple" shortly after Jobs returned from an apple orchard in Oregon. Wozniak's Apple I was similar to the Altair 8800, the first commercially available microcomputer, except the Apple I had no provision for internal expansion cards. With expansion cards the Altair could attach to a computer terminal and be programmed in BASIC. In contrast, the Apple I was a hobbyist machine. Wozniak's design included a $25 microprocessor (MOS 6502) on a single circuit board with 256 bytes of ROM, 4K or 8K bytes of RAM, and a 40-character by 24-row display controller. Apple's first computer lacked a case, power supply, keyboard, and display, all components the user had to provide.
After the success of the Apple I, Wozniak designed the Apple II, the first personal computer that had the ability to display color graphics, and BASIC programming language built-in. Inspired by "the technique Atari used to simulate colors on its first arcade games", Wozniak found a way of putting colors into the NTSC system by using a $1 chip, while colors in the PAL system were achieved by "accident" when a dot occurred on a line, and to this day he has no idea how it works. During the design stage, Steve Jobs argued that the Apple II should have two expansion slots, while Wozniak wanted eight. After a heated argument, during which Wozniak had threatened for Jobs to 'go get himself another computer', they decided to go with eight slots. The Apple II became one of the first highly successful mass-produced personal computers in the world.

In 1980, Apple went public to instant and significant financial profitability, making Jobs and Wozniak both millionaires. The Apple II's eventual successor, the Apple III, released the same year, was not nearly as successful as the Apple II. According to Wozniak, the Apple III "had 100 percent hardware failures", and that the primary reason for these failures was that the system was designed by Apple's marketing department, unlike Apple's previous engineering-driven projects.

During the early design and development phase of the Macintosh 128K, Wozniak had heavy influence over the project until 1981. In a 2013 interview, Wozniak said that "Steve [Jobs] really took over the project when I had a plane crash and wasn't there."

On February 7, 1981, the Beechcraft Bonanza A36TC Wozniak was piloting crashed soon after takeoff from the Sky Park Airport in Scotts Valley, California. The plane stalled while climbing, then bounced down the runway, went through two fences, and crashed into an embankment. Wozniak and his three passengers—then-fiancée Candice Clark, his brother, and his girlfriend—were injured. Wozniak sustained severe face and head injuries, including losing a tooth, and also suffered for five weeks after the crash from anterograde amnesia, the inability to create new memories. He had no memory of the crash, and did not remember his name in the hospital or the things he did after he was released. He would later state that Apple II computer games are what helped him regain his memory. The National Transportation Safety Board investigation report cited premature liftoff and pilot inexperience as probable causes of the crash.

Wozniak did not immediately return to Apple after recovering from the airplane crash, seeing it as a good reason to leave.

In May 1982 and 1983, Wozniak, with help from professional concert promotor Bill Graham, founded and sponsored two US Festivals to celebrate evolving technologies; they ended up as a technology exposition and a rock festival as a combination of music, computers, television and people. After losing several million dollars on the 1982 festival, he stated that unless the 1983 event turned a profit, he would end his involvement with rock festivals and get back to designing computers. Later that year, Wozniak returned to Apple product development, desiring no more of a role than that of an engineer and a motivational factor for the Apple workforce.

In the mid-1980s he designed the Apple Desktop Bus, a proprietary bit-serial peripheral bus introduced on many later Macintosh and NeXT computer models. However, even with the success he helped create at Apple, Wozniak felt that the company was hindering him from being who he wanted to be, and that it was "the bane of his existence". He enjoyed engineering, not management, and said that he missed "the fun of the early days". Although its products provided about 85% of Apple's sales in early 1985, the company's January 1985 annual meeting did not mention the Apple II division or employees, a move that frustrated Wozniak. As other engineers joined the growing company, he no longer felt needed there and by early 1985, Wozniak left Apple again, stating that the company had "been going in the wrong direction for the last five years". He then sold most of his stock.

After his career at Apple, Wozniak enrolled at UC Berkeley to complete his degree. Because his name was well known at this point, he enrolled under the name Rocky Raccoon Clark, which is the name listed on his diploma.

One thing Wozniak wanted to do was teach elementary school because of the important role teachers play in students' lives. Eventually, he did teach computer classes to children from the fifth through ninth grades and teachers as well.

Wozniak founded CL 9 in 1985, which developed and brought the first programmable universal remote control to market in 1987.

In 2001, Wozniak founded Wheels of Zeus (WOZ), to create wireless GPS technology to "help everyday people find everyday things much more easily." In 2002, he joined the board of directors of Ripcord Networks, Inc., joining Ellen Hancock, Gil Amelio, Mike Connor, and Wheels of Zeus co-founder Alex Fielding, all Apple alumni, in a new telecommunications venture. Later the same year he joined the board of directors of Danger, Inc., the maker of the Hip Top.

In 2006, Wheels of Zeus was closed, and Wozniak founded Acquicor Technology, a holding company for acquiring technology companies and developing them, with Apple alumni Hancock and Amelio.
From 2009 through 2014 he was chief scientist at Fusion-io. In 2014 he became chief scientist at Primary Data, which was founded by some former Fusion-io executives.

Despite leaving Apple as a day-to-day employee in 1985, Wozniak chose to never remove himself from the official employee list, and continues to represent the company at events or in interviews. Today he receives a stipend from Apple for this role, estimated to be $120,000 per year. He is also an Apple shareholder. He maintained a friendly acquaintance with Steve Jobs until Jobs' death in October 2011. However, in 2006, Wozniak stated that he and Jobs were not as close as they used to be. In a 2013 interview, Wozniak said that the original Macintosh "failed" under Steve Jobs, and that it was not until Jobs left that it became a success. He called the Apple Lisa group the team that had kicked Jobs out, and that Jobs liked to call the Lisa group "idiots for making [the Lisa computer] too expensive". To compete with the Lisa, Jobs and his new team produced a cheaper computer, one that, according to Wozniak, was "weak", "lousy" and "still at a fairly high price". "He made it by cutting the RAM down, by forcing you to swap disks here and there", says Wozniak. He attributed the eventual success of the Macintosh to people like John Sculley "who worked to build a Macintosh market when the Apple II went away".

Silicon Valley Comic Con (SVCC) is an annual pop culture and technology convention at the San Jose McEnery Convention Center in San Jose, California. The convention was co-founded by Wozniak and Rick White with Trip Hunter as CEO; the inaugural event was held March 18–20, 2016.

Wozniak is listed as the sole inventor on the following Apple patents:

In 1990, Wozniak helped found the Electronic Frontier Foundation, providing some of the organization's initial funding and serving on its founding Board of Directors. He was the founding sponsor of the Tech Museum, Silicon Valley Ballet and Children's Discovery Museum of San Jose. Also since leaving Apple, Wozniak has provided all the money, as well as a good amount of on-site technical support, for the technology program in his local school district in Los Gatos. Un.U.Son. (Unite Us In Song), an organization Wozniak formed to organize the two US festivals, is now primarily tasked with supporting his educational and philanthropic projects. In 1986, Wozniak lent his name to the Stephen G. Wozniak Achievement Awards (popularly known as "Wozzie Awards"), which he presented to six Bay Area high school and college students for their innovative use of computers in the fields of business, art and music. More recently, Wozniak was the subject of a student-made film production of his friend's (Joe Patane) nonprofit Dream Camp Foundation for high-level need youth titled "Camp Woz: The Admirable Lunacy of Philanthropy".

In 1979, Wozniak was awarded the ACM Grace Murray Hopper Award. In 1985, he received the National Medal of Technology (with Steve Jobs) from US President Ronald Reagan. In December 1989, he received an honorary Doctor of Engineering degree from the University of Colorado at Boulder. Later he donated funds to create the "Woz Lab" at the University of Colorado at Boulder. In 1998, he was named a Fellow of the Computer History Museum "for co-founding Apple Computer and inventing the Apple I personal computer." The city of San Jose named a street "Woz Way" in his honour.

In September 2000, Wozniak was inducted into the National Inventors Hall of Fame, and in 2001 he was awarded the 7th Annual Heinz Award for Technology, the Economy and Employment. The American Humanist Association awarded him the Isaac Asimov Science Award in 2011.
In December 2005, Wozniak was awarded an honorary Doctor of Engineering degree from Kettering University. He also received honorary degrees from North Carolina State University and Nova Southeastern University, and the Telluride Tech Festival Award of Technology. In May 2011, Wozniak received an honorary Doctor of Engineering degree from Michigan State University. In June 2012, Wozniak was awarded an honorary Doctor of Engineering degree from Santa Clara University.

He was awarded the Global Award of the President of Armenia for Outstanding Contribution to Humanity Through IT in 2011.

On February 17, 2014, in Los Angeles, Wozniak was awarded the 66th Hoover Medal from IEEE President & CEO J. Roberto de Marca. The award is presented to an engineer whose professional achievements and personal endeavors have advanced the well-being of humankind and is administered by a board representing five engineering organizations: The American Society of Mechanical Engineers; the American Society of Civil Engineers; the American Institute of Chemical Engineers; the American Institute of Mining, Metallurgical, and Petroleum Engineers; and Institute of Electrical and Electronics Engineers.

The New York City Chapter of Young Presidents' Organization presented their 2014 Lifetime Achievement Award to Wozniak on October 16, 2014 at the American Museum of Natural History.

In November 2014, "Industry Week" added Wozniak to the Manufacturing Hall of Fame.

On June 19, 2015, Wozniak received the Legacy for Children Award from the Children's Discovery Museum of San Jose. The Legacy for Children Award honors an individual whose legacy has significantly benefited the learning and lives of children. The purpose of the Award is to focus Silicon Valley's attention on the needs of our children, encouraging us all to take responsibility for their well-being. Candidates are nominated by a committee of notable community members involved in children's education, health care, human and social services, and the arts. The Children's Discovery Museum of San Jose is at 180 Woz Way.

On June 20, 2015, The Cal Alumni Association (UC Berkeley's Alumni Association) presented Wozniak with the 2015 Alumnus of the Year Award. "We are honored to recognize Steve Wozniak with CAA’s most esteemed award," said CAA President Cynthia So Schroeder '91. "His invaluable contributions to education and to UC Berkeley place him among Cal's most accomplished and respected alumni."

In March 2016, High Point University announced that Wozniak will serve as their Innovator in Residence. Wozniak was High Point University’s commencement speaker in 2013. Through this ongoing partnership, Wozniak will connect with High Point University students on a variety of topics and make campus-visits periodically.

In March 2017, Wozniak was listed by UK-based company Richtopia at number 18 in the list of 200 Most Influential Philanthropists and Social Entrepreneurs.

For his contributions to technology, Wozniak has been awarded a number of Honorary Doctor of Engineering degrees, which include the following:

Wozniak has been mentioned, represented or interviewed many times in media, including the following programs:



After seeing her stand-up performance in Saratoga, California, Wozniak began dating comedian Kathy Griffin. Together, they attended the 2007 Emmy Awards, and subsequently made many appearances on the fourth season of her show "". Wozniak is on the show as her date for the Producers Guild of America award show. However, on a June 19, 2008 appearance on "The Howard Stern Show", Griffin confirmed that they were no longer dating and decided to remain friends.

Wozniak portrays a parody of himself in the first episode of the television series "Code Monkeys"; he plays the owner of Gameavision before selling it to help fund Apple. He later appears again in the twelfth episode when he is in Las Vegas at the annual Video Game Convention and sees Dave and Jerry. He also appears in a parody of the "Get a Mac" ads featured in the final episode of "Code Monkeys" second season. Wozniak is also interviewed and featured in the documentary "Hackers Wanted" and on BBC.

Wozniak competed on Season 8 of "Dancing with the Stars" in 2009 where he danced with Karina Smirnoff. Despite Wozniak and Smirnoff receiving 10 combined points from the three judges out of 30, the lowest score of the evening, he remained in the competition. He later posted on a social networking site that he felt that the vote count was not legitimate and suggested that the "Dancing with the Stars" judges had lied about the vote count to keep him on the show. After being briefed on the method of judging and vote counting, he retracted and apologized for his statements. Despite suffering a pulled hamstring and a fracture in his foot, Wozniak continued to compete, but was eliminated from the competition on March 31, with a score of 12 out of 30 for an Argentine Tango.

On September 30, 2010, he appeared as himself on "The Big Bang Theory" season 4 episode "The Cruciferous Vegetable Amplification". While dining in The Cheesecake Factory where Penny works, he is approached by Sheldon via telepresence on a Texai robot. Leonard tries to explain to Penny who Wozniak is, but she says she already knows him from "Dancing with the Stars".

On September 30, 2013, he appeared along with early Apple employees Daniel Kottke and Andy Hertzfeld on the television show "John Wants Answers" to discuss the movie "Jobs".

In March 2015, Wozniak stated that while he had originally dismissed the writings of Ray Kurzweil who stated machine intelligence will outpace human intelligence within several decades, Wozniak had come to change his mind: "I agree that the future is scary and very bad for people. If we build these devices to take care of everything for us, eventually they'll think faster than us and they'll get rid of the slow humans to run companies more efficiently." Wozniak stated that he had started to feel a contradictory sense of foreboding about artificial intelligence, while still supporting the advance of technology. 

By June 2015, Wozniak changed his mind, stating that a superintelligence takeover would be good for humans: "They're going to be smarter than us and if they're smarter than us then they'll realise they need us... We want to be the family pet and be taken care of all the time... I got this idea a few years ago and so I started feeding my dog filet steak and chicken every night because 'do unto others'". 

In 2016, Wozniak changed his mind again, stating that he no longer worried about the possibility of superintelligence emerging because he is skeptical that computers will be able to compete with human "intuition": "A computer could figure out a logical endpoint decision, but that’s not the way intelligence works in humans". Wozniak added that if computers do become superintelligent, "they're going to be partners of humans over all other species just forever".

Wozniak lives in Los Gatos, California. He applied for Australian citizenship in 2012, and has stated that he would like to live in Melbourne, Australia in the future. Wozniak has been referred to frequently by the nickname "Woz", or "The Woz"; he has also been called "The Wonderful Wizard of Woz" and "The Second Steve" (in regard to his early business partner and longtime friend, Steve Jobs). "WoZ" (short for "Wheels of Zeus") is the name of a company Wozniak founded in 2002.

He is a Freemason, despite not having faith in a supreme being (which is required by Masonic rules except in "Liberal" or Continental Freemasonry). Wozniak describes his impetus for joining the Freemasons as being able to spend more time with his then-wife, Alice Robertson, who belonged to the Order of the Eastern Star, associated with the Masons. Wozniak has said that he quickly rose to a third degree Freemason because, whatever he does, he tries to do well. He was initiated in 1979 at Charity Lodge No. 362 in Campbell, California, now part of Mt. Moriah Lodge No. 292 in Los Gatos.

Wozniak was married to slalom canoe gold-medalist Candice Clark from June 1981 to 1987. They have three children together, the youngest being born after their divorce was finalized.
After a high-profile relationship with actress Kathy Griffin, who described him on "Tom Green's House Tonight" in 2008 as "the biggest techno-nerd in the Universe", Wozniak married Janet Hill, his current spouse.
On his religious views, Wozniak called himself an "atheist or agnostic".

He is a member of a Segway Polo team, the "Silicon Valley Aftershocks".

In 2006, he co-authored with Gina Smith his autobiography, "iWoz". The book made "The New York Times Best Seller list".

Wozniak's favorite video game is "Tetris", and he had a high score for "Sabotage". In the 1990s he submitted so many high scores for Tetris to "Nintendo Power" that they would no longer print his scores, so he started sending them in under the alphabetically reversed name "Evets Kainzow".

Wozniak has the condition prosopagnosia, or face-blindness.





</doc>
<doc id="27850" url="https://en.wikipedia.org/wiki?curid=27850" title="Saxons">
Saxons

The Saxons (, , , , , ) were a Germanic people whose name was given in the early Middle Ages to a large country (Old Saxony, ) near the North Sea coast of what is now Germany. Earlier, in the late Roman Empire, the name was used to refer to Germanic inhabitants of what is now England, and also as a word something like the later "Viking", as a term for raiders and pirates. In Merovingian times, continental Saxons were associated with the coast of what later became Normandy. Though sometimes described as also fighting inland, coming in conflict with the Franks and Thuringians, no clear homeland can be defined. There is possibly a single classical reference to a smaller homeland of an early Saxon tribe, but it is disputed. According to this proposal, the Saxons' earliest area of settlement is believed to have been Northern Albingia. This general area is close to the probable homeland of the Angles. 

In contrast, the British "Saxons", today referred to in English as Anglo-Saxons, became a single nation bringing together Germanic peoples (Frisian, Jutish, Angle) with the Romanized populations, establishing long-lasting post-Roman kingdoms equivalent to those formed by the Franks on the continent. Their earliest weapons and clothing south of the Thames were based on late Roman military fashions, but later immigrants north of the Thames showed a stronger North German influence. The term "Anglo-Saxon" came into use by the 8th century (for example Paul the Deacon) to distinguish English Saxons from continental Saxons (referred to in the "Anglo-Saxon Chronicle" as "Ealdseaxe", "old Saxons"), but the Saxons of Britain and those of Old Saxony (Northern Germany) continued to be referred to as 'Saxons' in an indiscriminate manner, especially in the languages of Britain and Ireland. 

However, while the English Saxons were no longer raiders, the political history of the continental Saxons is unclear until the time of the conflict between their semi-legendary hero Widukind and the Frankish emperor Charlemagne. 

While the continental Saxons are no longer a distinctive ethnic group or country, their name lives on in the names of several regions and states of Germany, including Lower Saxony (which includes the original Saxon homeland known as Old Saxony), as well as the two states that make up Upper Saxony, known today as Saxony-Anhalt and Saxony. The latter have their names from dynastic history, and not their ethnic history.

The Saxons may have derived their name from "seax", a kind of knife for which they were known. The seax has a lasting symbolic impact in the English counties of Essex and Middlesex, both of which feature three seaxes in their ceremonial emblem. Their names, along with those of Sussex and Wessex, contain a remnant of the word "Saxon".

The Elizabethan era play "Edmund Ironside" suggests the Saxon name derives from the Latin "saxa" (stone):

In the Celtic languages, the words designating English nationality derive from the Latin word "Saxones". The most prominent example, a loanword in English, is the Scottish word "Sassenach", used by Scots- or Scottish English-speakers in the 21st century as a jocular term for an English person. The "Oxford English Dictionary" (OED) gives 1771 as the date of the earliest written use of the word in English.

It derives from the Scottish Gaelic "Sasannach" (older spelling: "Sasunnach"). The Gaelic name for England is "Sasann", and "Sasannach" (formed with a common adjective suffix -ach) means "English" in reference to people and things, though not to the English Language, which is "Beurla".

"Sasanach", the Irish word for an Englishman, has the same derivation, as do the words used in Welsh to describe the English people ("Saeson", sing. "Sais") and the language and things English in general: "Saesneg" and "Seisnig".

Cornish terms the English "Sawsnek," from the same derivation. In the 16th century Cornish-speakers used the phrase "Meea navidna cowza sawzneck" to feign ignorance of the English language.

"England" in Scottish Gaelic is "Sasann" (older spelling: "Sasunn", Genitive: "Sasainn"). Other examples include the Welsh "Saesneg" (the English language), Irish "Sasana" (England), Breton "saoz(on)" (English, "saozneg" "the English language", Bro-saoz "England"), and Cornish "Sowson" (English people), "Sowsnek" (English language), and "Pow Sows" for 'Land [Pays] of Saxons'.

The label "Saxons" (in Romanian: "Sași") also became attached to German settlers who migrated during the 13th century to southeastern Transylvania. From Transylvania, some of these Saxons migrated to neighbouring Moldavia, as the name of the town "Sas-cut" shows. Sascut lies in the part of Moldavia that is today part of Romania.

During Georg Friederich Händel's visit to Italy (1706- ), much was made of his origins in Saxony; in particular, the Venetians greeted the 1709 performance of his opera "Agrippina" with the cry "Viva il caro Sassone", "Cheers for the beloved Saxon!"

The Finns and Estonians have changed their usage of the term "Saxony" over the centuries to denote now the whole country of Germany ("Saksa" and "Saksamaa" respectively) and the Germans ("saksalaiset" and "sakslased", respectively). The Finnish word "sakset" scissors reflects the name of the old Saxon single-edged sword Seax from which 'Saxon' is supposedly derived. In Estonian, "saks" means a nobleman or, colloquially, a wealthy or powerful person. As a result of the Northern Crusades in the Middle Ages, Estonia's upper class had been mostly of German origin until well into the 20th century.

The word also survives as the surnames of Saß/Sass (in Low German or Low Saxon), Sachse and Sachs. The Dutch female first name, "Saskia," originally meant "A Saxon woman" (metathesis of "Saxia").

Following the downfall of Henry the Lion (11291195, Duke of Saxony 11421180), and the subsequent splitting of the Saxon tribal duchy into several territories, the name of the Saxon duchy was transferred to the lands of the Ascanian family. This led to the differentiation between "Lower Saxony", lands settled by the Saxon tribe and "Upper Saxony", the lands belonging to the House of Wettin. Gradually, the latter region became known as "Saxony", ultimately usurping the name's original meaning. The area formerly known as Upper Saxony now lies in Central Germany.

Ptolemy's "Geographia," written in the 2nd century, is sometimes considered to contain the first mentioning of the Saxons. Some copies of this text mention a tribe called "Saxones" in the area to the north of the lower Elbe. However, other versions refer to the same tribe as "Axones." This may be a misspelling of the tribe that Tacitus in his "Germania" called "Aviones". According to this theory, "Saxones" was the result of later scribes trying to correct a name that meant nothing to them. On the other hand, Schütte, in his analysis of such problems in "Ptolemy's Maps of Northern Europe", believed that "Saxones" is correct. He notes that the loss of first letters occurs in numerous places in various copies of Ptolemy's work, and also that the manuscripts without "Saxones" are generally inferior overall.

Schütte also remarks that there was a medieval tradition of calling this area "Old Saxony" (covering Westphalia, Angria and Eastphalia). This view is in line with Bede who mentions Old Saxony was near the Rhine, somewhere to the north of the river Lippe (Westphalia, northeastern part of modern German state Nordrhein-Westfalen).

The first undisputed mention of the Saxon name in its modern form is from AD 356, when Julian, later the Roman Emperor, mentioned them in a speech as allies of Magnentius, a rival emperor in Gaul. Zosimus also mentions a specific tribe of Saxons, called the "Kouadoi", which have been interpreted as a misunderstanding for the Chauci, or Chamavi. They entered the Rhineland and displaced the recently settled Salian Franks from Batavi, whereupon some of the Salians began to move into the Belgian territory of Toxandria, supported by Julian.

Both in this case and in others the Saxons were associated with using boats for their raids. In order to defend against Saxon raiders, the Romans created a military district called the "Litus Saxonicum" ("Saxon Coast") on both sides of the English Channel.

In 441–442 AD, Saxons are mentioned for the first time as inhabitants of Britain, when an unknown Gaulish historian wrote: "The British provinces...have been reduced to Saxon rule".

Saxons as inhabitants of present-day Northern Germany are first mentioned in 555, when the Frankish king Theudebald died, and the Saxons used the opportunity for an uprising. The uprising was suppressed by Chlothar I, Theudebald's successor. Some of their Frankish successors fought against the Saxons, others were allied with them. The Thuringians frequently appeared as allies of the Saxons.
The Continental Saxons living in what was known as "Old Saxony" (c. 531-804) appear to have become consolidated by the end of the 8th century. After subjugation by the Emperor Charlemagne, a political entity called the Duchy of Saxony (804-1296) appeared, covering Westphalia, Eastphalia, Angria and Nordalbingia (Holstein, southern part of modern-day Schleswig-Holstein state).

The Saxons long resisted becoming Christians and being incorporated into the orbit of the Frankish kingdom. In 776 the Saxons promised to convert to Christianity and vow loyalty to the king, but, during Charlemagne's campaign in Hispania (778), the Saxons advanced to Deutz on the Rhine and plundered along the river. This was an oft-repeated pattern when Charlemagne was distracted by other matters. 
They were conquered by Charlemagne in a long series of annual campaigns, the Saxon Wars (772804). With defeat came enforced baptism and conversion as well as the union of the Saxons with the rest of the Germanic, Frankish empire. Their sacred tree or pillar, a symbol of Irminsul, was destroyed. Charlemagne also deported 10,000 Nordalbingian Saxons to Neustria and gave their now largely vacant lands in Wagria (approximately modern Plön and Ostholstein districts) to the loyal king of the Abotrites. Einhard, Charlemagne's biographer, says on the closing of this grand conflict:

The war that had lasted so many years was at length ended by their acceding to the terms offered by the king; which were renunciation of their national religious customs and the worship of devils, acceptance of the sacraments of the Christian faith and religion, and union with the Franks to form one people.
Under Carolingian rule, the Saxons were reduced to tributary status. There is evidence that the Saxons, as well as Slavic tributaries such as the Abodrites and the Wends, often provided troops to their Carolingian overlords. The dukes of Saxony became kings (Henry I, the Fowler, 919) and later the first emperors (Henry's son, Otto I, the Great) of Germany during the 10th century, but they lost this position in 1024. The duchy was divided in 1180 when Duke Henry the Lion refused to follow his cousin, Emperor Frederick Barbarossa, into war in Lombardy.

During the High Middle Ages, under the Salian emperors and, later, under the Teutonic Knights, German settlers moved east of the Saale into the area of a western Slavic tribe, the Sorbs. The Sorbs were gradually Germanised. This region subsequently acquired the name Saxony through political circumstances, though it was initially called the March of Meissen. The rulers of Meissen acquired control of the Duchy of Saxony (only a remnant of the previous Duchy) in 1423; they eventually applied the name "Saxony" to the whole of their kingdom. Since then, this part of eastern Germany has been referred to as Saxony (German: "Sachsen"), a source of some misunderstanding about the original homeland of the Saxons, with a central part in the present-day German state of Lower Saxony (German: "Niedersachsen").

In the Netherlands, Saxons occupied the territory south of the Frisians and north of the Franks. In the west it reached as far as the Gooi region, in the south as far as the Lower Rhine. After the conquest of Charlemagne, this area formed the main part of the Bishopric of Utrecht. The Saxon duchy of Hamaland played an important role in the formation of the duchy of Guelders.

The local language, although strongly influenced by standard Dutch, is still officially recognised as Dutch Low Saxon.

In 569, some Saxons accompanied the Lombards into Italy under the leadership of Alboin and settled there. In 572, they raided southeastern Gaul as far as "Stablo", now Estoublon. Divided, they were easily defeated by the Gallo-Roman general Mummolus. When the Saxons regrouped, a peace treaty was negotiated whereby the Italian Saxons were allowed to settle with their families in Austrasia. Gathering their families and belongings in Italy, they returned to Provence in two groups in 573. One group proceeded by way of Nice and another via Embrun, joining up at Avignon. They plundered the territory and were as a consequence stopped from crossing the Rhône by Mummolus. They were forced to pay compensation for what they had robbed before they could enter Austrasia. These people are known only by documents, and their settlement cannot be compared to the archeological artifacts and remains that attest to Saxon settlements in northern and western Gaul.

A Saxon king named Eadwacer conquered Angers in 463 only to be dislodged by Childeric I and the Salian Franks, allies of the Roman Empire. It is possible that Saxon settlement of Great Britain began only in response to expanding Frankish control of the Channel coast.

Some Saxons already lived along the Saxon shore of Gaul as Roman foederati. They can be traced in documents, but also in archeology and in toponymy. The "Notitia Dignitatum" mentions the "Tribunus cohortis primae novae Armoricanae, Grannona in litore Saxonico". The location of "Grannona" is uncertain and was identified by the historians and toponymists at different places: mainly with the town known today as Granville (in Normandy) or nearby. The "Notitia Dignitatum" does not explain where these "Roman" soldiers came from. Some toponymists have proposed Graignes ("Grania" 1109–1113) as the location for "Grannona"/"Grannonum". Although some scholars believe it could be the same element "*gran", that is recognised in Guernsey ("Greneroi" 11th century), it most likely derives from the Gaulish god Grannos. This location is closer to Bayeux, where Gregory of Tours evokes otherwise the "Saxones Bajocassini" (Bessin Saxons), which were ineffective to defeat the Breton Waroch II in 579.

A Saxon unit of "laeti" settled at Bayeuxthe "Saxones Baiocassenses". These Saxons became subjects of Clovis I late in the 5th century. The Saxons of Bayeux comprised a standing army and were often called upon to serve alongside the local levy of their region in Merovingian military campaigns. They were ineffective against the Breton Waroch in this capacity in 579. In 589, the Saxons wore their hair in the Breton fashion at the orders of Fredegund and fought with them as allies against Guntram. Beginning in 626, the Saxons of the Bessin were used by Dagobert I for his campaigns against the Basques. One of their own, Aeghyna, was created a "dux" over the region of Vasconia.

In 843 and 846 under king Charles the Bald, other official documents mention a "pagus" called "Otlinga Saxonia" in the Bessin region, but the meaning of "Otlinga" is unclear. Different Bessin toponyms were identified as typically Saxon, ex : Cottun ("Coltun" 1035–1037 ; "Cola" 's "town"). It is the only place name in Normandy that can be interpreted as a "-tun" one (English "-ton"; cf. Colton). In contrast to this one example in Normandy are numerous "-thun" villages in the north of France, in Boulonnais, for example Alincthun, Verlincthun, and Pelingthun. showing with other toponyms, an important Saxon or Anglo-Saxon settlement. comparing the concentration of "-ham"/"-hem" (Anglo-Saxon "hām" > home) toponyms in the Bessin and in the Boulonnais gives more examples of Saxon settlement. In the area known today as Normandy, the "-ham" cases of Bessin are uniquethey do not exist elsewhere. Other cases were considered, but there is no determining example. For example, Canehan ("Kenehan" 1030/"Canaan" 1030–1035) could be the biblical name "Canaan" or Airan ("Heidram" 9thcentury), the Germanic masculine name "Hairammus".

The Bessin examples are clear; for example, Ouistreham ("Oistreham" 1086), Étréham ("Oesterham" 1350 ?), Huppain ("*Hubbehain" ; "Hubba" 's "home"), and Surrain ("Surrehain" 11thcentury). Another significant example can be found in the Norman onomastics: the widespread surname Lecesne, with variant spellings: LeCesne, Lesène, Lecène, and Cesne. It comes from Gallo-Romance "the Saxon", which is "saisne" in Old French. These examples are not derived from more recent Anglo-Scandinavian toponyms, because in that case they would have been numerous in the Norman regions (pays deCaux, Basse-Seine, North-Cotentin) settled by Germanic peoples. That is not the case, nor does Bessin belong to the "pagii," which were affected by an important wave of Anglo-Scandinavian immigration.

In addition, archaeological finds add evidence to the documents and the results of toponymic research. Around the city of Caen and in the Bessin (Vierville-sur-Mer, Bénouville, Giverville, Hérouvillette), excavations have yielded numerous examples of Anglo-Saxon jewellery, design elements, settings, and weapons. All of these things were discovered in cemeteries in a context of the 5th, 6th and 7thcenturies AD.

The oldest and most spectacular Saxon site found in France to date is Vron, in Picardy. There, archaeologists excavated a large cemetery with tombs dating from the Roman Empire until the 6thcentury. Furniture and other grave goods, as well as the human remains, revealed a group of people buried in the 4th and 5thcenturies AD. Physically different from the usual local inhabitants found before this period, they instead resembled the Germanic populations of the north. At the beginning (4thcentury), 92% were buried, sometimes with typical Germanic weapons. Then they were ranked to the east, when they were buried in the 5th and later to the beginning of the 6th century. A strong Anglo-Saxon influence became obvious for the middle of the period, but this influence later disappeared. Archaeological material, neighbouring toponymy, and texts support the same conclusion: settlement of Saxon foederati with their families. Further anthropological research by Joël Blondiaux shows these people were from Low Saxony.

Saxons, along with Angles, Frisians and Jutes, invaded or migrated to the island of Great Britain (Britannia) around the time of the collapse of the Western Roman Empire. Saxon raiders had been harassing the eastern and southern shores of Britannia for centuries before, prompting the construction of a string of coastal forts called the "Litora Saxonica" or Saxon Shore. Before the end of Roman rule in Britannia, many Saxons and other folk had been permitted to settle in these areas as farmers.

According to tradition, the Saxons (and other tribes) first entered Britain en masse as part of an agreement to protect the Britons from the incursions of the Picts, Gaels and others. The story, as reported in such sources as the "Historia Brittonum" and Gildas, indicates that the British king Vortigern allowed the Germanic warlords, later named as Hengist and Horsa by Bede, to settle their people on the Isle of Thanet in exchange for their service as mercenaries. According to Bede, Hengist manipulated Vortigern into granting more land and allowing for more settlers to come in, paving the way for the Germanic settlement of Britain.

Historians are divided about what followed: some argue that the takeover of southern Great Britain by the Anglo-Saxons was peaceful. The known account from a native Briton who lived in the mid-5th century AD, Gildas, described events as a forced takeover by armed attack:

Gildas described how the Saxons were later slaughtered at the battle of Mons Badonicus 44 years before he wrote his history, and their conquest of Britain halted. The 8th-century English historian Bede tells how their advance resumed thereafter. He said this resulted in a swift overrunning of the entirety of South-Eastern Britain, and the foundation of the Anglo-Saxon kingdoms.

Four separate Saxon realms emerged:

During the period of the reigns from Egbert to Alfred the Great, the kings of Wessex emerged as Bretwalda, unifying the country. They eventually organised it as the kingdom of England in the face of Viking invasions.

Bede, a Northumbrian writing around the year 730, remarks that "the old (that is, the continental) Saxons have no king, but they are governed by several ealdormen (or "satrapa") who, during war, cast lots for leadership but who, in time of peace, are equal in power." The "regnum Saxonum" was divided into three provinces – Westphalia, Eastphalia and Angria – which comprised about one hundred "pagi" or "Gaue". Each "Gau" had its own satrap with enough military power to level whole villages that opposed him.

In the mid-9th century, Nithard first described the social structure of the Saxons beneath their leaders. The caste structure was rigid; in the Saxon language the three castes, excluding slaves, were called the "edhilingui" (related to the term aetheling), "frilingi" and "lazzi". These terms were subsequently Latinised as "nobiles" or "nobiliores"; "ingenui", "ingenuiles" or "liberi"; and "liberti", "liti" or "serviles". According to very early traditions that are presumed to contain a good deal of historical truth, the "edhilingui" were the descendants of the Saxons who led the tribe out of Holstein and during the migrations of the 6th century. They were a conquering warrior elite. The "frilingi" represented the descendants of the "amicii", "auxiliarii" and "manumissi" of that caste. The "lazzi" represented the descendants of the original inhabitants of the conquered territories, who were forced to make oaths of submission and pay tribute to the "edhilingui".

The "Lex Saxonum" regulated the Saxons' unusual society. Intermarriage between the castes was forbidden by the "Lex," and wergilds were set based upon caste membership. The "edhilingui" were worth 1,440 solidi, or about 700 head of cattle, the highest wergild on the continent; the price of a bride was also very high. This was six times as much as that of the "frilingi" and eight times as much as the "lazzi". The gulf between noble and ignoble was very large, but the difference between a freeman and an indentured labourer was small.

According to the "Vita Lebuini antiqua", an important source for early Saxon history, the Saxons held an annual council at Marklo (Westphalia) where they "confirmed their laws, gave judgment on outstanding cases, and determined by common counsel whether they would go to war or be in peace that year." All three castes participated in the general council; twelve representatives from each caste were sent from each "Gau". In 782, Charlemagne abolished the system of "Gaue" and replaced it with the "Grafschaftsverfassung", the system of counties typical of Francia. By prohibiting the Marklo councils, Charlemagne pushed the "frilingi" and "lazzi" out of political power. The old Saxon system of "Abgabengrundherrschaft", lordship based on dues and taxes, was replaced by a form of feudalism based on service and labour, personal relationships and oaths.

Saxon religious practices were closely related to their political practices. The annual councils of the entire tribe began with invocations of the gods. The procedure by which dukes were elected in wartime, by drawing lots, is presumed to have had religious significance, i.e. in giving trust to divine providenceit seemsto guide the random decision making. There were also sacred rituals and objects, such as the pillars called Irminsul; these were believed to connect heaven and earth, as with other examples of trees or ladders to heaven in numerous religions. Charlemagne had one such pillar chopped down in 772 close to the Eresburg stronghold.

Early Saxon religious practices in Britain can be gleaned from place names and the Germanic calendar in use at that time. The Germanic gods Woden, Frigg, Tiw and Thunor, who are attested to in every Germanic tradition, were worshipped in Wessex, Sussex and Essex. They are the only ones directly attested to, though the names of the third and fourth months (March and April) of the Old English calendar bear the names "Hrethmonath" and "Eosturmonath", meaning "month of Hretha" and "month of Ēostre." It is presumed that these are the names of two goddesses who were worshipped around that season. The Saxons offered cakes to their gods in February ("Solmonath"). There was a religious festival associated with the harvest, "Halegmonath" ("holy month" or "month of offerings", September). The Saxon calendar began on 25 December, and the months of December and January were called Yule (or "Giuli"). They contained a "Modra niht" or "night of the mothers", another religious festival of unknown content.

The Saxon freemen and servile class remained faithful to their original beliefs long after their nominal conversion to Christianity. Nursing a hatred of the upper class, which, with Frankish assistance, had marginalised them from political power, the lower classes (the "plebeium vulgus" or "cives") were a problem for Christian authorities as late as 836. The "Translatio S.Liborii" remarks on their obstinacy in pagan "ritus et superstitio" (usage and superstition).

The conversion of the Saxons in England from their original Germanic religion to Christianity occurred in the early to late 7th century under the influence of the already converted Jutes of Kent. In the 630s, Birinus became the "apostle to the West Saxons" and converted Wessex, whose first Christian king was Cynegils. The West Saxons begin to emerge from obscurity only with their conversion to Christianity and keeping written records. The Gewisse, a West Saxon people, were especially resistant to Christianity; Birinus exercised more efforts against them and ultimately succeeded in conversion. In Wessex, a bishopric was founded at Dorchester. The South Saxons were first evangelised extensively under Anglian influence; Aethelwalh of Sussex was converted by Wulfhere, King of Mercia and allowed Wilfrid, Bishop of York, to evangelise his people beginning in 681. The chief South Saxon bishopric was that of Selsey. The East Saxons were more pagan than the southern or western Saxons; their territory had a superabundance of pagan sites. Their king, Saeberht, was converted early and a diocese was established at London. Its first bishop, Mellitus, was expelled by Saeberht's heirs. The conversion of the East Saxons was completed under Cedd in the 650s and 660s.

The continental Saxons were evangelised largely by English missionaries in the late 7th and early 8th centuries. Around 695, two early English missionaries, Hewald the White and Hewald the Black, were martyred by the "vicani", that is, villagers. Throughout the century that followed, villagers and other peasants proved to be the greatest opponents of Christianisation, while missionaries often received the support of the "edhilingui" and other noblemen. Saint Lebuin, an Englishman who between 745 and 770 preached to the Saxons, mainly in the eastern Netherlands, built a church and made many friends among the nobility. Some of them rallied to save him from an angry mob at the annual council at Marklo (near river Weser, Bremen). Social tensions arose between the Christianity-sympathetic noblemen and the pagan lower castes, who were staunchly faithful to their traditional religion.

Under Charlemagne, the Saxon Wars had as their chief object the conversion and integration of the Saxons into the Frankish empire. Though much of the highest caste converted readily, forced baptisms and forced tithing made enemies of the lower orders. Even some contemporaries found the methods employed to win over the Saxons wanting, as this excerpt from a letter of Alcuin of York to his friend Meginfrid, written in 796, shows:

If the light yoke and sweet burden of Christ were to be preached to the most obstinate people of the Saxons with as much determination as the payment of tithes has been exacted, or as the force of the legal decree has been applied for fault of the most trifling sort imaginable, perhaps they would not be averse to their baptismal vows.
Charlemagne's successor, Louis the Pious, reportedly treated the Saxons more as Alcuin would have wished, and as a consequence they were faithful subjects. The lower classes, however, revolted against Frankish overlordship in favour of their old paganism as late as the 840s, when the "Stellinga" rose up against the Saxon leadership, who were allied with the Frankish emperor Lothair I. After the suppression of the "Stellinga", in 851 Louis the German brought relics from Rome to Saxony to foster a devotion to the Roman Catholic Church. The Poeta Saxo, in his verse "Annales" of Charlemagne's reign (written between 888 and 891), laid an emphasis on his conquest of Saxony. He celebrated the Frankish monarch as on par with the Roman emperors and as the bringer of Christian salvation to people. References are made to periodic outbreaks of pagan worship, especially of Freya, among the Saxon peasantry as late as the 12th century.

In the 9th century, the Saxon nobility became vigorous supporters of monasticism and formed a bulwark of Christianity against the existing Slavic paganism to the east and the Nordic paganism of the Vikings to the north. Much Christian literature was produced in the vernacular Old Saxon, the notable ones being a result of the literary output and wide influence of Saxon monasteries such as Fulda, Corvey and Verden; and the theological controversy between the Augustinian Gottschalk and Rabanus Maurus.

From an early date, Charlemagne and Louis the Pious supported Christian vernacular works in order to evangelise the Saxons more efficiently. The "Heliand", a verse epic of the life of Christ in a Germanic setting, and "Genesis", another epic retelling of the events of the first book of the Bible, were commissioned in the early 9th century by Louis to disseminate scriptural knowledge to the masses. A council of Tours in 813 and then a synod of Mainz in 848 both declared that homilies ought to be preached in the vernacular. The earliest preserved text in the Saxon language is a baptismal vow from the late 8th or early 9thcentury; the vernacular was used extensively in an effort to Christianise the lowest castes of Saxon society.





</doc>
<doc id="27852" url="https://en.wikipedia.org/wiki?curid=27852" title="Superparamagnetism">
Superparamagnetism

Superparamagnetism is a form of magnetism which appears in small ferromagnetic or ferrimagnetic nanoparticles. In sufficiently small nanoparticles, magnetization can randomly flip direction under the influence of temperature. The typical time between two flips is called the Néel relaxation time. In the absence of an external magnetic field, when the time used to measure the magnetization of the nanoparticles is much longer than the Néel relaxation time, their magnetization appears to be in average zero; they are said to be in the superparamagnetic state. In this state, an external magnetic field is able to magnetize the nanoparticles, similarly to a paramagnet. However, their magnetic susceptibility is much larger than that of paramagnets.

Normally, any ferromagnetic or ferrimagnetic material undergoes a transition to a paramagnetic state above its Curie temperature. Superparamagnetism is different from this standard transition since it occurs below the Curie temperature of the material.

Superparamagnetism occurs in nanoparticles which are single-domain, i.e. composed of a single magnetic domain. This is possible when their diameter is below 3–50 nm, depending on the materials. In this condition, it is considered that the magnetization of the nanoparticles is a single giant magnetic moment, sum of all the individual magnetic moments carried by the atoms of the nanoparticle. Those in the field of superparamagnetism call this “macro-spin approximation”.

Because of the nanoparticle’s magnetic anisotropy, the magnetic moment has usually only two stable orientations antiparallel to each other, separated by an energy barrier. The stable orientations define the nanoparticle’s so called “easy axis”. At finite temperature, there is a finite probability for the magnetization to flip and reverse its direction. The mean time between two flips is called the Néel relaxation time formula_1 and is given by the following Néel-Arrhenius equation:

where:

This length of time can be anywhere from a few nanoseconds to years or much longer. In particular, it can be seen that the Néel relaxation time is an exponential function of the grain volume, which explains why the flipping probability becomes rapidly negligible for bulk materials or large nanoparticles.

Let us imagine that the magnetization of a single superparamagnetic nanoparticle is measured and let us define formula_5 as the measurement time. If formula_6, the nanoparticle magnetization will flip several times during the measurement, then the measured magnetization will average to zero. If formula_7, the magnetization will not flip during the measurement, so the measured magnetization will be what the instantaneous magnetization was at the beginning of the measurement. In the former case, the nanoparticle will appear to be in the superparamagnetic state whereas in the latter case it will appear to be “blocked” in its initial state. The state of the nanoparticle (superparamagnetic or blocked) depends on the measurement time. A transition between superparamagnetism and blocked state occurs when formula_8. In several experiments, the measurement time is kept constant but the temperature is varied, so the transition between superparamagnetism and blocked state is seen as a function of the temperature. The temperature for which formula_8 is called the blocking temperature:

For typical laboratory measurements, the value of the logarithm in the previous equation is in the order of 20–25.

When an external magnetic field is applied to an assembly of superparamagnetic nanoparticles, their magnetic moments tend to align along the applied field, leading to a net magnetization. The magnetization curve of the assembly, i.e. the magnetization as a function of the applied field, is a reversible S-shaped increasing function. This function is quite complicated but for some simple cases:
In the above equations:

The initial slope of the formula_16 function is the magnetic susceptibility of the sample formula_17:
The later susceptibility is also valid for all temperatures formula_20 if the easy axes of the nanoparticles are randomly oriented.

It can be seen from these equations that large nanoparticles have a larger "µ" and so a larger susceptibility. This explains why superparamagnetic nanoparticles have a much larger susceptibility than standard paramagnets: they behave exactly as a paramagnet with a huge magnetic moment.

There is no time-dependence of the magnetization when the nanoparticles are either completely blocked (formula_21) or completely superparamagnetic (formula_22). There is, however, a narrow window around formula_23 where the measurement time and the relaxation time have comparable magnitude. In this case, a frequency-dependence of the susceptibility can be observed. For a randomly oriented sample, the complex susceptibility is:

where

From this frequency-dependent susceptibility, the time-dependence of the magnetization for low-fields can be derived:

A superparamagnetic system can be measured with AC susceptibility measurements, where an applied magnetic field varies in time, and the magnetic response of the system is measured. A superparamagnetic system will show a characteristic frequency dependence: When the frequency is much higher than 1/τ, there will be a different magnetic response than when the frequency is much lower than 1/τ, since in the latter case, but not the former, the ferromagnetic clusters will have time to respond to the field by flipping their magnetization. The precise dependence can be calculated from the Néel-Arrhenius equation, assuming that the neighboring clusters behave independently of one another (if clusters interact, their behavior becomes more complicated). It is also possible to perform magneto-optical AC susceptibility measurements with magneto-optically active superparamagnetic materials such as iron oxide nanoparticles in the visible wavelength range.

Superparamagnetism sets a limit on the storage density of hard disk drives due to the minimum size of particles that can be used. This limit is known as the superparamagnetic limit.










</doc>
<doc id="27854" url="https://en.wikipedia.org/wiki?curid=27854" title="Separability">
Separability

Separability may refer to:




</doc>
<doc id="27855" url="https://en.wikipedia.org/wiki?curid=27855" title="Separable space">
Separable space

In mathematics, a topological space is called separable if it contains a countable, dense subset; that is, there exists a sequence formula_1 of elements of the space such that every nonempty open subset of the space contains at least one element of the sequence.

Like the other axioms of countability, separability is a "limitation on size", not necessarily in terms of cardinality (though, in the presence of the Hausdorff axiom, this does turn out to be the case; see below) but in a more subtle topological sense. In particular, every continuous function on a separable space whose image is a subset of a Hausdorff space is determined by its values on the countable dense subset.

Contrast separability with the related notion of second countability, which is in general stronger but equivalent on the class of metrizable spaces.

Any topological space which is itself finite or countably infinite is separable, for the whole space is a countable dense subset of itself. An important example of an uncountable separable space is the real line, in which the rational numbers form a countable dense subset. Similarly the set of all vectors formula_2 in which formula_3 is rational for all "i" is a countable dense subset of formula_4; so for every formula_5 the formula_5-dimensional Euclidean space is separable.

A simple example of a space which is not separable is a discrete space of uncountable cardinality.

Further examples are given below.

Any second-countable space is separable: if formula_7 is a countable base, choosing any formula_8 from the non-empty formula_9 gives a countable dense subset. Conversely, a metrizable space is separable if and only if it is second countable, which is the case if and only if it is Lindelöf.

To further compare these two properties:

The property of separability does not in and of itself give any limitations on the cardinality of a topological space: any set endowed with the trivial topology is separable, as well as second countable, quasi-compact, and connected. The "trouble" with the trivial topology is its poor separation properties: its Kolmogorov quotient is the one-point space.

A first-countable, separable Hausdorff space (in particular, a separable metric space) has at most the continuum cardinality formula_10. In such a space, closure is determined by limits of sequences and any convergent sequence has at most one limit, so there is a surjective map from the set of convergent sequences with values in the countable dense subset to the points of formula_11.

A separable Hausdorff space has cardinality at most formula_12, where formula_10 is the cardinality of the continuum. For this closure is characterized in terms of limits of filter bases: if formula_14 and formula_15, then formula_16 if and only if there exists a filter base formula_17 consisting of subsets of formula_18 which converges to formula_19. The cardinality of the set formula_20 of such filter bases is at most formula_21. Moreover, in a Hausdorff space, there is at most one limit to every filter base. Therefore, there is a surjection formula_22 when formula_23

The same arguments establish a more general result: suppose that a Hausdorff topological space formula_11 contains a dense subset of cardinality formula_25.
Then formula_11 has cardinality at most formula_27 and cardinality at most formula_28 if it is first countable.

The product of at most continuum many separable spaces is a separable space . In particular the space formula_29 of all functions from the real line to itself, endowed with the product topology, is a separable Hausdorff space of cardinality formula_12. More generally, if formula_25 is any infinite cardinal, then a product of at most formula_32 spaces with dense subsets of size at most formula_25 has itself a dense subset of size at most formula_25 (Hewitt–Marczewski–Pondiczery theorem).

Separability is especially important in numerical analysis and constructive mathematics, since many theorems that can be proved for nonseparable spaces have constructive proofs only for separable spaces. Such constructive proofs can be turned into algorithms for use in numerical analysis, and they are the only sorts of proofs acceptable in constructive analysis. A famous example of a theorem of this sort is the Hahn–Banach theorem.




"For nonseparable spaces":



</doc>
<doc id="27856" url="https://en.wikipedia.org/wiki?curid=27856" title="Schrödinger's cat">
Schrödinger's cat

Schrödinger's cat is a thought experiment, sometimes described as a paradox, devised by Austrian physicist Erwin Schrödinger in 1935. It illustrates what he saw as the problem of the Copenhagen interpretation of quantum mechanics applied to everyday objects. The scenario presents a cat that may be simultaneously both alive and dead, a state known as a quantum superposition, as a result of being linked to a random subatomic event that may or may not occur. The thought experiment is also often featured in theoretical discussions of the interpretations of quantum mechanics. Schrödinger coined the term "Verschränkung" (entanglement) in the course of developing the thought experiment.

Schrödinger intended his thought experiment as a discussion of the EPR article—named after its authors Einstein, Podolsky, and Rosen—in 1935. The EPR article highlighted the bizarre nature of quantum superpositions, in which a quantum system such as an atom or photon can exist as a combination of multiple states corresponding to different possible outcomes. The prevailing theory, called the Copenhagen interpretation, said that a quantum system remains in superposition until it interacts with, or it’s observed by the external world. When this happens, the superposition collapses into one or another of the possible definite states. The EPR experiment showed that a system with multiple particles separated by large distances could be in such a superposition. Schrödinger and Einstein exchanged letters about Einstein's EPR article, in the course of which Einstein pointed out that the state of an unstable keg of gunpowder will, after a while, contain a superposition of both exploded and unexploded states.

To further illustrate, Schrödinger described how one could, in principle, create a superposition in a large-scale system by making it dependent on a quantum particle that was in a superposition. He proposed a scenario with a cat in a locked steel chamber, wherein the cat's life or death depended on the state of a radioactive atom, whether it had decayed and emitted radiation or not. According to Schrödinger, the Copenhagen interpretation implies that "the cat remains both alive and dead" until the state has been observed. Schrödinger did not wish to promote the idea of dead-and-alive cats as a serious possibility; on the contrary, he intended the example to illustrate the absurdity of the existing view of quantum mechanics. However, since Schrödinger's time, other interpretations of the mathematics of quantum mechanics have been advanced by physicists, some of which regard the "alive and dead" cat superposition as quite real. Intended as a critique of the Copenhagen interpretation (the prevailing orthodoxy in 1935), the Schrödinger's cat thought experiment remains a defining touchstone for modern interpretations of quantum mechanics. Physicists often use the way each interpretation deals with Schrödinger's cat as a way of illustrating and comparing the particular features, strengths, and weaknesses of each interpretation.

Schrödinger wrote:
Schrödinger's famous thought experiment poses the question, ""when" does a quantum system stop existing as a superposition of states and become one or the other?" (More technically, when does the actual quantum state stop being a linear combination of states, each of which resembles different classical states, and instead begin to have a unique classical description?) If the cat survives, it remembers only being alive. But explanations of the EPR experiments that are consistent with standard microscopic quantum mechanics require that macroscopic objects, such as cats and notebooks, do not always have unique classical descriptions. The thought experiment illustrates this apparent paradox. Our intuition says that no observer can be in a mixture of states—yet the cat, it seems from the thought experiment, can be such a mixture. Is the cat required to be an observer, or does its existence in a single well-defined classical state require another external observer? Each alternative seemed absurd to Einstein, who was impressed by the ability of the thought experiment to highlight these issues. In a letter to Schrödinger dated 1950, he wrote:

Note that the charge of gunpowder is not mentioned in Schrödinger's setup, which uses a Geiger counter as an amplifier and hydrocyanic poison instead of gunpowder. The gunpowder had been mentioned in Einstein's original suggestion to Schrödinger 15 years before, and Einstein carried it forward to the present discussion.

Since Schrödinger's time, other interpretations of quantum mechanics have been proposed that give different answers to the questions posed by Schrödinger's cat of how long superpositions last and when (or "whether") they collapse.

A commonly held interpretation of quantum mechanics is the Copenhagen interpretation. In the Copenhagen interpretation, a system stops being a superposition of states and becomes either one or the other when an observation takes place. This thought experiment makes apparent the fact that the nature of measurement, or observation, is not well-defined in this interpretation. The experiment can be interpreted to mean that while the box is closed, the system simultaneously exists in a superposition of the states "decayed nucleus/dead cat" and "undecayed nucleus/living cat", and that only when the box is opened and an observation performed does the wave function collapse into one of the two states.

However, one of the main scientists associated with the Copenhagen interpretation, Niels Bohr, never had in mind the observer-induced collapse of the wave function, so that Schrödinger's cat did not pose any riddle to him. The cat would be either dead or alive long before the box is opened by a conscious observer. Analysis of an actual experiment found that measurement alone (for example by a Geiger counter) is sufficient to collapse a quantum wave function before there is any conscious observation of the measurement, although the validity of their design is disputed. The view that the "observation" is taken when a particle from the nucleus hits the detector can be developed into objective collapse theories. The thought experiment requires an "unconscious observation" by the detector in order for waveform collapse to occur. In contrast, the many worlds approach denies that collapse ever occurs.

In 1957, Hugh Everett formulated the many-worlds interpretation of quantum mechanics, which does not single out observation as a special process. In the many-worlds interpretation, both alive and dead states of the cat persist after the box is opened, but are decoherent from each other. In other words, when the box is opened, the observer and the possibly-dead cat split into an observer looking at a box with a dead cat, and an observer looking at a box with a live cat. But since the dead and alive states are decoherent, there is no effective communication or interaction between them.

When opening the box, the observer becomes entangled with the cat, so "observer states" corresponding to the cat's being alive and dead are formed; each observer state is entangled or linked with the cat so that the "observation of the cat's state" and the "cat's state" correspond with each other. Quantum decoherence ensures that the different outcomes have no interaction with each other. The same mechanism of quantum decoherence is also important for the interpretation in terms of consistent histories. Only the "dead cat" or the "alive cat" can be a part of a consistent history in this interpretation.

Roger Penrose criticises this:

A variant of the Schrödinger's cat experiment, known as the quantum suicide machine, has been proposed by cosmologist Max Tegmark. It examines the Schrödinger's cat experiment from the point of view of the cat, and argues that by using this approach, one may be able to distinguish between the Copenhagen interpretation and many-worlds.

The ensemble interpretation states that superpositions are nothing but subensembles of a larger statistical ensemble. The state vector would not apply to individual cat experiments, but only to the statistics of many similarly prepared cat experiments. Proponents of this interpretation state that this makes the Schrödinger's cat paradox a trivial matter, or a non-issue.

This interpretation serves to "discard" the idea that a single physical system in quantum mechanics has a mathematical description that corresponds to it in any way.

The relational interpretation makes no fundamental distinction between the human experimenter, the cat, or the apparatus, or between animate and inanimate systems; all are quantum systems governed by the same rules of wavefunction evolution, and all may be considered "observers". But the relational interpretation allows that different observers can give different accounts of the same series of events, depending on the information they have about the system. The cat can be considered an observer of the apparatus; meanwhile, the experimenter can be considered another observer of the system in the box (the cat plus the apparatus). Before the box is opened, the cat, by nature of its being alive or dead, has information about the state of the apparatus (the atom has either decayed or not decayed); but the experimenter does not have information about the state of the box contents. In this way, the two observers simultaneously have different accounts of the situation: To the cat, the wavefunction of the apparatus has appeared to "collapse"; to the experimenter, the contents of the box appear to be in superposition. Not until the box is opened, and both observers have the same information about what happened, do both system states appear to "collapse" into the same definite result, a cat that is either alive or dead.

In the transactional interpretation the apparatus emits an advanced wave backward in time, which combined with the wave that the source emits forward in time, forms a standing wave. The waves are seen as physically real, and the apparatus is considered an "observer". In the transactional interpretation, the collapse of the wavefunction is "atemporal" and occurs along the whole transaction between the source and the apparatus. The cat is never in superposition. Rather the cat is only in one state at any particular time, regardless of when the human experimenter looks in the box. The transactional interpretation resolves this quantum paradox.

The Zeno effect is known to cause delays to any changes from the initial state.

On the other hand, the anti-Zeno effect accelerates the changes. For example, if you peek a look into the cat box frequently you may either cause delays to the fateful choice or, conversely, accelerate it. Both the Zeno effect and the anti-Zeno effect are real and known to happen to real atoms. The quantum system being measured must be strongly coupled to the surrounding environment (in this case to the apparatus, the experiment room ... etc.) in order to obtain more accurate information. But while there is no information passed to the outside world, it is considered to be a "quasi-measurement", but as soon as the information about the cat's well-being is passed on to the outside world (by peeking into the box) quasi-measurement turns into measurement. Quasi-measurements, like measurements, cause the Zeno effects. Zeno effects teach us that even without peeking into the box, the death of the cat would have been delayed or accelerated anyway due to its environment.

According to objective collapse theories, superpositions are destroyed spontaneously (irrespective of external observation), when some objective physical threshold (of time, mass, temperature, irreversibility, etc.) is reached. Thus, the cat would be expected to have settled into a definite state long before the box is opened. This could loosely be phrased as "the cat observes itself", or "the environment observes the cat".

Objective collapse theories require a modification of standard quantum mechanics to allow superpositions to be destroyed by the process of time evolution.

The experiment as described is a purely theoretical one, and the machine proposed is not known to have been constructed. However, successful experiments involving similar principles, e.g. superpositions of relatively large (by the standards of quantum physics) objects have been performed. These experiments do not show that a cat-sized object can be superposed, but the known upper limit on "cat states" has been pushed upwards by them. In many cases the state is short-lived, even when cooled to near absolute zero.

In quantum computing the phrase "cat state" often refers to the special entanglement of qubits wherein the qubits are in an equal superposition of all being 0 and all being 1; e.g.,

Wigner's friend is a variant on the experiment with two human observers: the first makes an observation on whether a flash of light is seen and then communicates his observation to a second observer. The issue here is, does the wave function "collapse" when the first observer looks at the experiment, or only when the second observer is informed of the first observer's observations?

In another extension, prominent physicists have gone so far as to suggest that astronomers observing dark energy in the universe in 1998 may have "reduced its life expectancy" through a pseudo-Schrödinger's cat scenario, although this is a controversial viewpoint.



</doc>
<doc id="27859" url="https://en.wikipedia.org/wiki?curid=27859" title="Sphere">
Sphere

A sphere (from Greek σφαῖρα — "sphaira", "globe, ball") is a perfectly round geometrical object in three-dimensional space that is the surface of a completely round ball (viz., analogous to the circular objects in two dimensions, where a "circle" circumscribes its "disk").

Like a circle in a two-dimensional space, a sphere is defined mathematically as the set of points that are all at the same distance from a given point, but in a three-dimensional space. This distance is the radius of the ball, which is made up from all points with a distance less than from the given point, which is the center of the mathematical ball. These are also referred to as the radius and center of the sphere, respectively. The longest straight line through the ball, connecting two points of the sphere, passes through the center and its length is thus twice the radius; it is a diameter of both the sphere and its ball.

While outside mathematics the terms "sphere" and "ball" are sometimes used interchangeably, in mathematics the above distinction is made between a "sphere", which is a two-dimensional closed surface, embedded in a three-dimensional Euclidean space, and a "ball", which is a three-dimensional shape that includes the sphere and everything "inside" the sphere (a "closed ball"), or, more often, just the points "inside", but "not on" the sphere (an "open ball"). This distinction has not always been maintained and especially older mathematical references talk about a sphere as a solid. This is analogous to the situation in the plane, where the terms "circle" and "disk" can also be confounded.

In analytic geometry, a sphere with center and radius is the locus of all points such that

Let be real numbers with and put
Then the equation
has no real points as solutions if formula_4 and is called the equation of an imaginary sphere. If formula_5 the only solution of formula_6 is the point formula_7 and the equation is said to be the equation of a point sphere. Finally, in the case formula_8, formula_6 is an equation of a sphere whose center is formula_10 and whose radius is formula_11.

If in the above equation is zero then is the equation of a plane. Thus, a plane may be thought of as a sphere of infinite radius whose center is a point at infinity.

The points on the sphere with radius formula_12 and center formula_13 can be parameterized via

A sphere of any radius centered at zero is an integral surface of the following differential form:

This equation reflects that position and velocity vectors of a point, and , traveling on the sphere are always orthogonal to each other.

A sphere can also be constructed as the surface formed by rotating a circle about any of its diameters. Since a circle is a special type of ellipse, a sphere is a special type of ellipsoid of revolution. Replacing the circle with an ellipse rotated about its major axis, the shape becomes a prolate spheroid; rotated about the minor axis, an oblate spheroid.

In three dimensions, the volume inside a sphere (that is the volume of a ball) is 
where is the radius of the sphere. Archimedes first derived this formula, by showing that the volume inside a sphere is twice the difference in the volumes between the inside of a sphere and the inside of a circumscribed cylinder (having the height and diameter equal to the diameter of the sphere). This assertion can be obtained from Cavalieri's principle. This formula can also be derived using integral calculus, i.e. disk integration to sum the volumes of an infinite number of circular disks of infinitesimally small thickness stacked side by side and centered along the -axis from to , assuming the sphere of radius is centered at the origin. 

At any given , the incremental volume () equals the product of the cross-sectional area of the disk at and its thickness ():

The total volume is the summation of all incremental volumes:

In the limit as approaches zero this equation becomes:

At any given , a right-angled triangle connects , and to the origin; hence, applying the Pythagorean theorem yields:

Using this substitution gives

that can be evaluated to give the result

Alternatively, this formula is found using spherical coordinates, with volume element
so
For most practical purposes, the volume inside a sphere inscribed in a cube can be approximated as 52.4% of the volume of the cube, since , where is the diameter of the sphere and also the length of a side of the cube and  ≈ 0.5236. For example, a sphere with diameter 1 meter has 52.4% the volume of a cube with edge length 1 meter, or about 0.524 m.

The surface area of a sphere of radius is:

Archimedes first derived this formula from the fact that the projection to the lateral surface of a circumscribed cylinder is area-preserving. Another approach to obtaining the formula comes from the fact that it equals the derivative of the formula for the volume with respect to because the total volume inside a sphere of radius can be thought of as the summation of the surface area of an infinite number of spherical shells of infinitesimal thickness concentrically stacked inside one another from radius 0 to radius . At infinitesimal thickness the discrepancy between the inner and outer surface area of any given shell is infinitesimal, and the elemental volume at radius is simply the product of the surface area at radius and the infinitesimal thickness.

At any given radius , the incremental volume () equals the product of the surface area at radius () and the thickness of a shell ():

The total volume is the summation of all shell volumes:

In the limit as approaches zero this equation becomes:

Substitute :

Differentiating both sides of this equation with respect to yields as a function of :

This is generally abbreviated as:
where is now considered to be the fixed radius of the sphere.

Alternatively, the area element on the sphere is given in spherical coordinates by . In Cartesian coordinates, the area element is
For more generality, see area element.

The total area can thus be obtained by integration:

The sphere has the smallest surface area of all surfaces that enclose a given volume, and it encloses the largest volume among all closed surfaces with a given surface area. The sphere therefore appears in nature: for example, bubbles and small water drops are roughly spherical because the surface tension locally minimizes surface area.

The surface area relative to the mass of a ball is called the specific surface area and can be expressed from the above stated equations as
where is the density (the ratio of mass to volume).

A sphere is uniquely determined by four points that are not coplanar. More generally, a sphere is uniquely determined by four conditions such as passing through a point, being tangent to a plane, etc. This property is analogous to the property that three non-collinear points determine a unique circle in a plane.

Consequently, a sphere is uniquely determined by (that is, passes through) a circle and a point not in the plane of that circle.

By examining the common solutions of the equations of two spheres, it can be seen that two spheres intersect in a circle and the plane containing that circle is called the radical plane of the intersecting spheres. Although the radical plane is a real plane, the circle may be imaginary (the spheres have no real point in common) or consist of a single point (the spheres are tangent at that point).

The angle between two spheres at a real point of intersection is the dihedral angle determined by the tangent planes to the spheres at that point. Two spheres intersect at the same angle at all points of their circle of intersection. They intersect at right angles (are orthogonal) if and only if the squares of the distance between their centers is equal to the sum of the squares of their radii.

If and are the equations of two distinct spheres then 
is also the equation of a sphere for arbitrary values of the parameters and . The set of all spheres satisfying this equation is called a pencil of spheres determined by the original two spheres. In this definition a sphere is allowed to be a plane (infinite radius, center at infinity) and if both the original spheres are planes then all the spheres of the pencil are planes, otherwise there is only one plane (the radical plane) in the pencil.

If the pencil of spheres does not consist of all planes, then there are three types of pencils:

All the tangent lines from a fixed point of the radical plane to the spheres of a pencil have the same length.

The radical plane is the locus of the centers of all the spheres that are orthogonal to all the spheres in a pencil. Moreover, a sphere orthogonal to any two spheres of a pencil of spheres is orthogonal to all of them and its center lies in the radical plane of the pencil.

Pairs of points on a sphere that lie on a straight line through the sphere's center are called antipodal points. A great circle is a circle on the sphere that has the same center and radius as the sphere and, consequently, divides it into two equal parts. The plane sections of a sphere are called "spheric sections". They are all circles and those that are not great circles are called "small circles". 

The shortest distance along the surface between two distinct non-antipodal points on the sphere is the length of the smaller of the two arcs on the unique great circle that includes the two points. Equipped with this "great-circle distance", a great circle becomes the Riemannian circle.

If a particular point on a sphere is (arbitrarily) designated as its "north pole", then the corresponding antipodal point is called the "south pole", and the equator is the great circle that is equidistant to them. Great circles through the two poles are called lines (or meridians) of longitude, and the line connecting the two poles is called the axis of rotation. Circles on the sphere that are parallel to the equator are lines of latitude. This terminology is also used for such approximately spheroidal astronomical bodies as the planet Earth (see geoid).

Any plane that includes the center of a sphere divides it into two equal hemispheres. Any two intersecting planes that include the center of a sphere subdivide the sphere into four lunes or biangles, the vertices of which all coincide with the antipodal points lying on the line of intersection of the planes.

The antipodal quotient of the sphere is the surface called the real projective plane, which can also be thought of as the northern hemisphere with antipodal points of the equator identified.

The hemisphere is conjectured to be the optimal (least area) isometric filling of the Riemannian circle.

Spheres can be generalized to spaces of any number of dimensions. For any natural number , an "-sphere," often written as , is the set of points in ()-dimensional Euclidean space that are at a fixed distance from a central point of that space, where is, as before, a positive real number. In particular:

Spheres for are sometimes called hyperspheres.

The -sphere of unit radius centered at the origin is denoted and is often referred to as "the" -sphere. Note that the ordinary sphere is a 2-sphere, because it is a 2-dimensional surface (which is embedded in 3-dimensional space).

The surface area of the unit ()-sphere is

where is Euler's gamma function.

Another expression for the surface area is

and the volume is the surface area times or

General recursive formulas also exist for the volume of an -ball.

More generally, in a metric space , the sphere of center and radius is the set of points such that .

If the center is a distinguished point that is considered to be the origin of , as in a normed space, it is not mentioned in the definition and notation. The same applies for the radius if it is taken to equal one, as in the case of a unit sphere.

Unlike a ball, even a large sphere may be an empty set. For example, in with Euclidean metric, a sphere of radius is nonempty only if can be written as sum of squares of integers.

In topology, an -sphere is defined as a space homeomorphic to the boundary of an -ball; thus, it is homeomorphic to the Euclidean -sphere, but perhaps lacking its metric.

The -sphere is denoted . It is an example of a compact topological manifold without boundary. A sphere need not be smooth; if it is smooth, it need not be diffeomorphic to the Euclidean sphere.

The Heine–Borel theorem implies that a Euclidean -sphere is compact. The sphere is the inverse image of a one-point set under the continuous function . Therefore, the sphere is closed. is also bounded; therefore it is compact.

Remarkably, it is possible to turn an ordinary sphere inside out in a three-dimensional space with possible self-intersections but without creating any crease, in a process called sphere eversion.

The basic elements of Euclidean plane geometry are points and lines. On the sphere, points are defined in the usual sense. The analogue of the "line" is the geodesic, which is a great circle; the defining characteristic of a great circle is that the plane containing all its points also passes through the center of the sphere. Measuring by arc length shows that the shortest path between two points lying on the sphere is the shorter segment of the great circle that includes the points.

Many theorems from classical geometry hold true for spherical geometry as well, but not all do because the sphere fails to satisfy some of classical geometry's postulates, including the parallel postulate. In spherical trigonometry, angles are defined between great circles. Spherical trigonometry differs from ordinary trigonometry in many respects. For example, the sum of the interior angles of a spherical triangle always exceeds 180 degrees. Also, any two similar spherical triangles are congruent.

In their book "Geometry and the Imagination" David Hilbert and Stephan Cohn-Vossen describe eleven properties of the sphere and discuss whether these properties uniquely determine the sphere. Several properties hold for the plane, which can be thought of as a sphere with infinite radius. These properties are:




</doc>
<doc id="27861" url="https://en.wikipedia.org/wiki?curid=27861" title="Sápmi">
Sápmi

Sápmi (), in English commonly known as Lapland (), is the cultural region traditionally inhabited by the Sami people, traditionally known in English as Lapps. Sápmi is located in Northern Europe and includes the northern parts of Fennoscandia. The region stretches over four countries: Norway, Sweden, Finland, and Russia. On the north it is bounded by the Barents Sea, on the west by the Norwegian Sea and on the east by the White Sea.

Despite being the namesake of the region, the Sami people are estimated to only make up around 5% of its total population. No political organization advocates secession, although several groups desire more territorial autonomy and/or more self-determination for the region's indigenous population.

Sápmi (and corresponding terms in other Sami languages) refers to both the Sami land and the Sami people. In fact, the word "Sámi" is only the accusative-genitive form of the noun "Sápmi"—making the name's ("Sámi olbmot") meaning "people of Sápmi." The origin of the word is speculated to be related to the Baltic word "*žēmē" that simply means "land". Also "Häme", the Finnish name for Tavastia, a historical province of Finland, is thought to have the same origin, and the same word is at least speculated to be the origin of "Suomi", the Finnish name for Finland.
Sápmi is the name in North Sami, while the Julev Sami name is "Sábme" and the South Sami name is "Saemie". In Norwegian and Swedish the term "Sameland" is often used.

In modern Swedish and Norwegian, Sápmi is known as "Sameland", but in older Swedish it was known as "Lappmarken", "Lappland", and Finnmark, respectively. Originally these two names did refer to the entire Sápmi, but subsequently became applied to areas "exclusively" inhabited by the Sami. "Lappland" (Laponia) became the name of Sweden's northernmost province ("landskap") which in 1809 was split into one part that remained Swedish and one part falling under Finland (which became part of the Russian Empire). "Lappland" survives as the name of both Sweden's northernmost province and Finland's, also containing part of the old Ostrobothnian province.

In older Norwegian, Sápmi was known as "Finnmork" or "Finnmark"; which is now the name of Norway's northernmost province. Both Northern Norway and Murmansk Oblast are sometimes marketed as "Norwegian Lapland" and "Russian Lapland", respectively.

In the 17th century, Johannes Schefferus assumed the etymology of the lesser used term "Lapland" to be related to the Swedish word for "running", ""löpa"" (cognate with English, "to leap").

The largest part of Sápmi lies north of the Arctic Circle. The western portion is an area of fjords, deep valleys, glaciers, and mountains, the highest point being Mount Kebnekaise (2,111 m/6,926 ft), in Swedish Lapland. The part of Sápmi falling on the Swedish side of the border is characterized by great rivers running from the northwest to the southeast. From the Norwegian province of Finnmark and eastwards, the terrain is that of a low plateau that contains many marshes and lakes, the largest of which is Lake Inari in Finnish Lapland. The extreme northeastern section lies within the tundra region, but it does not have permafrost.

In the 19th century scientific expeditions to Sápmi were undertaken, for instance by Jöns Svanberg.

The climate is subarctic and vegetation is sparse, except in the densely forested southern portion. The mountainous west coast has significantly milder winters and more precipitation than the large areas east of the mountain chain. North of the Arctic Circle polar night characterize the winter season and midnight sun the summer season—both phenomena are longer the further north you go. Traditionally, the Sami divide the year in "eight" seasons instead of four.

Sápmi contains valuable mineral deposits, particularly iron ore in Sweden, copper in Norway, and nickel and apatite in Russia. Reindeer, wolf, bear, and birds are the main forms of animal life, in addition to a myriad of insects in the short summer. Sea and river fisheries abound in the region. Steamers are operated on some of the lakes, and many ports are ice-free throughout the year. All ports along the Norwegian Sea in the west and the Barents Sea in the northeast to Murmansk are ice-free all year. The Gulf of Bothnia usually freezes over in winter. The ocean floor to the north and west of Sápmi has deposits of petroleum and natural gas.

"East Sápmi" consists of the Kola peninsula and the Lake Inari region, and is home to the eastern Sami languages. While being the most heavily populated part of Sápmi, this is also the region where the indigenous population and their culture is weakest. Corresponds to the regions marked 6 through 9 on the map below.

"Central Sápmi" consists of the western part of Finland's Sami Domicile Area, the parts of Norway north of the Saltfjellet mountains and areas on the Swedish side corresponding to this. Central Sápmi is the region where Sami culture is strongest, and home to North Sami—the most widely used Sami language. In the southernmost part of this subregion, however, Sami culture is rather weak—this is where the moribound "Bithun" Sami language is used. The areas around the Tysfjord fjord in Norway and the river Lule in Sweden are home to the "Julev" Sami language, one of the more widely used Sami languages. These correspond to the regions marked 3 through 5 on the map below.

"South Sápmi" consists of the areas south of Saltfjellet and corresponding areas in Sweden, and is home to the southern languages. In this area Sami culture is mostly visible inland and on the coast of Baltic Sea, and the languages are spoken by few. Corresponds to the regions marked 1 and 2 on the map below plus Dalarna County to the south-east of region 1 in Sweden.

The inner parts of Sápmi are often referred to as Lapland or Lappi, a name deriving from a former name given to the Sami, which is today considered derogatory by many Sami. The name is also found on the Russian side as "Laplandige" (the name of a natural reservation) and the Norwegian county of Finnmark is sometimes titled the "Norwegian Lapland", especially by the travel industry. "Lappi-" appears as a common component of place-names throughout central and southern Finland as well; in many cases, it probably refers to earlier Sami presence, though in some cases the underlying meaning may be merely "periphery" or "outlying district".

Finally, Sápmi may also be sub-divided into cultural regions according to the states' borders, that obviously affects daily life for people no matter their ethnicity. By Sami, these regions are commonly referred to as "sides", for example "the Norwegian side" ("norgga bealli") or "the Finnish side" ("suoma bealli").

The Saamic languages are the region's main minority languages and also its original languages. They belong to the Uralic language family, and are most closely related to the Finnic languages. Many Sami languages are mutually unintelligible, but the languages originally formed a dialect continuum stretching southwest-northeast, so that a message could hypothetically be passed between Sami speakers from one end to the other and be understood by all. Today, however, many of the languages are moribund and thus there are "gaps" in the original continuum.

On the map to the right numbers indicate Sámi Languages (Darkened areas represent municipalities that recognize Sami as an official language.): 1. South (Åarjil) Sámi, 2. Ume (Upme) Sámi, 3. Pite (Bitthun) Sámi, 4. Lule (Julev) Sámi, 5. North (Davvi) Sámi, 6. Skolt Sámi, 7. Inari (Ánár) Sámi, 8. Kildin Sámi, 9. Ter Sámi. Of these languages the Northern one is by far the most vital; whereas Ume, Pite and Ter seem to be dying languages. Kemi Sámi is extinct.

North Sami is subdivided into three main dialects: West, East and Coast. The written standard is based on the Western dialect.

The language spoken by most people in the region is Russian, which is an East Slavic language. It is the dominant language on the Russian side of the border and also spoken by recently immigrated minority groups elsewhere in Sápmi. Earlier, a common pidgin language was spoken on the northern coast of Sápmi that combined elements of Russian, Norwegian, North Sami and Kven. This language was known as Russenorsk. On the Russian side, there are also speakers of the East Slavic Belarusian and Ukrainian languages.

Norwegian and Swedish dominate the largest part of Sápmi, including the entire Southern region and most of the Central region. There also used to be minorities speaking Norwegian on the Kola Peninsula. The Scandinavian languages are to a very large degree mutually intelligible, much more so than South Sami and North Sami. The Norwegian dialects spoken particularly in North and Central Norway Sami areas differ very much from the written bokmål standard. In Central Sápmi the Scandinavian dialects have taken the Uralic trait of having a more or less constant emphasis on the first syllable of each spoken word. In the inner and northernmost parts of Sweden and Norway, however, people often speak Norwegian and Swedish close to the written standard, though with a heavy Uralic accent.

The Finnic (i.e. Baltic Finnic) languages are spoken on the Finnish (Finnish), Swedish (Meänkieli—spoken by the Tornedalians) and Norwegian (Kven) sides of the borders. There also used to be minorities speaking Finnish on the Kola Peninsula. The languages are as mutually intelligible as the Scandinavian languages. Other Finnic languages include Karelian, Estonian, Livonian, Veps, Votic and Izhoran. Many are mutually intelligible.

The approximate number of people living in Sápmi is about 2 million, though it is difficult to give the precise number of inhabitants since certain counties and provinces only include "parts" of Sápmi. It is also quite difficult to account for the distribution of ethnic groups as many people have double or multiple ethnic identities—both seeing themselves as members of the majority population "and" being part of one or more minority groups.

Different criteria are set when calculating the number of Sami, but the number is generally given as somewhere between 80,000 and 100,000. Many live in areas outside Sápmi such as Oulu, Oslo, Stockholm and Helsinki. Some Sapmi people have migrated to places outside the Sapmi vernacular region, such as in Canada and the United States. Many Sapmi people have settled in the northern parts of Minnesota.

About 900,000 people inhabit Murmansk province (oblast'), but parts of this area lie outside Sápmi. About 758,600 of Murmansk's population claim to be exclusively Russian. It should be noted, however, that ethnic Russians also live elsewhere in Sápmi. The Russian side of Sápmi is very ethnically diverse, with particularly big Ukrainian and Belarusian minorities. The Sami are one of the minor minorities in this part of Sápmi.

About 850,000 people inhabit the Norwegian regions North Norway (fully within Sápmi) and Trøndelag (mostly within Sápmi). However, many of the regions' inhabitants—particularly those of North Norway—are not exclusively Norwegian. Notable minority groups include the Sami, Finns, and Kvens.

About 700,000 people inhabit the Swedish counties Norrbotten, Västerbotten, Västernorrland and Jämtland. Many of the counties' inhabitants are not exclusively Swedish. Notable minority groups include the Sami, Tornedalians, Kvens, and Finns.

13,226 people inhabit the Sami native region of Lapland, Finland. A great portion of these are Sami.

These two ethnic groups, closely related to each other and also the Finns, mainly live on the Swedish and Norwegian sides of Sápmi, respectively.

Norway, Finland and Sweden all have Sami Parliaments that to varying degrees are involved in governing the region—though mostly they only have authority over the matters of the Sami citizens of the states in which they are situated.

Every Norwegian citizen registered as a Sami has the right to vote in the elections for the Sami Parliament of Norway. Elections are held every four years by direct vote from 13 constituencies covering all of Norway (12 of which are in Sápmi), and run parallel to the general Norwegian parliamentary elections. This is the Sami Parliament with the most influence over any part of Sápmi, as it is involved in the autonomy established by the Finnmark Act.The parliament is situated in Kárášjohka and its current President is Egil Olli from the Norwegian Labour Party.

The Sami Parliament of Sweden, situated in Kiruna (Northern Sami: "Giron"), is elected by a general vote where all registered Sami citizens of Sweden may attend. The current President is Lars-Anders Baer.

Voting for elections to the Sámi Parliament of Finland is restricted to inhabitants of the Sami Domicile Area. The Parliament is located in Inari (), and its current President is Pekka Aikio.

In Russia there is no Sami Parliament. There are two Sami organizations that are members of the national umbrella organisation of indigenous peoples, the Russian Association of Indigenous Peoples of the North and represent the Russian Sami in the Sami Council. RAIPON is represented in Russia's Public Chamber by Pavel Sulyandziga. On 14 December 2008 the first Congress of the Russian Sámi took place. The Conference decided to demand the formation of a Russian Sámi Parliament, to be elected by the local Sami. A suggestion to have the Russian Federation pick representatives to the Parliament was voted down with a clear majority. The Congress also chose a Council of Representatives that were to work for the establishment of a Parliament and otherwise represent the Russian Sami. It is headed by Valentina Sovkina.

On 2 March 2000, the Sami parliaments of Norway and Finland founded the Sami Parliamentary Council, and the Sami Parliament of Sweden joined two years later. Each parliament sends seven representatives, and observers are sent from the Sami organizations of Russia and the Sami Council (see below). The Sami Parliamentary Council discuss cross-border cooperation, hand out the annual "Gollegiella" language development award and represent the Sami people abroad.

In addition to the parliaments and their common council, there is a Saami Council based on Saami organizations. This council also organizes inter-state cooperation between the Saami, and also often represent the Saami in international fora such as the Barents Region. This organization is older than the Parliamentary Council, but not connected to the parliaments except for the fact that some of the NGOs double as party lists in Sami parliament elections.

The Russian Federation consists of several types of subunits. The Russian side of Sápmi is contained within an "oblast" (province). "Oblasti" are governed by popularly elected parliaments, and formally headed by governors. The governors are nominated by the President of the Russian Federation, and accepted or discarded by the parliaments. However, should the parliament refuse to accept the President's nominee, the President is entitled to dissolve parliament and call for new "oblast" elections.

Murmansk Oblast covers the Kola Peninsula and is home to Murmánska (Northern Sami) or Murmansk (Russian), the largest city north of the Arctic Circle and in Sápmi. It is subdivided into several districts, of which the geographically largest is Lovozersky District. This is also the part of Russia where the Sami population is most numerous and visible. In the west of the province there is a large natural reserve known as "Laplandiya". The current governor of Murmansk Province is Yuriy A. Yevdokimov, who has run the province since 1997 and helped found the pro-Putin party "Jedinstvo" that after Putin's victory combined with its main opponent to become the "Yedinaya Rossiya" Party.

The counties of Norway are governed by popularly elected assemblies, headed by county mayors. Formally, the counties are headed by county governors, but in practice these have limited influence today.

The largest of Norway's counties, Finmárku (Northern Sami) or Finnmark (Norwegian), is located in Sápmi and has a special form of autonomy: 95% (about 46,000 km) of the area is owned by the Finnmark Estate. The board of the Estate consists of equally many representatives from the Sami Parliament of Norway and Finnmark's county council. The two institutions appoint leaders of the board alternately. The administrative centre of Finmárku is Čáhcesuolu or Vadsø, located far east in the county. The current county governor is Runar Sjåstad from the Norwegian Labour Party.

Romsa or Troms is situated to the southwest of Finmárku. Its administrative centre is the city after which the county is named, Romsa or Tromsø. Romsa is North Norway's biggest city and Sápmi's biggest city after Murmansk. Current "fylkesordfører" is Terje Olsen from the Conservative Party. A similar solution to the Finnmark Estate, Hålogalandsallmenningen, has been proposed for Romsa county and its southern neighbour Nordlánda.

Nordland or Nordlánda (not official name) covers a long strip of coast that includes both North Sami, Julev Sami, Bithun Sami and South Sami areas. Its administrative centre is Bådåddjo or Bodø. The current county governor is Mariette Korsrud from the Norwegian Labour Party.

The southernmost parts of Norwegian Sapmi lie in Nord-Trøndelag and partially in Sør-Trøndelag, and the administrative centres of which are Steinkjer and Trondheim respectively. The latter city is outside Sápmi but well known for being the site of the first international Sami conference in February 1917. The county governors are Gunnar Viken (the Conservative Party) in Nord-Trøndelag and Tore Sandvik (Norwegian Labour Party) in Sør-Trøndelag.

"Lappland" is the name of a large northwestern province of Sweden, wholly within Sápmi. The traditional provinces of Sweden are cultural and historical entities; for administrative and political purposes they were replaced by the counties of Sweden (län) in 1634.

Five counties are wholly or partially within Sapmi. "Län" are formally governed by the "landshövding", who is an envoy of the government and runs the government-appointed "länsstyrelse" that coordinates administration with national political goals for the county. Much of county politics is run by the county council or "landsting", which is elected by the inhabitants of the county; but the counties' top positions are still determined by those who win the general elections of Sweden.

Norrbotten is more or less covered by Sápmi, although the lower Tornedalen region is often excluded. The administrative centre is Luleå in the Julev Sami area (Norrbotten includes North, Julev and Bithun areas). Current landshövding is Per-Ola Eriksson of the Centre Party.

Sápmi covers the interior majority of Västerbotten, which are Upmeje and South Sami regions. The administrative centre is Umeå, and the current landshövding is Chris Heister from the conservative Moderate Party.

Västernorrland is an old part of Sapmi and still is. There is a lot of Sami in the coast of Baltic Sea (Gulf of Bothnia).

Jämtland is wholly within Sápmi, and is a South Sami county. The administrative centre is Östersund. Current landshövding is Maggi Kristina Maria Mikaelsson from the socialist Left Party.

Sápmi covers the interior majority of Dalarna, which is traditionally a South Sami region.

Finland is subdivided into nineteen regions ("maakunta"). The regions are governed by regional councils, which are generally forums of cooperation between the municipalities and not elected by direct popular vote. "Lapland" ("Lappi") is the name of the northernmost of the regions, which stretches further south than Sápmi. North Sami, Skolt Sami and Aanaar Sami are indigenous to the region.

Four municipalities in the northern part of Finnish Lapland constitute the Sami Domicile Area; "Sámiid Ruovttoguovlu", a region which is autonomous on issues regarding Sami culture and language.

The region has its own football team, the Sápmi football team, which is organized by FA Sápmi. It is a member of ConIFA and the host of 2014 ConIFA World Football Cup. Sápmi football team won the 2006 VIVA World Cup and hosted the 2008 event.

The Tour de Barents is a cross-country skiing race held in the region.

The following towns and villages have a significant Sami population or host Sami institutions. Norwegian, Swedish, Finnish or Russian toponyms are in parenthesis.







</doc>
<doc id="27862" url="https://en.wikipedia.org/wiki?curid=27862" title="Sydney">
Sydney

Sydney () is the state capital of New South Wales and the most populous city in Australia and Oceania. Located on Australia's east coast, the metropolis surrounds Port Jackson and sprawls about on its periphery towards the Blue Mountains to the west, Hawkesbury to the north, and Macarthur to the south. Sydney is made up of 658 suburbs, 40 local government areas and 15 contiguous regions. Residents of the city are known as "Sydneysiders". As of June 2017, Sydney's estimated population was 5,131,326.

The Sydney area has been inhabited by indigenous Australians for at least 30,000 years. Lieutenant James Cook first landed at Kurnell in 1770, when navigating his way up the east coast of Australia on his ship, "HMS Endeavour". It was not until 1788 when the "First Fleet", which contained convicts and was led by Captain Arthur Phillip, arrived in Botany Bay to found Sydney as a penal colony, the first European settlement in Australia. Phillip named the city "Sydney" in recognition of Thomas Townshend, 1st Viscount Sydney, Home Secretary in 1788. The Sydney region is one of the richest in Australia in terms of Aboriginal archaeological sites, with significant rock art and engravings located in the protected Ku-ring-gai Chase National Park.

Since convict transportation ended in the mid-19th century, the city has transformed from a colonial outpost into a major global cultural and economic centre. The municipal council of Sydney was incorporated in 1842 and became Australia's first city. Gold was discovered in the colony in 1851 and with it came thousands of people seeking to make money. Sydney became one of the most multicultural cities in the world after the mass migration following the second World War. According to the , more than 250 different languages were spoken in Sydney and about 40 percent of residents spoke a language other than English at home. Furthermore, 36% of the population reported having been born overseas.

Despite being one of the most expensive cities in the world, the 2018 Mercer Quality of Living Survey ranks Sydney tenth in the world in terms of quality of living, making it one of the most livable cities. It is classified as an Alpha World City by Globalization and World Cities Research Network, indicating its influence in the region and throughout the world. Ranked eleventh in the world for economic opportunity, Sydney has an advanced market economy with strengths in finance, manufacturing and tourism. There is a significant concentration of foreign banks and multinational corporations in Sydney and the city is promoted as one of Asia Pacific's leading financial hubs. Established in 1850, the University of Sydney is Australia's first university and is regarded as one of the world's leading universities. Sydney is also home to the oldest library in Australia, State Library of New South Wales, opened in 1826.

Sydney has hosted international multi-sport events such as the 1938 British Empire Games and 2000 Summer Olympics. The city is amongst the top fifteen most-visited cities in the world, with millions of tourists coming each year to see the city's landmarks. Boasting over of nature reserves and parks, its notable natural features include Sydney Harbour, the Royal National Park, Royal Botanic Garden and Hyde Park, the oldest parkland in the country. Built attractions such as Sydney Tower, the Sydney Harbour Bridge and the Sydney Opera House (which became a World Heritage Site in 2007) are also well known to international visitors. The main passenger airport serving the metropolitan area is Kingsford-Smith Airport, one of the world's oldest continually operating airports. Established in 1906, Central station, the largest and busiest railway station in the state, is the main hub of the city's rail network.

The first people to inhabit the area now known as Sydney were indigenous Australians having migrated from northern Australia and before that from southeast Asia. Radiocarbon dating suggests human activity first started to occur in the Sydney area from around 30,735 years ago. However, numerous Aboriginal stone tools were found in Western Sydney's gravel sediments that were dated from 45,000 to 50,000 years BP, which would indicate that there was human settlement in Sydney earlier than thought.

The first meeting between the native people and the British occurred on 29 April 1770 when Lieutenant James Cook landed at Botany Bay on the Kurnell Peninsula and encountered the Gweagal clan. He noted in his journal that they were confused and somewhat hostile towards the foreign visitors. Cook was on a mission of exploration and was not commissioned to start a settlement. He spent a short time collecting food and conducting scientific observations before continuing further north along the east coast of Australia and claiming the new land he had discovered for Britain. Prior to the arrival of the British there were 4,000 to 8,000 native people in Sydney from as many as 29 different clans.

The earliest British settlers called the natives Eora people. "Eora" is the term the indigenous population used to explain their origins upon first contact with the British. Its literal meaning is "from this place".
Sydney Cove from Port Jackson to Petersham was inhabited by the Cadigal clan. The principal language groups were Darug, Guringai, and Dharawal. The earliest Europeans to visit the area noted that the indigenous people were conducting activities such as camping and fishing, using trees for bark and food, collecting shells, and cooking fish.

Britain—before that, England—and Ireland had for a long time been sending their convicts across the Atlantic to the American colonies. That trade was ended with the Declaration of Independence by the United States in 1776. Britain decided in 1786 to found a new penal outpost in the territory discovered by Cook some 16 years earlier.

Captain Philip led the "First Fleet" of 11 ships and about 850 convicts into Botany Bay on 18 January 1788, though deemed the location unsuitable due to poor soil and a lack of fresh water. He travelled a short way further north and arrived at Port Jackson on 26 January 1788. This was to be the location for the new colony. Phillip described Sydney Cove as being "without exception the finest harbour in the world". The colony was at first to be titled "New Albion", after Albion in Great Britain, but Phillip decided on "Sydney". The official proclamation and naming of the colony happened on 7 February 1788. Lieutenant William Dawes produced a town plan in 1790 but it was ignored by the colony's leaders. Sydney's layout today reflects this lack of planning.

Between 1788 and 1792, 3,546 male and 766 female convicts were landed at Sydney—many "professional criminals" with few of the skills required for the establishment of a colony. The food situation reached crisis point in 1790. Early efforts at agriculture were fraught and supplies from overseas were scarce. From 1791 on, however, the more regular arrival of ships and the beginnings of trade lessened the feeling of isolation and improved supplies. 

The colony was not founded on the principles of freedom and prosperity. Maps from this time show no prison buildings; the punishment for convicts was transportation rather than incarceration, but serious offences were penalised by flogging and hanging. Phillip sent exploratory missions in search of better soils and fixed on the Parramatta region as a promising area for expansion and moved many of the convicts from late 1788 to establish a small township, which became the main centre of the colony's economic life, leaving Sydney Cove only as an important port and focus of social life. Poor equipment and unfamiliar soils and climate continued to hamper the expansion of farming from Farm Cove to Parramatta and Toongabbie, but a building programme, assisted by convict labour, advanced steadily.

Officers and convicts alike faced starvation as supplies ran low and little could be cultivated from the land. The region's indigenous population was also suffering. It is estimated that half of the native people in Sydney died during the smallpox epidemic of 1789. Enlightened for his age, Phillip's personal intent was to establish harmonious relations with local Aboriginal people and try to reform as well as discipline the convicts of the colony. Phillip and several of his officers—most notably Watkin Tench—left behind journals and accounts of which tell of immense hardships during the first years of settlement. Part of Macquarie's effort to transform the colony was his authorisation for convicts to re-enter society as free citizens. Roads, bridges, wharves, and public buildings were constructed using convict labour and by 1822 the town had banks, markets, and well-established thoroughfares. Parramatta Road was opened in 1811, which is one of Sydney's oldest roads and Australia's first highway between two cities – Sydney CBD and Parramatta.

Conditions in the colony were not conducive to the development of a thriving new metropolis, but the more regular arrival of ships and the beginnings of maritime trade (such as wool) helped to lessen the burden of isolation. Between 1788 and 1792, convicts and their jailers made up the majority of the population; in one generation, however, a population of emancipated convicts who could be granted land began to grow. These people pioneered Sydney's private sector economy and were later joined by soldiers whose military service had expired, and later still by free settlers who began arriving from Britain. Governor Phillip departed the colony for England on 11 December 1792, with the new settlement having survived near starvation and immense isolation for four years.

Between 1790 and 1816, Sydney became one of the many sites of the Australian Frontier Wars, a series of conflicts between the Kingdom of Great Britain and the resisting Indigenous clans. In 1790, when the British established farms along the Hawkesbury River, an Aboriginal leader Pemulwuy resisted the Europeans by waging a guerrilla-style warfare on the settlers in a series of wars known as the Hawkesbury and Nepean Wars which took place in western Sydney. He raided farms until Governor Macquarie dispatched troops from the British Army 46th Regiment in 1816 and ended the conflict by killing 14 Indigenous Australians in a raid on their campsite.

In 1804, Irish convicts led the Castle Hill Rebellion, a rebellion by convicts against colonial authority in the Castle Hill area of the British colony of New South Wales. The first and only major convict uprising in Australian history suppressed under martial law, the rebellion ended in a battle fought between convicts and the colonial forces of Australia at Rouse Hill. The Rum Rebellion of 1808 was the only successful armed takeover of government in Australian history, where the Governor of New South Wales, William Bligh, was ousted by the New South Wales Corps under the command of Major George Johnston, who led the rebellion. Conflicts arose between the governors and the officers of the Rum Corps, many of which were land owners such as John Macarthur.

Early Sydney was molded by the hardship suffered by early settlers. In the early years, drought and disease caused widespread problems, but the situation soon improved. The military colonial government was reliant on the army, the New South Wales Corps. Macquarie served as the last autocratic Governor of New South Wales, from 1810 to 1821 and had a leading role in the social and economic development of Sydney which saw it transition from a penal colony to a budding free society. He established public works, a bank, churches, and charitable institutions and sought good relations with the Aborigines. 

Over the course of the 19th-century Sydney established many of its major cultural institutions. Governor Lachlan Macquarie's vision for Sydney included the construction of grand public buildings and institutions fit for a colonial capital. Macquarie Street began to take shape as a ceremonial thoroughfare of grand buildings. The year 1840 was the final year of convict transportation to Sydney, which by this time had a population of 35,000. Gold was discovered in the colony in 1851 and with it came thousands of people seeking to make money. Sydney's population reached 200,000 by 1871. Demand for infrastructure to support the growing population and subsequent economic activity led to massive improvements to the city's railway and port systems throughout the 1850s and 1860s. 

After a period of rapid growth, further discoveries of gold in Victoria began drawing new residents away from Sydney towards Melbourne in the 1850s, which created a strong rivalry between Sydney and Melbourne that still exists to this day. Nevertheless, Sydney exceeded Melbourne's population in the early twentieth century and remains Australia's largest city. Following the depression of the 1890s, the six colonies agreed to form the Commonwealth of Australia. Sydney's beaches had become popular seaside holiday resorts, but daylight sea bathing was considered indecent until the early 20th century. 

Under the reign of Queen Victoria federation of the six colonies occurred on 1 January 1901. Sydney, with a population of 481,000, then became the state capital of New South Wales. The Great Depression of the 1930s had a severe effect on Sydney's economy, as it did with most cities throughout the industrial world. For much of the 1930s up to one in three breadwinners was unemployed. Construction of the Sydney Harbour Bridge served to alleviate some of the effects of the economic downturn by employing 1,400 men between 1924 and 1932. The population continued to boom despite the Depression, having reached 1 million in 1925. 

When Britain declared war on Germany in 1939, Australia too entered. During the war Sydney experienced a surge in industrial development to meet the needs of a wartime economy. Far from mass unemployment, there were now labour shortages and women becoming active in male roles. Sydney's harbour was attacked by the Japanese in May and June 1942 with a direct attack from Japanese submarines with some loss of life. Households throughout the city had built air raid shelters and performed drills. Sydney saw a surge in industrial development to meet the needs of a war economy, and also the elimination of unemployment. Labour shortages forced the government to accept women in more active roles in war work.

Consequently, Sydney experienced population growth and increased cultural diversification throughout the post-war period. The people of Sydney warmly welcomed Queen Elizabeth II in 1954 when the reigning monarch stepped onto Australian soil for the first time to commence her Australian Royal Tour. Having arrived on the Royal Yacht Britannia through Sydney Heads, Her Majesty came ashore at Farm Cove. There were 1.7 million people living in Sydney at 1950 and almost 3 million by 1975. The Australian government launched a large scale multicultural immigration program. 

New industries such as information technology, education, financial services and the arts have risen. Sydney's iconic Opera House was opened in 1973 by Her Majesty. A new skyline of concrete and steel skyscrapers swept away much of the old lowrise and often sandstone skyline of the city in the 1960s and 1970s, with Australia Square being the tallest building in Sydney from its completion in 1967 until 1976 and is also notable for being the first skyscraper in Australia. Since the 1970s Sydney has undergone a rapid economic and social transformation. As a result, the city has become a cosmopolitan melting pot. 

To relieve congestion on the Sydney Harbour Bridge, the Sydney Harbour Tunnel opened in August 1992. The 2000 Summer Olympics were held in Sydney and became known as the "best Olympic Games ever" by the President of the International Olympic Committee. Sydney has maintained extensive political, economic and cultural influence over Australia as well as international renown in recent decades. Following the Olympics, the city hosted the 2003 Rugby World Cup, the APEC Australia 2007 and Catholic World Youth Day 2008, led by Pope Benedict XVI.

Sydney is a coastal basin with the Tasman Sea to the east, the Blue Mountains to the west, the Hawkesbury River to the north, and the Woronora Plateau to the south. The inner city measures , the Greater Sydney region covers , and the city's urban area is in size.

Sydney spans two geographic regions. The Cumberland Plain lies to the south and west of the Harbour and is relatively flat. The Hornsby Plateau is located to the north and is dissected by steep valleys. The flat areas of the south were the first to be developed as the city grew. It was not until the construction of the Sydney Harbour Bridge that the northern reaches of the coast became more heavily populated. Seventy beaches can be found along its coastline with Bondi Beach being one of the most famous.

The Nepean River wraps around the western edge of the city and becomes the Hawkesbury River before reaching Broken Bay. Most of Sydney's water storages can be found on tributaries of the Nepean River. The Parramatta River is mostly industrial and drains a large area of Sydney's western suburbs into Port Jackson. The southern parts of the city are drained by the Georges River and the Cooks River into Botany Bay.
Sydney is made up of mostly Triassic rock with some recent igneous dykes and volcanic necks. The Sydney Basin was formed when the Earth's crust expanded, subsided, and filled with sediment in the early Triassic period. The sand that was to become the sandstone of today was washed there by rivers from the south and northwest, and laid down between 360 and 200 million years ago. The sandstone has shale lenses and fossil riverbeds.

The Sydney Basin bioregion includes coastal features of cliffs, beaches, and estuaries. Deep river valleys known as rias were carved during the Triassic period in the Hawkesbury sandstone of the coastal region where Sydney now lies. The rising sea level between 18,000 and 6,000 years ago flooded the rias to form estuaries and deep harbours. Port Jackson, better known as Sydney Harbour, is one such ria.

The most prevalent plant communities in the Sydney region are "Dry Sclerophyll Forests" which consist of eucalyptus trees, casuarinas, melaleucas, sclerophyll shrubs (typically wattles and banksias) and a semi-continuous grass in the understory, mainly in an open woodland setting. These plants tend to have rough and spiky leaves, as they're grown in areas with low soil fertility. Wet sclerophyll forests are found in the damp, elevated areas of Sydney, such as in the northeast. They are defined by straight, tall tree canopies with an elaborate, moist understorey of soft-leaved shrubs, tree ferns and herbs.

Sydney is home to dozens of bird species, which commonly include the Australian raven, Australian magpie, crested pigeon, noisy miner and the pied currawong, among others. Introduced bird species ubiquitously found in Sydney are the common myna, common starling, house sparrow and the spotted dove. Reptile species are also numerous and predominantly include skinks. Sydney has a few mammal and spider species, such as the grey-headed flying fox and the Sydney funnel-web, respectively.

Under the Köppen–Geiger classification, Sydney has a humid subtropical climate ("Cfa") with warm summers, cool winters and uniform rainfall throughout the year. At Sydney's primary weather station at Observatory Hill, extreme temperatures have ranged from on 18 January 2013 to on 22 June 1932. An average of 14.9 days a year have temperatures at or above in the central business district (CBD). In contrast, the metropolitan area averages between 35 and 65 days, depending on the suburb. The highest minimum temperature recorded at Observatory Hill is , in February 2011 while the lowest maximum temperature is , recorded in July 1868.

The weather is moderated by proximity to the ocean, and more extreme temperatures are recorded in the inland western suburbs. Sydney experiences an urban heat island effect. This makes certain parts of the city more vulnerable to extreme heat, including coastal suburbs. In late spring and summer, temperatures over are not uncommon, though hot, dry conditions are usually ended by a southerly buster. This powerful storm brings gale winds and rapid fall in temperature, followed by brief heavy rain and thunder. The far-western suburbs, which border the Blue Mountains, experience a Föhn-like wind in the warm months that originates from the Central Tablelands. Due to the inland location, frost at night is recorded in Western Sydney a few times in winter. Autumn and spring are the transitional seasons, with spring showing a larger temperature variation than autumn. 

The rainfall has a moderate to low variability and it is spread through the months, but is slightly higher during the first half of the year. From 1990–1999, Sydney received around 20 thunderstorms per year. In late autumn and winter, east coast lows may bring large amounts of rainfall, especially in the CBD. Depending on the wind direction, summer weather may be humid or dry, with the late summer/autumn period having a higher average humidity and dewpoints than late spring/early summer. In summer, most rain falls from thunderstorms and in winter from cold fronts. Snowfall was last reported in the Sydney City area in 1836, while a fall of graupel, or soft hail, mistaken by many for snow, in July 2008, has raised the possibility that the 1836 event was not snow, either.

The city is rarely affected by cyclones, although remnants of ex-cyclones do affect the city. The El Niño–Southern Oscillation plays an important role in determining Sydney's weather patterns: drought and bushfire on the one hand, and storms and flooding on the other, associated with the opposite phases of the oscillation. Many areas of the city bordering bushland have experienced bushfires, these tend to occur during the spring and summer. The city is also prone to severe storms. One such storm was the 1999 hailstorm, which produced massive hailstones up to in diameter.

The Bureau of Meteorology has reported that 2002 through 2005 were the warmest summers in Sydney since records began in 1859. The summer of 2007–08, however, proved to be the coolest since 1996–97 and is the only summer this century to be at or below average in temperatures. In 2009, dry conditions brought a severe dust storm towards eastern Australia.

The average annual temperature of the sea ranges from in September to in February.

The regions of Sydney include the CBD or City of Sydney (colloquially referred to as 'the City') and Inner West, the Eastern Suburbs, Southern Sydney, Greater Western Sydney (including the South-west), and the Northern Suburbs (including the North-west). The Greater Sydney Commission divides Sydney into five districts based on the 33 LGAs in the metropolitan area; the Western City, the Central City, the Eastern City, the North District, and the South District.

The CBD extends about south from Sydney Cove. It is bordered by Farm Cove within the Royal Botanic Garden to the east and Darling Harbour to the west. Suburbs surrounding the CBD include Woolloomooloo and Potts Point to the east, Surry Hills and Darlinghurst to the south, Pyrmont and Ultimo to the west, and Millers Point and The Rocks to the north. Most of these suburbs measure less than in area. The Sydney CBD is characterised by considerably narrow streets and thoroughfares, created in its convict beginnings in the 18th century.

Several localities, distinct from suburbs, exist throughout Sydney's inner reaches. Central and Circular Quay are transport hubs with ferry, rail, and bus interchanges. Chinatown, Darling Harbour, and Kings Cross are important locations for culture, tourism, and recreation. The Strand Arcade, which is located between Pitt Street Mall and George Street, is a historical Victorian-style shopping arcade. Opened on 1 April 1892, its shop fronts are an exact replica of the original internal shopping facades. Westfield Sydney, located beneath the Sydney Tower, is the largest shopping centre by area in Sydney.

There is a long trend of gentrification amongst Sydney's inner suburbs. Pyrmont located on the harbour was redeveloped from a centre of shipping and international trade to an area of high density housing, tourist accommodation, and gambling. Originally located well outside of the city, Darlinghurst is the location of a former gaol, manufacturing, and mixed housing. It had a period when it was known as an area of prostitution. The terrace style housing has largely been retained and Darlinghurst has undergone significant gentrification since the 1980s.

Green Square is a former industrial area of Waterloo which is undergoing urban renewal worth $8 billion. On the city harbour edge the historic suburb and wharves of Millers Point are being built up as the new area of Barangaroo. The Millers Point/Barangaroo development has significant controversy regardless of the $6 billion worth of economic activity it is generating. The suburb of Paddington is a well known suburb for its streets of restored terrace houses, Victoria Barracks, and shopping including the weekly Oxford Street markets.

The Inner West generally includes the Inner West Council, Municipality of Burwood, Municipality of Strathfield, and City of Canada Bay. These span up to about 11km west of the CBD. Suburbs in the Inner West have historically housed working class industrial workers, but have undergone gentrification over the 20th century. The region now mainly features medium- and high-density housing. Major features in the area include the University of Sydney and the Parramatta River, as well as a large cosmopolitan community. The Anzac Bridge spans Johnstons Bay and connects Rozelle to Pyrmont and the City, forming part of the Western Distributor.

The area is serviced by the T1, T2, and T3 railway lines, including the Main Suburban Line; which is the first to be constructed in New South Wales. Strathfield Railway Station is a secondary railway hub within Sydney, and major station on the Suburban and Northern lines. It was constructed in 1876, and will be a future terminus of Parramatta Light Rail. The area is also serviced by numerous bus routes and cycleways. Other shopping centres in the area include Westfield Burwood and DFO in Homebush.

The Eastern Suburbs encompass the Municipality of Woollahra, the City of Randwick, the Waverley Municipal Council, and parts of the Bayside Council. The Greater Sydney Commission envisions a resident population of 1,338,250 people by 2036 in its Eastern City District (including the City and Inner West).

They include some of the most affluent and advantaged areas in the country, with some streets being amongst the most expensive in the world. Wolseley Road, in Point Piper, has a top price of $20,900 per square metre, making it the ninth-most expensive street in the world. More than 75% of neighbourhoods in the Electoral District of Wentworth fall under the top decile of SEIFA advantage, making it the least disadvantaged area in the country.

Major landmarks include Bondi Beach, a major tourist site; which was added to the Australian National Heritage List in 2008; and Bondi Junction, featuring a Westfield shopping centre and an estimated office work force of 6,400 by 2035, as well as a train station on the T4 Eastern Suburbs Line. The suburb of Randwick contains the Randwick Racecourse, the Royal Hospital for Women, the Prince of Wales Hospital, Sydney Children's Hospital, and the UNSW Kensington Campus. Randwick's 'Collaboration Area' has a baseline estimate of 32,000 jobs by 2036, according to the Greater Sydney Commission.

Construction is underway for the CBD and South East Light Rail line. Although main construction was due to complete in 2018, completion has potentially been delayed to March 2020. The project aims to provide reliable and high-capacity tram services to residents in the City and South-East.

Major shopping centres in the area include Westfield Bondi Junction and Westfield Eastgardens, although many residents shop in the City.

Southern Sydney includes the suburbs in the local government areas of former Rockdale, Georges River Council (collectively known as the St George area), and broadly it also includes the suburbs in the local government area of Sutherland, south of the Georges River (colloquially known as 'The Shire').

The Kurnell peninsula, near Botany Bay, is the site of the first landfall on the eastern coastline made by Lt. (later Captain) James Cook in 1770. La Perouse, a historic suburb named after the French navigator Jean-François de Galaup, comte de Lapérouse (1741–88), is notable for its old military outpost at Bare Island and the Botany Bay National Park. 

The suburb of Cronulla in southern Sydney is close to Royal National Park, Australia's oldest national park. Hurstville, a large suburb with a multitude of commercial buildings and high-rise residential buildings dominating the skyline, has become a CBD for the southern suburbs.
Because 'Northern Suburbs' is not a clearly defined region, 'Northern Suburbs' may also include the suburbs in the Upper North Shore, Lower North Shore and the Northern Beaches.

The Northern Suburbs include several landmarks – Macquarie University, Gladesville Bridge, Ryde Bridge, Macquarie Centre and Curzon Hall in Marsfield. This area includes suburbs in the local government areas of Hornsby Shire, City of Ryde, the Municipality of Hunter's Hill and parts of the City of Parramatta.

The North Shore, an informal geographic term referring to the northern metropolitan area of Sydney, consists of , , , , Killara, , , and many others. The North Shore, an upper middle class area, has one of the highest property prices in Sydney with the recent property price inflation sending the average property prices in suburbs such as Roseville,
Lindfield, Killara and Gordon over 2 million dollars.

The North Shore includes the commercial centres of North Sydney and Chatswood. North Sydney itself consists of a large commercial centre, with its own business centre, which contains the second largest concentration of high-rise buildings in Sydney, after the CBD. North Sydney is dominated by advertising, marketing businesses and associated trades, with many large corporations holding office in the region.

The Lower North Shore usually refers to the suburbs adjacent to the harbour such as , , Mosman, , Cremorne Point, , Milsons Point, , Northbridge, and North Sydney. The Lower North Shore's eastern boundary is Middle Harbour, or at the Roseville Bridge at and . The Upper North Shore usually refers to the suburbs between and . It is made up of suburbs located within Ku-ring-gai and Hornsby Shire councils.

The Northern Beaches area includes Manly, one of Sydney's most popular holiday destinations for much of the nineteenth and twentieth centuries. The Northern Beaches area extends south to the entrance of Port Jackson (Sydney Harbour), west to Middle Harbour and north to the entrance of Broken Bay. The 2011 Australian census found the Northern Beaches to be the most white and mono-ethnic district in Australia, contrasting with its more-diverse neighbours, the North Shore and the Central Coast.

The Hills district generally refers to the suburbs in north-western Sydney including the local government areas of The Hills Shire, parts of the City of Parramatta Council and Hornsby Shire. Actual suburbs and localities that are considered to be in the Hills District can be somewhat amorphous and variable. For example, the Hills District Historical Society restricts its definition to the Hills Shire local government area, yet its study area extends from Parramatta to the Hawkesbury. The region is so named for its characteristically comparatively hilly topography as the Cumberland Plain lifts up, joining the Hornsby Plateau. Several of its suburbs also have 'Hills' in their names, such as Baulkham Hills, Castle Hill, Seven Hills, Pendle Hill, Beaumont Hills, and Winston Hills, among others. Windsor and Old Windsor Roads are historic roads in Australia, as they are the second and third roads, respectively, laid in the colony.

The greater western suburbs encompasses the areas of Parramatta, the sixth largest business district in Australia, settled the same year as the harbour-side colony, Bankstown, Liverpool, Penrith, and Fairfield. Covering and having an estimated resident population as at 30 June 2008 of 1,665,673, western Sydney has the most multicultural suburbs in the country. The population is predominantly of a working class background, with major employment in the heavy industries and vocational trade.

The western suburb of Prospect, in the City of Blacktown, is home to Wet'n'Wild, a water park operated by Village Roadshow Theme Parks. Auburn Botanic Gardens, a botanical garden situated in Auburn, attracts thousands of visitors each year, including a significant number from outside Australia. Another prominent park and garden in the west is Central Gardens Nature Reserve in Merrylands West. The greater west also includes Sydney Olympic Park, a suburb created to host the 2000 Summer Olympics, and Sydney Motorsport Park, a motorsport circuit located in Eastern Creek. The Boothtown Aqueduct in Greystanes is a 19th century water bridge that is listed on the New South Wales State Heritage Register as a site of State significance.

To the northwest, Featherdale Wildlife Park, an Australian zoo in Doonside, near Blacktown, is a major tourist attraction, not just for Western Sydney, but for NSW and Australia. Westfield Parramatta in Parramatta is Australia's busiest Westfield shopping centre, having 28.7 million customer visits per annum. Established in 1799, the Old Government House, a historic house museum and tourist spot in Parramatta, was included in the Australian National Heritage List on 1 August 2007 and World Heritage List in 2010 (as part of the 11 penal sites constituting the Australian Convict Sites), making it the only site in greater western Sydney to be featured in such lists. Moreover, the house is Australia's oldest surviving public building. Prospect Hill, a historically significant ridge in the west, is also listed on the NSW State Heritage Register.

Further to the southwest is the region of Macarthur and the city of Campbelltown, a significant population centre until the 1990s considered a region separate to Sydney proper. Macarthur Square, a shopping complex in Campbelltown, become one of the largest shopping complexes in Sydney. The southwest also features Bankstown Reservoir, the oldest elevated reservoir constructed in reinforced concrete that is still in use and is listed on the New South Wales State Heritage Register.

The earliest structures in the colony were built to the bare minimum of standards. Upon his appointment, Governor Lachlan Macquarie set ambitious targets for the architectural design of new construction projects. The city now has a world heritage listed building, several national heritage listed buildings, and dozens of Commonwealth heritage listed buildings as evidence of the survival of Macquarie's ideals.

In 1814 the Governor called on a convict named Francis Greenway to design Macquarie Lighthouse. The lighthouse and its Classical design earned Greenway a pardon from Macquarie in 1818 and introduced a culture of refined architecture that remains to this day. Greenway went on to design the Hyde Park Barracks in 1819 and the Georgian style St James's Church in 1824. Gothic-inspired architecture became more popular from the 1830s. John Verge's Elizabeth Bay House and St Philip's Church of 1856 were built in Gothic Revival style along with Edward Blore's Government House of 1845. Kirribilli House, completed in 1858, and St Andrew's Cathedral, Australia's oldest cathedral, are rare examples of Victorian Gothic construction.

From the late 1850s there was a shift towards Classical architecture. Mortimer Lewis designed the Australian Museum in 1857. The General Post Office, completed in 1891 in Victorian Free Classical style, was designed by James Barnet. Barnet also oversaw the 1883 reconstruction of Greenway's Macquarie Lighthouse. Customs House was built in 1844 to the specifications of Lewis, with additions from Barnet in 1887 and W L Vernon in 1899. The neo-Classical and French Second Empire style Town Hall was completed in 1889. Romanesque designs gained favour amongst Sydney's architects from the early 1890s. Sydney Technical College was completed in 1893 using both Romanesque Revival and Queen Anne approaches. The Queen Victoria Building was designed in Romanesque Revival fashion by George McRae and completed in 1898. It was built on the site of the Sydney Central Markets and accommodates 200 shops across its three storeys.

The Great Depression had a tangible influence on Sydney's architecture. New structures became more restrained with far less ornamentation than was common before the 1930s. The most notable architectural feat of this period is the Harbour Bridge. Its steel arch was designed by John Jacob Crew Bradfield and completed in 1932. A total of 39,000 tonnes of structural steel span the between Milsons Point and Dawes Point.

Modern and International architecture came to Sydney from the 1940s. Since its completion in 1973 the city's Opera House has become a World Heritage Site and one of the world's most renowned pieces of Modern design. It was conceived by Jørn Utzon with contributions from Peter Hall, Lionel Todd, and David Littlemore. Utzon was awarded the Pritzker Prize in 2003 for his work on the Opera House. Sydney is home to Australia's first building by renowned Canadian architect Frank Gehry, the Dr Chau Chak Wing Building (2015), based on the design of a tree house. An entrance from The Goods Line–a pedestrian pathway and former railway line–is located on the eastern border of the site.

Sydney's first tower was Culwulla Chambers on the corner of King Street and Castlereagh Street which topped out at . With the lifting of height restrictions in the 1960s there came a surge of high-rise construction. Acclaimed architects such as Jean Nouvel, Harry Seidler, Richard Rogers, Renzo Piano, Norman Foster, and Frank Gehry have each made their own contribution to the city's skyline.

Important buildings in the CBD include Citigroup Centre, Aurora Place, Chifley Tower, the Reserve Bank building, Deutsche Bank Place, MLC Centre, and Capita Centre. The tallest structure is Sydney Tower, designed by Donald Crone and completed in 1981. Regulations limit new buildings to a height of due to the proximity of Sydney Airport, although strict restrictions employed in the early 2000s have slowly been relaxed in the past ten years.

Sydney real estate prices are some of the most expensive in the world, surpassing both New York City and Paris. There were 1.5 million dwellings in Sydney in 2006 including 940,000 detached houses and 180,000 semi-detached terrace houses. Units or apartments make up 25.8% of Sydney's dwellings, more than the 12.8% which are semi-detached but less than the 60.9% which are separate houses. Whilst terrace houses are common in the inner city areas, it is detached houses that dominate the landscape in the outer suburbs.

About 80% of all dwellings in Western Sydney are separate houses. Due to environmental and economic pressures there has been a noted trend towards denser housing. There was a 30% increase in the number of apartments in Sydney between 1996 and 2006. Public housing in Sydney is managed by the Government of New South Wales. Suburbs with large concentrations of public housing include Claymore, Macquarie Fields, Waterloo, and Mount Druitt. The Government has announced plans to sell nearly 300 historic public housing properties in the harbourside neighbourhoods of Millers Point, Gloucester Street, and The Rocks.

Sydney is one of the most expensive real estate markets globally. It is only second to Hong Kong with the average property costing 14 times the annual Sydney salary as of December 2016. A range of heritage housing styles can be found throughout Sydney. Terrace houses are found in the inner suburbs such as Paddington, The Rocks, Potts Point and Balmain–many of which have been the subject of gentrification. These terraces, particularly those in suburbs such as The Rocks, were historically home to Sydney's miners and labourers. In the present day, terrace houses now make up some of the most valuable real estate in the city.

Federation homes, constructed around the time of Federation in 1901, are located in Penshurst, Turramurra, and in Haberfield. Haberfield is known as "The Federation Suburb" due to the extensive number of Federation homes. Workers cottages are found in Surry Hills, Redfern, and Balmain. California bungalows are common in Ashfield, Concord, and Beecroft. Modern, 'McMansion'-type of homes are predominantly found in the outer suburbs, such as in, Stanhope Gardens, Kellyville Ridge and Bella Vista to the northwest, Bossley Park. Abbotsbury and Cecil Hills to the greater west, and Hoxton Park, Harrington Park and Oran Park to the southwest.

The Royal Botanic Garden is the most important green space in the Sydney region, hosting both scientific and leisure activities. There are 15 separate parks under the administration of the City of Sydney. Parks within the city centre include Hyde Park, The Domain and Prince Alfred Park.

The outer suburbs include Centennial Park and Moore Park in the east, Sydney Park and Royal National Park in the south, Ku-ring-gai Chase National Park in the north, and Western Sydney Parklands in the west, which is one of the largest urban parks in the world. The Royal National Park was proclaimed on 26 April 1879 and with is the second oldest national park in the world. The largest park in the Sydney metropolitan area is Ku-ring-gai Chase National Park, established in 1894 with an area of . It is regarded for its well-preserved records of indigenous habitation and more than 800 rock engravings, cave drawings and middens have been located in the park.

The area now known as The Domain was set aside by Governor Arthur Phillip in 1788 as his private reserve. Under the orders of Macquarie the land to the immediate north of The Domain became the Royal Botanic Garden in 1816. This makes them the oldest botanic garden in Australia. The Gardens are not just a place for exploration and relaxation, but also for scientific research with herbarium collections, a library and laboratories. The two parks have a total area of with 8,900 individual plant species and receive over 3.5 million annual visits.

To the south of The Domain is Hyde Park. It is the oldest public parkland in Australia and measures in area. Its location was used for both relaxation and the grazing of animals from the earliest days of the colony. Macquarie dedicated it in 1810 for the "recreation and amusement of the inhabitants of the town" and named it in honour of the original Hyde Park in London.

Researchers from Loughborough University have ranked Sydney amongst the top ten world cities that are highly integrated into the global economy. The Global Economic Power Index ranks Sydney number eleven in the world. The Global Cities Index recognises it as number fourteen in the world based on global engagement.

The prevailing economic theory in effect during early colonial days was mercantilism, as it was throughout most of Western Europe. The economy struggled at first due to difficulties in cultivating the land and the lack of a stable monetary system. Governor Lachlan Macquarie solved the second problem by creating two coins from every Spanish silver dollar in circulation. The economy was clearly capitalist in nature by the 1840s as the proportion of free settlers increased, the maritime and wool industries flourished, and the powers of the East India Company were curtailed.

Wheat, gold, and other minerals became additional export industries towards the end of the 1800s. Significant capital began to flow into the city from the 1870s to finance roads, railways, bridges, docks, courthouses, schools and hospitals. Protectionist policies after federation allowed for the creation of a manufacturing industry which became the city's largest employer by the 1920s. These same policies helped to relieve the effects of the Great Depression during which the unemployment rate in New South Wales reached as high as 32%. From the 1960s onwards Parramatta gained recognition as the city's second CBD and finance and tourism became major industries and sources of employment.

Sydney's nominal gross domestic product was AU$400.9 billion and AU$80,000 per capita in 2015. Its gross domestic product was AU$337 billion in 2013, the largest in Australia. The Financial and Insurance Services industry accounts for 18.1% of gross product and is ahead of Professional Services with 9% and Manufacturing with 7.2%. In addition to Financial Services and Tourism, the Creative and Technology sectors are focus industries for the City of Sydney and represented 9% and 11% of its economic output in 2012.

There were 451,000 businesses based in Sydney in 2011, including 48% of the top 500 companies in Australia and two-thirds of the regional headquarters of multinational corporations. Global companies are attracted to the city in part because its time zone spans the closing of business in North America and the opening of business in Europe. Most foreign companies in Sydney maintain significant sales and service functions but comparably less production, research, and development capabilities. There are 283 multinational companies with regional offices in Sydney.

Sydney has been ranked between the fifteenth and the fifth most expensive city in the world and is the most expensive city in Australia. To compensate, workers receive the seventh highest wage levels of any city in the world. Sydney's residents possess the highest purchasing power of any city after Zürich. Working residents of Sydney work an average of 1,846 hours per annum with 15 days of leave.

The labour force of Greater Sydney Region in 2016 was 2,272,722 with a participation rate of 61.6%. It was made up of 61.2% full-time workers, 30.9% part-time workers, and 6.0% unemployed individuals. The largest reported occupations are professionals, clerical and administrative workers, managers, technicians and trades workers, and community and personal service workers. The largest industries by employment across Greater Sydney are Health Care and Social Assistance with 11.6%, Professional Services with 9.8%, Retail Trade with 9.3%, Construction with 8.2%, Education and Training with 8.0%, Accommodation and Food Services 6.7%, and Financial and Insurance Services with 6.6%. The Professional Services and Financial and Insurance Services industries account for 25.4% of employment within the City of Sydney.

In 2016, 57.6% of working age residents had a total weekly income of less than $1,000 and 14.4% had a total weekly income of $1,750 or more.The median weekly income for the same period was $719 for individuals, $1,988 for families, and $1,750 for household.

Unemployment in the City of Sydney averaged 4.6% for the decade to 2013, much lower than the current rate of unemployment in Western Sydney of 7.3%. Western Sydney continues to struggle to create jobs to meet its population growth despite the development of commercial centres like Parramatta. Each day about 200,000 commuters travel from Western Sydney to the CBD and suburbs in the east and north of the city.

Home ownership in Sydney was less common than renting prior to the Second World War but this trend has since reversed. Median house prices have increased by an average of 8.6% per annum since 1970. The median house price in Sydney in March 2014 was $630,000. The primary cause for rising prices is the increasing cost of land which made up 32% of house prices in 1977 compared to 60% in 2002. 31.6% of dwellings in Sydney are rented, 30.4% are owned outright and 34.8% are owned with a mortgage. 11.8% of mortgagees in 2011 had monthly loan repayments of less than $1,000 and 82.9% had monthly repayments of $1,000 or more. 44.9% of renters for the same period had weekly rent of less than $350 whilst 51.7% had weekly rent of $350 or more. The median weekly rent in Sydney is $450.

Macquarie gave a charter in 1817 to form the first bank in Australia, the Bank of New South Wales. New private banks opened throughout the 1800s but the financial system was unstable. Bank collapses were a frequent occurrence and a crisis point was reached in 1893 when 12 banks failed.

The Bank of New South Wales exists to this day as Westpac. The Commonwealth Bank of Australia was formed in Sydney in 1911 and began to issue notes backed by the resources of the nation. It was replaced in this role in 1959 by the Reserve Bank of Australia which is also based in Sydney. The Australian Securities Exchange began operating in 1987 and with a market capitalisation of $1.6 trillion is now one of the ten largest exchanges in the world.

The Financial and Insurance Services industry now constitutes 43% of the economic product of the City of Sydney. Sydney makes up half of Australia's finance sector and has been promoted by consecutive Commonwealth Governments as Asia Pacific's leading financial centre. Structured finance was pioneered in Sydney and the city is a leading hub for asset management firms. In the 2017 Global Financial Centres Index, Sydney was ranked as having the eighth most competitive financial center in the world.

In 1985 the Federal Government granted 16 banking licences to foreign banks and now 40 of the 43 foreign banks operating in Australia are based in Sydney, including the People's Bank of China, Bank of America, Citigroup, UBS, Mizuho Bank, Bank of China, Banco Santander, Credit Suisse, State Street, HSBC, Deutsche Bank, Barclays, Royal Bank of Canada, Société Générale, Royal Bank of Scotland, Sumitomo Mitsui, ING Group, BNP Paribas, and Investec.

Sydney has been a manufacturing city since the protectionist policies of the 1920s. By 1961 the industry accounted for 39% of all employment and by 1970 over 30% of all Australian manufacturing jobs were in Sydney. Its status has declined in more recent decades, making up 12.6% of employment in 2001 and 8.5% in 2011. Between 1970 and 1985 there was a loss of 180,000 manufacturing jobs. The city is still the largest manufacturing centre in Australia. Its manufacturing output of $21.7 billion in 2013 was greater than that of Melbourne with $18.9 billion. Observers have noted Sydney's focus on the domestic market and high-tech manufacturing as reasons for its resilience against the high Australian dollar of the early 2010s.

Sydney is a gateway to Australia for many international visitors. It has hosted over 2.8 million international visitors in 2013, or nearly half of all international visits to Australia. These visitors spent 59 million nights in the city and a total of $5.9 billion. The countries of origin in descending order were China, New Zealand, the United Kingdom, the United States, South Korea, Japan, Singapore, Germany, Hong Kong, and India.

The city also received 8.3 million domestic overnight visitors in 2013 who spent a total of $6 billion. 26,700 workers in the City of Sydney were directly employed by tourism in 2011. There were 480,000 visitors and 27,500 people staying overnight each day in 2012. On average, the tourism industry contributes $36 million to the city's economy per day.

Popular destinations include the Sydney Opera House, the Sydney Harbour Bridge, Watsons Bay, The Rocks, Sydney Tower, Darling Harbour, the State Library of New South Wales, the Royal Botanic Garden, the Royal National Park, the Australian Museum, the Museum of Contemporary Art, the Art Gallery of New South Wales, the Queen Victoria Building, Sea Life Sydney Aquarium, Taronga Zoo, Bondi Beach, the Blue Mountains, and Sydney Olympic Park.

Major developmental projects designed to increase Sydney's tourism sector include a casino and hotel at Barangaroo and the redevelopment of East Darling Harbour, which involves a new exhibition and convention centre, now Australia's largest.

Sydney is the highest ranking city in the world for international students. More than 50,000 international students study at the city's universities and a further 50,000 study at its vocational and English language schools. International education contributes $1.6 billion to the local economy and creates demand for 4,000 local jobs each year.

The population of Sydney in 1788 was less than 1,000. With convict transportation it tripled in ten years to 2,953. For each decade since 1961 the population has increased by more than 250,000. Sydney's population at the time of the 2011 census was 4,391,674. It has been forecast that the population will grow to between 8 and 8.9 million by 2061. Despite this increase, the Australian Bureau of Statistics predicts that Melbourne will replace Sydney as Australia's most populous city by 2053. The four most densely populated suburbs in Australia are located in Sydney with each having more than 13,000 residents per square kilometre (33,700 residents per square mile).
The median age of Sydney residents is 36 and 12.9% of people are 65 or older. The married population accounts for 49.7% of Sydney whilst 34.7% of people have never been married. 48.9% of families are couples with children, 33.5% are couples without children, and 15.7% are single-parent families. 32.5% of people in Sydney speak a language other than English at home with Arabic, Mandarin, Cantonese, Vietnamese and Greek the most widely spoken.

There were 54,746 people of indigenous heritage living in Sydney in 2011. Most immigrants to Sydney between 1840 and 1930 were British, Irish or Chinese. There were significant clusters of people based on nationality or religion throughout the history of Sydney development. In the early 20th century Irish people were centred in Surry Hills, the Scottish in Paddington.

Following World War II, Sydney's ethnic groups began to diversify. Common ethnic groups in Sydney include, but are not limited to, Dutch, Sri Lankan, Indian, Assyrian, Turkish, Thai, Russian, Vietnamese, Filipino, Korean, Greek, Lebanese, Italian, Jewish, Polish, German, Serbian, Macedonian, and Maltese communities. As of the 2011 census night there were 1,503,620 people living in Sydney that were born overseas, accounting for 42.5% of the population of the City of Sydney and 34.2% of the population of Sydney, the seventh greatest proportion of any city in the world. The 2016 census reported that 39 percent of Greater Sydney were migrants, above New York City (36 percent), Paris (25 percent), Berlin (13 percent) and Tokyo (2 percent). If local residents with at least one migrant-born parent is included, then 65 percent of Sydney's population is migrant.

Sydney's largest ancestry groups are English, Australian, Irish, Chinese and Scottish. Foreign countries of birth with the greatest representation are England, China, India, New Zealand and Vietnam. The concentration of immigrants in Sydney, relative to the rest of Australia (excluding Melbourne), make it the exception rather than the norm on having such a high overseas-born population.

Ku-ring-gai Chase National Park is rich in Indigenous Australian heritage, containing around 1,500 pieces of Aboriginal rock art – the largest cluster of Indigenous sites in Australia, surpassing Kakadu, which has around 5,000 sites but over a much greater land mass. The park's indigenous sites include rock engravings, art sites, burial sites, caves, marriage areas, birthing areas, midden sites, and tool manufacturing locations, among others, which are dated to be around 5,000 years old. The inhabitants of the area were the Garigal people. 

The Australian Museum opened in Sydney in 1857 with the purpose of collecting and displaying the natural wealth of the colony. It remains Australia's oldest natural history museum. In 1995 the Museum of Sydney opened on the site of the first Government House. It recounts the story of the city's development. Other museums based in Sydney include the Powerhouse Museum and the Australian National Maritime Museum.

In 1866 then Queen Victoria gave her assent to the formation of the Royal Society of New South Wales. The Society exists "for the encouragement of studies and investigations in science, art, literature, and philosophy". It is based in a terrace house in Darlington owned by the University of Sydney. The Sydney Observatory building was constructed in 1859 and used for astronomy and meteorology research until 1982 before being converted into a museum.

The Museum of Contemporary Art was opened in 1991 and occupies an Art Deco building in Circular Quay. Its collection was founded in the 1940s by artist and art collector John Power and has been maintained by the University of Sydney. Sydney's other significant art institution is the Art Gallery of New South Wales which coordinates the coveted Archibald Prize for portraiture. Contemporary art galleries are found in Waterloo, Surry Hills, Darlinghurst, Paddington, Chippendale, Newtown, and Woollahra.

Sydney's first commercial theatre opened in 1832 and nine more had commenced performances by the late 1920s. The live medium lost much of its popularity to cinema during the Great Depression before experiencing a revival after World War II. Prominent theatres in the city today include State Theatre, Theatre Royal, Sydney Theatre, The Wharf Theatre, and Capitol Theatre. Sydney Theatre Company maintains a roster of local, classical, and international plays. It occasionally features Australian theatre icons such as David Williamson, Hugo Weaving, and Geoffrey Rush. The city's other prominent theatre companies are New Theatre, Belvoir, and Griffin Theatre Company.

The Sydney Opera House is the home of Opera Australia and Sydney Symphony. It has staged over 100,000 performances and received 100 million visitors since opening in 1973. Two other important performance venues in Sydney are Town Hall and the City Recital Hall. The Sydney Conservatorium of Music is located adjacent to the Royal Botanic Garden and serves the Australian music community through education and its biannual Australian Music Examinations Board exams.

Many writers have originated in and set their work in Sydney. The city was the headquarters for Australia's first published newspaper, the "Sydney Gazette". Watkin Tench's "A Narrative of the Expedition to Botany Bay" (1789) and "A Complete Account of the Settlement at Port Jackson in New South Wales" (1793) have remained the best-known accounts of life in early Sydney. Since the infancy of the establishment, much of the literature set in Sydney were concerned with life in the city's slums and working-class communities, notably William Lane's "The Working Man's Paradise" (1892), Christina Stead's "Seven Poor Men of Sydney" (1934) and Ruth Park's "The Harp in the South" (1948). The first Australian-born female novelist, Louisa Atkinson, set various of her novels in Sydney. Contemporary writers, such as Elizabeth Harrower, were born in the city and thus set most of the work there–Harrower's debut book "Down in the City" (1958) was set in a King's Cross apartment. Well known contemporary novels set in the city include Melina Marchetta's "Looking for Alibrandi" (1992), Peter Carey's "30 Days in Sydney: A Wildly Distorted Account" (1999), J.M. Coetzee's "Diary of a Bad Year" (2007) and Kate Grenville's "The Secret River" (2010). The Sydney Writers' Festival is held every year between April and May.

Filmmaking in Sydney was quite prolific until the 1920s when spoken films were introduced and American productions gained dominance in Australian cinema. The Australian New Wave of filmmaking saw a resurgence in film production in the city–with many notable features shot in the city between the 1970s and 80s, helmed by directors such as Bruce Beresford, Peter Weir and Gillian Armstrong. Fox Studios Australia commenced production in Sydney in 1998. Successful films shot in Sydney since then include "The Matrix", "Lantana", "", "Moulin Rouge!", "Australia", and "The Great Gatsby". The National Institute of Dramatic Art is based in Sydney and has several famous alumni such as Mel Gibson, Judy Davis, Baz Luhrmann, Cate Blanchett, Hugo Weaving and Jacqueline Mckenzie.
Sydney is the host of several festivals throughout the year. The city's New Year's Eve celebrations are the largest in Australia. The Royal Easter Show is held every year at Sydney Olympic Park. Sydney Festival is Australia's largest arts festival. Big Day Out is a travelling rock music festival that originated in Sydney. The city's two largest film festivals are Sydney Film Festival and Tropfest. Vivid Sydney is an annual outdoor exhibition of art installations, light projections, and music.

In 2015, Sydney was ranked 13th for being the top fashion capitals in the world. It hosts the Australian Fashion Week in autumn. The Sydney Mardi Gras has commenced each February since 1979. Sydney's Chinatown has had numerous locations since the 1850s. It moved from George Street to Campbell Street to its current setting in Dixon Street in 1980. The Spanish Quarter is based in Liverpool Street whilst Little Italy is located in Stanley Street. Popular nightspots are found at Kings Cross, Oxford Street, Circular Quay, and The Rocks. The Star is the city's only casino and is situated around Darling Harbour.

The indigenous people of Sydney held totemic beliefs known as "dreamings". Governor Lachlan Macquarie made an effort to found a culture of formal religion throughout the early settlement and ordered the construction of churches such as St Matthew's, St Luke's, St James's, and St Andrew's. According to 2011 census, these and other religious institutions have contributed to the education and health of Sydney's residents over time. 28.3% identify themselves as Catholic, whilst 17.6% practice no religion, 16.1% are Anglican, 4.7% are Muslim, 4.2% are Eastern Orthodox, 4.1% are Buddhist, 2.6% are Hindu, and 0.9% are Jewish.

"The Sydney Morning Herald" is Australia's oldest newspaper still in print. Now a compact form paper owned by Fairfax Media, it has been published continuously since 1831. Its competitor is the News Corporation tabloid "The Daily Telegraph" which has been in print since 1879. Both papers have Sunday tabloid editions called "The Sun-Herald" and "The Sunday Telegraph" respectively. "The Bulletin" was founded in Sydney in 1880 and became Australia's longest running magazine. It closed after 128 years of continuous publication. Sydney heralded Australia’s first newspaper, the Sydney Gazette, published until 1842.

Each of Australia's three commercial television networks and two public broadcasters is headquartered in Sydney. Nine's offices and news studios are based in Willoughby, Ten and Seven are based in Pyrmont, Seven has a news studio in the Sydney CBD in Martin Place the Australian Broadcasting Corporation is located in Ultimo, and the Special Broadcasting Service is based in Artarmon. Multiple digital channels have been provided by all five networks since 2000. Foxtel is based in North Ryde and sells subscription cable television to most parts of the urban area. Sydney's first radio stations commenced broadcasting in the 1920s. Radio became a popular tool for politics, news, religion, and sport and has managed to survive despite the introduction of television and the Internet. 2UE was founded in 1925 and under the ownership of Fairfax Media is the oldest station still broadcasting. Competing stations include the more popular 2GB, 702 ABC Sydney, KIIS 106.5, Triple M, Nova 96.9, and 2Day FM.

Sydney's earliest migrants brought with them a passion for sport but were restricted by the lack of facilities and equipment. The first organised sports were boxing, wrestling, and horse racing from 1810 in Hyde Park. Horse racing remains popular to this day and events such as the Golden Slipper Stakes attract widespread attention. The first cricket club was formed in 1826 and matches were played within Hyde Park throughout the 1830s and 1840s. Cricket is a favoured sport in summer and big matches have been held at the Sydney Cricket Ground since 1878. The New South Wales Blues compete in the Sheffield Shield league and the Sydney Sixers and Sydney Thunder contest the national Big Bash Twenty20 competition.

First played in Sydney in 1865, rugby grew to be the city's most popular football code by the 1880s. One-tenth of the state's population attended a New South Wales versus New Zealand rugby match in 1907. Rugby league separated from rugby union in 1908. The New South Wales Waratahs contest the Super Rugby competition. The national Wallabies rugby union team competes in Sydney in international matches such as the Bledisloe Cup, Rugby Championship, and World Cup. Sydney is home to nine of the sixteen teams in the National Rugby League competition: Canterbury-Bankstown Bulldogs, Cronulla-Sutherland Sharks, Manly Sea Eagles, Penrith Panthers, Parramatta Eels, South Sydney Rabbitohs, St George Illawarra Dragons, Sydney Roosters, and Wests Tigers. New South Wales contests the annual State of Origin series against Queensland.

Sydney FC and the Western Sydney Wanderers compete in the A-League (men's) and W-League (women's) soccer competitions and Sydney frequently hosts matches for the Australian national men's team, the Socceroos. The Sydney Swans and Greater Western Sydney Giants are local Australian rules football clubs that play in the Australian Football League. The Giants also compete in AFL Women's. The Sydney Kings compete in the National Basketball League. The Sydney Uni Flames play in the Women's National Basketball League. The Sydney Blue Sox contest the Australian Baseball League. The Waratahs are a member of the Australian Hockey League. The Sydney Bears and Sydney Ice Dogs play in the Australian Ice Hockey League. The Swifts are competitors in the national women's netball league.
Women were first allowed to participate in recreational swimming when separate baths were opened at Woolloomooloo Bay in the 1830s. From being illegal at the beginning of the century, sea bathing gained immense popularity during the early 1900s and the first surf lifesaving club was established at Bondi Beach. Disputes about appropriate clothing for surf bathing surfaced from time to time and concerned men as well as women. The City2Surf is an annual running race from the CBD to Bondi Beach and has been held since 1971. In 2010, 80,000 runners participated which made it the largest run of its kind in the world.

Sailing races have been held on Sydney Harbour since 1827. Yachting has been popular amongst wealthier residents since the 1840s and the Royal Sydney Yacht Squadron was founded in 1862. The Sydney to Hobart Yacht Race is a event that starts from Sydney Harbour on Boxing Day. Since its inception in 1945 it has been recognised as one of the most difficult yacht races in the world. Six sailors died and 71 vessels of the fleet of 115 failed to finish in the 1998 edition.

The Royal Sydney Golf Club is based in Rose Bay and since its opening in 1893 has hosted the Australian Open on 13 occasions. Royal Randwick Racecourse opened in 1833 and holds several major cups throughout the year. Sydney benefitted from the construction of significant sporting infrastructure in preparation for its hosting of the 2000 Summer Olympics. Sydney Olympic Park accommodates athletics, aquatics, tennis, hockey, archery, baseball, cycling, equestrian, and rowing facilities. It also includes the high capacity Stadium Australia used for rugby, soccer, and Australian rules football. Sydney Football Stadium was completed in 1988 and is used for rugby and soccer matches. Sydney Cricket Ground was opened in 1878 and is used for both cricket and Australian rules football fixtures.

A tennis tournament is held here at the beginning of each year as the warm-up for the Grand Slam in Melbourne. Two of the most successful tennis players in history: Ken Rosewall and Todd Woodbridge were born in and live in the city.

During early colonial times the presiding Governor and his military shared absolute control over the population. This lack of democracy eventually became unacceptable for the colony's growing number of free settlers. The first indications of a proper legal system emerged with the passing of a Charter of Justice in 1814. It established three new courts, including the Supreme Court, and dictated that English law was to be followed. In 1823 the British Parliament passed an act to create the Legislative Council in New South Wales and give the Supreme Court the right of review over new legislation. From 1828 all of the common laws in force in England were to be applied in New South Wales wherever it was appropriate. Another act from the British Parliament in 1842 provided for members of the Council to be elected for the first time.

The Constitution Act of 1855 gave New South Wales a bicameral government. The existing Legislative Council became the upper house and a new body called the Legislative Assembly was formed to be the lower house. An Executive Council was introduced and constituted five members of the Legislative Assembly and the Governor. It became responsible for advising the ruling Governor on matters related to the administration of the state. The colonial settlements elsewhere on the continent eventually seceded from New South Wales and formed their own governments. Tasmania separated in 1825, Victoria did so in 1850, and Queensland followed in 1859. With the proclamation of the Commonwealth of Australia in 1901 the status of local governments across Sydney was formalised and they became separate institutions from the state of New South Wales.

Sydney is divided into local government areas (also known as councils or shires) which are comparable in nature to London's boroughs. These local government areas have elected councils which are responsible for functions delegated to them by the New South Wales Government. The 31 local government areas making up Sydney according to the New South Wales Division of Local Government are:



Sydney is the location of the secondary official residences of the Governor-General of Australia and the Prime Minister of Australia, Admiralty House and Kirribilli House respectively. The Parliament of New South Wales sits in Parliament House on Macquarie Street. This building was completed in 1816 and first served as a hospital. The Legislative Council moved into its northern wing in 1829 and by 1852 had entirely supplanted the surgeons from their quarters. Several additions have been made to the building as the Parliament has expanded, but it retains its original Georgian façade. Government House was completed in 1845 and has served as the home of 25 Governors and 5 Governors-General. The Cabinet of Australia also meets in Sydney when needed.
The highest court in the state is the Supreme Court of New South Wales which is located in Queen's Square in Sydney. The city is also the home of numerous branches of the intermediate District Court of New South Wales and the lower Local Court of New South Wales.

Public activities such as main roads, traffic control, public transport, policing, education, and major infrastructure projects are controlled by the New South Wales Government. It has tended to resist attempts to amalgamate Sydney's more populated local government areas as merged councils could pose a threat to its governmental power. Established in 1842, the City of Sydney is one such local government area and includes the CBD and some adjoining inner suburbs. It is responsible for fostering development in the local area, providing local services (waste collection and recycling, libraries, parks, sporting facilities), representing and promoting the interests of residents, supporting organisations that target the local community, and attracting and providing infrastructure for commerce, tourism, and industry. The City of Sydney is led by an elected Council and Lord Mayor who has in the past been treated as a representative of the entire city.

In federal politics, Sydney was initially considered as a possibility for Australia's capital city; the newly created city of Canberra ultimately filled this role. Six Australian Prime Ministers have been born in Sydney, more than any other city, including first Prime Minister Edmund Barton and incumbent Malcolm Turnbull.

Education became a proper focus for the colony from the 1870s when public schools began to form and schooling became compulsory. The population of Sydney is now highly educated. 90% of working age residents have completed some schooling and 57% have completed the highest level of school. 1,390,703 people were enrolled in an educational institution in 2011 with 45.1% of these attending school and 16.5% studying at a university. Undergraduate or postgraduate qualifications are held by 22.5% of working age Sydney residents and 40.2% of working age residents of the City of Sydney. The most common fields of tertiary qualification are commerce (22.8%), engineering (13.4%), society and culture (10.8%), health (7.8%), and education (6.6%).

There are six public universities based in Sydney: the University of Sydney, the University of New South Wales, the University of Technology Sydney, Macquarie University, the Western Sydney University, and the Australian Catholic University. Four public universities maintain secondary campuses in the city: the University of Notre Dame Australia, the University of Wollongong, Curtin University of Technology, and the University of Newcastle. 5.2% of residents of Sydney are attending a university. University of Sydney is ranked in the top 50 in the world, the University of Technology Sydney is ranked 193, while Macquarie University ranks 237, and the Western Sydney University below 500.

Sydney has public, denominational, and independent schools. 7.8% of Sydney residents are attending primary school and 6.4% are enrolled in secondary school. There are 935 public preschool, primary, and secondary schools in Sydney that are administered by the New South Wales Department of Education. 14 of the 17 selective secondary schools in New South Wales are based in Sydney.

Public vocational education and training in Sydney is run by TAFE New South Wales and began with the opening of the Sydney Technical College in 1878. It offered courses in areas such as mechanical drawing, applied mathematics, steam engines, simple surgery, and English grammar. The college became the Sydney Institute in 1992 and now operates alongside its sister TAFE facilities across the Sydney metropolitan area, namely the Northern Sydney Institute, the Western Sydney Institute, and the South Western Sydney Institute. At the 2011 census, 2.4% of Sydney residents are enrolled in a TAFE course.

The first hospital in the new colony was a collection of tents at The Rocks. Many of the convicts that survived the trip from England continued to suffer from dysentery, smallpox, scurvy, and typhoid. Healthcare facilities remained hopelessly inadequate despite the arrival of a prefabricated hospital with the Second Fleet and the construction of brand new hospitals at Parramatta, Windsor, and Liverpool in the 1790s.

Governor Lachlan Macquarie arranged for the construction of Sydney Hospital and saw it completed in 1816. Parts of the facility have been repurposed for use as Parliament House but the hospital itself still operates to this day. The city's first emergency department was established at Sydney Hospital in 1870. Demand for emergency medical care increased from 1895 with the introduction of an ambulance service. The Sydney Hospital also housed Australia's first teaching facility for nurses, the Nightingale Wing, established with the input of Florence Nightingale in 1868.

Healthcare gained recognition as a citizen's right in the early 1900s and Sydney's public hospitals came under the oversight of the Government of New South Wales. The administration of healthcare across Sydney is handled by eight local health districts: Central Coast, Illawarra Shoalhaven, Sydney, Nepean Blue Mountains, Northern Sydney, South Eastern Sydney, South Western Sydney, and Western Sydney. The Prince of Wales Hospital was established in 1852 and became the first of several major hospitals to be opened in the coming decades. St Vincent's Hospital was founded in 1857, followed by Royal Alexandra Hospital for Children in 1880, the Prince Henry Hospital in 1881, the Royal Prince Alfred Hospital in 1882, the Royal North Shore Hospital in 1885, the St George Hospital in 1894, and the Nepean Hospital in 1895. Westmead Hospital in 1978 was the last major facility to open.

The motor vehicle, more than any other factor, has determined the pattern of Sydney's urban development since World War II. The growth of low density housing in the city's outer suburbs has made car ownership necessary for hundreds of thousands of households. The percentage of trips taken by car has increased from 13% in 1947 to 50% in 1960 and to 70% in 1971. The most important roads in Sydney were the nine Metroads, including the Sydney Orbital Network. Widespread criticism over Sydney's reliance on sprawling road networks, as well as the motor vehicle, have stemmed largely from proponents of mass public transport and high density housing. On an international scale, Sydney was ranked at 51 out of 100 cities in the world for sustainability and effectiveness of public transport in a report by Arcadis–lagging behind Brisbane, but ahead of both Melbourne and Perth.

There can be up to 350,000 cars using Sydney's roads simultaneously during peak hour, leading to significant traffic congestion. 84.9% of Sydney households own a motor vehicle and 46.5% own two or more. Car dependency is high in Sydney–of people that travel to work, 58.4% use a car, 9.1% catch a train, 5.2% take a bus, and 4.1% walk. In contrast, only 25.2% of working residents in the City of Sydney use a car, whilst 15.8% take a train, 13.3% use a bus, and 25.3% walk. With a rate of 26.3%, Sydney has the highest utilisation of public transport for travel to work of any Australian capital city.

Sydney once had one of the largest tram networks in the world. It was the second largest in the British Empire, after London, with routes covering . The internal combustion engine made buses more flexible than trams and consequently more popular, leading to the progressive closure of the tram network with the final tram operating in 1961. From 1930 there were 612 buses across Sydney carrying 90 million passengers per annum.

In 1997, the Inner West Light Rail (also known as the Dulwich Hill Line) opened between Central station and Wentworth Park. It was extended to Lilyfield in 2000 and then Dulwich Hill in 2014. It links the Inner West and Darling Harbour with Central station and facilitated 9.1 million journeys in the 2016-17 financial year. A second, the CBD and South East Light Rail line serving the CBD and south-eastern suburbs is planned to open in early 2019. When the light rail project is completed, it would cover a total distance of 12 km with 19 different stops. The Parramatta Light Rail has also been announced.

Bus services today are conducted by a mixture of Government and private operators. In areas previously serviced by trams the government State Transit Authority operates, in other areas, there are private (albeit part funded by the state government) operators. Integrated tickets called Opal cards operate on both government and private bus routes. State Transit alone operated a fleet of 2,169 buses and serviced over 160 million passengers during 2014. In total, nearly 225 million boardings were recorded across the bus network NightRide is a nightly bus service that operate between midnight and 5am, also replacing trains for most of this period.
Train services are operated by Sydney Trains. The organisation maintains 176 stations and of railway and provides 281 million journeys each year. Sydney's railway was first constructed in 1854 with progressive extension to the network to serve both freight and passengers across the city, suburbs, and beyond to country NSW. In the 1850s and 1860s the railway reached Parramatta, Campbelltown, Liverpool, Blacktown, Penrith, and Richmond. In 2014 94.2% of trains arrived on time and 99.5% of services ran as scheduled. Construction of Sydney Metro, an automated rapid transit system separate from the existing suburban network, started in 2013. The first stage is expected to open in 2019, with plans in place to extend the system through the CBD by 2024.

At the time the Sydney Harbour Bridge opened in 1932, the city's ferry service was the largest in the world. Patronage declined from 37 million passengers in 1945 to 11 million in 1963 but has recovered somewhat in recent years. From its hub at Circular Quay the ferry network extends from Manly to Parramatta. Sydney Airport, officially "Sydney Kingsford-Smith Airport", is located in the inner southern suburb of Mascot with two of the runways going into Botany Bay. It services 46 international and 23 domestic destinations. As the busiest airport in Australia it handled 37.9 million passengers in 2013 and 530,000 tonnes of freight in 2011.

It has been announced that a new facility named Western Sydney Airport will be constructed at Badgerys Creek from 2016 at a cost of $2.5 billion. Bankstown Airport is Sydney's second busiest airport, and serves general aviation, charter and some scheduled cargo flights. Bankstown is also the fourth busiest airport in Australia by number of aircraft movements. Port Botany has surpassed Port Jackson as the city's major shipping port. Cruise ship terminals are located at Sydney Cove and White Bay.

As climate change, greenhouse gas emissions and pollution have become a major issue for Australia, Sydney has in the past been criticised for its lack of focus on reducing pollution, cutting back on emissions and maintaining water quality. Since 1995, there have been significant developments in the analysis of air pollution in the Sydney metropolitan region. The development led to the release of the Metropolitan Air Quality Scheme (MAQS), which led to a broader understanding of the causation of pollution in Sydney, allowing the government to form appropriate responses to the pollution.

Australian cities are some of the most car dependent cities in the world. Sydney in particular has a very high level of car dependency, especially by world city standards. It also has a low level of mass-transit services, with a historically low-density layout and significant urban sprawl, thus increasing the likelihood of car dependency. Strategies have been implemented to reduce private vehicle pollution by encouraging mass and public transit, initiating the development of high density housing and introducing a fleet of 10 new Nissan LEAF electric cars, the largest order of the pollution-free vehicle in Australia. Electric cars do not produce carbon monoxide and nitrous oxide, gases which contribute to climate change. Cycling trips have increased by 113% across Sydney's inner-city since March 2010, with about 2,000 bikes passing through top peak-hour intersections on an average weekday. Transport developments in the north-west and east of the city have been designed to encourage the use of Sydney's expanding public transportation system.

The City of Sydney became the first council in Australia to achieve formal certification as carbon-neutral in 2008. The city has reduced its 2007 carbon emissions by 6% and since 2006 has reduced carbon emissions from city buildings by up to 20%. The City of Sydney introduced a "Sustainable Sydney 2030" program, with various targets planned and a comprehensive guide on how to reduce energy in homes and offices within Sydney by 30%. Reductions in energy consumption have slashed energy bills by $30 million a year. Solar panels have been established on many CBD buildings in an effort to minimise carbon pollution by around 3,000 tonnes a year.

The city also has an "urban forest growth strategy", in which it aims to regular increase the tree coverage in the city by frequently planting trees with strong leaf density and vegetation to provide cleaner air and create moisture during hot weather, thus lowering city temperatures. Sydney has also become a leader in the development of green office buildings and enforcing the requirement of all building proposals to be energy-efficient. The One Central Park development, completed in 2013, is an example of this implementation and design.

Obtaining sufficient fresh water was difficult during early colonial times. A catchment called the Tank Stream sourced water from what is now the CBD but was little more than an open sewer by the end of the 1700s. The Botany Swamps Scheme was one of several ventures during the mid 1800s that saw the construction of wells, tunnels, steam pumping stations, and small dams to service Sydney's growing population.

The first genuine solution to Sydney's water demands was the Upper Nepean Scheme which came into operation in 1886 and cost over £2 million. It transports water from the Nepean, Cataract, and Cordeaux rivers and continues to service about 15% of Sydney's total water needs. Dams were built on these three rivers between 1907 and 1935. In 1977 the Shoalhaven Scheme brought several more dams into service.

The WaterNSW now manages eleven major dams: Warragamba one of the largest domestic water supply dams in the world, Woronora, Cataract, Cordeaux, Nepean, Avon, Wingecarribee Reservoir, Fitzroy Falls Reservoir, Tallowa, the Blue Mountains Dams, and Prospect Reservoir. Water is collected from five catchment areas covering and total storage amounts to . The Sydney Desalination Plant came into operation in 2010.

The two distributors which maintain Sydney's electricity infrastructure are Ausgrid and Endeavour Energy. Their combined networks include over 815,000 power poles and of electricity cables.




</doc>
<doc id="27863" url="https://en.wikipedia.org/wiki?curid=27863" title="Sword">
Sword

A sword is a bladed weapon intended for slashing or thrusting that is longer than a knife or dagger. The precise definition of the term varies with the historical epoch or the geographic region under consideration. A sword consists of a long blade attached to a hilt. The blade can be straight or curved. Thrusting swords have a pointed tip on the blade, and tend to be straighter; slashing swords have a sharpened cutting edge on one or both sides of the blade, and are more likely to be curved. Many swords are designed for both thrusting and slashing.

Historically, the sword developed in the Bronze Age, evolving from the dagger; the earliest specimens date to about 1600 BC. The later Iron Age sword remained fairly short and without a crossguard. The spatha, as it developed in the Late Roman army, became the predecessor of the European sword of the Middle Ages, at first adopted as the Migration Period sword, and only in the High Middle Ages, developed into the classical arming sword with crossguard. The word "sword" continues the Old English, "sweord".

The use of a sword is known as swordsmanship or, in a modern context, as fencing. In the Early Modern period, western sword design diverged into roughly two forms, the thrusting swords and the sabers.

The thrusting swords such as the rapier and eventually the smallsword were designed to impale their targets quickly and inflict deep stab wounds. Their long and straight yet light and well balanced design made them highly maneuverable and deadly in a duel but fairly ineffective when used in a slashing or chopping motion. A well aimed lunge and thrust could end a fight in seconds with just the sword's point, leading to the development of a fighting style which closely resembles modern fencing.

The saber (sabre) and similar blades such as the cutlass were built more heavily and were more typically used in warfare. Built for slashing and chopping at multiple enemies, often from horseback, the saber's long curved blade and slightly forward weight balance gave it a deadly character all its own on the battlefield. Most sabers also had sharp points and double edged blades, making them capable of piercing soldier after soldier in a cavalry charge. Sabers continued to see battlefield use until the early 20th century. The US Navy kept tens of thousands of sturdy cutlasses in their armory well into World War II and many were issued to marines in the Pacific as jungle machetes.

Non-European weapons called "sword" include single-edged weapons such as the Middle Eastern scimitar, the Chinese dao and the related Japanese katana. The Chinese jian is an example of a non-European double-edged sword, like the European models derived from the double-edged Iron Age sword.

The first weapons that can be described as "swords" date to around 3300 BC. They have been found in Arslantepe, Turkey, are made from arsenical bronze, and are about long. Some of them are inlaid with silver.

The sword developed from the knife or dagger. A knife is unlike a dagger in that a knife has only one cutting surface, while a dagger has two cutting surfaces. When the construction of longer blades became possible, from the late 3rd millennium BC in the Middle East, first in arsenic copper, then in tin-bronze.

Blades longer than were rare and not practical until the late Bronze Age because the Young's modulus of bronze is relatively low, and consequently longer blades would bend easily. The development of the sword out of the dagger was gradual; the first weapons that can be classified as swords without any ambiguity are those found in Minoan Crete, dated to about 1700 BC, reaching a total length of more than 100 cm. These are the "type A" swords of the Aegean Bronze Age.

One of the most important, and longest-lasting, types swords of the European Bronze Age was the "Naue II" type (named for Julius Naue who first described them), also known as "Griffzungenschwert" (lit. "grip-tongue sword"). This type first appears in c. the 13th century BC in Northern Italy (or a general Urnfield background), and survives well into the Iron Age, with a life-span of about seven centuries. During its lifetime, metallurgy changed from bronze to iron, but not its basic design.

Naue II swords were exported from Europe to the Aegean, and as far afield as Ugarit, beginning about 1200 BC, i.e. just a few decades before the final collapse of the palace cultures in the Bronze Age collapse. Naue II swords could be as long as 85 cm, but most specimens fall into the 60 to 70 cm range. Robert Drews linked the Naue Type II Swords, which spread from Southern Europe into the Mediterranean, with the Bronze Age collapse. Naue II swords, along with Nordic full-hilted swords, were made with functionality and aesthetics in mind. The hilts of these swords were beautifully crafted and often contained false rivets in order to make the sword more visually appealing. Swords coming from northern Denmark and northern Germany usually contained three or more fake rivets in the hilt.

Sword production in China is attested from the Bronze Age Shang Dynasty. The technology for bronze swords reached its high point during the Warring States period and Qin Dynasty. Amongst the Warring States period swords, some unique technologies were used, such as casting high tin edges over softer, lower tin cores, or the application of diamond shaped patterns on the blade (see sword of Goujian). Also unique for Chinese bronzes is the consistent use of high tin bronze (17–21% tin) which is very hard and breaks if stressed too far, whereas other cultures preferred lower tin bronze (usually 10%), which bends if stressed too far. Although iron swords were made alongside bronze, it was not until the early Han period that iron completely replaced bronze.

In South Asia earliest available Bronze age swords of copper were discovered in the Harappan sites, in present-day Pakistan, and date back to 2300 BC. Swords have been recovered in archaeological findings throughout the Ganges-Jamuna Doab region of India, consisting of bronze but more commonly copper. Diverse specimens have been discovered in Fatehgarh, where there are several varieties of hilt. These swords have been variously dated to times between 1700–1400 BC, but were probably used more in the opening centuries of the 1st millennium BC.

Iron became increasingly common from the 13th century BC. Before that the use of swords was less frequent. The iron was not quench-hardened although often containing sufficient carbon, but work-hardened like bronze by hammering. This made them comparable or only slightly better in terms of strength and hardness to bronze swords. They could still bend during use rather than spring back into shape. But the easier production, and the better availability of the raw material for the first time permitted the equipment of entire armies with metal weapons, though Bronze Age Egyptian armies were sometimes fully equipped with bronze weapons.

Ancient swords are often found at burial sites. The sword was often placed on the right side of the corpse. Many times the sword was kept over the corpse. In many late Iron Age graves, the sword and the scabbard were bent at 180 degrees. It was known as killing the sword. Thus they might have considered swords as the most potent and powerful object.

By the time of Classical Antiquity and the Parthian and Sassanid Empires in Iran, iron swords were common. The Greek xiphos and the Roman gladius are typical examples of the type, measuring some . The late Roman Empire introduced the longer spatha (the term for its wielder, spatharius, became a court rank in Constantinople), and from this time, the term "longsword" is applied to swords comparatively long for their respective periods.

Swords from the Parthian and Sassanian Empires were quite long, the blades on some late Sassanian swords being just under a metre long.

Swords were also used to administer various physical punishments, such as non-surgical amputation or capital punishment by decapitation. The use of a sword, an honourable weapon, was regarded in Europe since Roman times as a privilege reserved for the nobility and the upper classes.

The Periplus of the Erythraean Sea mentions swords of Indian iron and steel being exported from India to Greece. Sri Lankan and Indian Blades made of Damascus steel also found their way into Persia.

In the first millennium BC the Persian armies used a sword that was originally of Scythian design called the akinaka (acinaces). However, the great conquests of the Persians made the sword more famous as a Persian weapon, to the extent that the true nature of the weapon has been lost somewhat as the name Akinaka has been used to refer to whichever form of sword the Persian army favoured at the time.

It is widely believed that the original akinaka was a 14 to 18 inch double-edged sword. The design was not uniform and in fact identification is made more on the nature of the scabbard than the weapon itself; the scabbard usually has a large, decorative mount allowing it to be suspended from a belt on the wearer’s right side. Because of this, it is assumed that the sword was intended to be drawn with the blade pointing downwards ready for surprise stabbing attacks.

In the 12th century, the Seljuq dynasty had introduced the curved shamshir to Persia, and this was in extensive use by the early 16th century.

Chinese steel swords made their first appearance in the later part of the Western Zhou Dynasty, but were not widely used until the 3rd century BC Han Dynasty.
The Chinese Dao (刀 pinyin dāo) is single-edged, sometimes translated as sabre or broadsword, and the Jian (劍 or 剑 pinyin jiàn) is double-edged. The zhanmadao (literally "horse chopping sword"), an extremely long, anti-cavalry sword from the Song dynasty era.

During the Middle Ages sword technology improved, and the sword became a very advanced weapon. It was frequently used by men in battle, particularly during an attack.
The spatha type remained popular throughout the Migration period and well into the Middle Ages. Vendel Age spathas were decorated with Germanic artwork (not unlike the Germanic bracteates fashioned after Roman coins). The Viking Age saw again a more standardized production, but the basic design remained indebted to the spatha.

Around the 10th century, the use of properly quenched hardened and tempered steel started to become much more common than in previous periods. The Frankish 'Ulfberht' blades (the name of the maker inlaid in the blade) were of particularly consistent high quality. Charles the Bald tried to prohibit the export of these swords, as they were used by Vikings in raids against the Franks.

Wootz steel which is also known as Damascus steel was a unique and highly prized steel developed on the Indian subcontinent as early as the 5th century BC. Its properties were unique due to the special smelting and reworking of the steel creating networks of iron carbides described as a globular cementite in a matrix of pearlite. The use of Damascus steel in swords became extremely popular in the 16th and 17th centuries.

It was only from the 11th century that Norman swords began to develop the crossguard (quillons). During the Crusades of the 12th to 13th century, this cruciform type of arming sword remained essentially stable, with variations mainly concerning the shape of the pommel. These swords were designed as cutting weapons, although effective points were becoming common to counter improvements in armour, especially the 14th-century change from mail to plate armour.

It was during the 14th century, with the growing use of more advanced armour, that the hand and a half sword, also known as a "bastard sword", came into being. It had an extended grip that meant it could be used with either one or two hands. Though these swords did not provide a full two-hand grip they allowed their wielders to hold a shield or parrying dagger in their off hand, or to use it as a two-handed sword for a more powerful blow.

In the Middle Ages, the sword was often used as a symbol of the word of God. The names given to many swords in mythology, literature, and history reflected the high prestige of the weapon and the wealth of the owner.

The earliest evidence of curved swords, or scimitars (and other regional variants as the Arabian saif, the Persian shamshir and the Turkic kilij) is from the 9th century, when it was used among soldiers in the Khurasan region of Persia.

As steel technology improved, single-edged weapons became popular throughout Asia. Derived from the Chinese Jian or dao, the Korean hwandudaedo are known from the early medieval Three Kingdoms. Production of the Japanese tachi, a precursor to the katana, is recorded from c. AD 900 (see Japanese sword).

Japan was famous for the swords it forged in the early 13th century for the class of warrior-nobility known as the Samurai. The types of swords used by the Samurai included the ōdachi (extra long field sword), tachi (long cavalry sword), katana (long sword), and wakizashi (shorter companion sword for katana). Japanese swords that pre-date the rise of the samurai caste include the tsurugi (straight double-edged blade) and chokutō (straight one-edged blade). Japanese swordmaking reached the height of its development in the 15th and 16th centuries, when samurai increasingly found a need for a sword to use in closer quarters, leading to the creation of the modern katana.

Western historians have said that Japanese katana were among the finest cutting weapons in world military history.

The "Khanda" is a double-edge straight sword. It is often featured in religious iconography, theatre and art depicting the ancient history of India. Some communities venerate the weapon as a symbol of Shiva. It is a common weapon in the martial arts in the Indian subcontinent. Khanda often appears in Hindu, Buddhist and Sikh scriptures and art. In Sri Lanka, a unique wind furnace was used to produce the high quality steel. This gave the blade a very hard cutting edge and beautiful patterns. For these reasons it became a very popular trading material.
The Urumi: (Tamil: சுருள் பட்டாக்கத்தி surul pattai, lit. curling blade; Sinhalese: එතුණු කඩුව ethunu kaduwa; Hindi: aara) is a longsword with a flexible whip-like blade from India. Originating in the country's southern states, it is thought to have existed as far back as the Maurya dynasty (322–185 BC). The urumi is considered one of the most difficult weapons to master due to the risk of injuring oneself. It is treated as a steel whip, and therefore requires prior knowledge of that weapon.

The Firangi: () derived from the Arabic term for a Western European a "Frank") was a sword type which used blades manufactured in Western Europe and imported by the Portuguese, or made locally in imitation of European blades. Because of its length the firangi is usually regarded as primarily a cavalry weapon. The sword has been especially associated with the Marathas, who were famed for their cavalry. However, the firangi was also widely used by Sikhs and Rajputs.

The Talwar: (Hindi: तलवार) is a type of curved sword from India and other countries of the Indian subcontinent, it was adopted by communities such as Rajputs, Sikhs and Marathas, who favored the sword as their main weapon. It became more widespread in the medieval era.

In Indonesia, the images of Indian style swords can be found in Hindu gods statues from ancient Java circa 8th to 10th century. However the native types of blade known as kris, parang, klewang and golok were more popular as weapons. These daggers are shorter than sword but longer than common dagger.
In The Philippines, traditional large swords known as the Kampilan and the Panabas were used in combat by the natives. A notable wielder of the "kampilan" was Lapu-Lapu, the king of Mactan and his warriors who defeated the Spaniards and killed Portuguese explorer Ferdinand Magellan at the Battle of Mactan on 27 April 1521. Traditional swords in the Philippines were immediately banned, but the training in swordsmanship was later hidden from the occupying Spaniards by practices in dances. But because of the banning, Filipinos were forced to use swords that were disguised as farm tools. Bolos and baliswords were used during the revolutions against the colonialists not only because ammunition for guns was scarce, but also for concealability while walking in crowded streets and homes. Bolos were also used by young boys who joined their parents in the revolution and by young girls and their mothers in defending the town while the men were on the battlefields. During the Philippine–American War in events such as the Balangiga Massacre, most of an American company was hacked to death or seriously injured by bolo-wielding guerillas in Balangiga, Samar. When the Japanese took control of the country, several American special operations groups stationed in the Philippines were introduced to the Filipino Martial Arts and swordsmanship, leading to this style reaching America despite the fact that natives were reluctant to allow outsiders in on their fighting secrets.

From around 1300 to 1500, in concert with improved armour, innovative sword designs evolved more and more rapidly. The main transition was the lengthening of the grip, allowing two-handed use, and a longer blade. By 1400, this type of sword, at the time called "langes Schwert" (longsword) or "spadone", was common, and a number of 15th- and 16th-century "Fechtbücher" offering instructions on their use survive. Another variant was the specialized armour-piercing swords of the estoc type. The longsword became popular due to its extreme reach and its cutting and thrusting abilities.

The estoc became popular because of its ability to thrust into the gaps between plates of armour. The grip was sometimes wrapped in wire or coarse animal hide to provide a better grip and to make it harder to knock a sword out of the user's hand.

A number of manuscripts covering longsword combat and techniques dating from the 13th–16th centuries exist in German, Italian, and English, providing extensive information on longsword combatives as used throughout this period. Many of these are now readily available online.

In the 16th century, the large zweihänder was used by the elite German and Swiss mercenaries known as doppelsöldners. Zweihänder, literally translated, means two-hander. The zweihänder possesses a long blade, as well as a huge guard for protection. It is estimated that some zweihänder swords were over long, with the one ascribed to Frisian warrior Pier Gerlofs Donia being long. The gigantic blade length was perfectly designed for manipulating and pushing away enemy pole-arms, which were major weapons around this time, in both Germany and Eastern Europe. Doppelsöldners also used katzbalgers, which means 'cat-gutter'. The katzbalger's S-shaped guard and blade made it perfect for bringing in when the fighting became too close to use a zweihänder.

Civilian use of swords became increasingly common during the late Renaissance, with duels being a preferred way to honourably settle disputes. 

The side-sword was a type of war sword used by infantry during the Renaissance of Europe. This sword was a direct descendant of the arming sword. Quite popular between the 16th and 17th centuries, they were ideal for handling the mix of armoured and unarmoured opponents of that time. A new technique of placing one's finger on the ricasso to improve the grip (a practice that would continue in the rapier) led to the production of hilts with a guard for the finger. This sword design eventually led to the development of the civilian rapier, but it was not replaced by it, and the side-sword continued to be used during the rapier's lifetime. As it could be used for both cutting and thrusting, the term cut and thrust sword is sometimes used interchangeably with side-sword. Also of note is that as rapiers became more popular, attempts were made to hybridize the blade, sacrificing the effectiveness found in each unique weapon design. These are still considered side-swords and are sometimes labeled "sword rapier" or "cutting rapier" by modern collectors.

Also of note, side-swords used in conjunction with bucklers became so popular that it caused the term swashbuckler to be coined. This word stems from the new fighting style of the side-sword and buckler which was filled with much "swashing and making a noise on the buckler".

Within the Ottoman Empire, the use of a curved sabre called the Yatagan started in the mid-16th century. It would become the weapon of choice for many in Turkey and the Balkans.

The sword in this time period was the most personal weapon, the most prestigious, and the most versatile for close combat, but it came to decline in military use as technology, such as the crossbow and firearms changed warfare. However, it maintained a key role in civilian self-defence.

A single-edged type of sidearm used by the Hussites was popularized in 16th-century Germany under its Czech name "Dusack", also known as "Säbel auf Teutsch gefasst" ("sabre fitted in the German manner"). A closely related weapon is the "schnepf" or Swiss sabre used in Early Modern Switzerland.

The cut-and-thrust mortuary sword was used after 1625 by cavalry during the English Civil War. This (usually) two-edged sword sported a half-basket hilt with a straight blade some 90–105 cm long. Later in the 17th century, the swords used by cavalry became predominantly single-edged. The so-called walloon sword ("épée wallone") was common in the Thirty Years' War and Baroque era. Its hilt was ambidextrous with shell-guards and knuckle-bow that inspired 18th century continental hunting hangers. Following their campaign in the Netherlands in 1672, the French began producing this weapon as their first regulation sword. Weapons of this design were also issued to the Swedish army from the time of Gustavus Adolphus until as late as the 1850s.

The rapier is believed to have evolved either from the Spanish "espada ropera" or from the swords of the Italian nobility somewhere in the later part of the 16th century. The rapier differed from most earlier swords in that it was not a military weapon but a primarily civilian sword. 
Both the rapier and the Italian schiavona developed the crossguard into a basket-shaped guard for hand protection. 
During the 17th and 18th centuries, the shorter smallsword became an essential fashion accessory in European countries and the New World, though in some places such as the Scottish Highlands large swords as the basket-hilted broadsword were preferred, and most wealthy men and military officers carried one slung from a belt. Both the smallsword and the rapier remained popular dueling swords well into the 18th century.

As the wearing of swords fell out of fashion, canes took their place in a gentleman's wardrobe. This developed to the gentlemen in the Victorian era to use the umbrella. Some examples of canes—those known as sword canes or swordsticks—incorporate a concealed blade. The French martial art "la canne" developed to fight with canes and swordsticks and has now evolved into a sport. The English martial art singlestick is very similar.
With the rise of the pistol duel, the duelling sword fell out of fashion long before the practice of duelling itself. By about 1770, English duelists enthusiastically adopted the pistol, and sword duels dwindled. However, the custom of duelling with epées persisted well into the 20th century in France. Such modern duels were not fought to the death, the duellists' aim was instead merely to draw blood from the opponent's sword arm.

Towards the end of its useful life, the sword served more as a weapon of self-defence than for use on the battlefield, and the military importance of swords steadily decreased during the Modern Age. Even as a personal sidearm, the sword began to lose its preeminence in the early 19th century, reflecting the development of reliable handguns.

However, swords were still used in combat, especially in Colonial Wars between native populations and Colonial Empires. For example, during the Aceh War the Acehnese Klewangs, a sword similar to the machete, proved very effective in close quarters combat with Dutch troops, leading the Royal Netherlands East Indies Army to adopt a heavy cutlass, also called klewang (very similar in appearance to the US Navy Model 1917 Cutlass) to counter it. Mobile troops armed with carbines and klewangs succeeded in suppressing Aceh resistance where traditional infantry with rifle and bayonet had failed. From that time on until the 1950s the Royal Dutch East Indies Army, Royal Dutch Army, Royal Dutch Navy and Dutch police used these cutlasses called Klewang.
Swords continued in general peacetime use by cavalry of most armies during the years prior to World War I. For example, the British Army formally adopted a completely new design of cavalry sword in 1908, almost the last change in British Army weapons before the outbreak of the war. At the outbreak of World War I infantry officers in all combatant armies still carried swords as part of their field equipment. On mobilization in August 1914 all serving British Army officers were required to have their swords sharpened as the only peacetime use of the weapon had been for saluting on parade. The high visibility and limited practical use of the sword however led to it being abandoned within weeks, although most cavalry continued to carry sabres throughout the War. It was not until the late 1920s and early 1930s that this historic weapon was finally discarded for all but ceremonial purposes by most remaining horse mounted regiments of Europe and the Americas.

In China troops used the long anti-cavalry Miao dao well into the Second Sino-Japanese War. The last units of British heavy cavalry switched to using armoured vehicles as late as 1938. Swords and other dedicated melee weapons were used occasionally by many countries during World War II, but typically as a secondary weapon as they were outclassed by coexisting firearms.

Swords are commonly worn as a ceremonial item by officers in many military and naval services throughout the world. Occasions to wear swords include any event in dress uniforms where the rank-and-file carry arms: parades, reviews, courts-martial, tattoos, and changes of command. They are also commonly worn for officers' weddings, and when wearing dress uniforms to church—although they are rarely actually worn in the church itself.

In the British forces they are also worn for any appearance at Court. In the United States, every Naval officer at or above the rank of Lieutenant Commander is required to own a sword, which can be prescribed for any formal outdoor ceremonial occasion; they are normally worn for changes of command and parades. For some Navy parades, cutlasses are issued to Petty Officers and Chief Petty Officers.

In the U.S. Marine Corps every officer must own a sword, which is prescribed for formal parades and other ceremonies where dress uniforms are worn and the rank-and-file are under arms. On these occasions depending on their billet, Marine Non-Commissioned Officers (E-6 and above) may also be required to carry swords, which have hilts of a pattern similar to U.S. Naval officers' swords but are actually sabres. The USMC Model 1859 NCO Sword is the longest continuously-issued edged weapon in the U.S. inventory

The Marine officer swords are of the Mameluke pattern which was adopted in 1825 in recognition of the Marines' key role in the capture of the Tripolitan city of Derna during the First Barbary War. Taken out of issue for approximately 20 years from 1855 until 1875, it was restored to service in the year of the Corps' centennial and has remained in issue since.

The production of replicas of historical swords originates with 19th-century historicism. Contemporary replicas can range from cheap factory produced look-alikes to exact recreations of individual artifacts, including an approximation of the historical production methods.

Some kinds of swords are still commonly used today as weapons, often as a side arm for military infantry. The Japanese katana, wakizashi and tanto are carried by some infantry and officers in Japan and other parts of Asia and the kukri is the official melee weapon for Nepal. Other swords in use today are the sabre, the scimitar, the shortsword and the machete.


The sword consists of the blade and the hilt.
The term "scabbard" applies to the cover for the sword blade when not in use.

There is considerable variation in the detailed design of sword blades. The diagram opposite shows a typical Medieval European sword.

Early iron blades have rounded points due to the limited metallurgy of the time. These were still effective for thrusting against lightly armoured opponents. As armour advanced, blades were made narrower, stiffer and sharply pointed to defeat the armour by thrusting.

Dedicated cutting blades are wide and thin, and often have grooves known as fullers which lighten the blade at the cost of some of the blade's stiffness. The edges of a cutting sword are almost parallel. Blades oriented for the thrust have thicker blades, sometimes with a distinct midrib for increased stiffness, with a strong taper and an acute point.
The geometry of a cutting sword blade allows for acute edge angles. It should be noted, however, that an edge with an acuter angle is more inclined to degrade quickly in combat situations than an edge with a more obtuse angle. Also, an acute edge angle is not the primary factor of a blade's sharpness.

The part of the blade between the center of percussion (CoP) and the point is called the "foible" (weak) of the blade, and that between the center of balance (CoB) and the hilt is the "forte" (strong). The section in between the CoP and the CoB is the "middle".

The "ricasso" or "shoulder" identifies a short section of blade immediately below the guard that is left completely unsharpened. Many swords have no ricasso. On some large weapons, such as the German "Zweihänder", a metal cover surrounded the ricasso, and a swordsman might grip it in one hand to wield the weapon more easily in close-quarter combat.
The ricasso normally bears the maker's mark.

The tang is the extension of the blade to which the hilt is fitted.

On Japanese blades, the maker's mark appears on the tang under the grip.

The hilt is the collective term for the parts allowing for the handling and control of the blade; these consist of the grip, the pommel, and a simple or elaborate guard, which in post-Viking Age swords could consist of only a crossguard (called a cruciform hilt or quillons). The pommel was originally designed as a stop to prevent the sword slipping from the hand. From around the 11th century onward it became a counterbalance to the blade, allowing a more fluid style of fighting.
It can also be used as a blunt instrument at close range, and its weight affects the centre of percussion. In later times a "sword knot" or "tassel" was sometimes added. By the 17th century, with the growing use of firearms and the accompanying decline in the use of armour, many rapiers and dueling swords had developed elaborate basket hilts, which protect the palm of the wielder and rendered the gauntlet obsolete.

In late medieval and Renaissance era European swords, a flap of leather called the "chappe" or "rain guard" was attached to a sword's crossguard at the base of the hilt to protect the mouth of the scabbard and prevent water from entering.

Common accessories to the sword include the scabbard, as well as the "sword belt".


Sword typology is based on morphological criteria on one hand (blade shape (cross-section, taper, and length), shape and size of the hilt and pommel)
and age and place of origin on the other (Bronze Age, Iron Age, European (medieval, early modern, modern), Asian).

The relatively comprehensive Oakeshott typology was created by historian and illustrator Ewart Oakeshott as a way to define and catalogue European swords of the medieval period based on physical form, including blade shape and hilt configuration. The typology also focuses on the smaller, and in some cases contemporary, single-handed swords such as the arming sword.

As noted above, the terms longsword, broad sword, great sword, and Gaelic claymore are used relative to the era under consideration, and each term designates a particular type of sword.

In most Asian countries, a sword (jian 劍, geom (검), ken/tsurugi (剣), pedang) is a double-edged straight-bladed weapon, while a knife or saber (dāo 刀, do (도), to/katana (刀), pisau, golok) refers to a single-edged object.

In Sikh history, the sword is held in very high esteem. A single-edged sword is called a kirpan, and its double-edged counterpart a khanda or tega.

The South Indian "churika" is a handheld double-edged sword traditionally used in the Malabar region of Kerala. It is also worshipped as the weapon of Vettakkorumakan, the hunter god in Hinduism.

European terminology does give generic names for single-edged and double-edged blades but refers to specific types with the term 'sword' covering them all. For example, the backsword may be so called because it is single-edged but the falchion which is also single-edged is given its own specific name.

Two-handed sword may be used to refer to any sword that usually requires two hands to wield. However, in its proper sense it should be used only to refer to the very large swords of the 16th century.

Throughout history two-handed swords have generally been less common than their one-handed counterparts, one exception being their common use in Japan.

A Hand and a half sword, colloquially known as a "bastard sword", was a sword with an extended grip and sometimes pommel so that it could be used with either one or two hands. Although these swords may not provide a full two-hand grip, they allowed its wielders to hold a shield or parrying dagger in their off hand, or to use it as a two-handed sword for a more powerful blow. These should not be confused with a longsword, two-handed sword, or Zweihänder, which were always intended to be used with two hands.

In fantasy, magic swords often appear, based on their use in myth and legend. The science fiction counterpart to these is known as an energy sword (sometimes also referred to as a "beam sword" or "laser sword"), a sword whose blade consists of, or is augmented by, concentrated energy. A well known example of this type of sword is the lightsaber, shown in the "Star Wars" franchise.






</doc>
<doc id="27865" url="https://en.wikipedia.org/wiki?curid=27865" title="Surface (topology)">
Surface (topology)

In topology and differential geometry, a surface is a two-dimensional manifold, and, as such, may be an "abstract surface" not embedded in any Euclidean space. For example, the Klein bottle is a surface, which cannot be represented in the three-dimensional Euclidean space without introducing self-intersections (it cannot be embedded in the three dimensional Euclidean space).

In mathematics, a surface is a geometrical shape that resembles a deformed plane. The most familiar examples arise as boundaries of solid objects in ordinary three-dimensional Euclidean space R, such as spheres. The exact definition of a surface may depend on the context. Typically, in algebraic geometry, a surface may cross itself (and may have other singularities), while, in topology and differential geometry, it may not.

A surface is a two-dimensional space; this means that a moving point on a surface may move in two directions (it has two degrees of freedom). In other words, around almost every point, there is a "coordinate patch" on which a two-dimensional coordinate system is defined. For example, the surface of the Earth resembles (ideally) a two-dimensional sphere, and latitude and longitude provide two-dimensional coordinates on it (except at the poles and along the 180th meridian).

The concept of surface is widely used in physics, engineering, computer graphics, and many other disciplines, primarily in representing the surfaces of physical objects. For example, in analyzing the aerodynamic properties of an airplane, the central consideration is the flow of air along its surface.

A "(topological) surface" is a topological space in which every point has an open neighbourhood homeomorphic to some open subset of the Euclidean plane E. Such a neighborhood, together with the corresponding homeomorphism, is known as a "(coordinate) chart". It is through this chart that the neighborhood inherits the standard coordinates on the Euclidean plane. These coordinates are known as "local coordinates" and these homeomorphisms lead us to describe surfaces as being "locally Euclidean".

In most writings on the subject, it is often assumed, explicitly or implicitly, that as a topological space a surface is also nonempty, second countable, and Hausdorff. It is also often assumed that the surfaces under consideration are connected.

The rest of this article will assume, unless specified otherwise, that a surface is nonempty, Hausdorff, second countable, and connected.

More generally, a "(topological) surface with boundary" is a Hausdorff topological space in which every point has an open neighbourhood homeomorphic to some open subset of the closure of the upper half-plane H in C. These homeomorphisms are also known as "(coordinate) charts". The boundary of the upper half-plane is the "x"-axis. A point on the surface mapped via a chart to the "x"-axis is termed a "boundary point". The collection of such points is known as the "boundary" of the surface which is necessarily a one-manifold, that is, the union of closed curves. On the other hand, a point mapped to above the "x"-axis is an "interior point". The collection of interior points is the "interior" of the surface which is always non-empty. The closed disk is a simple example of a surface with boundary. The boundary of the disc is a circle.

The term "surface" used without qualification refers to surfaces without boundary. In particular, a surface with empty boundary is a surface in the usual sense. A surface with empty boundary which is compact is known as a 'closed' surface. The two-dimensional sphere, the two-dimensional torus, and the real projective plane are examples of closed surfaces.

The Möbius strip is a surface on which the distinction between clockwise and counterclockwise can be defined locally, but not globally. In general, a surface is said to be "orientable" if it does not contain a homeomorphic copy of the Möbius strip; intuitively, it has two distinct "sides". For example, the sphere and torus are orientable, while the real projective plane is not (because the real projective plane with one point removed is homeomorphic to the open Möbius strip).

In differential and algebraic geometry, extra structure is added upon the topology of the surface. This added structures can be a smoothness structure (making it possible to define differentiable maps to and from the surface), a Riemannian metric (making it possible to define length and angles on the surface), a complex structure (making it possible to define holomorphic maps to and from the surface — in which case the surface is called a Riemann surface), or an algebraic structure (making it possible to detect singularities, such as self-intersections and cusps, that cannot be described solely in terms of the underlying topology).

Historically, surfaces were initially defined as subspaces of Euclidean spaces. Often, these surfaces were the locus of zeros of certain functions, usually polynomial functions. Such a definition considered the surface as part of a larger (Euclidean) space, and as such was termed "extrinsic".

In the previous section, a surface is defined as a topological space with certain properties, namely Hausdorff and locally Euclidean. This topological space is not considered a subspace of another space. In this sense, the definition given above, which is the definition that mathematicians use at present, is "intrinsic".

A surface defined as intrinsic is not required to satisfy the added constraint of being a subspace of Euclidean space. It may seem possible for some surfaces defined intrinsically to not be surfaces in the extrinsic sense. However, the Whitney embedding theorem asserts every surface can in fact be embedded homeomorphically into Euclidean space, in fact into E: The extrinsic and intrinsic approaches turn out to be equivalent.

In fact, any compact surface that is either orientable or has a boundary can be embedded in E; on the other hand, the real projective plane, which is compact, non-orientable and without boundary, cannot be embedded into E (see Gramain). Steiner surfaces, including Boy's surface, the Roman surface and the cross-cap, are models of the real projective plane in E, but only the Boy surface is an immersed surface. All these models are singular at points where they intersect themselves.

The Alexander horned sphere is a well-known pathological embedding of the two-sphere into the three-sphere.
The chosen embedding (if any) of a surface into another space is regarded as extrinsic information; it is not essential to the surface itself. For example, a torus can be embedded into E in the "standard" manner (which looks like a bagel) or in a knotted manner (see figure). The two embedded tori are homeomorphic, but not isotopic: They are topologically equivalent, but their embeddings are not.

The image of a continuous, injective function from R to higher-dimensional R is said to be a parametric surface. Such an image is so-called because the "x"- and "y"- directions of the domain R are 2 variables that parametrize the image. A parametric surface need not be a topological surface. A surface of revolution can be viewed as a special kind of parametric surface.

If "f" is a smooth function from R to R whose gradient is nowhere zero, then the locus of zeros of "f" does define a surface, known as an "implicit surface". If the condition of non-vanishing gradient is dropped, then the zero locus may develop singularities.
Each closed surface can be constructed from an oriented polygon with an even number of sides, called a fundamental polygon of the surface, by pairwise identification of its edges. For example, in each polygon below, attaching the sides with matching labels ("A" with "A", "B" with "B"), so that the arrows point in the same direction, yields the indicated surface.

Any fundamental polygon can be written symbolically as follows. Begin at any vertex, and proceed around the perimeter of the polygon in either direction until returning to the starting vertex. During this traversal, record the label on each edge in order, with an exponent of -1 if the edge points opposite to the direction of traversal. The four models above, when traversed clockwise starting at the upper left, yield

Note that the sphere and the projective plane can both be realized as quotients of the 2-gon, while the torus and Klein bottle require a 4-gon (square).

The expression thus derived from a fundamental polygon of a surface turns out to be the sole relation in a presentation of the fundamental group of the surface with the polygon edge labels as generators. This is a consequence of the Seifert–van Kampen theorem.

Gluing edges of polygons is a special kind of quotient space process. The quotient concept can be applied in greater generality to produce new or alternative constructions of surfaces. For example, the real projective plane can be obtained as the quotient of the sphere by identifying all pairs of opposite points on the sphere. Another example of a quotient is the connected sum.

The connected sum of two surfaces "M" and "N", denoted "M" # "N", is obtained by removing a disk from each of them and gluing them along the boundary components that result. The boundary of a disk is a circle, so these boundary components are circles. The Euler characteristic formula_5 of is the sum of the Euler characteristics of the summands, minus two:

The sphere S is an identity element for the connected sum, meaning that . This is because deleting a disk from the sphere leaves a disk, which simply replaces the disk deleted from "M" upon gluing.

Connected summation with the torus T is also described as attaching a "handle" to the other summand "M". If "M" is orientable, then so is . The connected sum is associative, so the connected sum of a finite collection of surfaces is well-defined.

The connected sum of two real projective planes, , is the Klein bottle K. The connected sum of the real projective plane and the Klein bottle is homeomorphic to the connected sum of the real projective plane with the torus; in a formula, . Thus, the connected sum of three real projective planes is homeomorphic to the connected sum of the real projective plane with the torus. Any connected sum involving a real projective plane is nonorientable.

A closed surface is a surface that is compact and without boundary. Examples are spaces like the sphere, the torus and the Klein bottle. Examples of non-closed surfaces are: an open disk, which is a sphere with a puncture; a cylinder, which is a sphere with two punctures; and the Möbius strip. As with any closed manifold, a surface embedded in Euclidean space that is closed with respect to the inherited Euclidean topology is "not" necessarily a closed surface; for example, a disk embedded in formula_7 that contains its boundary is a surface that is topologically closed, but not a closed surface.

The "classification theorem of closed surfaces" states that any connected closed surface is homeomorphic to some member of one of these three families:

The surfaces in the first two families are orientable. It is convenient to combine the two families by regarding the sphere as the connected sum of 0 tori. The number "g" of tori involved is called the "genus" of the surface. The sphere and the torus have Euler characteristics 2 and 0, respectively, and in general the Euler characteristic of the connected sum of "g" tori is .

The surfaces in the third family are nonorientable. The Euler characteristic of the real projective plane is 1, and in general the Euler characteristic of the connected sum of "k" of them is .

It follows that a closed surface is determined, up to homeomorphism, by two pieces of information: its Euler characteristic, and whether it is orientable or not. In other words, Euler characteristic and orientability completely classify closed surfaces up to homeomorphism.

Closed surfaces with multiple connected components are classified by the class of each of their connected components, and thus one generally assumes that the surface is connected.

Relating this classification to connected sums, the closed surfaces up to homeomorphism form a commutative monoid under the operation of connected sum, as indeed do manifolds of any fixed dimension. The identity is the sphere, while the real projective plane and the torus generate this monoid, with a single relation , which may also be written , since . This relation is sometimes known as ' after Walther von Dyck, who proved it in , and the triple cross surface is accordingly called '.

Geometrically, connect-sum with a torus () adds a handle with both ends attached to the same side of the surface, while connect-sum with a Klein bottle () adds a handle with the two ends attached to opposite sides of an orientable surface; in the presence of a projective plane (), the surface is not orientable (there is no notion of side), so there is no difference between attaching a torus and attaching a Klein bottle, which explains the relation.

Compact surfaces, possibly with boundary, are simply closed surfaces with a finite number of holes (open discs that have been removed). Thus, a connected compact surface is classified by the number of boundary components and the genus of the corresponding closed surface – equivalently, by the number of boundary components, the orientability, and Euler characteristic. The genus of a compact surface is defined as the genus of the corresponding closed surface.

This classification follows almost immediately from the classification of closed surfaces: removing an open disc from a closed surface yields a compact surface with a circle for boundary component, and removing "k" open discs yields a compact surface with "k" disjoint circles for boundary components. The precise locations of the holes are irrelevant, because the homeomorphism group acts "k"-transitively on any connected manifold of dimension at least 2.

Conversely, the boundary of a compact surface is a closed 1-manifold, and is therefore the disjoint union of a finite number of circles; filling these circles with disks (formally, taking the cone) yields a closed surface.

The unique compact orientable surface of genus "g" and with "k" boundary components is often denoted formula_10 for example in the study of the mapping class group.

A closely related example to the classification of compact 2-manifolds is the classification of compact Riemann surfaces, i.e., compact complex 1-manifolds. (Note that the 2-sphere and the torus are both complex manifolds, in fact algebraic varieties.) Since every complex manifold is orientable, the connected sums of projective planes are not complex manifolds. Thus, compact Riemann surfaces are characterized topologically simply by their genus. The genus counts the number of holes in the manifold: the sphere has genus 0, the one-holed torus genus 1, etc.

Non-compact surfaces are more difficult to classify. As a simple example, a non-compact surface can be obtained by puncturing (removing a finite set of points from) a closed manifold. On the other hand, any open subset of a compact surface is itself a non-compact surface; consider, for example, the complement of a Cantor set in the sphere, otherwise known as the Cantor tree surface. However, not every non-compact surface is a subset of a compact surface; two canonical counterexamples are the Jacob's ladder and the Loch Ness monster, which are non-compact surfaces with infinite genus.

A non-compact surface M has a non-empty space of ends E(M), which informally speaking describes the ways that the surface "goes off to infinity". The space E(M) is always topologically equivalent to a closed subspace of the Cantor set. M may have a finite or countably infinite number N of handles, as well as a finite or countably infinite number N of projective planes. If both N and N are finite, then these two numbers, and the topological type of space of ends, classify the surface M up to topological equivalence. If either or both of N and N is infinite, then the topological type of M depends not only on these two numbers but also on how the infinite one(s) approach the space of ends. In general the topological type of M is determined by the four subspaces of E(M) that are limit points of infinitely many handles and infinitely many projective planes, limit points of only handles, limit points of only projective planes, and limit points of neither.

There exist (necessarily non-compact) topological surfaces having no countable base for their topology. Perhaps the simplest example is the cartesian product of the long line with the space of real numbers.

Another surface having no countable base for its topology, but "not" requiring the Axiom of Choice to prove its existence, is the Prüfer manifold, which can be described by simple equations that show it to be a real-analytic surface. The Prüfer manifold may be thought of as the upper half plane together with one additional "tongue" "T" hanging down from it directly below the point ("x",0), for each real "x".

In 1925, Tibor Radó proved the theorem that non-compact Riemann surfaces (i.e., one-dimensional complex manifolds) are necessarily second countable. By contrast, the existence of the Prüfer surface shows that there exist two-dimensional complex manifolds (which are necessarily 4-dimensional real manifolds) with no countable base. (This is because any "n"-real-dimensional real-analytic manifold "Q" can be extended to an "n"-complex-dimensional complex manifold "W" that contains "Q" as a real-analytic submanifold.)

The classification of closed surfaces has been known since the 1860s, and today a number of proofs exist.

Topological and combinatorial proofs in general rely on the difficult result that every compact 2-manifold is homeomorphic to a simplicial complex, which is of interest in its own right. The most common proof of the classification is , which brings every triangulated surface to a standard form. A simplified proof, which avoids a standard form, was discovered by John H. Conway circa 1992, which he called the "Zero Irrelevancy Proof" or "ZIP proof" and is presented in .

A geometric proof, which yields a stronger geometric result, is the uniformization theorem. This was originally proven only for Riemann surfaces in the 1880s and 1900s by Felix Klein, Paul Koebe, and Henri Poincaré.

Polyhedra, such as the boundary of a cube, are among the first surfaces encountered in geometry. It is also possible to define "smooth surfaces", in which each point has a neighborhood diffeomorphic to some open set in E. This elaboration allows calculus to be applied to surfaces to prove many results.

Two smooth surfaces are diffeomorphic if and only if they are homeomorphic. (The analogous result does not hold for higher-dimensional manifolds.) Thus closed surfaces are classified up to diffeomorphism by their Euler characteristic and orientability.

Smooth surfaces equipped with Riemannian metrics are of foundational importance in differential geometry. A Riemannian metric endows a surface with notions of geodesic, distance, angle, and area. It also gives rise to Gaussian curvature, which describes how curved or bent the surface is at each point. Curvature is a rigid, geometric property, in that it is not preserved by general diffeomorphisms of the surface. However, the famous Gauss–Bonnet theorem for closed surfaces states that the integral of the Gaussian curvature "K" over the entire surface "S" is determined by the Euler characteristic:
This result exemplifies the deep relationship between the geometry and topology of surfaces (and, to a lesser extent, higher-dimensional manifolds).

Another way in which surfaces arise in geometry is by passing into the complex domain. A complex one-manifold is a smooth oriented surface, also called a Riemann surface. Any complex nonsingular algebraic curve viewed as a complex manifold is a Riemann surface.

Every closed orientable surface admits a complex structure. Complex structures on a closed oriented surface correspond to conformal equivalence classes of Riemannian metrics on the surface. One version of the uniformization theorem (due to Poincaré) states that any Riemannian metric on an oriented, closed surface is conformally equivalent to an essentially unique metric of constant curvature. This provides a starting point for one of the approaches to Teichmüller theory, which provides a finer classification of Riemann surfaces than the topological one by Euler characteristic alone.

A "complex surface" is a complex two-manifold and thus a real four-manifold; it is not a surface in the sense of this article. Neither are algebraic curves defined over fields other than the complex numbers,
nor are algebraic surfaces defined over fields other than the real numbers.





</doc>
<doc id="27871" url="https://en.wikipedia.org/wiki?curid=27871" title="Stephen III">
Stephen III

Stephen III may refer to:


</doc>
<doc id="27872" url="https://en.wikipedia.org/wiki?curid=27872" title="Stephen II">
Stephen II

Stephen II may refer to:



</doc>
<doc id="27873" url="https://en.wikipedia.org/wiki?curid=27873" title="Surjective function">
Surjective function

In mathematics, a function "f" from a set "X" to a set "Y" is surjective (or onto), or a surjection, if for every element "y" in the codomain "Y" of "f" there is at least one element "x" in the domain "X" of "f" such that "f"("x") = "y". It is not required that "x" is unique; the function "f" may map one or more elements of "X" to the same element of "Y".

The term "surjective" and the related terms "injective" and "bijective" were introduced by Nicolas Bourbaki, a group of mainly French 20th-century mathematicians who under this pseudonym wrote a series of books presenting an exposition of modern advanced mathematics, beginning in 1935. The French prefix "sur" means "over" or "above" and relates to the fact that the image of the domain of a surjective function completely covers the function's codomain.

Any function induces a surjection by restricting its codomain to its range. Every surjective function has a right inverse, and every function with a right inverse is necessarily a surjection. The composite of surjective functions is always surjective. Any function can be decomposed into a surjection and an injection.

A surjective function is a function whose image is equal to its codomain. Equivalently, a function "f" with domain "X" and codomain "Y" is surjective if for every "y" in "Y" there exists at least one "x" in "X" with formula_1. Surjections are sometimes denoted by a two-headed rightwards arrow (), as in "f" : "X" ↠ "Y".

Symbolically,

For any set "X", the identity function id on "X" is surjective.

The function defined by "f"("n") = "n" mod 2 (that is, even integers are mapped to 0 and odd integers to 1) is surjective.

The function defined by "f"("x") = 2"x" + 1 is surjective (and even bijective), because for every real number "y" we have an "x" such that "f"("x") = "y": an appropriate "x" is ("y" − 1)/2.

The function defined by "f"("x") = "x" − 3"x" is surjective, because the pre-image of any real number "y" is the solution set of the cubic polynomial equation "x" − 3"x" − "y" = 0 and every cubic polynomial with real coefficients has at least one real root. However, this function is not injective (and hence not bijective) since e.g. the pre-image of "y" = 2 is {"x" = −1, "x" = 2}. (In fact, the pre-image of this function for every "y", −2 ≤ "y" ≤ 2 has more than one element.)

The function defined by "g"("x") = "x" is "not" surjective, because there is no real number "x" such that "x" = −1. However, the function defined by "g"("x") = "x" (with restricted codomain) "is" surjective because for every "y" in the nonnegative real codomain "Y" there is at least one "x" in the real domain "X" such that "x" = "y".

The natural logarithm function is a surjective and even bijective mapping from the set of positive real numbers to the set of all real numbers. Its inverse, the exponential function, is not surjective as its range is the set of positive real numbers and its domain is usually defined to be the set of all real numbers. The matrix exponential is not surjective when seen as a map from the space of all "n"×"n" matrices to itself. It is, however, usually defined as a map from the space of all "n"×"n" matrices to the general linear group of degree "n", i.e. the group of all "n"×"n" invertible matrices. Under this definition the matrix exponential is surjective for complex matrices, although still not surjective for real matrices.

The projection from a cartesian product to one of its factors is surjective unless the other factor is empty.

In a 3D video game, vectors are projected onto a 2D flat screen by means of a surjective function.

A function is bijective if and only if it is both surjective and injective.

If (as is often done) a function is identified with its graph, then surjectivity is not a property of the function itself, but rather a relationship between the function and its codomain. Unlike injectivity, surjectivity cannot be read off of the graph of the function alone.

The function is said to be a right inverse of the function if "f"("g"("y")) = "y" for every "y" in "Y" ("g" can be undone by "f"). In other words, "g" is a right inverse of "f" if the composition of "g" and "f" in that order is the identity function on the domain "Y" of "g". The function "g" need not be a complete inverse of "f" because the composition in the other order, , may not be the identity function on the domain "X" of "f". In other words, "f" can undo or ""reverse"" "g", but cannot necessarily be reversed by it.

Every function with a right inverse is necessarily a surjection. The proposition that every surjective function has a right inverse is equivalent to the axiom of choice.

If is surjective and "B" is a subset of "Y", then "f"("f"("B")) = "B". Thus, "B" can be recovered from its preimage .

For example, in the first illustration, above, there is some function "g" such that "g"("C") = 4. There is also some function "f" such that "f"(4) = "C". It doesn't matter that "g"("C") can also equal 3; it only matters that "f" "reverses" "g".

A function is surjective if and only if it is right-cancellative: given any functions , whenever "g" "f" = "h" "f", then "g" = "h". This property is formulated in terms of functions and their composition and can be generalized to the more general notion of the morphisms of a category and their composition. Right-cancellative morphisms are called epimorphisms. Specifically, surjective functions are precisely the epimorphisms in the category of sets. The prefix "epi" is derived from the Greek preposition "ἐπί" meaning "over", "above", "on".

Any morphism with a right inverse is an epimorphism, but the converse is not true in general. A right inverse "g" of a morphism "f" is called a section of "f". A morphism with a right inverse is called a split epimorphism.

Any function with domain "X" and codomain "Y" can be seen as a left-total and right-unique binary relation between "X" and "Y" by identifying it with its function graph. A surjective function with domain "X" and codomain "Y" is then a binary relation between "X" and "Y" that is right-unique and both left-total and right-total.

The cardinality of the domain of a surjective function is greater than or equal to the cardinality of its codomain: If is a surjective function, then "X" has at least as many elements as "Y", in the sense of cardinal numbers. (The proof appeals to the axiom of choice to show that a function 

Specifically, if both "X" and "Y" are finite with the same number of elements, then is surjective if and only if "f" is injective.

Given two sets "X" and "Y", the notation is used to say that either "X" is empty or that there is a surjection from "Y" onto "X". Using the axiom of choice one can show that and together imply that |"Y"| = |"X"|, a variant of the Schröder–Bernstein theorem.

The composite of surjective functions is always surjective: If "f" and "g" are both surjective, and the codomain of "g" is equal to the domain of "f", then is surjective. Conversely, if is surjective, then "f" is surjective (but "g", the function applied first, need not be). These properties generalize from surjections in the category of sets to any epimorphisms in any category.

Any function can be decomposed into a surjection and an injection: For any function there exist a surjection and an injection such that "h" = "g" "f". To see this, define "Y" to be the set of preimages where "z" is in . These preimages are disjoint and partition "X". Then "f" carries each "x" to the element of "Y" which contains it, and "g" carries each element of "Y" to the point in "Z" to which "h" sends its points. Then "f" is surjective since it is a projection map, and "g" is injective by definition.

Any function induces a surjection by restricting its codomain to its range. Any surjective function induces a bijection defined on a quotient of its domain by collapsing all arguments mapping to a given fixed image. More precisely, every surjection can be factored as a projection followed by a bijection as follows. Let "A"/~ be the equivalence classes of "A" under the following equivalence relation: "x" ~ "y" if and only if "f"("x") = "f"("y"). Equivalently, "A"/~ is the set of all preimages under "f". Let "P"(~) : "A" → "A"/~ be the projection map which sends each "x" in "A" to its equivalence class ["x"], and let "f" : "A"/~ → "B" be the well-defined function given by "f"(["x"]) = "f"("x"). Then "f" = "f" o "P"(~).



</doc>
<doc id="27875" url="https://en.wikipedia.org/wiki?curid=27875" title="Stephen Jay Gould">
Stephen Jay Gould

Stephen Jay Gould (; September 10, 1941 – May 20, 2002) was an American paleontologist, evolutionary biologist, and historian of science. He was also one of the most influential and widely read writers of popular science of his generation. Gould spent most of his career teaching at Harvard University and working at the American Museum of Natural History in New York. In 1996, Gould was hired as the Vincent Astor Visiting Research Professor of Biology at New York University, where he divided his time teaching there and at Harvard.

Gould's most significant contribution to evolutionary biology was the theory of punctuated equilibrium, which he developed with Niles Eldredge in 1972. The theory proposes that most evolution is characterized by long periods of evolutionary stability, which is infrequently punctuated by swift periods of branching speciation. The theory was contrasted against phyletic gradualism, the popular idea that evolutionary change is marked by a pattern of smooth and continuous change in the fossil record.

Most of Gould's empirical research was based on the land snail genera "Poecilozonites" and "Cerion". He also made important contributions to evolutionary developmental biology, receiving professional recognition for his book "Ontogeny and Phylogeny". In evolutionary theory he opposed strict selectionism, sociobiology as applied to humans, and evolutionary psychology. He campaigned against creationism and proposed that science and religion should be considered two distinct fields (or "non-overlapping magisteria") whose authorities do not overlap.

Gould was known by the general public mainly for his 300 popular essays in "Natural History" magazine, and his numerous books written for both the specialist and non-specialist.
In April 2000, the US Library of Congress named him a "Living Legend".

Stephen Jay Gould was born in Queens, New York on September 10, 1941. His father Leonard was a court stenographer and a World War II veteran in the United States Navy. His mother Eleanor was an artist, whose parents were Jewish immigrants living and working in the city's Garment District. Gould and his younger brother Peter were raised in Bayside, a middle class neighborhood in the northeastern section of Queens. He attended and graduated from Jamaica High School.

When Gould was five years old his father took him to the Hall of Dinosaurs in the American Museum of Natural History, where he first encountered "Tyrannosaurus rex". "I had no idea there were such things—I was awestruck," Gould once recalled. It was in that moment that he decided to become a paleontologist.

Raised in a secular Jewish home, Gould did not formally practice religion and preferred to be called an agnostic. When asked directly if he was an agnostic in "Skeptic" magazine, he responded:
Though he "had been brought up by a Marxist father" he stated that his father's politics were "very different" from his own. In describing his own political views, he has said they "tend to the left of center." According to Gould the most influential political books he read were C. Wright Mills' "The Power Elite" and the political writings of Noam Chomsky.

While attending Antioch College in the early 1960s, Gould was active in the civil rights movement and often campaigned for social justice. When he attended the University of Leeds as a visiting undergraduate, he organized weekly demonstrations outside a Bradford dance hall which refused to admit black people. Gould continued these demonstrations until the policy was revoked. Throughout his career and writings, he spoke out against cultural oppression in all its forms, especially what he saw as the pseudoscience used in the service of racism and sexism.

Interspersed throughout his scientific essays for "Natural History" magazine, Gould frequently referred to his nonscientific interests and pastimes. As a boy he collected baseball cards and remained an avid New York Yankees fan throughout his life. As an adult he was fond of science fiction movies, but often deplored their poor storytelling and presentation of science. His other interests included singing baritone in the Boston Cecilia, and he was a great aficionado of Gilbert and Sullivan operas. He collected rare antiquarian books, possessed an enthusiasm for architecture, and delighted in city walks. He often traveled to Europe, and spoke French, German, Russian, and Italian. He sometimes alluded ruefully to his tendency to put on weight.

Gould married artist Deborah Lee on October 3, 1965. Gould met Lee while they were students together at Antioch College. They had two sons, Jesse and Ethan, and were married for 30 years. His second marriage in 1995 was to artist and sculptor Rhonda Roland Shearer.

In July 1982 Gould was diagnosed with peritoneal mesothelioma, a deadly form of cancer affecting the abdominal lining (the peritoneum). This cancer is frequently found in people who have ingested or inhaled asbestos fibers, a mineral which was used in the construction of Harvard's Museum of Comparative Zoology. After a difficult two-year recovery, Gould published a column for "Discover" magazine titled "The Median Isn't the Message", which discusses his stunned reaction to discovering that, "mesothelioma is incurable, with a median mortality of only eight months after discovery." He then describes the actual significance behind this number, and his relief upon recognizing that statistical averages are merely useful abstractions, and by themselves do not encompass "our actual world of variation, shadings, and continua."

The median is the halfway point, which means that 50% of people will die before eight months, but the other half will live longer, potentially much longer. He then needed to determine where his individual characteristics placed him within this range. Given that his cancer was detected early, he was young, optimistic, and had the best treatments available, Gould reasoned that he likely fell within the favorable tail of a right skewed distribution. After an experimental treatment of radiation, chemotherapy, and surgery, Gould made a full recovery, and his column became a source of comfort for many cancer patients.

Gould was also an advocate of medical cannabis. When undergoing his cancer treatments he smoked marijuana to help alleviate the long periods of intense and uncontrollable nausea. According to Gould, the drug had a "most important effect" on his eventual recovery. He later complained that he could not understand how "any humane person would withhold such a beneficial substance from people in such great need simply because others use it for different purposes." On August 5, 1998, Gould's testimony assisted in the successful lawsuit of HIV activist Jim Wakeford, who sued the Government of Canada for the right to cultivate, possess, and use marijuana for medical purposes.

In February 2002, a lesion was found on Gould's chest radiograph, and oncologists diagnosed him with fourth-stage cancer. Gould died 10 weeks later on May 20, 2002 from a metastatic adenocarcinoma of the lung, an aggressive form of cancer which had already spread to his brain, liver, and spleen. This cancer was unrelated to his previous bout of abdominal cancer in 1982. He died in his home "in a bed set up in the library of his SoHo loft, surrounded by his wife Rhonda, his mother Eleanor, and the many books he loved."

Gould began his higher education at Antioch College, graduating with a double major in geology and philosophy in 1963. During this time, he also studied at the University of Leeds in the United Kingdom. After completing graduate work at Columbia University in 1967 under the guidance of Norman Newell, he was immediately hired by Harvard University where he worked until the end of his life (1967–2002). In 1973, Harvard promoted him to professor of geology and curator of invertebrate paleontology at the institution's Museum of Comparative Zoology.

In 1982 Harvard awarded him the title of Alexander Agassiz Professor of Zoology. The following year, 1983, he was awarded a fellowship at the American Association for the Advancement of Science, where he later served as president (1999–2001). The AAAS news release cited his "numerous contributions to both scientific progress and the public understanding of science." He also served as president of the Paleontological Society (1985–1986) and of the Society for the Study of Evolution (1990–1991).

In 1989 Gould was elected into the body of the National Academy of Sciences. Through 1996–2002 Gould was Vincent Astor Visiting Research Professor of Biology at New York University. In 2001, the American Humanist Association named him the Humanist of the Year for his lifetime of work. In 2008, he was posthumously awarded the Darwin-Wallace Medal, along with 12 other recipients. (Until 2008, this medal had been awarded every 50 years by the Linnean Society of London.)

Early in his career, Gould and his colleague Niles Eldredge developed the theory of punctuated equilibrium, which describes the rate of speciation in the fossil record as occurring relatively rapidly, which then alternates to a longer period of evolutionary stability. It was Gould who coined the term "punctuated equilibria" though the theory was originally presented by Eldredge in his doctoral dissertation on Devonian trilobites and his article published the previous year on allopatric speciation.

According to Gould, punctuated equilibrium revised a key pillar "in the central logic of Darwinian theory." Some evolutionary biologists have argued that while punctuated equilibrium was "of great interest to biology generally," it merely modified neo-Darwinism in a manner that was fully compatible with what had been known before.
Other biologists emphasize the theoretical novelty of punctuated equilibrium, and argued that evolutionary stasis had been "unexpected by most evolutionary biologists" and "had a major impact on paleontology and evolutionary biology."

Comparisons were made to George Gaylord Simpson's work in "Tempo and Mode in Evolution" (1941). However Simpson describes the paleontological record as being characterized by predominantly gradual change (which he termed horotely), though he also documented examples of slow (bradytely), and rapid (tachytely) rates of evolution. Punctuated equilibrium and phyletic gradualism are not mutually exclusive, and examples of each have been documented in different lineages. The debate between these two models is often misunderstood by non-scientists, and according to Richard Dawkins has been oversold by the media.
Some critics jokingly referred to the theory of punctuated equilibrium as "evolution by jerks",
which prompted Gould to describe phyletic gradualism as "evolution by creeps."

Gould made significant contributions to evolutionary developmental biology, especially in his work "Ontogeny and Phylogeny". In this book he emphasized the process of heterochrony, which encompasses two distinct processes: neoteny and terminal additions. Neoteny is the process where ontogeny is slowed down and the organism does not reach the end of its development. Terminal addition is the process by which an organism adds to its development by speeding and shortening earlier stages in the developmental process. Gould's influence in the field of evolutionary developmental biology continues to be seen today in areas such as the evolution of feathers.

Gould was a champion of biological constraints, internal limitations upon developmental pathways, as well as other non-selectionist forces in evolution. Rather than direct adaptations, he considered many higher functions of the human brain to be the unintended side consequence of natural selection. To describe such co-opted features, he coined the term exaptation with paleontologist Elisabeth Vrba. Gould believed this feature of human mentality undermines an essential premise of human sociobiology and evolutionary psychology.

In 1975, Gould's Harvard colleague E. O. Wilson introduced his analysis of animal behavior (including human behavior) based on a sociobiological framework that suggested that many social behaviors have a strong evolutionary basis. In response, Gould, Richard Lewontin, and others from the Boston area wrote the subsequently well-referenced letter to "The New York Review of Books" entitled, "Against 'Sociobiology'". This open letter criticized Wilson's notion of a "deterministic view of human society and human action."

But Gould did not rule out sociobiological explanations for many aspects of animal behavior, and later wrote: "Sociobiologists have broadened their range of selective stories by invoking concepts of inclusive fitness and kin selection to solve (successfully I think) the vexatious problem of altruism—previously the greatest stumbling block to a Darwinian theory of social behavior... Here sociobiology has had and will continue to have success. And here I wish it well. For it represents an extension of basic Darwinism to a realm where it should apply."

With Richard Lewontin, Gould wrote an influential 1979 paper entitled, "The Spandrels of San Marco and the Panglossian Paradigm", 
which introduced the architectural term "spandrel" into evolutionary biology. In architecture, a spandrel is a triangular space which exists over the haunches of an arch. Spandrels—more often called pendentives in this context—are found particularly in classical architecture, especially Byzantine and Renaissance churches.

When visiting Venice in 1978, Gould noted that the spandrels of the San Marco cathedral, while quite beautiful, were not spaces planned by the architect. Rather the spaces arise as "necessary architectural byproducts of mounting a dome on rounded arches." Gould and Lewontin thus defined "spandrels" in the evolutionary biology context to mean any biological feature of an organism that arises as a necessary side consequence of other features, which is not directly selected for by natural selection. Proposed examples include the "masculinized genitalia in female hyenas, exaptive use of an umbilicus as a brooding chamber by snails, the shoulder hump of the giant Irish deer, and several key features of human mentality."

In Voltaire's "Candide", Dr. Pangloss is portrayed as a clueless scholar who, despite the evidence, insists that "all is for the best in this best of all possible worlds". Gould and Lewontin asserted that it is Panglossian for evolutionary biologists to view all traits as atomized things that had been naturally selected for, and criticised biologists for not granting theoretical space to other causes, such as phyletic and developmental constraints. The relative frequency of spandrels, so defined, versus adaptive features in nature, remains a controversial topic in evolutionary biology. An illustrative example of Gould's approach can be found in Elisabeth Lloyd's case study suggesting that the female orgasm is a by-product of shared developmental pathways. Gould also wrote on this topic in his essay "Male Nipples and Clitoral Ripples" prompted by Lloyd's earlier work.

Gould was criticized by philosopher Dan Dennett for using the term spandrel instead of pendentive, a spandrel that curves across a right angle to support a dome. Robert Mark, a professor of civil engineering at Princeton, offered his expertise in the pages of "American Scientist", noting that these definitions are often misunderstood in architectural theory. Mark concluded, "Gould and Lewontin's misapplication of the term spandrel for pendentive perhaps implies a wider latitude of design choice than they intended for their analogy. But Dennett's critique of the architectural basis of the analogy goes even further astray because he slights the technical rationale of the architectural elements in question."

Gould favored the argument that evolution has no inherent drive towards long-term "progress". Uncritical commentaries often portray evolution as a ladder of progress, leading towards bigger, faster, and smarter organisms, the assumption being that evolution is somehow driving organisms to get more complex and ultimately more like humankind. Gould argued that evolution's drive was not towards complexity, but towards diversification. Because life is constrained to begin with a simple starting point (like bacteria), any diversity resulting from this start, by random walk, will have a skewed distribution and therefore be perceived to move in the direction of higher complexity. But life, Gould argued, can also easily adapt towards simplification, as is often the case with parasites.

In a review of "", Richard Dawkins approved of Gould's general argument, but suggested that he saw evidence of a "tendency for lineages to improve cumulatively their adaptive fit to their particular way of life, by increasing the numbers of features which combine together in adaptive complexes. ... By this definition, adaptive evolution is not just incidentally progressive, it is deeply, dyed-in-the-wool, indispensably progressive."

Gould never embraced cladistics as a method of investigating evolutionary lineages and process, possibly because he was concerned that such investigations would lead to neglect of the details in historical biology, which he considered all-important. In the early 1990s this led him into a debate with Derek Briggs, who had begun to apply quantitative cladistic techniques to the Burgess Shale fossils, about the methods to be used in interpreting these fossils. Around this time cladistics rapidly became the dominant method of classification in evolutionary biology. Inexpensive but increasingly powerful personal computers made it possible to process large quantities of data about organisms and their characteristics. Around the same time the development of effective polymerase chain reaction techniques made it possible to apply cladistic methods of analysis to biochemical and genetic features as well.

Most of Gould's empirical research pertained to land snails. He focused his early work on the Bermudian genus "Poecilozonites", while his later work concentrated on the West Indian genus "Cerion". According to Gould ""Cerion" is the land snail of maximal diversity in form throughout the entire world. There are 600 described species of this single genus. In fact, they're not really species, they all interbreed, but the names exist to express a real phenomenon which is this incredible morphological diversity. Some are shaped like golf balls, some are shaped like pencils. ... Now my main subject is the evolution of form, and the problem of how it is that you can get this diversity amid so little genetic difference, so far as we can tell, is a very interesting one. And if we could solve this we'd learn something general about the evolution of form."

Given "Cerion's" extensive geographic diversity, Gould later lamented that if Christopher Columbus had only catalogued a single "Cerion" it would have ended the scholarly debate about which island Columbus had first set foot on in America.

Gould is one of the most frequently cited scientists in the field of evolutionary theory. His 1979 "spandrels" paper has been cited more than 5,000 times. In "Paleobiology"—the flagship journal of his own speciality—only Charles Darwin and George Gaylord Simpson have been cited more often. Gould was also a considerably respected historian of science. Historian Ronald Numbers has been quoted as saying: "I can't say much about Gould's strengths as a scientist, but for a long time I've regarded him as the second most influential historian of science (next to Thomas Kuhn)."

Shortly before his death, Gould published "The Structure of Evolutionary Theory" (2002), a long treatise recapitulating his version of modern evolutionary theory. In an interview for the Dutch TV series "Of Beauty and Consolation" Gould remarked, "In a couple of years I will be able to gather in one volume my view of how evolution works. It is to me a great consolation because it represents the putting together of a lifetime of thinking into one source. That book will never be particularly widely read. It's going to be far too long, and it's only for a few thousand professionals—very different from my popular science writings—but it is of greater consolation to me because it is a chance to put into one place a whole way of thinking about evolution that I've struggled with all my life."

Gould became widely known through his popular essays on evolution in the "Natural History" magazine. His essays were published in a series entitled "This View of Life" (a phrase from the concluding paragraph of Charles Darwin's "Origin of Species") from January 1974 to January 2001, amounting to a continuous publication of 300 essays. Many of his essays were reprinted in collected volumes that became bestselling books such as "Ever Since Darwin" and "The Panda's Thumb", "Hen's Teeth and Horse's Toes", and "The Flamingo's Smile".

A passionate advocate of evolutionary theory, Gould wrote prolifically on the subject, trying to communicate his understanding of contemporary evolutionary biology to a wide audience. A recurring theme in his writings is the history and development of pre-evolutionary and evolutionary thought. He was also an enthusiastic baseball fan and sabermetrician (analyst of baseball statistics), and made frequent reference to the sport in his essays. Many of his baseball essays were anthologized in his posthumously published book "Triumph and Tragedy in Mudville" (2003).

Although a self-described Darwinist, Gould's emphasis was less gradualist and reductionist than most neo-Darwinists. He fiercely opposed many aspects of sociobiology and its intellectual descendant evolutionary psychology. He devoted considerable time to fighting against creationism, creation science, and intelligent design. Most notably, Gould provided expert testimony against the equal-time creationism law in "McLean v. Arkansas". Gould later developed the term "non-overlapping magisteria" (NOMA) to describe how, in his view, science and religion should not comment on each other's realm. Gould went on to develop this idea in some detail, particularly in the books "Rocks of Ages" (1999) and "The Hedgehog, the Fox, and the Magister's Pox" (2003). In a 1982 essay for "Natural History" Gould wrote:

An "anti-evolution petition" drafted by the Discovery Institute inspired the National Center for Science Education to create a pro-evolution counterpart called "Project Steve," which is named in Gould's honor. At a meeting of the executive council of the Committee for Skeptical Inquiry (CSI) in 2011 selected Gould for inclusion in CSI's "Pantheon of Skeptics" created to remember the legacy of deceased CSI fellows and their contributions to the cause of scientific skepticism.

Gould also became a noted public face of science, often appearing on television. In 1984 Gould received his own "NOVA" special on PBS. Other appearances included interviews on CNN's "Crossfire" and "Talkback Live", NBC's "The Today Show", and regular appearances on PBS's "Charlie Rose" show. Gould was also a guest in all seven episodes of the Dutch talk series "A Glorious Accident", in which he appeared with his close friend Oliver Sacks.

Gould was featured prominently as a guest in Ken Burns's PBS documentary "Baseball", as well as PBS's "Evolution" series. Gould was also on the Board of Advisers to the influential Children's Television Workshop television show "3-2-1 Contact", where he made frequent guest appearances.

In 1997 he voiced a cartoon version of himself on the television series "The Simpsons". In the episode "Lisa the Skeptic", Lisa finds a skeleton that many people believe is an apocalyptic angel. Lisa contacts Gould and asks him to test the skeleton's DNA. The fossil is discovered to be a marketing gimmick for a new mall. During production the only phrase Gould objected to was a line in the script that introduced him as the "world's most brilliant paleontologist". In 2002 the show paid tribute to Gould after his death, dedicating the season 13 finale to his memory. Gould had died two days before the episode aired.

Gould received many accolades for his scholarly work and popular expositions of natural history, but a number of biologists felt his public presentations were out of step with mainstream evolutionary thinking. The public debates between Gould's supporters and detractors have been so quarrelsome that they have been dubbed "The Darwin Wars" by several commentators.

John Maynard Smith, the eminent British evolutionary biologist, was among Gould's strongest critics. Maynard Smith thought that Gould misjudged the vital role of adaptation in biology, and was critical of Gould's acceptance of species selection as a major component of biological evolution. In a review of Daniel Dennett's book "Darwin's Dangerous Idea", Maynard Smith wrote that Gould "is giving non-biologists a largely false picture of the state of evolutionary theory." But Maynard Smith was not consistently negative, writing in a review of "The Panda's Thumb" that "Stephen Gould is the best writer of popular science now active... Often he infuriates me, but I hope he will go right on writing essays like these." Maynard Smith was also among those who welcomed Gould's reinvigoration of evolutionary paleontology.

One reason for criticism was that Gould appeared to be presenting his ideas as a revolutionary way of understanding evolution, and argued for the importance of mechanisms other than natural selection, mechanisms which he believed had been ignored by many professional evolutionists. As a result, many non-specialists sometimes inferred from his early writings that Darwinian explanations had been proven to be unscientific (which Gould never tried to imply). Along with many other researchers in the field, Gould's works were sometimes deliberately taken out of context by creationists as "proof" that scientists no longer understood how organisms evolved. Gould himself corrected some of these misinterpretations and distortions of his writings in later works.

The conflicts between Richard Dawkins and Gould were popularized by philosopher Kim Sterelny in his 2001 book "Dawkins vs. Gould". Sterelny documents their disagreements over theoretical issues, including the prominence of gene selection in evolution. Dawkins argues that natural selection is best understood as competition among genes (or replicators), while Gould advocated multi-level selection, which includes selection amongst genes, nucleic acid sequences, cell lineages, organisms, demes, species, and clades.

Dawkins accused Gould of deliberately underplaying the differences between rapid gradualism and macromutation in his published accounts of punctuated equilibrium. He also devoted entire chapters to critiquing Gould's account of evolution in his books "The Blind Watchmaker" and "Unweaving the Rainbow", as did Daniel Dennett in his 1995 book "Darwin's Dangerous Idea".

Gould's interpretation of the Cambrian Burgess Shale fossils in his book "Wonderful Life" emphasized the striking morphological disparity (or "weirdness") of the Burgess Shale fauna, and the role of chance in determining which members of this fauna survived and flourished. He used the Cambrian fauna as an example of the role of contingency in the broader pattern of evolution.

His view was criticized by Simon Conway Morris in his 1998 book "The Crucible of Creation". Conway Morris stressed those members of the Cambrian fauna that resemble modern taxa. He also promoted convergent evolution as a mechanism producing similar forms in similar environmental circumstances, and argued in a subsequent book that the appearance of human-like animals is likely. Paleontologist Richard Fortey noted that prior to the release of "Wonderful Life", Conway Morris shared many of Gould's sentiments and views. It was only after publication of "Wonderful Life" that Conway Morris revised his interpretation and adopted a more progressive stance towards the history of life.

Paleontologists Derek Briggs and Richard Fortey have also argued that much of the Cambrian fauna may be regarded as stem groups of living taxa, though this is still a subject of intense research and debate, and the relationship of many Cambrian taxa to modern phyla has not been established in the eyes of many palaeontologists.

Richard Dawkins also disagreed with Gould's view that new phyla suddenly appeared in the Cambrian fauna, arguing:

The extreme Gouldian view—certainly the view inspired by his rhetoric, though it is hard to tell from his own words whether he literally holds it himself—is radically different from and utterly incompatible with the standard neo-Darwinian model. ... For a new body plan—a new phylum—to spring into existence, what actually has to happen on the ground is that a child is born which suddenly, out of the blue, is as different from its parents as a snail is from an earthworm. No zoologist who thinks through the implications, not even the most ardent saltationist, has ever supported any such notion.

Gould also had a long-running public feud with E. O. Wilson and other evolutionary biologists concerning the disciplines of human sociobiology and evolutionary psychology, both of which Gould and Lewontin opposed, but which Richard Dawkins, Daniel Dennett, and Steven Pinker advocated. These debates reached their climax in the 1970s, and included strong opposition from groups such as the Sociobiology Study Group and Science for the People. Pinker accuses Gould, Lewontin, and other opponents of evolutionary psychology of being "radical scientists", whose stance on human nature is influenced by politics rather than science. Gould stated that he made "no attribution of motive in Wilson's or anyone else's case" but cautioned that all human beings are influenced, especially unconsciously, by our personal expectations and biases. He wrote:

Gould's primary criticism held that human sociobiological explanations lacked evidential support, and argued that adaptive behaviors are frequently assumed to be genetic for no other reason than their supposed universality, or their adaptive nature. Gould emphasized that adaptive behaviors can be passed on through culture as well, and either hypothesis is equally plausible. Gould did not deny the relevance of biology to human nature, but reframed the debate as "biological potentiality vs. biological determinism." Gould stated that the human brain allows for a wide range of behaviors. Its flexibility "permits us to be aggressive or peaceful, dominant or submissive, spiteful or generous… Violence, sexism, and general nastiness "are" biological since they represent one subset of a possible range of behaviors. But peacefulness, equality, and kindness are just as biological—and we may see their influence increase if we can create social structures that permit them to flourish."

Gould was the author of "The Mismeasure of Man" (1981), a history and inquiry of psychometrics and intelligence testing, generating perhaps the greatest controversy of all his books and receiving both widespread praise and extensive criticism, including claims of misrepresentation. Gould investigated the methods of nineteenth century craniometry, as well as the history of psychological testing. Gould claimed that both theories developed from an unfounded belief in biological determinism, the view that "social and economic differences between human groups—primarily races, classes, and sexes—arise from inherited, inborn distinctions and that society, in this sense, is an accurate reflection of biology." The book was reprinted in 1996 with the addition of a new foreword and a critical review of "The Bell Curve".

In 2011, a study conducted by six anthropologists criticized Gould's claim that Samuel Morton unconsciously manipulated his skull measurements, arguing that his analysis of Morton was influenced by his opposition to racism. The group's paper was briefly reviewed in the journal "Nature", which recommended a degree of caution, noting that while Gould's opposition to racism might have biased his interpretation of Morton's data, the paper's authors had biases of their own. In 2015, the group's paper was critically reviewed in the journal "Evolution & Development" by philosopher of science Michael Weisberg, also of the University of Pennsylvania, who argued that Gould's arguments were sound and that Morton's initial measurements were indeed tainted by racial bias. Biologists and philosophers Jonathan Kaplan, Massimo Pigliucci, and Joshua Banta also published a critique of the groups's paper, arguing that many of its claims were misleading and the re-measurements were irrelevant to an analysis of Gould's arguments.

In his book "Rocks of Ages" (1999), Gould put forward what he described as "a blessedly simple and entirely conventional resolution to ... the supposed conflict between science and religion." He defines the term "magisterium" as "a domain where one form of teaching holds the appropriate tools for meaningful discourse and resolution." The non-overlapping magisteria (NOMA) principle therefore divides the magisterium of science to cover "the empirical realm: what the Universe is made of (fact) and why does it work in this way (theory). The magisterium of religion extends over questions of ultimate meaning and moral value. These two magisteria do not overlap, nor do they encompass all inquiry." He suggests that "NOMA enjoys strong and fully explicit support, even from the primary cultural stereotypes of hard-line traditionalism" and that NOMA is "a sound position of general consensus, established by long struggle among people of goodwill in both magisteria."

This view has not been without criticism, however. For example, in his book "The God Delusion", Richard Dawkins argues that the division between religion and science is not so simple as Gould claims, as few religions exist without claiming the existence of miracles, which "by definition, violate the principles of science." Dawkins also opposes the idea that religion has anything meaningful to say about ethics and values, and therefore has no authority to claim a magisterium of its own. He goes on to say that he believes Gould is disingenuous in much of what he says in "Rocks of Ages". Similarly, humanist philosopher Paul Kurtz argues that Gould was wrong to posit that science has nothing to say about questions of ethics. In fact, Kurtz claims that science is a much better method than religion for determining moral principles.

Gould's publications were numerous. One review of his publications between 1965 and 2000 noted 479 peer-reviewed papers, 22 books, 300 essays, and 101 "major" book reviews. A selected number of his papers are listed online.

The following is a list of books either written or edited by Stephen Jay Gould, including those published posthumously, after his death in 2002. While some books have been republished at later dates, by multiple publishers, the list below comprises the original publisher and publishing date.




</doc>
<doc id="27879" url="https://en.wikipedia.org/wiki?curid=27879" title="Skíðblaðnir">
Skíðblaðnir

Skíðblaðnir (Old Norse 'assembled from thin pieces of wood'), sometimes anglicized as Skidbladnir or Skithblathnir, is the best of ships in Norse mythology. It is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources, and in the "Prose Edda" and "Heimskringla", both written in the 13th century by Snorri Sturluson. All sources note that the ship is the finest of ships, and the "Poetic Edda" and "Prose Edda" attest that it is owned by the god Freyr, while the euhemerized account in "Heimskringla" attributes it to the magic of Odin. Both "Heimskringla" and the "Prose Edda" attribute to it the ability to be folded up—as cloth may be—into one's pocket when not needed.

References to the ship occur in the "Poetic Edda", the "Prose Edda", and in "Heimskringla". The ship is mentioned twice in the "Poetic Edda" and both incidents therein occur in the poem "Grímnismál". In "Grímnismál", Odin (disguised as "Grímnir"), tortured, starved, and thirsty, imparts in the young Agnar cosmological knowledge, including information about the origin of the ship Skíðblaðnir:

"Skíðblaðnir" is mentioned several times in the "Prose Edda", where it appears in the books "Gylfaginning" and "Skáldskaparmál". The first mention of "Skíðblaðnir" in the "Poetic Edda" occurs in chapter 43, where the enthroned figure of High tells Gangleri (king Gylfi in disguise) that the god Odin is an important deity. High quotes the second of the above-mentioned "Grímnismál" stanzas in support.

The boat is first directly addressed in chapter 43; there Gangleri asks that, if "Skíðblaðnir" is the best of ships, what there is to know about it, and asks if there is no other ship as good or as large as it. High responds that while "Skíðblaðnir" is the finest ship and the most ingeniously created, the biggest ship is in fact "Naglfar", which is owned by Muspell. The Sons of Ivaldi, who High adds are dwarfs, crafted the ship and gave it to Freyr. High continues that the ship is big enough for all of the gods to travel aboard it with wargear and weapons in tow, and that, as soon as its sail is hoisted, the ship finds good wind, and goes wherever it need be. It is made up of so many parts and with such craftsmanship that, when it is not needed at sea, it may be folded up like cloth and placed into one's pocket. Gangleri comments that "Skíðblaðnir" sounds like a great ship, and that it must have taken a lot of magic to create something like it.

The next mention of the ship occurs in "Skáldskaparmál" where, in chapter 6, poetic ways of referring to Freyr are provided. Among other names, Freyr is referred to as "possessor of "Skidbladnir" and of the boar known as Gullinbursti". The first of the two "Grímnismál" stanzas mentioned above is then provided as reference.

In chapter 35, a myth explaining "Skíðblaðnir"s creation is provided. The chapter details that the god Loki once cut off the goddess's Sif's hair in an act of mischief. Sif's husband, Thor, enraged, found Loki, caught hold of him, and threatened to break every last bone in his body. Loki promises to have the Svartálfar make Sif a new head of hair that will grow just as any other. Loki goes to the dwarfs known as Ivaldi's sons, and they made not only Sif a new head of gold hair but also Skíðblaðnir and the spear Gungnir. As the tale continues, Loki risks his neck for the creation of the devastating hammer Mjöllnir, the multiplying ring Draupnir, and the speedy, sky-and-water traveling, bright-bristled boar Gullinbursti. In the end, Loki's wit saves him his head, but results in the stitching together of his lips. The newly created items are doled out by the dwarfs to Sif, Thor, Odin, and Freyr. Freyr is gifted both Gullinbursti and "Skíðblaðnir", the latter of which is again said to receive fair wind whenever its sail was set, and that it will go wherever it needs to, and that it can be folded up much as cloth and placed in one's pocket at will.

"Skíðblaðnir" receives a final mention in "Skáldskaparmál" where, in chapter 75, it appears on a list of ships.

The ship gets a single mention in the "Heimskringla" book "Ynglinga saga". In chapter 7, an euhemerized Odin is said to have had various magical abilities, including that "he was also able with mere words to extinguish fires, to calm the sea, and to turn the winds any way he pleased. He had a ship called "Skíthblathnir" with which he sailed over great seas. It could be folded together like a cloth."




</doc>
<doc id="27881" url="https://en.wikipedia.org/wiki?curid=27881" title="Sleipnir">
Sleipnir

In Norse mythology, Sleipnir (Old Norse "slippy" or "the slipper") is an eight-legged horse ridden by Odin. Sleipnir is attested in the "Poetic Edda", compiled in the 13th century from earlier traditional sources, and the "Prose Edda", written in the 13th century by Snorri Sturluson. In both sources, Sleipnir is Odin's steed, is the child of Loki and Svaðilfari, is described as the best of all horses, and is sometimes ridden to the location of Hel. The "Prose Edda" contains extended information regarding the circumstances of Sleipnir's birth, and details that he is grey in color.

Sleipnir is also mentioned in a riddle found in the 13th century legendary saga "Hervarar saga ok Heiðreks", in the 13th-century legendary saga "Völsunga saga" as the ancestor of the horse Grani, and book I of "Gesta Danorum", written in the 12th century by Saxo Grammaticus, contains an episode considered by many scholars to involve Sleipnir. Sleipnir is generally accepted as depicted on two 8th century Gotlandic image stones: the Tjängvide image stone and the Ardre VIII image stone.

Scholarly theories have been proposed regarding Sleipnir's potential connection to shamanic practices among the Norse pagans. In modern times, Sleipnir appears in Icelandic folklore as the creator of Ásbyrgi, in works of art, literature, software, and in the names of ships.

In the "Poetic Edda", Sleipnir appears or is mentioned in the poems "Grímnismál", "Sigrdrífumál", "Baldrs draumar", and "Hyndluljóð". In "Grímnismál", Grimnir (Odin in disguise and not yet having revealed his identity) tells the boy Agnar in verse that Sleipnir is the best of horses ("Odin is the best of the Æsir, Sleipnir of horses"). In "Sigrdrífumál", the valkyrie Sigrdrífa tells the hero Sigurðr that runes should be cut "on Sleipnir's teeth and on the sledge's strap-bands." In "Baldrs draumar", after the Æsir convene about the god Baldr's bad dreams, Odin places a saddle on Sleipnir and the two ride to the location of Hel. The "Völuspá hin skamma" section of "Hyndluljóð" says that Loki produced "the wolf" with Angrboða, produced Sleipnir with Svaðilfari, and thirdly "one monster that was thought the most baleful, who was descended from Býleistr's brother."

In the "Prose Edda" book "Gylfaginning", Sleipnir is first mentioned in chapter 15 where the enthroned figure of High says that every day the Æsir ride across the bridge Bifröst, and provides a list of the Æsir's horses. The list begins with Sleipnir: "best is Sleipnir, he is Odin's, he has eight legs." In chapter 41, High quotes the "Grímnismál" stanza that mentions Sleipnir.

In chapter 42, Sleipnir's origins are described. Gangleri (described earlier in the book as King Gylfi in disguise) asks High who the horse Sleipnir belongs to and what there is to tell about it. High expresses surprise in Gangleri's lack of knowledge about Sleipnir and its origin. High tells a story set "right at the beginning of the gods' settlement, when the gods established Midgard and built Val-Hall" about an unnamed builder who has offered to build a fortification for the gods in three seasons that will keep out invaders in exchange for the goddess Freyja, the sun, and the moon. After some debate, the gods agree to this, but place a number of restrictions on the builder, including that he must complete the work within three seasons with the help of no man. The builder makes a single request; that he may have help from his stallion Svaðilfari, and due to Loki's influence, this is allowed. The stallion Svaðilfari performs twice the deeds of strength as the builder, and hauls enormous rocks to the surprise of the gods. The builder, with Svaðilfari, makes fast progress on the wall, and three days before the deadline of summer, the builder was nearly at the entrance to the fortification. The gods convene, and figured out who was responsible, resulting in a unanimous agreement that, along with most trouble, Loki was to blame.

The gods declare that Loki would deserve a horrible death if he could not find a scheme that would cause the builder to forfeit his payment, and threatened to attack him. Loki, afraid, swore oaths that he would devise a scheme to cause the builder to forfeit the payment, whatever it would cost himself. That night, the builder drove out to fetch stone with his stallion Svaðilfari, and out from a wood ran a mare. The mare neighed at Svaðilfari, and "realizing what kind of horse it was," Svaðilfari became frantic, neighed, tore apart his tackle, and ran towards the mare. The mare ran to the wood, Svaðilfari followed, and the builder chased after. The two horses ran around all night, causing the building work to be held up for the night, and the previous momentum of building work that the builder had been able to maintain was not continued.

When the Æsir realize that the builder is a hrimthurs, they disregard their previous oaths with the builder, and call for Thor. Thor arrives, and kills the builder by smashing the builder's skull into shards with the hammer Mjöllnir. However, Loki had "such dealings" with Svaðilfari that "somewhat later" Loki gave birth to a grey foal with eight legs; the horse Sleipnir, "the best horse among gods and men."

In chapter 49, High describes the death of the god Baldr. Hermóðr agrees to ride to Hel to offer a ransom for Baldr's return, and so "then Odin's horse Sleipnir was fetched and led forward." Hermóðr mounts Sleipnir and rides away. Hermóðr rides for nine nights in deep, dark valleys where Hermóðr can see nothing. The two arrive at the river Gjöll and then continue to Gjöll bridge, encountering a maiden guarding the bridge named Móðguðr. Some dialogue occurs between Hermóðr and Móðguðr, including that Móðguðr notes that recently there had ridden five battalions of dead men across the bridge that made less sound than he. Sleipnir and Hermóðr continue "downwards and northwards" on the road to Hel, until the two arrive at Hel's gates. Hermóðr dismounts from Sleipnir, tightens Sleipnir's girth, mounts him, and spurs Sleipnir on. Sleipnir "jumped so hard and over the gate that it came nowhere near." Hermóðr rides up to the hall, and dismounts from Sleipnir. After Hermóðr's pleas to Hel to return Baldr are accepted under a condition, Hermóðr and Baldr retrace their path backward and return to Asgard.

In chapter 16 of the book "Skáldskaparmál", a kenning given for Loki is "relative of Sleipnir." In chapter 17, a story is provided in which Odin rides Sleipnir into the land of Jötunheimr and arrives at the residence of the jötunn Hrungnir. Hrungnir asks "what sort of person this was" wearing a golden helmet, "riding sky and sea," and says that the stranger "has a marvellously good horse." Odin wagers his head that no horse as good could be found in all of Jötunheimr. Hrungnir admitted that it was a fine horse, yet states that he owns a much longer-paced horse; Gullfaxi. Incensed, Hrungnir leaps atop Gullfaxi, intending to attack Odin for Odin's boasting. Odin gallops hard ahead of Hrungnir, and, in his, fury, Hrungnir finds himself having rushed into the gates of Asgard. In chapter 58, Sleipnir is mentioned among a list of horses in "Þorgrímsþula": "Hrafn and Sleipnir, splendid horses [...]". In addition, Sleipnir occurs twice in kennings for "ship" (once appearing in chapter 25 in a work by the skald Refr, and "sea-Sleipnir" appearing in chapter 49 in "Húsdrápa", a work by the 10th century skald Úlfr Uggason).

In "Hervarar saga ok Heiðreks", the poem "Heiðreks gátur" contains a riddle that mentions Sleipnir and Odin:

In chapter 13 of "Völsunga saga", the hero Sigurðr is on his way to a wood and he meets a long-bearded old man he had never seen before. Sigurd tells the old man that he is going to choose a horse, and asks the old man to come with him to help him decide. The old man says that they should drive the horses down to the river Busiltjörn. The two drive the horses down into the deeps of Busiltjörn, and all of the horses swim back to land but a large, young, and handsome grey horse that no one had ever mounted. The grey-bearded old man says that the horse is from "Sleipnir's kin" and that "he must be raised carefully, because he will become better than any other horse." The old man vanishes. Sigurd names the horse Grani, and the narrative adds that the old man was none other than (the god) Odin.

Sleipnir is generally considered as appearing in a sequence of events described in book I of "Gesta Danorum". 

In book I, the young Hadingus encounters "a certain man of great age who had lost an eye" who allies him with Liserus. Hadingus and Liserus set out to wage war on Lokerus, ruler of Kurland. Meeting defeat, the old man takes Hadingus with him onto his horse as they flee to the old man's house, and the two drink an invigorating drought. The old man sings a prophecy, and takes Hadingus back to where he found him on his horse. During the ride back, Hadingus trembles beneath the old man's mantle, and peers out of its holes. Hadingus realizes that he is flying through the air: "and he saw that before the steps of the horse lay the sea; but was told not to steal a glimpse of the forbidden thing, and therefore turned his amazed eyes from the dread spectacle of the roads that he journeyed." 

In book II, Biarco mentions Odin and Sleipnir: "If I may look on the awful husband of Frigg, howsoever he be covered in his white shield, and guide his tall steed, he shall in no way go safe out of Leire; it is lawful to lay low in war the war-waging god."

Two of the 8th century picture stones from the island of Gotland, Sweden depict eight-legged horses, which are thought by most scholars to depict Sleipnir: the Tjängvide image stone and the Ardre VIII image stone. Both stones feature a rider sitting atop an eight-legged horse, which some scholars view as Odin. Above the rider on the Tjängvide image stone is a horizontal figure holding a spear, which may be a valkyrie, and a female figure greets the rider with a cup. The scene has been interpreted as a rider arriving at the world of the dead. The mid-7th century Eggja stone bearing the Odinic name "haras" (Old Norse 'army god') may be interpreted as depicting Sleipnir.
John Lindow theorizes that Sleipnir's "connection to the world of the dead grants a special poignancy to one of the kennings in which Sleipnir turns up as a horse word," referring to the skald Úlfr Uggason's usage of "sea-Sleipnir" in his "Húsdrápa", which describes the funeral of Baldr. Lindow continues that "his use of Sleipnir in the kenning may show that Sleipnir's role in the failed recovery of Baldr was known at that time and place in Iceland; it certainly indicates that Sleipnir was an active participant in the mythology of the last decades of paganism." Lindow adds that the eight legs of Sleipnir "have been interpreted as an indication of great speed or as being connected in some unclear way with cult activity."

Hilda Ellis Davidson says that "the eight-legged horse of Odin is the typical steed of the shaman" and that in the shaman's journeys to the heavens or the underworld, a shaman "is usually represented as riding on some bird or animal." Davidson says that while the creature may vary, the horse is fairly common "in the lands where horses are in general use, and Sleipnir's ability to bear the god through the air is typical of the shaman's steed" and cites an example from a study of shamanism by Mircea Eliade of an eight-legged foal from a story of a Buryat shaman. Davidson says that while attempts have been made to connect Sleipnir with hobby horses and steeds with more than four feet that appear in carnivals and processions, but that "a more fruitful resemblance seems to be on the bier on which a dead man is carried in the funeral procession by four bearers; borne along thus, he may be described as riding on a steed with eight legs." As an example, Davidson cites a funeral dirge from the Gondi people in India as recorded by Verrier Elwin, stating that "it contains references to Bagri Maro, the horse with eight legs, and it is clear from the song that it is the dead man's bier." Davidson says that the song is sung when a distinguished Muria dies, and provides a verse:

Davidson adds that the representation of Odin's steed as eight-legged could arise naturally out of such an image, and that "this is in accordance with the picture of Sleipnir as a horse that could bear its rider to the land of the dead."

Ulla Loumand cites Sleipnir and the flying horse Hófvarpnir as "prime examples" of horses in Norse mythology as being able to "mediate between earth and sky, between Ásgarðr, Miðgarðr and Útgarðr and between the world of mortal men and the underworld."

The "Encyclopedia of Indo-European Culture" theorizes that Sleipnir's eight legs may be the remnants of horse-associated divine twins found in Indo-European cultures and ultimately stemming from Proto-Indo-European religion. The encyclopedia states that "[...] Sleipnir is born with an extra set of legs, thus representing an original pair of horses. Like Freyr and Njörðr, Sleipnir is responsible for carrying the dead to the otherworld." The encyclopedia cites parallels between the birth of Sleipnir and myths originally pointing to a Celtic goddess who gave birth to the Divine horse twins. These elements include a demand for a goddess by an unwanted suitor (the hrimthurs demanding the goddess Freyja) and the seduction of builders.

According to Icelandic folklore, the horseshoe-shaped canyon Ásbyrgi located in Jökulsárgljúfur National Park, northern Iceland was formed by Sleipnir's hoof. Sleipnir is depicted with Odin on Dagfin Werenskiold's wooden relief "Odin på Sleipnir" (1945–1950) on the exterior of the Oslo City Hall in Oslo, Norway. Sleipnir has been and remains a popular name for ships in northern Europe, and Rudyard Kipling's short story entitled "Sleipnir, late Thurinda" (1888) features a horse named Sleipnir. A statue of Sleipnir (1998) stands in Wednesbury, England, a town which takes its name from the Anglo-Saxon version of Odin, Wōden.




</doc>
<doc id="27884" url="https://en.wikipedia.org/wiki?curid=27884" title="Walter Scott">
Walter Scott

Sir Walter Scott, 1st Baronet (15 August 1771 – 21 September 1832) was a Scottish historical novelist, playwright, poet and historian. Many of his works remain classics of both English-language literature and of Scottish literature. Famous titles include "Ivanhoe", "Rob Roy", "Old Mortality", "The Lady of the Lake", "Waverley", "The Heart of Midlothian" and "The Bride of Lammermoor".

Although primarily remembered for his extensive literary works and his political engagement, Scott was an advocate, judge and legal administrator by profession, and throughout his career combined his writing and editing work with his daily occupation as Clerk of Session and Sheriff-Depute of Selkirkshire.

A prominent member of the Tory establishment in Edinburgh, Scott was an active member of the Highland Society and served a long term as President of the Royal Society of Edinburgh (1820–32).

Scott gathered the disparate strands of contemporary novel-writing techniques into his own hands and harnessed them to his deep interest in Scottish history and his knowledge of antiquarian lore. The technique of the omniscient narrator and the use of regional speech, localized settings, sophisticated character delineation, and romantic themes treated in a realistic manner were all combined by him into virtually a new literary form, the historical novel. His influence on other European and American novelists was immediate and profound, and though interest in some of his books declined somewhat in the 20th century, his reputation remains secure.

 Walter Scott was born on 15 August 1771. He was the ninth child of Walter Scott, a Writer to the Signet (solicitor), and Anne Rutherford (sister of Daniel Rutherford). His father was a member of a cadet branch of the Scotts Clan, and his mother descended from the Haliburton family, the descent from whom granted Walter's family the hereditary right of burial in Dryburgh Abbey. Via the Haliburton family, Walter (b.1771) was a cousin of the pre-eminent contemporaneous property developer James Burton, who was a Haliburton who had shortened his surname, and of his son, the architect Decimus Burton. Walter subsequently became a member of the Clarence Club, of which the Burtons were also members.

Five of Walter's siblings died in infancy, and a sixth died when he was five months of age. Walter was born in a third-floor flat on College Wynd in the Old Town of Edinburgh, a narrow alleyway leading from the Cowgate to the gates of the University of Edinburgh (Old College). He survived a childhood bout of polio in 1773 that left him lame, a condition that was to have a significant effect on his life and writing.
To cure his lameness he was sent in 1773 to live in the rural Scottish Borders at his paternal grandparents' farm at Sandyknowe, adjacent to the ruin of Smailholm Tower, the earlier family home. Here he was taught to read by his aunt Jenny, and learned from her the speech patterns and many of the tales and legends that characterised much of his work. In January 1775 he returned to Edinburgh, and that summer went with his aunt Jenny to take spa treatment at Bath in England, where they lived at 6 South Parade. In the winter of 1776 he went back to Sandyknowe, with another attempt at a water cure at Prestonpans during the following summer.

In 1778, Scott returned to Edinburgh for private education to prepare him for school, and joined his family in their new house built as one of the first in George Square. In October 1779 he began at the Royal High School of Edinburgh (in High School Yards). He was now well able to walk and explore the city and the surrounding countryside. His reading included chivalric romances, poems, history and travel books. He was given private tuition by James Mitchell in arithmetic and writing, and learned from him the history of the Church of Scotland with emphasis on the Covenanters. After finishing school he was sent to stay for six months with his aunt Jenny in Kelso, attending the local grammar school where he met James and John Ballantyne, who later became his business partners and printed his books.

Scott began studying classics at the University of Edinburgh in November 1783, at the age of 12, a year or so younger than most of his fellow students. In March 1786 he began an apprenticeship in his father's office to become a Writer to the Signet. While at the university Scott had become a friend of Adam Ferguson, the son of Professor Adam Ferguson who hosted literary salons. Scott met the blind poet Thomas Blacklock, who lent him books and introduced him to James Macpherson's Ossian cycle of poems. During the winter of 1786–87 the 15-year-old Scott saw Robert Burns at one of these salons, for what was to be their only meeting. When Burns noticed a print illustrating the poem "The Justice of the Peace" and asked who had written the poem, only Scott knew that it was by John Langhorne, and was thanked by Burns. When it was decided that he would become a lawyer, he returned to the university to study law, first taking classes in Moral Philosophy and Universal History in 1789–90.

After completing his studies in law, he became a lawyer in Edinburgh. As a lawyer's clerk he made his first visit to the Scottish Highlands directing an eviction. He was admitted to the Faculty of Advocates in 1792. He had an unsuccessful love suit with Williamina Belsches of Fettercairn, who married Scott's friend Sir William Forbes, 7th Baronet.

As a boy, youth and young man, Scott was fascinated by the oral traditions of the Scottish Borders. He was an obsessive collector of stories, and developed an innovative method of recording what he heard at the feet of local story-tellers using carvings on twigs, to avoid the disapproval of those who believed that such stories were neither for writing down nor for printing. At the age of 25 he began to write professionally, translating works from German, his first publication being rhymed versions of ballads by Gottfried August Bürger in 1796. He then published an idiosyncratic three-volume set of collected ballads of his adopted home region, "The Minstrelsy of the Scottish Border". This was the first sign from a literary standpoint of his interest in Scottish history.

As a result of his early polio infection, Scott had a pronounced limp. He was described in 1820 as tall, well formed (except for one ankle and foot which made him walk lamely), neither fat nor thin, with forehead very high, nose short, upper lip long and face rather fleshy, complexion fresh and clear, eyes very blue, shrewd and penetrating, with hair now silvery white. Although a determined walker, on horseback he experienced greater freedom of movement. Unable to consider a military career, Scott enlisted as a volunteer in the 1st Lothian and Border yeomanry.

On a trip to the Lake District with old college friends he met Charlotte Charpentier (or Carpenter), daughter of Jean Charpentier of Lyon in France, and ward of Lord Downshire in Cumberland, an Episcopalian. After three weeks of courtship, Scott proposed and they were married on Christmas Eve 1797 in St Mary's Church, Carlisle (a church set up in the now destroyed nave of Carlisle Cathedral). After renting a house in George Street, they moved to nearby South Castle Street. They had five children, of whom four survived by the time of Scott's death, most baptized by an Episcopalian clergyman. In 1799 he was appointed Sheriff-Depute of the County of Selkirk, based in the Royal Burgh of Selkirk. In his early married days Scott had a decent living from his earnings at the law, his salary as Sheriff-Depute, his wife's income, some revenue from his writing, and his share of his father's rather meagre estate.

After their third son was born in 1801, they moved to a spacious three-storey house built for Scott at 39 North Castle Street. This remained Scott's base in Edinburgh until 1826, when he could no longer afford two homes. From 1798 Scott had spent the summers in a cottage at Lasswade, where he entertained guests including literary figures, and it was there that his career as an author began. There were nominal residency requirements for his position of Sheriff-Depute, and at first he stayed at a local inn during the circuit. In 1804 he ended his use of the Lasswade cottage and leased the substantial house of Ashestiel, from Selkirk. It was sited on the south bank of the River Tweed, and the building incorporated an old tower house.

Scott's father, also Walter (1729–1799), was a Freemason, being a member of Lodge St David, No.36 (Edinburgh), and Scott also became a Freemason in his father's Lodge in 1801, albeit only after the death of his father.

In 1796, Scott's friend James Ballantyne founded a printing press in Kelso, in the Scottish Borders. Through Ballantyne, Scott was able to publish his first works, including "Glenfinlas" and "The Eve of St. John", and his poetry then began to bring him to public attention. In 1805, "The Lay of the Last Minstrel" captured wide public imagination, and his career as a writer was established in spectacular fashion.
He published many other poems over the next ten years, including the popular "The Lady of the Lake", printed in 1810 and set in the Trossachs. Portions of the German translation of this work were set to music by Franz Schubert. One of these songs, "Ellens dritter Gesang", is popularly labelled as "Schubert's "Ave Maria"".

Beethoven's opus 108 "Twenty-Five Scottish Songs" includes 3 folk songs whose words are by Walter Scott.

"Marmion", published in 1808, produced lines that have become proverbial. Canto VI. Stanza 17 reads:

In 1809 Scott persuaded James Ballantyne and his brother to move to Edinburgh and to establish their printing press there. He became a partner in their business. As a political conservative, Scott helped to found the Tory "Quarterly Review", a review journal to which he made several anonymous contributions. Scott was also a contributor to the "Edinburgh Review", which espoused Whig views.

Scott was ordained as an elder in the Presbyterian Church of Duddington and sat in the General Assembly for a time as representative elder of the burgh of Selkirk.

When the lease of Ashestiel expired in 1811, Scott bought Cartley Hole Farm, on the south bank of the River Tweed nearer Melrose. The farm had the nickname of "Clarty Hole", and when Scott built a family cottage there in 1812 he named it "Abbotsford". He continued to expand the estate, and built Abbotsford House in a series of extensions.

In 1813 Scott was offered the position of Poet Laureate. He declined, due to concerns that "such an appointment would be a poisoned chalice", as the Laureateship had fallen into disrepute, due to the decline in quality of work suffered by previous title holders, "as a succession of poetasters had churned out conventional and obsequious odes on royal occasions." He sought advice from the Duke of Buccleuch, who counseled him to retain his literary independence, and the position went to Scott's friend, Robert Southey.

Although Scott had attained worldwide celebrity through his poetry, he soon tried his hand at documenting his researches into the oral tradition of the Scottish Borders in prose fiction—stories and novels—at the time still considered aesthetically inferior to poetry (above all to such classical genres as the epic or poetic tragedy) as a mimetic vehicle for portraying historical events. In an innovative and astute action, he wrote and published his first novel, "Waverley", anonymously in 1814. It was a tale of the Jacobite rising of 1745. Its English protagonist, Edward Waverley, like Don Quixote a great reader of romances, has been brought up by his Tory uncle, who is sympathetic to Jacobitism, although Edward's own father is a Whig. The youthful Waverley obtains a commission in the Whig army and is posted in Dundee. On leave, he meets his uncle's friend, the Jacobite Baron Bradwardine and is attracted to the Baron's daughter Rose. On a visit to the Highlands, Edward overstays his leave and is arrested and charged with desertion but is rescued by the Highland chieftain Fergus MacIvor and his mesmerizing sister Flora, whose devotion to the Stuart cause, "as it exceeded her brother's in fanaticism, excelled it also in purity". Through Flora, Waverley meets Bonnie Prince Charlie, and under her influence goes over to the Jacobite side and takes part in the Battle of Prestonpans. He escapes retribution, however, after saving the life of a Whig colonel during the battle. Waverley (whose surname reflects his divided loyalties) eventually decides to lead a peaceful life of establishment respectability under the House of Hanover rather than live as a proscribed rebel. He chooses to marry the beautiful Rose Bradwardine, rather than cast his lot with the sublime Flora MacIvor, who, after the failure of the '45 rising, retires to a French convent.

There followed a succession of novels over the next five years, each with a Scottish historical setting. Mindful of his reputation as a poet, Scott maintained the anonymity he had begun with "Waverley", publishing the novels under the name "Author of Waverley" or as "Tales of..." with no author. Among those familiar with his poetry, his identity became an open secret, but Scott persisted in maintaining the façade, perhaps because he thought his old-fashioned father would disapprove of his engaging in such a trivial pursuit as novel writing. During this time Scott became known by the nickname "The Wizard of the North". In 1815 he was given the honour of dining with George, Prince Regent, who wanted to meet the "Author of Waverley".

Scott's 1819 series "Tales of my Landlord" is sometimes considered a subset of the "Waverley" novels and was intended to illustrate aspects of Scottish regional life. Among the best known is "The Bride of Lammermoor", a fictionalized version of an actual incident in the history of the Dalrymple family that took place in the Lammermuir Hills in 1669. In the novel, Lucie Ashton and the nobly born but now dispossessed and impoverished Edgar Ravenswood exchange vows. But the Ravenswoods and the wealthy Ashtons, who now own the former Ravenswood lands, are enemies, and Lucie's mother forces her daughter to break her engagement to Edgar and marry the wealthy Sir Arthur Bucklaw. Lucie falls into a depression and on their wedding night stabs the bridegroom, succumbs to insanity, and dies. In 1821, French Romantic painter Eugène Delacroix painted a portrait depicting himself as the melancholy, disinherited Edgar Ravenswood. The prolonged, climactic coloratura mad scene for Lucia in Donizetti's 1835 bel canto opera "Lucia di Lammermoor" is based on what in the novel were just a few bland sentences.

"Tales of my Landlord "includes the now highly regarded novel "Old Mortality", set in 1679–89 against the backdrop of the ferocious anti-Covenanting campaign of the Tory Graham of Claverhouse, subsequently made Viscount Dundee (called "Bluidy Clavers" by his opponents but later dubbed "Bonnie Dundee" by Scott). The Covenanters were presbyterians who had supported the Restoration of Charles II on promises of a Presbyterian settlement, but he had instead reintroduced Episcopalian church government with draconian penalties for Presbyterian worship. This led to the destitution of around 270 ministers who had refused to take an oath of allegiance and submit themselves to bishops, and who continued to conduct worship among a remnant of their flock in caves and other remote country spots. The relentless persecution of these conventicles and attempts to break them up by military force had led to open revolt. The story is told from the point of view of Henry Morton, a moderate Presbyterian, who is unwittingly drawn into the conflict and barely escapes summary execution. In writing "Old Mortality" Scott drew upon the knowledge he had acquired from his researches into ballads on the subject for "The Minstrelsy of the Scottish Border". Scott's background as a lawyer also informed his perspective, for at the time of the novel, which takes place before the Act of Union of 1707, English law did not apply in Scotland, and afterwards Scotland has continued to have its own Scots law as a hybrid legal system. A recent critic, who is a legal as well as a literary scholar, argues that "Old Mortality" not only reflects the dispute between Stuart's absolute monarchy and the jurisdiction of the courts, but also invokes a foundational moment in British sovereignty, namely, the Habeas Corpus Act (also known as the Great Writ), passed by the English Parliament in 1679. Oblique reference to the origin of "Habeas corpus" underlies Scott's next novel, "Ivanhoe", set during the era of the creation of the Magna Carta, which political conservatives like Walter Scott and Edmund Burke regarded as rooted in immemorial British custom and precedent.

"Ivanhoe" (1819), set in 12th-century England, marked a move away from Scott's focus on the local history of Scotland. Based partly on Hume's "History of England" and the ballad cycle of Robin Hood, "Ivanhoe" was quickly translated into many languages and inspired countless imitations and theatrical adaptations. "Ivanhoe" depicts the cruel tyranny of the Norman overlords (Norman Yoke) over the impoverished Saxon populace of England, with two of the main characters, Rowena and Locksley (Robin Hood), representing the dispossessed Saxon aristocracy. When the protagonists are captured and imprisoned by a Norman baron, Scott interrupts the story to exclaim:

The institution of the Magna Carta, which happens outside the time frame of the story, is portrayed as a progressive (incremental) reform, but also as a step towards the recovery of a lost golden age of liberty endemic to England and the English system. Scott puts a derisive prophecy in the mouth of the jester Wamba:

Although on the surface an entertaining escapist romance, alert contemporary readers would have quickly recognised the political subtext of "Ivanhoe", which appeared immediately after the English Parliament, fearful of French-style revolution in the aftermath of Waterloo, had passed the Habeas Corpus Suspension acts of 1817 and 1818 and other extremely repressive measures, and when traditional English Charter rights versus revolutionary human rights was a topic of discussion.

"Ivanhoe" was also remarkable in its sympathetic portrayal of Jewish characters: Rebecca, considered by many critics the book's real heroine, does not in the end get to marry Ivanhoe, whom she loves, but Scott allows her to remain faithful to her own religion, rather than having her convert to Christianity. Likewise, her father, Isaac of York, a Jewish moneylender, is shown as a victim rather than a villain. In "Ivanhoe", which is one of Scott's Waverley novels, religious and sectarian fanatics are the villains, while the eponymous hero is a bystander who must weigh the evidence and decide where to take a stand. Scott's positive portrayal of Judaism, which reflects his humanity and concern for religious toleration, also coincided with a contemporary movement for the Emancipation of the Jews in England.

Scott's fame grew as his explorations and interpretations of Scottish history and society captured popular imagination. Impressed by this, the Prince Regent (the future George IV) gave Scott permission to conduct a search for the Crown Jewels ("Honours of Scotland"). During the years of the Protectorate under Cromwell the Crown Jewels had been hidden away, but had subsequently been used to crown Charles II. They were not used to crown subsequent monarchs, but were regularly taken to sittings of Parliament, to represent the absent monarch, until the Act of Union 1707. Thereafter, the honours were stored in Edinburgh Castle, but the large locked box in which they were stored was not opened for more than 100 years, and stories circulated that they had been "lost" or removed. In 1818, Scott and a small team of military men opened the box, and "unearthed" the honours from the Crown Room in the depths of Edinburgh Castle. A grateful Prince Regent granted Scott the title of baronet, and in March 1820 he received the baronetcy in London, becoming Sir Walter Scott, 1st Baronet.

After George's accession to the throne, the city council of Edinburgh invited Scott, at the King's behest, to stage-manage the 1822 visit of King George IV to Scotland. With only three weeks for planning and execution, Scott created a spectacular and comprehensive pageant, designed not only to impress the King, but also in some way to heal the rifts that had destabilised Scots society. He used the event to contribute to the drawing of a line under an old world that pitched his homeland into regular bouts of bloody strife. He, along with his "production team", mounted what in modern days could be termed a PR event, in which the King was dressed in tartan, and was greeted by his people, many of whom were also dressed in similar tartan ceremonial dress. This form of dress, proscribed after the 1745 rebellion against the English, became one of the seminal, potent and ubiquitous symbols of Scottish identity.

In his novel "Kenilworth", Elizabeth I is welcomed to the castle of that name by means of an elaborate pageant, the details of which Scott was well qualified to itemize.

Much of Scott's autograph work shows an almost stream-of-consciousness approach to writing. He included little in the way of punctuation in his drafts, leaving such details to the printers to supply. He eventually acknowledged in 1827 that he was the author of the Waverley Novels.

In 1825 a UK-wide banking crisis resulted in the collapse of the Ballantyne printing business, of which Scott was the only partner with a financial interest; the company's debts of £130,000 () caused his very public ruin. Rather than declare himself bankrupt, or to accept any kind of financial support from his many supporters and admirers (including the king himself), he placed his house and income in a trust belonging to his creditors, and determined to write his way out of debt. He kept up his prodigious output of fiction, as well as producing a biography of Napoleon Bonaparte, until 1831. By then his health was failing, but he nevertheless undertook a grand tour of Europe, and was welcomed and celebrated wherever he went. He returned to Scotland and, in September 1832, during the epidemic in Scotland that year, died of typhus at Abbotsford, the home he had designed and had built, near Melrose in the Scottish Borders. (His wife, Lady Scott, had died in 1826 and was buried as an Episcopalian.) Two Presbyterian ministers and one Episcopalian officiated at his funeral. Scott died owing money, but his novels continued to sell, and the debts encumbering his estate were discharged shortly after his death.

Scott married Charlotte Carpenter in St Mary's Church, Carlisle Cathedral on Christmas Eve 1797.

Scott's eldest son, Lt Walter Scott, inherited his father's estate and possessions. He married Jane Jobson, only daughter of William Jobson of Lochore (died 1822) and his wife Rachel Stuart (died 1863), on 3 February 1825.

Scott, Sr.'s lawyer from at least 1814 was Hay Donaldson WS (died 1822), who was also agent to the Duke of Buccleuch. Scott was Donaldson's proposer when he was elected a Fellow of the Royal Society of Edinburgh.

Scott was raised a Presbyterian but later also adhered to the Scottish Episcopal Church. Many have suggested this demonstrates both his nationalistic and unionistic tendencies. He was ordained as an elder in the Presbyterian Church of Duddington and sat in the General Assembly for a time as representative elder of the burgh of Selkirk. However, he received an Episcopal funeral at his own insistence. His Christian beliefs were explained and developed upon in his Religious Discourses of 1828.

His distant cousin was the poet Randall Swingler.

When Scott was a boy, he sometimes travelled with his father from Selkirk to Melrose, where some of his novels are set. At a certain spot the old gentleman would stop the carriage and take his son to a stone on the site of the Battle of Melrose (1526).

During the summers from 1804, Scott made his home at the large house of Ashestiel, on the south bank of the River Tweed north of Selkirk. When his lease on this property expired in 1811, Scott bought Cartley Hole Farm, downstream on the Tweed nearer Melrose. The farm had the nickname of "Clarty Hole", and when Scott built a family cottage there in 1812 he named it "Abbotsford". He continued to expand the estate, and built Abbotsford House in a series of extensions.
The farmhouse developed into a wonderful home that has been likened to a fairy palace. Scott was a pioneer of the Scottish Baronial style of architecture, therefore Abbotsford is festooned with turrets and stepped gabling. Through windows enriched with the insignia of heraldry the sun shone on suits of armour, trophies of the chase, a library of more than 9,000 volumes, fine furniture, and still finer pictures. Panelling of oak and cedar and carved ceilings relieved by coats of arms in their correct colours added to the beauty of the house.

It is estimated that the building cost Scott more than £25,000 (). More land was purchased until Scott owned nearly . A Roman road with a ford near Melrose used in olden days by the abbots of Melrose suggested the name of Abbotsford. Scott was buried in Dryburgh Abbey, where his wife had earlier been interred. Nearby is a large statue of William Wallace, one of Scotland's many romanticised historical figures. Abbotsford later gave its name to the Abbotsford Club, founded in 1834 in memory of Sir Walter Scott.

Although he continued to be extremely popular and widely read, both at home and abroad, Scott's critical reputation declined in the last half of the 19th century as serious writers turned from romanticism to realism, and Scott began to be regarded as an author suitable for children. This trend accelerated in the 20th century. For example, in his classic study "Aspects of the Novel" (1927), E. M. Forster harshly criticized Scott's clumsy and slapdash writing style, "flat" characters, and thin plots. In contrast, the novels of Scott's contemporary Jane Austen, once appreciated only by the discerning few (including, as it happened, Scott himself) rose steadily in critical esteem, though Austen, as a female writer, was still faulted for her narrow ("feminine") choice of subject matter, which, unlike Scott, avoided the grand historical themes traditionally viewed as masculine.

Nevertheless, Scott's importance as an innovator continued to be recognized. He was acclaimed as the inventor of the genre of the modern historical novel (which others trace to Jane Porter, whose work in the genre predates Scott's) and the inspiration for enormous numbers of imitators and genre writers both in Britain and on the European continent. In the cultural sphere, Scott's Waverley novels played a significant part in the movement (begun with James Macpherson's "Ossian" cycle) in rehabilitating the public perception of the Scottish Highlands and its culture, which had been formerly suppressed as barbaric, and viewed in the southern mind as a breeding ground of hill bandits, religious fanaticism, and Jacobite rebellions. Scott served as chairman of the Royal Society of Edinburgh and was also a member of the Royal Celtic Society. His own contribution to the reinvention of Scottish culture was enormous, even though his re-creations of the customs of the Highlands were fanciful at times, despite his extensive travels around his native country. It is a testament to Scott's contribution in creating a unified identity for Scotland that Edinburgh's central railway station, opened in 1854 by the North British Railway, is called Waverley. The fact that Scott was a Lowland Presbyterian, rather than a Gaelic-speaking Catholic Highlander, made him more acceptable to a conservative English reading public. Scott's novels were certainly influential in the making of the Victorian craze for all things Scottish among British royalty, who were anxious to claim legitimacy through their rather attenuated historical connection with the royal house of Stuart.

At the time Scott wrote, Scotland was poised to move away from an era of socially divisive clan warfare to a modern world of literacy and industrial capitalism. Through the medium of Scott's novels, the violent religious and political conflicts of the country's recent past could be seen as belonging to history—which Scott defined, as the subtitle of "Waverley" ("'Tis Sixty Years Since") indicates, as something that happened at least 60 years ago. Scott's advocacy of objectivity and moderation and his strong repudiation of political violence on either side also had a strong, though unspoken, contemporary resonance in an era when many conservative English speakers lived in mortal fear of a revolution in the French style on British soil. Scott's orchestration of King George IV's visit to Scotland, in 1822, was a pivotal event intended to inspire a view of his home country that, in his view, accentuated the positive aspects of the past while allowing the age of quasi-medieval blood-letting to be put to rest, while envisioning a more useful, peaceful future.

After Scott's work had been essentially unstudied for many decades, a revival of critical interest began from the 1960s. Postmodern tastes favoured discontinuous narratives and the introduction of the "first person", yet they were more favourable to Scott's work than Modernist tastes. While F. R. Leavis had disdained Scott, seeing him as a thoroughly bad novelist and a thoroughly bad influence ("The Great Tradition" [1948]), György Lukács ("The Historical Novel" [1937, trans. 1962]) and David Daiches ("Scott's Achievement as a Novelist" [1951]) offered a Marxian political reading of Scott's fiction that generated a great deal of genuine interest in his work. Scott is now seen as an important innovator and a key figure in the development of Scottish and world literature, and particularly as the principal inventor of the historical novel.

During his lifetime, Scott's portrait was painted by Sir Edwin Landseer and fellow Scots Sir Henry Raeburn and James Eckford Lauder. In Edinburgh, the 61.1-metre-tall Victorian Gothic spire of the Scott Monument was designed by George Meikle Kemp. It was completed in 1844, 12 years after Scott's death, and dominates the south side of Princes Street. Scott is also commemorated on a stone slab in Makars' Court, outside The Writers' Museum, Lawnmarket, Edinburgh, along with other prominent Scottish writers; quotes from his work are also visible on the Canongate Wall of the Scottish Parliament building in Holyrood. There is a tower dedicated to his memory on Corstorphine Hill in the west of the city and, as mentioned, Edinburgh's Waverley railway station takes its name from one of his novels.

In Glasgow, Walter Scott's Monument dominates the centre of George Square, the main public square in the city. Designed by David Rhind in 1838, the monument features a large column topped by a statue of Scott.
There is a statue of Scott in New York City's Central Park.

Numerous Masonic Lodges have been named after him and his novels. For example: Lodge Sir Walter Scott, No. 859 (Perth, Australia) and Lodge Waverley, No. 597 (Edinburgh, Scotland).

The annual Walter Scott Prize for Historical Fiction was created in 2010 by the Duke and Duchess of Buccleuch, whose ancestors were closely linked to Sir Walter Scott. At £25,000, it is one of the largest prizes in British literature. The award has been presented at Scott's historic home, Abbotsford House.

Scott has been credited with rescuing the Scottish banknote. In 1826, there was outrage in Scotland at the attempt of Parliament to prevent the production of banknotes of less than five pounds. Scott wrote a series of letters to the "Edinburgh Weekly Journal" under the pseudonym ""Malachi Malagrowther"" for retaining the right of Scottish banks to issue their own banknotes. This provoked such a response that the Government was forced to relent and allow the Scottish banks to continue printing pound notes. This campaign is commemorated by his continued appearance on the front of all notes issued by the Bank of Scotland. The image on the 2007 series of banknotes is based on the portrait by Henry Raeburn.

During and immediately after World War I there was a movement spearheaded by President Wilson and other eminent people to inculcate patriotism in American school children, especially immigrants, and to stress the American connection with the literature and institutions of the "mother country" of Great Britain, using selected readings in middle school textbooks. Scott's "Ivanhoe" continued to be required reading for many American high school students until the end of the 1950s.

A bust of Scott is in the Hall of Heroes of the National Wallace Monument in Stirling.

Letitia Elizabeth Landon was a great admirer of Scott and, on his death, she wrote two tributes to him: "On Walter Scott" in the Literary Gazette, and "Sir Walter Scott" in Fisher's Drawing Room Scrap Book, 1833. Towards the end of her life she began a series called "The Female Picture Gallery" with a series of character analyses based on the women in Scott's works.

In Charles Baudelaire's "La Fanfarlo" (1847), poet Samuel Cramer says of Scott:

In the novella, however, Cramer proves as deluded a romantic as any hero in one of Scott's novels.

In Anne Brontë's "The Tenant of Wildfell Hall" (1848) the narrator, Gilbert Markham, brings an elegantly bound copy of "Marmion" as a present to the independent "tenant of Wildfell Hall" (Helen Graham) whom he is courting, and is mortified when she insists on paying for it.

In a speech delivered at Salem, Massachusetts, on 6 January 1860, to raise money for the families of the executed abolitionist John Brown and his followers, Ralph Waldo Emerson calls Brown an example of true chivalry, which consists not in noble birth but in helping the weak and defenseless and declares that "Walter Scott would have delighted to draw his picture and trace his adventurous career".

In his 1870 memoir, "Army Life in a Black Regiment", New England abolitionist Thomas Wentworth Higginson (later editor of Emily Dickinson), described how he wrote down and preserved Negro spirituals or "shouts" while serving as a colonel in the First South Carolina Volunteers, the first authorized Union Army regiment recruited from freedmen during the Civil War (memorialized in the 1989 film "Glory"). He wrote that he was "a faithful student of the Scottish ballads, and had always envied Sir Walter the delight of tracing them out amid their own heather, and of writing them down piecemeal from the lips of aged crones".

According to his daughter Eleanor, Scott was "an author to whom Karl Marx again and again returned, whom he admired and knew as well as he did Balzac and Fielding".

In his 1883 "Life on the Mississippi", Mark Twain satirized the impact of Scott's writings, declaring (with humorous hyperbole) that Scott "had so large a hand in making Southern character, as it existed before the [American Civil] war", that he is "in great measure responsible for the war". He goes on to coin the term "Sir Walter Scott disease", which he blames for the South's lack of advancement. Twain also targeted Scott in "Adventures of Huckleberry Finn", where he names a sinking boat the "Walter Scott" (1884); and, in "A Connecticut Yankee in King Arthur's Court" (1889), the main character repeatedly utters "great Scott" as an oath; by the end of the book, however, he has become absorbed in the world of knights in armor, reflecting Twain's ambivalence on the topic.

The idyllic Cape Cod retreat of suffragists Verena Tarrant and Olive Chancellor in Henry James' "The Bostonians" (1886) is called Marmion, evoking what James considered the Quixotic idealism of these social reformers.

In "To the Lighthouse" by Virginia Woolf, Mrs. Ramsey glances at her husband:
In 1951, science-fiction author Isaac Asimov wrote "Breeds There a Man...?", a short story with a title alluding vividly to Scott's "The Lay of the Last Minstrel" (1805).

In "To Kill a Mockingbird" (1960), the protagonist's brother is made to read Walter Scott's book "Ivanhoe" to the ailing Mrs. Henry Lafayette Dubose, and he refers to the author as "Sir Walter Scout", in reference to his own sister's nickname.

In "Mother Night" (1961) by Kurt Vonnegut Jr., memoirist and playwright Howard W. Campbell Jr. prefaces his text with the six lines beginning "Breathes there the man..."

In "Knights of the Sea" (2010) by Canadian author Paul Marlowe, there are several quotes from and references to "Marmion", as well as an inn named after "Ivanhoe", and a fictitious Scott novel entitled "The Beastmen of Glen Glammoch".

The Waverley Novels is the title given to the long series of Scott novels released from 1814 to 1832 which takes its name from the first novel, "Waverley". The following is a chronological list of the entire series:


Other novels:


Many of the short poems or songs released by Scott (or later anthologized) were originally not separate pieces but parts of longer poems interspersed throughout his novels, tales, and dramas.








</doc>
<doc id="27885" url="https://en.wikipedia.org/wiki?curid=27885" title="Savoy">
Savoy

Savoy (; , ; ; ; ) is a cultural region in Western Europe. It comprises roughly the territory of the Western Alps between Lake Geneva in the north and Dauphiné in the south.

The historical land of Savoy emerged as the feudal territory of the House of Savoy during the 11th to 14th centuries. The historical territory is shared among the modern countries of France, Italy, and Switzerland.

Installed by Rudolph III, King of Burgundy, officially in 1003, the House of Savoy became the longest surviving royal house in Europe. It ruled the County of Savoy to 1416 and then the Duchy of Savoy from 1416 to 1860.

The territory of Savoy was annexed to France in 1792 under the French First Republic, before being returned to the Kingdom of Sardinia in 1815. Savoy, along with the county of Nice, was finally annexed to France by a plebiscite, under the Second French Empire in 1860, as part of a political agreement (Treaty of Turin) brokered between the French emperor Napoleon III and King Victor Emmanuel II of the Kingdom of Sardinia that began the final steps in the process of unification of Italy. Victor Emmanuel's dynasty, the House of Savoy, retained its Italian lands of Piedmont and Liguria and became the ruling dynasty of Italy.

In modern France, Savoy is part of the Auvergne-Rhône-Alpes region. Following its annexation to France in 1860, the territory of Savoy was divided administratively into two separate departments, Savoie and Haute-Savoie.

The traditional capital remains Chambéry (Ciamberì), on the rivers Leysse and Albane, hosting the castle of the House of Savoy and the Savoyard senate. The state included six districts:

The County and Duchy of Savoy incorporated Turin and other territories in Piedmont, a region in northwestern Italy that borders Savoy, which were also possessions of the House of Savoy. The capital of the Duchy remained at the traditional Savoyard capital of Chambéry until 1563, when it was moved to Turin.

The region was occupied by the Allobroges, a Gaulish people that the Roman Republic subdued in 121 BC. The name "Savoy" stems from the Late Latin "Sapaudia", referring to a fir forest. It is first recorded in Ammianus Marcellinus (354), to describe the southern part of "Maxima Sequanorum". According to the "Chronica Gallica" of 452, it was separated from the rest of Burgundian territories in 443, after the Burgundian defeat by Flavius Aetius.

By the 8th century, the territory that would later become known as Savoy was part of Francia, and at the division of Francia at the Treaty of Verdun in 843, it became part of the short-lived kingdom of Middle Francia. After only 12 years, at the death of Lothair I in 855, Middle Francia was divided into Lotharingia north of the Alps, Italy south of the Alps, and the parts of Burgundy in the Western Alps, inherited by Charles of Provence. This latter territory comprised what would become known as Savoy and Provence.

From the 10th to 14th century, parts of what would ultimately become Savoy remained within the Kingdom of Arles. Beginning in the 11th century, the gradual rise to power of the House of Savoy is reflected in the increasing territory of their County of Savoy between 1003 and 1416.

The County of Savoy was detached "de jure" from the Kingdom of Arles by Charles IV, Holy Roman Emperor in 1361. It acquired the County of Nice in 1388, and in 1401 added the County of Geneva, the area of Geneva except for the city proper, which was ruled by its prince-bishop, nominally under the duke's rule: the bishops of Geneva, by unspoken agreement, came from the House of Savoy; this agreement came to an end in 1533.

On February 19, 1416, Sigismund, Holy Roman Emperor, made the County of Savoy an independent duchy, with Amadeus VIII as the first duke. Straddling the Alps, Savoy lay within two competing spheres of influence, a French sphere and a North Italian one. At the time of the Renaissance, Savoy showed only modest development. Its towns were few and small. Savoy derived its subsistence from agriculture. The geographic location of Savoy was also of military importance. During the interminable wars between France and Spain over the control of northern Italy, Savoy was important to France because it provided access to Italy. Savoy was important to Spain because it served as a buffer between France and the Spanish held lands in Italy. In 1563 Emmanuel Philibert moved the capital from Chambéry to Turin, which was less vulnerable to French interference.

Vaud was annexed by Bern in 1536, and Savoy officially ceded Vaud to Bern in the Treaty of Lausanne of 30 October 1564.

In 1714, as a consequence of the War of the Spanish Succession, Savoy was technically subsumed into the Kingdom of Sicily, then (after that island was traded to Austria for Sardinia) the Kingdom of Sardinia from 1720.
While the heads of the House of Savoy were known as the Kings of Sardinia, Turin remained their capital.

Savoy was occupied by French revolutionary forces between 1792 and 1815. The region was first added to the département of Mont-Blanc, then in 1798 was divided between the départements of Mont-Blanc and Léman (French name of Lake Geneva.) In 1801, Savoy officially left the Holy Roman Empire. On September 13, 1793 the combined forces of Savoy, Piedmont and Aosta Valley fought against and lost to the occupying French forces at the Battle of Méribel (Sallanches). Two-thirds of Savoy was restored to the Kingdom of Sardinia in the First Restoration of 1814 following Napoleon's abdication; approximately one-third of Savoy, including the two most important cities of Chambéry and Annecy, remained in France. Following Napoleon's brief return to power during the Hundred Days and subsequent defeat at Waterloo, the remaining one-third of Savoy was restored to the Kingdom of Sardinia at the Congress of Vienna to strengthen Sardinia as a buffer state on France's southeastern border.

From 1815 until 1860, Savoy was part of the Kingdom of Sardinia.

The French Second Republic first attempted to annex Savoy in 1848. A corps of 1,500 was dispatched from Lyon and invaded Savoy on 3 April, occupying the capital, Chambéry, and proclaiming the annexation to France. On learning about the invasion countrymen rushed to Chambéry. The corps were chased away by the local population. Five Frenchmen were killed and 800 captured.

On July 21, 1858 in Plombières (Vosges) the prime minister of the Kingdom of Sardinia, Camillo Benso, Count of Cavour, met in secret with Napoleon III to secure French military support against the Austrian Empire during the conflicts associated with the Italian unification. During the discussion, Cavour promised that Sardinia would cede the County of Nice and Duchy of Savoy to the Second French Empire. Though this was a secret arrangement, it quickly became widely known.

The treaty annexing Nice and Savoy to France was signed in Turin on March 24, 1860 (Treaty of Turin). In the northern provinces of the Chablais and Faucigny, there was some support for annexation to neighboring Switzerland, with which the northern provinces had longstanding economic ties. To help reduce the attractiveness of Switzerland, the French government conceded a free-trade Zone that maintained the longstanding duty-free relationship of northern Savoyard communes to Geneva. The treaty was followed on April 22–23 by a plebiscite employing universal male suffrage, in which voters were offered the option of voting "yes" to approve the treaty and join France or rejecting the treaty with a no vote. The disallowed options of either joining Switzerland, remaining with Italy, or regaining its independence, were the source of some opposition. With a 99.8% vote in favour of joining France, there were allegations of vote-rigging, notably by the British government, which opposed continental expansion by its traditional French enemy.

The correspondent of "The Times" in Savoy who was in Bonneville on April 22 called the vote "the lowest and most immoral farce(s) which was ever played in the history of nations". He finished his letter with those words: I leave you to draw your own conclusions from this trip, which will show clearly what the vote was in this part of Savoy. The vote was the bitterest irony ever made on popular suffrage. The ballot-box in the hands of those very authorities who issued the proclamations; no control possible; even travellers suspected and dogged lest they should pry into the matter; all opposition put down by intimidation, and all liberty of action completely taken away. One can really scarcely reproach the Opposition with having given up the game; there was too great force used against them. As for the result of the vote, therefore, no one need trouble himself about it; it will be just as brilliant as that in Nice. The only danger is lest the Savoy authorities in their zeal should fare as some of the French did in the vote of 1852, finding to their surprise rather more votes than voters inscribed on the list. In his letter to the ambassador of Vienna Lord A. Loftus, the then-Secretary of State for Foreign and Commonwealth Affairs, John Russell, 1st Earl Russell, said, "Voting in Savoy and Nice a farce ... we are neither entertained or edified".

The annexation was promulgated on June 14, 1860. On August 23, 1860 and March 7, 1861, two agreements were signed between the Second French Empire and the Kingdom of Sardinia to settle the remaining issues concerning the annexation.

In 1919, France officially (but contrary to the annexation treaty) ended the military neutrality of the parts of the country of Savoy that had originally been agreed to at the Congress of Vienna, and also eliminated the free trade zone – both treaty articles having been broken unofficially in World War I. France was condemned in 1932 by the international court for noncompliance with the measures of the Treaty of Turin regarding the provinces of Savoy and Nice.

In 1960, the term "annexation" having acquired negative connotations in France, particularly after Germany's 1871 annexation of Alsace-Lorraine, the annexation was renamed "Rattachement de la Savoie à la France" (Incorporation of Savoy to France). It was the latter term which was used by the French authorities during the festivities celebrating the 100th anniversary of the annexation. Daniel Rops of the French Academy justified the new title with these words:

Savoy has begun to solemnize the feasts in 1960, commemorating the centenary of its incorporation ("rattachement") to France. It is on purpose that the word incorporation ("rattachement") is highlighted here: the Savoyards attach great value to it, and it is the only one they have resolved to use in the official terminology of the Centenary. In that, they are infinitely right.
Yesterday another term that was used: annexation. Looking at it more closely it was wrong! Can we say annexation when we talk about a decision which was approved by 130,889 voters over 135,449? [...]. Savoy was not annexed [...] but actually incorporated freely and by the will of its inhabitants.
A former French deputy, P. Taponnier, spoke of the annexation:

In late March 1860, the betrothal ceremony of Savoy to France took place in Tuileries Palace [...], a ceremony which was a pact of love and fidelity [...] it is with free consent that she [Savoy] gave itself to France by a solemn plebiscite of which our leaders can ignore neither the terms nor the commitments. [...] May the bells of our cities [...] in Savoy vibrate in unison to glorify, in this magnificent Centenary, the indefectible commitment of Savoy to France.
The Savoyards did not feel Italian. Besides, they spoke French. This explains why in 1858–1859 when rumours ran of the Plombières secret agreement, where Napoleon III and Cavour decided of the fate of Savoy, the Savoyards themselves took the initiative to ask for the incorporation ("rattachement"). [...] Incorporation, not annexation [...] The incorporation was an act of free will, in the logical order of geography and history [...].

Since the mid-20th century, regionalist movements have appeared in Savoy much as in other historic provinces of France. The "Mouvement Région Savoie" (Savoy Regional Movement) was founded in December 1971 as a 'movement' (rather than a traditional political party) in favour of regional autonomy. Unlike other historic provinces, including Normandy and Brittany, Savoy does not currently have its own region within France and is part of the Auvergne-Rhône-Alpes region. In the 1996 local elections, the Savoy Regional Movement received 19,434 votes; it received 4,849 in the 1998 regional elections. A new non-party organisation, "La Région Savoie, j’y crois !" ("I believe in the Savoy Region!"), was founded in 1998. The organisation campaigns for the replacement of the Savoie and Haute-Savoie departments with a regional government, separate from the Auvergne-Rhône-Alpes region, with greater devolved powers.

A very marginal separatist movement has also appeared in Savoy within the past twenty years, most prominently represented by the "Ligue Savoisienne", founded in 1994. In the March 1998 regional elections, 1 seat (out of 23) was won by Patrice Abeille, leader of the Ligue, which won a total of 17,865 votes across the two departments. In 2004, "Waiting for Freedom in Savoy" was founded to promote the peaceful separatist cause to young people.

According to surveys conducted in 2000, between 41% and 55% of the population were in favour of the proposal for a separate Savoy region, while 19% to 23% were in favour of separation from France. Towards the end of 2005, Hervé Gaymard called for Savoie to be given special status, similar to a French region, under his proposed "Conseil des Pays de Savoie".

In recent years, sparked by the tiny Savoyard separatist movement, much attention has been focused on questioning the validity of the 1860 annexation. The Ligue Savoisienne, for example, rejects the Treaty of Turin and subsequent plebiscite as null and void, arguing that the plebiscite did not meet the standards of a free and fair vote. Today, historians generally acknowledge that the plebiscite of 1860 did feature irregularities, but they also affirm that the annexation instrument was the Treaty of Turin and not the plebiscite, whose main purpose was to demonstrate favorable public opinion in Savoy for the annexation after the signature of the treaty. In an interview for the newspaper "Le Dauphiné Libéré", Sylvain Milbach, a historian at the University of Savoy, qualifies the vote as Napoleonic, but also argues that a completely free and fair vote would not have dramatically changed the outcome, as the majority of Savoyards wished to become French. This is today the official stance of the General Council of Savoie.




</doc>
<doc id="27886" url="https://en.wikipedia.org/wiki?curid=27886" title="Suffolk">
Suffolk

Suffolk () is an East Anglian county of historic origin in England. It has borders with Norfolk to the north, Cambridgeshire to the west and Essex to the south. The North Sea lies to the east. The county town is Ipswich; other important towns include Lowestoft, Bury St Edmunds, Newmarket and Felixstowe, one of the largest container ports in Europe.

The county is low-lying with very few hills, and is largely arable land with the wetlands of the Broads in the north. The Suffolk Coast and Heaths are an Area of Outstanding Natural Beauty.

By the fifth century, the Angles (after whom East Anglia and England are named) had established control of the region. The Angles later became the "north folk" and the "south folk", from which developed the names "Norfolk" and "Suffolk". Suffolk and several adjacent areas became the kingdom of East Anglia, which later merged with Mercia and then Wessex.

Suffolk was originally divided into four separate Quarter Sessions divisions. In 1860, the number of divisions was reduced to two. The eastern division was administered from Ipswich and the western from Bury St Edmunds. Under the Local Government Act 1888, the two divisions were made the separate administrative counties of East Suffolk and West Suffolk; Ipswich became a county borough. A few Essex parishes were also added to Suffolk: Ballingdon-with-Brundon and parts of Haverhill and Kedington.

On 1 April 1974, under the Local Government Act 1972, East Suffolk, West Suffolk, and Ipswich were merged to form the unified county of Suffolk. The county was divided into several local government districts: Babergh, Forest Heath, Ipswich, Mid Suffolk, St. Edmundsbury, Suffolk Coastal, and Waveney. This act also transferred some land near Great Yarmouth to Norfolk. As introduced in Parliament, the Local Government Act would have transferred Newmarket and Haverhill to Cambridgeshire and Colchester from Essex; such changes were not included when the act was passed into law.

In 2007, the Department for Communities and Local Government referred Ipswich Borough Council's bid to become a new unitary authority to the Boundary Committee. The Boundary Committee consulted local bodies and reported in favour of the proposal. It was not, however, approved by the Secretary of State for Communities and Local Government.

Beginning in February 2008, the Boundary Committee again reviewed local government in the county, with two possible options emerging. One was that of splitting Suffolk into two unitary authorities – Ipswich and Felixstowe and Rural Suffolk; and the other, that of creating a single county-wide controlling authority – the "One Suffolk" option. In February 2010, the then-Minister Rosie Winterton announced that no changes would be imposed on the structure of local government in the county as a result of the review, but that the government would be: "asking Suffolk councils and MPs to reach a consensus on what unitary solution they want through a countywide constitutional convention". Following the May 2010 general election, all further moves towards any of the suggested unitary solutions ceased on the instructions of the incoming Coalition government. In 2018 it was determined that Forest Heath and St Edmundsbury will be merged to form a new West Suffolk district; while Waveney and Suffolk Coastal will similarly form a new East Suffolk district. These changes will take effect on 1st April 2019.

West Suffolk, like nearby East Cambridgeshire, is renowned for archaeological finds from the Stone Age, the Bronze Age, and the Iron Age. Bronze Age artefacts have been found in the area between Mildenhall and West Row, in Eriswell and in Lakenheath.
Many bronze objects, such as swords, spearheads, arrows, axes, palstaves, knives, daggers, rapiers, armour, decorative equipment (in particular for horses), and fragments of sheet bronze, are entrusted to St. Edmundsbury heritage service, housed at West Stow just outside Bury St. Edmunds. Other finds include traces of cremations and barrows.

In the east of the county is Sutton Hoo, the site of one of England's most significant Anglo-Saxon archaeological finds, a ship burial containing a collection of treasures including a Sword of State, gold and silver bowls, and jewellery and a lyre.

The majority of agriculture in Suffolk is either arable or mixed. Farm sizes vary from anything around 80 acres (32 hectares) to over 8,000. Soil types vary from heavy clays to light sands. Crops grown include:winter wheat, winter barley, sugar beet, oilseed rape, winter and spring beans and linseed, although smaller areas of rye and oats can be found growing in areas with lighter soils along with a variety of vegetables.

The continuing importance of agriculture in the county is reflected in the Suffolk Show, which is held annually in May at Ipswich. Although latterly somewhat changed in nature, this remains primarily an agricultural show.

Below is a chart of regional gross value added of Suffolk at current basic prices published by "Office for National Statistics" with figures in millions of British Pounds Sterling.

Well-known companies in Suffolk include Greene King and Branston Pickle in Bury St Edmunds. Birds Eye has its largest UK factory in Lowestoft, where all its meat products and frozen vegetables are processed. Huntley & Palmers biscuit company has a base in Sudbury. The UK horse racing industry is based in Newmarket. There are two USAF bases in the west of the county close to the A11. Sizewell B nuclear power station is at Sizewell on the coast near Leiston. Bernard Matthews Farms have some processing units in the county, specifically Holton. Southwold is the home of Adnams Brewery. The Port of Felixstowe is the largest container port in the United Kingdom. Other ports are at Lowestoft and Ipswich, run by Associated British Ports. BT has its main research and development facility at Martlesham Heath.

There are several towns in the county with Ipswich being the largest and most populous. At the time of the 2011 census, a population of 730,000 lived in the county with 133,384 living in Ipswich. The table below shows all towns with over 20,000 inhabitants.
Located in the East of England, much of Suffolk is low-lying, founded on Pleistocene sand and clays. These rocks are relatively unresistant and the coast is eroding rapidly. Coastal defences have been used to protect several towns, but several cliff-top houses have been lost to coastal erosion and others are under threat. The continuing protection of the coastline and the estuaries, including the Blyth, Alde and Deben, has been, and remains, a matter of considerable discussion.

The coastal strip to the East contains an area of heathland known as "The Sandlings" which runs almost the full length of the coastline. Suffolk is also home to nature reserves, such as the RSPB site at Minsmere, and Trimley Marshes, a wetland under the protection of Suffolk Wildlife Trust.

The west of the county lies on more resistant Cretaceous chalk. This chalk is responsible for a sweeping tract of largely downland landscapes that stretches from Dorset in the south west to Dover in the south east and north through East Anglia to the Yorkshire Wolds. The chalk is less easily eroded so forms the only significant hills in the county. The highest point in the county is Great Wood Hill, the highest point of the Newmarket Ridge, near the village of Rede, which reaches .

The county flower is the oxlip.

According to estimates by the Office for National Statistics, the population of Suffolk in 2014 was 738,512, split almost evenly between males and females. Roughly 22% of the population was aged 65 or older, and 90.84% were "White British".

Historically, the county's population has mostly been employed as agricultural workers. An 1835 survey showed Suffolk to have 4,526 occupiers of land employing labourers, 1,121 occupiers not employing labourers, 33,040 labourers employed in agriculture, 676 employed in manufacture, 18,167 employed in retail trade or handicraft, 2,228 'capitalists, bankers etc.', 5,336 labourers (non-agricultural), 4,940 other males aged over 20, 2,032 male servants and 11,483 female servants. The same publication records the total population of the county at 296,304.

Most English counties have nicknames for people from that county, such as a Tyke from Yorkshire and a Yellowbelly from Lincolnshire; the traditional nickname for people from Suffolk is 'Suffolk Fair-Maids', or 'Silly Suffolk', referring respectively to the supposed beauty of its female inhabitants in the Middle Ages, and to the long history of Christianity in the county and its many fine churches (from Anglo-Saxon "selige", originally meaning holy).

In the arts, Suffolk is noted for having been the home to two of England's best regarded painters, Thomas Gainsborough and John Constable – the Stour Valley area is branded as "Constable Country" – and one of its most noted composers, Benjamin Britten. Other artists of note from Suffolk include the cartoonist Carl Giles (a bronze statue of his character "Grandma" to commemorate this is located in Ipswich town centre), poets George Crabbe and Robert Bloomfield, writer and Literary editor Ronald Blythe, actors Ralph Fiennes and Bob Hoskins, actress and singer Kerry Ellis, musician and record producer Brian Eno, singer Dani Filth, of the Suffolk-based extreme metal group, Cradle of Filth, singer-songwriter Ed Sheeran, and coloratura soprano Christina Johnston. Hip-hop DJ Tim Westwood is originally from Suffolk and the influential DJ and radio presenter John Peel made the county his home. Contemporary painter, Maggi Hambling, was born, and resides, in Suffolk.

Suffolk's contributions to sport include Formula One magnate Bernie Ecclestone and former England footballers Terry Butcher, Kieron Dyer and Matthew Upson. Due to Newmarket being the centre of British horse racing many jockeys have settled in the county, including Lester Piggott and Frankie Dettori.

Significant ecclesiastical figures from Suffolk include Simon Sudbury, a former Archbishop of Canterbury; Tudor-era Catholic prelate Thomas, Cardinal Wolsey; and author, poet and Benedictine monk John Lydgate.

Other significant persons from Suffolk include the suffragette Dame Millicent Garrett Fawcett; the captain of "HMS Beagle", Robert FitzRoy; Witch-finder General Matthew Hopkins; educationist Hugh Catchpole; and Britain's first female physician and mayor, Elizabeth Garrett Anderson. Charity leader Sue Ryder settled in Suffolk and based her charity in Cavendish.

King of East Anglia and Christian martyr St Edmund (after whom the town of Bury St Edmunds is named) was killed by invading Danes in the year 869. St Edmund was the patron saint of England until he was replaced by St George in the 13th century. 2006 saw the failure of a campaign to have St Edmund named as the patron saint of England, but in 2007 he was named patron saint of Suffolk, with St Edmund's Day falling on 20 November. His flag is flown in Suffolk on that day.

Suffolk has a comprehensive education system with fourteen independent schools. Unusually for the UK, some of Suffolk has a 3-tier school system in place with primary schools (ages 5–9), middle schools (ages 9–13) and upper schools (ages 13–16). However, a 2006 Suffolk County Council study concluded that Suffolk should move to the 2-tier school system used in the majority of the UK. For the purpose of conversion to 2-tier, the 3-tier system has been divided into 4 geographical area groupings and corresponding phases. The first phase was the conversion of schools in Lowestoft and Haverhill in 2011, followed by schools in north and west Suffolk in 2012. The remainder of the changeovers to 2-tier will take place from 2013, for those schools that stay within Local government control, and not become Academies and/or free schools. The majority of schools thus now (2013) operate the more common primary to high school (11–16). Many of the county's upper schools have a sixth form and most further education colleges in the county offer A-level courses. In terms of school population, Suffolk's individual schools are large with the Ipswich district with the largest school population and Forest Heath the smallest, with just two schools.
In 2013, a letter said that "...nearly a fifth of the schools inspected were judged inadequate. This is unacceptable and now means that Suffolk has a higher proportion of pupils educated in inadequate schools than both the regional and national averages."

The Royal Hospital School near Ipswich is the largest independent boarding school in Suffolk. Other boarding schools within Suffolk include Culford School, Framlingham College, Barnardiston Hall Prepatory School, Saint Felix School and Finborough School. 

The Castle Partnership Academy Trust in Haverhill is the county's only All-through Academy Chain. Comprising Castle Manor Academy and Place Farm Primary Academy, the Academy Trust supports all-through education and provides opportunities for young people aged 3 to 18.

Sixth form colleges in the county include Lowestoft Sixth Form College and One in Ipswich. Suffolk is home to four further education colleges: Lowestoft College, Easton & Otley College, Suffolk New College (Ipswich) and West Suffolk College (Bury St Edmunds).

The county has one university, with branches spread across different towns. University of Suffolk was, prior to August 2016, known as University Campus Suffolk. Up until it became independent it was a collaboration between the University of Essex and the University of East Anglia which sponsored its formation and validated its degrees. UOS accepted its first students in September 2007. Until then Suffolk was one of only four counties in England which did not have a University campus. The University of Suffolk was granted Taught Degree Awarding Powers by the Quality Assurance Agency for Higher Education in November 2015, and in May 2016 it was awarded University status by the Privy Council and renamed The University of Suffolk on 1 August 2016.

The University operates at five sites with its central hub in Ipswich. Others include Lowestoft, Bury St. Edmunds, and Great Yarmouth in Norfolk. The University operates two academic faculties and in had students. Some 30% of the student body are classed as mature students and 68% of University students are female.

Founded in 1948 by Benjamin Britten, the annual Aldeburgh Festival is one of the UK's major classical music festivals. Originating in Aldeburgh, it has been held at the nearby Snape Maltings since 1967. Since 2006, Henham Park, has been home to the annual Latitude Festival. This mainly open-air festival, which has grown considerably in size and scope, includes popular music, comedy, poetry and literary events. The FolkEast festival is held at Glemham Hall in August and attracts international acoustic, folk and roots musicians whilst also championing local businesses, heritage and crafts. In 2015 it was also home to the first instrumental festival of musical instruments and makers. More recently, LeeStock Music Festival has been held in Sudbury. A celebration of the county, "Suffolk Day", was instigated in 2017.
The Suffolk dialect is very distinctive. Epenthesis and yod-dropping is common, along with non-conjugation of verbs.

The county's sole professional football club is Ipswich Town. Formed in 1878, the club were Football League champions in 1961–62, FA Cup winners in 1977–78 and UEFA Cup winners in 1980–81. Ipswich Town currently play in the Football League Championship, the second tier of English football. The next highest ranked teams in Suffolk are Leiston, Lowestoft Town and Needham Market, who all participate in the Isthmian League Premier Division, the seventh tier of English football.

The town of Newmarket is the headquarters of British horseracing – home to the largest cluster of training yards in the country and many key horse racing organisations including the National Stud, and Newmarket Racecourse. Tattersalls bloodstock auctioneers and the National Horseracing Museum are also in the town. Point to point racing takes place at Higham and Ampton.

Speedway racing has been staged in Suffolk since at least the 1950s, following the construction of the Foxhall Stadium, just outside Ipswich, home of the Ipswich Witches. The Witches are currently members of the Premier League, the UK's second division. National League team Mildenhall Fen Tigers are also from Suffolk. 

Suffolk C.C.C. compete in the Eastern Division of the Minor Counties Championship. The club has won the championship three times outright and has shared the title one other time as well as winning the MCCA Knockout Trophy once. Home games are played in Bury St Edmunds, Copdock, Exning, Framlingham, Ipswich and Mildenhall.

Novels set in Suffolk include parts of "David Copperfield" by Charles Dickens, "The Fourth Protocol", by Frederick Forsyth, "Unnatural Causes" by P.D. James, Dodie Smith's "The Hundred and One Dalmatians", "The Rings of Saturn" by W. G. Sebald, and among Arthur Ransome's children's books, "We Didn't Mean to Go to Sea, Coot Club" and "Secret Water" take place in part in the county. Roald Dahl's short story "The Mildenhall Treasure" is set in Mildenhall.

A TV series about a British antiques dealer, "Lovejoy", was filmed in various locations in Suffolk. The reality TV Series "Space Cadets" was filmed in Rendlesham Forest, although the producers fooled participants into believing that they were in Russia. Several towns and villages in the county have been used for location filming of other television programmes and cinema films. These include the BBC Four TV series "Detectorists", an episode of "Kavanagh QC," and the films "Iris" and "Drowning by Numbers".

The Rendlesham Forest Incident is one of the most famous UFO events in England and is sometimes referred to as "Britain's Roswell".

The song "Castle on the Hill" by singer-songwriter Ed Sheeran was referred to by him as "a love letter to Suffolk", with lyrical reference to his hometown of Framlingham and Framlingham Castle.




</doc>
<doc id="27888" url="https://en.wikipedia.org/wiki?curid=27888" title="Scylla">
Scylla

In Greek mythology, Scylla ( ; , , "Skylla") was a monster that lived on one side of a narrow channel of water, opposite her counterpart Charybdis. The two sides of the strait were within an arrow's range of each other—so close that sailors attempting to avoid Charybdis would pass dangerously close to Scylla and vice versa.

Scylla made her first appearance in Homer's "Odyssey", where Odysseus and his crew encounter her and Charybdis on their travels. Later myth gave her an origin story as a beautiful nymph who gets turned into a monster.

The strait where Scylla dwelled has been associated with the Strait of Messina between Italy and Sicily, for example, as in Book Three of Virgil's "Aeneid". The idiom "between Scylla and Charybdis" has come to mean being forced to choose between two similarly dangerous situations.

The parentage of Scylla varies according to author. Homer, Ovid, Apollodorus, Servius, and a scholiast on Plato, all name Crataeis as the mother of Scylla. Neither Homer nor Ovid mention a father, but Apollodorus says that the father was either Trienus (Triton?) or Phorcus (a variant of Phorkys), similarly the Plato scholiast, perhaps following Apollodorus, gives the father as Tyrrhenus or Phorcus, while Eustathius on Homer, "Odyssey" 12.85, gives the father as Triton.

Other authors have Hecate as Scylla's mother. The Hesiodic "Megalai Ehoiai" gives Hecate and Phoebus Apollo as the parents of Scylla, while Acusilaus says that Scylla's parents were Hecate and Phorkys (so also schol. "Odyssey" 12.85).

Perhaps trying to reconcile these conflicting accounts, Apollonius of Rhodes says that Crataeis was another name for Hecate, and that she and Phorcys were the parents of Scylla. Likewise, Semos of Delos ("FGrHist" 396 F 22) says that Crataeis was the daughter of Hecate and Triton, and mother of Scylla by Deimos. Stesichorus (alone) names Lamia as the mother of Scylla, possibly the Lamia who was the daughter of Poseidon, while according to Hyginus, Scylla was the offspring of Typhon and Echidna.

According to John Tzetzes and Servius' commentary on the "Aeneid", Scylla was a beautiful naiad who was claimed by Poseidon, but the jealous Amphitrite turned her into a monster by poisoning the water of the spring where Scylla would bathe.

A similar story is found in Hyginus, according to whom Scylla was loved by Glaucus, but Glaucus himself was also loved by the goddess sorceress Circe. While Scylla was bathing in the sea, the jealous Circe poured a baleful potion into the sea water which caused Scylla to transform into a frightful monster with four eyes and six long snaky necks equipped with grisly heads, each of which contained three rows of sharp shark's teeth. Her body consisted of 12 tentacle-like legs and a cat's tail, while six dog's heads ringed her waist. In this form, she attacked the ships of passing sailors, seizing one of the crew with each of her heads.

In a late Greek myth, recorded in Eustathius' commentary on Homer and John Tzetzes, Heracles encountered Scylla during a journey to Sicily and slew her. Her father, the sea-god Phorcys, then applied flaming torches to her body and restored her to life.

In Homer's "Odyssey" XII, Odysseus is advised by Circe to sail closer to Scylla, for Charybdis could drown his whole ship: "Hug Scylla's crag—sail on past her—top speed! Better by far to lose six men and keep your ship than lose your entire crew." She also tells Odysseus to ask Scylla's mother, the river nymph Crataeis, to prevent Scylla from pouncing more than once. Odysseus successfully navigates the strait, but when he and his crew are momentarily distracted by Charybdis, Scylla snatches six sailors off the deck and devours them alive.
""...they writhed<br>gasping as Scylla swung them up her cliff and there<br>at her cavern's mouth she bolted them down raw—<br>screaming out, flinging their arms toward me,<br>lost in that mortal struggle.""

According to Ovid, the fisherman-turned-sea god Glaucus falls in love with the beautiful Scylla, but she is repulsed by his piscine form and flees to a promontory where he cannot follow. When Glaucus goes to Circe to request a love potion that will win Scylla's affections, the enchantress herself becomes enamored with him. Meeting with no success, Circe becomes hatefully jealous of her rival and therefore prepares a vial of poison and pours it into the sea pool where Scylla regularly bathed, transforming her into a thing of terror even to herself.""In vain she offers from herself to run<br>And drags about her what she strives to shun."" The story was later adapted into a five-act tragic opera, "Scylla et Glaucus" (1746), by the French composer Jean-Marie Leclair.

In John Keats' loose retelling of Ovid's version of the myth of Scylla and Glaucus in Book 3 of "Endymion" (1818), the evil Circe does not transform Scylla into a monster but merely murders the beautiful nymph. Glaucus then takes her corpse to a crystal palace at the bottom of the ocean where lie the bodies of all lovers who have died at sea. After a thousand years, she is resurrected by Endymion and reunited with Glaucus.

At the Carolingian abbey of Corvey in Westphalia, a unique ninth-century wall painting depicts, among other things, Odysseus' fight with Scylla, an illustration not noted elsewhere in medieval arts.

In the Renaissance and after, it was the story of Glaucus and Scylla that caught the imagination of painters across Europe. In Agostino Carracci's 1597 fresco cycle of "The Loves of the Gods" in the Farnese Gallery, the two are shown embracing, a conjunction that is not sanctioned by the myth. More orthodox versions show the maiden scrambling away from the amorous arms of the god, as in the oil on copper painting of Fillipo Lauri and the oil on canvas by Salvator Rosa in the Musée des Beaux-Arts de Caen.

Other painters picture them divided by their respective elements of land and water, as in the paintings of the Flemish Bartholomäus Spranger (1587), now in the Kunsthistorisches Museum, Vienna. Some add the detail of Cupid aiming at the sea-god with his bow, as in the painting of Laurent de la Hyre (1640/4) in the J. Paul Getty Museum and that of Jacques Dumont le Romain (1726) at the Musée des beaux-arts de Troyes. Two cupids can also be seen fluttering around the fleeing Scylla in the late painting of the scene by J. M. W. Turner (1841), now in the Kimbell Art Museum.

Peter Paul Rubens shows the moment when the horrified Scylla first begins to change, under the gaze of Glaucus (c.1636), while Eglon van der Neer's 1695 painting in the Rijksmuseum shows Circe poisoning the water as Scylla prepares to bathe. There are also two Pre-Raphaelite treatments of the latter scene by John Melhuish Strudwick (1886) and John William Waterhouse ("Circe Invidiosa", 1892).




</doc>
<doc id="27889" url="https://en.wikipedia.org/wiki?curid=27889" title="September 22">
September 22




</doc>
<doc id="27891" url="https://en.wikipedia.org/wiki?curid=27891" title="Solitaire">
Solitaire

Solitaire is any tabletop game which one can play by oneself. The term "solitaire" is also used for single-player games of concentration and skill using a set layout of tiles, pegs or stones rather than cards. These games include peg solitaire and mahjong solitaire. Most solitaire games function as a puzzle which, due to a different starting position, may (or may not) be solved in a different fashion each time.



</doc>
<doc id="27893" url="https://en.wikipedia.org/wiki?curid=27893" title="Glossary of patience terms">
Glossary of patience terms

There are a number of common features in many Patience games or solitaire games as they are called in the US, such as "building down" and the "foundations" and "tableau", used to simplify the description of new games.

The layout describes the piles of cards in use during the game, and the restrictions on these piles. There are a number of different kinds of piles which have become standard across a number of games.

Building (or Packing) involves cards being placed in stacks or cascades according to various rules. The "Building" terms are usually combined in game explanations. For instance, a game may describe "building up in sequence by suit". The terms in this table are generally preceded by the word "building" (as in the previous sentence).

The terms above are useful for describing the rules of the game. The terms in this section tend to be more useful for describing things happening during the state of play. Most are derived from Lady Cadogan (see below).

Here are a few additional terms used by Peter Arnold in his book "Card Games for One" () and may be terms exclusively used in British English in explaining solitaire games.


Terms from the book Lady Cadogan's "Illustrated Games of Patience". This defines the forgotten term. Talon (or Stock), which is still in use in Germany. Also note the term Marriage of cards.




</doc>
<doc id="27899" url="https://en.wikipedia.org/wiki?curid=27899" title="Syrinx">
Syrinx

In classical Greek mythology, Syrinx (Greek Σύριγξ) was a nymph and a follower of Artemis, known for her chastity. Pursued by the amorous god Pan, she ran to a river's edge and asked for assistance from the river nymphs. In answer, she was transformed into hollow water reeds that made a haunting sound when the god's frustrated breath blew across them. Pan cut the reeds to fashion the first set of pan pipes, which were thenceforth known as "syrinx". The word "syringe" was derived from this word.

The story of the syrinx is told in Achilles Tatius' "Leukippe and Kleitophon" where the heroine is subjected to a virginity test by entering a cave where Pan has left syrinx pipes that will sound a melody if she passes.
The story became popular among artists and writers in the 19th century. The Victorian artist and poet Thomas Woolner wrote "Silenus", a long narrative poem about the myth, in which Syrinx becomes the lover of Silenus, but drowns when she attempts to escape rape by Pan. As a result of the crime, Pan is transmuted into a demon figure and Silenus becomes a drunkard. Amy Clampitt's poem "Syrinx" refers to the myth by relating the whispering of the reeds to the difficulties of language.

Longus makes reference to Syrinx in his tale of "Daphnis and Chloe" in Book 2:34. Whilst the description of the tale here is modified to that of Ovid, it nevertheless incorporates Pan's desire to have her. Longus, however, makes no reference to Syrinx receiving aid from the Nymphs in his version, instead Syrinx hides from Pan in amongst some reeds and disappeared into the marsh. Upon realising what had happened to Syrinx, Pan created the first set of panpipes from the reeds she was transformed into, allowing her to be with him for the rest of his days.

The story was used as a central theme by Aifric Mac Aodha in her poetry collection "Gabháil Syrinx".

Samuel R. Delany features an instrument called a syrynx in his science-fiction novel "Nova".

Syrinx is the name of one of the main characters in the Night's Dawn Trilogy of space opera novels by British author Peter F. Hamilton. In the trilogy, Syrinx is a member of the transhumanist future society known as Edenism, and serves as the captain of the "Oenone", a living starship.

A 1972 poem by James Merrill, titled "Syrinx", draws on several aspects on the mythological tale, with the poet himself identifying with the celebrated nymph, desiring to become not just a "reed" but a "thinking reed" (in contrast to a "thinking stone", as critic Helen Vendler has observed, noting the influence of a Wallace Stevens lyric, "Le Monocle de Mon Oncle"). The poet aspires to return to his "scarred case" with minimal suffering inflicted by "the great god Pain", a play of words on the Greek god Pan. "Syrinx" is the final poem in Merrill's 1972 collection, "Braving the Elements".

In "Dark Places of Wisdom", Peter Kingsley discusses in some detail the use of the word in Parmenides' poem and in association with the ancient practice of incubation

The Britiah Victorian artist Arthur Hacker depicted Syrinx in his 1892 nude. This painting in oil on canvas is currently on display in Manchester Art Gallery.

Sculptor Adolph Wolter was commissioned in 1973 to create a replacement for a stolen sculpture of Syrinx in Indianapolis, United States. This work was a replacement for a similar statue by Myra Reynolds Richards that had been stolen. The sculpture sits in University Park located in the city's Indiana World War Memorial Plaza.

Claude Debussy based his 1913 "Syrinx (La Flute De Pan)" on Pan's sadness over losing his love. The piece is still popular today; it was used as incidental music in the play "Psyché" by Gabriel Mourey.

Maurice Ravel incorporated the character of the Syrinx into his ballet "Daphnis et Chloé".

Gustav Holst alludes to the story of Pan and Syrinx in the opening of his "First Choral Symphony," which draws from the text of John Keats' 1818 poem "Endymion."

French Baroque composer Michel Pignolet de Montéclair composed "Pan et Syrinx", a cantata for voice and ensemble (No. 4 of "Second livre de cantates").

Danish composer Carl Nielsen composed "Pan and Syrinx" ("Pan og Syrinx"), Op. 49, FS 87.

Canadian electronic progressive rock band Syrinx took their name from the legend.

Canadian progressive rock band Rush have a movement titled "The Temples of Syrinx" in their song "2112" on their album "2112". The song is about a dystopian futuristic society in which the arts, particularly music, have been suppressed by the Priests of the Temples of Syrinx.

Related to the Rush reference, Maryland based rockers Clutch mention the Temples of Syrinx in their song "10001110101" from their album "Robot Hive/Exodus".


</doc>
<doc id="27900" url="https://en.wikipedia.org/wiki?curid=27900" title="Sambo">
Sambo

Sambo may refer to:






</doc>
