<doc id="25216" url="https://en.wikipedia.org/wiki?curid=25216" title="Quake II">
Quake II

Quake II is a first-person shooter video game released in December 1997. It was developed by id Software and published by Activision. It is not a direct sequel to "Quake"; id decided to revert to an existing trademark when the game's fast-paced, tactile feel felt closer to a "Quake" game than a new franchise.

The soundtrack for "Quake II" was mainly provided by Sonic Mayhem, with some additional tracks by Bill Brown; the main theme was also composed by Bill Brown and Rob Zombie, and one track by Jer Sypult. The soundtrack for the Nintendo 64 version of the game was composed by Aubrey Hodges, credited as Ken "Razor" Richmond.

"Quake II" is a first-person shooter, in which the player shoots enemies from the perspective of the main character. The gameplay is very similar to that featured in "Quake", in terms of movement and controls, although the player's movement speed has been slowed down, and the player now has the ability to crouch. The game retains four of the eight weapons from "Quake" (the Shotgun, Super Shotgun, Grenade Launcher, and Rocket Launcher), although they have been redesigned visually and made to function in slightly different ways. The remainder of "Quake"s eight weapons (the Axe, Nailgun, Super Nailgun, and Thunderbolt) are not present in "Quake II". The six newly introduced weapons are the Blaster, Machine Gun, Chain Gun, Hyperblaster, Railgun, and BFG10K. The Quad Damage power up from "Quake" is present in "Quake II", and new power-ups include the Ammo Pack, Invulnerability, Bandolier, Enviro-Suit, Rebreather, and Silencer.

The single player game features a number of changes from "Quake". First, the player is given mission-based objectives that correspond to the storyline, including stealing a Tank Commander's head to open a door and calling down an air-strike on a bunker. CGI cutscenes are used to illustrate the player's progress through the main objectives, although they are all essentially the same short piece of video, showing a computerized image of the player character as he moves through game's levels. Another addition is the inclusion of a non-hostile character type: the player character's captured comrades. It is not possible to interact with these characters, however, as they have all been driven insane by their Strogg captors.

The game features much larger levels than "Quake", with many more wide open areas. There is also a hub system that allows the player to travel back and forth between levels, which is necessary to complete certain objectives. Some of the textures and symbols that appear in the game are very similar to some of those found in "Quake". Enemies demonstrate visible wounds after they have taken damage.

The multiplayer portion is similar to that of "Quake". It can be played as a free-for-all deathmatch game mode, a cooperative version of the single-player game, or as a 1 vs 1 match that is used in official tournaments, like the Cyberathlete Professional League. It can also be played in Capture the Flag mode (CTF).
The deathmatch game benefited from the release of eight specifically designed levels that id Software added after the game's initial release. They were introduced to the game via one of the early patches, that were released free of charge. Prior to the release of these maps, players were limited to playing multiplayer games on the single-player levels, which, while functional as multiplayer levels, were not designed with deathmatch gameplay specifically in mind.

As in "Quake", it is possible to customize the way in which the player appears to other people in multiplayer games. However, whereas in "Quake", the only option was to change the color of the player's uniform unless third party modifications were used, now the game comes with a selection of three different player models: a male marine, a female marine, and a male cyborg; choice of player model also affects the speech effects the player's character will make, such as exhaling in effort while jumping or groaning when injured. Each model can be customized from in the in-game menu via the selection of pre-drawn skins, which differ in many ways; for example, skin color, camouflage style, and application of facepaint.

"Quake II" takes place in a science fiction environment. In the single-player game, the player assumes the role of a Marine named Bitterman taking part in "Operation Alien Overlord", a desperate attempt to prevent an alien invasion of Earth by launching a counterattack against the home planet of the hostile Strogg civilization. Most of the other soldiers are captured or killed as soon as they approach the planned landing zone. Bitterman survives only because another Marine's personal capsule collided with his upon launch, causing him to crash far short of the landing zone. It falls upon Bitterman to penetrate the Strogg capital city alone and assassinate the Strogg leader, the Makron.

Originally, Quake II was supposed to be an entirely new game and IP; titles like 'Strogg,' 'Lock and Load,' and even just 'Load' were toyed with in the early days of development. But after numerous failed attempts, the team at id decided to stick with 'Quake II' and forego the gothic Lovecraftian theme from the original in favor of a more sci-fi aesthetic.

Artist and co-owner Adrian Carmack had said that Quake II is his favorite game in the series because "it was different and a cohesive project."

Unlike "Quake", where hardware accelerated graphics controllers were supported only with later patches, "Quake II" came with OpenGL support out of the box. Later downloads from id Software added support for AMD's 3DNow! instruction set for improved performance on their K6-2 processors, and Rendition released a native renderer for their V1000 graphics chip. The latest version is 3.21. This update includes numerous bug fixes and new levels designed for multiplayer deathmatch. Version 3.21, available as source code on id Software's FTP server, has no improved functionality over version 3.20 and is simply a slight modification to make compiling for Linux easier.

"Quake II" uses an improved client–server network model introduced in "Quake". The game code of "Quake II", which defines all the functionality for weapons, entities, and game mechanics, can be changed in any way because id Software published the source code of their own implementation that shipped with the game. "Quake II" uses the shared library functionality of the operating system to load the game library at run-time—this is how mod authors are able to alter the game and provide different gameplay mechanics, new weapons, and much more. The full source code to "Quake II" version 3.19 was released under the terms of the GNU GPL on December 22, 2001. Version 3.21 followed later. A LCC-friendly version was released on January 1, 2002 by a modder going by the name of Major Bitch.

Since the release of the "Quake II" source code, several third-party update projects to the game engine have been created; the most prominent of these are projects focused on graphical enhancements to the game such as "Quake2maX", "EGL", "Quake II Evolved", and "KMQuake II". The source release also revealed numerous security flaws which can result in remote compromise of both the "Quake II" client and server. As id Software no longer maintains "Quake II", most third-party engines include fixes for these bugs. The unofficial patch 3.24 that fixes bugs and adds only meager tweaks is recommended for "Quake II" purists, as it is not intended to add new features or be an engine mod in its own right. The most popular server-side engine modification for multiplayer, "R1Q2", is generally recommended as a replacement for the 3.20 release for both clients and servers. In July 2003, Vertigo Software released a port of "Quake II" for the Microsoft .NET platform, using Managed C++, called "Quake II .NET". It became a poster application for the language, showcasing the powerful interoperability between .NET and standard C++ code. It remains one of the top downloads on the Visual C++ website. In May 2004, Bytonic Software released a port of "Quake II" (called "Jake2") written in Java using JOGL. In 2010 Google ported "Jake2" to HTML5, running in Safari and Chrome.

"Quake II"s game engine was a popular license, and formed the basis for several commercial and free games, such as "", "War§ow", "SiN", "Anachronox", "Heretic II", "Daikatana", "Soldier of Fortune", "", and "". Valve Corporation's 1998 video game "Half-Life", which went on to sell over eight million copies, was originally going to use the "Quake II" engine during early development stages. However, the final version runs on a heavily modified version of the "Quake" engine, "GoldSrc", with a small amount of the "Quake II" code.

Ports of "Quake II" were released in 1999 on the Nintendo 64 (ported by Raster Productions) and PlayStation (ported by Hammerhead) video game consoles. In both cases, the core gameplay was largely identical; however, changes were made to the game sequence and split-screen multiplayer replaced network or Internet play. A Macintosh port was developed by Logicware and released in 1999. "Quake II: Colossus" ("Quake II" with both official add-ons) was ported to Linux by id Software and published by Macmillan Digital Publishing in 1999. Be Inc. officially ported "Quake II: Colossus" to the BeOS to test their OpenGL acceleration in 1999, and provided the game files for free download at a later date—a Windows, Macintosh, or Linux install CD was required to install the game, with the official add-ons being optional.

"Jake2" is a "Quake II" port shown by the JOGL team for JavaOne 2004, to present an example of Java-OpenGL interoperability. "Jake2" has since been used by Sun as an example of Java Web Start capabilities for games distribution over the Internet. In 2009, Tectoy Digital ported "Quake II" to the Brazilian gaming console Zeebo. The game is available for free, but does not feature CG movies or multiplayer support of any kind.
The PlayStation version contains abridged versions of Units 1, 3, 6, 7, 8, and 10 of the PC version, redesigned to meet the console's technical limitations. For example, many short airlock-like corridors were added to maps to provide loading pauses inside what were contiguous areas in the PC version. In addition, part of the first mission of the N64 port is used as a prologue. Some enemy types were removed and two new enemies was added: the Arachnid, a human-spider cyborg with twin railgun arms, and the Guardian, a bipedal boss enemy. Saving the game is only possible between levels and at mid-level checkpoints where the game loads, while in the PC version the game could be saved and loaded at any time. The game supports the PlayStation Mouse peripheral to provide a greater parity with the PC version's gameplay. The music used in this port is a combination of the "Quake II" original music score and tracks from the PC version's mission packs, while the opening and closing cut-scenes are taken from the Ground Zero expansion pack.

The PlayStation version uses a new engine developed by Hammerhead for their future PlayStation projects and runs at a 512x240 resolution at 30 frames per second. The developer was keen to retain a visual parity with the PC version and avoid tricks such as the use of environmental fog. Colored lights for levels and enemies, and yellow highlights for gunfire and explosions, are carried across from the PC version, with the addition of lens flare effects located around the light sources on the original lightmaps. There is no skybox; instead, a flat Gouraud-textured purple sky is drawn around the top of the level. The game uses particles to render blood, debris, and rail gun beams analogously to the PC version.

There is also a split-screen multiplayer mode for two to four players (a four player game is possible using the PlayStation's Multi-tap). The only available player avatar is a modified version of the male player avatar from the PC version, the most noticeable difference being the addition of a helmet. Players can only customize the color of their avatar's armor and change their name. The twelve multiplayer levels featured are unique to the PlayStation version, with none of the PC multiplayer maps being carried over.

The Nintendo 64 version has completely different single player levels and multiplayer maps, and features multiplayer support for up to four players. This version also has new lighting effects, mostly seen in gunfire, and also uses the Expansion Pak for extra graphical detail. This port also features an entirely new soundtrack, consisting mostly of dark ambient pieces, composed by Aubrey Hodges.

A port of "Quake II" was included with "Quake 4" for the Xbox 360 on a bonus disc. This is a direct port of the original game, with some graphical improvements. However, it allows for System Link play for up to sixteen players, split-screen for four players, and cooperative play in single-player for up to sixteen players or four players with split-screen alone.

As with the original "Quake", "Quake II" was designed to allow players to easily create custom content. A large number of mods, maps, graphics such as player models and skins, and sound effects were created and distributed to others free of charge via the Internet. Popular websites such as PlanetQuake and Telefragged allowed players to gain access to custom content. Another improvement over "Quake" was that it was easier to select custom player models, skins, and sound effects because they could be selected from an in-game menu. Two unofficial expansions were released on CDs in 1998: "Zaero", developed by Team Evolve and published by Macmillan Digital Publishing, and "Juggernaut: The New Story", developed by Canopy Games and published by HeadGames Publishing. Other notable mods include "Action Quake 2", "Rocket Arena", "Weapons Factory", "Loki's Minions Capture the Flag", and "RailwarZ Insta-Gib Capture the Flag".

Despite the title, "Quake II" is a sequel to the original "Quake" in name only. The scenario, enemies, and theme are entirely separate and do not fall into the same continuity as "Quake". id initially wanted to set it separately from "Quake", but due to legal reasons (most of their suggested names were already taken), they decided to use the working title. "Quake II" was also adopted as a name to leverage the popularity of "Quake" according to Paul Jaquays. "Quake II" has been released on Steam, but this version does not include the soundtrack. The game was released on a bonus disc included with "Quake 4" Special Edition for the PC, along with both expansion packs. This version also lacks the soundtrack. "Quake II" is also available on a bonus disc with the Xbox 360 version of "Quake 4". This version is a direct port featuring the original soundtrack and multiplayer maps.

In 2015, "Quake II: Quad Damage", a bundle containing the original game with the mission packs has been released at GOG.com, unlike the previous releases, this one contains a new customizeable launcher and the official soundtrack in OGG format which was made possible to play in-game, making it the only digital release to include music.

"Quake II Mission Pack: The Reckoning" is the first official expansion pack, released on May 31, 1998. It was developed by Xatrix Entertainment. It features eighteen new single player levels, six new deathmatch levels, three new weapons (the Ion Ripper, Phalanx Particle Cannon, and Trap), a new power-up, two new enemies, seven modified versions of existing enemies, and five new music tracks. The storyline follows Joker, a member of an elite squad of marines on a mission to infiltrate a Strogg base on one of Stroggos' moons and destroy the Strogg fleet, which is preparing to attack. Joker crash lands in the swamps outside of the compound where his squad is waiting. He travels through the swamps and bypasses the compounds outer defenses and enters through the main gate, finding his squad just in time to watch them get executed by Strogg forces. Next, Joker escapes on his own to the fuel refinery where he helps the Air Force destroy all fuel production, then infiltrates the Strogg spaceport, boards a cargo ship and reaches the Moon Base, destroying it and the Strogg fleet. Notably, the section of the game that takes place on the Moon Base has low gravity, something that was previously used on one secret level of the original "Quake".

"Quake II Mission Pack: Ground Zero" is the second official expansion pack, released on August 31, 1998. It was developed by Rogue Entertainment. It comes with fourteen new single-player levels, ten new multiplayer maps, five additional music tracks, five new enemies, seven new power-ups, and five new weapons. In the expansion's story the Gravity Well has trapped the Earth Fleet in orbit above the planet Stroggos. One of the marines who managed to land, Stepchild, must now make his way to the Gravity Well to destroy it and free the fleet above and disable the entire defenses of the planet.

Patrick Baggatta of IGN gave the expansion 7.5/10, describing it as similar to the original, but noting occasionally confusing map design. Elliott Chin of GameSpot gave the game 7.9/10, citing it as decent for an expansion and praising the monsters and enhanced AI. Johnny B. of Game Revolution rated the expansion D+, citing bad level design and few additions to the original game, and noted the multiplayer power-up gameplay as the only fun feature.

"Quake II Netpack I: Extremities" contains, among other features, 11 game mods and 12 deathmatch maps.

"Quake II" is included in a number of official compilations:


"Quake II" entered PC Data's monthly computer game sales rankings at #2 for December 1997, behind "Riven". The game's sales in the United States alone reached 240,913 copies by the end of 1997, after its release on December 9. It was the country's 22nd-best-selling computer game of 1997. The following year, "Quake II" secured fifth place on PC Data's charts for January and February 1998, before dropping to #8 in March and #9 in April. It remained in PC Data's top 20 for another two months, before exiting in July 1998. The game surpassed 850,000 units shipped by May 1998, and 900,000 by June.

According to PC Data, "Quake II" was the United States' 14th-best-selling computer game during the January-November 1998 period. It ultimately secured 15th place for the full year, with sales of 279,536 copies and revenues of $12.6 million. GameDaily reported in January 1999 that "Quake II"s sales in the United States had reached 550,000 units; this number rose to 610,000 units by December of that year. Worldwide, "Quake II" sold over 1 million copies by 2002.

"Quake II" received positive reviews. Aggregating review website GameRankings gave the PC version 87.31%, the Nintendo 64 version 81.27%, and the PlayStation version 79.81%. AllGame editor Michael L. House praised Quake II by stating "the beauty of Quake II is not in the single-player game, it's in the multi-player feature". Gamespot editor Vince Broady described Quake II as "the only first-person shooter to render the original Quake entirely obsolete".

"Quake II" won "Macworld"s 1999 "Best Shoot-'Em-Up" award, and the magazine's Christopher Breen wrote, "In either single-player or multiplayer mode, for careening-through-corridor-carnage satisfaction, Quake II is a must-have." It also won "Computer Gaming World"s 1997 "Action Game of the Year" award. The editors wrote that "for pure adrenaline-pumping, visceral, instantly gratifying action, "Quake II" is the hands-down winner. No game gave us the rush that "Quake II" did."

The game is followed by Quake IV.



</doc>
<doc id="25217" url="https://en.wikipedia.org/wiki?curid=25217" title="Qi">
Qi

In traditional Chinese culture, qi or ch'i () is believed to be a vital force forming part of any living entity. "Qi" translates as "air" and figuratively as "material energy", "life force", or "energy flow". "Qi" is the central underlying principle in Chinese traditional medicine and in Chinese martial arts. The practice of cultivating and balancing "qi" is called "qigong".

Believers of "qi" describe it as a vital energy whose flow must be balanced for health. Qi is a pseudoscientific, unverified concept, which has never been directly observed, and is unrelated to the concept of energy used in science (vital energy is itself an abandoned scientific notion).

The cultural keyword "qì" is analyzable in terms of Chinese and Sino-Xenic pronunciations. Possible etymologies include the logographs 氣, 气, and 気 with various meanings ranging from "vapor" to "anger", and the English loanword "qi" or "ch'i".

The logograph 氣 is read with two Chinese pronunciations, the usual "qì" 氣 "air; vital energy" and the rare archaic "xì" 氣 "to present food" (later disambiguated with 餼).

Pronunciations of 氣 in modern varieties of Chinese with standardized IPA equivalents include: Standard Chinese "qì" /t͡ɕʰi⁵¹/, Wu Chinese "qi" /t͡ɕʰi³⁴/, Southern Min "khì" /kʰi²¹/, Eastern Min "ké" /kʰɛi²¹³/, Standard Cantonese "hei" /hei̯³³/, and Hakka Chinese "hi" /hi⁵⁵/.

Pronunciations of 氣 in Sino-Xenic borrowings include: Japanese "ki", Korean "gi", and Vietnamese "khi".

Reconstructions of the Middle Chinese pronunciation of 氣 standardized to IPA transcription include: /kʰe̯i/ (Bernard Karlgren), /kʰĭəi/ (Wang Li), /kʰiəi/ (Li Rong), /kʰɨj/ (Edwin Pulleyblank), and /kʰɨi/ (Zhengzhang Shangfang).

Reconstructions of the Old Chinese pronunciation of 氣 standardized to IPA transcription include: /*kʰɯds/ (Zhengzhang Shangfang) and /*C.qʰəp-s/ (William H. Baxter and Laurent Sagart).

The etymology of "qì" interconnects with Kharia "kʰis" "anger", Sora "kissa" "move with great effort", Khmer "kʰɛs" "strive after; endeavor", and Gyalrongic "kʰɐs" "anger".

In the East Asian languages, "qì" has three logographs:
In addition, "qì" 炁 is an uncommon character especially used in writing Daoist talismans. Historically, the word "qì" was generally written as 气 until the Han dynasty (206 BCE–220 CE), when it was replaced by the 氣 graph clarified with "mǐ" 米 "rice" indicating "steam (rising from rice as it cooks.)"

This primary logograph 气, the earliest written character for "qì," consisted of three wavy horizontal lines seen in Shang dynasty (c. 1600–1046 BCE) oracle bone script, Zhou dynasty (1046–256 BCE) bronzeware script and large seal script, and Qin dynasty (221–206 BCE) small seal script. These oracle, bronze, and seal scripts logographs 气 were used in ancient times as a phonetic loan character to write "qǐ" 乞 "plead for; beg; ask" which did not have an early character.

The vast majority of Chinese characters are classified as radical-phonetic characters. Such characters combine a semantically suggestive "radical characters" with a phonetic element approximating ancient pronunciation. For example, the widely known word "dào" 道 "the Dao; the way" graphically combines the "walk" radical 辶 with a "shǒu" 首 "head" phonetic. Although the modern "dào" and "shǒu" pronunciations are dissimilar, the Old Chinese "*lˤuʔ-s" 道 and "*l̥uʔ-s" 首 were alike. The regular script character "qì" 氣 is unusual because "qì" 气 is both the "air radical" and the phonetic, with "mǐ" 米 "rice" semantically indicating "steam; vapor".

This "qì" 气 "air/gas radical" was only used in a few native Chinese characters like "yīnyūn" 氤氲 "thick mist/smoke", but was also used to create new scientific characters for gaseous chemical elements. Some examples are based on pronunciations in European languages: "fú" 氟 (with a "fú" 弗 phonetic) "fluorine" and "nǎi" 氖 (with a "nǎi" 乃 phonetic) "neon". Others are based on semantics: "qīng" 氫 (with a "jīng" 巠 phonetic, abbreviating "qīng" 輕 "light-weight") "hydrogen (the lightest element)" and "lǜ" 氯 (with a "lù" 彔 phonetic, abbreviating "lǜ" 綠 "green") "(greenish-yellow) chlorine".

"Qì" 氣 is the phonetic element in a few characters such as "kài" 愾 "hate" with the "heart-mind radical" 忄or 心, "xì" 熂 "set fire to weeds" with the "fire radical" 火, and "xì" 餼 "to present food" with the "food radical" 食.

The first Chinese dictionary of characters, the "Shuowen Jiezi"(121 CE) notes that the primary "qì" 气 is a pictographic character depicting 雲气 "cloudy vapors", and that the full 氣 combines 米 "rice" with the phonetic "qi" 气, meaning 饋客芻米 "present provisions to guests" (later disambiguated as "xì" 餼).

Qi is a polysemous word. The unabridged Chinese-Chinese character dictionary "Hanyu Da Zidian" defines it as "present food or provisions" for the "xì" pronunciation but also lists 23 meanings for the "qì" pronunciation. The modern "ABC Chinese-English Comprehensive Dictionary," which enters "xì" 餼 "grain; animal feed; make a present of food", and a "qì" 氣 entry with seven translation equivalents for the noun, two for bound morphemes, and three equivalents for the verb. n. ① air; gas ② smell ③ spirit; vigor; morale ④ vital/material energy (in Ch[inese] metaphysics) ⑤ tone; atmosphere; attitude ⑥ anger ⑦ breath; respiration b.f. ① weather 天氣 "tiānqì" ② [linguistics] aspiration 送氣 "sòngqì" v. ① anger ② get angry ③ bully; insult.

Qi was an early Chinese loanword in English. It was romanized as "k'i" in Church Romanization in the early-19th century, as "ch'i" in Wade–Giles in the mid-19th century (sometimes misspelled "chi" omitting the apostrophe), and as qi in Pinyin in the mid-20th century. The "Oxford English Dictionary" entry for "qi" gives the pronunciation as IPA (tʃi), the etymology from Chinese "qì" "air; breath", and a definition of "The physical life-force postulated by certain Chinese philosophers; the material principle." It also gives eight usage examples, with the first recorded example of "k'í" in 1850 ("The Chinese Repository"), of "ch'i" in 1917 ("The Encyclopaedia Sinica"), and "qi" in 1971 (Felix Mann's "Acupuncture")

References to concepts analogous to qi are found in many Asian belief systems. Philosophical conceptions of qi from the earliest records of Chinese philosophy (5th century BCE) correspond to Western notions of humours, the ancient Hindu yogic concept of "prana", and the traditional Jewish concept of "nefesh". An early form of qi comes from the writings of the Chinese philosopher Mencius (4th century BCE).

The ancient Chinese described qi as "life force". They believed it permeated everything and linked their surroundings together. Qi was also linked to the flow of energy around and through the body, forming a cohesive functioning unit. By understanding the rhythm and flow of qi, they believed they could guide exercises and treatments to provide stability and longevity.

Although the concept has been important within many Chinese philosophies, over the centuries the descriptions of qi have varied and have sometimes been in conflict. Until China came into contact with Western scientific and philosophical ideas, the Chinese had not categorized all things in terms of matter and energy. Qi and "li" (理: "pattern") were 'fundamental' categories similar to matter and energy.

Fairly early on, some Chinese thinkers began to believe that there were different fractions of qi—the coarsest and heaviest fractions formed solids, lighter fractions formed liquids, and the most ethereal fractions were the "lifebreath" that animated living beings. "Yuán qì" is a notion of innate or prenatal qi which is distinguished from acquired qi that a person may develop over their lifetime.

The earliest texts that speak of qi give some indications of how the concept developed. In the Analects of Confucius qi could mean "breath". Combining it with the Chinese word for blood (making 血氣, "xue"–"qi", blood and breath), the concept could be used to account for motivational characteristics:

The philosopher Mozi used the word qi to refer to noxious vapors that would in eventually arise from a corpse were it not buried at a sufficient depth. He reported that early civilized humans learned how to live in houses to protect their qi from the moisture that troubled them when they lived in caves. He also associated maintaining one's qi with providing oneself with adequate nutrition. In regard to another kind of qi, he recorded how some people performed a kind of prognostication by observing qi (clouds) in the sky.

Mencius described a kind of qi that might be characterized as an individual's vital energies. This qi was necessary to activity and it could be controlled by a well-integrated willpower. When properly nurtured, this qi was said to be capable of extending beyond the human body to reach throughout the universe. It could also be augmented by means of careful exercise of one's moral capacities. On the other hand, the qi of an individual could be degraded by adverse external forces that succeed in operating on that individual.

Living things were not the only things believed to have qi. Zhuangzi indicated that wind is the "qi" of the Earth. Moreover, cosmic yin and yang "are the greatest of qi. He described qi as "issuing forth" and creating profound effects. He also said "Human beings are born [because of] the accumulation of "qi". When it accumulates there is life. When it dissipates there is death... There is one "qi" that connects and pervades everything in the world."

Another passage traces life to intercourse between Heaven and Earth: "The highest Yin is the most restrained. The highest Yang is the most exuberant. The restrained comes forth from Heaven. The exuberant issues forth from Earth. The two intertwine and penetrate forming a harmony, and [as a result] things are born."

The Guanzi essay "Neiye" (Inward Training) is the oldest received writing on the subject of the cultivation of vapor "[qi]" and meditation techniques. The essay was probably composed at the Jixia Academy in Qi in the late fourth century B.C.

Xun Zi, another Confucian scholar of the Jixia Academy, followed in later years. At 9:69/127, Xun Zi says, "Fire and water have "qi" but do not have life. Grasses and trees have life but do not have perceptivity. Fowl and beasts have perceptivity but do not have "yi" (sense of right and wrong, duty, justice). Men have "qi", life, perceptivity, and "yi"." Chinese people at such an early time had no concept of radiant energy, but they were aware that one can be heated by a campfire from a distance away from the fire. They accounted for this phenomenon by claiming ""qi"" radiated from fire. At 18:62/122, he also uses ""qi"" to refer to the vital forces of the body that decline with advanced age.

Among the animals, the gibbon and the crane were considered experts at inhaling the "qi". The Confucian scholar Dong Zhongshu (ca. 150 BC) wrote in Luxuriant Dew of the Spring and Autumn Annals: "The gibbon resembles a macaque, but he is larger, and his color is black. His forearms being long, he lives eight hundred years, because he is expert in controlling his breathing." ("猿似猴。大而黑。長前臂。所以壽八百。好引氣也。")

Later, the syncretic text assembled under the direction of Liu An, the Huai Nan Zi, or "Masters of Huainan", has a passage that presages most of what is given greater detail by the Neo-Confucians:

The "Huangdi Neijing" "(""The Yellow Emperor's Classic of Medicine", circa 2nd century BCE) is historically credited with first establishing the pathways, called meridians, through which qi circulates in the human body.

In traditional Chinese medicine, symptoms of various illnesses are believed to be either the product of disrupted, blocked, and unbalanced "qi" movement through meridians or deficiencies and imbalances of qi in the "Zang Fu" organs. Traditional Chinese medicine often seeks to relieve these imbalances by adjusting the circulation of "qi" using a variety of techniques including herbology, food therapy, physical training regimens (qigong, t'ai chi ch'uan, and other martial arts training), moxibustion, "tui na", or acupuncture.

The nomenclature of Qi in the human body is different depending on its sources, roles, and locations. For sources there is a difference between so-called "Primordial Qi" (acquired at birth from one's parents) and Qi acquired throughout one's life. Or again Chinese medicine differentiates between Qi acquired from the air we breathe (so called "Clean Air") and Qi acquired from food and drinks (so-called "Grain Qi"). Looking at roles Qi is divided into "Defensive Qi" and "Nutritive Qi". Defensive Qi's role is to defend the body against invasions while Nutritive Qi's role is to provide sustenance for the body. Lastly, looking at locations, Qi is also named after the Zang-Fu organ or the Meridian in which it resides: "Liver Qi", "Spleen Qi", etc. 
A qi field ("chu-chong") refers to the cultivation of an energy field by a group, typically for healing or other benevolent purposes. A qi field is believed to be produced by visualization and affirmation. They are an important component of Wisdom Healing'Qigong ("Zhineng Qigong"), founded by Grandmaster Ming Pang.

Concepts similar to qi can be found in many cultures.

"Prana" in Hinduism and Indian culture, "chi" in the Igbo religion, "pneuma" in ancient Greece, "mana" in Hawaiian culture, "lüng" in Tibetan Buddhism, "manitou" in the culture of the indigenous peoples of the Americas, "ruah" in Jewish culture. In Western philosophy, notions of "energeia", "élan vital", or vitalism are purported to be similar.

Some elements of the "qi" concept can be found in the term 'energy' when used in the context of various esoteric forms of spirituality and alternative medicine.

Elements of the concept of Qi can also be found in Eastern and Western popular culture:


Qi is a non-scientific, unverifiable concept.

A 1997 consensus statement on acupuncture by the United States National Institutes of Health noted that concepts such as qi "are difficult to reconcile with contemporary biomedical information".

The 2014 Skeptoid podcast episode titled "Your Body's Alleged Energy Fields" related a Reiki practitioner's report of what was happening as she passed her hands over a subject's body:

Evaluating these claims, author and scientific skeptic Brian Dunning reported:

The traditional Chinese art of geomancy, the placement and arrangement of space called feng shui, is based on calculating the balance of qi, interactions between the five elements, yin and yang, and other factors. The retention or dissipation of qi is believed to affect the health, wealth, energy level, luck, and many other aspects of the occupants. Attributes of each item in a space affect the flow of qi by slowing it down, redirecting it or accelerating it. This is said to influence the energy level of the occupants.

One use for a "luopan" is to detect the flow of qi. The quality of qi may rise and fall over time. Feng shui with a compass might be considered a form of divination that assesses the quality of the local environment.

Qìgōng (气功 or 氣功) involves coordinated breathing, movement, and awareness. It is traditionally viewed as a practice to cultivate and balance qi. With roots in traditional Chinese medicine, philosophy and martial arts, "qigong" is now practiced worldwide for exercise, healing, meditation, and training for martial arts. Typically a "qigong" practice involves rhythmic breathing, slow and stylized movement, a mindful state, and visualization of guiding qi.

Qi is a didactic concept in many Chinese, Korean and Japanese martial arts. Martial qigong is a feature of both internal and external training systems in China and other East Asian cultures. The most notable of the qi-focused "internal" force (jin) martial arts are Baguazhang, Xing Yi Quan, T'ai Chi Ch'uan, Southern Praying Mantis, Snake Kung Fu, Southern Dragon Kung Fu, Aikido, Kendo, Hapkido, Aikijujutsu, Luohan Quan, and Liu He Ba Fa.

Demonstrations of "qi" or "ki" are popular in some martial arts and may include the unraisable body, the unbendable arm, and other feats of power. Some of these feats can alternatively be explained using biomechanics and physics.

Acupuncture is a part of traditional Chinese medicine that involves insertion of needles into superficial structures of the body (skin, subcutaneous tissue, muscles) at acupuncture points to balance the flow of qi. This is often accompanied by moxibustion, a treatment that involves burning mugwort on or near the skin at an acupuncture point.




</doc>
<doc id="25219" url="https://en.wikipedia.org/wiki?curid=25219" title="Quanta">
Quanta

Quanta is the plural of quantum.

Quanta may also refer to:



</doc>
<doc id="25220" url="https://en.wikipedia.org/wiki?curid=25220" title="Quantum computing">
Quantum computing

Quantum computing is computing using quantum-mechanical phenomena, such as superposition and entanglement. A quantum computer is a device that performs quantum computing. They are different from binary digital electronic computers based on transistors. Whereas common digital computing requires that the data be encoded into binary digits (bits), each of which is always in one of two definite states (0 or 1), quantum computation uses quantum bits or qubits, which can be in superpositions of states. A quantum Turing machine is a theoretical model of such a computer, and is also known as the universal quantum computer. The field of quantum computing was initiated by the work of Paul Benioff and Yuri Manin in 1980, Richard Feynman in 1982, and David Deutsch in 1985. 

Large-scale quantum computers would theoretically be able to solve certain problems much more quickly than any classical computers that use even the best currently known algorithms, like integer factorization using Shor's algorithm (which is a quantum algorithm) and the simulation of quantum many-body systems. There exist quantum algorithms, such as Simon's algorithm, that run faster than any possible probabilistic classical algorithm.
A classical computer could in principle (with exponential resources) simulate a quantum algorithm, as quantum computation does not violate the Church–Turing thesis. On the other hand, quantum computers may be able to efficiently solve problems which are not "practically" feasible on classical computers.

A classical computer has a memory made up of bits, where each bit is represented by either a one or a zero. A quantum computer, on the other hand, maintains a sequence of qubits, which can represent a one, a zero, or any quantum superposition of those two qubit states; a pair of qubits can be in any quantum superposition of 4 states, and three qubits in any superposition of 8 states. In general, a quantum computer with formula_1 qubits can be in an arbitrary superposition of up to formula_2 different states simultaneously. (This compares to a normal computer that can only be in "one" of these formula_2 states at any one time). 

A quantum computer operates on its qubits using quantum gates and measurement (which also alters the observed state). An algorithm is composed of a fixed sequence of quantum logic gates and a problem is encoded by setting the initial values of the qubits, similar to how a classical computer works. The calculation usually ends with a measurement, collapsing the system of qubits into one of the formula_2 eigenstates, where each qubit is zero or one, decomposing into a classical state. The outcome can therefore be at most formula_1 classical bits of information (or, if the algorithm did not end with a measurement, the result is an unobserved quantum state). 

Quantum algorithms are often probabilistic, in that they provide the correct solution only with a certain known probability. Note that the term non-deterministic computing must not be used in that case to mean probabilistic (computing), because the term non-deterministic has a different meaning in computer science.

An example of an implementation of qubits of a quantum computer could start with the use of particles with two spin states: "down" and "up" (typically written formula_6 and formula_7, or formula_8 and formula_9). This is true because any such system can be mapped onto an effective spin-1/2 system.

A quantum computer with a given number of qubits is fundamentally different from a classical computer composed of the same number of classical bits. For example, representing the state of an "n"-qubit system on a classical computer requires the storage of 2 complex coefficients, while to characterize the state of a classical "n"-bit system it is sufficient to provide the values of the "n" bits, that is, only "n" numbers. Although this fact may seem to indicate that qubits can hold exponentially more information than their classical counterparts, care must be taken not to overlook the fact that the qubits are only in a probabilistic superposition of all of their states. This means that when the final state of the qubits is measured, they will only be found in one of the possible configurations they were in before the measurement. It is generally incorrect to think of a system of qubits as being in one particular state before the measurement, since the fact that they were in a superposition of states before the measurement was made directly affects the possible outcomes of the computation.

To better understand this point, consider a classical computer that operates on a three-bit register. If the exact state of the register at a given time is not known, it can be described as a probability distribution over the formula_10 different three-bit strings 000, 001, 010, 011, 100, 101, 110, and 111. If there is no uncertainty over its state, then it is in exactly one of these states with probability 1. However, if it is a probabilistic computer, then there is a possibility of it being in any "one" of a number of different states.

The state of a three-qubit quantum computer is similarly described by an eight-dimensional vector formula_11 (or a one dimensional vector with
each vector node holding the amplitude and the state as the bit string of qubits). Here, however, the coefficients formula_12 are complex numbers, and it is the sum of the "squares" of the coefficients' absolute values, formula_13, that must equal 1. For each formula_14, the absolute value squared formula_15 gives the probability of the system being found in the formula_14-th state after a measurement. However, because a complex number encodes not just a magnitude but also a direction in the complex plane, the phase difference between any two coefficients (states) represents a meaningful parameter. This is a fundamental difference between quantum computing and probabilistic classical computing.

If you measure the three qubits, you will observe a three-bit string. The probability of measuring a given string is the squared magnitude of that string's coefficient (i.e., the probability of measuring 000 = formula_17, the probability of measuring 001 = formula_18, etc.). Thus, measuring a quantum state described by complex coefficients formula_11 gives the classical probability distribution formula_20 and we say that the quantum state "collapses" to a classical state as a result of making the measurement.

An eight-dimensional vector can be specified in many different ways depending on what basis is chosen for the space. The basis of bit strings (e.g., 000, 001, …, 111) is known as the computational basis. Other possible bases are unit-length, orthogonal vectors and the eigenvectors of the Pauli-x operator. Ket notation is often used to make the choice of basis explicit. For example, the state formula_11 in the computational basis can be written as:

The computational basis for a single qubit (two dimensions) is formula_24 and formula_25.

Using the eigenvectors of the Pauli-x operator, a single qubit is formula_26 and formula_27.

While a classical 3-bit state and a quantum 3-qubit state are each eight-dimensional vectors, they are manipulated quite differently for classical or quantum computation. For computing in either case, the system must be initialized, for example into the all-zeros string, formula_28, corresponding to the vector (1,0,0,0,0,0,0,0). In classical randomized computation, the system evolves according to the application of stochastic matrices, which preserve that the probabilities add up to one (i.e., preserve the L1 norm). In quantum computation, on the other hand, allowed operations are unitary matrices, which are effectively rotations (they preserve that the sum of the squares add up to one, the Euclidean or L2 norm). (Exactly what unitaries can be applied depend on the physics of the quantum device.) Consequently, since rotations can be undone by rotating backward, quantum computations are reversible. (Technically, quantum operations can be probabilistic combinations of unitaries, so quantum computation really does generalize classical computation. See quantum circuit for a more precise formulation.)

Finally, upon termination of the algorithm, the result needs to be read off. In the case of a classical computer, we "sample" from the probability distribution on the three-bit register to obtain one definite three-bit string, say 000. Quantum mechanically, one "measures" the three-qubit state, which is equivalent to collapsing the quantum state down to a classical distribution (with the coefficients in the classical state being the squared magnitudes of the coefficients for the quantum state, as described above), followed by sampling from that distribution. This destroys the original quantum state. Many algorithms will only give the correct answer with a certain probability. However, by repeatedly initializing, running and measuring the quantum computer's results, the probability of getting the correct answer can be increased. In contrast, counterfactual quantum computation allows the correct answer to be inferred when the quantum computer is not actually running in a technical sense, though earlier initialization and frequent measurements are part of the counterfactual computation protocol.

For more details on the sequences of operations used for various quantum algorithms, see universal quantum computer, Shor's algorithm, Grover's algorithm, Deutsch–Jozsa algorithm, amplitude amplification, quantum Fourier transform, quantum gate, quantum adiabatic algorithm and quantum error correction.

Integer factorization, which underpins the security of public key cryptographic systems, is believed to be computationally infeasible with an ordinary computer for large integers if they are the product of few prime numbers (e.g., products of two 300-digit primes). By comparison, a quantum computer could efficiently solve this problem using Shor's algorithm to find its factors. This ability would allow a quantum computer to break many of the cryptographic systems in use today, in the sense that there would be a polynomial time (in the number of digits of the integer) algorithm for solving the problem. In particular, most of the popular public key ciphers are based on the difficulty of factoring integers or the discrete logarithm problem, both of which can be solved by Shor's algorithm. In particular the RSA, Diffie–Hellman, and elliptic curve Diffie–Hellman algorithms could be broken. These are used to protect secure Web pages, encrypted email, and many other types of data. Breaking these would have significant ramifications for electronic privacy and security.

However, other cryptographic algorithms do not appear to be broken by those algorithms. Some public-key algorithms are based on problems other than the integer factorization and discrete logarithm problems to which Shor's algorithm applies, like the McEliece cryptosystem based on a problem in coding theory. Lattice-based cryptosystems are also not known to be broken by quantum computers, and finding a polynomial time algorithm for solving the dihedral hidden subgroup problem, which would break many lattice based cryptosystems, is a well-studied open problem. It has been proven that applying Grover's algorithm to break a symmetric (secret key) algorithm by brute force requires time equal to roughly 2 invocations of the underlying cryptographic algorithm, compared with roughly 2 in the classical case, meaning that symmetric key lengths are effectively halved: AES-256 would have the same security against an attack using Grover's algorithm that AES-128 has against classical brute-force search (see Key size). Quantum cryptography could potentially fulfill some of the functions of public key cryptography. Quantum-based cryptographic systems could therefore be more secure than traditional systems against quantum hacking.

Besides factorization and discrete logarithms, quantum algorithms offering a more than polynomial speedup over the best known classical algorithm have been found for several problems, including the simulation of quantum physical processes from chemistry and solid state physics, the approximation of Jones polynomials, and solving Pell's equation. No mathematical proof has been found that shows that an equally fast classical algorithm cannot be discovered, although this is considered unlikely. For some problems, quantum computers offer a polynomial speedup. The most well-known example of this is "quantum database search", which can be solved by Grover's algorithm using quadratically fewer queries to the database than are required by classical algorithms. In this case the advantage is provable. Several other examples of provable quantum speedups for query problems have subsequently been discovered, such as for finding collisions in two-to-one functions and evaluating NAND trees.

Consider a problem that has these four properties:
An example of this is a password cracker that attempts to guess the password for an encrypted file (assuming that the password has a maximum possible length).

For problems with all four properties, the time for a quantum computer to solve this will be proportional to the square root of the number of inputs. It can be used to attack symmetric ciphers such as Triple DES and AES by attempting to guess the secret key.

Since chemistry and nanotechnology rely on understanding quantum systems, and such systems are impossible to simulate in an efficient manner classically, many believe quantum simulation will be one of the most important applications of quantum computing. Quantum simulation could also be used to simulate the behavior of atoms and particles at unusual conditions such as the reactions inside a collider.

Adiabatic quantum computation relies on the adiabatic theorem to undertake calculations. A system is placed in the ground state for a simple Hamiltonian, which is slowly evolved to a more complicated Hamiltonian whose ground state represents the solution to the problem in question. The adiabatic theorem states that if the evolution is slow enough the system will stay in its ground state at all times through the process.

The Quantum algorithm for linear systems of equations or "HHL Algorithm", named after its discoverers Harrow, Hassidim and Lloyd, is expected to provide speedup over classical counterparts.

John Preskill has introduced the term "quantum supremacy" to refer to the hypothetical speedup advantage that a quantum computer would have over a classical computer in a certain field. Google announced in 2017 that it expected to achieve quantum supremacy by the end of the year, and IBM says that the best classical computers will be beaten on some task within about five years. Quantum supremacy has not been achieved yet, and skeptics like Gil Kalai doubt that it will ever be. Bill Unruh doubted the practicality of quantum computers in a paper published back in 1994. Paul Davies pointed out that a 400-qubit computer would even come into conflict with the cosmological information bound implied by the holographic principle. Those such as Roger Schlafly have pointed out that the claimed theoretical benefits of quantum computing go beyond the proven theory of quantum mechanics and imply non-standard interpretations, such as multiple worlds and negative probabilities. Schlafly maintains that the Born rule is just "metaphysical fluff" and that quantum mechanics doesn't rely on probability any more than other branches of science but simply calculates the expected values of observables. He also points out that arguments about Turing complexity cannot be run backwards. Those who prefer Bayesian interpretations of quantum mechanics have questioned the physical nature of the mathematical abstractions employed.

There are a number of technical challenges in building a large-scale quantum computer, and thus far quantum computers have yet to solve a problem faster than a classical computer. David DiVincenzo, of IBM, listed the following requirements for a practical quantum computer:

One of the greatest challenges is controlling or removing quantum decoherence. This usually means isolating the system from its environment as interactions with the external world cause the system to decohere. However, other sources of decoherence also exist. Examples include the quantum gates, and the lattice vibrations and background thermonuclear spin of the physical system used to implement the qubits. Decoherence is irreversible, as it is effectively non-unitary, and is usually something that should be highly controlled, if not avoided. Decoherence times for candidate systems, in particular the transverse relaxation time "T" (for NMR and MRI technology, also called the "dephasing time"), typically range between nanoseconds and seconds at low temperature. Currently, some quantum computers require their qubits to be cooled to 20 millikelvins in order to prevent significant decoherence.

As a result, time consuming tasks may render some quantum algorithms inoperable, as maintaining the state of qubits for a long enough duration will eventually corrupt the superpositions.

These issues are more difficult for optical approaches as the timescales are orders of magnitude shorter and an often-cited approach to overcoming them is optical pulse shaping. Error rates are typically proportional to the ratio of operating time to decoherence time, hence any operation must be completed much more quickly than the decoherence time.

As described in the Quantum threshold theorem, If the error rate is small enough, it is thought to be possible to use quantum error correction to suppress errors and decoherence. This allows the total calculation time to be longer than the decoherence time if the error correction scheme can correct errors faster than decoherence introduces them. An often cited figure for required error rate in each gate for fault tolerant computation is 10, assuming the noise is depolarizing.

Meeting this scalability condition is possible for a wide range of systems. However, the use of error correction brings with it the cost of a greatly increased number of required qubits. The number required to factor integers using Shor's algorithm is still polynomial, and thought to be between "L" and "L", where "L" is the number of qubits in the number to be factored; error correction algorithms would inflate this figure by an additional factor of "L". For a 1000-bit number, this implies a need for about 10 bits without error correction. With error correction, the figure would rise to about 10 bits. Computation time is about "L" or about 10 steps and at 1 MHz, about 10 seconds.

A very different approach to the stability-decoherence problem is to create a topological quantum computer with anyons, quasi-particles used as threads and relying on braid theory to form stable logic gates.

There are a number of quantum computing models, distinguished by the basic elements in which the computation is decomposed. The four main models of practical importance are:
The quantum Turing machine is theoretically important but direct implementation of this model is not pursued. All four models of computation have been shown to be equivalent; each can simulate the other with no more than polynomial overhead.

For physically implementing a quantum computer, many different candidates are being pursued, among them (distinguished by the physical system used to realize the qubits):


The large number of candidates demonstrates that the topic, in spite of rapid progress, is still in its infancy. There is also a vast amount of flexibility.

In 1959 Richard Feynman in his lecture "There's Plenty of Room at the Bottom" states the possibility of using quantum effects for computation.

In 1980 Paul Benioff described quantum mechanical Hamiltonian models of computers and the Russian mathematician Yuri Manin motivated the development of quantum computers.

In 1981, at a conference co-organized by MIT and IBM, physicist Richard Feynman urged the world to build a quantum computer. He said "Nature isn't classical, dammit, and if you want to make a simulation of nature, you'd better make it quantum mechanical, and by golly it's a wonderful problem, because it doesn't look so easy."

In 1984, BB84 is published, the world's first quantum cryptography protocol by IBM scientists Charles Bennett and Gilles Brassard.

In 1993, an international group of six scientists, including Charles Bennett, showed that perfect quantum teleportation is possible in principle, but only if the original is destroyed.

In 1996, The DiVincenzo's criteria are published which is a list of conditions that are necessary for constructing a quantum computer proposed by the theoretical physicist David P. DiVincenzo in his 2000 paper "The Physical Implementation of Quantum Computation".

In 2001, researchers demonstrated Shor's algorithm to factor 15 using a 7-qubit NMR computer.

In 2005, researchers at the University of Michigan built a semiconductor chip ion trap. Such devices from standard lithography, may point the way to scalable quantum computing.

In 2009, researchers at Yale University created the first solid-state quantum processor. The two-qubit superconducting chip had artificial atom qubits made of a billion aluminum atoms that acted like a single atom that could occupy two states.

A team at the University of Bristol, also created a silicon chip based on quantum optics, able to run Shor's algorithm.
Further developments were made in 2010.
Springer publishes a journal ("Quantum Information Processing") devoted to the subject.

In February 2010, Digital Combinational Circuits like adder, subtractor etc. are designed with the help of Symmetric Functions organized from different quantum gates.

In April 2011, a team of scientists from Australia and Japan made a breakthrough in quantum teleportation. They successfully transferred a complex set of quantum data with full transmission integrity, without affecting the qubits' superpositions.

In 2011, D-Wave Systems announced the first commercial quantum annealer, the D-Wave One, claiming a 128 qubit processor. On May 25, 2011, Lockheed Martin agreed to purchase a D-Wave One system. Lockheed and the University of Southern California (USC) will house the D-Wave One at the newly formed USC Lockheed Martin Quantum Computing Center. D-Wave's engineers designed the chips with an empirical approach, focusing on solving particular problems. Investors liked this more than academics, who said D-Wave had not demonstrated they really had a quantum computer. Criticism softened after a D-Wave paper in "Nature", that proved the chips have some quantum properties. Two published papers have suggested that the D-Wave machine's operation can be explained classically, rather than requiring quantum models. Later work showed that classical models are insufficient when all available data is considered. Experts remain divided on the ultimate classification of the D-Wave systems though their quantum behavior was established concretely with a demonstration of entanglement.

During the same year, researchers at the University of Bristol created an all-bulk optics system that ran a version of Shor's algorithm to successfully factor 21.

In September 2011 researchers proved quantum computers can be made with a Von Neumann architecture (separation of RAM).

In November 2011 researchers factorized 143 using 4 qubits.

In February 2012 IBM scientists said that they had made several breakthroughs in quantum computing with superconducting integrated circuits.

In April 2012 a multinational team of researchers from the University of Southern California, Delft University of Technology, the Iowa State University of Science and Technology, and the University of California, Santa Barbara, constructed a two-qubit quantum computer on a doped diamond crystal that can easily be scaled up and is functional at room temperature. Two logical qubit directions of electron spin and nitrogen kernels spin were used, with microwave impulses. This computer ran Grover's algorithm generating the right answer from the first try in 95% of cases.

In September 2012, Australian researchers at the University of New South Wales said the world's first quantum computer was just 5 to 10 years away, after announcing a global breakthrough enabling manufacture of its memory building blocks. A research team led by Australian engineers created the first working qubit based on a single atom in silicon, invoking the same technological platform that forms the building blocks of modern-day computers.

In October 2012, Nobel Prizes were presented to David J. Wineland and Serge Haroche for their basic work on understanding the quantum world, which may help make quantum computing possible.

In November 2012, the first quantum teleportation from one macroscopic object to another was reported by scientists at the University of Science and Technology of China in Hefei.

In December 2012, the first dedicated quantum computing software company, 1QBit was founded in Vancouver, BC. 1QBit is the first company to focus exclusively on commercializing software applications for commercially available quantum computers, including the D-Wave Two. 1QBit's research demonstrated the ability of superconducting quantum annealing processors to solve real-world problems.

In February 2013, a new technique, boson sampling, was reported by two groups using photons in an optical lattice that is not a universal quantum computer but may be good enough for practical problems. "Science" Feb 15, 2013

In May 2013, Google announced that it was launching the Quantum Artificial Intelligence Lab, hosted by NASA Ames Research Center, with a 512-qubit D-Wave quantum computer. The USRA (Universities Space Research Association) will invite researchers to share time on it with the goal of studying quantum computing for machine learning. Google added that they had "already developed some quantum machine learning algorithms" and had "learned some useful principles", such as that "best results" come from "mixing quantum and classical computing".

In early 2014 it was reported, based on documents provided by former NSA contractor Edward Snowden, that the U.S. National Security Agency (NSA) is running a $79.7 million research program (titled "Penetrating Hard Targets") to develop a quantum computer capable of breaking vulnerable encryption.

In 2014, a group of researchers from ETH Zürich, USC, Google and Microsoft reported a definition of quantum speedup, and were not able to measure quantum speedup with the D-Wave Two device, but did not explicitly rule it out.

In 2014, researchers at University of New South Wales used silicon as a protectant shell around qubits, making them more accurate, increasing the length of time they will hold information, and possibly making quantum computers easier to build.

In April 2015 IBM scientists claimed two critical advances towards the realization of a practical quantum computer. They claimed the ability to detect and measure both kinds of quantum errors simultaneously, as well as a new, square quantum bit circuit design that could scale to larger dimensions.

In October 2015 researchers at University of New South Wales built a quantum logic gate in silicon for the first time.

In December 2015 NASA publicly displayed the world's first fully operational $15-million quantum computer made by the Canadian company D-Wave at the Quantum Artificial Intelligence Laboratory at its Ames Research Center in California's Moffett Field. The device was purchased in 2013 via a partnership with Google and Universities Space Research Association. The presence and use of quantum effects in the D-Wave quantum processing unit is more widely accepted. In some tests it can be shown that the D-Wave quantum annealing processor outperforms Selby’s algorithm. Only 2 of this computer has been made so far.

In May 2016, IBM Research announced that for the first time ever it is making quantum computing available to members of the public via the cloud, who can access and run experiments on IBM’s quantum processor. The service is called the IBM Quantum Experience. The quantum processor is composed of five superconducting qubits and is housed at the IBM T. J. Watson Research Center in New York.

In August 2016, scientists at the University of Maryland successfully built the first reprogrammable quantum computer.

In October 2016 Basel University described a variant of the electron hole based quantum computer, which instead of manipulating
electron spins uses electron holes in a semiconductor at low (mK) temperatures which are a lot less vulnerable to decoherence. 
This has been dubbed the "positronic" quantum computer as the quasi-particle behaves like it has a positive electrical charge.

In March 2017, IBM announced an industry-first initiative to build commercially available universal quantum computing systems called IBM Q. The company also released a new API (Application Program Interface) for the IBM Quantum Experience that enables developers and programmers to begin building interfaces between its existing five quantum bit (qubit) cloud-based quantum computer and classical computers, without needing a deep background in quantum physics.

In May 2017, IBM announced that it has successfully built and tested its most powerful universal quantum computing processors. The first is a 16 qubit processor that will allow for more complex experimentation than the previously available 5 qubit processor. The second is IBM's first prototype commercial processor with 17 qubits and leverages significant materials, device, and architecture improvements to make it the most powerful quantum processor created to date by IBM.

In July 2017, a group of U.S. researchers announced a quantum simulator with 51 qubits. The announcement was made by Mikhail Lukin of Harvard University at the International Conference on Quantum Technologies in Moscow. A quantum simulator differs from a computer. Lukin’s simulator was designed to solve one equation. Solving a different equation would require building a new system. A computer can solve many different equations.

In September 2017, IBM Research scientists use a 7 qubit device to model the largest molecule, Beryllium hydride, ever by a quantum computer. The results were published as the cover story in the peer-reviewed journal Nature.

In October 2017, IBM Research scientists successfully "broke the 49-qubit simulation barrier" and simulated 49- and 56-qubit short-depth circuits, using the Lawrence Livermore National Laboratory's Vulcan supercomputer, and the University of Illinois' Cyclops Tensor Framework (originally developed at the University of California). The results were published in arxiv.

In November 2017, the University of Sydney research team in Australia successfully made a microwave circulator, an important quantum computer part, 1000 times smaller than a conventional circulator by using topological insulators to slow down the speed of light in a material.

In November 2017, IBM announced the availability of its most-powerful 20 qubit commercial processor, and the first prototype 50 qubit processor. The 20 qubit processor has an industry-leading 90 μs coherence time for the systems' operations.

In December 2017, IBM announced its first IBM Q Network clients. The companies, universities, and labs to explore practical quantum applications, using IBM Q 20 qubit commercial systems, for business and science include: JPMorgan Chase, Daimler AG, Samsung, JSR Corporation, Barclays, Hitachi Metals, Honda, Nagase, Keio University, Oak Ridge National Lab, Oxford University and University of Melbourne.

In December 2017, Microsoft released a preview version of a "Quantum Development Kit". It includes a programming language, Q#, which can be used to write programs that are run on an emulated quantum computer.

In 2017 D-Wave reported to start selling a 2000 qubit quantum computer.

In February 2018, scientists reported, for the first time, the discovery of a new form of light, which may involve polaritons, that could be useful in the development of quantum computers.

In March 2018, Google Quantum AI Lab announced a 72 qubit processor called Bristlecone.

In April 2018, IBM Research announced eight quantum computing startups joined the IBM Q Network, including: Zapata Computing, Strangeworks, QxBranch, Quantum Benchmark, QC Ware, Q-CTRL, Cambridge Quantum Computing, and 1QBit.

The class of problems that can be efficiently solved by quantum computers is called BQP, for "bounded error, quantum, polynomial time". Quantum computers only run probabilistic algorithms, so BQP on quantum computers is the counterpart of BPP ("bounded error, probabilistic, polynomial time") on classical computers. It is defined as the set of problems solvable with a polynomial-time algorithm, whose probability of error is bounded away from one half. A quantum computer is said to "solve" a problem if, for every instance, its answer will be right with high probability. If that solution runs in polynomial time, then that problem is in BQP.

BQP is contained in the complexity class "#P" (or more precisely in the associated class of decision problems "P"), which is a subclass of PSPACE.

BQP is suspected to be disjoint from NP-complete and a strict superset of P, but that is not known. Both integer factorization and discrete log are in BQP. Both of these problems are NP problems suspected to be outside BPP, and hence outside P. Both are suspected to not be NP-complete. There is a common misconception that quantum computers can solve NP-complete problems in polynomial time. That is not known to be true, and is generally suspected to be false.

The capacity of a quantum computer to accelerate classical algorithms has rigid limits—upper bounds of quantum computation's complexity. The overwhelming part of classical calculations cannot be accelerated on a quantum computer. A similar fact takes place for particular computational tasks, like the search problem, for which Grover's algorithm is optimal.

Bohmian Mechanics is a non-local hidden variable interpretation of quantum mechanics. It has been shown that a non-local hidden variable quantum computer could implement a search of an N-item database at most in formula_29 steps. This is slightly faster than the formula_30 steps taken by Grover's algorithm. Neither search method will allow quantum computers to solve NP-Complete problems in polynomial time.

Although quantum computers may be faster than classical computers for some problem types, those described above can't solve any problem that classical computers can't already solve. A Turing machine can simulate these quantum computers, so such a quantum computer could never solve an undecidable problem like the halting problem. The existence of "standard" quantum computers does not disprove the Church–Turing thesis. It has been speculated that theories of quantum gravity, such as M-theory or loop quantum gravity, may allow even faster computers to be built. Currently, "defining" computation in such theories is an open problem due to the "problem of time", i.e., there currently exists no obvious way to describe what it means for an observer to submit input to a computer and later receive output.


 


</doc>
<doc id="25222" url="https://en.wikipedia.org/wiki?curid=25222" title="QT">
QT

QT or Qt may refer to:








</doc>
<doc id="25223" url="https://en.wikipedia.org/wiki?curid=25223" title="Quasigroup">
Quasigroup

In mathematics, especially in abstract algebra, a quasigroup is an algebraic structure resembling a group in the sense that "division" is always possible. Quasigroups differ from groups mainly in that they are not necessarily associative.

A quasigroup with an identity element is called a loop.

There are at least two structurally equivalent formal definitions of quasigroup. One defines a quasigroup as a set with one binary operation, and the other, from universal algebra, defines a quasigroup as having three primitive operations. The homomorphic image of a quasigroup defined with a single binary operation, however, need not be a quasigroup. We begin with the first definition.

A quasigroup is a set, "Q", with a binary operation, ∗, (that is, a magma), obeying the Latin square property. This states that, for each "a" and "b" in "Q", there exist unique elements "x" and "y" in "Q" such that both
hold. (In other words: Each element of the set occurs exactly once in each row and exactly once in each column of the quasigroup's multiplication table, or Cayley table. This property ensures that the Cayley table of a finite quasigroup is a Latin square.) The uniqueness requirement can be replaced by the requirement that the magma be cancellative.

The unique solutions to these equations are written and . The operations '\' and '/' are called, respectively, left and right division.

The empty set equipped with the empty binary operation satisfies this definition of a quasigroup. Some authors accept the empty quasigroup but others explicitly exclude it.

Given some algebraic structure, an identity is an equation in which all variables are tacitly universally quantified, and in which all operations are among the primitive operations proper to the structure. Algebraic structures axiomatized solely by identities are called varieties. Many standard results in universal algebra hold only for varieties. Quasigroups are varieties if left and right division are taken as primitive.

A quasigroup is a type (2,2,2) algebra (i.e., equipped with three binary operations) satisfying the identities:

In other words: Multiplication and division in either order, one after the other, on the same side by the same element, have no net effect.

Hence if is a quasigroup according to the first definition, then is the same quasigroup in the sense of universal algebra.

A loop is a quasigroup with an identity element; that is, an element, "e", such that

It follows that the identity element, "e", is unique, and that every element of "Q" has a unique left and right inverse. Since the presence of an identity element is essential, a loop cannot be empty.

A quasigroup with an idempotent element is called a pique ("pointed idempotent quasigroup"); this is a weaker notion than loop but common nonetheless because given an abelian group, , its subtraction operation (as quasigroup multiplication) yields a pique with the abelian group's zero/identity turned into a "pointed idempotent"; i.e., there's a principal isotopy .

A loop that is associative is a group. A group can have a non-associative pique isotope, but it cannot
have a nonassociative loop isotope. There are also some weaker associativity-like properties which have been given special names.

A Bol loop is a loop that satisfies either:
for each "x", "y" and "z" in "Q" (a "left Bol loop"),

or else
for each "x", "y" and "z" in "Q" (a "right Bol loop").

A loop that is both a left and right Bol loop is a Moufang loop. This is equivalent to any one of the following single Moufang identities holding for all "x", "y", "z":

Smith (2007) names the following important subclasses:

A quasigroup is semisymmetric if all of the following equivalent identities hold:

Although this class may seem special, every quasigroup "Q" induces a semisymmetric quasigroup "Q"Δ on the direct product cube "Q" via following operation:

where "//" and "\\" are the conjugate division operations; the latter formula more explicitly shows that the construction is exploiting an orbit of S.

A narrower class that is a total symmetric quasigroup (sometimes abbreviated TS-quasigroup) in which all conjugates coincide as one operation: . Another way to define (the same notion of) totally symmetric quasigroup is as a semisymmetric quasigroup which additionally is commutative, i.e. .

Idempotent total symmetric quasigroups are precisely (i.e. in a bijection with) Steiner triples, so such a quasigroup is also called a Steiner quasigroup, and sometimes the latter is even abbreviated as squag; the term sloop is defined similarly for a Steiner quasigroup that is also a loop. Without idempotency, total symmetric quasigroups correspond to the geometric notion of extended Steiner triple, also called Generalized Elliptic Cubic Curve (GECC).

A quasigroup is called totally anti-symmetric if for all , both of the following implications hold:

It is called weakly totally anti-symmetric if only the first implication holds.

This property is required, for example, in the Damm algorithm.


Quasigroups have the cancellation property: if , then . This follows from the uniqueness of left division of "ab" or "ac" by "a". Similarly, if , then .

The definition of a quasigroup can be treated as conditions on the left and right multiplication operators , defined by 

The definition says that both mappings are bijections from "Q" to itself. A magma "Q" is a quasigroup precisely when all these operators, for every "x" in "Q", are bijective. The inverse mappings are left and right division, that is, 

In this notation the identities among the quasigroup's multiplication and division operations (stated in the section on universal algebra) are

where 1 denotes the identity mapping on "Q".

The multiplication table of a finite quasigroup is a Latin square: an table filled with "n" different symbols in such a way that each symbol occurs exactly once in each row and exactly once in each column.

Conversely, every Latin square can be taken as the multiplication table of a quasigroup in many ways: the border row (containing the column headers) and the border column (containing the row headers) can each be any permutation of the elements. See small Latin squares and quasigroups.

Every loop element has a unique left and right inverse given by

A loop is said to have ("two-sided") "inverses" if formula_7 for all "x". In this case the inverse element is usually denoted by formula_8.

There are some stronger notions of inverses in loops which are often useful:

A loop has the "inverse property" if it has both the left and right inverse properties. Inverse property loops also have the antiautomorphic and weak inverse properties. In fact, any loop which satisfies any two of the above four identities has the inverse property and therefore satisfies all four.

Any loop which satisfies the left, right, or antiautomorphic inverse properties automatically has two-sided inverses.

A quasigroup or loop homomorphism is a map between two quasigroups such that . Quasigroup homomorphisms necessarily preserve left and right division, as well as identity elements (if they exist).

Let "Q" and "P" be quasigroups. A quasigroup homotopy from "Q" to "P" is a triple of maps from "Q" to "P" such that
for all "x", "y" in "Q". A quasigroup homomorphism is just a homotopy for which the three maps are equal.

An isotopy is a homotopy for which each of the three maps is a bijection. Two quasigroups are isotopic if there is an isotopy between them. In terms of Latin squares, an isotopy is given by a permutation of rows α, a permutation of columns β, and a permutation on the underlying element set γ.

An autotopy is an isotopy from a quasigroup to itself. The set of all autotopies of a quasigroup form a group with the automorphism group as a subgroup.

Every quasigroup is isotopic to a loop. If a loop is isotopic to a group, then it is isomorphic to that group and thus is itself a group. However, a quasigroup which is isotopic to a group need not be a group. For example, the quasigroup on R with multiplication given by is isotopic to the additive group , but is not itself a group. Every medial quasigroup is isotopic to an abelian group by the Bruck–Toyoda theorem.

Left and right division are examples of forming a quasigroup by permuting the variables in the defining equation. From the original operation ∗ (i.e., ) we can form five new operations: (the opposite operation), / and \, and their opposites. That makes a total of six quasigroup operations, which are called the conjugates or parastrophes of ∗. Any two of these operations are said to be "conjugate" or "parastrophic" to each other (and to themselves).

If the set "Q" has two quasigroup operations, ∗ and ·, and one of them is isotopic to a conjugate of the other, the operations are said to be isostrophic to each other. There are also many other names for this relation of "isostrophe", e.g., paratopy.

An "n"-ary quasigroup is a set with an "n"-ary operation, with , such that the equation has a unique solution for any one variable if all the other "n" variables are specified arbitrarily. Polyadic or multiary means "n"-ary for some nonnegative integer "n".

A 0-ary, or nullary, quasigroup is just a constant element of "Q". A 1-ary, or unary, quasigroup is a bijection of "Q" to itself. A binary, or 2-ary, quasigroup is an ordinary quasigroup.

An example of a multiary quasigroup is an iterated group operation, ; it is not necessary to use parentheses to specify the order of operations because the group is associative. One can also form a multiary quasigroup by carrying out any sequence of the same or different group or quasigroup operations, if the order of operations is specified.

There exist multiary quasigroups that cannot be represented in any of these ways. An "n"-ary quasigroup is irreducible if its operation cannot be factored into the composition of two operations in the following way:
where and . Finite irreducible "n"-ary quasigroups exist for all ; see Akivis and Goldberg (2001) for details.

An "n"-ary quasigroup with an "n"-ary version of associativity is called an n-ary group.

A right-quasigroup is a type (2,2) algebra satisfying both identities:
"y" = ("y" / "x") ∗ "x";
"y" = ("y" ∗ "x") / "x".

Similarly, a left-quasigroup is a type (2,2) algebra satisfying both identities:
"y" = "x" ∗ ("x" \ "y");
"y" = "x" \ ("x" ∗ "y").

The number of isomorphism classes of small quasigroups and loops is given here:





</doc>
<doc id="25225" url="https://en.wikipedia.org/wiki?curid=25225" title="Quaestor">
Quaestor

A (, , ) was a public official in Ancient Rome. The position served different functions depending on the period. In the Roman Kingdom, ' (quaestors with judicial powers) were appointed by the king to investigate and handle murders. In the Roman Republic, quaestors (Lat. ') were elected officials who supervised the state treasury and conducted audits. It was the lowest ranking position in the ' (course of offices). However, this means that in the political environment of Rome, it was quite common for many aspiring politicians to take the position of quaestor as an early rung on the political ladder. In the Roman Empire, the position, which was initially replaced by the ' (prefect), reemerged during the late empire as ", a position appointed by the emperor to lead the imperial council and respond to petitioners.

The earliest quaestors were ' (quaestors with judicial power), an office dating back to the Kingdom of Rome. ' were chosen to investigate capital crimes, and may have been appointed as needed rather than holding a permanent position. Ancient authors disagree on the exact manner of selection for this office as well as on its earliest institution, with some dating it to the mythical reign of Romulus.

In the Roman Republic, quaestors were elected officials who supervised the treasury and financial accounts of the state, its armies and its officers. The quaestors tasked with financial supervision were also called ', because they oversaw the ' (public treasury) in the Temple of Saturn. The earliest origins of the office is obscure, but by about 420 BC there were four quaestors elected each year by the " (Assembly of the People). After 267 BC, the number was expanded to ten.

The office of quaestor, usually a former broad-striped tribune, was adopted as the first official post of the ' ( course of offices), the standard sequence that made up a career in public service. Once elected as quaestor, a Roman man earned the right to sit in the Senate and began progressing through the '. Quaestors were also given a ' (a bound bundle of wooden rods symbolizing a magistrate's authority and jurisdiction) and were entitled to one ' (civil servant bodyguard). Quaestors were almost always a member of the "" group, the upper class. Only members of the "plebeian" group (lower class) could be a tribune or aedile.

Every Roman consul, the highest elected official in the ", and every provincial governor was appointed a quaestor. Some quaestors were assigned to work in the city and others in the provinces where their responsibilities could include being recruited into the military. Some provincial quaestors were assigned as staff to military generals or served as second-in-command to governors in the Roman provinces. Still others were assigned to oversee military finances.

Lucius Cornelius Sulla's reforms in 81 BC raised the number of quaestors to 20 and the minimum age for a quaestorship was 30 for patricians (members of ruling class families) and 32 for plebeians (commoners). Additionally, the reforms granted quaestors automatic membership in the Senate upon being elected, whereas previously, membership in the Senate was granted only after censors revised the Senate rolls, which occurred less frequently than the annual induction of quaestors.

This relationship between a consul and a quaestor was similar to that between a patron and a client. The quaestor was essential a client to their superior. There was some level of mutual respect between the two individuals, but a defined sense of place and knowledge of each other's roles. This relationship often continued past the designated terms of either individual, and the quaestor could be called upon for assistance or other needs by the consul. Breaking this pact or doing harm by a former superior would make the quaestor seem dishonorable or even treasonous.

Constantine the Great created the office ' (quaestor of the sacred place) which functioned as the Roman Empire's senior legal official. Emperor Justinian I also created the offices ', a judicial and police official for Constantinople, and ' (quaestor of the army), a short-lived joint military-administrative post covering the border of the lower Danube. The ' survived long into the Byzantine Empire, although its duties were altered to match the ". The term is last attested in 14th century Byzantium as a purely honorific title.

In the early republic, there were two quaestors, and their duties were maintaining the public treasury, both taking in funds and deciding who to pay them to. This continued until 421 BCE when the number of quaestors was doubled to 4. While two continued with the same duties of those that had come before, the other two had additional responsibilities, each being in service to the one of the consuls.'"

When consuls went to war, each was assigned a quaestor. The quaestor's main responsibilities involved the distribution of war spoils between the aerarium, or public treasury, and the army. The key responsibility of the quaestor was the administration of public funds to higher-ranking officials in order to pursue their goals, whether those involve military conquests which require funding for armies or public works projects.

The office of quaestor was a position bound to their superior, whether that be a consul, governor, or other magistrate, and the duties would often reflect their superiors. For example, Gaius Gracchus was quaestor under the consul Orestes in Sardinia, and many of his responsibilities involved leading military forces. While not in direct command of the army, the quaestor would be in charge of organizational and lesser duties that were a necessary part of the war machine.

During the reign of the Emperor Constantine I, the office of quaestor was reorganized into a judicial position known as the quaestor sacri palatii. The office functioned as the primary legal adviser to the emperor, and was charged with the creation of laws as well as answers petitions to the emperor.

From 440 onward, the office of the quaestor worked in conjunction with the praetorian prefect of the East to oversee the supreme tribunal, or supreme court, at Constantinople. There they heard appeals from the various subordinate courts and governors.

Under the Emperor Justinian I, an additional office named quaestor was created to control police and judicial matters in Constantinople. In this new position, a quaestor was responsible for wills, as well as supervision of complaints by tenants regarding their landlords, and finally over the homeless.

See also .

Following the death of his brother Tiberius Gracchus, Gaius Gracchus stayed out of the political spotlight for a period until he was forced to defend a good friend of his named Vettius in court. Hearing his vocal abilities, the Senate began to fear that Gaius would arouse the people in the same manner as his brother and appointed him quaestor to Gnaeus Aufidius Orestes in Sardinia to prevent him from becoming a tribune. Gaius used his position as quaestor to successfully defeat his enemies as well as gain a large amount of loyalty among his troops. Following an incident where Gaius won the support of a local village to provide for his troops, the Senate attempted to keep Gaius in Sardinia indefinitely by reappointing Orestes to stay in Sardinia. Gaius was not pleased by this and returned to Rome demanding an explanation, actions which eventually led to his election as a tribune of the people.

Marcus Antonius, or Mark Antony, who is most well known for his civil war with Octavian, started off his political career in the position of quaestor after being a prefect in Syria and then one of Julius Caesar's legates in Gaul. Through a combination of Caesar's favor and his oratory skills defending the legacy of Publius Clodius, Antony was able to win the quaestorship in 51 BCE. This then led to Antony's election as augur and tribune of the people in 50 BC due to Caesar's efforts to reward his ally.

While Julius Caesar served as Quaestor to the Governor or Proconsul/Propraetor in Hispania Ulterior he took major military action against the rebellious tribes of the region. His time as Quaestor was uneventful although when he became Governor there, he settled the disputes.

Marcus Tullius Cicero was the Quaestor to the Propraetor/Proconsul of Sicily. He fixed major agricultural problems in the region and improved on the purchase and selling of grain. The farmers after this loved Cicero and began to travel to Rome to vote for him in elections every year.

Quaestor derives from the Latin verb ', ', meaning "to inquire". The job title has traditionally been understood as deriving from the original investigative function of the ". Ancient authors, perhaps influenced by etymology, reasoned that the investigative role of the " had evolved to include financial matters, giving rise to the similarly-named later offices. However, this connection has been questioned by modern scholars.




</doc>
<doc id="25228" url="https://en.wikipedia.org/wiki?curid=25228" title="Q.E.D.">
Q.E.D.

Q.E.D. (also written QED and in italics: "QED") is an initialism of the Latin phrase meaning "what was to be demonstrated" or "what was to be shown." Some may also use a less direct translation instead: "thus it has been demonstrated." Traditionally, the phrase is placed in its abbreviated form at the end of a mathematical proof or philosophical argument when the original proposition has been restated exactly, as the conclusion of the demonstration or completion of the proof.

The phrase, "quod erat demonstrandum", is a translation into Latin from the Greek (; abbreviated as "ΟΕΔ"). Translating from the Latin into English yields, "what was to be demonstrated", however, translating the Greek phrase produces a slightly different meaning. Since the verb also means "to show" or "to prove", a better translation from the Greek would read, "The very thing it was required to have shown."

The Greek phrase was used by many early Greek mathematicians, including Euclid and Archimedes.

During the European Renaissance, scholars often wrote in Latin, and phrases such as "Q.E.D." were often used to conclude proofs.
Perhaps the most famous use of "Q.E.D." in a philosophical argument is found in the "Ethics" of Baruch Spinoza, published posthumously in 1677. Written in Latin, it is considered by many to be Spinoza's "magnum opus". The style and system of the book are, as Spinoza says, "demonstrated in geometrical order", with axioms and definitions followed by propositions. For Spinoza, this is a considerable improvement over René Descartes's writing style in the "Meditations", which follows the form of a diary.

There is another Latin phrase with a slightly different meaning, usually shortened similarly, but being less common in use. , originating from the Greek geometers' closing (), meaning "which had to be done". Because of the difference in meaning, the two phrases should not be confused.

Euclid used the phrase, Quod Erat Faciendum (Q.E.F.), to close propositions that were not proofs of theorems, but constructions. For example, Euclid's first proposition showing how to construct an equilateral triangle, given one side, is concluded this way.

"Q.E.D." has acquired many translations in various languages, including:

There is no common formal English equivalent, although the end of a proof may be announced with a simple statement such as "this completes the proof", "as required", "hence proved", "ergo", or by using a similar locution. WWWWW or W – an abbreviation of "Which Was What Was Wanted" – has been used similarly. Often this is considered to be more tongue-in-cheek than the usual Halmos symbol "(see below)" or "Q.E.D."

Due to the paramount importance of proofs in mathematics, mathematicians since the time of Euclid have developed conventions to demarcate the beginning and end of proofs. In printed English language texts, the formal statements of theorems, lemmas, and propositions are set in italics by tradition. The beginning of a proof usually follows immediately thereafter, and is indicated by the word "proof" in boldface or italics. On the other hand, several symbolic conventions exist to indicate the end of a proof.

While some authors still use the classical abbreviation, Q.E.D., this practice is increasingly viewed as archaic or even pretentious. Paul Halmos pioneered the use of a solid black square at the end of a proof as a Q.E.D symbol, a practice which has become standard, although not universal. Halmos adopted this use of a symbol from magazine typography customs in which simple geometric shapes had been used to indicate the end of an article. This symbol was later called the "tombstone" or "Halmos symbol" or even a "halmos" by mathematicians. Often the Halmos symbol is drawn on chalkboard to signal the end of a proof during a lecture, although this practice is not so common as its use in printed text.

The tombstone symbol appears in TeX as the character formula_1 (filled square, \blacksquare) and sometimes, as a formula_2 (hollow square, \square). In the AMS Theorem Environment for LaTeX, the hollow square is the default end-of-proof symbol. Unicode explicitly provides the "End of proof" character, U+220E (∎). Some authors use other Unicode symbols to note the end of a proof, including, ▮ (U+25AE, a black vertical rectangle), and ‣ (U+2023, a triangular bullet). Other authors have adopted two forward slashes (//) or four forward slashes (////). In other cases, authors have elected to segregate proofs typographically by displaying them as indented blocks.

In Joseph Heller's book "Catch-22", the Chaplain, having been told to examine a forged letter allegedly signed by him (which he knew he didn't sign), verified that his "name" was in fact there. His investigator replied, "Then you wrote it. Q.E.D." The chaplain said he didn't write it and that it wasn't his handwriting, to which the investigator replied, "Then you signed your name in somebody else's handwriting again."

In the 1978 science-fiction radio comedy, and later in the television and novel adaptations of "The Hitchhiker's Guide to the Galaxy", "Q.E.D." is referred to in the Guide's entry for the babel fish, when it is claimed that the babel fish – which serves the "mind-bogglingly" useful purpose of being able to translate any spoken language when inserted into a person's ear – is used as evidence for existence and non-existence of God. The exchange from the novel is as follows: "'I refuse to prove I exist,' says God, 'for proof denies faith, and without faith I am nothing.' 'But,' says Man, 'The babel fish is a dead giveaway, isn't it? It could not have evolved by chance. It proves you exist, and so therefore, by your own arguments, you don't. QED.' 'Oh dear,' says God, 'I hadn't thought of that,' and promptly vanishes in a puff of logic."

In Neal Stephenson's 1999 novel "Cryptonomicon", Q.E.D. is used as a punchline to several humorous anecdotes in which characters go to great lengths to prove something non-mathematical.

Singer-songwriter Thomas Dolby's 1988 song "Airhead" includes the lyric, "Quod erat demonstrandum, baby," referring to the self-evident vacuousness of the eponymous subject; and in response, a female voice squeals, delightedly, "Oooh... you speak French!" 




</doc>
<doc id="25229" url="https://en.wikipedia.org/wiki?curid=25229" title="Quagga">
Quagga

The quagga ( or ) ("Equus quagga quagga") is an extinct subspecies of plains zebra that lived in South Africa until becoming extinct late in the 19th century. It was long thought to be a distinct species, but genetic studies have shown it to be the southernmost subspecies of plains zebra. It is considered particularly close to Burchell's zebra. Its name was derived from its call, which sounded like "kwa-ha-ha".

The quagga is believed to have been around long and tall at the shoulder. It was distinguished from other zebras by its limited pattern of primarily brown and white stripes, mainly on the front part of the body. The rear was brown and without stripes, and therefore more horse-like. The distribution of stripes varied considerably between individuals. Little is known about the quagga's behaviour, but it may have gathered into herds of 30–50 individuals. Quaggas were said to be wild and lively, yet were also considered more docile than Burchell's zebra. They were once found in great numbers in the Karoo of Cape Province and the southern part of the Orange Free State in South Africa.

After the Dutch settlement of South Africa began, the quagga was heavily hunted as it competed with domesticated animals for forage. While some individuals were taken to zoos in Europe, breeding programs were unsuccessful. The last wild population lived in the Orange Free State, and the quagga was extinct in the wild by 1878. The last captive specimen died in Amsterdam on 12 August 1883. Only one quagga was ever photographed alive and only 23 skins are preserved today. In 1984, the quagga was the first extinct animal to have its DNA analysed, and the Quagga Project is trying to recreate the phenotype of hair coat pattern and related characteristics by selectively breeding Burchell's zebras.

The name "quagga" is derived from the Khoikhoi word for "zebra" and is onomatopoeic, being said to resemble the quagga's call, variously transcribed as "kwa-ha-ha", "kwahaah", or "oug-ga". The name is still used colloquially for the plains zebra. The quagga was originally classified as a distinct species, "Equus quagga", in 1778 by Dutch naturalist Pieter Boddaert. Traditionally, the quagga and the other plains and mountain zebras were placed in the subgenus "Hippotigris".

There has been much debate over the status of the quagga in relation to the plains zebra. It is poorly represented in the fossil record, and the identification of these fossils is uncertain, as they were collected at a time when the name "quagga" referred to all zebras. Fossil skulls of "Equus mauritanicus" from Algeria have been claimed to show affinities with the quagga and the plains zebra, but they may be too badly damaged to allow definite conclusions to be drawn from them. Quaggas have also been identified in cave art attributed to the San. Reginald Innes Pocock was perhaps the first to suggest that the quagga was a subspecies of plains zebra in 1902. As the quagga was scientifically described and named before the plains zebra, the trinomial name for the quagga becomes "E. quagga quagga" under this scheme, and the other subspecies of plains zebra are placed under "E. quagga" as well.

Historically, quagga taxonomy was further complicated by the fact that the extinct southernmost population of Burchell's zebra ("Equus quagga burchellii", formerly "Equus burchellii burchellii") was thought to be a distinct subspecies (also sometimes thought a full species, "E. burchellii"). The extant northern population, the "Damara zebra", was later named "Equus quagga antiquorum", which means that it is today also referred to as "E. q. burchellii", after it was realised they were the same taxon. The extinct population was long thought very close to the quagga, since it also showed limited striping on its hind parts. As an example of this, Shortridge placed the two in the now disused subgenus "Quagga" in 1934. Most experts now suggest that the two subspecies represent two ends of a cline.

Different subspecies of plains zebra were recognised as members of "Equus quagga" by early researchers, though there was much confusion over which species were valid. Quagga subspecies were described on the basis of differences in striping patterns, but these differences were since attributed to individual variation within the same populations. Some subspecies and even species, such as "E. q. danielli" and "Hippotigris isabellinus", were only based on illustrations (iconotypes) of aberrant quagga specimens. Some authors have described the quagga as a kind of wild horse rather than a zebra, and one craniometric study from 1980 seemed to confirm its affiliation with the horse ("Equus caballus"). It has been pointed out that early morphological studies were erroneous; using skeletons from stuffed specimens can be problematical, as early taxidermists sometimes used donkey and horse skulls inside their mounts when the originals were unavailable.

The quagga was the first extinct animal to have its DNA analysed, and this 1984 study launched the field of ancient DNA analysis. It confirmed that the quagga was more closely related to zebras than to horses, with the quagga and mountain zebra ("Equus zebra") sharing an ancestor 3–4 million years ago. An immunological study published the following year found the quagga to be closest to the plains zebra. A 1987 study suggested that the mtDNA of the quagga diverged at a range of roughly 2% per million years, similar to other mammal species, and again confirmed the close relation to the plains zebra.

Later morphological studies came to conflicting conclusions. A 1999 analysis of cranial measurements found that the quagga was as different from the plains zebra as the latter is from the mountain zebra. A 2004 study of skins and skulls instead suggested that the quagga was not a distinct species, but a subspecies of the plains zebra. In spite of these findings, many authors subsequently kept the plains zebra and the quagga as separate species.

A genetic study published in 2005 confirmed the subspecific status of the quagga. It showed that the quagga had little genetic diversity, and that it diverged from the other plains zebra subspecies only between 120,000 and 290,000 years ago, during the Pleistocene, and possibly the penultimate glacial maximum. Its distinct coat pattern perhaps evolved rapidly because of geographical isolation and/or adaptation to a drier environment. In addition, plains zebra subspecies tend to have less striping the further south they live, and the quagga was the most southern-living of them all. Other large African ungulates diverged into separate species and subspecies during this period as well, probably because of the same climate shift. The simplified cladogram below is based on the 2005 analysis (some taxa shared haplotypes and could therefore not be differentiated):

The quagga is believed to have been long and tall at the shoulder. Its coat pattern was unique among equids: zebra-like in the front but more like a horse in the rear. It had brown and white stripes on the head and neck, brown upper parts and a white belly, tail and legs. The stripes were boldest on the head and neck and became gradually fainter further down the body, blending with the reddish brown of the back and flanks, until disappearing along the back. It appears to have had a high degree of polymorphism, with some individuals having almost no stripes and others having patterns similar to the extinct southern population of Burchell's zebra, where the stripes covered most of the body except for the hind parts, legs and belly. It also had a broad dark dorsal stripe on its back. It had a standing mane with brown and white stripes.

The only quagga to have been photographed alive was a mare at the Zoological Society of London's Zoo. Five photographs of this specimen are known, taken between 1863 and 1870. On the basis of photographs and written descriptions, many observers suggest that the stripes on the quagga were light on a dark background, unlike other zebras. Reinhold Rau, pioneer of the Quagga Project, claimed that this is an optical illusion: that the base colour is a creamy white and that the stripes are thick and dark. Embryological evidence supports zebras being dark coloured with white as an addition.

Living in the very southern end of the plains zebra's range, the quagga had a thick winter coat that moulted each year. Its skull was described as having a straight profile and a concave diastema, and as being relatively broad with a narrow occiput. Like other plains zebras, the quagga did not have a dewlap on its neck as the mountain zebra does. The 2004 morphological study found that the skeletal features of the southern Burchell's zebra population and the quagga overlapped, and that they were impossible to distinguish. Some specimens also appeared to be intermediate between the two in striping, and individuals of the extant Burchell's zebra population still exhibit limited striping. It can therefore be concluded that the two subspecies graded morphologically into each other. Today, some stuffed specimens of quaggas and southern Burchell's zebra are so similar that they are impossible to definitely identify as either, since no location data was recorded. The female specimens used in the study were larger than the males on average.

The quagga was the southernmost distributed plains zebra, mainly living south of the Orange River. It was a grazer, and its habitat range was restricted to the grasslands and arid interior scrubland of the Karoo region of South Africa, today forming parts of the provinces of Northern Cape, Eastern Cape, Western Cape and the Free State. These areas were known for distinctive flora and fauna and high amounts of endemism.

Little is known about the behaviour of quaggas in the wild, and it is sometimes unclear what exact species of zebra is referred to in old reports. The only source that unequivocally describes the quagga in the Free State is that of the English military engineer and hunter Major Sir William Cornwallis Harris. His 1840 account reads as follows:

The geographical range of the quagga does not appear to extend to the northward of the river Vaal. The animal was formerly extremely common within the colony; but, vanishing before the strides of civilisation, is now to be found in very limited numbers and on the borders only. Beyond, on those sultry plains which are completely taken possession of by wild beasts, and may with strict propriety be termed the domains of savage nature, it occurs in interminable herds; and, although never intermixing with its more elegant congeners, it is almost invariably to be found ranging with the white-tailed gnu and with the ostrich, for the society of which bird especially it evinces the most singular predilection. Moving slowly across the profile of the ocean-like horizon, uttering a shrill, barking neigh, of which its name forms a correct imitation, long files of quaggas continually remind the early traveller of a rival caravan on its march. Bands of many hundreds are thus frequently seen doing their migration from the dreary and desolate plains of some portion of the interior, which has formed their secluded abode, seeking for those more luxuriant pastures where, during the summer months, various herbs thrust forth their leaves and flowers to form a green carpet, spangled with hues the most brilliant and diversified.

Quaggas have been reported gathering into herds of 30–50 individuals and sometimes travelled in a linear fashion. They may have been sympatric with Burchell's zebra between the Vaal and Orange rivers. This is disputed, and there is no evidence that they interbred. It could also have shared a small portion of its range with Hartmann's mountain zebra ("Equus zebra hartmannae").

Quaggas were said to be lively and highly strung, especially the stallions. During the 1830s, quaggas were used as harness animals for carriages in London, the males probably being gelded to mitigate their volatile nature. Local farmers used them as guards for their livestock, as they were likely to attack intruders. On the other hand, captive quaggas in European zoos were said to be tamer and more docile than Burchell's zebra. One specimen was reported to have lived in captivity for 21 years and 4 months, dying in 1872.

Since the practical function of striping has not been determined for zebras in general, it is unclear why the quagga lacked stripes on its hind parts. A cryptic function for protection from predators (stripes obscure the individual zebra in a herd) and biting flies (which are less attracted to striped objects), as well as various social functions, have been proposed for zebras in general. Differences in hind quarter stripes may have aided species recognition during stampedes of mixed herds, so that members of one subspecies or species would follow its own kind. It has also been evidence that the zebras developed striping patterns as thermoregulation to cool themselves down, and that the quagga lost them due to living in a cooler climate, although one problem with this is that the mountain zebra lives in similar environments and has a bold striping pattern. A 2014 study strongly supported the biting-fly hypothesis, and the quagga appears to have lived in areas with lesser amounts of fly activity than other zebras.

As it was easy to find and kill, the quagga was hunted by early Dutch settlers and later by Afrikaners to provide meat or for their skins. The skins were traded or used locally. The quagga was probably vulnerable to extinction due to its limited distribution, and it may have competed with domestic livestock for forage. The quagga had disappeared from much of its range by the 1850s. The last population in the wild, in the Orange Free State, was extirpated in the late 1870s. The last known wild individual died in 1878.

Individual quaggas were also captured and shipped to Europe, where they were displayed in zoos. Lord Morton tried to save the animal from extinction by starting a captive breeding program. He was only able to obtain a single male which, in desperation, he bred with a female horse. This produced a female hybrid with zebra stripes on its back and legs. Lord Morton's mare was sold and was subsequently bred with a black stallion, resulting in offspring that again had zebra stripes. An account of this was published in 1820 by the Royal Society. This led to new ideas on telegony, referred to as "pangenesis" by Charles Darwin. At the close of the 19th century, the Scottish zoologist James Cossar Ewart argued against these ideas and proved, with several cross-breeding experiments, that zebra stripes can pop up as an atavistic trait at any time.

The quagga was long regarded a suitable candidate for domestication, as it counted as the most docile of the striped horses. The earliest Dutch colonists in South Africa had already fantasized about this possiblity, because their imported work-horses did not perform very well in the extreme climate and regularly fell prey to the feared African Horse sickness. In 1843, the English naturalist Charles Hamilton Smith wrote that the quagga was 'unquestionably best calculated for domestication, both as regards strenght and docility'. There are only a few descriptions of tame or domesticated quaggas in South Africa. In Europe, the only confirmed cases are two stallions driven in a phaeton by Joseph Wilfred Parkins, sheriff of London in 1819-1820, and the quaggas and their hybrid offspring of London Zoo, which were used to pull a cart and transport vegetables from the market to the zoo. Nevertheless, the reveries continued long after the death of the last quagga in 1883. In 1889, the naturalist Henry Bryden wrote: "That an animal so beautiful, so capable of domestication and use, and to be found not long since in so great abundance, should have been allowed to be swept from the face of the earth, is surely a disgrace to our latter-day civilization." 

The specimen in London died in 1872 and the one in Berlin in 1875. The last captive individual, a female in Amsterdam's Natura Artis Magistra zoo, lived there from 9 May 1867 until it died on 12 August 1883, but its origin and cause of death are unclear. Its death was not recognised as signifying the extinction of its kind at the time, and the zoo requested another specimen; hunters believed it could still be found "closer to the interior" in the Cape Colony. Since locals used the term quagga to refer to all zebras, this may have led to the confusion. The extinction of the quagga was internationally accepted by the 1900 Convention for the Preservation of Wild Animals, Birds and Fish in Africa. The last specimen was featured on a Dutch stamp in 1988. There are 23 known stuffed and mounted quagga specimens throughout the world, including a juvenile, two foals, and a foetus. In addition, there is a mounted head and neck, a foot, seven complete skeletons, and samples of various tissues. A twenty-fourth mounted specimen was destroyed in Königsberg, Germany, during World War II, and various skeletons and bones have also been lost.

After the very close relationship between the quagga and extant plains zebras was discovered, Reinhold Rau started the Quagga Project in 1987 in South Africa to create a quagga-like zebra population by selectively breeding for a reduced stripe pattern from plains zebra stock, with the eventual aim of introducing them to the quagga's former range. To differentiate between the quagga and the zebras of the project, they refer to it as "Rau quaggas". The founding population consisted of 19 individuals from Namibia and South Africa, chosen because they had reduced striping on the rear body and legs. The first foal of the project was born in 1988. Once a sufficiently quagga-like population has been created, it will be released in the Western Cape.

Introduction of these quagga-like zebras could be part of a comprehensive restoration program including such ongoing efforts as eradication of non-native trees. Quaggas, wildebeest, and ostriches, which occurred together during historical times in a mutually beneficial association, could be kept together in areas where the indigenous vegetation has to be maintained by grazing. In early 2006, the third and fourth generation animals produced by the project were considered looking much like the depictions and preserved specimens of the quagga. This type of selective breeding is called "breeding back". The practice is controversial, since the resulting zebras will resemble the quaggas only in external appearance, but will be genetically different. The technology to use recovered DNA for cloning does not yet exist.



</doc>
<doc id="25231" url="https://en.wikipedia.org/wiki?curid=25231" title="QuickTime">
QuickTime

QuickTime is an extensible multimedia framework developed by Apple Inc., capable of handling various formats of digital video, picture, sound, panoramic images, and interactivity. First made in 1991, the latest Mac version, QuickTime X, is currently available on Mac OS X Snow Leopard and newer. Apple ceased support for the Windows version of QuickTime in 2016.

As of Mac OS X Lion, the underlying media framework for QuickTime, QTKit, is deprecated in favor of a newer graphics framework, AV Foundation. In iOS, the video player on the Internet was QuickTime-based, used to play videos on the internet.

QuickTime is bundled with macOS. QuickTime for Microsoft Windows is downloadable as a standalone installation, and was bundled with Apple's iTunes prior to iTunes 10.5, but is no longer supported and therefore security vulnerabilities will no longer be patched.

Software development kits (SDK) for QuickTime are available to the public with an Apple Developer Connection (ADC) subscription.

It is available free of charge for both macOS and Windows operating systems. There are some other free player applications that rely on the QuickTime framework, providing features not available in the basic QuickTime Player. For example, iTunes can export audio in WAV, AIFF, MP3, AAC, and Apple Lossless. In addition, macOS has a simple AppleScript that can be used to play a movie in full-screen mode, but since version 7.2 full-screen viewing is now supported in the non-Pro version.

QuickTime Player 7 is limited to only basic playback operations unless a QuickTime Pro license key is purchased from Apple. Until recently, Apple's professional applications (e.g. Final Cut Studio, Logic Studio) included a QuickTime Pro license. Pro keys are specific to the major version of QuickTime for which they are purchased and unlock additional features of the QuickTime Player application on macOS or Windows. The Pro key does not require any additional downloads; entering the registration code immediately unlocks the hidden features.

QuickTime 7 is still available for download from Apple, but as of mid 2016, Apple stopped selling registration keys for the Pro version.

Features enabled by the Pro license include, but are not limited to:

Mac OS X Snow Leopard includes QuickTime X. QuickTime Player X lacks cut, copy and paste and will only export to four formats, but its limited export feature is free. Users do not have an option to upgrade to a Pro version of QuickTime X, but those who have already purchased QuickTime 7 Pro and are upgrading to Snow Leopard from a previous version of Mac OS X will have QuickTime 7 stored in the Utilities or user defined folder. Otherwise, users will have to install QuickTime 7 from the "Optional Installs" directory of the Snow Leopard DVD after installing the OS.

Mac OS X Lion and later also include QuickTime X. No installer for QuickTime 7 is included with these software packages, but users can download the QuickTime 7 installer from the Apple support site. QuickTime X on later versions of macOS support cut, copy and paste functions similarly to the way QuickTime 7 Pro did; the interface has been significantly modified to simplify these operations, however. The downloadable version of QuickTime 7 is also still supported.

The QuickTime framework provides the following:


As of early 2008, the framework hides many older codecs listed below from the user although the option to "Show legacy encoders" exists in QuickTime Preferences to use them. The framework supports the following file types and codecs natively:

PictureViewer is a component of QuickTime for Microsoft Windows and the Mac OS 8 and Mac OS 9 operating systems. It is used to view picture files from the still image formats that QuickTime supports. In macOS, it is replaced by Preview.

As of version 7.7.9, the Windows version requires one to go to their "Windows Uninstall Or Change A Program" screen to "modify" their installation of QuickTime 7 to include the "Legacy QuickTime Feature" of "QuickTime PictureViewer".

The native file format for QuickTime video, QuickTime File Format, specifies a multimedia container file that contains one or more tracks, each of which stores a particular type of data: audio, video, effects, or text (e.g. for subtitles). Each track either contains a digitally encoded media stream (using a specific format) or a data reference to the media stream located in another file. The ability to contain abstract data references for the media data, and the separation of the media data from the media offsets and the track edit lists means that QuickTime is particularly suited for editing, as it is capable of importing and editing in place (without data copying).

Other file formats that QuickTime supports natively (to varying degrees) include AIFF, WAV, DV-DIF, MP3, and MPEG program stream. With additional QuickTime Components, it can also support ASF, DivX Media Format, Flash Video, Matroska, Ogg, and many others.

On February 11, 1998, the ISO approved the QuickTime file format as the basis of the MPEG‑4 file format. The MPEG-4 file format specification was created on the basis of the QuickTime format specification published in 2001. The MP4 (.mp4) file format was published in 2001 as the revision of the MPEG-4 Part 1: Systems specification published in 1999 (ISO/IEC 14496-1:2001). In 2003, the first version of MP4 format was revised and replaced by MPEG-4 Part 14: MP4 file format (ISO/IEC 14496-14:2003). The MP4 file format was generalized into the ISO Base Media File Format ISO/IEC 14496-12:2004, which defines a general structure for time-based media files. It in turn is used as the basis for other multimedia file formats (for example 3GP, Motion JPEG 2000). A list of all registered extensions for ISO Base Media File Format is published on the official registration authority website www.mp4ra.org. This registration authority for code-points in "MP4 Family" files is Apple Computer Inc. and it is named in Annex D (informative) in MPEG-4 Part 12.

By 2000, MPEG-4 formats became industry standards, first appearing with support in QuickTime 6 in 2002. Accordingly, the MPEG-4 container is designed to capture, edit, archive, and distribute media, unlike the simple file-as-stream approach of MPEG-1 and MPEG-2.

QuickTime 6 added limited support for MPEG-4; specifically encoding and decoding using Simple Profile (SP). Advanced Simple Profile (ASP) features, like B-frames, were unsupported (in contrast with, for example, encoders such as XviD or 3ivx). QuickTime 7 supports the H.264 encoder and decoder.

Because both MOV and MP4 containers can use the same MPEG-4 codecs, they are mostly interchangeable in a QuickTime-only environment. MP4, being an international standard, has more support. This is especially true on hardware devices, such as the Sony PSP and various DVD players; on the software side, most DirectShow / Video for Windows codec packs include a MP4 parser, but not one for MOV.

In QuickTime Pro's MPEG-4 Export dialog, an option called "Passthrough" allows a clean export to MP4 without affecting the audio or video streams. QuickTime 7 now supports multi-channel AAC-LC and HE-AAC audio (used, for example, in the high-definition trailers on Apple's site), for both .MOV and .MP4 containers.

Apple released the first version of QuickTime on December 2, 1991 as a multimedia add-on for System 6 and later. The lead developer of QuickTime, Bruce Leak, ran the first public demonstration at the May 1991 Worldwide Developers Conference, where he played Apple's famous 1984 advertisement in a window at 320×240 pixels resolution.

The original video codecs included:


The first commercial project produced using QuickTime 1.0 was the CD-ROM From Alice to Ocean. The first publicly visible use of QuickTime was Ben & Jerry's interactive factory tour (dubbed "The Rik & Joe Show" after its in-house developers). "The Rik and Joe Show" was demonstrated onstage at MacWorld in San Francisco when John Sculley announced QuickTime.

Apple released QuickTime 1.5 for Mac OS in the latter part of 1992. This added the SuperMac-developed Cinepak vector-quantization video codec (initially known as Compact Video). It could play video at 320×240 resolution at 30 frames per second on a 25 MHz Motorola 68040 CPU. It also added "text" tracks, which allowed for captioning, lyrics and other potential uses.

Apple contracted San Francisco Canyon Company to port QuickTime to the Windows platform. Version 1.0 of QuickTime for Windows provided only a subset of the full QuickTime API, including only movie playback functions driven through the standard movie controller.

QuickTime 1.6 came out the following year. Version 1.6.2 first incorporated the "QuickTime PowerPlug" which replaced some components with PowerPC-native code when running on PowerPC Macs.

Apple released QuickTime 2.0 for System Software 7 in February 1994—the only version never released for free. It added support for music tracks, which contained the equivalent of MIDI data and which could drive a sound-synthesis engine built into QuickTime itself (using a limited set of instrument sounds licensed from Roland), or any external MIDI-compatible hardware, thereby producing sounds using only small amounts of movie data.

Following Bruce Leak's departure to Web TV, the leadership of the QuickTime team was taken over by Peter Hoddie.

QuickTime 2.0 for Windows appeared in November 1994 under the leadership of Paul Charlton. As part of the development effort for cross-platform QuickTime, Charlton (as architect and technical lead), along with ace individual contributor Michael Kellner and a small highly effective team including Keith Gurganus, ported a subset of the Macintosh Toolbox to Intel and other platforms (notably, MIPS and SGI Unix variants) as the enabling infrastructure for the QuickTime Media Layer (QTML) which was first demonstrated at the Apple Worldwide Developers Conference (WWDC) in May 1996. The QTML later became the foundation for the Carbon API which allowed legacy Macintosh applications to run on the Darwin kernel in Mac OS X.

The next versions, 2.1 and 2.5, reverted to the previous model of giving QuickTime away for free. They improved the music support and added sprite tracks which allowed the creation of complex animations with the addition of little more than the static sprite images to the size of the movie. QuickTime 2.5 also fully integrated QuickTime VR 2.0.1 into QuickTime as a QuickTime extension. On January 16, 1997, Apple released the QuickTime MPEG Extension (PPC only) as an add-on to QuickTime 2.5, which added software MPEG-1 playback capabilities to QuickTime.

In 1994, Apple filed suit against software developer San Francisco Canyon for intellectual property infringement and breach of contract. Apple alleged that San Francisco Canyon had helped develop Video for Windows using several hundred lines of unlicensed QuickTime source code, which was subsequently unilaterally removed. Microsoft and Intel were added to the lawsuit in 1995. The suit ended in a settlement in 1997.

The release of QuickTime 3.0 for Mac OS on March 30, 1998 introduced the now-standard revenue model of releasing the software for free, but with additional features of the Apple-provided MoviePlayer application that end-users could only unlock by buying a QuickTime Pro license code. Since the "Pro" features were the same as the existing features in QuickTime 2.5, any previous user of QuickTime could continue to use an older version of the central MoviePlayer application for the remaining lifespan of Mac OS to 2002; indeed, since these additional features were limited to MoviePlayer, any other QuickTime-compatible application remained unaffected.

QuickTime 3.0 added support for graphics importer components that could read images from GIF, JPEG, TIFF and other file formats, and video output components which served primarily to export movie data via FireWire. Apple also licensed several third-party technologies for inclusion in QuickTime 3.0, including the Sorenson Video codec for advanced video compression, the QDesign Music codec for substantial audio compression, and the complete Roland Sound Canvas instrument set and GS Format extensions for improved playback of MIDI music files. It also added video "effects" which programmers could apply in real-time to video tracks. Some of these effects would even respond to mouse clicks by the user, as part of the new movie interaction support (known as wired movies).

During the development cycle for QuickTime 3.0, part of the engineering team was working on a more advanced version of QuickTime to be known as QuickTime interactive or QTi. Although similar in concept to the wired movies feature released as part of QuickTime 3.0, QuickTime interactive was much more ambitious. It allowed any QuickTime movie to be a fully interactive and programmable container for media. A special track type was added that contained an interpreter for a custom programming language based on 68000 assembly language. This supported a comprehensive user interaction model for mouse and keyboard event handling based in part on the AML language from the Apple Media Tool.

The QuickTime interactive movie was to have been the playback format for the next generation of HyperCard authoring tool. Both the QuickTime interactive and the HyperCard 3.0 projects were canceled in order to concentrate engineering resources on streaming support for QuickTime 4.0, and the projects were never released to the public.

Apple released QuickTime 4.0 on June 8, 1999 for Mac OS 7.5.5 through 8.6 (later Mac OS 9) and Windows 95, Windows 98, and Windows NT. Three minor updates (versions 4.0.1, 4.0.2, and 4.0.3) followed.
It introduced features that most users now consider basic:


On December 17, 1999, Apple provided QuickTime 4.1, this version's first major update. Two minor versions (4.1.1 and 4.1.2) followed. The most notable improvements in the 4.1.x family were:


QuickTime 5 was one of the shortest-lived versions of QuickTime, released in April 2001 and superseded by QuickTime 6 a little over a year later. This version was the last to have greater capabilities under Mac OS 9 than under Mac OS X, and the last version of QuickTime to support Mac OS versions 7.5.5 through 8.5.1 on a PowerPC Mac and Windows 95. Version 5.0 was initially only released for Mac OS and Mac OS X on April 14, 2001, and version 5.0.1 followed shortly thereafter on April 23, 2001, supporting the classic Mac OS, Mac OS X, and Windows. Three more updates to QuickTime 5 (versions 5.0.2, 5.0.4, and 5.0.5) were released over its short lifespan.

QuickTime 5 delivered the following enhancements:

On July 15, 2002, Apple released QuickTime 6.0, providing the following features:


QuickTime 6 was initially available for Mac OS 8.6 – 9.x, Mac OS X (10.1.5 minimum), and Windows 98, Me, 2000, and XP. Development of QuickTime 6 for Mac OS slowed considerably in early 2003, after the release of Mac OS X v10.2 in August 2002. QuickTime 6 for Mac OS continued on the 6.0.x path, eventually stopping with version 6.0.3.

QuickTime 6.1 & 6.1.1 for Mac OS X v10.1 and Mac OS X v10.2 (released October 22, 2002) and QuickTime 6.1 for Windows (released March 31, 2003) offered ISO-Compliant MPEG-4 file creation and fixed the CAN-2003-0168 vulnerability.

Apple released QuickTime 6.2 exclusively for Mac OS X on April 29, 2003 to provide support for iTunes 4, which allowed AAC encoding for songs in the iTunes library. (iTunes was not available for Windows until October 2003.)

On June 3, 2003, Apple released QuickTime 6.3, delivering the following:


QuickTime 6.4, released on October 16, 2003 for Mac OS X v10.2, Mac OS X v10.3, and Windows, added the following:


On December 18, 2003, Apple released QuickTime 6.5, supporting the same systems as version 6.4. Versions 6.5.1 and 6.5.2 followed on April 28, 2004 and October 27, 2004. These versions would be the last to support Windows 98 and Me. The 6.5 family added the following features:


QuickTime 6.5.3 was released on October 12, 2005 for Mac OS X v10.2.8 after the release of QuickTime 7.0, fixing a number of security issues.

Initially released on April 29, 2005 in conjunction with Mac OS X v10.4 (for version 10.3.9 and 10.4.x), QuickTime 7.0 featured the following:

After a couple of preview Windows releases, Apple released 7.0.2 as the first stable release on September 7, 2005 for Windows 2000 and Windows XP. Version 7.0.4, released on January 10, 2006 was the first universal binary version. But it suffered numerous bugs, including a buffer overrun, which is more problematic to most users.

Apple dropped support for Windows 2000 with the release of QuickTime 7.2 on July 11, 2007. The last version available for Windows 2000, 7.1.6, contains numerous security vulnerabilities. References to this version have been removed from the QuickTime site, but it can be downloaded from Apple's support section. Apple has not indicated that they will be providing any further security updates for older versions. QuickTime 7.2 is the first version for Windows Vista.

Apple dropped support for Flash content in QuickTime 7.3, breaking content that relied on Flash for interactivity, or animation tracks. Security concerns seem to be part of the decision. Flash flv files can still be played in QuickTime if the free Perian plugin is added.

In QuickTime 7.3, a processor that supports SSE is required. QuickTime 7.4 does not require SSE. Unlike versions 7.2 and 7.3, QuickTime 7.4 cannot be installed on Windows XP SP1 system (its setup program checks if Service Pack 2 is installed).

QuickTime 7.5 was released on June 10, 2008. QuickTime 7.5.5 was released on September 9, 2008, which requires Mac OS X v10.4 or higher, dropping 10.3 support. QuickTime 7.6 was released on January 21, 2009. QuickTime 7.7 was released on August 23, 2011.

QuickTime 7.6.6 is available for OS X, 10.6.3 Snow Leopard through the current, 10.12 Sierra. There is a 7.7 release of QuickTime 7 for OS X, but it is only for Leopard 10.5.

QuickTime 7.7.6 is the last release for Windows XP.

QuickTime 7.7.9 is the last Windows release of QuickTime. Apple stopped supporting QuickTime on Windows afterwards.

QuickTime X (pronounced "QuickTime Ten") was initially demonstrated at WWDC on June 8, 2009, and shipped with Mac OS X v10.6.

It includes visual chapters, conversion, sharing to YouTube, video editing, capture of video and audio streams, screen recording, GPU acceleration, and live streaming.

But it removed support for various widely used formats; in particular the omission of MIDI caused significant inconvenience and trouble to many musicians and their potential audiences.

In addition, a screen recorder is featured which records whatever is on the screen. However, to prevent bootlegging the user is unable to record any video that is played on the DVD Player or purchased content from iTunes, thus being greyed out.

The reason for the jump in numbering from 7 to 10 (X) was to indicate a similar break with the previous versions of the product that Mac OS X indicated. QuickTime X is fundamentally different from previous versions, in that it is provided as a Cocoa (Objective-C) framework and breaks compatibility with the previous QuickTime 7 C-based APIs that were previously used. QuickTime X was completely rewritten to implement modern audio video codecs in 64-bit. QuickTime X is a combination of two technologies: QuickTime Kit Framework (QTKit) and QuickTime X Player. QTKit is used by QuickTime player to display media. QuickTime X does not implement all of the functionality of the previous QuickTime as well as some of the codecs. When QuickTime X attempts to operate with a 32-bit codec or perform an operation not supported by QuickTime X, it will start a 32-bit helper process to perform the requested operation. The website "Ars Technica" revealed that QuickTime X uses QuickTime 7.x via QTKit to run older codecs that have not made the transition to 64-bit.

QuickTime 7 may still be required to support older formats on Snow Leopard such as QTVR, interactive QuickTime movies, and MIDI files. In such cases, a compatible version of QuickTime 7 is included on Snow Leopard installation disc and may be installed side-by-side with QuickTime X. Users who have a Pro license for QuickTime 7 can then activate their license.

A Snow Leopard compatible version of QuickTime 7 may also be downloaded from Apple Support website.

The software got an increment with the release of Mavericks, and as of June 2015, the current version is v10.4. It contains more sharing options (email, YouTube, Facebook, Flickr etc.), more export options (including web export in multiple sizes, and export for iPhone 4/iPad/Apple TV (but not Apple TV 2) ). It also includes a new way of fast forwarding through a video and mouse support for scrolling.

QuickTime X provides the QTKit Framework on Mac OS 10.6 and greater.

QuickTime consists of two major subsystems: the Movie Toolbox and the Image Compression Manager. The Movie Toolbox consists of a general API for handling time-based data, while the Image Compression Manager provides services for dealing with compressed raster data as produced by video and photo codecs.

Developers can use the QuickTime software development kit (SDK) to develop multimedia applications for Mac or Windows with the C programming language or with the Java programming language (see QuickTime for Java), or, under Windows, using COM/ActiveX from a language supporting this.

The COM/ActiveX option was introduced as part of QuickTime 7 for Windows and is intended for programmers who want to build standalone Windows applications using high-level QuickTime movie playback and control with some import, export, and editing capabilities. This is considerably easier than mastering the original QuickTime C API.

QuickTime 7 for Mac introduced the QuickTime Kit (aka QTKit), a developer framework that is intended to replace previous APIs for Cocoa developers. This framework is for Mac only, and exists as Objective-C abstractions around a subset of the C interface. Mac OS X v10.5 extends QTKit to full 64-bit support. The QTKit allows multiplexing between QuickTime X and QuickTime 7 behind the scenes so that the user need not worry about which version of QuickTime they need to use.

QuickTime 7.4 was found to disable Adobe's video compositing program, After Effects. This was due to the DRM built into version 7.4 since it allowed movie rentals from iTunes. QuickTime 7.4.1 resolved this issue.

Versions 4.0 through 7.3 contained a buffer overflow bug which could compromise the security of a PC using either the QuickTime Streaming Media client, or the QuickTime player itself. The bug was fixed in version 7.3.1.

QuickTime 7.5.5 and earlier are known to have a list of significant vulnerabilities that allow a remote attacker to execute arbitrary code or cause a denial of service (out-of-bounds memory access and application crash) on a targeted system. The list includes six types of buffer overflow, data conversion, signed vs. unsigned integer mismatch, and uninitialized memory pointer.

QuickTime 7.6 has been found to disable Mac users' ability to play certain games, such as "Civilization IV" and "The Sims 2". There are fixes available from the publisher, Aspyr.

QuickTime 7 lacks support for H.264 Sample Aspect Ratio. QuickTime X does not have this limitation, but many Apple products (such as iTunes and Apple TV) still use the older QuickTime 7 engine.

QuickTime 7.7.x on Windows fails to encode H.264 on multi-core systems with more than approximately 20 threads, e.g. HP Z820 with 2× 8-core CPUs. A suggested solution is to disable hyper-threading/limit CPU cores. Encoding speed and stability depends on the scaling of the player window.

On April 14, 2016, Christopher Budd of Trend Micro announced that Apple has ceased all security patching of QuickTime for Windows, and called attention to two Zero Day Initiative advisories, ZDI-16-241
Apple responded with a statement that QuickTime 7 for Windows is no longer supported by Apple.




</doc>
<doc id="25232" url="https://en.wikipedia.org/wiki?curid=25232" title="Quoin (disambiguation)">
Quoin (disambiguation)

Quoin or Du Quoin may refer to:




</doc>
<doc id="25233" url="https://en.wikipedia.org/wiki?curid=25233" title="Quartz">
Quartz

Quartz is a mineral composed of silicon and oxygen atoms in a continuous framework of SiO silicon–oxygen tetrahedra, with each oxygen being shared between two tetrahedra, giving an overall chemical formula of SiO. Quartz is the second most abundant mineral in Earth's continental crust, behind feldspar.

Quartz crystals are chiral, and exist in two forms, the normal α-quartz and the high-temperature β-quartz. The transformation from α-quartz to β-quartz takes place abruptly at 573 °C (846 K). Since the transformation is accompanied by a significant change in volume, it can easily induce fracturing of ceramics or rocks passing through this temperature limit.

There are many different varieties of quartz, several of which are semi-precious gemstones. Since antiquity, varieties of quartz have been the most commonly used minerals in the making of jewelry and hardstone carvings, especially in Eurasia.

The word "quartz" is derived from the German word "Quarz", which had the same form in the first half of the 14th century in Middle High German in East Central German and which came from the Polish dialect term "kwardy", which corresponds to the Czech term "tvrdý" ("hard").

The Ancient Greeks referred to quartz as κρύσταλλος ("krustallos") derived from the Ancient Greek "κρύος" ("kruos") meaning "icy cold", because some philosophers (including Theophrastus) apparently believed the mineral to be a form of supercooled ice. Today, the term "rock crystal" is sometimes used as an alternative name for the purest form of quartz.

Quartz belongs to the trigonal crystal system. The ideal crystal shape is a six-sided prism terminating with six-sided pyramids at each end. In nature quartz crystals are often twinned (with twin right-handed and left-handed quartz crystals), distorted, or so intergrown with adjacent crystals of quartz or other minerals as to only show part of this shape, or to lack obvious crystal faces altogether and appear massive. Well-formed crystals typically form in a 'bed' that has unconstrained growth into a void; usually the crystals are attached at the other end to a matrix and only one termination pyramid is present. However, doubly terminated crystals do occur where they develop freely without attachment, for instance within gypsum. A quartz geode is such a situation where the void is approximately spherical in shape, lined with a bed of crystals pointing inward.

α-quartz crystallizes in the trigonal crystal system, space group "P"321 or "P"321 depending on the chirality. β-quartz belongs to the hexagonal system, space group "P"622 and "P"622, respectively. These space groups are truly chiral (they each belong to the 11 enantiomorphous pairs). Both α-quartz and β-quartz are examples of chiral crystal structures composed of achiral building blocks (SiO tetrahedra in the present case). The transformation between α- and β-quartz only involves a comparatively minor rotation of the tetrahedra with respect to one another, without change in the way they are linked.

Although many of the varietal names historically arose from the color of the mineral, current scientific naming schemes refer primarily to the microstructure of the mineral. Color is a secondary identifier for the cryptocrystalline minerals, although it is a primary identifier for the macrocrystalline varieties.

Pure quartz, traditionally called rock crystal or clear quartz, is colorless and transparent or translucent, and has often been used for hardstone carvings, such as the Lothair Crystal. Common colored varieties include citrine, rose quartz, amethyst, smoky quartz, milky quartz, and others.

The most important distinction between types of quartz is that of "macrocrystalline" (individual crystals visible to the unaided eye) and the microcrystalline or cryptocrystalline varieties (aggregates of crystals visible only under high magnification). The cryptocrystalline varieties are either translucent or mostly opaque, while the transparent varieties tend to be macrocrystalline. Chalcedony is a cryptocrystalline form of silica consisting of fine intergrowths of both quartz, and its monoclinic polymorph moganite. Other opaque gemstone varieties of quartz, or mixed rocks including quartz, often including contrasting bands or patterns of color, are agate, carnelian or sard, onyx, heliotrope, and jasper.

Amethyst is a form of quartz that ranges from a bright to dark or dull purple color. The world's largest deposits of amethysts can be found in Brazil, Mexico, Uruguay, Russia, France, Namibia and Morocco. Sometimes amethyst and citrine are found growing in the same crystal. It is then referred to as ametrine. An amethyst is formed when there is iron in the area where it was formed.

Blue quartz contains inclusions of fibrous magnesio-riebeckite or crocidolite.

Inclusions of the mineral dumortierite within quartz pieces often result in silky-appearing splotches with a blue hue, shades giving off purple and/or grey colors additionally being found. "Dumortierite quartz" (sometimes called "blue quartz") will sometimes feature contrasting light and dark color zones across the material. Interest in the certain quality forms of blue quartz as a collectible gemstone particularly arises in India and in the United States.

Citrine is a variety of quartz whose color ranges from a pale yellow to brown due to ferric impurities. Natural citrines are rare; most commercial citrines are heat-treated amethysts or smoky quartzes. However, a heat-treated amethyst will have small lines in the crystal, as opposed to a natural citrine's cloudy or smokey appearance. It is nearly impossible to differentiate between cut citrine and yellow topaz visually, but they differ in hardness. Brazil is the leading producer of citrine, with much of its production coming from the state of Rio Grande do Sul. The name is derived from the Latin word "citrina" which means "yellow" and is also the origin of the word "citron". Sometimes citrine and amethyst can be found together in the same crystal, which is then referred to as ametrine. Citrine has been referred to as the "merchant's stone" or "money stone", due to a superstition that it would bring prosperity.

Citrine was first appreciated as a golden-yellow gemstone in Greece between 300 and 150 BC, during the Hellenistic Age. The yellow quartz was used prior to that to decorate jewelry and tools but it was not highly sought-after.

Milk quartz or milky quartz is the most common variety of crystalline quartz. The white color is caused by minute fluid inclusions of gas, liquid, or both, trapped during crystal formation, making it of little value for optical and quality gemstone applications.

Rose quartz is a type of quartz which exhibits a pale pink to rose red hue. The color is usually considered as due to trace amounts of titanium, iron, or manganese, in the material. Some rose quartz contains microscopic rutile needles which produces an asterism in transmitted light. Recent X-ray diffraction studies suggest that the color is due to thin microscopic fibers of possibly dumortierite within the quartz.

Additionally, there is a rare type of pink quartz (also frequently called crystalline rose quartz) with color that is thought to be caused by trace amounts of phosphate or aluminium. The color in crystals is apparently photosensitive and subject to fading. The first crystals were found in a pegmatite found near Rumford, Maine, USA and in Minas Gerais, Brazil.

Smoky quartz is a gray, translucent version of quartz. It ranges in clarity from almost complete transparency to a brownish-gray crystal that is almost opaque. Some can also be black. The translucency results from natural irradiation creating free silicon within the crystal.

Prasiolite, also known as "vermarine", is a variety of quartz that is green in color. Since 1950, almost all natural prasiolite has come from a small Brazilian mine, but it is also seen in Lower Silesia in Poland. Naturally occurring prasiolite is also found in the Thunder Bay area of Canada. It is a rare mineral in nature; most green quartz is heat-treated amethyst.

Not all varieties of quartz are naturally occurring. Some clear quartz crystals can be treated using heat or gamma-irradiation to induce color where it would not otherwise have occurred naturally. Susceptibility to such treatments depends on the location from which the quartz was mined.

Prasiolite, an olive colored material, is produced by heat treatment; natural prasiolite has also been observed in Lower Silesia in Poland. Although citrine occurs naturally, the majority is the result of heat-treated amethyst. Carnelian is widely heat-treated to deepen its color.

Because natural quartz is often twinned, synthetic quartz is produced for use in industry. Large, flawless, single crystals are synthesized in an autoclave via the hydrothermal process; emeralds are also synthesized in this fashion.

Like other crystals, quartz may be coated with metal vapors to give it an attractive sheen.

Quartz is a defining constituent of granite and other felsic igneous rocks. It is very common in sedimentary rocks such as sandstone and shale. It is a common constituent of schist, gneiss, quartzite and other metamorphic rocks. Quartz has the lowest potential for weathering in the Goldich dissolution series and consequently it is very common as a residual mineral in stream sediments and residual soils.

While the majority of quartz crystallizes from molten magma, much quartz also chemically precipitates from hot hydrothermal veins as gangue, sometimes with ore minerals like gold, silver and copper. Large crystals of quartz are found in magmatic pegmatites. Well-formed crystals may reach several meters in length and weigh hundreds of kilograms.

Naturally occurring quartz crystals of extremely high purity, necessary for the crucibles and other equipment used for growing silicon wafers in the semiconductor industry, are expensive and rare. A major mining location for high purity quartz is the Spruce Pine Gem Mine in Spruce Pine, North Carolina, United States. Quartz may also be found in Caldoveiro Peak, in Asturias, Spain.

The largest documented single crystal of quartz was found near Itapore, Goiaz, Brazil; it measured approximately 6.1×1.5×1.5 m and weighed more than 44 tonnes.

Tridymite and cristobalite are high-temperature polymorphs of SiO that occur in high-silica volcanic rocks. Coesite is a denser polymorph of SiO found in some meteorite impact sites and in metamorphic rocks formed at pressures greater than those typical of the Earth's crust. Stishovite is a yet denser and higher-pressure polymorph of SiO found in some meteorite impact sites. Lechatelierite is an amorphous silica glass SiO which is formed by lightning strikes in quartz sand.

The word "quartz" comes from the German , which is of Slavic origin (Czech miners called it "křemen"). Other sources attribute the word's origin to the Saxon word "Querkluftertz", meaning "cross-vein ore".

Quartz is the most common material identified as the mystical substance maban in Australian Aboriginal mythology. It is found regularly in passage tomb cemeteries in Europe in a burial context, such as Newgrange or Carrowmore in Ireland. The Irish word for quartz is "grianchloch", which means 'sunstone'. Quartz was also used in Prehistoric Ireland, as well as many other countries, for stone tools; both vein quartz and rock crystal were knapped as part of the lithic technology of the prehistoric peoples.

While jade has been since earliest times the most prized semi-precious stone for carving in East Asia and Pre-Columbian America, in Europe and the Middle East the different varieties of quartz were the most commonly used for the various types of jewelry and hardstone carving, including engraved gems and cameo gems, rock crystal vases, and extravagant vessels. The tradition continued to produce objects that were very highly valued until the mid-19th century, when it largely fell from fashion except in jewelry. Cameo technique exploits the bands of color in onyx and other varieties.

Roman naturalist Pliny the Elder believed quartz to be water ice, permanently frozen after great lengths of time. (The word "crystal" comes from the Greek word "κρύσταλλος", "ice".) He supported this idea by saying that quartz is found near glaciers in the Alps, but not on volcanic mountains, and that large quartz crystals were fashioned into spheres to cool the hands. This idea persisted until at least the 17th century. He also knew of the ability of quartz to split light into a spectrum.

In the 17th century, Nicolas Steno's study of quartz paved the way for modern crystallography. He discovered that regardless of a quartz crystal's size or shape, its long prism faces always joined at a perfect 60° angle.

Quartz's piezoelectric properties were discovered by Jacques and Pierre Curie in 1880. The quartz oscillator or resonator was first developed by Walter Guyton Cady in 1921. George Washington Pierce designed and patented quartz crystal oscillators in 1923. Warren Marrison created the first quartz oscillator clock based on the work of Cady and Pierce in 1927.

Efforts to synthesize quartz began in the mid nineteenth century as scientists attempted to create minerals under laboratory conditions that mimicked the conditions in which the minerals formed in nature: German geologist Karl Emil von Schafhäutl (1803–1890) was the first person to synthesize quartz when in 1845 he created microscopic quartz crystals in a pressure cooker. However, the quality and size of the crystals that were produced by these early efforts were poor.
By the 1930s, the electronics industry had become dependent on quartz crystals. The only source of suitable crystals was Brazil; however, World War II disrupted the supplies from Brazil, so nations attempted to synthesize quartz on a commercial scale. German mineralogist Richard Nacken (1884–1971) achieved some success during the 1930s and 1940s. After the war, many laboratories attempted to grow large quartz crystals. In the United States, the U.S. Army Signal Corps contracted with Bell Laboratories and with the Brush Development Company of Cleveland, Ohio to synthesize crystals following Nacken's lead. (Prior to World War II, Brush Development produced piezoelectric crystals for record players.) By 1948, Brush Development had grown crystals that were 1.5 inches (3.8 cm) in diameter, the largest to date. By the 1950s, hydrothermal synthesis techniques were producing synthetic quartz crystals on an industrial scale, and today virtually all the quartz crystal used in the modern electronics industry is synthetic.

Some types of quartz crystals have piezoelectric properties; they develop an electric potential upon the application of mechanical stress. An early use of this property of quartz crystals was in phonograph pickups. One of the most common piezoelectric uses of quartz today is as a crystal oscillator. The quartz clock is a familiar device using the mineral. The resonant frequency of a quartz crystal oscillator is changed by mechanically loading it, and this principle is used for very accurate measurements of very small mass changes in the quartz crystal microbalance and in thin-film thickness monitors.




</doc>
<doc id="25234" url="https://en.wikipedia.org/wiki?curid=25234" title="Quadrivium">
Quadrivium

The quadrivium (plural: quadrivia) is the four subjects, or arts, taught after teaching the trivium. The word is Latin, meaning "four ways", and its use for the four subjects has been attributed to Boethius or Cassiodorus in the 6th century. Together, the trivium and the quadrivium comprised the seven liberal arts (based on thinking skills), as distinguished from the practical arts (such as medicine and architecture).

The quadrivium consisted of arithmetic, geometry, music, and astronomy. These followed the preparatory work of the trivium, consisting of grammar, logic, and rhetoric. In turn, the quadrivium was considered preparatory work for the study of philosophy (sometimes called the "liberal art "par excellence"") and theology.

These four studies compose the secondary part of the curriculum outlined by Plato in "The Republic" and are described in the seventh book of that work (in the order Arithmetic, Geometry, Astronomy, Music). 
The quadrivium is implicit in early Pythagorean writings and in the "De nuptiis" of Martianus Capella, although the term "quadrivium" was not used until Boethius, early in the sixth century. As Proclus wrote:
The Pythagoreans considered all mathematical science to be divided into four parts: one half they marked off as concerned with quantity, the other half with magnitude; and each of these they posited as twofold. A quantity can be considered in regard to its character by itself or in its relation to another quantity, magnitudes as either stationary or in motion. Arithmetic, then, studies quantities as such, music the relations between quantities, geometry magnitude at rest, spherics [astronomy] magnitude inherently moving.
At many medieval universities, this would have been the course leading to the degree of Master of Arts (after the BA). After the MA, the student could enter for bachelor's degrees of the higher faculties (Theology, Medicine or Law). To this day, some of the postgraduate degree courses lead to the degree of Bachelor (the B.Phil and B.Litt. degrees are examples in the field of philosophy).

The study was eclectic, approaching the philosophical objectives sought by considering it from each aspect of the quadrivium within the general structure demonstrated by Proclus (AD 412–485), namely arithmetic and music on the one hand and geometry and cosmology on the other.

The subject of music within the quadrivium was originally the classical subject of harmonics, in particular the study of the proportions between the musical intervals created by the division of a monochord. A relationship to music as actually practised was not part of this study, but the framework of classical harmonics would substantially influence the content and structure of music theory as practised in both European and Islamic cultures.

In modern applications of the liberal arts as curriculum in colleges or universities, the quadrivium may be considered to be the study of number and its relationship to space or time: arithmetic was pure number, geometry was number in space, music was number in time, and astronomy was number in space and time. Morris Kline classified the four elements of the quadrivium as pure (arithmetic), stationary (geometry), moving (astronomy), and applied (music) number.

This schema is sometimes referred to as "classical education", but it is more accurately a development of the 12th- and 13th-century Renaissance with recovered classical elements, rather than an organic growth from the educational systems of antiquity. The term continues to be used by the Classical education movement and at the independent Oundle School, in the United Kingdom.



</doc>
<doc id="25236" url="https://en.wikipedia.org/wiki?curid=25236" title="Quadrupedalism">
Quadrupedalism

Quadrupedalism or pronograde posture is a form of terrestrial locomotion in animals using four limbs or legs. An animal or machine that usually moves in a quadrupedal manner is known as a quadruped, meaning "four feet" (from the Latin "quattuor" for "four" and "pes" for "foot"). The majority of quadrupeds are vertebrate animals, including mammals such as cattle, dogs and cats, and reptiles such as lizards.

Few other animals are quadrupedal, though a few birds like the shoebill sometimes use their wings to right themselves after lunging at prey.

Although the words "quadruped" and "tetrapod" are both derived from terms meaning "four-footed", they have distinct meanings. A tetrapod is any member of the taxonomic unit "Tetrapoda" (which is defined by descent from a specific four-limbed ancestor) whereas a quadruped actually uses four limbs for locomotion. Not all tetrapods are quadrupeds and not all quadrupeds are tetrapods.

The distinction between quadrupeds and tetrapods is important in evolutionary biology, particularly in the context of tetrapods whose limbs have adapted to other roles (e.g., hands in the case of humans, wings in the case of birds, and fins in the case of whales). All of these animals are tetrapods, but none is a quadruped. Even snakes, whose limbs have become vestigial or lost entirely, are nevertheless tetrapods.

Most quadrupedal animals are tetrapods but there are a few exceptions. For instance, among the insects, the praying mantis is a quadruped.

In July 2005, in rural Turkey, scientists discovered five Kurdish siblings who had learned to walk naturally on their hands and feet. Unlike chimpanzees, which ambulate on their knuckles, the Kurdish siblings walked on their palms, allowing them to preserve the dexterity of their fingers.

Many people, especially practitioners of parkour and freerunning and Georges Hébert's Natural Method, find benefit in quadrupedal movements to build full body strength. Kenichi Ito is a Japanese man famous for speed running on four limbs. Quadrupedalism is sometimes referred to as being on all fours, and is observed in crawling especially by infants.

BigDog is a dynamically stable quadruped robot created in 2005 by Boston Dynamics with Foster-Miller, the NASA Jet Propulsion Laboratory, and the Harvard University Concord Field Station.

Also by NASA JPL, in collaboration with University of California, Santa Barbara Robotics Lab, is RoboSimian, with emphasis on stability and deliberation. It has been demonstrated at the DARPA Robotics Challenge.




</doc>
<doc id="25237" url="https://en.wikipedia.org/wiki?curid=25237" title="Quarantine">
Quarantine

A quarantine is used to separate and restrict the movement of people; it is 'a restraint upon the activities or communication of persons or the transport of goods designed to prevent the spread of disease or pests', for a certain period of time. This is often used in connection to disease and illness, such as those who may possibly have been exposed to a communicable disease. The term is often erroneously used to mean medical isolation, which is "to separate ill persons who have a communicable disease from those who are healthy". The word comes from an Italian variant (seventeenth-century Venetian) of 'quaranta giorni', meaning forty days, the period that all ships were required to be isolated before passengers and crew could go ashore during the Black Death plague epidemic. Quarantine can be applied to humans, but also to animals of various kinds, and both as part of border control as well as within a country.

The quarantining of people often raises questions of civil rights, especially in cases of long confinement or segregation from society, such as that of Mary Mallon (aka Typhoid Mary), a typhoid fever carrier who spent the last 24 years and 7 months her life under quarantine.

Quarantine periods can be very short, such as in the case of a suspected anthrax attack, in which persons are allowed to leave as soon as they shed their potentially contaminated garments and undergo a decontamination shower. For example, an article entitled "Daily News workers quarantined" describes a brief quarantine that lasted until people could be showered in a decontamination tent. (Kelly Nankervis, Daily News).

The February/March 2003 issue of "HazMat Magazine" suggests that people be "locked in a room until proper decon could be performed", in the event of "suspect anthrax".

"Standard-Times" senior correspondent Steve Urbon (14 February 2003) describes such temporary quarantine powers:

Civil rights activists in some cases have objected to people being rounded up, stripped and showered against their will. But Capt. Chmiel said local health authorities have "certain powers to quarantine people".
The purpose of such quarantine-for-decontamination is to prevent the spread of contamination, and to contain the contamination such that others are not put at risk from a person fleeing a scene where contamination is suspect. It can also be used to limit exposure, as well as eliminate a vector.

The first astronauts to visit the Moon were quarantined upon their return at the specially built Lunar Receiving Laboratory.

New developments for quarantine include new concepts in quarantine vehicles such as the ambulance bus, mobile hospitals, and lockdown/invacuation (inverse evacuation) procedures, as well as docking stations for an ambulance bus to dock to a facility that's under lockdown.

The word "quarantine" originates from the Venetian dialect form of the Italian "quaranta giorni", meaning 'forty days'. This is due to the 40-day isolation of ships and people before entering the city-state of Ragusa (modern Dubrovnik, Croatia). This was practiced as a measure of disease prevention related to the Black Death. Between 1348 and 1359, the Black Death wiped out an estimated 30% of Europe's population, and a significant percentage of Asia's population. The original document from 1377, which is kept in the Archives of Dubrovnik, states that before entering the city, newcomers had to spend 30 days (a "trentine") in a restricted place (originally nearby islands) waiting to see whether the symptoms of Black Death would develop. Later, isolation was prolonged to 40 days and was called quarantine.

Other diseases lent themselves to the practice of quarantine before and after the devastation of the plague. Those afflicted with leprosy were historically isolated from society, as were the attempts to check the invasion of syphilis in northern Europe in about 1490, the advent of yellow fever in Spain at the beginning of the 19th century, and the arrival of Asiatic cholera in 1831.

Venice took the lead in measures to check the spread of plague, having appointed three guardians of public health in the first years of the Black Death (1348). The next record of preventive measures comes from Reggio in Modena in 1374. The first lazaret was founded by Venice in 1403, on a small island adjoining the city. In 1467, Genoa followed the example of Venice, and in 1476 the old leper hospital of Marseille was converted into a plague hospital. The great lazaret of Marseilles, perhaps the most complete of its kind, was founded in 1526 on the island of Pomègues. The practice at all the Mediterranean lazarets was not different from the English procedure in the Levantine and North African trade. On the approach of cholera in 1831 some new lazarets were set up at western ports, notably a very extensive establishment near Bordeaux, afterwards turned to another use.

Since 1852 several conferences were held involving European powers, with a view to uniform action in keeping out infection from the East and preventing its spread within Europe. All but that of 1897 were concerned with cholera. No result came of those at Paris (1852), Constantinople (1866), Vienna (1874), and Rome (1885), but each of the subsequent ones doctrine of constructive infection of a ship as coming from a scheduled port, and an approximation to the principles advocated by Great Britain for many years. The principal countries which retained the old system at the time were Spain, Portugal, Turkey, Greece and Russia (the British possessions at the time, Gibraltar, Malta and Cyprus, being under the same influence). The aim of each international sanitary convention had been to bind the governments to a uniform minimum of preventive action, with further restrictions permissible to individual countries. The minimum specified by international conventions was very nearly the same as the British practice, which had been in turn adapted to continental opinion in the matter of the importation of rags.
The Venice convention of 30 January 1892 dealt with cholera by the Suez Canal route; that of Dresden of 15 April 1893, with cholera within European countries; that of Paris of 3 April 1894, with cholera by the pilgrim traffic; and that of Venice, on 19 March 1897, was in connection with the outbreak of plague in the East, and the conference met to settle on an international basis the steps to be taken to prevent, if possible, its spread into Europe. An additional convention was signed in Paris on 3 December 1903.

A multilateral international sanitary convention was concluded at Paris on 17 January 1912. This convention was most comprehensive and was designated to replace all previous conventions on that matter. It was signed by 40 countries, and consisted of 160 articles. Ratifications by 16 of the signatories were exchanged in Paris on 7 October 1920. Another multilateral convention was signed in Paris on 21 June 1926, to replace that of 1912. It was signed by 58 countries worldwide, and consisted of 172 articles.

In Latin America, a series of regional sanitary conventions were concluded. Such a convention was concluded in Rio de Janeiro on 12 June 1904. A sanitary convention between the governments of Argentina, Brazil, Paraguay and Uruguay was concluded in Montevideo on 21 April 1914. The convention covers cases of Asiatic cholera, oriental plague and yellow fever. It was ratified by the Uruguayan government on 13 October 1914, by the Paraguayan government on 27 September 1917 and by the Brazilian government on 18 January 1921.

Sanitary conventions were also concluded between European states. A Soviet-Latvian sanitary convention was signed on 24 June 1922, for which ratifications were exchanged on 18 October 1923. A bilateral sanitary convention was concluded between the governments of Latvia and Poland on 7 July 1922, for which ratifications were exchanged on 7 April 1925. Another was concluded between the governments of Germany and Poland in Dresden on 18 December 1922, and entered into effect on 15 February 1923. Another one was signed between the governments of Poland and Romania on 20 December 1922. Ratifications were exchanged on 11 July 1923. The Polish government also concluded such a convention with the Soviet government on 7 February 1923, for which ratifications were exchanged on 8 January 1924. A sanitary convention was also concluded between the governments of Poland and Czechoslovakia on 5 September 1925, for which ratifications were exchanged on 22 October 1926. A convention was signed between the governments of Germany and Latvia on 9 July 1926, for which ratifications were exchanged on 6 July 1927.

One of the first points to be dealt with in 1897 was to settle the incubation period for this disease, and the period to be adopted for administrative purposes. It was admitted that the incubation period was, as a rule, a comparatively short one, namely, of some three or four days. After much discussion ten days was accepted by a very large majority. The principle of disease notification was unanimously adopted. Each government had to notify to other governments on the existence of plague within their several jurisdictions, and at the same time state the measures of prevention which are being carried out to prevent its diffusion. The area deemed to be infected was limited to the actual district or village where the disease prevailed, and no locality was deemed to be infected merely because of the importation into it of a few cases of plague while there has been no diffusion of the malady. As regards the precautions to be taken on land frontiers, it was decided that during the prevalence of plague every country had the inherent right to close its land frontiers against traffic. As regards the Red Sea, it was decided after discussion that a healthy vessel could pass through the Suez Canal, and continue its voyage in the Mediterranean during the period of incubation of the disease the prevention of which is in question. It was also agreed that vessels passing through the Canal in quarantine might, subject to the use of the electric light, coal in quarantine at Port Said by night as well as by day, and that passengers might embark in quarantine at that port. Infected vessels, if these carry a doctor and are provided with a disinfecting stove, have a right to navigate the Canal, in quarantine, subject only to the landing of those who were suffering from plague.

Plain yellow, green, and even black flags have been used to symbolize disease in both ships and ports, with the color yellow having a longer historical precedent, as a color of marking for houses of infection, previous to its use as a maritime marking color for disease. The present flag used for the purpose is the "Lima" (L) flag, which is a mixture of yellow and black flags previously used. It is sometimes called the "yellow jack" but this was also a name for yellow fever, which probably derives its common name from the flag, not the color of the victims (cholera ships also used a yellow flag). The plain yellow flag ("Quebec" or Q in international maritime signal flags) probably derives its letter symbol for its initial use in "quarantine", but this flag in modern times indicates the opposite—a ship that declares itself free of quarantinable disease, and requests boarding and routine port inspection.

Australia has perhaps the world's strictest quarantine standards. Quarantine in northern Australia is important because of its proximity to South-east Asia and the Pacific, which have many pests and diseases not present in Australia. For this reason, the region from Cairns to Broome—including the Torres Strait—is the focus for many important quarantine activities that protect all Australians. As Australia has been geographically isolated from other major continents for millions of years, there is an endemically unique ecosystem free of several severe pests and diseases that are present in many parts of the world. If other products are brought inside along with pests and diseases, it would damage the ecosystem seriously and add millions of costs in the local agricultural businesses.

The Australian Quarantine and Inspection Service is responsible for border-inspection of any products which are brought into Australia, and assess the potential risks the products might harm Australian environment. Visitors are required to fill in the information card truthfully before arriving in Australia, and declare what food and any products made of wood and other natural materials they have processed. If the visitor fails to do so, usually a quarantine fine of 220 Australian dollars are to be paid as quarantine infringement notice, and if not, the visitor may face criminal convictions of fining 100,000 Australian dollars and 10 years imprisonment.

There are three quarantine Acts of Parliament in Canada: "Quarantine Act" (humans) and "Health of Animals Act" (animals) and "Plant Protection Act" (vegetations). The first legislation is enforced by the Canada Border Services Agency after a complete rewrite in 2005. The second and third legislations are enforced by the Canadian Food Inspection Agency. If a health emergency exists, the Governor in Council can prohibit importation of anything that it deems necessary under the "Quarantine Act".

Under the "Quarantine Act", all travellers must submit to screening and if they believe they might have come into contact with communicable diseases or vectors, they must disclose their whereabouts to a Border Services Officer. If the officer has reasonable grounds to believe that the traveller is or might have been infected with a communicable disease or refused to provide answers, a quarantine officer (QO) must be called and the person is to be isolated. If a person refuses to be isolated, any peace officer may arrest without warrant.

A QO who has reasonable grounds to believe that the traveller has or might have a communicable disease or is infested with vectors, after the medical examination of a traveller, can order him/her into treatment or measures to prevent the person from spreading the disease. QO can detain any traveller who refuses to comply with his/her orders or undergo health assessments as required by law.

Under the "Health of Animals Act" and "Plant Protection Act", inspectors can prohibit access to an infected area, dispose or treat any infected or suspected to be infected animals or plants. The Minister can order for compensation to be given if animals/plants were destroyed pursuant to these acts.

Each province also enacts its own quarantine/environmental health legislations.

Under the "Prevention and Control of Disease Ordinance" (HK Laws. Chap 599), a health officer may seize articles he/she believes to be infectious or contains infectious agents. All travellers, if requested, must submit themselves to a health officer. Failure to do so is against the law and is subject to arrest and prosecution.

The law allows for a health officer who have reasonable grounds to detain, isolate, quarantine anyone or anything believed to be infected and to restrict any articles from leaving a designated quarantine area. He/she may also order the Civil Aviation Department to prohibit the landing or leaving, embarking or disembarking of an aircraft. This power also extends to land, sea or air crossings.

Under the same ordinance, any police officer, health officer, members of the Civil Aid Service or Auxiliary Medical Service can arrest a person who obstructs or escape from detention.

To reduce the risk of introducing rabies from continental Europe, the United Kingdom used to require that dogs, and most other animals introduced to the country, spend six months in quarantine at an HM Customs and Excise pound; this policy was abolished in 2000 in favour of a scheme generally known as Pet Passports, where animals can avoid quarantine if they have documentation showing they are up to date on their appropriate vaccinations.

The plague had disappeared from England, never to return, for more than thirty years before the practice of quarantine against it was definitely established by the Quarantine Act 1710 ("9 Ann.") The first act was called for, owing to an alarm, lest plague should be imported from Poland and the Baltics; the second act of 1721 was due to the disastrous prevalence of plague at Marseille and other places in Provence, France; it was renewed in 1733 owing to a fresh outbreak of the malady on the continent of Europe, and again in 1743, owing to the disastrous epidemic at Messina. In 1752 a rigorous quarantine clause was introduced into an act regulating the Levantine trade; and various arbitrary orders were issued during the next twenty years to meet the supposed danger of infection from the Baltics. Although no plague cases ever came to England all those years, the restrictions on traffic became more and more stringent (following the movements of medical dogma), and in 1788 a very oppressive Quarantine Act was passed, with provisions affecting cargoes in particular. The first year of the nineteenth century marked the turning-point in quarantine legislation; a parliamentary committee sat on the practice, and a more reasonable act arose on their report. In 1805 there was another new act, and in 1823–24 again an elaborate inquiry followed by an act making the quarantine only at discretion of the privy council, and at the same time recognizing yellow fever or other highly infectious disorder as calling for quarantine measures along with plague. The steady approach of cholera in 1831 was the last occasion in England of a thoroughgoing resort to quarantine restrictions. The pestilence invaded every country of Europe despite all efforts to keep it out. In England the experiment of hermetically sealing the ports was not seriously tried when cholera returned in 1849, 1853 and 1865–66. In 1847 the privy council ordered all arrivals with clean bills from the Black Sea and the Levant to be admitted to free pratique, provided there had been no case of plague during the voyage; and therewith the last remnant of the once formidable quarantine practice against plague may be said to have disappeared.

For a number of years after the passing of the first Quarantine Act (1710) the protective practices in England were of the most haphazard and arbitrary kind. In 1721 two vessels laden with cotton goods from Cyprus, then a seat of plague, were ordered to be burned with their cargoes, the owners receiving as indemnity. By the clause in the Levant Trade Act of 1752 vessels for the United Kingdom with a foul bill (i.e. coming from a country where plague existed) had to repair to the lazarets of Malta, Venice, Messina, Livorno, Genoa or Marseille, to perform their quarantine or to have their cargoes sufficiently opened and aired. Since 1741 Stangate Creek (on the Medway) had been made the quarantine station at home; but it would appear from the above clause that it was available only for vessels with clean bills. In 1755 lazarets in the form of floating hulks were established in England for the first time, the cleansing of cargo (particularly by exposure to dews) having been done previously on the ship's deck. There was no medical inspection employed, but the whole routine left to the officers of customs and quarantine. In 1780, when plague was in Poland, even vessels with grain from the Baltic had to lie forty days in quarantine, and unpack and air the sacks; but owing to remonstrances, which came chiefly from Edinburgh and Leith, grain was from that date declared to be a non-susceptible article. About 1788 an order of the council required every ship liable to quarantine, in case of meeting any vessel at sea, or within four leagues of the coast of Great Britain or Ireland, to hoist a yellow flag in the daytime and show a light at the main topmast head at night, under a penalty of After 1800, ships from plague-countries (or with foul bills) were enabled to perform their quarantine on arrival in the Medway instead of taking a Mediterranean port on the way for that purpose; and about the same time an extensive lazaret was built on Chetney Hill near Chatham at an expense of which was almost at once condemned owing to its marshy foundations, and the materials sold for The use of floating hulks as lazarets continued as before. In 1800 two ships with hides from Mogador (Morocco) were ordered to be sunk with their cargoes at the Nore, the owners receiving About this period it was merchandise that was chiefly suspected: there was a long schedule of susceptible articles, and these were first exposed on the ship's deck for twenty-one days or less (six days for each instalment of the cargo), and then transported to the lazaret, where they were opened and aired forty days more. The whole detention of the vessel was from sixty to sixty-five days, including the time for reshipment of her cargo. Pilots had to pass fifteen days on board a convalescent ship. The expenses may be estimated from one or two examples. In 1820 the "Asia", 763 tons, arrived in the Medway with a foul bill from Alexandria, laden with linseed; her freight was and her quarantine dues The same year the "Pilato", 495 tons, making the same voyage, paid quarantine dues on a freight of In 1823 the expenses of the quarantine service (at various ports) were and the dues paid by shipping (nearly all with clean bills) A return for the United Kingdom and colonies in 1849 showed, among other details, that the expenses of the lazaret at Malta for ten years from 1839 to 1848 had been From 1846 onwards the establishments in the United Kingdom were gradually reduced, while the last vestige of the British quarantine law was removed by the Public Health Act 1896, which repealed the Quarantine Act 1825 (with dependent clauses of other acts), and transferred from the privy council to the Local Government Board the powers to deal with ships arriving infected with yellow fever or plague, the powers to deal with cholera ships having been already transferred by the Public Health Act 1875.

The British regulations of 9 November 1896 applied to yellow fever, plague and cholera. Officers of the Royal Customs, as well as of Royal Coast Guard and Board of Trade (for signalling), were empowered to take the initial steps. They certified in writing the master of a supposed infected ship, and detained the vessel provisionally for not more than twelve hours, giving notice meanwhile to the port sanitary authority. The medical officer of the port boarded the ship and examined every person in it. Every person found infected was certified of the fact, removed to a hospital provided (if his condition allow), and kept under the orders of the medical officer. If the sick could be removed, the vessel remained under his orders. Every person suspected (owing to his or her immediate attendance on the sick) could be detained on board for 48 hours or removed to the hospital for a similar period. All others were free to land on giving the addresses of their destinations to be sent to the respective local authorities, so that the dispersed passengers and crew could be kept individually under observation for a few days. The ship was then disinfected, dead bodies buried at sea, infected clothing, bedding, etc., destroyed or disinfected, and bilge-water and water-ballast (subject to exceptions) pumped out at a suitable distance before the ship entered a dock or basin. Mail was subject to no detention. A stricken ship within 3 miles of the shore had to fly at the main mast a yellow and black flag borne quarterly from sunrise to sunset.

The United States puts immediate quarantines on imported products if the disease can be traced back to a certain shipment or product. All imports will also be quarantined if the diseases breakout in other countries. 
According to Title 42 U.S.C. §§264 and 266, these statutes provide the Secretary of the Department of Health and Human Services ("the Secretary") peacetime and wartime authority, respectively, to control the movement of persons into and within the United States to prevent the spread of communicable disease.

Communicable diseases for which apprehension, detention, or conditional release of persons are authorized must be specified in Executive Orders of the President. Executive Order 13295 (Revised List of Quarantinable Communicable Diseases, April 4, 2003) and its amendments (executive orders 13375 and 13674) specify the following infectious diseases: (1) cholera, (2) diphtheria, (3) infectious tuberculosis, (4) plague, (5) smallpox, (6) yellow fever, (7) viral hemorrhagic fevers (Lassa, Marburg, Ebola, Crimean-Congo, South American, and others not yet isolated or named), (8) severe acute respiratory syndromes, and (9) influenza, from a novel or re-emergent source. In the event of conflict of federal, state, local, and/or tribal health authorities in the use of legal quarantine power, federal law is supreme.

The Division of Global Migration and Quarantine (DGMQ) of the US Center for Disease Control (CDC) operates small quarantine facilities at a number of US ports of entry. As of 2014, these included one land crossing (in El Paso, Texas) and 19 international airports.
Besides the port of entry where it is located, each station is also responsible for quarantining potentially infected travelers entering through any ports of entry in its assigned region. These facilities are fairly small; each one is operated by a few staff members and capable of accommodating 1-2 travelers for a short observation period. Cost estimates for setting up a temporary larger facility, capable of accommodating 100 to 200 travelers for several weeks, have been published by the Airport Cooperative Research Program in 2008.

Quarantine law began in Colonial America in 1663, when in an attempt to curb an outbreak of smallpox, the city of New York established a quarantine. In the 1730s, the city built a quarantine station on the Bedloe's Island. The Philadelphia Lazaretto was the first quarantine hospital in the United States, built in 1799, in Tinicum Township, Delaware County, Pennsylvania. There are similar national landmarks such as Swinburne Island and Angel Island (a much more famous historic site, Ellis Island, is often mistakenly assumed to have been a quarantine station, however its marine hospital only qualified as a contagious disease facility to handle less virulent diseases like measles, trachoma and less advanced stages of tuberculosis and diphtheria; persons afflicted with smallpox, yellow fever, cholera, leprosy or typhoid fever, could neither be received nor treated there).
During the 1918 flu pandemic, people were also quarantined. Most commonly suspect cases of infectious diseases are requested to voluntarily quarantine themselves, and Federal and local quarantine statutes only have been uncommonly invoked since then, including for a suspected smallpox case in 1963.

Also other TB carriers who refuse to wear a mask in public have been indefinitely involuntarily committed to regular jails, and cut off from contacting the world.

U.S. President John F. Kennedy euphemistically referred to the U.S. Navy's interdiction of shipping en route to Cuba during the Cuban Missile Crisis as a "quarantine" rather than a blockade, because a quarantine is a legal act in peacetime, whereas a blockade is defined as an act of aggression under the U.N. Charter.

In computer science, "quarantining" describes putting files infected by computer viruses into a special directory, so as to eliminate the threat they pose, without irreversibly deleting them.

The Spanish term for quaratine, "la cuarantina", refers also to the period of postpartum confinement in which a new mother and her baby are sheltered from the outside world.






</doc>
<doc id="25239" url="https://en.wikipedia.org/wiki?curid=25239" title="Quasar">
Quasar

A quasar () (also known as a QSO or quasi-stellar object) is an extremely luminous active galactic nucleus (AGN). Most large galaxies contain a supermassive central black hole with mass ranging from millions to billions of Solar masses. In quasars and other types of AGN, the black hole is surrounded by a gaseous accretion disk. As gas in the accretion disk falls toward the black hole, energy is released in the form of electromagnetic radiation. This radiation can be observed across the electromagnetic spectrum at radio, infrared, visible, ultraviolet, and X-ray, and gamma wavelengths. The power radiated by quasars is enormous: the most powerful quasars have luminosities exceeding 10 watts, thousands of times greater than an ordinary large galaxy such as the Milky Way.

The term "quasar" originated as a contraction of quasi-stellar "[star-like]" radio source, because quasars were first identified during the 1950s as sources of radio-wave emission of unknown physical origin, and when identified in photographic images at visible wavelengths they resembled faint star-like points of light. High-resolution images of quasars, particularly from the Hubble Space Telescope, have demonstrated that quasars occur in the centers of galaxies, and that some quasar host galaxies are strongly interacting or merging galaxies. As with other categories of AGN, the observed properties of a quasar depend on many factors including the mass of the black hole, the rate of gas accretion, the orientation of the accretion disk relative to the observer, the presence or absence of a jet, and the degree of obscuration by gas and dust within the host galaxy.

Quasars are found over a very broad range of distances (corresponding to redshifts ranging from z < 0.1 for the nearest quasars to z > 7 for the most distant known quasars), and quasar discovery surveys have demonstrated that quasar activity was more common in the distant past. The peak epoch of quasar activity in the Universe corresponds to redshifts around 2, or approximately 10 billion years ago. As of 2017, the most distant known quasar is ULAS J1342+0928 at redshift z=7.54; light observed from this quasar was emitted when the Universe was only 690 million years old. The supermassive black hole in this quasar is the most distant black hole identified to date, and is estimated to have a mass that is 800 million times the mass of our Sun.

The term "quasar" was coined by Chinese-born U.S. astrophysicist Hong-Yee Chiu in May 1964, in "Physics Today", to describe certain astronomically-puzzling objects:

Between 1917 and 1922, it became clear from work by Heber Curtis, Ernst Öpik and others, that some objects ("nebulae") seen by astronomers were in fact distant galaxies like our own. But when radio astronomy commenced in the 1950s, astronomers detected, among the galaxies, a small number of anomalous objects with properties that defied explanation.

The objects emitted large amounts of radiation of many frequencies, but no source could be located optically, or in some cases only a faint and point-like object somewhat like a distant star. The spectral lines of these objects, which identify the chemical elements of which the object is composed, were also extremely strange and defied explanation. Some of them changed their luminosity very rapidly in the optical range and even more rapidly in the X-ray range, suggesting an upper limit on their size, perhaps no larger than our own Solar System. This implies an extremely high power density. Considerable discussion took place over what these objects might be. They were described as ""quasi-stellar" [meaning: star-like] "radio sources"", or ""quasi-stellar objects"" (QSOs), a name which reflected their unknown nature, and this became shortened to "quasar".

The first quasars (3C 48 and 3C 273) were discovered in the late 1950s, as radio sources in all-sky radio surveys. They were first noted as radio sources with no corresponding visible object. Using small telescopes and the Lovell Telescope as an interferometer, they were shown to have a very small angular size. Hundreds of these objects were recorded by 1960 and published in the Third Cambridge Catalogue as astronomers scanned the skies for their optical counterparts. In 1963, a definite identification of the radio source 3C 48 with an optical object was published by Allan Sandage and Thomas A. Matthews. Astronomers had detected what appeared to be a faint blue star at the location of the radio source and obtained its spectrum, which contained many unknown broad emission lines. The anomalous spectrum defied interpretation.

British-Australian astronomer John Bolton made many early observations of quasars, including a breakthrough in 1962. Another radio source, 3C 273, was predicted to undergo five occultations by the Moon. Measurements taken by Cyril Hazard and John Bolton during one of the occultations using the Parkes Radio Telescope allowed Maarten Schmidt to find a visible counterpart to the radio source and obtain an optical spectrum using the 200-inch Hale Telescope on Mount Palomar. This spectrum revealed the same strange emission lines. Schmidt was able to demonstrate that these were likely to be the ordinary spectral lines of hydrogen redshifted by 15.8 percent - an extreme redshift never seen in astronomy before. If this was due to the physical motion of the "star", then 3C 273 was receding at an emormous velocity, around 47,000 km/s, far beyond the speed of any known star and defying any obvious explanation. Nor would an extreme velocity help to explain 3C 273's huge radio emissions. 

Although it raised many questions, Schmidt's discovery quickly revolutionized quasar observation. The strange spectrum of 3C 48 was quickly identified by Schmidt, Greenstein and Oke as hydrogen and magnesium redshifted by 37%. Shortly afterwards, two more quasar spectra in 1964 and five more in 1965, were also confirmed as ordinary light that had been redshifted to an extreme degree. 

Although the observations and redshifts themselves were not doubted, their correct interpretation was heavily debated, and Bolton's suggestion that the radiation detected from quasars were ordinary spectral lines from distant highly redshifted sources with extreme velocity was not widely accepted at the time.

One great topic of debate during the 1960s was how to interpret the redshifted light and comparatively small sizes seen with quasars. Schmidt's discovery that quasar redshifts could be explained as ordinary spectral lines redshifted by very large amounts, had quickly raised far more questions than it could answer.

An extreme redshift could imply great distance and velocity, but could also be due to extreme mass, or perhaps some other unknown laws of nature. Extreme velocity and distance would also imply immense power output, which lacked explanation, and conflicted with the traditional and predominant Steady State theory of the universe. The small sizes were confirmed by interferometry and by observing the speed with which the quasar as a whole varied in output, and by their inability to be seen in even the most powerful visible light telescopes as anything more than faint starlike points of light. But if they were small, the power output became harder to explain. If quasars were very small and nearer to our galaxy, it would be easy to explain the apparent power output, but less easy to explain their redshifts and lack of detectable movement against the background of the universe.

Various explanations were proposed over time. It was suggested, for example, that the redshift of quasars was not due to the expansion of space but rather to light escaping a deep gravitational well. However a star of sufficient mass to form such a well would be unstable and in excess of the Hayashi limit. Quasars also show forbidden spectral emission lines which were previously only seen in hot gaseous nebulae of low density, which would be too diffuse to both generate the observed power and fit within a deep gravitational well. There were also serious concerns regarding the idea of cosmologically distant quasars. One strong argument against them was that they implied energies that were far in excess of known energy conversion processes, including nuclear fusion. There were some suggestions that quasars were made of some hitherto unknown form of stable antimatter regions and that this might account for their brightness. Others speculated that quasars were a white hole end of a wormhole, or a chain reaction of numerous supernovae.

Schmidt noted that redshift is also associated with the expansion of the universe, as codified in Hubble's law. If the measured redshift was due to expansion, then the object in question would have to be very far away. In that case, it would have to have an extraordinarily high luminosity, equally beyond any object seen to date. This extreme luminosity would also explain the large radio signal. Schmidt concluded quasars are very distant, very luminous objects.

Schmidt's explanation for the high redshift was not widely accepted at the time. Another explanation that was offered was that it was gravitational redshift that was being measured; this would require a massive object that would also explain the high luminosities. A star large enough to produce the measured redshift would be well beyond the Hayashi limit. Several other mechanisms were proposed as well, each with their own problems.

A major concern was the enormous amount of energy these objects would have to be radiating, if they were distant. No commonly-accepted mechanism could account for this. The correct explanation, that it was due to matter in an accretion disc falling into an supermassive black hole, was only suggested in 1964 by Salpeter and Yakov Zel'dovich, and even then it was rejected by many astronomers, because the existence of black holes was still widely seen as theoretical and too exotic in the 1960s, and because it was not yet confirmed that many galaxies (including our own) have supermassive black holes at their center. The strange spectral lines in their radiation, and the speed of change seen in some quasars, also suggested to many astronomers and cosmologists that the objects were comparatively small and therefore perhaps bright, massive and not far away; accordingly that their redshifts were not due to distance or velocity, and must be due to some other reason or an unknown process, meaning that the quasars were not really powerful objects nor at extreme distances, as their redshifted light implied. A common alternative explanation was that the redshifts were caused by extreme mass (gravitational redshifting explained by general relativity) and not by extreme velocity (explained by special relativity). In 1984, it was stated that "one of the few statements [about Active Galactic Nuclei] to command general agreement has been that the power supply is primarily gravitational", with the cosmological origin of the redshift being taken as given.

Eventually, starting from about the 1970s, many lines of evidence (including the first X-Ray space observatories, knowledge of black holes and modern models of cosmology) gradually demonstrated that the quasar redshifts are genuine, and due to the expansion of space, that quasars are in fact as powerful and as distant as Schmidt and some other astronomers had suggested, and that their energy source is matter from an accretion disc falling onto a supermassive black hole. This included crucial evidence from optical and X-Ray viewing of quasar host galaxies, finding of 'intervening' absorption lines which explained various spectral anomalies, observations from gravitational lensing, Peterson and Gunn's 1971 finding that galaxies containing quasars showed the same redshift as the quasars, and Kristian's 1973 finding that the "fuzzy" surrounding of many quasars was consistent with a less luminous host galaxy.

This model also fits well with other observations that suggest many or even most galaxies have a massive central black hole. It would also explain why quasars are more common in the early universe: as a quasar draws matter from their accretion disc, there will come a point where there is less matter nearby, and energy production falls off or ceases as the quasar becomes a more ordinary type of galaxy.

The accretion disc energy-production mechanism was finally modeled in the 1970s, and black holes were also directly detected (including evidence showing that supermassive black holes could be found at the centers of our own and many other galaxies), which resolved the concern that quasars were too luminous to be a result of very distant objects or that a suitable mechanism could not be confirmed to exist in nature. By 1987 it was "well accepted" that this was the correct explanation for quasars, and the cosmological distance and energy output of quasars was accepted by almost all researchers.

Later it was found that not all quasars have strong radio emission; in fact only about 10% are "radio-loud". Hence the name 'QSO' (quasi-stellar object) is used (in addition to "quasar") to refer to these objects, including the 'radio-loud' and the 'radio-quiet' classes. The discovery of the quasar had large implications for the field of astronomy in the 1960s, including drawing physics and astronomy closer together.

In 1979 the gravitational lens effect predicted by Einstein's General Theory of Relativity was confirmed observationally for the first time with images of the double quasar 0957+561.

It is now known that quasars are distant but extremely luminous objects, so any light which reaches the Earth is redshifted due to the metric expansion of space. 

Quasars inhabit the center of active galaxies, and are among the most luminous, powerful, and energetic objects known in the universe, emitting up to a thousand times the energy output of the Milky Way, which contains 200–400 billion stars. This radiation is emitted across the electromagnetic spectrum, almost uniformly, from X-rays to the far-infrared with a peak in the ultraviolet-optical bands, with some quasars also being strong sources of radio emission and of gamma-rays. With high-resolution imaging from ground-based telescopes and the Hubble Space Telescope, the "host galaxies" surrounding the quasars have been detected in some cases. These galaxies are normally too dim to be seen against the glare of the quasar, except with special techniques. Most quasars, with the exception of 3C 273 whose average apparent magnitude is 12.9, cannot be seen with small telescopes.

Quasars are believed - and in many cases confirmed - to be powered by accretion of material into supermassive black holes in the nuclei of distant galaxies. Light and other radiation cannot escape from within the event horizon of a black hole, but the energy produced by a quasar is generated "outside" the black hole, by gravitational stresses and immense friction within the material nearest to the black hole, as it orbits and falls inward. The huge luminosity of quasars results from the accretion discs of central supermassive black holes, which can convert between 6% and 32% of the mass of an object into energy, compared to just 0.7% for the p-p chain nuclear fusion process that dominates the energy production in Sun-like stars. Central masses of 10 to 10 solar masses have been measured in quasars by using reverberation mapping. Several dozen nearby large galaxies - including our own Milky Way galaxy, that do not have an active center and do not show any activity similar to a quasar, are confirmed to contain a similar supermassive black hole in their nuclei (galactic center), so it is now thought that all large galaxies have a black hole of this kind, but only a small fraction have sufficient matter in the right kind of orbit at their center, to become active and power radiation in this way. It is the activity of these black holes that are seen as quasars. 

This also explains why quasars were more common in the early universe, as this energy production ends when the supermassive black hole consumes all of the gas and dust near it. This means that it is possible that most galaxies, including the Milky Way, have gone through an active stage, appearing as a quasar or some other class of active galaxy that depended on the black hole mass and the accretion rate, and are now quiescent because they lack a supply of matter to feed into their central black holes to generate radiation.

The matter accreting onto the black hole is unlikely to fall directly in, but will have some angular momentum around the black hole that will cause the matter to collect into an accretion disc. Quasars may also be ignited or re-ignited when normal galaxies merge and the black hole is infused with a fresh source of matter. In fact, it has been suggested that a quasar could form when the Andromeda Galaxy collides with our own Milky Way galaxy in approximately 3–5 billion years.

In the 1980s, unified models were developed in which quasars were classified as a particular kind of active galaxy, and a consensus emerged that in many cases it is simply the viewing angle that distinguishes them from other active galaxies, such as blazars and radio galaxies. 

The mechanism of brightness changes probably involves relativistic beaming of astrophysical jets pointed nearly directly toward Earth. The highest redshift quasar known () is ULAS J1120+0641, with a redshift of 7.085, which corresponds to a comoving distance of approximately 29 billion light-years from Earth (these distances are much larger than the distance light could travel in the universe's 13.8 billion year history because space itself has also been expanding).

More than 200,000 quasars are known, most from the Sloan Digital Sky Survey. All observed quasar spectra have redshifts between 0.056 and 7.085.
Applying Hubble's law to these redshifts, it can be shown that they are between 600 million and 28.85 billion light-years away (in terms of comoving distance). Because of the great distances to the farthest quasars and the finite velocity of light, they and their surrounding space appear as they existed in the very early universe.

The power of quasars originates from supermassive black holes that are believed to exist at the core of most galaxies. The Doppler shifts of stars near the cores of galaxies indicate that they are rotating around tremendous masses with very steep gravity gradients, suggesting black holes.

Although quasars appear faint when viewed from Earth, they are visible from extreme distances, being the most luminous objects in the known universe. The brightest quasar in the sky is 3C 273 in the constellation of Virgo. It has an average apparent magnitude of 12.8 (bright enough to be seen through a medium-size amateur telescope), but it has an absolute magnitude of −26.7. From a distance of about 33 light-years, this object would shine in the sky about as brightly as our sun. This quasar's luminosity is, therefore, about 4 trillion (4 × 10) times that of the Sun, or about 100 times that of the total light of giant galaxies like the Milky Way. This assumes the quasar is radiating energy in all directions, but the active galactic nucleus is believed to be radiating preferentially in the direction of its jet. In a universe containing hundreds of billions of galaxies, most of which had active nuclei billions of years ago but only seen today, it is statistically certain that thousands of energy jets should be pointed toward the Earth, some more directly than others. In many cases it is likely that the brighter the quasar, the more directly its jet is aimed at the Earth.

The hyperluminous quasar APM 08279+5255 was, when discovered in 1998, given an absolute magnitude of −32.2. High resolution imaging with the Hubble Space Telescope and the 10 m Keck Telescope revealed that this system is gravitationally lensed. A study of the gravitational lensing of this system suggests that the light emitted has been magnified by a factor of ~10. It is still substantially more luminous than nearby quasars such as 3C 273.

Quasars were much more common in the early universe than they are today. This discovery by Maarten Schmidt in 1967 was early strong evidence against the Steady State cosmology of Fred Hoyle, and in favor of the Big Bang cosmology. Quasars show the locations where massive black holes are growing rapidly (via accretion). These black holes grow in step with the mass of stars in their host galaxy in a way not understood at present. One idea is that jets, radiation and winds created by the quasars, shut down the formation of new stars in the host galaxy, a process called 'feedback'. The jets that produce strong radio emission in some quasars at the centers of clusters of galaxies are known to have enough power to prevent the hot gas in those clusters from cooling and falling onto the central galaxy.

Quasars' luminosities are variable, with time scales that range from months to hours. This means that quasars generate and emit their energy from a very small region, since each part of the quasar would have to be in contact with other parts on such a time scale as to allow the coordination of the luminosity variations. This would mean that a quasar varying on a time scale of a few weeks cannot be larger than a few light-weeks across. The emission of large amounts of power from a small region requires a power source far more efficient than the nuclear fusion that powers stars. The conversion of gravitational potential energy to radiation by infalling to a black hole converts between 6% and 32% of the mass to energy, compared to 0.7% for the conversion of mass to energy in a star like our sun. It is the only process known that can produce such high power over a very long term. (Stellar explosions such as supernovas and gamma-ray bursts, and direct matter-antimatter annihilation, can also produce very high power output, but supernovae only last for days, and the universe does not appear to have had large amounts of antimatter at the relevant times).

Since quasars exhibit all the properties common to other active galaxies such as Seyfert galaxies, the emission from quasars can be readily compared to those of smaller active galaxies powered by smaller supermassive black holes. To create a luminosity of 10 watts (the typical brightness of a quasar), a super-massive black hole would have to consume the material equivalent of 10 stars per year. The brightest known quasars devour 1000 solar masses of material every year. The largest known is estimated to consume matter equivalent to 600 Earths per minute. Quasar luminosities can vary considerably over time, depending on their surroundings. Since it is difficult to fuel quasars for many billions of years, after a quasar finishes accreting the surrounding gas and dust, it becomes an ordinary galaxy. 

Radiation from quasars is partially 'nonthermal' (i.e., not due to black body radiation), and approximately 10 percent are observed to also have jets and lobes like those of radio galaxies that also carry significant (but poorly understood) amounts of energy in the form of particles moving at relativistic speeds. Extremely high energies might be explained by several mechanisms (see Fermi acceleration and Centrifugal mechanism of acceleration). Quasars can be detected over the entire observable electromagnetic spectrum including radio, infrared, visible light, ultraviolet, X-ray and even gamma rays. Most quasars are brightest in their rest-frame near-ultraviolet wavelength of 121.6 nm Lyman-alpha emission line of hydrogen, but due to the tremendous redshifts of these sources, that peak luminosity has been observed as far to the red as 900.0 nm, in the near infrared. A minority of quasars show strong radio emission, which is generated by jets of matter moving close to the speed of light. When viewed downward, these appear as blazars and often have regions that seem to move away from the center faster than the speed of light (superluminal expansion). This is an optical illusion due to the properties of special relativity.

Quasar redshifts are measured from the strong spectral lines that dominate their visible and ultraviolet emission spectra. These lines are brighter than the continuous spectrum. They exhibit Doppler broadening corresponding to mean speed of several percent of the speed of light. Fast motions strongly indicate a large mass. Emission lines of hydrogen (mainly of the Lyman series and Balmer series), helium, carbon, magnesium, iron and oxygen are the brightest lines. The atoms emitting these lines range from neutral to highly ionized, leaving it highly charged. This wide range of ionization shows that the gas is highly irradiated by the quasar, not merely hot, and not by stars, which cannot produce such a wide range of ionization.

Like all (unobscured) active galaxies, quasars can be strong X-ray sources. Radio-loud quasars can also produce X-rays and gamma rays by inverse Compton scattering of lower-energy photons by the radio-emitting electrons in the jet.

"Iron quasars" show strong emission lines resulting from low ionization iron (FeII), such as IRAS 18508-7815.

Quasars also provide some clues as to the end of the Big Bang's reionization. The oldest known quasars (redshift = 6) display a Gunn-Peterson trough and have absorption regions in front of them indicating that the intergalactic medium at that time was neutral gas. More recent quasars show no absorption region but rather their spectra contain a spiky area known as the Lyman-alpha forest; this indicates that the intergalactic medium has undergone reionization into plasma, and that neutral gas exists only in small clouds.

Quasars show evidence of elements heavier than helium, indicating that galaxies underwent a massive phase of star formation, creating population III stars between the time of the Big Bang and the first observed quasars. Light from these stars may have been observed in 2005 using NASA's Spitzer Space Telescope, although this observation remains to be confirmed.

The taxonomy of quasars includes various subtypes representing subsets of the quasar population having distinct properties. 


Because quasars are extremely distant, bright, and small in apparent size, they are useful reference points in establishing a measurement grid on the sky.
The International Celestial Reference System (ICRS) is based on hundreds of extra-galactic radio sources, mostly quasars, distributed around the entire sky. Because they are so distant, they are apparently stationary to our current technology, yet their positions can be measured with the utmost accuracy by very-long-baseline interferometry (VLBI). The positions of most are known to 0.001 arcsecond or better, which is orders of magnitude more precise than the best optical measurements.

A multiple-image quasar is a quasar whose light undergoes gravitational lensing, resulting in double, triple or quadruple images of the same quasar. The first such gravitational lens to be discovered was the double-imaged quasar Q0957+561 (or Twin Quasar) in 1979. A grouping of two or more quasars can result from a chance alignment, physical proximity, actual close physical interaction, or effects of gravity bending the light of a single quasar into two or more images.

As quasars are rare objects, the probability of three or more separate quasars being found near the same location is very low. The first true triple quasar was found in 2007 by observations at the W. M. Keck Observatory Mauna Kea, Hawaii. LBQS 1429-008 (or QQQ J1432-0106) was first observed in 1989 and was found to be a double quasar; itself a rare occurrence. When astronomers discovered the third member, they confirmed that the sources were separate and not the result of gravitational lensing. This triple quasar has a red shift of "z" = 2.076, which is equivalent to 10.5 billion light years. The
components are separated by an estimated 30–50 kpc, which is typical of interacting galaxies. An example of a triple quasar that is formed by lensing is PG1115 +08.

In 2013, the second true triplet quasars QQQ J1519+0627 was found with redshift "z" = 1.51 (approx 9 billion light years) by an international team of astronomers led by Farina of the University of Insubria, the whole system is well accommodated within 25" (i.e., 200 kpc in projected distance). The team accessed data from observations collected at the La Silla Observatory with the New Technology Telescope (NTT) of the European Southern Observatory (ESO) and at the Calar Alto Observatory with the 3.5m telescope of the Centro Astronómico Hispano
Alemán (CAHA).

The first quadruple quasar was discovered in 2015.

When two quasars are so nearly in the same direction as seen from Earth that they appear to be a single quasar but may be separated by the use of telescopes, they are referred to as a "double quasar", such as the Twin Quasar. These are two different quasars, and not the same quasar that is gravitationally lensed. This configuration is similar to the optical double star. Two quasars, a "quasar pair", may be closely related in time and space, and be gravitationally bound to one another. These may take the form of two quasars in the same galaxy cluster. This configuration is similar to two prominent stars in a star cluster. A "binary quasar", may be closely linked gravitationally and form a pair of interacting galaxies. This configuration is similar to that of a binary star system.




</doc>
<doc id="25240" url="https://en.wikipedia.org/wiki?curid=25240" title="Quinquagesima">
Quinquagesima

Quinquagesima is one of the names used in the Western Church for the Sunday before Ash Wednesday. It is also called Quinquagesima Sunday, Quinquagesimae, Estomihi, Shrove Sunday, or the Sunday next before Lent.

The name Quinquagesima originates from Latin "quinquagesimus" (fiftieth). This is in reference to the fifty days before Easter Day using inclusive counting which counts both Sundays (normal counting would count only one of these). Since the forty days of the Lent do not include Sundays, the first day of Lent, Ash Wednesday, succeeds Quinquagesima Sunday by only three days. 

The name Estomihi is derived from the incipit or opening words of the Introit for the Sunday, "Esto mihi in Deum protectorem, et in locum refugii, ut salvum me facias", ("Be Thou unto me a God, a Protector, and a place of refuge, to save me") .

The earliest Quinquagesima Sunday can occur is February 1 and the latest is March 7.

In the Roman Catholic Church, the terms for this Sunday (and the two immediately before it — Sexagesima and Septuagesima Sundays) were eliminated in the reforms following the Second Vatican Council, and these Sundays are part of Ordinary Time.

According to the reformed Roman Rite Roman Catholic calendar, this Sunday is now known by its number within Ordinary Time — fourth through ninth, depending upon the date of Easter. The earlier form of the Roman Rite, with its references to Quinquagesima Sunday, and to the Sexagesima and Septuagesima Sundays, continues to be observed in some communities.

In traditional lectionaries, the Sunday concentrates on , "Jesus took the twelve aside and said, 'Lo, we go to Jerusalem, and everything written by the prophets about the Son of Man shall be fulfilled' ... The disciples, however, understood none of this," which from verse 35 is followed by Luke's version of Healing the blind near Jericho. The passage presages the themes of Lent and Holy Week.

This Sunday has different names in the two different calendars used in the Church of England: in the "Book of Common Prayer" calendar (1662) this Sunday is known as "Quinquagesima", while in the "Common Worship" calendar (2000) it is known as the "Sunday next before Lent". In this latter calendar it is part of the period of Ordinary Time that falls between the feasts of the Presentation of Christ in the Temple (the end of the Epiphany season) and Ash Wednesday.

In the Revised Common Lectionary the Sunday before Lent is designated "Transfiguration Sunday", and the gospel reading is the story of the Transfiguration of Jesus from Matthew, Mark, or Luke. Some churches whose lectionaries derive from the RCL, e.g. the Church of England, use these readings but do not designate the Sunday "Transfiguration Sunday".

In the Eastern Orthodox Church, its equivalent, the Sunday before Great Lent, is called "Forgiveness Sunday", "Maslenitsa Sunday", or "Cheesefare Sunday". The latter name comes because this Sunday concludes Maslenitsa, the week in which butter and cheese may be eaten, which are prohibited during Great Lent. The former name derives from the fact that this Sunday is followed by a special Vespers called "Forgiveness Vespers" which opens Great Lent.
On this day the Eastern Orthodox Church Christians at the liturgy listen to the Gospel speaking of forgiveness of sins, fasting, and the gathering of treasures in heaven. On this day, all Orthodox Christians ask each other for forgiveness to begin the Great Lent with a good heart, to focus on the spiritual life, to purify the heart from sin in confession, and to meet Easter - the day of the Resurrection of Jesus with a pure heart.
This is the last day before Lent when non-lenten food is eaten.

In Lutheranism is combined with (Paul's praise of love).

Composers writing cantatas for Estomihi Sunday include:



</doc>
<doc id="25241" url="https://en.wikipedia.org/wiki?curid=25241" title="Quassia">
Quassia

Quassia ( or ) is a flora genus in the family Simaroubaceae. Its size is disputed; some botanists treat it as consisting of only one species, "Quassia amara" from tropical South America, while others treat it in a wide circumscription as a pantropical genus containing up to 40 species of trees and shrubs. The genus was named after a former slave from Surinam, Graman Quassi in the eighteenth century. He discovered the medicinal properties of the bark of "Quassia amara".

Broader treatments of the genus include the following and other species:

It is the source of the quassinoids quassin and neo-quassin.



</doc>
<doc id="25243" url="https://en.wikipedia.org/wiki?curid=25243" title="Quisling">
Quisling

Quisling (; ) is a term originating in Norway, which is used in Scandinavian languages and in English for a person who collaborates with an enemy occupying force – or more generally as a synonym for traitor. The word originates from the surname of the Norwegian war-time leader Vidkun Quisling, who headed a domestic Nazi collaborationist regime during World War II.

Use of Quisling's surname as a term predates World War II. The first recorded use of the term was by Norwegian Labour Party politician Oscar Torp in a 2 January 1933 newspaper interview, where he used it as a general term for followers of Vidkun Quisling. Quisling was at this point in the process of establishing the Nasjonal Samling (National Unity) party, a fascist party modelled on the German Nazi Party. Further uses of the term were made by Aksel Sandemose, in a newspaper article in "Dagbladet" in 1934, and by the newspaper "Vestfold Arbeiderblad", in 1936.. The term with the opposite meaning, a Norwegian patriot, is Jøssing.

The use of the name as a term for collaborators or traitors in general probably came about upon Quisling's unsuccessful coup d'état in 1940, when he attempted to seize power and make Norway cease resisting the invading Germans. The term was widely introduced to an English-speaking audience by the British newspaper "The Times". It published an editorial on the 19th April 1940 titled "Quislings everywhere", in which it was asserted that "To writers, the word Quisling is a gift from the gods. If they had been ordered to invent a new word for traitor... they could hardly have hit upon a more brilliant combination of letters. Aurally it contrives to suggest something at once slippery and tortuous." The "Daily Mail" picked up the term four days after "The Times" editorial was published. "The War Illustrated" discussed "potential Quislings" among the Dutch during the German invasion of the Netherlands. Subsequently, the BBC brought the word into common use internationally.

Chips Channon described how during the Norway Debate of 7–8 May 1940, he and other Conservative MPs who supported Prime Minister of the United Kingdom Neville Chamberlain called those who voted against a motion of confidence "Quislings". Chamberlain's successor Winston Churchill used the term during an address to the Allied Delegates at St. James's Palace on 21 June 1941, when he said: "A vile race of Quislings—to use a new word which will carry the scorn of mankind down the centuries—is hired to fawn upon the conqueror, to collaborate in his designs and to enforce his rule upon their fellow countrymen while grovelling low themselves." He used the term again in an address to both houses of Congress in the United States of America on 26 December 1941. Commenting upon the effect of a number of Allied victories against Axis forces, and moreover the United States’ decision to enter the war, Churchill opined: "Hope has returned to the hearts of scores of millions of men and women, and with that hope there burns the flame of anger against the brutal, corrupt invader. And still more fiercely burn the fires of hatred and contempt for the filthy Quislings whom he has suborned." The term subsequently entered the language and became a target for political cartoonists.

In the United States it was used often. Some examples include: In the Warner Bros. cartoon "Tom Turk and Daffy" (1944), it was uttered by a Thanksgiving turkey whose presence is betrayed to Porky Pig by Daffy Duck. In the American film "Edge of Darkness" (1943), about the Resistance in Norway, the heroine's brother is often described as a quisling.

The back-formed verb, "to quisle" () existed. This back-formed verb gave rise to a much less common version of the noun: "quisler".

However, H. L. Mencken (generally considered to be a leading authority on the common English usage in the United States) even in 1944 appeared not to be aware of the existence of the verb form, and "to quisle" has entirely disappeared from contemporary usage.

"Quisling" was applied to some Communist figures who participated in the establishment of Communist regimes. As an illustration, the renegade socialist Zdeněk Fierlinger of Czechoslovakia was frequently derided as "Quislinger" for his collaboration with the Communist Party of Czechoslovakia.

"The Patriot Game", one of the best known songs to emerge from the Irish nationalist struggle, includes the line "...those quislings who sold out the Patriot Game" in some versions (although the original uses "cowards" and other versions substitute "rebels" or "traitors").

In a June 2018 New York Times column, Paul Krugman called US President Donald Trump a "quisling", in reference to what Krugman described as Trump's "serv[ing] the interests of foreign masters at his own country’s expense" and "defend[ing] Russia while attacking our closest allies". Other publications also revived the term in the 2010s for use in describing President Trump and his associates and supporters.



</doc>
<doc id="25244" url="https://en.wikipedia.org/wiki?curid=25244" title="Quadrangle">
Quadrangle

Quadrangle or The Quadrangle may refer to :



</doc>
<doc id="25247" url="https://en.wikipedia.org/wiki?curid=25247" title="Quill">
Quill

A quill pen is a writing implement made from a moulted flight feather (preferably a primary wing-feather) of a large bird. Quills were used for writing with ink before the invention of the dip pen, the metal-nibbed pen, the fountain pen, and, eventually, the ballpoint pen. The hand-cut goose quill is rarely used as a calligraphy tool, because many papers are now derived from wood pulp and wear down the quill very quickly. However, it is still the tool of choice for a few scribes who noted that quills provide an unmatched sharp stroke as well as greater flexibility than a steel pen. 

In a carefully prepared quill the slit does not widen through wetting and drying with ink. It will retain its shape adequately and only requires infrequent sharpening and can be used time and time again until there is little left of it. The hollow shaft of the feather (the "calamus") acts as an ink reservoir and ink flows to the tip by capillary action. 

The strongest quills come from the primary flight feathers discarded by birds during their annual moult. Generally the left wing (it is supposed) is favored by the right-handed majority of British writers because the feather curves away from the sight line, over the back of the hand. The quill barrel is cut to six or seven inches in length, so no such consideration of curvature or 'sight-line' is necessary. Additionally, writing with the left-hand in the long era of the quill was discouraged, and quills were never sold as left and right-handed, only by their size and species. 

Goose feathers are most commonly used; scarcer, more expensive swan feathers are used for larger lettering. Depending on availability and strength of the feather, as well as quality and characteristic of the line wanted by the writer, other feathers used for quill-pen making include feathers from the crow, eagle, owl, hawk, and turkey. On a true quill the barbs are always stripped off completely on the trailing edge. (The pinion for example only has significant barbs on one side of the barrel.) Later a fashion developed for stripping partially and leaving a decorative top of a few barbs. The fancy, fully plumed quill is mostly a Hollywood invention and has little basis in reality. Most, if not all, manuscript illustrations of scribes show a quill devoid of decorative barbs, or at least mostly stripped.

Quill pens were used to write the vast majority of medieval manuscripts, the Magna Carta and the Declaration of Independence. 
Quill pens are still used today mainly by professional scribes and calligraphers.

Quills are also used as the plectrum material in string instruments, particularly the harpsichord.

Quills were the primary writing instrument in the western world from the 6th to the 19th century. The best quills were usually made from goose, swan, and later turkey feathers. Quills went into decline after the invention of the metal pen, mass production beginning in Great Britain as early as 1822 by John Mitchell of Birmingham.

Quill pens were the instrument of choice during the medieval era due to their compatibility with parchment and vellum. Before this the reed pen had been used, but a finer letter was achieved on animal skin using a cured quill. Other than written text, they were often used to create figures, decorations, and images on manuscripts, although many illuminators and painters preferred fine brushes for their work. The variety of different strokes in formal hands was accomplished by good penmanship as the tip was square cut and rigid, exactly as it is today with modern steel pens.

It was much later, in the 1600s, with the increased popularity of writing, especially in the copperplate script promoted by the many printed manuals available from the 'Writing Masters', that quills became more pointed and flexible.

According to the Supreme Court Historical Society, 20 goose-quill pens, neatly crossed, are placed at the four counsel tables each day the U.S. Supreme Court is in session; "most lawyers appear before the Court only once, and gladly take the quills home as souvenirs." This has been done since the earliest sessions of the Court.

Quills are denominated from the order in which they are fixed in the wing; the first is favoured by the expert calligrapher, the second and third quills being very satisfactory also, plus the pinion feather. Flags the 5th and 6th feathers are also used. No other feather on the wing would be considered suitable by a professional scribe.

Information can be obtained on the techniques of curing and cutting quills

In order to harden a quill that is soft, thrust the barrel into hot ashes, stirring it till it is soft; then taking it out, press it almost flat upon your knees with the back of a penknife, and afterwards reduce it to a roundness with your fingers. If you have a number to harden, set water and alum over the fire; and while it is boiling put in a handful of quills, the barrels only, for a minute, and then lay them by.

An accurate account of the Victorian process by William Bishop, from researches with one of the last London quill dressers, is recorded in the "Calligrapher's Handbook" cited on this page.

A quill knife was the original primary tool used for cutting and sharpening quills, known as "dressing".

Following the decline of the quill in the 1820s, after the introduction of the maintenance-free, mass-produced steel dip nib by John Mitchell, knives were still manufactured but became known as desk knives, stationery knives or latterly as the name stuck "pen" knives. 

A "pen" knife by contrast has two flat sides. This distinction is not recognised by modern traders, dealers or collectors who define a quill knife as any small knife with a fixed or hinged blade, including such items as ornamental fruit knives.

Plectra for psalteries and lutes can be cut similarly to writing pens. The rachis, the portion of the stem between the barbs, not the calamus, of the primary flight feathers of birds of the crow family was preferred for harpsichords. In modern instruments, plastic is more common, but they are often still called "quills". The lesiba uses a quill attached to a string to produce sound.




</doc>
<doc id="25248" url="https://en.wikipedia.org/wiki?curid=25248" title="Quo vadis?">
Quo vadis?

Quō vādis? (, ) is a Latin phrase meaning "Where are you going?"

It also may refer to a Christian tradition regarding Saint Peter. According to the apocryphal Acts of Peter (Vercelli Acts XXXV), Peter flees from crucifixion in Rome at the hands of the government, and along the road outside the city, he meets the risen Jesus. In the Latin translation, Peter asks Jesus, ""Quō vādis?"" He replies, ""Rōmam eō iterum crucifīgī" ("I am going to Rome to be crucified again"). Peter then gains the courage to continue his ministry and returns to the city, where he is martyred by being crucified upside-down. The Church of Domine Quo Vadis in Rome is built where the meeting between Peter and Jesus allegedly took place.
The words "quo vadis" as a question also occur five times in the Latin Vulgate: in Genesis 16:8, Genesis 32:17, Judges 19:17, John 13:36, and John 16:5.

The Polish writer Henryk Sienkiewicz authored the novel "Quo Vadis: A Narrative of the Time of Nero" (1895), which in turn has been made into motion pictures several times, including a 1951 version that was nominated for eight Academy Awards. For this and other novels, Sienkiewicz received the 1905 Nobel Prize for Literature.

In the Academy Award winning film "Alice Doesn't Live Here Anymore", Ellen Burstyn's character says the phrase before a job interview at "Club Manhattan".

In a season four episode of "M*A*S*H" entitled "Quo Vadis, Captain Chandler?" the reference pertains to Jesus Christ. A shellshocked officer arrives at the Hospital believing he is the Christ. He has numerous conversations with the cast, including Father Mulcahy. He ultimately leaves the MASH unit for an evacuation hospital, still unrecovered and leaving the audience wanting for resolution. Thus the title, "where are you going, Captain Chandler?"

In the film "Rescue Dawn", Eugene has "Quo Vadis" written on the back of his jumpsuit.

The 8th Season of the American television medical drama ER featured an episode titled 'Quo Vadis?'.



</doc>
<doc id="25249" url="https://en.wikipedia.org/wiki?curid=25249" title="QED">
QED

QED may refer to:







</doc>
<doc id="25254" url="https://en.wikipedia.org/wiki?curid=25254" title="Qantas">
Qantas

<nowiki> </nowiki>Qantas Airways (; ) is the flag carrier of Australia and its largest airline by fleet size, international flights and international destinations. It is the third oldest airline in the world, after KLM and Avianca having been founded in November 1920; it began international passenger flights in May 1935. The Qantas name comes from ""QANTAS"", an acronym for its original name, ""Queensland and Northern Territory Aerial Services"", and it is nicknamed "The Flying Kangaroo". Qantas is a founding member of the Oneworld airline alliance.

The airline is based in the Sydney suburb of Mascot with its main hub at Sydney Airport. , Qantas had a 65% share of the Australian domestic market and carried 14.9% of all passengers travelling in and out of Australia. Various subsidiary airlines operate to regional centres and on some trunk routes within Australia under the QantasLink banner. Its subsidiary Jetconnect provides services between Australia and New Zealand, flying under the Qantas brand. Qantas also owns Jetstar Airways, a low-cost airline that operates both international services from Australia and domestic services within Australia and New Zealand; and holds stakes in a number of other Jetstar-branded airlines.

Qantas was founded in Winton, Queensland on 16 November 1920 by Hudson Fysh, Paul McGinness and Fergus McMaster as Queensland and Northern Territory Aerial Services Limited. The airline's first aircraft was an Avro 504K. In 1920 Queensland and Northern Territory Aerial Services Ltd had its headquarters in Winton before moving to Longreach, Queensland in 1921 and Brisbane, Queensland in 1930.

In 1934, QANTAS and Britain's Imperial Airways (a forerunner of British Airways) formed a new company, Qantas Empire Airways Limited (QEA). The new airline commenced operations in December 1934, flying between Brisbane and Darwin. QEA flew internationally from May 1935, when the service from Darwin was extended to Singapore (Imperial Airways operated the rest of the service through to London). After World War II began, enemy action and accidents destroyed half of the fleet of ten, when most of the fleet was taken over by the Australian government for war service.
Flying boat services were resumed in 1943, with flights between the Swan River at Crawley in Perth, Western Australia and Koggala lake in Ceylon (now Sri Lanka). This linked up with the British Overseas Airways Corporation (BOAC, the successor airline to Imperial Airways) service to London. Qantas' kangaroo logo was first used on the "Kangaroo Route", begun in 1944, from Sydney to Karachi, where BOAC crews took over for the rest of the journey to the UK.

In 1947, QEA was nationalised by the Australian government led by Labor Prime Minister Ben Chifley. QANTAS Limited was then wound up. After nationalisation, Qantas' remaining domestic network, in Queensland, was transferred to the also nationally owned Trans Australia Airlines, leaving Qantas with a purely international network. Shortly after nationalisation, QEA began its first services outside the British Empire – to Tokyo. Services to Hong Kong began around the same time. In 1957 a head office, Qantas House, opened in Sydney. In June 1959 Qantas entered the jet age when the first Boeing 707-138 was delivered.

On , Qantas merged with nationally owned domestic airline, Australian Airlines (renamed from Trans Australia Airlines in 1986). The airline started to be rebranded to Qantas in the following year. Qantas was gradually privatised between 1993 and 1997. Under the legislation passed to allow the privatisation, Qantas must be at least 51% owned by Australian shareholders.

In 1998, Qantas co-founded the oneworld alliance with American Airlines, British Airways, Canadian Airlines, and Cathay Pacific, with other airlines joining subsequently.

The main domestic competitor to Qantas, Ansett Australia, collapsed on 14 September 2001. Market share for Qantas immediately neared 90%, but with the entry of new budget airline Virgin Blue into the domestic market, Qantas' market share fell. Qantas created the budget Jetstar Airways in 2001 to compete, and the market share of the Qantas Group settled at a relatively stable position of about 65%, with 30% for Virgin Blue and other regional airlines accounting for the rest of the market.

Qantas briefly revived the Australian Airlines name for a short-lived international budget airline between 2002 and 2006, but this subsidiary was shut down in favour of expanding Jetstar internationally, including to New Zealand. In 2004, the Qantas group expanded into the Asian budget airline market with Jetstar Asia Airways, in which Qantas owns a minority stake. A similar model was used for the investment into Jetstar Pacific, headquartered in Vietnam, in 2007, and Jetstar Japan, launched in 2012.

In December 2006, Qantas was the subject of a failed bid from a consortium calling itself Airline Partners Australia. Merger talks with British Airways in 2008 also did not proceed to an agreement. In 2011, an industrial relations dispute between Qantas and the Transport Workers Union of Australia resulted in the grounding of all Qantas aircraft and lock-out of the airline's staff for two days.

On 24 March 2018, a Boeing 787 Dreamliner became the first scheduled non-stop commercial flight between Australia and Europe. Flight QF9, was a 17-hour, 14,498km (9,009-mile) journey from Perth to London Heathrow.

The key trends for the Qantas Group (Qantas Airways Ltd and Controlled Entities, which includes Jetstar and Qantas Cargo), are shown below (as at year ending 30 June):

Qantas' headquarters are located at the Qantas Centre in the Bayside Council suburb of Mascot, Sydney, New South Wales.

Qantas has operated a number of passenger airline subsidiaries since inception, including:

Qantas operates a freight service under the name Qantas Freight and also wholly owns the logistics and air freight company Australian air Express and leases cargo aircraft from Atlas Air.

Qantas, through its Aboriginal and Torres Strait Islander Programme, has some links with the Aboriginal Australian community. As of 2007, the company has run the programme for more than ten years and 1–2% of its staff are Aboriginal and Torres Strait Islander. Qantas employs a full-time Diversity Coordinator, who is responsible for the programme.

Qantas has also bought and donated some Aboriginal art. In 1993, the airline bought a painting — "Honey Ant and Grasshopper Dreaming" — from the Central Australian desert region. As of 2007, this painting is on permanent loan to Yiribana at the Art Gallery of New South Wales. In 1996, Qantas donated five extra bark paintings to the gallery. Qantas has also sponsored and supported Aboriginal artists in the past.

An early television campaign, starting in 1969 and running for several decades, was aimed at American audiences; it featured a live koala, voiced by Howard Morris, who complained that too many tourists were coming to Australia and concluded "I hate Qantas." The koala ads have been ranked among the greatest commercials of all time. A long-running advertising campaign features renditions by children's choirs of Peter Allen's "I Still Call Australia Home", at various famous landmarks in Australia and foreign locations such as Venice.

Qantas is the main sponsor of the Qantas Wallabies, the Australian national Rugby Union team. It also sponsors the Socceroos, Australia's national association football team. Qantas is the main sponsor for the Formula One Australian Grand Prix. On 26 December 2011, Qantas signed a four-year deal with Australian cricket's governing body Cricket Australia, to be the official carrier of the Australia national cricket team. However, Qantas is very disappointed due to recent ball tampering by Australian national cricket team. According to Alan Joyce, the head of airline, Qantas is in discussion with Cricket Australia as the issue unfolds. Qantas want the authorities to complete the inquiry urgently and take appropriate actions.

Qantas management has expressed strong support for Marriage Equality and LGBTIQ issues, with CEO Alan Joyce said to be, "arguably the most prominent corporate voice in the marriage equality campaign." As official airline partner for the Sydney Mardi Gras, Qantas decorated one of its aircraft with rainbow wording and positioned a rainbow flag next to the tail’s flying kangaroo. Qantas also served pride cookies to its passengers. It had a rainbow roo float in the Mardi Gras parade. There has been criticism of Qantas using its corporate power to prosecute the private interests on their staff and the community. Peter Dutton has said that chief executives such as Alan Joyce at Qantas should "stick to their knitting" rather than using the company's brand to advocate for political causes. A senior church leader has made similar comments. Despite the criticism, Qantas will continue to advocate for marriage equality which will include offering customers specially commissioned rings with the phrase, "until we all belong". This phrase will also appear on, Qantas boarding passes and other paraphernalia. The cost of the campaign by Qantas and other participating companies is expected to be more than $5 million.

The "Qantas Sale Act", under which the airline was privatised, limits foreign ownership of Qantas to 49 percent. Foreign airlines are subject to further restrictions under the act, which stipulates a 35-percent limit for all foreign airline shareholdings combined. In addition, a single foreign entity can hold no more than 25 percent of the airline's shares. This act was amended in 2014 to repeal parts of paragraph 7.

In August 2011 the company announced that, due to financial losses and a decline in market share, major structural changes would be made. Up to 1,000 jobs would be lost in Australia, and a new Asia-based premium airline would be set up, operating under a different name. It would also launch a budget airline, called Jetstar Japan, in partnership with Japan Airlines and Mitsubishi Corporation. The change became necessary because of losses in the airline's international operations, due to airlines such as Emirates and Singapore Airlines becoming more competitive and because of the deregulation of Australian international routes during the mid-to-late 1980s. Included in the changes was the cessation of services to London via Hong Kong and Bangkok; Qantas still operated to these cities, but with onward flights to London via its Oneworld partner British Airways under a code-share service.

Qantas is attempting to turn around its international operations, which lost about A$200 million ($209 million) for the year ending June 2011. Therefore, on 26 March 2012, Qantas announced it would set up Jetstar Hong Kong with China Eastern Airlines Corporation, which was intended to begin flights in 2013, but became embroiled in a protracted approval process.

Due to high fuel prices, intense competition and industrial disputes, Qantas reported a A$245 million full-year loss to the end of June 2012, which was its first loss since Qantas was fully privatised 17 years previously, in 1995, and led to the airline cancelling its order of 35 new Boeing 787 Dreamliner aircraft, to reduce its spending. In focusing on core business, Qantas also divested itself of its 50% holding of StarTrack, Australia's largest road freight company, in part for acquiring full interest in Australian Air Express.

Qantas and Emirates began their alliance on 31 March 2013, in which their combined carriers offered 98 flights per week to Dubai, that saw bookings up six-fold. To accommodate Muslim sensitivities, the airline stopped serving pork on flights bound to/from Europe, which provoked a backlash on social media. In September that year, following the announcement the carrier expected an A$250 million ( million) net loss for the half-year period that ended on 31 December and the implementation of cost-cutting measures that would see the cut of 1,000 jobs within a year, S&P downgraded Qantas credit from BBB- (the lowest investment grade) to BB+, which may imply a rise in borrowing costs and a limitation in the investment potential. Moody's applied a similar downgrading a month later.

The Qantas Group reported a loss of A$235 million ( million) for the first half of FY 2014. Cost-cutting measures to save A$2 billion, including the loss of 5,000 jobs that will see the workforce lowered from 32,000 to 27,000 by 2017, were announced in . In May 2014 the company said it would have shed 2,200 jobs by June 2014, including those of 100 pilots. The carrier also reduced the size of its fleet by retiring aircraft and deferring deliveries; and planned to sell some of its assets. With 2,200 employees laid off by June 2014, another 1,800 job positions were expected to be cut by June 2015.

In 2015, Qantas sold its lease of Terminal 3 at Sydney Airport, which was due to continue until 2019, back to Sydney Airport Corporation for $535 million. This means Sydney Airport resumes operational responsibility of the terminal, including the lucrative retail areas.

Paris-based Australian designer Martin Grant is responsible for the new Qantas airline staff uniforms that were publicly unveiled on 16 April 2013. These were to replace the previous uniforms, dubbed colloquially as "Morrisey" by staff after the designer, Peter Morrissey. Qantas ambassador and model Miranda Kerr assisted with the launch of the new outfits for which the colours of navy blue, red and fuchsia pink are combined. Qantas chief executive Alan Joyce stated that the new design "speaks of Australian style on the global stage" at the launch event that involved Qantas employees modelling the uniforms. Grant consulted with Qantas staff members over the course of one year to finalise the 35 styles that were eventually created. Not all employees were happy with the new uniform, however, with one flight attendant being quoted as saying "The uniforms are really tight and they are simply not practical for the very physical job we have to do."

Qantas flies to 20 domestic destinations and 21 international destinations in 14 countries across Africa, the Americas, Asia, Europe and Oceania excluding the destinations served by its subsidiaries. The entire Qantas group serves 65 domestic and 31 international destinations.

Qantas operates flightseeing charters to Antarctica on behalf of Croydon Travel. It first flew Antarctic flightseeing trips in 1977. They were suspended for a number of years due to the crash of Air New Zealand Flight 901 on Mount Erebus in 1979. Qantas restarted the flights in 1994. Although these flights do not touch down, they require specific polar operations and crew training due to factors like sector whiteout, which contributed to the 1979 Air New Zealand disaster.

With non-stop service between Sydney and Dallas/Fort Worth aboard the Airbus A380 starting on 29 September 2014, Qantas operated the world's longest passenger flight on the world's largest passenger aircraft. This was overtaken by Emirates' Auckland-Dubai flight, which started on 1 March 2016. After the Boeing 787 aircraft are delivered, Qantas announced an intention to launch non-stop flights between Australia and the United Kingdom during March 2018 from Perth, Western Australia to London. The flight was launched on the 24th of March 2018.

Qantas has codeshare agreements with the following airlines:
As of April 2018, the Qantas mainline fleet consists of the following aircraft:

, Qantas and its subsidiaries operate 285 aircraft, which includes 71 aircraft by Jetstar Airways, 82 by the various QantasLink-branded airlines, 8 by Jetconnect and 5 by Express Freighters Australia (on behalf of Qantas Freight, which also wet leases three Atlas Air Boeing 747-400Fs).

On 22 August 2012, Qantas announced that, due to losses and to conserve capital, it had cancelled its 35-aircraft Boeing 787–9 order while keeping the 15-aircraft 787-8 order for Jetstar Airways and moving forward 50 purchase rights. On 20 August 2015 Qantas announced that it had ordered eight Boeing 787-9s for delivery from 2017.

On 27 October 2016, Qantas revealed revamped 787 interiors as well as a new livery for the airline, featuring a new typeface and streamlined logo.

Qantas has named its aircraft since 1926. Themes have included Greek gods, stars, people in Australian aviation history, and Australian birds. Since 1959, the majority of Qantas aircraft have been named after Australian cities. The Airbus A380 series, the flagship of the airline, is named after Australian aviation pioneers, with the first A380 named "Nancy-Bird Walton".

One Qantas Boeing 737-800 is decorated with an Indigenous Australian art scheme. The livery, called "Mendoowoorrji", was revealed in November 2013. The design was drawn from the late West Australian Aboriginal artist Paddy Bedford.

A previous scheme on another 737-800, titled "Yananyi Dreaming," featured a depiction of Uluru. The scheme was designed by Uluru-based artist Rene Kulitja, in collaboration with Balarinji Studio in Adelaide. It was painted on the 737 at the Boeing factory prior to its delivery in 2002. It was repainted into the standard livery in 2014.

Two other Australian Aboriginal art designs have been displayed on Qantas aircraft. Two Boeing 747s (a −400 and later a −400ER) were adorned in a paint scheme called "Wunala Dreaming". "Wunala Dreaming" was the first Australian Aboriginal art scheme and was unveiled in 1994. The "motif" was an overall-red design depicting ancestral spirits in the form of kangaroos travelling in the outback. The second design was called "Nalanji Dreaming" and was painted on one of the airline's now-retired Boeing 747-300s in 1995. "Nalanji Dreaming" was a bright blue design inspired by rainforest landscape and tropical seas.

In November 2014 the airline revealed that the 75th Boeing 737–838 jet to be delivered would carry a 'retro-livery' based on the airline's 1970s' colour scheme design featuring the iconic 'Flying Kangaroo' on its tail and other aspects drawn from its 1970s fleet. The aircraft was delivered on 17 November. On 16 November 2015 the airline unveiled a second Boeing 737–838 in a different 'retro-livery' from 1959 to celebrate the airline's 95th birthday.

Several Qantas aircraft have been decorated with promotional liveries, promoting telecommunications company Optus; the Disney motion picture "Planes"; the Australian national association football team, the Socceroos; and the Australian national rugby union team, the Wallabies. Two aircraft – an Airbus A330-200 and a Boeing 747–400 – were decorated with special liveries promoting the Oneworld airline alliance (of which Qantas is a member) in 2009. On 29 September 2014, nonstop Airbus A380 service to Dallas/Fort Worth International Airport was inaugurated using an A380 decorated with a commemorative cowboy hat and bandana on the kangaroo tail logo. Prior to the 2017 Sydney Mardi Gras, Qantas decorated one of its Airbus A330-300 aircraft with rainbow lettering and depicted a rainbow flag on the tail of the aircraft.

Qantas has several in-flight entertainment systems installed on its aircraft. Across the fleet, the in-flight experience is referred to as "On:Q". Every Qantas mainline aircraft has some form of video audio entertainment. "iQ" is featured in all classes of the Airbus A380, refurbished 747s, A330-300s, and refurbished Airbus A330-200s. Additionally, it has been implemented on new Boeing 737-800s and refurbished Boeing 747s. This audio video on demand (AVOD) experience is based on the Panasonic Avionics system and features expanded entertainment options; touch screens; and new communications-related features such as Wi-Fi and mobile phone functionality; as well as increased support for electronics (such as USB and iPod connectivity).

The "Total Entertainment System" by Rockwell Collins is featured on the one unrefurbished Boeing 747 and two unrefurbished Airbus A330-200 aircraft. This AVOD system includes personal LCD screens in all classes, located in the seat back for economy and business class, and in the armrest for premium economy and first class.

The Mainscreen System, where video screens are the only available form of video entertainment; movies are shown on the screens for lengthier flights, or TV programmes on shorter flights. A news telecast will usually feature at the start of the flight. Audio options are less varied than on iQ or the Total Entertainment System. The Mainscreen System is installed on all domestic configured Boeing 737-800s delivered before 2011.

Since 2014, Sky News Australia has provided multiple news bulletins both in-flight and in Qantas branded lounges. Previously, the Australian Nine Network provided a news bulletin for Qantas entitled "Nine's Qantas Inflight News", which was the same broadcast as Nine's "Early Morning News", however Nine lost the contract to Sky News.

Q Streaming is an in-flight entertainment system in which entertainment is streamed to iPads, available in all classes. A selection of movies, TV, music and a kids' choice are available.

"Qantas The Australian Way" is the airline's in-flight magazine. In mid-2015, the magazine ended a 14-year publishing deal with Bauer Media, switching its publisher to Medium Rare.

Boeing's cancellation of the Connexion by Boeing system caused concerns that in-flight internet would not be available on next-generation aircraft such as Qantas' fleet of Airbus A380s. However, Qantas announced in July 2007 that all service classes in its fleet of A380s would have wireless internet access, as well as seat-back access to e-mail and cached web browsing. Certain elements would also be retrofitted into existing Boeing 747-400s. The in-flight entertainment system indicates that Internet access is provided by OnAir.

In April 2007, Qantas announced a trial for use of mobile telephones with AeroMobile, during domestic services for three months on a Boeing 767. During the trial, passengers were allowed to send and receive text messages and emails, but were not able to make or receive calls.

Qantas moved from an in-house Passenger Service System known as QUBE (Qantas Universal Business Environment) to an outsourced solution provided by Amadeus in late 2000. In September 2007, Qantas announced a ten-year extension of the outsourcing agreement.

In July 2015, Qantas signed a deal with American cable network HBO to provide over 120 hours of television programming in-flight from the network which will be updated monthly, as well as original lifestyle and entertainment programming from both Foxtel and the National Geographic Channel.

First class is offered exclusively on all 12 Airbus A380s and one Boeing 747-400.

It offers 14 individual suites in a 1-1-1 layout. The seats rotate, facing forward for takeoff, but rotating to the side for dining and sleeping, with 83.5 in seat pitch (extending to a 212 cm fully flat bed) and a width of . Each suite has a widescreen HD monitor with 1,000 AVOD programs. In addition to 110 V AC power outlets, USB ports are offered for connectivity. Passengers are also able to make use of the on-board business lounge on the upper deck. Complimentary access to either the first class or business class lounges (or affiliated lounges) is offered.

On the Boeing 747–400, there are 14 flat-bed seats, located on the main deck. The seats are slightly shorter than on the A380, due to their position near the nose of the aircraft: versus .

Business class is offered on all Qantas mainline passenger aircraft.

International Business Class is available on the Boeing 747, International Airbus A330-200s, the A330-300 and the Airbus A380. On the Boeing 747, seating is in a 2-3-2 configuration on the main deck and a 2–2 configuration on the upper deck. The A330 features a 2-2-2 configuration. Two versions of what Qantas calls its "Skybed", the lie-flat business-class seat, are available. Older versions of the lie-flat Skybeds featured of seat pitch and width; however passengers slept at a distinct slope to the cabin floor. Later versions of the Skybed have an pitch, and lie fully horizontal. By the end of 2016, the business class of its entire fleet of Airbus A330 aircraft was fitted with lie flat seats designed by Mark Newson.

The Boeing 747s and Airbus A330s feature a touchscreen monitor with 400 AVOD programs. Qantas' new international business class product is featured on the Airbus A380. It features 64 fully flat Skybed seats with seat pitch (converting to a 200 cm long bed). These seats are located on the upper-deck in a 2-2-2 configuration in two separate cabins. Features include a 30 cm touchscreen monitor with 1,000 AVOD programmes and an on-board lounge. The latest business class product introduced on the A330 in October 2014 features a fully flat 198 cm bed in a 1-2-1 configuration making each passenger having direct aisle access. This seat can be reclined during take off and landing while sporting the latest Panasonic eX3 system with a touchscreen. 
The A330 would fly to its Asian and transcontinental routes across Australia while serving smaller routes such as the East Coast triangle. The 747 would fly to its Asian and African routes and serves North and South America. The A380 would be seen on its flagship routes such as London via Dubai, Los Angeles, Dallas and a seasonal route to Hong Kong.

Complimentary access to the Qantas business class lounge (or affiliated lounges) is also offered.

Premium economy class is only available on Airbus A380, Boeing 787-9 and all Boeing 747–400 aircraft. It has a seat pitch of on the Boeing 747 and it ranges from on the Airbus A380, with a width of . On the Boeing 747, it is configured in a 2-4-2 seating arrangement around the middle of the main deck, whilst it is in a 2-3-2 at the rear of the upper deck on the A380. All A380s have 35 seats.

Qantas premium economy is presented as a lighter business class product rather than most other airlines' premium economy, which is often presented as a higher economy class, however Qantas premium economy does not offer access to premium lounges, and meals are only a slightly uprated version of economy class meals.

Economy class is available on all Qantas mainline passenger aircraft.

Seat pitch is usually and seat width ranges from . Layouts are 3–3 on the 737, 2-4-2 on the A330 and 3-4-3 on the 747. On the A380, the layout is 3-4-3 and there are four self-service snack bars located in between cabins.

Qantas has smartphone application programs ("apps") for Android, iOS and Windows Phone platforms. The iOS apps are separated into two – one (named Qantas Points) for members of its Qantas Frequent Flyer programme to manage their points, while the other (named Qantas) provides mobile check-in and boarding passes to link with Passbook, live flight updates and information on airport lounges, fare sales/alerts, and the ability to book flights and hotels. An Android app was launched on 7 August 2013.

The Qantas frequent flyer programme is aimed at rewarding customer loyalty. Points are accrued based on distance flown, with bonuses that vary by travel class. Points can also be earned on other Oneworld airlines as well as through other non-airline partners. Points can be redeemed for flights or upgrades on flights operated by Qantas, Oneworld airlines, and other partners. Other partners include credit cards, car rental companies, hotels and many others. To join the programme, passengers living in Australia or New Zealand pay a one-off joining fee, and then become a Bronze Frequent Flyer (residents of other countries may join without a fee). All accounts remain active as long as there is points activity once every eighteen months. Flights with Qantas and selected partner airlines earn Status Credits — and accumulation of these allows progression to Silver status (Oneworld Ruby), Gold status (Oneworld Sapphire), Platinum and Platinum One status (Oneworld Emerald).

Qantas has faced criticism regarding availability of seats for members redeeming points. In 2004, the Australian Competition and Consumer Commission directed Qantas to provide greater disclosure to members regarding the availability of frequent-flyer seats.

In March 2008, an analyst at JPMorgan Chase suggested that the Qantas frequent-flyer program could be worth A$2 billion (US$1.9 billion), representing more than a quarter of the total market value of Qantas.

On 1 July 2008 a major overhaul of the programme was announced. The two key new features of the programme were Any Seat rewards, in which members could now redeem any seat on an aircraft, rather than just selected seats — at a price. The second new feature was Points Plus Pay, which has enabled members to use a combination of cash and points to redeem an award. Additionally, the Frequent Flyer store was also expanded to include a greater range of products and services.
Announcing the revamp, Qantas confirmed it would be seeking to raise about A$1 billion in 2008 by selling up to 40% of the frequent flyer program. However, in September 2008, it stated it would defer the float, citing volatile market conditions.

The Qantas Club is the airline lounge for Qantas with airport locations around Australia and the world. Additionally, Qantas operates dedicated international first-class lounges in Sydney, Melbourne, Auckland and Los Angeles. Domestically, Qantas also offers dedicated Business Lounges at Sydney, Melbourne, Brisbane, Canberra and Perth for domestic Business Class, Qantas Platinum and Platinum One, and OneWorld Emerald frequent flyers.

In April 2013, Qantas opened its new flagship lounge in Singapore, the Qantas Singapore Lounge. This replaced the former separate first- and business-class lounges as a result of the new Emirates alliance. Similar combined lounges were also opened in Hong Kong in April 2014 and in Brisbane in October 2016. These new lounges provide the same service currently offered by Sofitel in its flagship First lounges in Sydney and Melbourne and a dining experience featuring Neil Perry's Spice Temple inspired dishes and signature cocktails.

Qantas Club Members, Gold Frequent Flyers and Oneworld Sapphire holders are permitted to enter domestic Qantas Clubs when flying on Qantas or Jetstar flights along with one guest who need not be travelling. Platinum and Oneworld Emerald Members are permitted to bring in two guests who do not need to be travelling. Internationally, members use Qantas International Business Class lounges (or the Oneworld equivalent). Guests of the member must be travelling to gain access to international lounges. When flying with American Airlines, members have access to Admirals Club lounges and when flying on British Airways, members have access to British Airways' Terraces and Galleries Lounges.

Platinum Frequent Flyers had previously been able to access the Qantas Club in Australian domestic terminals at any time, regardless of whether they were flying that day. Travellers holding Oneworld Sapphire or Emerald status are also allowed in Qantas Club lounges worldwide.

Access to Qantas First lounges is open to passengers travelling on internationally operated Qantas or Oneworld first-class flights, as well as Qantas platinum and Oneworld emerald frequent flyers. Emirates first-class passengers are also eligible for access to the Qantas first lounges in Sydney and Melbourne.

The Qantas Club also offers membership by paid subscription (one, two, or four years) or by achievement of Gold or Platinum frequent flyer status. Benefits of membership include lounge access, priority check-in, priority luggage handling and increased luggage allowances.

It is often claimed, most notably in the 1988 movie "Rain Man", that Qantas has never had an aircraft crash. While it is true that the company has neither lost a jet airliner nor had any jet fatalities, it had eight fatal accidents and an aircraft shot down between 1927 and 1945, with the loss of 63 people. Half of these accidents and the shoot-down occurred during World War II, when the Qantas aircraft were operating on behalf of Allied military forces. Post-war, it lost another four aircraft (one was owned by BOAC and operated by Qantas in a pooling arrangement) with a total of 21 people killed. The last fatal accidents suffered by Qantas were in 1951, with three fatal crashes in five months. Qantas' safety record in the jet airliner era was cited as a reason for it being named as the world's safest airline in 2014 and 2015.

Since the end of World War II, the following accidents and incidents have occurred:

On 26 May 1971 Qantas received a call from a "Mr. Brown" claiming that there was a bomb planted on a Hong Kong-bound jet and demanding $500,000 in unmarked $20 notes. The caller and threat were taken seriously when he directed police to an airport locker where a functional bomb was found. Arrangements were made to pick up the money in front of the head office of the airline in the heart of the Sydney business district. Qantas paid the money and it was collected, after which Mr. Brown called again, advising the "bomb on the plane" story was a hoax. The initial pursuit of the perpetrator was bungled by the New South Wales Police Force which, despite having been advised of the matter from the time of the first call, failed to establish adequate surveillance of the pick-up of the money. Directed not to use their radios (for fear of being "overheard"), the police were unable to communicate adequately. Tipped off by a still-unidentified informer, the police arrested an Englishman, Peter Macari, finding more than $138,000 hidden in an Annandale property. Convicted and sentenced to 15 years in prison, Macari served nine years before being deported to Britain. More than $224,000 remains unaccounted for. The 1990 telemovie "Call Me Mr. Brown", directed by Scott Hicks and produced by Terry Jennings, relates to this incident. On 4 July 1997 a copycat extortion attempt was thwarted by police and Qantas security staff.

In November 2005 it was revealed that Qantas had a policy of not seating adult male passengers next to unaccompanied children. This led to accusations of discrimination. The policy came to light following an incident in 2004 when Mark Wolsay, who was seated next to a young boy on a Qantas flight in New Zealand, was asked to change seats with a female passenger. A steward informed him that "it was the airline's policy that only women were allowed to sit next to unaccompanied children". Cameron Murphy of the NSW Council for Civil Liberties president criticised the policy and stated that "there was no basis for the ban". He said it was wrong to assume that all adult males posed a danger to children. The policy has also been criticised for failing to take female abusers into consideration.

In 2010, when British Airways was successfully sued to change its child seating policy, Qantas argued again that banning men from sitting next to unaccompanied children "reflected parents' concerns". In August 2012, the controversy resurfaced when a male passenger had to swap seats with a female passenger after the crew noticed he was sitting next to an unrelated girl travelling alone. The man felt discriminated against and humiliated before the other passengers as a possible paedophile. A Qantas spokesman defended the policy as consistent with that of other airlines in Australia and around the globe.

In 2006 a class action lawsuit, alleging price-fixing on air cargo freight, was commenced in Australia. The lawsuit was settled early in 2011 with Qantas agreeing to pay in excess of $21 million to settle the case.

Qantas has pleaded guilty to participating in a cartel that fixed the price of air cargo. Qantas Airways Ltd. was fined CAD$155,000 after it admitted that its freight division fixed surcharges on cargo exported on certain routes from Canada between May 2002 and February 2006.
In July 2007, Qantas pleaded guilty in the United States to price fixing and was fined a total of $61 million through the Department of Justice investigation. The executive in charge was jailed for six months. Other Qantas executives were granted immunity after the airline agreed to co-operate with authorities.
In 2008 the Australian Competition and Consumer Commission fined the airline $20 million for breaches of the acts associated with protecting consumers. In November 2010 Qantas was fined 8.8 million euros for its part in an air cargo cartel involving up to 11 other airlines. Qantas was fined NZ$6.5 million in April 2011 when it pleaded guilty in the New Zealand High Court to the cartel operation.

In response to ongoing industrial unrest over failed negotiations involving three unions (the Australian Licensed Aircraft Engineers Association (ALAEA), the Australian and International Pilots Association (AIPA) and the Transport Workers Union of Australia (TWU)), the company grounded its entire domestic and international fleet from 5 pm AEDT on 29 October. Employees involved would be locked out from 8 p.m. AEDT on 31 October. It was reported that the grounding would have a daily financial impact of A$20 million. In the early hours of 31 October, Fair Work Australia ordered that all industrial action taken by Qantas and the involved trade unions be terminated immediately. The order was requested by the federal government amid fears that an extended period of grounding would do significant damage to the national economy, especially the tourism and mining sectors. The grounding affected an estimated 68,000 customers worldwide.





</doc>
<doc id="25256" url="https://en.wikipedia.org/wiki?curid=25256" title="QED (text editor)">
QED (text editor)

QED is a line-oriented computer text editor that was developed by Butler Lampson and L. Peter Deutsch for the Berkeley Timesharing System running on the SDS 940. It was implemented by L. Peter Deutsch and Dana Angluin between 1965 and 1966.

QED (for "quick editor") addressed teleprinter usage, but systems "for CRT displays [were] not considered, since many of their design considerations [were] quite different."
Ken Thompson later wrote a version for CTSS; this version was notable for introducing regular expressions. Thompson rewrote QED in BCPL for Multics. The Multics version was ported to the GE-600 system used at Bell Labs in the late 1960s under GECOS and later GCOS after Honeywell took over GE's computer business. The GECOS-GCOS port used I/O routines written by A. W. Winklehoff. Dennis Ritchie, Ken Thompson and Brian Kernighan wrote the QED manuals used at Bell Labs.
Given that the authors were the primary developers of the Unix operating system, it is natural that QED had a strong influence on the classic UNIX text editors ed, sed and their descendants such as ex and sam, and more distantly AWK and Perl.

A version of QED named FRED (Friendly Editor) was written at the
University of Waterloo for Honeywell systems by Peter Fraser. A University of Toronto team consisting of Tom Duff, Rob Pike, Hugh Redelmeier, and David Tilbrook implemented a version of QED that runs on UNIX; David Tilbrook later included QED as part of his QEF tool set.

QED was also used as a character-oriented editor on the Norwegian-made Norsk Data systems, first Nord TSS, then Sintran III. It was implemented for the Nord-1 computer in 1971 by Bo Lewendal who after working with Deutsch and Lampson at Project Genie and at the Berkeley Computer Corporation, had taken a job with Norsk Data (and who developed the Nord TSS later in 1971). 




</doc>
<doc id="25257" url="https://en.wikipedia.org/wiki?curid=25257" title="Qusay Hussein">
Qusay Hussein

Qusay Saddam Hussein al-Tikriti (or Qusai, ; – ) was the second son of Iraqi President Saddam Hussein. He was appointed as his father's heir apparent in 2000.

Qusay was born in Baghdad in 1966 to Ba'athist revolutionary Saddam Hussein, who was in prison at the time, and his wife and cousin, Sajida Talfah. Unlike other members of his family and the government, little information is known about Qusay, politically or personally. He married Sahar Maher Abd al-Rashid; the daughter of Maher Abd al-Rashid, a top ranking military official, and had three sons: Mustapha Qusay (born 3 January 1989 – 22 July 2003); Yahya Qusay (born 1991) and Yaqub Qusay(chor).

Qusay played a role in crushing the Shiite uprising in the aftermath of the 1991 Gulf War and is also thought to have masterminded the destruction of the southern marshes of Iraq. The wholesale destruction of these marshes ended a centuries-old way of life that prevailed among the Shiite Marsh Arabs who made the wetlands their home, and ruined the habitat for dozens of species of migratory birds. The Iraqi government stated that the action was intended to produce usable farmland, though a number of outsiders believe the destruction was aimed against the Marsh Arabs as retribution for their participation in the 1991 uprising.

Qusay's older brother Uday was viewed as Saddam's heir-apparent until he sustained serious injuries in a 1996 assassination attempt. Unlike Uday, who was known for extravagance and erratic, violent behavior, Qusay Hussein kept a low profile.

Iraqi dissidents claim that Qusay was responsible for the killing of many political activists. "The Sunday Times" reported that Qusay ordered the killing of Khalis Mohsen al-Tikriti, an engineer at the military industrialization organization, because he believed Mohsen was planning to leave Iraq. In 1998, Iraqi opposition groups accused Qusay of ordering the execution of thousands of political prisoners after hundreds of inmates were similarly executed to make room for new prisoners in crowded jails.

Qusay's service in the Iraqi Republican Guard began in 2000. It is believed that he became the supervisor of the Guard and the head of internal security forces (possibly the Special Security Organization (SSO)), and had authority over other Iraqi military units.

On the afternoon of 22 July 2003, troops of the 101st Airborne 3/327th Infantry HQ and C-Company, aided by U.S. Special Forces, killed Qusay, his 14-year-old son Mustapha, and his older brother Uday, during a raid on a home in the northern Iraqi city of Mosul. Acting on a tip from a cousin, a special forces team attempted to apprehend the inhabitants of the house. After being fired on, the special forces moved back and called for backup. As few as 40 101st Soldiers and 8 Task Force 121 operators were on the scene. After Task Force 121 members were wounded, the 3/327th Infantry surrounded and fired on the house with a TOW missile, Mark 19 Automatic Grenade Launcher, M2 50 Caliber Machine guns and small arms. After about four hours of battle (the whole operation lasted 6 hours), the soldiers entered the house and found four dead, including the two brothers and their bodyguard. There were reports that Qusay's 14-year-old son Mustapha was the fourth body found. Brig. Gen. Frank Helmick, the assistant commander of 101st Airborne, commented that all occupants of the house died during the fierce gun battle before U.S. troops entered.

On 23 July 2003, the American command said that it had conclusively identified two of the dead men as Saddam Hussein's sons from dental records. Because many Iraqis were skeptical of news of the deaths, the U.S. Government released photos of the corpses and allowed Iraq's governing council to identify the bodies despite the U.S. objection to the publication of American corpses on Arab television. They also announced that the informant, possibly the owner of the house, would receive the combined $30 million reward on the pair. Qusay was the ace of clubs in the coalition forces' most-wanted Iraqi playing cards. His father was the ace of spades and his brother was the ace of hearts.

Qusay's other two sons, Yahya Qusay and Yaqub Qusay, are presumed alive, but their whereabouts are unknown.



</doc>
<doc id="25263" url="https://en.wikipedia.org/wiki?curid=25263" title="Quatrain">
Quatrain

A quatrain is a type of stanza, or a complete poem, consisting of four lines.

Existing in a variety of forms, the quatrain appears in poems from the poetic traditions of various ancient civilizations including Ancient India, Ancient Greece, Ancient Rome, and China, and continues into the 21st century, where it is seen in works published in many languages. During Europe's Dark Ages, in the Middle East and especially Iran, polymath poets such as Omar Khayyam continued to popularize this form of poetry, also known as Ruba'i, well beyond their borders and time. Michel de Nostredame (Nostradamus) used the quatrain form to deliver his famous prophecies in the 16th century.

There are fifteen possible rhyme schemes, but the most traditional and common are: AAAA, ABAB, and ABBA.

<poem style="margin-left:3em">
The curfew tolls the knell of parting day,
The lowing herd wind slowly o'er the lea,
The plowman homeward plods his weary way,
And leaves the world to darkness and to me.
</poem>
<poem style="margin-left:3em">
Come, fill the Cup, and in the fire of Spring
Your Winter garment of Repentance fling:
To flutter—and the Bird is on the Wing.
</poem>

EXTRA
it comes from heart



</doc>
<doc id="25264" url="https://en.wikipedia.org/wiki?curid=25264" title="Quantum chromodynamics">
Quantum chromodynamics

In theoretical physics, quantum chromodynamics (QCD) is the theory of the strong interaction between quarks and gluons, the fundamental particles that make up composite hadrons such as the proton, neutron and pion. QCD is a type of quantum field theory called a non-abelian gauge theory, with symmetry group SU(3). The QCD analog of electric charge is a property called "color". Gluons are the force carrier of the theory, like photons are for the electromagnetic force in quantum electrodynamics. The theory is an important part of the Standard Model of particle physics. A large body of experimental evidence for QCD has been gathered over the years.

QCD exhibits two main properties:


Physicist Murray Gell-Mann (b. 1929) coined the word "quark" in its present sense. It originally comes from the phrase "Three quarks for Muster Mark" in "Finnegans Wake" by James Joyce. On June 27, 1978, Gell-Mann wrote a private letter to the editor of the "Oxford English Dictionary", in which he related that he had been influenced by Joyce's words: "The allusion to three quarks seemed perfect." (Originally, only three quarks had been discovered.) Gell-Mann, however, wanted to pronounce the word to rhyme with "fork" rather than with "park", as Joyce seemed to indicate by rhyming words in the vicinity such as "Mark". Gell-Mann got around that "by supposing that one ingredient of the line 'Three quarks for Muster Mark' was a cry of 'Three quarts for Mister ...' heard in H.C. Earwicker's pub", a plausible suggestion given the complex punning in Joyce's novel.

The three kinds of charge in QCD (as opposed to one in quantum electrodynamics or QED) are usually referred to as "color charge" by loose analogy to the three kinds of color (red, green and blue) perceived by humans. Other than this nomenclature, the quantum parameter "color" is completely unrelated to the everyday, familiar phenomenon of color.

The force between quarks is known as the colour force (or color force ) or strong interaction, and is responsible for the strong nuclear force.

Since the theory of electric charge is dubbed "electrodynamics", the Greek word χρῶμα "chroma" "color" is applied to the theory of color charge, "chromodynamics".

With the invention of bubble chambers and spark chambers in the 1950s, experimental particle physics discovered a large and ever-growing number of particles called hadrons. It seemed that such a large number of particles could not all be fundamental. First, the particles were classified by charge and isospin by Eugene Wigner and Werner Heisenberg; then, in 1953–56, according to strangeness by Murray Gell-Mann and Kazuhiko Nishijima (see Gell-Mann–Nishijima formula). To gain greater insight, the hadrons were sorted into groups having similar properties and masses using the "eightfold way", invented in 1961 by Gell-Mann and Yuval Ne'eman. Gell-Mann and George Zweig, correcting an earlier approach of Shoichi Sakata, went on to propose in 1963 that the structure of the groups could be explained by the existence of three flavors of smaller particles inside the hadrons: the quarks.

Perhaps the first remark that quarks should possess an additional quantum number was made as a short footnote in the preprint of Boris Struminsky in connection with Ω hyperon composed of three strange quarks with parallel spins (this situation was peculiar, because since quarks are fermions, such combination is forbidden by the Pauli exclusion principle): Boris Struminsky was a PhD student of Nikolay Bogolyubov. The problem considered in this preprint was suggested by Nikolay Bogolyubov, who advised Boris Struminsky in this research. In the beginning of 1965, Nikolay Bogolyubov, Boris Struminsky and Albert Tavkhelidze wrote a preprint with a more detailed discussion of the additional quark quantum degree of freedom. This work was also presented by Albert Tavchelidze without obtaining consent of his collaborators for doing so at an international conference in Trieste (Italy), in May 1965.

A similar mysterious situation was with the Δ baryon; in the quark model, it is composed of three up quarks with parallel spins. In 1964–65, Greenberg and Han–Nambu independently resolved the problem by proposing that quarks possess an additional SU(3) gauge degree of freedom, later called color charge. Han and Nambu noted that quarks might interact via an octet of vector gauge bosons: the gluons.

Since free quark searches consistently failed to turn up any evidence for the new particles, and because an elementary particle back then was "defined" as a particle which could be separated and isolated, Gell-Mann often said that quarks were merely convenient mathematical constructs, not real particles. The meaning of this statement was usually clear in context: He meant quarks are confined, but he also was implying that the strong interactions could probably not be fully described by quantum field theory.

Richard Feynman argued that high energy experiments showed quarks are real particles: he called them "partons" (since they were parts of hadrons). By particles, Feynman meant objects which travel along paths, elementary particles in a field theory.

The difference between Feynman's and Gell-Mann's approaches reflected a deep split in the theoretical physics community. Feynman thought the quarks have a distribution of position or momentum, like any other particle, and he (correctly) believed that the diffusion of parton momentum explained diffractive scattering. Although Gell-Mann believed that certain quark charges could be localized, he was open to the possibility that the quarks themselves could not be localized because space and time break down. This was the more radical approach of S-matrix theory.

James Bjorken proposed that pointlike partons would imply certain relations in deep inelastic scattering of electrons and protons, which were verified in experiments at SLAC in 1969. This led physicists to abandon the S-matrix approach for the strong interactions.

In 1973 the concept of color as the source of a "strong field" was developed into the theory of QCD by physicists Harald Fritzsch and Heinrich Leutwyler, together with physicist Murray Gell-Mann. In particular, they employed the general field theory developed in 1954 by Chen Ning Yang and Robert Mills (see Yang–Mills theory), in which the carrier particles of a force can themselves radiate further carrier particles. (This is different from QED, where the photons that carry the electromagnetic force do not radiate further photons.)

The discovery of asymptotic freedom in the strong interactions by David Gross, David Politzer and Frank Wilczek allowed physicists to make precise predictions of the results of many high energy experiments using the quantum field theory technique of perturbation theory. Evidence of gluons was discovered in three-jet events at PETRA in 1979. These experiments became more and more precise, culminating in the verification of perturbative QCD at the level of a few percent at the LEP in CERN.

The other side of asymptotic freedom is confinement. Since the force between color charges does not decrease with distance, it is believed that quarks and gluons can never be liberated from hadrons. This aspect of the theory is verified within lattice QCD computations, but is not mathematically proven. One of the Millennium Prize Problems announced by the Clay Mathematics Institute requires a claimant to produce such a proof. Other aspects of non-perturbative QCD are the exploration of phases of quark matter, including the quark–gluon plasma.

The relation between the short-distance particle limit and the confining long-distance limit is one of the topics recently explored using string theory, the modern form of S-matrix theory.

Every field theory of particle physics is based on certain symmetries of nature whose existence is deduced from observations. These can be

QCD is a gauge theory of the SU(3) gauge group obtained by taking the color charge to define a local symmetry.

Since the strong interaction does not discriminate between different flavors of quark, QCD has approximate flavor symmetry, which is broken by the differing masses of the quarks.

There are additional global symmetries whose definitions require the notion of chirality, discrimination between left and right-handed. If the spin of a particle has a positive projection on its direction of motion then it is called left-handed; otherwise, it is right-handed. Chirality and handedness are not the same, but become approximately equivalent at high energies.

As mentioned, "asymptotic freedom" means that at large energy – this corresponds also to "short distances" – there is practically no interaction between the particles. This is in contrast – more precisely one would say "dual"– to what one is used to, since usually one connects the absence of interactions with "large" distances. However, as already mentioned in the original paper of Franz Wegner, a solid state theorist who introduced 1971 simple gauge invariant lattice models, the high-temperature behaviour of the "original model", e.g. the strong decay of correlations at large distances, corresponds to the low-temperature behaviour of the (usually ordered!) "dual model", namely the asymptotic decay of non-trivial correlations, e.g. short-range deviations from almost perfect arrangements, for short distances. Here, in contrast to Wegner, we have only the dual model, which is that one described in this article.

The color group SU(3) corresponds to the local symmetry whose gauging gives rise to QCD. The electric charge labels a representation of the local symmetry group U(1) which is gauged to give QED: this is an abelian group. If one considers a version of QCD with "N" flavors of massless quarks, then there is a global (chiral) flavor symmetry group SU("N") × SU("N") × U(1) × U(1). The chiral symmetry is spontaneously broken by the QCD vacuum to the vector (L+R) SU("N") with the formation of a chiral condensate. The vector symmetry, U(1) corresponds to the baryon number of quarks and is an exact symmetry. The axial symmetry U(1) is exact in the classical theory, but broken in the quantum theory, an occurrence called an anomaly. Gluon field configurations called instantons are closely related to this anomaly.

There are two different types of SU(3) symmetry: there is the symmetry that acts on the different colors of quarks, and this is an exact gauge symmetry mediated by the gluons, and there is also a flavor symmetry which rotates different flavors of quarks to each other, or "flavor SU(3)". Flavor SU(3) is an approximate symmetry of the vacuum of QCD, and is not a fundamental symmetry at all. It is an accidental consequence of the small mass of the three lightest quarks.

In the QCD vacuum there are vacuum condensates of all the quarks whose mass is less than the QCD scale. This includes the up and down quarks, and to a lesser extent the strange quark, but not any of the others. The vacuum is symmetric under SU(2) isospin rotations of up and down, and to a lesser extent under rotations of up, down and strange, or full flavor group SU(3), and the observed particles make isospin and SU(3) multiplets.

The approximate flavor symmetries do have associated gauge bosons, observed particles like the rho and the omega, but these particles are nothing like the gluons and they are not massless. They are emergent gauge bosons in an approximate string description of QCD.

The dynamics of the quarks and gluons are controlled by the quantum chromodynamics Lagrangian. The gauge invariant QCD Lagrangian is

where formula_1 is the quark field, a dynamical function of spacetime, in the fundamental representation of the SU(3) gauge group, indexed by formula_2; formula_3 is the gauge covariant derivative; the γ are Dirac matrices connecting the spinor representation to the vector representation of the Lorentz group.

The symbol formula_4 represents the gauge invariant gluon field strength tensor, analogous to the electromagnetic field strength tensor, "F", in quantum electrodynamics. It is given by:

where formula_6 are the gluon fields, dynamical functions of spacetime, in the adjoint representation of the SU(3) gauge group, indexed by "a", "b"...; and "f" are the structure constants of SU(3). Note that the rules to move-up or pull-down the "a", "b", or "c" indices are "trivial", (+, ..., +), so that "f" = "f" = "f" whereas for the "μ" or "ν" indices one has the non-trivial "relativistic" rules corresponding to the metric signature (+ − − −).

The variables "m" and "g" correspond to the quark mass and coupling of the theory, respectively, which are subject to renormalization.

An important theoretical concept is the "Wilson loop" (named after Kenneth G. Wilson). In lattice QCD, the final term of the above Lagrangian is discretized via Wilson loops, and more generally the behavior of Wilson loops can distinguish confined and deconfined phases.

Quarks are massive spin- fermions which carry a color charge whose gauging is the content of QCD. Quarks are represented by Dirac fields in the fundamental representation 3 of the gauge group SU(3). They also carry electric charge (either − or +) and participate in weak interactions as part of weak isospin doublets. They carry global quantum numbers including the baryon number, which is for each quark, hypercharge and one of the flavor quantum numbers.

Gluons are spin-1 bosons which also carry color charges, since they lie in the adjoint representation 8 of SU(3). They have no electric charge, do not participate in the weak interactions, and have no flavor. They lie in the singlet representation 1 of all these symmetry groups.

Every quark has its own antiquark. The charge of each antiquark is exactly the opposite of the corresponding quark.

According to the rules of quantum field theory, and the associated Feynman diagrams, the above theory gives rise to three basic interactions: a quark may emit (or absorb) a gluon, a gluon may emit (or absorb) a gluon, and two gluons may directly interact. This contrasts with QED, in which only the first kind of interaction occurs, since photons have no charge. Diagrams involving Faddeev–Popov ghosts must be considered too (except in the unitarity gauge).

Detailed computations with the above-mentioned Lagrangian show that the effective potential between a quark and its anti-quark in a meson contains a term that increases in proportion to the distance between the quark and anti-quark (formula_7), which represents some kind of "stiffness" of the interaction between the particle and its anti-particle at large distances, similar to the entropic elasticity of a rubber band (see below). This leads to "confinement"  of the quarks to the interior of hadrons, i.e. mesons and nucleons, with typical radii "R", corresponding to former "Bag models" of the hadrons The order of magnitude of the "bag radius" is 1 fm (= 10 m). Moreover, the above-mentioned stiffness is quantitatively related to the so-called "area law" behaviour of the expectation value of the Wilson loop product "P" of the ordered coupling constants around a closed loop "W"; i.e. formula_8 is proportional to the "area" enclosed by the loop. For this behaviour the non-abelian behaviour of the gauge group is essential.

Further analysis of the content of the theory is complicated. Various techniques have been developed to work with QCD. Some of them are discussed briefly below.

This approach is based on asymptotic freedom, which allows perturbation theory to be used accurately in experiments performed at very high energies. Although limited in scope, this approach has resulted in the most precise tests of QCD to date.

Among non-perturbative approaches to QCD, the most well established one is lattice QCD. This approach uses a discrete set of spacetime points (called the lattice) to reduce the analytically intractable path integrals of the continuum theory to a very difficult numerical computation which is then carried out on supercomputers like the QCDOC which was constructed for precisely this purpose. While it is a slow and resource-intensive approach, it has wide applicability, giving insight into parts of the theory inaccessible by other means, in particular into the explicit forces acting between quarks and antiquarks in a meson. However, the numerical sign problem makes it difficult to use lattice methods to study QCD at high density and low temperature (e.g. nuclear matter or the interior of neutron stars).

A well-known approximation scheme, the expansion, starts from the idea that the number of colors is infinite, and makes a series of corrections to account for the fact that it is not. Until now, it has been the source of qualitative insight rather than a method for quantitative predictions. Modern variants include the AdS/CFT approach.

For specific problems effective theories may be written down which give qualitatively correct results in certain limits. In the best of cases, these may then be obtained as systematic expansions in some parameter of the QCD Lagrangian. One such effective field theory is chiral perturbation theory or ChiPT, which is the QCD effective theory at low energies. More precisely, it is a low energy expansion based on the spontaneous chiral symmetry breaking of QCD, which is an exact symmetry when quark masses are equal to zero, but for the u, d and s quark, which have small mass, it is still a good approximate symmetry. Depending on the number of quarks which are treated as light, one uses either SU(2) ChiPT or SU(3) ChiPT . Other effective theories are heavy quark effective theory (which expands around heavy quark mass near infinity), and soft-collinear effective theory (which expands around large ratios of energy scales). In addition to effective theories, models like the Nambu–Jona-Lasinio model and the chiral model are often used when discussing general features.

Based on an Operator product expansion one can derive sets of relations that connect different observables with each other.

In one of his recent works, Kei-Ichi Kondo derived as a low-energy limit of QCD, a theory linked to the Nambu–Jona-Lasinio model since it is basically a particular non-local version of the Polyakov–Nambu–Jona-Lasinio model. The later being in its local version, nothing but the Nambu–Jona-Lasinio model in which one has included the Polyakov loop effect, in order to describe a 'certain confinement'.

The Nambu–Jona-Lasinio model in itself is, among many other things, used because it is a 'relatively simple' model of chiral symmetry breaking, phenomenon present up to certain conditions (Chiral limit i.e. massless fermions) in QCD itself.
In this model, however, there is no confinement. In particular, the energy of an isolated quark in the physical vacuum turns out well defined and finite.

The notion of quark flavors was prompted by the necessity of explaining the properties of hadrons during the development of the quark model. The notion of color was necessitated by the puzzle of the . This has been dealt with in the section on the history of QCD.

The first evidence for quarks as real constituent elements of hadrons was obtained in deep inelastic scattering experiments at SLAC. The first evidence for gluons came in three jet events at PETRA.

Several good quantitative tests of perturbative QCD exist:

Quantitative tests of non-perturbative QCD are fewer, because the predictions are harder to make. The best is probably the running of the QCD coupling as probed through lattice computations of heavy-quarkonium spectra. There is a recent claim about the mass of the heavy meson B . Other non-perturbative tests are currently at the level of 5% at best. Continuing work on masses and form factors of hadrons and their weak matrix elements are promising candidates for future quantitative tests. The whole subject of quark matter and the quark–gluon plasma is a non-perturbative test bed for QCD which still remains to be properly exploited.

One qualitative prediction of QCD is that there exist composite particles made solely of gluons called glueballs that have not yet been definitively observed experimentally. A definitive observation of a glueball with the properties predicted by QCD would strongly confirm the theory. In principle, if glueballs could be definitively ruled out, this would be a serious experimental blow to QCD. But, as of 2013, scientists are unable to confirm or deny the existence of glueballs definitively, despite the fact that particle accelerators have sufficient energy to generate them.

There are unexpected cross-relations to solid state physics. For example, the notion of gauge invariance forms the basis of the well-known Mattis spin glasses, which are systems with the usual spin degrees of freedom formula_9 for "i" =1...,N, with the special fixed "random" couplings formula_10 Here the ε and ε quantities can independently and "randomly" take the values ±1, which corresponds to a most-simple gauge transformation formula_11 This means that thermodynamic expectation values of measurable quantities, e.g. of the energy formula_12 are invariant.

However, here the "coupling degrees of freedom" formula_13, which in the QCD correspond to the "gluons", are "frozen" to fixed values (quenching). In contrast, in the QCD they "fluctuate" (annealing), and through the large number of gauge degrees of freedom the entropy plays an important role (see below).

For positive "J" the thermodynamics of the Mattis spin glass corresponds in fact simply to a "ferromagnet in disguise", just because these systems have no "frustration" at all. This term is a basic measure in spin glass theory. Quantitatively it is identical with the loop product formula_14 along a closed loop "W". However, for a Mattis spin glass – in contrast to "genuine" spin glasses – the quantity "P" never becomes negative.

The basic notion "frustration" of the spin-glass is actually similar to the Wilson loop quantity of the QCD. The only difference is again that in the QCD one is dealing with SU(3) matrices, and that one is dealing with a "fluctuating" quantity. Energetically, perfect absence of frustration should be non-favorable and atypical for a spin glass, which means that one should add the loop product to the Hamiltonian, by some kind of term representing a "punishment". In the QCD the Wilson loop is essential for the Lagrangian rightaway.

The relation between the QCD and "disordered magnetic systems" (the spin glasses belong to them) were additionally stressed in a paper by Fradkin, Huberman and Shenker, which also stresses the notion of duality.

A further analogy consists in the already mentioned similarity to polymer physics, where, analogously to Wilson Loops, so-called "entangled nets" appear, which are important for the formation of the entropy-elasticity (force proportional to the length) of a rubber band. The non-abelian character of the SU(3) corresponds thereby to the non-trivial "chemical links", which glue different loop segments together, and "asymptotic freedom" means in the polymer analogy simply the fact that in the short-wave limit, i.e. for formula_15 (where "R" is a characteristic correlation length for the glued loops, corresponding to the above-mentioned "bag radius", while λ is the wavelength of an excitation) any non-trivial correlation vanishes totally, as if the system had crystallized.

There is also a correspondence between confinement in QCD – the fact that the color field is only different from zero in the interior of hadrons – and the behaviour of the usual magnetic field in the theory of type-II superconductors: there the magnetism is confined to the interior of the Abrikosov flux-line lattice,   i.e., the London penetration depth "λ" of that theory is analogous to the confinement radius "R" of quantum chromodynamics. Mathematically, this correspondendence is supported by the second term, formula_16 on the r.h.s. of the Lagrangian.





</doc>
<doc id="25265" url="https://en.wikipedia.org/wiki?curid=25265" title="Queue (abstract data type)">
Queue (abstract data type)

In computer science, a queue is a particular kind of abstract data type or collection in which the entities in the collection are kept in order and the principal (or only) operations on the collection are the addition of entities to the rear terminal position, known as "enqueue," and removal of entities from the front terminal position, known as "dequeue". This makes the queue a First-In-First-Out (FIFO) data structure. In a FIFO data structure, the first element added to the queue will be the first one to be removed. This is equivalent to the requirement that once a new element is added, all elements that were added before have to be removed before the new element can be removed. Often a "peek" or "front" operation is also entered, returning the value of the front element without dequeuing it. A queue is an example of a linear data structure, or more abstractly a sequential collection.

Queues provide services in computer science, transport, and operations research where various entities such as data, objects, persons, or events are stored and held to be processed later. In these contexts, the queue performs the function of a buffer.

Queues are common in computer programs, where they are implemented as data structures coupled with access routines, as an abstract data structure or in object-oriented languages as classes. Common implementations are circular buffers and linked lists.

Theoretically, one characteristic of a queue is that it does not have a specific capacity. Regardless of how many elements are already contained, a new element can always be added. It can also be empty, at which point removing an element will be impossible until a new element has been added again.

Fixed length arrays are limited in capacity, but it is not true that items need to be copied towards the head of the queue. The simple trick of turning the array into a closed circle and letting the head and tail drift around endlessly in that circle makes it unnecessary to ever move items stored in the array. If n is the size of the array, then computing indices modulo n will turn the array into a circle. This is still the conceptually simplest way to construct a queue in a high level language, but it does admittedly slow things down a little, because the array indices must be compared to zero and the array size, which is comparable to the time taken to check whether an array index is out of bounds, which some languages do, but this will certainly be the method of choice for a quick and dirty implementation, or for any high level language that does not have pointer syntax. The array size must be declared ahead of time, but some implementations simply double the declared array size when overflow occurs. Most modern languages with objects or pointers can implement or come with libraries for dynamic lists. Such data structures may have not specified fixed capacity limit besides memory constraints. Queue "overflow" results from trying to add an element onto a full queue and queue "underflow" happens when trying to remove an element from an empty queue.

A "bounded queue" is a queue limited to a fixed number of items.

There are several efficient implementations of FIFO queues. An efficient implementation is one that can perform the operations—enqueuing and dequeuing—in O(1) time.

Queues may be implemented as a separate data type, or may be considered a special case of a double-ended queue (deque) and not implemented separately. For example, Perl and Ruby allow pushing and popping an array from both ends, so one can use push and unshift functions to enqueue and dequeue a list (or, in reverse, one can use shift and pop), although in some cases these operations are not efficient.

C++'s Standard Template Library provides a "codice_1" templated class which is restricted to only push/pop operations. Since J2SE5.0, Java's library contains a interface that specifies queue operations; implementing classes include and (since J2SE 1.6) . PHP has an SplQueue class and third party libraries like beanstalk'd and Gearman.

A simple queue implemented in Ruby:

Queues can also be implemented as a purely functional data structure. Two versions of the implementation exist. The first one, called real-time queue, presented below, allows the queue to be persistent with operations in O(1) worst-case time, but requires lazy lists with memoization. The second one, with no lazy lists nor memoization is presented at the end of the sections. Its amortized time is formula_1 if the persistency is not used; but its worst-time complexity is formula_2 where "n" is the number of elements in the queue.

Let us recall that, for formula_3 a list, formula_4 denotes its length, that "NIL" represents an empty list and formula_5 represents the list whose head is "h" and whose tail is "t".

The data structure used to implement our queues consists of three linked lists formula_6 where "f" is the front of the queue, "r" is the rear of the queue in reverse order. The invariant of the structure is that "s" is the rear of "f" without its formula_7 first elements, that is formula_8. The tail of the queue formula_9 is then almost formula_6 and
inserting an element "x" to formula_6 is almost formula_12. It is said almost, because in both of those results, formula_13. An auxiliary function formula_14 must then be called for the invariant to be satisfied. Two cases must be considered, depending on whether formula_15 is the empty list, in which case formula_16, or not. The formal definition is formula_17 and formula_18 where formula_19 is "f" followed by "r" reversed.

Let us call formula_20 the function which returns "f" followed by "r" reversed. Let us furthermore assume that formula_16, since it is the case when this function is called. More precisely, we define a lazy function formula_22 which takes as input three list such that formula_16, and return the concatenation of "f", of "r" reversed and of "a". Then formula_24.
The inductive definition of rotate is formula_25 and formula_26. Its running time is formula_27, but, since lazy evaluation is used, the computation is delayed until the results is forced by the computation.

The list "s" in the data structure has two purposes. This list serves as a counter for formula_28, indeed, formula_29 if and only if "s" is the empty list. This counter allows us to ensure that the rear is never longer than the front list. Furthermore, using "s", which is a tail of "f", forces the computation of a part of the (lazy) list "f" during each "tail" and "insert" operation. Therefore, when formula_29, the list "f" is totally forced. If it was not the case, the internal representation of "f" could be some append of append of... of append, and forcing would not be a constant time operation anymore.

Note that, without the lazy part of the implementation, the real-time queue would be a non-persistent implementation of queue in formula_1 amortized time. In this case, the list "s" can be replaced by the integer formula_28, and the reverse function would be called when formula_15 is 0.





</doc>
<doc id="25266" url="https://en.wikipedia.org/wiki?curid=25266" title="Quake (video game)">
Quake (video game)

Quake is a first-person shooter video game, developed by id Software and published by GT Interactive in 1996. It is the first game in the "Quake" series. In the game, players must find their way through various maze-like, medieval environments while battling a variety of monsters using a wide array of weapons.

The successor to id Software's "Doom" series, "Quake" built upon the technology and gameplay of its predecessor. Unlike the "Doom" engine before it, the "Quake" engine offered full real-time 3D rendering and had early support for 3D acceleration through OpenGL. After "Doom" helped popularize multiplayer deathmatches, "Quake" added various multiplayer options. Online multiplayer became increasingly common, with the QuakeWorld update and software such as QuakeSpy making the process of finding and playing against others on the Internet easier and more reliable. It features music composed by Trent Reznor and Nine Inch Nails.

In "Quake" single-player mode, players explore and navigate to the exit of each Gothic and dark level, facing monsters and finding secret areas along the way. Usually there are switches to activate or keys to collect in order to open doors before the exit can be reached. Reaching the exit takes the player to the next level. Before accessing an episode, there is a set of three pathways with easy, medium, and hard skill levels. The fourth skill level, "Nightmare", was "so bad that it was hidden, so people won't wander in by accident"; the player must drop through water before the episode four entrance and go into a secret passage to access it.

"Quake" single-player campaign is organized into four individual episodes with seven to eight levels in each (including one secret level per episode, one of which is a "low gravity" level that challenges the player's abilities in a different way). As items are collected, they are carried to the next level, each usually more challenging than the last. If the player's character dies, he must restart at the beginning of the level. The game may be saved at any time. Upon completing an episode, the player is returned to the hub "START" level, where another episode can be chosen. Each episode starts the player from scratch, without any previously collected items. Episode one (which formed the shareware or downloadable demo version of "Quake") has the most traditional ideology of a boss in the last level. The ultimate objective at the end of each episode is to recover a magic rune. After all of the runes are collected, the floor of the hub level opens up to reveal an entrance to the "END" level which contains the final boss of the game.

In multiplayer mode, players on several computers connect to a server (which may be a dedicated machine or on one of the player's computers), where they can either play the single-player campaign together in co-op mode, or play against each other in multiplayer. When players die in multiplayer mode, they can immediately respawn, but will lose any items that were collected. Similarly, items that have been picked up previously respawn after some time, and may be picked up again. The most popular multiplayer modes are all forms of deathmatch. Deathmatch modes typically consist of either "free-for-all" (no organization or teams involved), one-on-one "duels", or organized "teamplay" with two or more players per team (or clan). Teamplay is also frequently played with one or another mod. Monsters are not normally present in teamplay, as they serve no purpose other than to get in the way and reveal the positions of the players.

The gameplay in "Quake" was considered unique for its time because of the different ways the player can maneuver through the game. For example: bunny hopping or strafe jumping can be used to move faster than normal, while rocket jumping enables the player to reach otherwise-inaccessible areas at the cost of some self-damage. The player can start and stop moving suddenly, jump unnaturally high, and change direction while moving through the air. Many of these non-realistic behaviors contribute to "Quake"s appeal. Multiplayer "Quake" was one of the first games singled out as a form of electronic sport. A notable participant was Dennis Fong who won John Carmack's Ferrari 328 at the Microsoft-sponsored Red Annihilation tournament in 1997.

In the single-player game, the player takes the role of the protagonist known as Ranger (voiced by Trent Reznor) who was sent into a portal in order to stop an enemy code-named "Quake". The government had been experimenting with teleportation technology and developed a working prototype called a "Slipgate"; the mysterious Quake compromised the Slipgate by connecting it with its own teleportation system, using it to send death squads to the "Human" dimension in order to test the martial capabilities of humanity.

The sole surviving protagonist in "Operation Counterstrike" is Ranger, who must advance, starting each of the four episodes from an overrun human military base, before fighting his way into other dimensions, reaching them via the Slipgate or their otherworld equivalent. After passing through the Slipgate, Ranger's main objective is to collect four magic runes from four dimensions of Quake; these are the key to stopping the enemy later discovered as Shub-Niggurath and ending the invasion of Earth.

The single-player campaign consists of 30 separate levels, or "maps", divided into four episodes (with a total of 26 regular maps and four secret ones), as well as a hub level to select a difficulty setting and episode, and the game's final boss level. Each episode represents individual dimensions that the player can access through magical portals (as opposed to the technological Slipgate) that are discovered over the course of the game. The various realms consist of a number of gothic, medieval, and lava-filled caves and dungeons, with a recurring theme of hellish and satanic imagery reminiscent of "Doom" (such as pentagrams and images of demons on the walls). The latter is inspired by several dark fantasy influences, most notably that of H. P. Lovecraft. Dimensional Shamblers appear as enemies, the "Spawn" enemies are called "Formless Spawn of Tsathoggua" in the manual, the boss of the first episode is named Chthon, and the final boss is named Shub-Niggurath (though actually resembling a Dark Young). Some levels have Lovecraftian names, such as the Vaults of Zin and The Nameless City. In addition, six levels exclusively designed for multiplayer deathmatch are also included. Originally, the game was supposed to include more Lovecraftian bosses, but this concept was scrapped due to time constraints.

A preview included with id's very first release, 1990's "Commander Keen", advertised a game entitled "The Fight for Justice" as a follow-up to the "Commander Keen" trilogy. It would feature a character named Quake, "the strongest, most dangerous person on the continent", armed with thunderbolts and a "Ring of Regeneration". Conceived as a VGA full-color side-scrolling role-playing game, "The Fight for Justice" was never released.

Lead designer and director John Romero later conceived of "Quake" as an action game taking place in a fully 3D world, inspired by Sega AM2's 3D fighting game "Virtua Fighter". "Quake" was also intended to feature "Virtua Fighter" influenced third-person melee combat. However, id Software considered it to be risky. Because the project was taking too long, the third-person melee was eventually dropped. This led to creative differences between Romero and id Software, and eventually his departure from the company after "Quake" was released.

"Quake" was given as a title to the game that id Software was working on shortly after the release of "". The earliest information released described "Quake" as focusing on a Thor-like character who wields a giant hammer, and is able to knock away enemies by throwing the hammer (complete with real-time inverse kinematics). Initially, the levels were supposed to be designed in an Aztec style, but the choice was dropped some months into the project. Early screenshots then showed medieval environments and dragons. The plan was for the game to have more RPG-style elements. However, work was very slow on the engine, since John Carmack, the main programmer of "Quake", was not only developing a fully 3D engine, but also a TCP/IP networking model (Carmack later said that he should have done two separate projects which developed those things).

Eventually, the whole id Software team began to think that the original concept may not have been as wise a choice as they first believed. Thus, the final game was very stripped down from its original intentions, and instead featured gameplay similar to "Doom" and its sequel, although the levels and enemies were closer to medieval RPG style rather than science-fiction. In a December 1, 1994 post to an online bulletin board, John Romero wrote, "Okay, people. It seems that everyone is speculating on whether Quake is going to be a slow, RPG-style light-action game. Wrong! What does id do best and dominate at? Can you say "action"? I knew you could. Quake will be constant, hectic action throughout – probably more so than Doom."

"Quake" was programmed by John Carmack, Michael Abrash, and John Cash. The levels and scenarios were designed by American McGee, Sandy Petersen, John Romero, and Tim Willits, and the graphics were designed by Adrian Carmack, Kevin Cloud and Paul Steed. Cloud created the monster and player graphics using Alias.

The game engine developed for "Quake", the "Quake" engine, popularized several major advances in the first-person shooter genre: polygonal models instead of prerendered sprites; full 3D level design instead of a 2.5D map; prerendered lightmaps; and allowing end users to partially program the game (in this case with QuakeC), which popularized fan-created modifications (mods).

Before the release of the full game or the shareware version of "Quake", id Software released "QTest" on February 24, 1996. It was described as a technology demo and was limited to three multiplayer maps. There was no single-player support and some of the gameplay and graphics were unfinished or different from their final versions. "QTest" gave gamers their first peek into the filesystem and modifiability of the "Quake" engine, and many entity mods (that placed monsters in the otherwise empty multiplayer maps) and custom player skins began appearing online before the full game was even released.

Initially, the game was designed so that when the player ran out of ammunition, the player character would hit enemies with a gun-butt. Shortly before release this was replaced with an axe.

"Quake"'s music and sound design was done by Trent Reznor and Nine Inch Nails, using ambient soundscapes and synthesized drones to create atmospheric tracks. In an interview, Reznor remarked that the "Quake" soundtrack "is not music, it's textures and ambiences and whirling machine noises and stuff. We tried to make the most sinister, depressive, scary, frightening kind of thing... It's been fun." The game also has some ammo boxes decorated with the Nine Inch Nails logo.

The first ports to be completed were the Linux and SPARC Solaris ports by id Software employee Dave D. Taylor in 1996. The first commercially released port was the 1997 port to Mac OS, done by MacSoft. ClickBOOM announced version for Amiga-computers in 1998. Finally in 1999, a retail version of the Linux port was distributed by Macmillan Digital Publishing USA in a bundle with the three add-ons as "Quake: The Offering".

"Quake" was also ported to home console systems. On December 2, 1997, the game was released for the Sega Saturn. Initially GT Interactive was to publish this version itself, but it later cancelled the release and the Saturn rights were picked up by Sega. Sega then took the project away from the original development team, who had been encountering difficulties getting the port to run at a decent frame rate, and assigned it to Lobotomy Software. The Sega Saturn port used Lobotomy Software's own 3D game engine, "SlaveDriver" (the same game engine that powered the Sega Saturn versions of "PowerSlave" and "Duke Nukem 3D"), instead of the original "Quake" engine. It is the only version of "Quake" that is rated "T" for Teen instead of "M" for Mature. "Quake" had also been ported to the Sony PlayStation by Lobotomy Software, but the port was cancelled due to difficulties finding a publisher. On March 24, 1998, the game was released for the Nintendo 64 by Midway Games. This version was developed by the same programming team that worked on "Doom 64". Both console ports required compromises because of the limited CPU power and ROM storage space for levels. The Sega Saturn version includes 28 of the 32 single-player levels from the original PC version of the game, though the secret levels, Ziggurat Vertigo (E1M8), The Underearth (E2M7), The Haunted Halls (E3M7), and The Nameless City (E4M8), were removed. Instead, it has four exclusive secret levels: Purgatorium, Hell's Aerie, The Coliseum, and Watery Grave. It also contains an exclusive unlockable, "Dank & Scuz", which is a story set in the Quake milieu and presented in the form of a slide show with voice acting. There are no multiplayer modes in the Sega Saturn version; as a result of this, all of the deathmatch maps from the PC version were removed from the Sega Saturn port. The Nintendo 64 version includes 25 single-player levels from the PC version, though it is missing The Grisly Grotto (E1M4), The Installation (E2M1), The Ebon Fortress (E2M4), The Wind Tunnels (E3M5), The Sewage System (E4M1), and Hell's Atrium (E4M5) levels. It also does not use the hub "START" map where the player chooses a difficulty level and an episode; the difficulty level is chosen from a menu when starting the game, and all of the levels are played in sequential order from The Slipgate Complex (E1M1) to Shub Niggurath's Pit (END). The Nintendo 64 version, while lacking the cooperative multiplayer mode, includes two player deathmatch. All six of the deathmatch maps from the PC version are in the Nintendo 64 port, and an exclusive deathmatch level, The Court of Death, is also included.

Two ports of "Quake" for the Nintendo DS exist, "QuakeDS" and "CQuake". Both run well, however, multiplayer does not work on "QuakeDS". Since the source code for "Quake" was released, a number of unofficial ports have been made available for PDAs and mobile phones, such as PocketQuake, as well as versions for the Symbian S60 series of mobile phones and Android mobile phones.

In 2005, id Software signed a deal with publisher Pulse Interactive to release a version of "Quake" for mobile phones. The game was engineered by Californian company Bear Naked Productions. Initially due to be released on only two mobile phones, the Samsung Nexus (for which it was to be an embedded game) and the LG VX360. "Quake mobile" was reviewed by GameSpot on the Samsung Nexus and they cited its US release as October 2005; they also gave it a "Best Mobile Game" in their E3 2005 Editor's Choice Awards. "It is unclear as to whether the game actually did ship with the Samsung Nexus. The game is only available for the DELL x50v and x51v both of which are PDAs not mobile phones. "Quake Mobile" does not feature the Nine Inch Nails soundtrack due to space constraints. "Quake Mobile" runs the most recent version of GL Quake (Quake v.1.09 GL 1.00) at 800x600 resolution and 25 fps. The most recent version of "Quake Mobile" is v.1.20 which has stylus support. There was an earlier version v.1.19 which lacked stylus support. The two "Quake" expansion packs, "Scourge of Armagon" and "Dissolution of Eternity", are also available for "Quake Mobile".

A Flash-based version of the game by Michael Rennie runs "Quake" at full speed in any Flash-enabled web browser. Based on the shareware version of the game, it includes only the first episode and is available for free on the web.

"Quake" can be heavily modified by altering the graphics, audio, or scripting in QuakeC, and has been the focus of many fan created "mods". The first mods were small gameplay fixes and patches initiated by the community, usually enhancements to weapons or gameplay with new enemies. Later mods were more ambitious and resulted in "Quake" fans creating versions of the game that were drastically different from id Software's original release.

The first major "Quake" mod was "Team Fortress". This mod consists of Capture the Flag gameplay with a class system for the players. Players choose a class, which creates various restrictions on weapons and armor types available to that player, and also grants special abilities. For example, the bread-and-butter "Soldier" class has medium armor, medium speed, and a well-rounded selection of weapons and grenades, while the "Scout" class is lightly armored, very fast, has a scanner that detects nearby enemies, but has very weak offensive weapons. One of the other differences with CTF is the fact that the flag is not returned automatically when a player drops it: running over one's flag in "Threewave CTF" would return the flag to the base, and in "TF" the flag remains in the same spot for preconfigured time and it has to be defended on remote locations. This caused a shift in defensive tactics compared to "Threewave CTF". "Team Fortress" maintained its standing as the most-played online "Quake" modification for many years.

Another popular mod was "Threewave Capture the Flag" (CTF), primarily authored by Dave 'Zoid' Kirsch. "Threewave CTF" is a partial conversion consisting of new levels, a new weapon (a grappling hook), power-ups, new textures, and new gameplay rules. Typically, two teams (red and blue) would compete in a game of Capture the flag, though a few maps with up to four teams (red, blue, green, and yellow) were created. Capture the Flag soon became a standard game mode included in most popular multiplayer games released after "Quake". "Rocket Arena" provides the ability for players to face each other in small, open arenas with changes in the gameplay rules so that item collection and detailed level knowledge are no longer factors. A series of short rounds, with the surviving player in each round gaining a point, instead tests the player's aiming and dodging skills and reflexes. "Clan Arena" is a further modification that provides team play using "Rocket Arena" rules. One mod category, "bots", was introduced to provide surrogate players in multiplayer mode.

There are a large number of custom levels that have been made by users and fans of "Quake". , new maps are still being made, over twenty years since the game's release. Custom maps are new maps that are playable by loading them into the original game. Custom levels of various gameplay types have been made, but most are in the single-player and deathmatch genres. More than 1500 single-player and a similar number of deathmatch maps have been made for "Quake".

According to David Kushner in "Masters of Doom", id Software released a retail shareware version of "Quake" before the game's full retail distribution by GT Interactive. These shareware copies could be converted into complete versions through passwords purchased via phone. However, Kushner wrote that "gamers wasted no time hacking the shareware to unlock the full version of the game for free." This problem, combined with the scale of the operation, led id Software to cancel the plan. As a result, the company was left with 150,000 unsold shareware copies in storage. The venture damaged "Quake"s initial sales and caused its retail push by GT Interactive to miss the holiday shopping season. Following the game's full release, Kushner remarked that its early "sales were good — with 250,000 units shipped — but not a phenomenon like "Doom II"."

In the United States, "Quake" placed sixth on PC Data's monthly computer game sales charts for November and December 1996. Its shareware edition was the sixth-best-selling computer game of 1996 overall, while its retail SKU claimed 20th place. It remained in PC Data's monthly top 10 from January to April 1997, but was absent by May. During its first 12 months, "Quake" sold 373,000 retail copies and earned $18 million in the United States, according to PC Data. Its final retail sales for 1997 were 273,936 copies, which made it the country's 16th-highest computer game seller for the year.

Sales of "Quake" reached 550,000 units in the United States alone by December 1999. Worldwide, it sold 1.1 million units by that date.

"Quake" was critically acclaimed on the PC. Aggregating review websites GameRankings and Metacritic gave the original PC version 93.22% and 94/100, the Nintendo 64 port 76.14% and 74/100, and the Sega Saturn version 64.50%. A "Next Generation" critic lauded the game's realistic 3D physics and genuinely unnerving sound effects. Major Mike of "GamePro" said "Quake" had been over-hyped but is excellent nonetheless, particularly its usage of its advanced 3D engine. He also praised the sound effects, atmospheric music, and graphics, though he criticized that the polygons used to construct the enemies are too obvious at close range.

Less than a month after "Quake" was released (and a month before they actually reviewed the game), "Next Generation" listed it as number 9 on their "Top 100 Games of All Time", saying that it is similar to "Doom" but supports a maximum of eight players instead of four. In 1996, "Computer Gaming World" listed "telefragged" as #1 on its list of "the 15 best ways to die in computer gaming". In 1997, the Game Developers Choice Awards gave Quake three spotlight awards for Best Sound Effects, Best Music or Soundtrack and Best On-Line/Internet Game.

As an example of the dedication that "Quake" has inspired in its fan community, a group of expert players recorded speedrun demos (replayable recordings of the player's movement) of "Quake" levels completed in record time on the "Nightmare" skill level. The footage was edited into a continuous 19 minutes, 49 seconds demo called "Quake done Quick" and released on June 10, 1997. Owners of "Quake" could replay this demo in the game engine, watching the run unfold as if they were playing it themselves.

This involved a number of players recording run-throughs of individual levels, using every trick and shortcut they could discover in order to minimize the time it took to complete, usually to a degree that even the original level designers found difficult to comprehend, and in a manner that often bypassed large areas of the level. Stitching a series of the fastest runs together into a coherent whole created a demonstration of the entire game. "Recamming" is also used with speedruns in order to make the experience more movie-like, with arbitrary control of camera angles, editing, and sound that can be applied with editing software after the runs are first recorded. However, the fastest possible time for a given level will not necessarily result in the fastest time used to contribute to "running" the entire game. One example is acquiring the grenade launcher in an early level, an act that slows down the time for that level over the best possible, but speeds up the overall game time by allowing the runner to bypass a big area in a later level that they could not otherwise do.

A second attempt, "Quake done Quicker", reduced the completion time to 16 minutes, 35 seconds (a reduction of 3 minutes, 14 seconds). "Quake done Quicker" was released on September 13, 1997. One of the levels included was the result of an online competition to see who could get the fastest time. The culmination of this process of improvement was "Quake done Quick with a Vengeance". Released three years to the day after "Quake done Quicker", this pared down the time taken to complete all four episodes, on Nightmare (hardest) difficulty, to 12 minutes, 23 seconds (a further reduction of 4 minutes, 12 seconds), partly by using techniques that had formerly been shunned in such films as being less aesthetically pleasing. This run was recorded as an in-game demo, but interest was such that an .avi video clip was created to allow those without the game to see the run.

Most full-game speedruns are a collaborative effort by a number of runners (though some have been done by single runners on their own). Although each particular level is credited to one runner, the ideas and techniques used are iterative and collaborative in nature, with each runner picking up tips and ideas from the others, so that speeds keep improving beyond what was thought possible as the runs are further optimized and new tricks or routes are discovered. Further time improvements of the continuous whole game run were achieved into the 21st century. In addition, many thousands of individual level runs are kept at Speed Demos Archive's "Quake" section, including many on custom maps. Speedrunning is a counterpart to multiplayer modes in making "Quake" one of the first games promoted as a virtual sport.

The source code of the "Quake" and "QuakeWorld" engines was licensed under the GPL on December 21, 1999. The id Software maps, objects, textures, sounds, and other creative works remain under their original proprietary license. The shareware distribution of "Quake" is still freely redistributable and usable with the GPLed engine code. One must purchase a copy of "Quake" in order to receive the registered version of the game which includes more single-player episodes and the deathmatch maps. Based on the success of the first "Quake" game, and later published "Quake II" and "Quake III Arena"; "Quake 4" was released in October 2005, developed by Raven Software using the "Doom 3" engine.

"Quake" was the game primarily responsible for the emergence of the machinima artform of films made in game engines, thanks to edited "Quake" demos such as "Ranger Gone Bad" and "Blahbalicious", the in-game film "The Devil's Covenant", and the in-game-rendered, four-hour epic film "The Seal of Nehahra". On June 22, 2006, it had been 10 years since the original uploading of the game to cdrom.com archives. Many Internet forums had topics about it, and it was a front-page story on Slashdot. On October 11, 2006, John Romero released the original map files for all of the levels in "Quake" under the GPL.

"Quake" has four sequels: "Quake II", "Quake III Arena", "Quake 4", and "". In 2002, a version of "Quake" was produced for mobile phones. A copy of "Quake" was also released as a compilation in 2001, labeled "Ultimate Quake", which included the original "Quake", "Quake II", and "Quake III Arena" which was published by Activision. In 2008, "Quake" was honored at the 59th Annual Technology & Engineering Emmy Awards for advancing the art form of user modifiable games. John Carmack accepted the award. Years after its original release, "Quake" is still regarded by many critics as one of the greatest and most influential games ever made.

There were two official expansion packs released for "Quake". The expansion packs pick up where the first game left off, include all of the same weapons, power-ups, monsters, and gothic atmosphere/architecture, and continue/finish the story of the first game and its protagonist. An unofficial third expansion pack, "Abyss of Pandemonium", was developed by the Impel Development Team, published by Perfect Publishing, and released on April 14, 1998; an updated version, version 2.0, titled "Abyss of Pandemonium – The Final Mission" was released as freeware. An authorized expansion pack, "Q!ZONE" was developed and published by WizardWorks, and released in 1996. In honor of "Quake"'s 20th anniversary, MachineGames, an internal development studio of ZeniMax Media, who are the current owners of the "Quake" IP, released online a new expansion pack for free, called "Episode 5: Dimension of the Past".

"Quake Mission Pack No. 1: Scourge of Armagon" was the first official mission pack, released on February 28, 1997. Developed by Hipnotic Interactive, it features three episodes divided into seventeen new single-player levels (three of which are secret), a new multiplayer level, a new soundtrack composed by Jeehun Hwang, and gameplay features not originally present in "Quake", including rotating structures and breakable walls. Unlike the main "Quake" game and Mission Pack No. 2, "Scourge" does away with the episode hub, requiring the three episodes to be played sequentially. The three new enemies include Centroids, large cybernetic scorpions with nailguns; Gremlins, small goblins that can steal weapons and multiply by feeding on enemy corpses; and Spike Mines, floating orbs that detonate when near the player. The three new weapons include the Mjolnir, a large lightning emitting hammer; the Laser Cannon, which shoots bouncing bolts of energy; and the Proximity Mine Launcher, which fires grenades that attach to surfaces and detonate when an opponent comes near. The three new power-ups include the Horn of Conjuring, which summons an enemy to protect the player; the Empathy Shield, which halves the damage taken by the player between the player and the attacking enemy; and the Wetsuit, which renders the player invulnerable to electricity and allows the player to stay underwater for a period of time. The storyline follows Armagon, a general of Quake's forces, planning to invade Earth via a portal known as the 'Rift'. Armagon resembles a giant gremlin with cybernetic legs and a combined rocket launcher/laser cannon for arms. 

The First Mission Pack received favourable reviews. It holds 82.00% from Gamerankings and gamespot given a score of 8.6/10. 

"Quake Mission Pack No. 2: Dissolution of Eternity" was the second official mission pack, released on March 31, 1997. Developed by Rogue Entertainment, it features two episodes divided into fifteen new single-player levels, a new multiplayer level, a new soundtrack, and several new enemies and bosses. Notably, the pack lacks secret levels. The eight new enemies include Electric Eels, Phantom Swordsmen, Multi-Grenade Ogres (which fire cluster grenades), Hell Spawn, Wraths (floating, robed undead), Guardians (resurrected ancient Egyptian warriors), Mummies, and statues of various enemies that can come to life. The four new types of bosses include Lava Men, Overlords, large Wraths, and a dragon guarding the "temporal energy converter". The two new power-ups include the Anti Grav Belt, which allows the player to jump higher; and the Power Shield, which lowers the damage the player receives. Rather than offering new weapons, the mission pack gives the player four new types of ammo for existing weapons, such as "lava nails" for the Nailgun, cluster grenades for the Grenade Launcher, rockets that split into four in a horizontal line for the Rocket Launcher, and plasma cells for the Thunderbolt, as well as a grappling hook to help with moving around the levels.

The Second Mission Pack also received favourable reviews. It holds 83.50% from Gamerankings and Gamespot given a score of 7.7/10. 

In late 1996, id Software released "VQuake", a port of the "Quake" engine to support hardware accelerated rendering on graphics cards using the Rendition Vérité chipset. Aside from the expected benefit of improved performance, "VQuake" offered numerous visual improvements over the original software-rendered "Quake". It boasted full 16-bit color, bilinear filtering (reducing pixelation), improved dynamic lighting, optional anti-aliasing, and improved source code clarity, as the improved performance finally allowed the use of gotos to be abandoned in favor of proper loop constructs. As the name implied, "VQuake" was a proprietary port specifically for the Vérité; consumer 3D acceleration was in its infancy at the time, and there was no standard 3D API for the consumer market. After completing "VQuake", John Carmack vowed to never write a proprietary port again, citing his frustration with Rendition's Speedy3D API.

To improve the quality of online play, id Software released "QuakeWorld" on December 17, 1996, a build of "Quake" that featured significantly revamped network code including the addition of client-side prediction. The original "Quake" network code would not show the player the results of his actions until the server sent back a reply acknowledging them. For example, if the player attempted to move forward, his client would send the request to move forward to the server, and the server would determine whether the client was actually able to move forward or if he ran into an obstacle, such as a wall or another player. The server would then respond to the client, and only then would the client display movement to the player. This was fine for play on a LAN, a high bandwidth, very low latency connection, but the latency over a dial-up Internet connection is much larger than on a LAN, and this caused a noticeable delay between when a player tried to act and when that action was visible on the screen. This made gameplay much more difficult, especially since the unpredictable nature of the Internet made the amount of delay vary from moment to moment. Players would experience jerky, laggy motion that sometimes felt like ice skating, where they would slide around with seemingly no ability to stop, due to a build-up of previously-sent movement requests. John Carmack has admitted that this was a serious problem which should have been fixed before release, but it was not caught because he and other developers had high-speed Internet access at home.

With the help of client-side prediction, which allowed players to see their own movement immediately without waiting for a response from the server, "QuakeWorld" network code allowed players with high-latency connections to control their character's movement almost as precisely as when playing in single-player mode. The Netcode parameters could be adjusted by the user so that "QuakeWorld" performed well for users with high and low latency.

The trade off to client-side prediction was that sometimes other players or objects would no longer be quite where they had appeared to be, or, in extreme cases, that the player would be pulled back to a previous position when the client received a late reply from the server which overrode movement the client had already previewed; this was known as "warping". As a result, some serious players, particularly in the U.S., still preferred to play online using the original "Quake" engine (commonly called "NetQuake") rather than "QuakeWorld". However, the majority of players, especially those on dial-up connections, preferred the newer network model, and "QuakeWorld" soon became the dominant form of online play. Following the success of "QuakeWorld", client-side prediction has become a standard feature of nearly all real-time online games. As with all other "Quake" upgrades, "QuakeWorld" was released as a free, unsupported add-on to the game and was updated numerous times through 1998.

On January 22, 1997, id Software released "GLQuake". This was designed to use the OpenGL 3D API to access hardware 3D graphics acceleration cards to rasterize the graphics, rather than having the computer's CPU fill in every pixel. In addition to higher framerates for most players, "GLQuake" provided higher resolution modes and texture filtering. "GLQuake" also experimented with reflections, transparent water, and even rudimentary shadows. "GLQuake" came with a driver enabling the subset of OpenGL used by the game to function on the 3dfx "Voodoo Graphics" card, the only consumer-level card at the time capable of running "GLQuake" well. Previously, John Carmack had experimented with a version of Quake specifically written for the Rendition Vérité chip used in the Creative Labs "PCI 3D Blaster" card. This version had met with only limited success, and Carmack decided to write for generic APIs in the future rather than tailoring for specific hardware.

On March 11, 1997, id Software released "WinQuake", a version of the non-OpenGL engine designed to run under Microsoft Windows; the original "Quake" had been written for DOS, allowing for launch from Windows 95, but could not run under Windows NT-based operating systems because it required direct access to hardware. "WinQuake" instead accessed hardware via Win32-based APIs such as DirectSound, DirectInput, and DirectDraw that were supported on Windows 95, Windows NT 4.0 and later releases. Like "GLQuake", "WinQuake" also allowed higher resolution video modes. This removed the last barrier to widespread popularity of the game. In 1998, LBE Systems and Laser-Tron released "Quake: Arcade Tournament Edition" in the arcades in limited quantities.

To celebrate "Quake"'s 20th anniversary, a mission pack was developed by MachineGames and released on June 24, 2016. It features 10 new single-player levels and a new multiplayer level, but does not use new gameplay additions from "Scourge of Armagon" and "Dissolution of Eternity". Chronologically, it is set between the main game and the expansions.

After the departure of Sandy Petersen, the remaining id employees chose to change the thematic direction substantially for "Quake II", making the design more technological and futuristic, rather than maintaining the focus on Lovecraftian fantasy. "Quake 4" followed the design themes of "Quake II", whereas "Quake III Arena" mixed these styles; it had a parallel setting that housed several "id all-stars" from various games as playable characters. The mixed settings occurred because "Quake II" originally began as a separate product line. The id designers fell back on the project's nickname of ""Quake II"" because the game’s fast-paced, tactile feel felt closer to a Quake game than a new franchise. Since any sequel to the original "Quake" had already been vetoed, it became a way of continuing the series without continuing the storyline or setting of the first game. In June 2011, John Carmack made an offhand comment that id Software was considering going back to the "...mixed up Cthulhu-ish Quake 1 world and rebooting [in] that direction." There was also another game released called "Quake Live" which is the latest game in the series. At E3 2016,Quake Champions was announced at the Bethesda press conference. The game will be a multiplayer-only shooter in the style of Quake 3 Arena and will be released exclusively for Windows.

On July 20, 2016, Axel Gneiting, an id Tech employee responsible for implementing the Vulkan rendering path to the id Tech 6 engine used in Doom (2016), released a port called "vkQuake" under the GPLv2.




</doc>
<doc id="25267" url="https://en.wikipedia.org/wiki?curid=25267" title="Quantum field theory">
Quantum field theory

In theoretical physics, quantum field theory (QFT) is the theoretical framework for constructing quantum mechanical models of subatomic particles in particle physics and quasiparticles in condensed matter physics. It is a set of notions and mathematical tools that combines classical fields, special relativity, and quantum mechanics. When combined with the cluster decomposition principle, it may be the "only" way to do so, while retaining the ideas of quantum point particles and locality. QFT was previously believed to be truly fundamental; however, it is now believed, primarily because of the continued failures of quantization of general relativity, to be only a very good low-energy approximation, i.e. an effective field theory, to a more fundamental theory.

QFT treats particles as excited states of an underlying field, so these are called field quanta. In quantum field theory, quantum mechanical interactions among particles are described by interaction terms among the corresponding underlying quantum fields. These interactions are conveniently visualized by Feynman diagrams, which are a formal tool of relativistically covariant perturbation theory, serving to evaluate particle processes.

Even though QFT is an unavoidable consequence of the reconciliation of quantum mechanics with special relativity (), historically, it emerged 
in the 1920s with the quantization of the electromagnetic field (the quantization being based on an analogy with the eigenmode expansion of a vibrating string with fixed endpoints).

The first achievement of quantum field theory, namely quantum electrodynamics (QED), is "still the paradigmatic example of a successful quantum field theory" (). Ordinary quantum mechanics (QM) cannot give an account of photons, which constitute the prime case of relativistic 'particles'. Since photons have rest mass zero, and correspondingly travel in the vacuum at the speed "c", a non-relativistic theory such as ordinary QM cannot give even an approximate description. Photons are implicit in the emission and absorption processes which have to be postulated, for instance, when one of an atom's electrons makes a transition between energy levels. The formalism of QFT is needed for an explicit description of photons. In fact most topics in the early development of quantum theory (the so-called old quantum theory, 1900–25) were related to the interaction of radiation and matter and thus should be treated by quantum field theoretical methods. However, quantum mechanics as formulated by Dirac, Heisenberg, and Schrödinger in 1926–27 started from atomic spectra and did not focus much on problems of radiation.

As soon as the conceptual framework of quantum mechanics was developed, a small group of theoreticians tried to extend quantum methods to electromagnetic fields. A good example is the famous paper by . (P. Jordan was especially acquainted with the literature on light quanta and made seminal contributions to QFT.) The basic idea was that in QFT the electromagnetic field should be represented by matrices in the same way that position and momentum were represented in QM by matrices (matrix mechanics oscillator operators). The ideas of QM were thus extended to systems having an infinite number of degrees of freedom, so an infinite array of quantum oscillators.

The inception of QFT is usually considered to be Dirac's famous 1927 paper on "The quantum theory of the emission and absorption of radiation". Here Dirac coined the name "quantum electrodynamics" (QED) for the part of QFT that was developed first. Dirac supplied a systematic procedure for transferring the characteristic quantum phenomenon of discreteness of physical quantities from the quantum-mechanical treatment of particles to a corresponding treatment of fields. Employing the theory of the quantum harmonic oscillator, Dirac gave a theoretical description of how photons appear in the quantization of the electromagnetic radiation field. Later, Dirac's procedure became a model for the quantization of other fields as well. These first approaches to QFT were further developed during the following three years. P. Jordan introduced creation and annihilation operators for fields obeying Fermi–Dirac statistics. These differ from the corresponding operators for Bose–Einstein statistics in that the former satisfy "anti-commutation relations" while the latter satisfy commutation relations.

The methods of QFT could be applied to derive equations resulting from the quantum-mechanical (field-like) treatment of particles, e.g. the Dirac equation, the Klein–Gordon equation and the Maxwell equations. Schweber points out that the idea and procedure of second quantization goes back to Jordan, in a number of papers from 1927, while the expression itself was coined by Dirac. Some difficult problems concerning commutation relations, statistics, and Lorentz invariance were eventually solved. The first comprehensive account of a general theory of quantum fields, in particular, the method of canonical quantization, was presented by Heisenberg & Pauli in 1929–30. Whereas Jordan's second quantization procedure applied to the coefficients of the normal modes of the field, Heisenberg & Pauli started with the fields themselves and subjected them to the canonical procedure. Heisenberg and Pauli thus established the basic structure of QFT as presented in modern introductions to QFT. Fermi and Dirac, as well as Fock and Podolsky, presented different formulations which played a heuristic role in the following years.

Quantum electrodynamics rests on two pillars, see e.g., the short and lucid "Historical Introduction" of . The first pillar is the quantization of the electromagnetic field, i.e., it is about photons as the quantized excitations or 'quanta' of the electromagnetic field. This procedure will be described in some more detail in the section on the particle interpretation. As Weinberg points out the "photon is the only particle that was known as a field before it was detected as a particle" so that it is natural that QED began with the analysis of the radiation field. The second pillar of QED consists of the relativistic theory of the electron, centered on the Dirac equation.

Quantum field theory started with a theoretical framework that was built in analogy to quantum mechanics. Although there was no unique and fully developed theory, quantum field theoretical tools could be applied to concrete processes. Examples are the scattering of radiation by free electrons, Compton scattering, the collision between relativistic electrons or the production of electron-positron pairs by photons. Calculations to the first order of approximation were quite successful, but most people working in the field thought that QFT still had to undergo a major change. On the one side, some calculations of effects for cosmic rays clearly differed from measurements. On the other side and, from a theoretical point of view more threatening, calculations of higher orders of the perturbation series led to infinite results. The self-energy of the electron as well as vacuum fluctuations of the electromagnetic field seemed to be infinite. The perturbation expansions did not converge to a finite sum and even most individual terms were divergent.

The various forms of infinities suggested that the divergences were more than failures of specific calculations. Many physicists tried to avoid the divergences by formal tricks (truncating the integrals at some value of momentum, or even ignoring infinite terms) but such rules were not reliable, violated the requirements of relativity and were not considered as satisfactory. Others came up with the first ideas for coping with infinities by a redefinition of the parameters of the theory and using a measured finite value, for example of the charge of the electron, instead of the infinite 'bare' value. This process is called renormalization.

From the point of view of the philosophy of science, it is remarkable that these divergences did not give enough reason to discard the theory. The years from 1930 to the beginning of World War II were characterized by a variety of attitudes towards QFT. Some physicists tried to circumvent the infinities by more-or-less arbitrary prescriptions, others worked on transformations and improvements of the theoretical framework. Most of the theoreticians believed that QED would break down at high energies. There was also a considerable number of proposals in favor of alternative approaches. These proposals included changes in the basic concepts e.g. negative probabilities and interactions at a distance instead of a field theoretical approach, and a methodological change to phenomenological methods that focuses on relations between observable quantities without an analysis of the microphysical details of the interaction, the so-called S-matrix theory where the basic elements are amplitudes for various scattering processes.

Despite the feeling that QFT was imperfect and lacking rigor, its methods were extended to new areas of applications. In 1933 Fermi's theory of the beta decay started with conceptions describing the emission and absorption of photons, transferred them to beta radiation and analyzed the creation and annihilation of electrons and neutrinos described by the weak interaction. Further applications of QFT outside of quantum electrodynamics succeeded in nuclear physics with the strong interaction. In 1934 Pauli & Weisskopf showed that a new type of field, the scalar field, described by the Klein–Gordon equation, could be quantized. This is another example of second quantization. This new theory for matter fields could be applied a decade later when new particles, pions, were detected.

After the end of World War II more reliable and effective methods for dealing with infinities in QFT were developed, namely coherent and systematic rules for performing relativistic field theoretical calculations, and a general renormalization theory. At three famous conferences, the Shelter Island Conference 1947, the Pocono Conference 1948, and the 1949 Oldstone Conference, developments in theoretical physics were confronted with relevant new experimental results. In the late forties, there were two different ways to address the problem of divergences. One of these was discovered by Richard Feynman, the other one (based on an operator formalism) by Julian Schwinger and, independently, by Shin'ichirō Tomonaga.

In 1949, Freeman Dyson showed that the two approaches are in fact equivalent and fit into an elegant field-theoretic framework. Thus, Freeman Dyson, Feynman, Schwinger, and Tomonaga became the inventors of renormalization theory. The most spectacular successes of renormalization theory were the calculations of the anomalous magnetic moment of the electron and the Lamb shift in the spectrum of hydrogen. These successes were so outstanding because the theoretical results were in better agreement with high-precision experiments than anything in physics encountered before. Nevertheless, mathematical problems lingered on and prompted a search for rigorous formulations (discussed 
below).

The rationale behind renormalization is to avoid divergences that appear in physical predictions by shifting them into a part of the theory where they do not influence empirical statements. Dyson could show that a rescaling of charge and mass ('renormalization') is sufficient to remove all divergences in QED consistently, to all orders of perturbation theory. A QFT is called renormalizable if all infinities can be absorbed into a redefinition of a "finite number" of coupling constants and masses. A consequence for QED is that the physical charge and mass of the electron must be measured and cannot be computed from first principles.

Perturbation theory yields well-defined predictions only in renormalizable quantum field theories; luckily, QED, the first fully developed QFT, belonged to this class of renormalizable theories. There are various technical procedures to renormalize a theory. One way is to cut off the integrals in the calculations at a certain value of the momentum which is large but finite. This cut-off procedure is successful if, after taking the limit  → ∞, the resulting quantities are independent of .
Feynman's formulation of QED is of special interest from a philosophical point of view. His so-called space-time approach is visualized by the celebrated Feynman diagrams that look like depicting paths of particles. Feynman's method of calculating scattering amplitudes is based on the functional integral formulation of field theory. A set of graphical rules can be derived so that the probability of a specific scattering process can be calculated by drawing a diagram of that process and then using that diagram to write down the precise mathematical expressions for calculating its amplitude in relativistically covariant perturbation theory.

The diagrams provide an effective way to organize and visualize the various terms in the perturbation series, and they naturally account for the flow of electrons and photons during the scattering process. External lines in the diagrams represent incoming and outgoing particles, internal lines are connected with virtual particles and vertices with interactions. Each of these graphical elements is associated with mathematical expressions that contribute to the amplitude of the respective process. The diagrams are part of Feynman's very efficient and elegant algorithm for computing the probability of scattering processes.

The idea of particles traveling from one point to another was heuristically useful in constructing the theory. This heuristic, based on Huygens' principle, is useful for concrete calculations and actually gives the correct particle propagators as derived more rigorously.

Regardless of heuristic merit, an analysis of the theoretical justification of the space-time approach shows that its success does not imply that particle paths need be taken literally. General arguments against a particle interpretation of QFT clearly reject the idea that the diagrams in the interaction area represent actual paths of particles. Feynman himself, however, had little interest for questions of ontology.

In 1933, Enrico Fermi had already established that the creation, annihilation and transmutation of particles in the weak interaction beta decay could best be described in QFT, specifically his quartic fermion interaction. As a result, field theory had become a prospective tool for other particle interactions. In the beginning of the 1950s, QED had become a reliable theory which no longer counted as preliminary. However, it took two decades from writing down the first equations until QFT could be applied successfully to important physical problems in a systematic way.

The theories explored relied on—indeed, were virtually fully specified by—a rich variety of symmetries pioneered and articulated by Murray Gell-Mann. The new developments made it possible to apply QFT to new particles and new interactions and fully explain their structure.

In the following decades, QFT was extended to well-describe not only the electromagnetic force but also weak and strong interaction so that new Lagrangians were found which contain new classes of particles or quantum fields. The search still continues for a more comprehensive theory of matter and energy, a "unified theory of all interactions". 
The new focus on symmetry led to the triumph of non-Abelian gauge theories (the development of such theories was pioneered in 1954–60 with the work of Yang and Mills; see Yang–Mills theory) and spontaneous symmetry breaking (by Yoichiro Nambu). Today, there are reliable theories of the strong, weak, and electromagnetic interactions of elementary particles which have an analogous structure to QED: They are the dominant framework of particle physics.

A combined renormalizable theory associated with the gauge group SU(3) × SU(2) × U(1) is dubbed the "standard model of elementary particle physics" (even though it is a full theory, and not just a model) and was assembled by Sheldon Glashow, Steven Weinberg and Abdus Salam in 1959–67 (see Electroweak unification), and Frank Wilczek, David Gross and David Politzer in 1973 (see Asymptotic freedom), on the basis of conceptual breakthroughs by Peter Higgs, François Englert, Robert Brout, Martin Veltman, and Gerard 't Hooft.
According to the standard model, there are, on the one hand, six types of leptons (e.g. the electron and its neutrino) and six types of quarks, where the members of both groups are all fermions with spin 1/2. On the other hand, there are spin 1 particles (thus bosons) that mediate the interaction between elementary particles and the fundamental forces, namely the photon for electromagnetic interaction, two W and one Z-boson for weak interaction, and the gluons for strong interaction. The linchpin of the symmetry breaking mechanism of the theory is the spin 0 Higgs boson, discovered 40 years after its prediction.

Parallel breakthroughs in the understanding of phase transitions in condensed matter physics led to novel insights based on the renormalization group. They emerged in the work of Leo Kadanoff (1966) and Kenneth Geddes Wilson & Michael Fisher (1972)—extending the work of Ernst Stueckelberg–André Petermann (1953) and Murray Gell-Mann–Francis Low (1954)—which led to the seminal reformulation of quantum field theory by Kenneth Geddes Wilson in 1975.
This reformulation provided insights into the evolution of effective field theories with scale, which classified all field theories, renormalizable or not (cf. subsequent section). The remarkable conclusion is that, in general, most observables are ""irrelevant"", i.e., the macroscopic physics is "dominated by only a few observables" in most systems.

During the same period, Kadanoff (1969) introduced an operator algebra formalism for the two-dimensional Ising model, a widely studied mathematical model of ferromagnetism in statistical physics. This development suggested that quantum field theory describes its scaling limit. Later, there developed the idea that a finite number of generating operators could represent all the correlation functions of the Ising model.

The existence of a much stronger symmetry for the scaling limit of two-dimensional critical systems was suggested by Alexander Belavin, Alexander Polyakov and Alexander Zamolodchikov in 1984, which eventually led to the development of conformal field theory, a special case of quantum field theory, which is presently utilized in different areas of particle physics and condensed matter physics.

The first chapter in is a very good short description of the earlier history of QFT. A detailed account of the historical development of QFT can be found in .

Most theories in standard particle physics are formulated as "relativistic quantum field theories", such as QED, QCD, and the Standard Model. QED, the quantum field-theoretic description of the electromagnetic field, approximately reproduces Maxwell's theory of electrodynamics in the low-energy limit, with small non-linear corrections to the Maxwell equations required due to virtual electron–positron pairs.

In the perturbative approach to quantum field theory, the full field interaction terms are approximated as a perturbative expansion in the number of particles involved. Each term in the expansion can be thought of as forces between particles being mediated by other particles. In QED, the electromagnetic force between two electrons is caused by an exchange of photons. Similarly, intermediate vector bosons mediate the weak force and gluons mediate the strong force in QCD. The notion of a force-mediating particle comes from perturbation theory, and does not make sense in the context of non-perturbative approaches to QFT, such as with bound states.

There is currently no complete quantum theory of the remaining fundamental force, gravity. Many of the proposed theories to describe gravity as a QFT postulate the existence of a graviton particle that mediates the gravitational force. Presumably, the as yet unknown correct quantum field-theoretic treatment of the gravitational field will behave like Einstein's general theory of relativity in the low-energy limit. Quantum field theory of the fundamental forces itself has been postulated to be the low-energy effective field theory limit of a more fundamental theory such as superstring theory.

Quantum electrodynamics (QED) has one electron field and one photon field; quantum chromodynamics (QCD) has one field for each type of quark; and, in condensed matter, there is an atomic displacement field that gives rise to phonon particles. Edward Witten describes QFT as "by far" the most difficult theory in modern physics – "so difficult that nobody fully believed it for 25 years."

Ordinary quantum mechanical systems have a fixed number of particles, with each particle having a finite number of degrees of freedom. In contrast, the excited states of a quantum field can represent any number of particles. This makes quantum field theories especially useful for describing systems where the particle count/number may change over time, a crucial feature of relativistic dynamics. A QFT is thus an organized infinite array of oscillators.

QFT interaction terms are similar in spirit to those between charges with electric and magnetic fields in Maxwell's equations. However, unlike the classical fields of Maxwell's theory, fields in QFT generally exist in quantum superpositions of states and are subject to the laws of quantum mechanics.

Because the fields are continuous quantities over space, there exist excited states with arbitrarily large numbers of particles in them, providing QFT systems with effectively an infinite number of degrees of freedom. Infinite degrees of freedom can easily lead to divergences of calculated quantities (e.g., the quantities become infinite). Techniques such as renormalization of QFT parameters or discretization of spacetime, as in lattice QCD, are often used to avoid such infinities so as to yield physically plausible results.

The gravitational field and the electromagnetic field are the only two fundamental fields in nature that have infinite range and a corresponding classical low-energy limit, which greatly diminishes and hides their "particle-like" excitations. Albert Einstein in 1905, attributed "particle-like" and discrete exchanges of momenta and energy, characteristic of "field quanta", to the electromagnetic field. Originally, his principal motivation was to explain the thermodynamics of radiation. Although the photoelectric effect and Compton scattering strongly suggest the existence of the photon, it might alternatively be explained by a mere quantization of emission; more definitive evidence of the quantum nature of radiation is now taken up into modern quantum optics as in the antibunching effect.

A classical field is a function defined over some region of space and duration in time. Two physical phenomena which are described by classical fields are Newtonian gravitation, described by Newtonian gravitational field g(x, "t"), and classical electromagnetism, described by the electric and magnetic fields E(x, "t") and B(x, "t"). Because such fields can in principle take on distinct values at each point in space, they are said to have infinite degrees of freedom.

Classical field theory does not, however, account for the quantum-mechanical aspects of such physical phenomena. For instance, it is known from quantum mechanics that certain aspects of electromagnetism involve discrete particles—photons—rather than continuous fields. The business of "quantum" field theory is to write down a field that is, like a classical field, a function defined over space and time, but which also accommodates the observations of quantum mechanics. This is a "quantum field".

To write down such a quantum field, one promotes the infinity of classical oscillators representing the modes of the classical fields to quantum harmonic oscillators. They thus become operator-valued functions (actually, distributions). (In its most general formulation, quantum mechanics is a theory of abstract operators (observables) acting on an abstract state space (Hilbert space), where the observables represent physically observable quantities and the state space represents the possible states of the system under study. For instance, the fundamental observables associated with the motion of a single quantum mechanical particle are the position and momentum operators formula_1 and formula_2. Field theory, by sharp contrast, treats "x" as a label, an index of the field rather than as an operator.)

There are two common ways of handling a quantum field: canonical quantization and the path integral formalism. The latter of these is pursued in this article.

Quantum field theory relies on the Lagrangian formalism from classical field theory. This formalism is analogous to the Lagrangian formalism used in classical mechanics to solve for the motion of a particle under the influence of a field. In classical field theory, one writes down a Lagrangian density, formula_3, involving a field, φ(x,"t"), and possibly its first derivatives (∂φ/∂"t" and ∇φ), and then applies a field-theoretic form of the Euler–Lagrange equation. Writing coordinates ("t", x) = ("x", "x", "x", "x") = "x", this form of the Euler–Lagrange equation is
where a sum over μ is performed according to the rules of Einstein notation.

By solving this equation, one arrives at the "equations of motion" of the field. For example, if one begins with the Lagrangian density
and then applies the Euler–Lagrange equation, one obtains the equation of motion
This equation is Newton's law of universal gravitation, expressed in differential form in terms of the gravitational potential φ("t", x) and the mass density ρ("t", x). Despite the nomenclature, the "field" under study is the gravitational potential, φ, rather than the gravitational field, g. Similarly, when classical field theory is used to study electromagnetism, the "field" of interest is the electromagnetic four-potential ("V"/"c", A), rather than the electric and magnetic fields E and B.

Quantum field theory uses this same Lagrangian procedure to determine the equations of motion for quantum fields. These equations of motion are then supplemented by commutation relations derived from the canonical quantization procedure described below, thereby incorporating quantum mechanical effects into the behavior of the field.

In non-relativistic quantum mechanics, a particle (such as an electron or proton) is described by a complex wavefunction, , whose time-evolution is governed by the Schrödinger equation:

There are several shortcomings to the above description of quantum mechanics, which are addressed by quantum field theory. First, it is unclear how to extend quantum mechanics to include the effects of special relativity. Attempted replacements for the Schrödinger equation, such as the Klein–Gordon equation or the Dirac equation, have many unsatisfactory qualities; for instance, they possess energy eigenvalues that extend to –∞, so that there seems to be no easy definition of a ground state. It turns out that such inconsistencies arise from relativistic wavefunctions not having a well-defined probabilistic interpretation in position space, as probability conservation is not a relativistically covariant concept. The second shortcoming, related to the first, is that in quantum mechanics there is no mechanism to describe particle creation and annihilation; this is crucial for describing phenomena such as pair production, which result from the conversion between mass and energy according to the relativistic relation "E" = "mc".

In this section, we will describe a method for constructing a quantum field theory called second quantization. This basically involves choosing a way to index the quantum mechanical degrees of freedom in the space of multiple identical-particle states. It is based on the Hamiltonian formulation of quantum mechanics.

Several other approaches exist, such as the Feynman path integral, which uses a Lagrangian formulation. For an overview of some of these approaches, see the article on quantization.

For simplicity, we will first discuss second quantization for bosons, which form perfectly symmetric quantum states. Let us denote the mutually orthogonal single-particle states which are possible in the system by formula_8 and so on. For example, the 3-particle state with one particle in state formula_9 and two in state formula_10 is

The first step in second quantization is to express such quantum states in terms of occupation numbers, by listing the number of particles occupying each of the single-particle states formula_12 etc. This is simply another way of labelling the states. For instance, the above 3-particle state is denoted as

An "N"-particle state belongs to a space of states describing systems of "N" particles. The next step is to combine the individual "N"-particle state spaces into an extended state space, known as Fock space, which can describe systems of any number of particles. This is composed of the state space of a system with no particles (the so-called vacuum state, written as formula_14), plus the state space of a 1-particle system, plus the state space of a 2-particle system, and so forth. States describing a definite number of particles are known as Fock states: a general element of Fock space will be a linear combination of Fock states. There is a one-to-one correspondence between the occupation number representation and valid boson states in the Fock space.

At this point, the quantum mechanical system has become a quantum field in the sense we described above. The field's elementary degrees of freedom are the occupation numbers, and each occupation number is indexed by a number formula_15 indicating which of the single-particle states formula_16 it refers to:

The properties of this quantum field can be explored by defining creation and annihilation operators, which add and subtract particles. They are analogous to ladder operators in the quantum harmonic oscillator problem, which added and subtracted energy quanta. However, these operators literally create and annihilate particles of a given quantum state. The bosonic annihilation operator formula_18 and creation operator formula_19 are easily defined in the occupation number representation as having the following effects:

It can be shown that these are operators in the usual quantum mechanical sense, i.e. linear operators acting on the Fock space. Furthermore, they are indeed Hermitian conjugates, which justifies the way we have written them. They can be shown to obey the commutation relation

where formula_23 stands for the Kronecker delta. These are precisely the relations obeyed by the ladder operators for an infinite set of independent quantum harmonic oscillators, one for each single-particle state. Adding or removing bosons from each state is, therefore, analogous to exciting or de-exciting a quantum of energy in a harmonic oscillator.

Applying an annihilation operator formula_24 followed by its corresponding creation operator formula_25 returns the number formula_26 of particles in the "k" single-particle eigenstate:
The combination of operators formula_28 is known as the number operator for the "k" eigenstate.

The Hamiltonian operator of the quantum field (which, through the Schrödinger equation, determines its dynamics) can be written in terms of creation and annihilation operators. For instance, for a field of free (non-interacting) bosons, the total energy of the field is found by summing the energies of the bosons in each energy eigenstate. If the "k" single-particle energy eigenstate has energy formula_29 and there are formula_26 bosons in this state, then the total energy of these bosons is formula_31. The energy in the "entire" field is then a sum over formula_32:
This can be turned into the Hamiltonian operator of the field by replacing formula_26 with the corresponding number operator, formula_28. This yields

It turns out that a different definition of creation and annihilation must be used for describing fermions. According to the Pauli exclusion principle, fermions cannot share quantum states, so their occupation numbers "N" can only take on the value 0 or 1. The fermionic annihilation operators "c" and creation operators formula_37 are defined by their actions on a Fock state thus

These obey an anticommutation relation:

One may notice from this that applying a fermionic creation operator twice gives zero, so it is impossible for the particles to share single-particle states, in accordance with the exclusion principle.

We have previously mentioned that there can be more than one way of indexing the degrees of freedom in a quantum field. Second quantization indexes the field by enumerating the single-particle quantum states. However, as we have discussed, it is more natural to think about a "field", such as the electromagnetic field, as a set of degrees of freedom indexed by position.

To this end, we can define "field operators" that create or destroy a particle at a particular point in space. In particle physics, these operators turn out to be more convenient to work with, because they make it easier to formulate theories that satisfy the demands of relativity.

Single-particle states are usually enumerated in terms of their momenta (as in the particle in a box problem.) We can construct field operators by applying the Fourier transform to the creation and annihilation operators for these states. For example, the bosonic field annihilation operator formula_43 is

The bosonic field operators obey the commutation relation

where formula_46 stands for the Dirac delta function. As before, the fermionic relations are the same, with the commutators replaced by anticommutators.

The field operator is not the same thing as a single-particle wavefunction. The former is an operator acting on the Fock space, and the latter is a quantum-mechanical amplitude for finding a particle in some position. However, they are closely related and are indeed commonly denoted with the same symbol. If we have a Hamiltonian with a space representation, say

where the indices "i" and "j" run over all particles, then the field theory Hamiltonian (in the non-relativistic limit and for negligible self-interactions) is

This looks remarkably like an expression for the expectation value of the energy, with formula_49 playing the role of the wavefunction. This relationship between the field operators and wave functions makes it very easy to formulate field theories starting from space projected Hamiltonians.

Once the Hamiltonian operator is obtained as part of the canonical quantization process, the time dependence of the state is described with the Schrödinger equation, just as with other quantum theories. 
Alternatively, the Heisenberg picture can be used where the time dependence is in the operators rather than in the states.

Probability amplitudes of observables in such systems are quite hard to evaluate, an enterprise which has absorbed considerable ingenuity in the last three quarters of a century. In practice, most often, expectation values of operators are computed systematically through "covariant perturbation theory", formulated through Feynman diagrams, but path integral computer simulations have also produced important results. Contemporary particle physics relies on extraordinarily accurate predictions of such techniques.

The "second quantization" procedure outlined in the previous section takes a set of single-particle quantum states as a starting point. Sometimes, it is impossible to define such single-particle states, and one must proceed directly to quantum field theory. For example, a quantum theory of the electromagnetic field "must" be a quantum field theory, because it is impossible (for various reasons) to define a wavefunction for a single photon. In such situations, the quantum field theory can be constructed by examining the mechanical properties of the classical field and guessing the corresponding quantum theory. For free (non-interacting) quantum fields, the quantum field theories obtained in this way have the same properties as those obtained using second quantization, such as well-defined creation and annihilation operators obeying commutation or anticommutation relations.

Quantum field theory thus provides a unified framework for describing "field-like" objects (such as the electromagnetic field, whose excitations are photons) and "particle-like" objects (such as electrons, which are treated as excitations of an underlying electron field), so long as one can treat interactions as "perturbations" of free fields.

The second quantization procedure relies crucially on the particles being identical. We would not have been able to construct a quantum field theory from a distinguishable many-particle system, because there would have been no way of separating and indexing the degrees of freedom.

Many physicists prefer to take the converse interpretation, which is that "quantum field theory explains what identical particles are". In ordinary quantum mechanics, there is not much theoretical motivation for using symmetric (bosonic) or antisymmetric (fermionic) states, and the need for such states is simply regarded as an empirical fact. From the point of view of quantum field theory, particles are identical if and only if they are excitations of the same underlying quantum field. Thus, the question "why are all electrons identical?" arises from mistakenly regarding individual electrons as fundamental objects, when in fact it is only the electron field that is fundamental.

During second quantization, we started with a Hamiltonian and state space describing a fixed number of particles ("N"), and ended with a Hamiltonian and state space for an arbitrary number of particles. Of course, in many common situations "N" is an important and perfectly well-defined quantity, e.g. if we are describing a gas of atoms sealed in a box. From the point of view of quantum field theory, such situations are described by quantum states that are eigenstates of the number operator formula_50, which measures the total number of particles present. As with any quantum mechanical observable, formula_50 is conserved if it commutes with the Hamiltonian. In that case, the quantum state is trapped in the "N"-particle subspace of the total Fock space, and the situation could equally well be described by ordinary "N"-particle quantum mechanics. (Strictly speaking, this is only true in the noninteracting case or in the low energy density limit of renormalized quantum field theories.)

For example, we can see that the free boson Hamiltonian described above conserves particle number. Whenever the Hamiltonian operates on a state, each particle destroyed by an annihilation operator formula_24 is immediately put back by the creation operator formula_25.

On the other hand, it is possible, and indeed common, to encounter quantum states that are "not" eigenstates of formula_50, which do not have well-defined particle numbers. Such states are difficult or impossible to handle using ordinary quantum mechanics, but they can be easily described in quantum field theory as quantum superpositions of states having different values of "N". For example, suppose we have a bosonic field whose particles can be created or destroyed by interactions with a fermionic field. The Hamiltonian of the combined system would be given by the Hamiltonians of the free boson and free fermion fields, plus a "potential energy" term such as

where formula_25 and formula_24 denotes the bosonic creation and annihilation operators, formula_58 and formula_59 denotes the fermionic creation and annihilation operators, and formula_60 is a parameter that describes the strength of the interaction. This "interaction term" describes processes in which a fermion in state "k" either absorbs or emits a boson, thereby being kicked into a different eigenstate formula_61. (In fact, this type of Hamiltonian is used to describe the interaction between conduction electrons and phonons in metals. The interaction between electrons and photons is treated in a similar way, but is a little more complicated because the role of spin must be taken into account.) One thing to notice here is that even if we start out with a fixed number of bosons, we will typically end up with a superposition of states with different numbers of bosons at later times. The number of fermions, however, is conserved in this case.

In condensed matter physics, states with ill-defined particle numbers are particularly important for describing the various superfluids. Many of the defining characteristics of a superfluid arise from the notion that its quantum state is a superposition of states with different particle numbers. In addition, the concept of a coherent state (used to model the laser and the BCS ground state) refers to a state with an ill-defined particle number but a well-defined phase.

Beyond the most general features of quantum field theories, special aspects such as renormalizability, gauge symmetry, and supersymmetry are outlined below.

Early in the history of quantum field theory, as detailed above, it was found that many seemingly innocuous calculations, such as the perturbative shift in the energy of an electron due to the presence of the electromagnetic field, yield infinite results. The reason is that the perturbation theory for the shift in an energy involves a sum over "all other energy levels, and there are infinitely many levels" at short distances, so that each gives a finite contribution which results in a divergent series.

Many of these problems are related to failures in classical electrodynamics that were identified but unsolved in the 19th century, and they basically stem from the fact that many of the supposedly "intrinsic" properties of an electron are tied to the electromagnetic field that it carries around with it. The energy carried by a single electron—its self-energy—is not simply the bare value, but also includes the energy contained in its electromagnetic field, its attendant cloud of photons. The energy in a field of a spherical source diverges in both classical and quantum mechanics, but as discovered by Weisskopf with help from Furry, in quantum mechanics "the divergence is much milder", going "only as the logarithm" of the radius of the sphere.

The solution to the problem, presciently suggested by Stueckelberg, independently by Bethe after the crucial experiment by Lamb and Retherford (the Lamb–Retherford experiment), implemented at one loop by Schwinger, and systematically extended to all loops by Feynman and Dyson, with converging work by Tomonaga in isolated postwar Japan, comes from recognizing that all the infinities in the interactions of photons and electrons can be isolated into redefining a finite number of quantities in the equations by replacing them with the observed values: specifically the electron's mass and charge: this is called renormalization. The technique of renormalization recognizes that the problem is tractable and essentially purely mathematical; and that, physically, extremely short distances are at fault.

In order to define a theory on a continuum, one may first place a cutoff on the fields, by postulating that quanta cannot have energies above some extremely high value. This has the effect of replacing continuous space by a structure where very short wavelengths do not exist, as on a lattice. Lattices break rotational symmetry, and one of the crucial contributions made by Feynman, Pauli and Villars, and modernized by 't Hooft and Veltman, is a symmetry-preserving cutoff for perturbation theory (this process is called regularization). There is no known symmetrical cutoff outside of perturbation theory, so for rigorous or numerical work people often use an actual lattice.

On a lattice, every quantity is finite but depends on the spacing. When taking the limit to zero spacing, one makes sure that the physically observable quantities like the observed electron mass stay fixed, which means that the constants in the Lagrangian defining the theory depend on the spacing. By allowing the constants to vary with the lattice spacing, all the results at long distances become insensitive to the lattice, defining a continuum limit.

The renormalization procedure only works for a certain limited class of quantum field theories, called "renormalizable quantum field theories". A theory is "perturbatively renormalizable" when the constants in the Lagrangian only diverge at worst as "logarithms" of the lattice spacing for very short spacings. The continuum limit is then well defined in perturbation theory, and even if it is not fully well defined non-perturbatively, the problems only show up at distance scales that are exponentially small in the inverse coupling for weak couplings. The Standard Model of particle physics is perturbatively renormalizable, and so are its component theories (quantum electrodynamics/electroweak theory and quantum chromodynamics). Of the three components, quantum electrodynamics is believed to not have a continuum limit by itself, while the asymptotically free "SU(2)" and "SU(3)" weak and strong color interactions are nonperturbatively well defined.

The renormalization group as developed along Wilson's breakthrough insights relates effective field theories at a given scale to such at contiguous scales. It thus describes how renormalizable theories emerge as the long distance low-energy effective field theory for any given high-energy theory. As a consequence, renormalizable theories are insensitive to the precise nature of the underlying high-energy short-distance phenomena (the macroscopic physics is dominated by only a few ""relevant" observables"). This is a blessing in practical terms, because it allows physicists to formulate low energy theories without detailed knowledge of high-energy phenomena. It is also a curse, because once a renormalizable theory such as the standard model is found to work, it provides very few clues to higher-energy processes.

The only way high-energy processes can be seen in the standard model is when they allow otherwise forbidden events, or else if they reveal predicted compelling quantitative relations among the coupling constants of the theories or models.

On account of renormalization, the couplings of QFT vary with scale, thereby confining quarks into hadrons, allowing the study of weakly-coupled quarks inside hadrons, and enabling speculation on ultra-high energy behavior.

A gauge theory is a theory that admits a symmetry with a local parameter. For example, in every quantum theory, the global phase of the wave function is arbitrary and does not represent something physical. Consequently, the theory is invariant under a global change of phases (adding a constant to the phase of all wave functions, everywhere); this is a global symmetry. In quantum electrodynamics, the theory is also invariant under a "local" change of phase, that is – one may shift the phase of all wave functions so that the shift may be different at every point in space-time. This is a "local" symmetry. However, in order for a well-defined derivative operator to exist, one must introduce a new field, the gauge field, which also transforms in order for the local change of variables (the phase in our example) not to affect the derivative. In quantum electrodynamics, this gauge field is the electromagnetic field. The change of local gauge of variables is termed gauge transformation.

By Noether's theorem, for every such symmetry there exists an associated conserved current. The aforementioned symmetry of the wavefunction under global phase changes implies the conservation of electric charge. Since the excitations of fields represent particles, the particle associated with excitations of the gauge field is the gauge boson, e.g., the photon in the case of quantum electrodynamics.

The degrees of freedom in quantum field theory are local fluctuations of the fields. The existence of a gauge symmetry reduces the number of degrees of freedom, simply because some fluctuations of the fields can be transformed to zero by gauge transformations, so they are equivalent to having no fluctuations at all, and they, therefore, have no physical meaning. Such fluctuations are usually called "non-physical degrees of freedom" or "gauge artifacts"; usually, some of them have a negative norm, making them inadequate for a consistent theory. Therefore, if a classical field theory has a gauge symmetry, then its quantized version (the corresponding quantum field theory) will have this symmetry as well. In other words, a gauge symmetry cannot have a quantum anomaly.

In general, the gauge transformations of a theory consist of several different transformations, which may not be commutative. These transformations are combine into the framework of a gauge group; infinitesimal gauge transformations are the gauge group generators. Thus, the number of gauge bosons is the group dimension (i.e., the number of generators forming the basis of the corresponding Lie algebra).

All the known fundamental interactions in nature are described by gauge theories (possibly barring the Higgs multiplet couplings, if considered in isolation). These are:

Supersymmetry assumes that every fundamental fermion has a superpartner that is a boson and vice versa. Its gauge theory, Supergravity, is an extension of general relativity. Supersymmetry is a key ingredient for the consistency of string theory.

It was utilized in order to solve the so-called Hierarchy Problem of the standard model, that is, to explain why particles not protected by any symmetry (like the Higgs boson) do not receive radiative corrections to their mass, driving it to the larger scales such as that of GUTs, or the Planck mass of gravity. The way supersymmetry protects scale hierarchies is the following: since for every particle there is a superpartner with the same mass but different statistics, any loop in a radiative correction is cancelled by the loop corresponding to its superpartner, rendering the theory more UV finite.

Since, however, no super partners have been observed, if supersymmetry existed it should be broken severely (through a so-called soft term, which breaks supersymmetry without ruining its helpful features). The simplest models of this breaking require that the energy of the superpartners not be too high; in these cases, supersymmetry could be observed by experiments at the Large Hadron Collider. However, to date, after the observation of the Higgs boson there, no such superparticles have been discovered.

The preceding description of quantum field theory follows the spirit in which most physicists approach the subject. However, it is not mathematically rigorous. Over the past several decades, there have been many attempts to put quantum field theory on a firm mathematical footing by formulating a set of axioms for it. Finding proper axioms for quantum field theory is still an open and difficult problem in mathematics. One of the Millennium Prize Problems—proving the existence of a mass gap in Yang–Mills theory—is linked to this issue. These attempts fall into two broad classes.

The first class of axioms, first proposed during the 1950s, include the Wightman, Osterwalder–Schrader, and Haag–Kastler systems. They attempted to formalize the physicists' notion of an "operator-valued field" within the context of functional analysis and enjoyed limited success. It was possible to prove that any quantum field theory satisfying these axioms satisfied certain general theorems, such as the spin-statistics theorem and the CPT theorem. Unfortunately, it proved extraordinarily difficult to show that any realistic field theory, including the Standard Model, satisfied these axioms. Most of the theories that could be treated with these analytic axioms were physically trivial, being restricted to low-dimensions and lacking interesting dynamics. The construction of theories satisfying one of these sets of axioms falls in the field of constructive quantum field theory. Important work was done in this area in the 1970s by Segal, Glimm, Jaffe and others.

During the 1980s, the second set of axioms based on topological ideas was proposed. Before 1980 all states of matter could be classified by geometry and the principle of broken symmetry. For example Einstein's theory of general relativity is based on the geometrical curvature of space and time, while crystals, magnets and superconductors can all be classified by the symmetries they break. In 1980 the quantum Hall effect provided the first example of a state of matter that has no spontaneous broken symmetry; its characterization is dependant on its topology and not on its geometry (See geometry v. topology). The quantum Hall effect can be described by extending quantum field theory into an effective topological quantum field theory based on the Chern–Simons theory. This line of investigation, which extends quantum field theories to topological quantum field theories, is associated most closely with Michael Atiyah and Graeme Segal, and was notably expanded upon by Edward Witten, Richard Borcherds, and Maxim Kontsevich. The main impact of topological quantum field theory has been in condensed matter physics where physicists have observed exotic quasiparticles such as magnetic monopoles and Majorana fermions.
Topological field considerations could have radical applications in a new form of electronics called spintronics and topological quantum computers. The Standard Model allows for topological terms but is generally not formulated as a topological quantum field theory. Topological quantum field theory has also had broad impact in mathematics, with important applications in representation theory, algebraic topology, and differential geometry.

From a mathematically rigorous perspective, there exists no interaction picture in a Lorentz-covariant quantum field theory. This implies that the perturbative approach of Feynman diagrams in QFT is not strictly justified, despite producing vastly precise predictions validated by experiment. This is called Haag's theorem, but most particle physicists relying on QFT largely shrug it off, as it is not really limiting the power of the theory.








Articles



</doc>
<doc id="25268" url="https://en.wikipedia.org/wiki?curid=25268" title="Quantum electrodynamics">
Quantum electrodynamics

In particle physics, quantum electrodynamics (QED) is the relativistic quantum field theory of electrodynamics. In essence, it describes how light and matter interact and is the first theory where full agreement between quantum mechanics and special relativity is achieved. QED mathematically describes all phenomena involving electrically charged particles interacting by means of exchange of photons and represents the quantum counterpart of classical electromagnetism giving a complete account of matter and light interaction.

In technical terms, QED can be described as a perturbation theory of the electromagnetic quantum vacuum. Richard Feynman called it "the jewel of physics" for its extremely accurate predictions of quantities like the anomalous magnetic moment of the electron and the Lamb shift of the energy levels of hydrogen.

The first formulation of a quantum theory describing radiation and matter interaction is attributed to British scientist Paul Dirac, who (during the 1920s) was able to compute the coefficient of spontaneous emission of an atom.

Dirac described the quantization of the electromagnetic field as an ensemble of harmonic oscillators with the introduction of the concept of creation and annihilation operators of particles. In the following years, with contributions from Wolfgang Pauli, Eugene Wigner, Pascual Jordan, Werner Heisenberg and an elegant formulation of quantum electrodynamics due to Enrico Fermi, physicists came to believe that, in principle, it would be possible to perform any computation for any physical process involving photons and charged particles. However, further studies by Felix Bloch with Arnold Nordsieck, and Victor Weisskopf, in 1937 and 1939, revealed that such computations were reliable only at a first order of perturbation theory, a problem already pointed out by Robert Oppenheimer. At higher orders in the series infinities emerged, making such computations meaningless and casting serious doubts on the internal consistency of the theory itself. With no solution for this problem known at the time, it appeared that a fundamental incompatibility existed between special relativity and quantum mechanics.
Difficulties with the theory increased through the end of the 1940s. Improvements in microwave technology made it possible to take more precise measurements of the shift of the levels of a hydrogen atom, now known as the Lamb shift and magnetic moment of the electron. These experiments exposed discrepancies which the theory was unable to explain.

A first indication of a possible way out was given by Hans Bethe in 1947, after attending the Shelter Island Conference. While he was traveling by train from the conference to Schenectady he made the first non-relativistic computation of the shift of the lines of the hydrogen atom as measured by Lamb and Retherford. Despite the limitations of the computation, agreement was excellent. The idea was simply to attach infinities to corrections of mass and charge that were actually fixed to a finite value by experiments. In this way, the infinities get absorbed in those constants and yield a finite result in good agreement with experiments. This procedure was named renormalization.

Based on Bethe's intuition and fundamental papers on the subject by Shin'ichirō Tomonaga, Julian Schwinger, Richard Feynman and Freeman Dyson, it was finally possible to get fully covariant formulations that were finite at any order in a perturbation series of quantum electrodynamics. Shin'ichirō Tomonaga, Julian Schwinger and Richard Feynman were jointly awarded with a Nobel prize in physics in 1965 for their work in this area. Their contributions, and those of Freeman Dyson, were about covariant and gauge invariant formulations of quantum electrodynamics that allow computations of observables at any order of perturbation theory. Feynman's mathematical technique, based on his diagrams, initially seemed very different from the field-theoretic, operator-based approach of Schwinger and Tomonaga, but Freeman Dyson later showed that the two approaches were equivalent. Renormalization, the need to attach a physical meaning at certain divergences appearing in the theory through integrals, has subsequently become one of the fundamental aspects of quantum field theory and has come to be seen as a criterion for a theory's general acceptability. Even though renormalization works very well in practice, Feynman was never entirely comfortable with its mathematical validity, even referring to renormalization as a "shell game" and "hocus pocus".

QED has served as the model and template for all subsequent quantum field theories. One such subsequent theory is quantum chromodynamics, which began in the early 1960s and attained its present form in the 1970s work by H. David Politzer, Sidney Coleman, David Gross and Frank Wilczek. Building on the pioneering work of Schwinger, Gerald Guralnik, Dick Hagen, and Tom Kibble, Peter Higgs, Jeffrey Goldstone, and others, Sheldon Glashow, Steven Weinberg and Abdus Salam independently showed how the weak nuclear force and quantum electrodynamics could be merged into a single electroweak force.

Near the end of his life, Richard P. Feynman gave a series of lectures on QED intended for the lay public. These lectures were transcribed and published as Feynman (1985), "QED: The strange theory of light and matter", a classic non-mathematical exposition of QED from the point of view articulated below.

The key components of Feynman's presentation of QED are three basic actions.
These actions are represented in the form of visual shorthand by the three basic elements of Feynman diagrams: a wavy line for the photon, a straight line for the electron and a junction of two straight lines and a wavy one for a vertex representing emission or absorption of a photon by an electron. These can all be seen in the adjacent diagram.

It is important not to over-interpret these diagrams. Nothing is implied about "how" a particle gets from one point to another. The diagrams do "not" imply that the particles are moving in straight or curved lines. They do "not" imply that the particles are moving with constant speeds. The fact that the photon is often represented, by convention, by a wavy line and not a straight one does "not" imply that it is thought that it is more wavelike than is an electron. The images are just symbols to represent the actions above: photons and electrons do, somehow, move from point to point and electrons, somehow, emit and absorb photons. The theory does not explain how these things happen, but it does tell us the probabilities of these things happening in various situations.

As well as the visual shorthand for the actions Feynman introduces another kind of shorthand for the numerical quantities called probability amplitudes. The probability is the square of the absolute value of total probability amplitude, formula_1. If a photon moves from one place and time formula_2 to another place and time formula_3, the associated quantity is written in Feynman's shorthand as formula_4. The similar quantity for an electron moving from formula_5 to formula_6 is written formula_7. The quantity that tells us about the probability amplitude for the emission or absorption of a photon he calls "j". This is related to, but not the same as, the measured electron charge "e".

QED is based on the assumption that complex interactions of many electrons and photons can be represented by fitting together a suitable collection of the above three building blocks and then using the probability amplitudes to calculate the probability of any such complex interaction. It turns out that the basic idea of QED can be communicated while assuming that the square of the total of the probability amplitudes mentioned above ("P"("A" to "B"), "E"("C" to "D") and "j") acts just like our everyday probability (a simplification made in Feynman's book). Later on, this will be corrected to include specifically quantum-style mathematics, following Feynman.

The basic rules of probability amplitudes that will be used are:

Suppose, we start with one electron at a certain place and time (this place and time being given the arbitrary label "A") and a photon at another place and time (given the label "B"). A typical question from a physical standpoint is: "What is the probability of finding an electron at "C" (another place and a later time) and a photon at "D" (yet another place and time)?". The simplest process to achieve this end is for the electron to move from "A" to "C" (an elementary action) and for the photon to move from "B" to "D" (another elementary action). From a knowledge of the probability amplitudes of each of these sub-processes – "E"("A" to "C") and "P"("B" to "D") – we would expect to calculate the probability amplitude of both happening together by multiplying them, using rule b) above. This gives a simple estimated overall probability amplitude, which is squared to give an estimated probability.

But there are other ways in which the end result could come about. The electron might move to a place and time "E", where it absorbs the photon; then move on before emitting another photon at "F"; then move on to "C", where it is detected, while the new photon moves on to "D". The probability of this complex process can again be calculated by knowing the probability amplitudes of each of the individual actions: three electron actions, two photon actions and two vertexes – one emission and one absorption. We would expect to find the total probability amplitude by multiplying the probability amplitudes of each of the actions, for any chosen positions of "E" and "F". We then, using rule a) above, have to add up all these probability amplitudes for all the alternatives for "E" and "F". (This is not elementary in practice and involves integration.) But there is another possibility, which is that the electron first moves to "G", where it emits a photon, which goes on to "D", while the electron moves on to "H", where it absorbs the first photon, before moving on to "C". Again, we can calculate the probability amplitude of these possibilities (for all points "G" and "H"). We then have a better estimation for the total probability amplitude by adding the probability amplitudes of these two possibilities to our original simple estimate. Incidentally, the name given to this process of a photon interacting with an electron in this way is Compton scattering.

There is an "infinite number" of other intermediate processes in which more and more photons are absorbed and/or emitted. For each of these possibilities, there is a Feynman diagram describing it. This implies a complex computation for the resulting probability amplitudes, but provided it is the case that the more complicated the diagram, the less it contributes to the result, it is only a matter of time and effort to find as accurate an answer as one wants to the original question. This is the basic approach of QED. To calculate the probability of "any" interactive process between electrons and photons, it is a matter of first noting, with Feynman diagrams, all the possible ways in which the process can be constructed from the three basic elements. Each diagram involves some calculation involving definite rules to find the associated probability amplitude.

That basic scaffolding remains when one moves to a quantum description, but some conceptual changes are needed. One is that whereas we might expect in our everyday life that there would be some constraints on the points to which a particle can move, that is "not" true in full quantum electrodynamics. There is a possibility of an electron at "A", or a photon at "B", moving as a basic action to "any other place and time in the universe". That includes places that could only be reached at speeds greater than that of light and also "earlier times". (An electron moving backwards in time can be viewed as a positron moving forward in time.)

Quantum mechanics introduces an important change in the way probabilities are computed. Probabilities are still represented by the usual real numbers we use for probabilities in our everyday world, but probabilities are computed as the square of probability amplitudes, which are complex numbers.

Feynman avoids exposing the reader to the mathematics of complex numbers by using a simple but accurate representation of them as arrows on a piece of paper or screen. (These must not be confused with the arrows of Feynman diagrams, which are simplified representations in two dimensions of a relationship between points in three dimensions of space and one of time.) The amplitude arrows are fundamental to the description of the world given by quantum theory. No satisfactory reason has been given for "why" they are needed. But pragmatically we have to accept that they are an essential part of our description of all quantum phenomena. They are related to our everyday ideas of probability by the simple rule that the probability of an event is the "square" of the length of the corresponding amplitude arrow. So, for a given process, if two probability amplitudes, v and w, are involved, the probability of the process will be given either by

or

The rules as regards adding or multiplying, however, are the same as above. But where you would expect to add or multiply probabilities, instead you add or multiply probability amplitudes that now are complex numbers.

Addition and multiplication are common operations in the theory of complex numbers and are given in the figures. The sum is found as follows. Let the start of the second arrow be at the end of the first. The sum is then a third arrow that goes directly from the beginning of the first to the end of the second. The product of two arrows is an arrow whose length is the product of the two lengths. The direction of the product is found by adding the angles that each of the two have been turned through relative to a reference direction: that gives the angle that the product is turned relative to the reference direction.

That change, from probabilities to probability amplitudes, complicates the mathematics without changing the basic approach. But that change is still not quite enough because it fails to take into account the fact that both photons and electrons can be polarized, which is to say that their orientations in space and time have to be taken into account. Therefore, "P"("A" to "B") consists of 16 complex numbers, or probability amplitude arrows. There are also some minor changes to do with the quantity "j", which may have to be rotated by a multiple of 90° for some polarizations, which is only of interest for the detailed bookkeeping.

Associated with the fact that the electron can be polarized is another small necessary detail, which is connected with the fact that an electron is a fermion and obeys Fermi–Dirac statistics. The basic rule is that if we have the probability amplitude for a given complex process involving more than one electron, then when we include (as we always must) the complementary Feynman diagram in which we exchange two electron events, the resulting amplitude is the reverse – the negative – of the first. The simplest case would be two electrons starting at "A" and "B" ending at "C" and "D". The amplitude would be calculated as the "difference", , where we would expect, from our everyday idea of probabilities, that it would be a sum.

Finally, one has to compute "P"("A" to "B") and "E"("C" to "D") corresponding to the probability amplitudes for the photon and the electron respectively. These are essentially the solutions of the Dirac equation, which describe the behavior of the electron's probability amplitude and the Klein–Gordon equation, which describes the behavior of the photon's probability amplitude. These are called Feynman propagators. The translation to a notation commonly used in the standard literature is as follows:

where a shorthand symbol such as formula_11 stands for the four real numbers that give the time and position in three dimensions of the point labeled "A".

A problem arose historically which held up progress for twenty years: although we start with the assumption of three basic "simple" actions, the rules of the game say that if we want to calculate the probability amplitude for an electron to get from "A" to "B", we must take into account "all" the possible ways: all possible Feynman diagrams with those endpoints. Thus there will be a way in which the electron travels to "C", emits a photon there and then absorbs it again at "D" before moving on to "B". Or it could do this kind of thing twice, or more. In short, we have a fractal-like situation in which if we look closely at a line, it breaks up into a collection of "simple" lines, each of which, if looked at closely, are in turn composed of "simple" lines, and so on "ad infinitum". This is a challenging situation to handle. If adding that detail only altered things slightly, then it would not have been too bad, but disaster struck when it was found that the simple correction mentioned above led to "infinite" probability amplitudes. In time this problem was "fixed" by the technique of renormalization. However, Feynman himself remained unhappy about it, calling it a "dippy process".

Within the above framework physicists were then able to calculate to a high degree of accuracy some of the properties of electrons, such as the anomalous magnetic dipole moment. However, as Feynman points out, it fails to explain why particles such as the electron have the masses they do. "There is no theory that adequately explains these numbers. We use the numbers in all our theories, but we don't understand them – what they are, or where they come from. I believe that from a fundamental point of view, this is a very interesting and serious problem."

Mathematically, QED is an abelian gauge theory with the symmetry group U(1). The gauge field, which mediates the interaction between the charged spin-1/2 fields, is the electromagnetic field.
The QED Lagrangian for a spin-1/2 field interacting with the electromagnetic field is given in natural units by the real part of
where

Substituting the definition of "D" into the Lagrangian gives

From this Lagrangian, the equations of motion for the "ψ" and "A" fields can be obtained.

Using the field-theoretic Euler–Lagrange equation for "ψ",

The derivatives of the Lagrangian concerning "ψ" are

Inserting these into () results in

with Hermitian conjugate

Bringing the middle term to the right-hand side yields

The left-hand side is like the original Dirac equation, and the right-hand side is the interaction with the electromagnetic field.

Using the Euler–Lagrange equation for the "A" field,

the derivatives this time are

Substituting back into () leads to

Now, if we impose the Lorenz gauge condition

the equations reduce to

which is a wave equation for the four-potential, the QED version of the classical Maxwell equations in the Lorenz gauge. (The square represents the D'Alembert operator, formula_28.)

This theory can be straightforwardly quantized by treating bosonic and fermionic sectors as free. This permits us to build a set of asymptotic states that can be used to start computation of the probability amplitudes for different processes. In order to do so, we have to compute an evolution operator, which for a given initial state formula_29 will give a final state formula_30 in such a way to have

This technique is also known as the S-matrix. The evolution operator is obtained in the interaction picture, where time evolution is given by the interaction Hamiltonian, which is the integral over space of the second term in the Lagrangian density given above:

and so, one has

where "T" is the time-ordering operator. This evolution operator only has meaning as a series, and what we get here is a perturbation series with the fine-structure constant as the development parameter. This series is called the Dyson series.

Despite the conceptual clarity of this Feynman approach to QED, almost no early textbooks follow him in their presentation. When performing calculations, it is much easier to work with the Fourier transforms of the propagators. Experimental tests of quantum electrodynamics are typically scattering experiments. In scattering theory, particles momenta rather than their positions are considered, and it is convenient to think of particles as being created or annihilated when they interact. Feynman diagrams then "look" the same, but the lines have different interpretations. The electron line represents an electron with a given energy and momentum, with a similar interpretation of the photon line. A vertex diagram represents the annihilation of one electron and the creation of another together with the absorption or creation of a photon, each having specified energies and momenta.

Using Wick theorem on the terms of the Dyson series, all the terms of the S-matrix for quantum electrodynamics can be computed through the technique of Feynman diagrams. In this case, rules for drawing are the following

To these rules we must add a further one for closed loops that implies an integration on momenta formula_34, since these internal ("virtual") particles are not constrained to any specific energy–momentum, even that usually required by special relativity (see Propagator for details).

From them, computations of probability amplitudes are straightforwardly given. An example is Compton scattering, with an electron and a photon undergoing elastic scattering. Feynman diagrams are in this case

and so we are able to get the corresponding amplitude at the first order of a perturbation series for the S-matrix:

from which we can compute the cross section for this scattering.

Higher-order terms can be straightforwardly computed for the evolution operator, but these terms display diagrams containing the following simpler ones

that, being closed loops, imply the presence of diverging integrals having no mathematical meaning. To overcome this difficulty, a technique called renormalization has been devised, producing finite results in very close agreement with experiments. It is important to note that a criterion for the theory being meaningful after renormalization is that the number of diverging diagrams is finite. In this case, the theory is said to be "renormalizable". The reason for this is that to get observables renormalized, one needs a finite number of constants to maintain the predictive value of the theory untouched. This is exactly the case of quantum electrodynamics displaying just three diverging diagrams. This procedure gives observables in very close agreement with experiment as seen e.g. for electron gyromagnetic ratio.

Renormalizability has become an essential criterion for a quantum field theory to be considered as a viable one. All the theories describing fundamental interactions, except gravitation, whose quantum counterpart is presently under very active research, are renormalizable theories.

An argument by Freeman Dyson shows that the radius of convergence of the perturbation series in QED is zero. The basic argument goes as follows: if the coupling constant were negative, this would be equivalent to the Coulomb force constant being negative. This would "reverse" the electromagnetic interaction so that "like" charges would "attract" and "unlike" charges would "repel". This would render the vacuum unstable against decay into a cluster of electrons on one side of the universe and a cluster of positrons on the other side of the universe. Because the theory is "sick" for any negative value of the coupling constant, the series does not converge but are at best an asymptotic series.

From a modern perspective, we say that QED is not well defined as a quantum field theory to arbitrarily high energy. The coupling constant runs to infinity at finite energy, signalling a Landau pole. The problem is essentially that QED appears to suffer from quantum triviality issues. This is one of the motivations for embedding QED within a Grand Unified Theory.




</doc>
<doc id="25270" url="https://en.wikipedia.org/wiki?curid=25270" title="Quine (computing)">
Quine (computing)

A quine is a non-empty computer program which takes no input and produces a copy of its own source code as its only output. The standard terms for these programs in the computability theory and computer science literature are "self-replicating programs", "self-reproducing programs", and "self-copying programs".

A quine is a fixed point of an execution environment, when the execution environment is viewed as a function transforming programs into their outputs. Quines are possible in any Turing complete programming language, as a direct consequence of Kleene's recursion theorem. For amusement, programmers sometimes attempt to develop the shortest possible quine in any given programming language.

The name "quine" was coined by Douglas Hofstadter, in his popular science book "", in honor of philosopher Willard Van Orman Quine (1908–2000), who made an extensive study of indirect self-reference, and in particular for the following paradox-producing expression, known as Quine's paradox:
"Yields falsehood when preceded by its quotation" yields falsehood when preceded by its quotation.
In some languages, particularly scripting languages, an empty source file is a fixed point of the language, being a valid program that produces no output. Such an empty program, submitted as "the world's smallest self reproducing program", once won the "worst abuse of the rules" prize in the International Obfuscated C Code Contest.

The idea of self-reproducing automata came from the dawn of computing, if not before. John von Neumann theorized about them in the 1940s. Later, Paul Bratley and Jean Millo's article "Computer Recreations: Self-Reproducing Automata" discussed them in 1972.
Bratley first became interested in self-reproducing programs after seeing the first known such program written in Atlas Autocode at Edinburgh in the 1960s by the University of Edinburgh lecturer and researcher Hamish Dewar.

The "download source" requirement of the Affero General Public License is based on the idea of a quine.

In general, the method used to create a quine in any programming language is to have, within the program, two pieces: (a) code used to do the actual printing and (b) data that represents the textual form of the code. The code functions by using the data to print the code (which makes sense since the data represents the textual form of the code), but it also uses the data, processed in a simple way, to print the textual representation of the data itself.

The following Java code demonstrates the basic structure of a quine.
public class Quine
The source code contains a string array of itself, which is output twice, once inside quotation marks.

The same idea is used in the following SQL quine:

SELECT REPLACE(REPLACE('SELECT REPLACE(REPLACE("$",CHAR(34),CHAR(39)),CHAR(36),"$") AS Quine',CHAR(34),CHAR(39)),CHAR(36),'SELECT REPLACE(REPLACE("$",CHAR(34),CHAR(39)),CHAR(36),"$") AS Quine') AS Quine
A very concise quine with the same basic structure can be written in Lua:

x = "x = [" .. "[" .. x .. "]" .. "]\nprint(" .. x)
print("x = [" .. "[" .. x .. "]" .. "]\nprint(" .. x)

And in Python:
s = 's = %r\nprint(s%%s)'
print(s%s)

As a one-liner in Matlab:

s='disp(char([115,61,39,s,39,59,s]));';disp(char([115,61,39,s,39,59,s]));
As a one-liner in C:

Actually another shorter one:

In SAS: 

SAS Metadata Program:
/* Assign Macro Variables */
%Let DSN=sashelp.cars ;
%Let Var=Type ;
%Let Option_1=nopct ;
%Let Option_2=nocum ;

/* Assign Output Fileref */
Filename outpgm 'E:\Data-driven Program.sas' ;

/* DATA Step to Produce External SAS Program */
DATA _NULL_ ;
RUN ;

Results:
/* Substitute Variable Names with Macro Variables */
title Display NLevels for Type with PROC FREQ ;
proc freq data=sashelp.cars nlevels ;
run ;
In Scala:

object Quine extends App {

In R:

m<-"m<-0;cat(sub(0,deparse(m),m))";cat(sub(0,deparse(m),m))
In Rust:

fn main() {

In Go:

package main

import "fmt"

func main() {

var s = `package main

import "fmt"

func main() {

var s = `
In OCaml:
(fun s -> Printf.printf "%s%S;;\n" s s) "(fun s -> Printf.printf \"%s%S;;\\n\" s s) ";;
In Haskell:
main = putStr s » print s where s = "main = putStr s » print s where s = "
Quines can take advantage of codice_1. For example, this Ruby quine:

eval s="print 'eval s=';p s"

Quines, per definition, cannot receive "any" form of input, including reading a file, which means a quine is considered to be "cheating" if it looks at its own source code. The following shell script is not a quine:

cat $0
Nor is this succinct use of the Shebang:

The above also applies to this JavaScript code:

function a() {
a()
In PHP:
<? highlight_file(__FILE__) ?>

In Python:
print(open(__file__).read())

A program in the joke language HQ9+ is a quine if and only if the source code consists only of zero or more '+' characters and a single 'Q' character (the 'Q' command prints a quine and '+' prints nothing):

One of the shortest cheating quines is provided by Forth, which has a built-in command to read its own source:

An even shorter cheating quine exists on many 1980s home computers (such as the ZX Spectrum and the Commodore 64) as follows:

Another even shorter cheating quine in bash:
Other questionable techniques include making use of compiler messages; for example, in the GW-BASIC environment, entering "Syntax Error" will cause the interpreter to respond with "Syntax Error". Ignoring the restriction that quines be non-empty, there are many examples of programming languages where an empty program is valid (such as C). Such programs generally do nothing, in effect, reproducing the program.

The quine concept can be extended to multiple levels or recursion, originating "ouroboros programs", or quine-relays. This should not be confused with Multiquines.

This Java program outputs the source for a C++ program that outputs the original Java code.
Such programs have been produced with various cycle lengths:


David Madore, creator of Unlambda, describes multiquines as follows:
"A multiquine is a set of r different programs (in r different languages — without this condition we could take them all equal to a single quine), each of which is able to print any of the r programs (including itself) according to the command line argument it is passed. (Note that cheating is not allowed: the command line arguments must not be too long — passing the full text of a program is considered cheating)."
A multiquine consisting of 2 languages (or biquine) would be a program which:

A biquine could then be seen as a set of two programs, both of which are able to print either of the two, depending on the command line argument supplied.

Theoretically, there is no limit on the number of languages in a multiquine,
a 5-part multiquine (or pentaquine) has been produced with Python, Perl, C, NewLISP, and F#
and there is also a 25-language multiquine.

A radiation-hardened quine is a quine that can have any single character removed and still produce the original program with no missing character. Of necessity, such quines are much more convoluted than ordinary quines, as is seen by the following example in Ruby:
eval='eval$q=%q(puts %q(10210/#{1 1 if 1==21}}/.i rescue##/

1 1"[13,213].max_by{|s|s.size}#"##").gsub(/\d/){["=\47eval$q=%q(#$q)#\47##\47

exit)#'##'

instance_eval='eval$q=%q(puts %q(10210/#{1 1 if 1==21}}/.i rescue##/

1 1"[13,213].max_by{|s|s.size}#"##").gsub(/\d/){["=\47eval$q=%q(#$q)#\47##\47

exit)#'##'

/#{eval eval if eval==instance_eval}}/.i rescue##/

eval eval"[eval||=9,instance_eval||=9].max_by{|s|s.size}#"##"



</doc>
<doc id="25271" url="https://en.wikipedia.org/wiki?curid=25271" title="Field of fractions">
Field of fractions

In abstract algebra, the field of fractions of an integral domain is the smallest field in which it can be embedded. The elements of the field of fractions of the integral domain formula_1 are equivalence classes (see the construction below) written as formula_2 with formula_3 and formula_4 in formula_1 and formula_6. The field of fractions of formula_1 is sometimes denoted by formula_8 or formula_9.

Mathematicians refer to this construction as the field of fractions, fraction field, field of quotients, or quotient field. All four are in common usage. The expression "quotient field" may sometimes run the risk of confusion with the quotient of a ring by an ideal, which is a quite different concept.


Let formula_1 be any integral domain. For formula_17 with formula_18, the fraction formula_19 denotes the equivalence class of pairs formula_20, where formula_20 is equivalent to formula_22 if and only if formula_23.
The "field of fractions" formula_8 is defined as the set of all such fractions formula_19.
The sum of formula_19 and formula_29 is defined as formula_30, and the product of formula_19 and formula_29 is defined as formula_33 (one checks that these are well defined).

The embedding of formula_1 in formula_8 maps each formula_36 in formula_1 to the fraction formula_38 for any nonzero formula_39 (the equivalence class is independent of the choice formula_40). This is modelled on the identity formula_41.

The field of fractions of formula_1 is characterised by the following universal property: if formula_43 is an injective ring homomorphism from formula_1 into a field formula_45, then there exists a unique ring homomorphism formula_46 which extends formula_47.

There is a categorical interpretation of this construction. Let formula_48 be the category of integral domains and injective ring maps. The functor from formula_48 to the category of fields which takes every integral domain to its fraction field and every homomorphism to the induced map on fields (which exists by the universal property) is the left adjoint of the forgetful functor from the category of fields to formula_48.

A multiplicative identity is not required for the role of the integral domain; this construction can be applied to any nonzero commutative rng formula_1 with no nonzero zero divisors. The embedding is given by formula_52 for any nonzero formula_53.

For any commutative ring formula_1 and any multiplicative set formula_55 in formula_1, the localization formula_55formula_1 is the commutative ring consisting of fractions formula_59 with formula_60 and formula_61,
where now formula_62 is equivalent to formula_63 if and only if there exists formula_64 such that formula_65.
Two special cases of this are notable:



</doc>
<doc id="25272" url="https://en.wikipedia.org/wiki?curid=25272" title="Quadratic reciprocity">
Quadratic reciprocity

In number theory, the law of quadratic reciprocity is a theorem about modular arithmetic that gives conditions for the solvability of quadratic equations modulo prime numbers. Due to its subtlety, it has many formulations, but the most standard statement is: 
.</math>

This law allows the easy calculation of any Legendre symbol, making it possible to determine whether there is an integer solution for any quadratic equation of the form formula_1 for "p" an odd prime; that is, to determine the "perfect squares" mod "p". However, this is a non-constructive result: it gives no help at all for "finding" a specific solution; for this, one uses quadratic residues.

The theorem was conjectured by Euler and Legendre and first proved by Gauss. He refers to it as the "fundamental theorem" in the "Disquisitiones Arithmeticae" and his papers, writing

Privately he referred to it as the "golden theorem." He published six proofs, and two more were found in his posthumous papers. There are now over 240 published proofs.

Since Gauss, generalizing the reciprocity law has been a leading problem in mathematics, and has been crucial to the development of much of the machinery of modern algebra, number theory, and algebraic geometry, culminating in Artin reciprocity, class field theory, and the Langlands program. 

Quadratic reciprocity arises from certain subtle factorization patterns involving perfect square numbers. In this section, we give examples which lead to the general case. 

Consider the polynomial formula_3 and its values for formula_4 The prime factorizations of these values are given as follows:

The prime factors formula_5 dividing formula_6 are formula_7, and every prime whose final digit is formula_8 or formula_9; no primes ending in formula_10 or formula_11 ever appear. Now, formula_5 is a prime factor of some formula_13 whenever formula_14, i.e. whenever formula_15, i.e. whenever 5 is a quadratic residue modulo formula_5. This happens for formula_17 and those primes with formula_18, and note that the latter numbers formula_19 and formula_20 are precisely the quadratic residues modulo formula_21. Therefore, except for formula_22, we have that formula_21 is a quadratic residue modulo formula_5 iff formula_5 is a quadratic residue modulo formula_21.

The law of quadratic reciprocity gives a similar characterization of prime divisors of formula_27 for any prime "q", which leads to a characterization for any integer formula_28.

Let "p" be an odd prime. A number modulo "p" is a quadratic residue whenever it is congruent to a square (mod "p"); otherwise it is a quadratic nonresidue. ("Quadratic" can be dropped if it is clear from context.) Here we exclude zero as a special case. Then we have:


Quadratic residues are entries in the following table: 

This table is complete for odd primes less than 50. To check whether a number "m" is a quadratic residue mod one of these primes "p", find "a" ≡ "m" (mod "p") and 0 ≤ "a" < "p". If "a" is in row "p", then "m" is a residue (mod "p"); if "a" is not in row "p" of the table, then "m" is a nonresidue (mod "p").

The quadratic reciprocity law is the statement that certain patterns found in the table are true in general.

Trivially 1 is a quadratic residue for all primes. The question becomes more interesting for −1. Examining the table, we find −1 in rows 5, 13, 17, 29, 37, and 41 but not in rows 3, 7, 11, 19, 23, 31, 43 or 47. The former set of primes are all congruent to 1 modulo 4, and the latter are congruent to 3 modulo 4.

Examining the table, we find 2 in rows 7, 17, 23, 31, 41, and 47, but not in rows 3, 5, 11, 13, 19, 29, 37, or 43. The former primes are all ≡ ±1 (mod 8), and the latter are all ≡ ±3 (mod 8). This leads to

−2 is in rows 3, 11, 17, 19, 41, 43, but not in rows 5, 7, 13, 23, 29, 31, 37, or 47. The former are ≡ 1 or ≡ 3 (mod 8), and the latter are ≡ 5, 7 (mod 8).

3 is in rows 11, 13, 23, 37, and 47, but not in rows 5, 7, 17, 19, 29, 31, 41, or 43. The former are ≡ ±1 (mod 12) and the latter are all ≡ ±5 (mod 12).

−3 is in rows 7, 13, 19, 31, 37, and 43 but not in rows 5, 11, 17, 23, 29, 41, or 47. The former are ≡ 1 (mod 3) and the latter ≡ 2 (mod 3).

Since the only residue (mod 3) is 1, we see that −3 is a quadratic residue modulo every prime which is a residue modulo 3.

5 is in rows 11, 19, 29, 31, and 41 but not in rows 3, 7, 13, 17, 23, 37, 43, or 47. The former are ≡ ±1 (mod 5) and the latter are ≡ ±2 (mod 5).

Since the only residues (mod 5) are ±1, we see that 5 is a quadratic residue modulo every prime which is a residue modulo 5.

−5 is in rows 3, 7, 23, 29, 41, 43, and 47 but not in rows 11, 13, 17, 19, 31, or 37. The former are ≡ 1, 3, 7, 9 (mod 20) and the latter are ≡ 11, 13, 17, 19 (mod 20).

The observations about −3 and 5 continue to hold: −7 is a residue modulo "p" if and only if "p" is a residue modulo 7, −11 is a residue modulo "p" if and only if "p" is a residue modulo 11, 13 is a residue (mod "p") if and only if "p" is a residue modulo 13, etc. The more complicated-looking rules for the quadratic characters of 3 and −5, which depend upon congruences modulo 12 and 20 respectively, are simply the ones for −3 and 5 working with the first supplement.

The generalization of the rules for −3 and 5 is Gauss's statement of quadratic reciprocity.

Another way to organize the data is to see which primes are residues mod which other primes, as illustrated in the following table. The entry in row "p" column "q" is R if "q" is a quadratic residue (mod "p"); if it is a nonresidue the entry is N.

If the row, or the column, or both, are ≡ 1 (mod 4) the entry is blue or green; if both row and column are ≡ 3 (mod 4), it is yellow or orange.

The blue and green entries are symmetric around the diagonal: The entry for row "p", column "q" is R (resp N) if and only if the entry at row "q", column "p", is R (resp N).

The yellow and orange ones, on the other hand, are antisymmetric: The entry for row "p", column "q" is R (resp N) if and only if the entry at row "q", column "p", is N (resp R).

The reciprocity law states that these patterns hold for all "p" and "q".

Quadratic Reciprocity (Gauss's Statement). If formula_33 then the congruence formula_34 is solvable if and only if formula_35 is solvable. If formula_36 then the congruence formula_34 is solvable if and only if formula_38 is solvable.

Quadratic Reciprocity (Combined Statement). Define formula_39. Then the congruence formula_34 is solvable if and only if formula_41 is solvable.

Quadratic Reciprocity (Legendre's Statement). If "p" or "q" are congruent to 1 modulo 4, then: formula_35 is solvable if and only if formula_34 is solvable. If "p" and "q" are congruent to 3 modulo 4, then: formula_35 is solvable if and only if formula_34 is not solvable.

The last is immediately equivalent to the modern form stated in the introduction above. It is a simple exercise to prove that Legendre's and Gauss's statements are equivalent – it requires no more than the first supplement and the facts about multiplying residues and nonresidues.

The theorem was formulated in many ways before its modern form: Euler and Legendre did not have Gauss's congruence notation, nor did Gauss have the Legendre symbol.

In this article "p" and "q" always refer to distinct positive odd primes, and "x" and "y" to unspecified integers.

Fermat proved (or claimed to have proved) a number of theorems about expressing a prime by a quadratic form:

He did not state the law of quadratic reciprocity, although the cases −1, ±2, and ±3 are easy deductions from these and other of his theorems.

He also claimed to have a proof that if the prime number "p" ends with 7, (in base 10) and the prime number "q" ends in 3, and "p" ≡ "q" ≡ 3 (mod 4), then

Euler conjectured, and Lagrange proved, that

Proving these and other statements of Fermat was one of the things that led mathematicians to the reciprocity theorem.

Translated into modern notation, Euler stated:


This is equivalent to quadratic reciprocity.

He could not prove it, but he did prove the second supplement.

Fermat proved that if "p" is a prime number and "a" is an integer, 

Thus, if "p" does not divide "a", 

Legendre lets "a" and "A" represent positive primes ≡ 1 (mod 4) and "b" and "B" positive primes ≡ 3 (mod 4), and sets out a table of eight theorems that together are equivalent to quadratic reciprocity:

He says that since expressions of the form

will come up so often he will abbreviate them as:

This is now known as the Legendre symbol, and an equivalent definition is used today: for all integers "a" and all odd primes "p"

He notes that these can be combined:

A number of proofs, especially those based on Gauss's Lemma, explicitly calculate this formula.

Legendre's attempt to prove reciprocity is based on a theorem of his:

Example. Theorem I is handled by letting "a" ≡ 1 and "b" ≡ 3 (mod 4) be primes and assuming that formula_59 and, contrary the theorem, that formula_60 Then formula_61 has a solution, and taking congruences (mod 4) leads to a contradiction.

This technique doesn't work for Theorem VIII. Let "b" ≡ "B" ≡ 3 (mod 4), and assume

Then if there is another prime "p" ≡ 1 (mod 4) such that

the solvability of formula_64 leads to a contradiction (mod 4). But Legendre was unable to prove there has to be such a prime "p"; he was later able to show that all that is required is:

but he couldn't prove that either. Hilbert symbol (below) discusses how techniques based on the existence of solutions to formula_66 can be made to work.

Gauss first proves the supplementary laws. He sets the basis for induction by proving the theorem for ±3 and ±5. Noting that it is easier to state for −3 and +5 than it is for +3 or −5, he states the general theorem in the form:

Introducing the notation "a" R "b" (resp. "a" N "b") to mean "a" is a quadratic residue (resp. nonresidue) (mod "b"), and letting "a", "a"′, etc. represent positive primes ≡ 1 (mod 4) and "b", "b"′, etc. positive primes ≡ 3 (mod 4), he breaks it out into the same 8 cases as Legendre:

In the next Article he generalizes this to what are basically the rules for the Jacobi symbol (below). Letting "A", "A"′, etc. represent any (prime or composite) positive numbers ≡ 1 (mod 4) and "B", "B"′, etc. positive numbers ≡ 3 (mod 4):

All of these cases take the form "if a prime is a residue (mod a composite), then the composite is a residue or nonresidue (mod the prime), depending on the congruences (mod 4)". He proves that these follow from cases 1) - 8).

Gauss needed, and was able to prove, a lemma similar to the one Legendre needed:

The proof of quadratic reciprocity uses complete induction.

These can be combined:

A number of proofs of the theorem, especially those based on Gauss sums derive this formula. or the splitting of primes in algebraic number fields,

Note that the statements in this section are equivalent to quadratic reciprocity: if, for example, Euler's version is assumed, the Legendre-Gauss version can be deduced from it, and vice versa.

This can be proven using Gauss's lemma.

Gauss's fourth proof consists of proving this theorem (by comparing two formulas for the value of Gauss sums) and then restricting it to two primes. He then gives an example: Let "a" = 3, "b" = 5, "c" = 7, and "d" = 11. Three of these, 3, 7, and 11 ≡ 3 (mod 4), so "m" ≡ 3 (mod 4). 5×7×11 R 3; 3×7×11 R 5; 3×5×11 R 7;  and  3×5×7 N 11, so there are an odd number of nonresidues.

The Jacobi symbol is a generalization of the Legendre symbol; the main difference is that the bottom number has to be positive and odd, but does not have to be prime. If it is prime, the two symbols agree. It obeys the same rules of manipulation as the Legendre symbol. In particular

and if both numbers are positive and odd (this is sometimes called "Jacobi's reciprocity law"):

However, if the Jacobi symbol is 1 but the denominator is not a prime, it does not necessarily follow that the numerator is a quadratic residue of the denominator. Gauss's cases 9) - 14) above can be expressed in terms of Jacobi symbols:

and since "p" is prime the left hand side is a Legendre symbol, and we know whether "M" is a residue modulo "p" or not.

The formulas listed in the preceding section are true for Jacobi symbols as long as the symbols are defined. Euler's formula may be written

Example.

2 is a residue modulo the primes 7, 23 and 31:

But 2 is not a quadratic residue modulo 5, so it can't be one modulo 15. This is related to the problem Legendre had: if formula_84 then "a" is a non-residue modulo every prime in the arithmetic progression "m" + 4"a", "m" + 8"a", ..., if there "are" any primes in this series, but that wasn't proved until decades after Legendre.

Eisenstein's formula requires relative primality conditions (which are true if the numbers are prime)

The quadratic reciprocity law can be formulated in terms of the Hilbert symbol formula_88 where "a" and "b" are any two nonzero rational numbers and "v" runs over all the non-trivial absolute values of the rationals (the archimedean one and the "p"-adic absolute values for primes "p"). The Hilbert symbol formula_88 is 1 or −1. It is defined to be 1 if and only if the equation formula_90 has a solution in the completion of the rationals at "v" other than formula_91. The Hilbert reciprocity law states that formula_88, for fixed "a" and "b" and varying "v", is 1 for all but finitely many "v" and the product of formula_88 over all "v" is 1. (This formally resembles the residue theorem from complex analysis.)

The proof of Hilbert reciprocity reduces to checking a few special cases, and the non-trivial cases turn out to be equivalent to the main law and the two supplementary laws of quadratic reciprocity for the Legendre symbol. There is no kind of reciprocity in the Hilbert reciprocity law; its name simply indicates the historical source of the result in quadratic reciprocity. Unlike quadratic reciprocity, which requires sign conditions (namely positivity of the primes involved) and a special treatment of the prime 2, the Hilbert reciprocity law treats all absolute values of the rationals on an equal footing. Therefore, it is a more natural way of expressing quadratic reciprocity with a view towards generalization: the Hilbert reciprocity law extends with very few changes to all global fields and this extension can rightly be considered a generalization of quadratic reciprocity to all global fields.

The early proofs of quadratic reciprocity are relatively unilluminating. The situation changed when Gauss used Gauss sums to show that quadratic fields are subfields of cyclotomic fields, and implicitly deduced quadratic reciprocity from a reciprocity theorem for cyclotomic fields. His proof was cast in modern form by later algebraic number theorists. This proof served as a template for class field theory, which can be viewed as a vast generalization of quadratic reciprocity.

Robert Langlands formulated the Langlands program, which gives a conjectural vast generalization of class field theory. He wrote:

There are also quadratic reciprocity laws in rings other than the integers.

In his second monograph on quartic reciprocity Gauss stated quadratic reciprocity for the ring formula_94 of Gaussian integers, saying that it is a corollary of the biquadratic law in formula_95 but did not provide a proof of either theorem. Dirichlet showed that the law in formula_94 can be deduced from the law for formula_97 without using biquadratic reciprocity.

For an odd Gaussian prime formula_98 and a Gaussian integer formula_99 relatively prime to formula_100 define the quadratic character for formula_94 by:

Let formula_103 be distinct Gaussian primes where "a" and "c" are odd and "b" and "d" are even. Then

Consider the following third root of unity:

The ring of Eisenstein integers is formula_106 For an Eisenstein prime formula_107 and an Eisenstein integer formula_99 with formula_109 define the quadratic character for formula_110 by the formula

Let λ = "a" + "bω" and μ = "c" + "dω" be distinct Eisenstein primes where "a" and "c" are not divisible by 3 and "b" and "d" are divisible by 3. Eisenstein proved

The above laws are special cases of more general laws that hold for the ring of integers in any imaginary quadratic number field. Let "k" be an imaginary quadratic number field with ring of integers formula_113 For a prime ideal formula_114 with odd norm formula_115 and formula_116 define the quadratic character for formula_117 as

for an arbitrary ideal formula_119 factored into prime ideals formula_120 define

and for formula_122 define

Let formula_124 i.e. formula_125 is an integral basis for formula_113 For formula_127 with odd norm formula_128 define (ordinary) integers "a", "b", "c", "d" by the equations,

and a function

If "m" = "Nμ" and "n" = "Nν" are both odd, Herglotz proved

Also, if

Then

Let "F" be a finite field with "q" = "p" elements, where "p" is an odd prime number and "n" is positive, and let "F"["x"] be the ring of polynomials in one variable with coefficients in "F". If formula_134 and "f" is irreducible, monic, and has positive degree, define the quadratic character for "F"["x"] in the usual manner:

If formula_136 is a product of monic irreducibles let

Dedekind proved that if formula_134 are monic and have positive degrees,

The attempt to generalize quadratic reciprocity for powers higher than the second was one of the main goals that led 19th century mathematicians, including Carl Friedrich Gauss, Peter Gustav Lejeune Dirichlet, Carl Gustav Jakob Jacobi, Gotthold Eisenstein, Richard Dedekind, Ernst Kummer, and David Hilbert to the study of general algebraic number fields and their rings of integers; specifically Kummer invented ideals in order to state and prove higher reciprocity laws.

The ninth in the list of 23 unsolved problems which David Hilbert proposed to the Congress of Mathematicians in 1900 asked for the 
"Proof of the most general reciprocity law [f]or an arbitrary number field". In 1923 Emil Artin, building upon work by Philipp Furtwängler, Teiji Takagi, Helmut Hasse and others, discovered a general theorem for which all known reciprocity laws are special cases; he proved it in 1927.

The links below provide more detailed discussions of these theorems.

The "Disquisitiones Arithmeticae" has been translated (from Latin) into English and German. The German edition includes all of Gauss's papers on number theory: all the proofs of quadratic reciprocity, the determination of the sign of the Gauss sum, the investigations into biquadratic reciprocity, and unpublished notes. Footnotes referencing the "Disquisitiones Arithmeticae" are of the form "Gauss, DA, Art. "n"".

The two monographs Gauss published on biquadratic reciprocity have consecutively numbered sections: the first contains §§ 1–23 and the second §§ 24–76. Footnotes referencing these are of the form "Gauss, BQ, § "n"". 

These are in Gauss's "Werke", Vol II, pp. 65–92 and 93–148. German translations are in pp. 511–533 and 534–586 of "Untersuchungen über höhere Arithmetik."

Every textbook on elementary number theory (and quite a few on algebraic number theory) has a proof of quadratic reciprocity. Two are especially noteworthy:

Franz Lemmermeyer's "Reciprocity Laws: From Euler to Eisenstein" has "many" proofs (some in exercises) of both quadratic and higher-power reciprocity laws and a discussion of their history. Its immense bibliography includes literature citations for 196 different published proofs for the quadratic reciprocity law.

Kenneth Ireland and Michael Rosen's "A Classical Introduction to Modern Number Theory" also has many proofs of quadratic reciprocity (and many exercises), and covers the cubic and biquadratic cases as well. Exercise 13.26 (p. 202) says it all



</doc>
<doc id="25274" url="https://en.wikipedia.org/wiki?curid=25274" title="Quantum information">
Quantum information

In physics and computer science, quantum information is information that is held in the state of a quantum system. Quantum information is the basic entity of study in quantum information theory, and can be manipulated using engineering techniques known as quantum information processing. Much like classical information can be processed with digital computers, transmitted from place to place, manipulated with algorithms, and analyzed with the mathematics of computer science, so also analogous concepts apply to quantum information. While the fundamental unit of classical information is the bit, in quantum information it is the qubit.

Quantum information differs strongly from classical information, epitomized by the bit, in many striking and unfamiliar ways. Among these are the following:


The study of all of the above topics and differences comprises quantum information theory.

Quantum mechanics studies how microscopic physical systems change dynamically in nature. In the field of quantum information theory, the quantum systems studied are abstracted away from any real world counterpart. A qubit might for instance physically be a photon in a linear optical quantum computer, an ion in a trapped ion quantum computer, or it might be a large collection of atoms as in a superconducting quantum computer. Regardless of the physical implementation, the limits and features of qubits implied by quantum information theory hold as all these systems are all mathematically described by the same apparatus of density matrices over the complex numbers. Another important difference with quantum mechanics is that, while quantum mechanics often studies infinite-dimensional systems such as a harmonic oscillator, quantum information theory concerns itself primarily with finite-dimensional systems.

Many journals publish research in quantum information science, although only a few are dedicated to this area. Among these are 



</doc>
<doc id="25275" url="https://en.wikipedia.org/wiki?curid=25275" title="Quinolone">
Quinolone

Quinolone may refer to:


</doc>
<doc id="25277" url="https://en.wikipedia.org/wiki?curid=25277" title="Quarterback">
Quarterback

A quarterback (commonly abbreviated "QB") is a position in American and Canadian football. Quarterbacks are members of the offensive team and line up directly behind the offensive line. In modern American football, the quarterback is usually considered the leader of the offensive team, and is often responsible for calling the play in the huddle.

In modern American football, the quarterback is usually the leader of the offense. The quarterback touches the ball on almost every offensive play, and his successes and failures can have a significant impact on the fortunes of his team. Accordingly, the quarterback is among the most glorified and scrutinized positions in team sports. Prior to each play, the quarterback will usually tell the rest of his team which play the team will run. After the team is lined up, the center will pass the ball back to the quarterback (a process called the snap). Usually on a running play, the quarterback will then hand or pitch the ball backwards to a halfback or fullback. On a passing play, the quarterback is almost always the player responsible for trying to throw the ball downfield to an eligible receiver downfield. Additionally, the quarterback will often run with the football himself, which could be part of a designed play like the option run or quarterback sneak, or it could be an effort to avoid being sacked by the defense.

Depending on the offensive scheme by his team, the quarterback's role can vary. In systems like the triple option the quarterback will only pass the ball a few times per game, if at all, while the pass-heavy spread offense as run by schools like Texas Tech requires quarterbacks to throw the ball in most plays. The passing game is emphasized heavily in the Canadian Football League (CFL), where there are only three downs as opposed to the four downs used in American football, a larger field of play and an extra eligible receiver. Different skillsets are required of the quarterback in each system - quarterbacks that perform well in a pass-heavy spread offensive system, a popular offensive scheme in the NCAA and NFHS, rarely perform well in the National Football League (NFL), as the fundamentals of the pro-style offense used in the NFL are very different from those in the spread system. while quarterbacks in Canadian football need to be able to throw the ball often and accurately. In general, quarterbacks need to have physical skills such as arm strength, mobility, and quick throwing motion, in addition to intangibles such as competitiveness, leadership, intelligence, and downfield vision.

In the NFL, quarterbacks are required to wear a uniform number between 1 and 19. In the National Collegiate Athletic Association (NCAA) and National Federation of State High School Associations (NFHS), quarterbacks are required to wear a uniform number between 1 and 49; in the NFHS, the quarterback can also wear a number between 80 and 89. In the CFL, the quarterback can wear any number from 0 to 49 and 70 to 99. Because of their numbering, quarterbacks are eligible receivers in the NCAA, NFHS, and CFL; in the NFL, quarterbacks are eligible receivers if they are not lined up directly under center.

In the NFL, while the starting quarterback has no other responsibility or authority, he may, depending on the league or individual team, have various informal duties, such as participation in pre-game ceremonies, the coin toss, or other events outside the game.

Often compared to captains of other team sports, before the implementation of NFL team captains in 2007, the starting quarterback was usually the "de facto" team leader and well-respected player on and off the field. Since 2007, when the NFL allowed teams to designate several captains to serve as on-field leaders, the starting quarterback has usually been one of the team captains as the leader of the team's offense.

After a Super Bowl victory, the starting quarterback is the first player (and third person after the team owner and head coach) to be presented with the Vince Lombardi Trophy. The starting quarterback of the victorious Super Bowl team is often chosen for the "I'm going to Disney World!" campaign (which includes a trip to Walt Disney World for them and their families), whether they are the Super Bowl MVP or not; examples include Joe Montana (XXIII), Trent Dilfer (XXXV), and Peyton Manning (50). Dilfer was chosen even though teammate Ray Lewis was the MVP of Super Bowl XXXV, due to the bad publicity from Lewis' murder trial the prior year.

In addition to their main role, quarterbacks are occasionally used in other roles. Most teams utilize a backup quarterback as their holder on placekicks. A benefit of using quarterbacks as holders is that it would be easier to pull off a fake field goal attempt, but many coaches prefer to use punters as holders because a punter will have far more time in practice sessions to work with the kicker than any quarterback would. In the Wildcat, a formation where a halfback lines up behind the center and the quarterback lines up out wide, the quarterback can be used as a receiving target or a blocker. A more rare use for a quarterback is to punt the ball himself, a play known as a quick kick. Denver Broncos quarterback John Elway was known to perform quick kicks occasionally, typically when the Broncos were facing a third-and-long situation. Philadelphia Eagles quarterback Randall Cunningham, an All-America punter in college, was also known to punt the ball occasionally, and was assigned as the team's default punter for certain situations, such as when the team was backed up inside their own five-yard line.

As Roger Staubach's back-up, Dallas Cowboys quarterback Danny White was also the team's punter, opening strategic possibilities for coach Tom Landry. Ascending the starting role upon Staubach's retirement, White held his position as the team's punter for several seasons—a double duty he performed to All-American standard at Arizona State University. White also had two touchdown receptions as a Dallas Cowboy, both from the halfback option.

If quarterbacks are uncomfortable with the formation the defense is using, they may call an audible change to their play. For example, if a quarterback receives the call to execute a running play, but he notices that the defense is ready to blitz—that is, to send additional defensive backs across the line of scrimmage in an attempt to tackle the quarterback or hurt his ability to pass—the quarterback may want to change the play. To do this, the quarterback yells a special code, like "Blue 42," or "Texas 29," which tells the offense to switch to a specific play or formation, but it all depends on the quarterback's judgment of the defense's alignment.

Quarterbacks can also "spike" (throw the football at the ground) to stop the official game clock. For example, if a team is down by a field goal with only seconds remaining, a quarterback may spike the ball to prevent the game clock from running out. This usually allows the field goal unit to come onto the field, or attempt a final "Hail Mary pass". However, if a team is winning, a quarterback can keep the clock running by kneeling after the snap. This is normally done when the opposing team has no timeouts and there is little time left in the game, as it allows a team to burn up the remaining time on the clock without risking a turnover or injury.

A dual-threat quarterback possesses the skills and physique to run with the ball if necessary. With the rise of several blitz-heavy defensive schemes and increasingly faster defensive players, the importance of a mobile quarterback has been redefined. While arm power, accuracy, and pocket presence – the ability to successfully operate from within the "pocket" formed by his blockers – are still the most important quarterback virtues, the ability to elude or run past defenders creates an additional threat that allows greater flexibility in a team's passing and running game.

Dual-threat quarterbacks have historically been more prolific at the college level. Typically, a quarterback with exceptional quickness is used in an option offense, which allows the quarterback to hand the ball off, run it himself, or pitch it to the running back following him at a distance of three yards outside and one yard behind. This type of offense forces defenders to commit to the running back up the middle, the quarterback around the end, or the running back trailing the quarterback. It is then that the quarterback has the "option" to identify which match-up is most favorable to the offense as the play unfolds and exploit that defensive weakness. In the college game, many schools employ several plays that are designed for the quarterback to run with the ball. This is much less common in professional football, except for a quarterback sneak, but there is still an emphasis on being mobile enough to escape a heavy pass rush. Historically, high-profile dual-threat quarterbacks in the NFL were uncommon, Steve Young and John Elway being among the notable exceptions, leading their teams to three and five Super Bowl appearances respectively; and Michael Vick, whose rushing ability was a rarity in the early 2000s, although he never led his team to a Super Bowl. In recent years, quarterbacks with dual-threat capabilities have become more popular. Current NFL quarterbacks considered to be dual-threats include Cam Newton, Russell Wilson, and Tyrod Taylor.

Some teams employ a strategy which involves the use of more than one quarterback during the course of a game. This is more common at lower levels of football, such as high school or small college, but rare in major college or professional football.

There are four circumstances in which a two-quarterback system may be used.

The first is when a team is in the process of determining which quarterback will eventually be the starter, and may choose to use each quarterback for part of the game in order to compare the performances. For instance, the Seattle Seahawks' Pete Carroll used the pre-season games in 2012 to select Russell Wilson as the starting quarterback over Matt Flynn and Tarvaris Jackson.

The second is a starter–reliever system, in which the starting quarterback splits the regular season playing time with the backup quarterback, although the former will start playoff games. This strategy is rare, and was last seen in the NFL in the "WoodStrock" combination of Don Strock and David Woodley, which took the Miami Dolphins to the Epic in Miami in 1982 and Super Bowl XVII the following year. The starter-reliever system is distinct from a one-off situation in which a starter is benched in favor of the back-up because the switch is part of the game plan (usually if the starter is playing poorly for that game), and the expectation is that the two players will assume the same roles game after game.

The third is if a coach decides that the team has two quarterbacks who are equally effective and proceeds to rotate the quarterbacks at predetermined intervals, such as after each quarter or after each series. Southern California high school football team Corona Centennial operated this model during the 2014 football season, rotating quarterbacks after every series. In a game against the Chicago Bears in the seventh week of the 1971 season, Dallas Cowboys head coach Tom Landry alternated Roger Staubach and Craig Morton on each play, sending in the quarterbacks with the play call from the sideline.

The fourth, still occasionally seen in major-college football, is the use of different quarterbacks in different game or down/distance situations. Generally this involves a running quarterback and a passing quarterback in an option or wishbone offense. In Canadian football, quarterback sneaks or other runs in short-yardage situations tend to be successful as a result of the distance between the offensive and defensive lines being one yard. Drew Tate, a quarterback for the Calgary Stampeders, was primarily used in short-yardage situations and led the CFL in rushing touchdowns during the 2014 season with ten scores as the backup to Bo Levi Mitchell. This strategy had all but disappeared from professional American football, but returned to some extent with the advent of the "wildcat" offense. There is a great debate within football circles as to the effectiveness of the so-called "two-quarterback system". Many coaches and media personnel remain skeptical of the model. 
Teams such as USC (Southern California), OSU (Oklahoma State), Northwestern, and smaller West Georgia have utilized the two-quarterback system; West Georgia, for example, uses the system due to the skill sets of its quarterbacks. Teams like these use this situation because of the advantages it gives them against defenses of the other team, so that the defense is unable to adjust to their game plan.

The quarterback position dates to the late 1800s, when American Ivy League schools playing a form of rugby union imported from the United Kingdom began to put their own spin on the game. Walter Camp, a prominent athlete and rugby player at Yale University, pushed through a change in rules at a meeting in 1880 that established a line of scrimmage and allowed for the football to be snapped to a quarterback. The change was meant to allow for teams to strategize their play more thoroughly and retain possession more easily than was possible in the chaos of a scrummage in rugby. In Camp's formulation, the "quarter-back" was the person who received a ball snapped back with another player's foot. Originally he was not allowed to run forward of the line of scrimmage:

The quarterback in this context was often called the "blocking back" as their duties usually involved blocking after the initial handoff. The "fullback" was the furthest back behind the line of scrimmage. The "halfback" was halfway between the fullback and the line of scrimmage, and the "quarter-back" was halfway between the halfback and the line of scrimmage. Hence, he was called a "quarter-back" by Walter Camp.

The requirement to stay behind the line of scrimmage was soon rescinded, but it was later re-imposed in six-man football. The exchange between the person snapping the ball (typically the center) and the quarterback was initially an awkward one because it involved a kick. At first, centers gave the ball a small boot, and then picked it up and handed it to the quarterback. By 1889, Yale center Bert Hanson was bouncing the ball on the ground to the quarterback between his legs. The following year, a rule change officially made snapping the ball using the hands between the legs legal. Several years later, Amos Alonzo Stagg at the University of Chicago invented the lift-up snap: the center passed the ball off the ground and between his legs to a standing quarterback. A similar set of changes were later adopted in Canadian football as part of the Burnside rules, a set of rules proposed by John Meldrum "Thrift" Burnside, the captain of the University of Toronto's football team.

The change from a scrummage to a "scrimmage" made it easier for teams to decide what plays they would run before the snap. At first, the captains of college teams were put in charge of play-calling, indicating with shouted codes which players would run with the ball and how the men on the line were supposed to block. Yale later used visual signals, including adjustments of the captain's knit hat, to call plays. Centers could also signal plays based on the alignment of the ball before the snap. In 1888, however, Princeton University began to have its quarterback call plays using number signals. That system caught on, and quarterbacks began to act as directors and organizers of offensive play.

Early on, quarterbacks were used in a variety of formations. Harvard's team put seven men on the line of scrimmage, with three halfbacks who alternated at quarterback and a lone fullback. Princeton put six men on the line and had one designated quarterback, while Yale used seven linemen, one quarterback and two halfbacks who lined up on either side of the fullback. This was the origin of the T-formation, an offensive set that remained in use for many decades afterward and gained popularity in professional football starting in the 1930s.

In 1906, the forward pass was legalized in American football; Canadian football did not adopt the forward pass until 1929. Despite the legalization of the forward pass, the most popular formations of the early 20th century focused mostly on the rushing game. The single-wing formation, a run-oriented offensive set, was invented by football coach Glenn "Pop" Warner around the year 1908. In the single-wing, the quarterback was positioned behind the line of scrimmage and was flanked by a tailback, fullback and wingback. He served largely as a blocking back; the tailback typically took the snap, either running forward with the ball or making a lateral pass to one of the other players in the backfield. The quarterback's job was usually to make blocks upfield to help the tailback or fullback gain yards. Passing plays were rare in the single-wing, an unbalanced power formation where four linemen lined up to one side of the center and two lined up to the other. The tailback was the focus of the offense, and was often a triple-threat man who would either pass, run or kick the ball.

Offensive play-calling continued to focus on rushing up through the 1920s, when professional leagues began to challenge the popularity of college football. In the early days of the professional National Football League (NFL), which was founded in 1920, games were largely low-scoring affairs. Two-thirds of all games in the 1920s were shutouts, and quarterbacks usually passed only out of desperation. In addition to a reluctance to risk turnovers by passing, various rules existed that limited the effectiveness of the forward pass: passers were required to drop back five yards behind the line of scrimmage before they could attempt a pass, and incomplete passes in the end zone resulted in a change of possession and a touchback Additionally, the rules required the ball to be snapped from the location on the field where it was ruled dead; if a play ended with a player going out of bounds, the center had to snap the ball from the sideline, an awkward place to start a play.

Despite these constraints, player-coach Curly Lambeau of the Green Bay Packers, along with several other NFL figures of his era, was a consistent proponent of the forward pass. The Packers found success in the 1920s and 1930s using variations on the single-wing that emphasized the passing game. Packers quarterback Red Dunn and New York Giants and Brooklyn Dodgers quarterback Benny Friedman were the leading passers of their era, but passing remained a relative rarity among other teams; between 1920 and 1932, there were three times as many running plays as there were passing plays.

Early NFL quarterbacks typically were responsible for calling the team's offensive plays with signals before the snap. The use of the huddle to call plays originated with Stagg in 1896, but only began to be used regularly in college games in 1921. In the NFL, players were typically assigned numbers, as were the gaps between offensive linemen. One player, usually the quarterback, would call signals indicating which player was to run the ball and which gap he would run toward. Play-calling or any other kind of coaching from the sidelines was not permitted during this period, leaving the quarterback to devise the offensive strategy (often, the quarterback doubled as head coach during this era). Substitutions were limited, and quarterbacks often played on both offense and defense.

The period between 1933 and 1945 was marked by numerous changes for the quarterback position. The rule requiring a quarterback to be five yards behind the line of scrimmage to pass was abolished. Hash marks were added to the field that established a limited zone between which the ball was placed before snaps, making offensive formations more flexible. Additionally, incomplete passes in the end zone were no longer counted as turnovers and touchbacks.

The single-wing continued to be in wide use throughout this, and a number of forward-passing tailbacks became stars, including Sammy Baugh of the Washington Redskins. In 1939, University of Chicago head football coach Clark Shaughnessy made modifications to the T-formation, a formation that put the quarterback behind the center and had him receive the snap directly. Shaughnessy altered the formation by having the linemen be spaced further apart, and he began having players go in motion behind the line of scrimmage before the snap to confuse defenses. These changes were picked up by Chicago Bears coach George Halas, a close friend of Shaughnessy, and they quickly caught on in the professional ranks. Utilizing the T-formation and led by quarterback Sid Luckman, the Bears reached the NFL championship game in 1940 and beat the Redskins by a score of 73–0. The blowout led other teams across the league to adopt variations on the T-formation, including the Philadelphia Eagles, Cleveland Rams and Detroit Lions. Baugh and the Redskins converted to the T-formation and continued to succeed.

Thanks in part to the emergence of the T-formation and changes in the rulebooks to liberalize the passing game, passing from the quarterback position became more common in the 1940s. Over the course of the decade, passing yards began to exceed rushing yards for the first time in the history of football. The Cleveland Browns of the late 1940s in the All-America Football Conference (AAFC), a professional league created to challenge the NFL, were one of the teams of that era that relied most on passing. Quarterback Otto Graham helped the Browns win four AAFC championships in the late 1940s in head coach Paul Brown's T-formation offense, which emphasized precision timing passes. Cleveland, along with several other AAFC teams, was absorbed by the NFL in 1950 after the dissolution of the AAFC that same year. By the end of the 1940s, all NFL teams aside from the Pittsburgh Steelers used the T-formation as their primary offensive formation.

As late as the 1960s, running plays occurred more frequently than passes. NFL quarterback Milt Plum later stated that during his career (1957-1969) passes typically only occurred on third downs and sometimes on first downs. Quarterbacks only increased in importance as rules changed to favor passing and higher scoring and as football gained popularity on television after the 1958 NFL Championship Game, often referred to as "The Greatest Game Ever Played". Early modern offenses evolved around the quarterback as a passing threat, boosted by rules changes in 1978 and 1979 that made it a penalty for defensive backs to interfere with receivers downfield and allowed offensive linemen to pass-block using their arms and open hands; the rules had limited them to blocking with their hands held to their chests. Average passing yards per game rose from 283.3 in 1977 to 408.7 in 1979.
The NFL continues to be a pass-heavy league, in part due to further rule changes that prescribed harsher penalties for hitting the quarterback and for hitting defenseless receivers as they awaited passes. Passing in wide-open offenses has also been an emphasis at the high school and college levels, and professional coaches have devised schemes to fit the talents of new generations of quarterbacks.

While quarterbacks and team captains usually called plays in football's early years, today coaches often decide which plays the offense will run. Some teams use an offensive coordinator, an assistant coach whose duties include offensive game-planning and often play-calling. In the NFL, coaches are allowed to communicate with quarterbacks and call plays using audio equipment built into the player's helmet. Quarterbacks are allowed to hear, but not talk to, their coaches until there are fifteen seconds left on the play clock. Once the quarterback receives the call, he may relay it to other players via signals or in a huddle.

Dallas Cowboys head coach Tom Landry was an early advocate of taking play calling out of the quarterback's hands. Although this remained a common practice in the NFL through the 1970s, fewer QBs were doing it by the 1980s and even Hall of Famers like Joe Montana did not call their own plays. Buffalo Bills QB Jim Kelly was one of the last to regularly call plays. Peyton Manning, formerly of the Indianapolis Colts and Denver Broncos, was the best modern example of a quarterback who called his own plays, primary using an uptempo, no-huddle-based attack. Manning had almost complete control over the offense. Baltimore Ravens quarterback Joe Flacco retains a high degree of control over the offense as well, particularly when running a no-huddle scheme, as does Ben Roethlisberger of the Pittsburgh Steelers.

During the 2013 season, 67 percent of NFL players were African American (blacks make up 13 percent of the US population), yet only 17 percent of quarterbacks were; 82 percent of quarterbacks were white. In 2017, the New York Giants Eli Manning was benched in favor of Geno Smith, who was declared the starter. The Giants were the last team to never field a black starting QB during an NFL season.

Since the inception of the game, only two quarterbacks with known black ancestry have led their team to a Super Bowl victory, Doug Williams in 1988 and Russell Wilson, who is multiracial, in 2014.




</doc>
<doc id="25278" url="https://en.wikipedia.org/wiki?curid=25278" title="Quadrilateral">
Quadrilateral

In Euclidean plane geometry, a quadrilateral is a polygon with four edges (or sides) and four vertices or corners. Sometimes, the term quadrangle is used, by analogy with triangle, and sometimes tetragon for consistency with pentagon (5-sided), hexagon (6-sided) and so on.

The origin of the word "quadrilateral" is the two Latin words "quadri", a variant of four, and "latus", meaning "side".

Quadrilaterals are simple (not self-intersecting) or complex (self-intersecting), also called crossed. Simple quadrilaterals are either convex or concave.

The interior angles of a simple (and planar) quadrilateral "ABCD" add up to 360 degrees of arc, that is

This is a special case of the "n"-gon interior angle sum formula ("n" − 2) × 180°.

All non-self-crossing quadrilaterals tile the plane by repeated rotation around the midpoints of their edges.

Any quadrilateral that is not self-intersecting is a simple quadrilateral.

In a convex quadrilateral, all interior angles are less than 180° and the two diagonals both lie inside the quadrilateral.



In a concave quadrilateral, one interior angle is bigger than 180° and one of the two diagonals lies outside the quadrilateral.

A self-intersecting quadrilateral is called variously a cross-quadrilateral, crossed quadrilateral, butterfly quadrilateral or bow-tie quadrilateral. In a crossed quadrilateral, the four "interior" angles on either side of the crossing (two acute and two reflex, all on the left or all on the right as the figure is traced out) add up to 720°.


The two diagonals of a convex quadrilateral are the line segments that connect opposite vertices.

The two bimedians of a convex quadrilateral are the line segments that connect the midpoints of opposite sides. They intersect at the "vertex centroid" of the quadrilateral (see Remarkable points below).

The four maltitudes of a convex quadrilateral are the perpendiculars to a side through the midpoint of the opposite side.

There are various general formulas for the area "K" of a convex quadrilateral "ABCD" with sides .

The area can be expressed in trigonometric terms as

where the lengths of the diagonals are "p" and "q" and the angle between them is "θ". In the case of an orthodiagonal quadrilateral (e.g. rhombus, square, and kite), this formula reduces to formula_3 since "θ" is 90°.

The area can be also expressed in terms of bimedians as
where the lengths of the bimedians are "m" and "n" and the angle between them is "φ".

Bretschneider's formula expresses the area in terms of the sides and two opposite angles:

where the sides in sequence are "a", "b", "c", "d", where "s" is the semiperimeter, and "A" and "C" are two (in fact, any two) opposite angles. This reduces to Brahmagupta's formula for the area of a cyclic quadrilateral when .

Another area formula in terms of the sides and angles, with angle "C" being between sides "b" and "c", and "A" being between sides "a" and "d", is

In the case of a cyclic quadrilateral, the latter formula becomes formula_7

In a parallelogram, where both pairs of opposite sides and angles are equal, this formula reduces to formula_8

Alternatively, we can write the area in terms of the sides and the intersection angle "θ" of the diagonals, so long as this angle is not 90°:

In the case of a parallelogram, the latter formula becomes formula_10

Another area formula including the sides "a", "b", "c", "d" is
where "x" is the distance between the midpoints of the diagonals and "φ" is the angle between the bimedians.

The last trigonometric area formula including the sides "a", "b", "c", "d" and the angle "α" between "a" and "b" is: 
which can also be used for the area of a concave quadrilateral (having the concave part opposite to angle "α") just changing the first sign + to - .

The following two formulas express the area in terms of the sides "a", "b", "c", "d", the semiperimeter "s", and the diagonals "p", "q":

The first reduces to Brahmagupta's formula in the cyclic quadrilateral case, since then "pq" = "ac" + "bd".

The area can also be expressed in terms of the bimedians "m", "n" and the diagonals "p", "q":

In fact, any three of the four values "m", "n", "p", and "q" suffice for determination of the area, since in any quadrilateral the four values are related by formula_17 The corresponding expressions are:

if the lengths of two bimedians and one diagonal are given, and

if the lengths of two diagonals and one bimedian are given.

The area of a quadrilateral "ABCD" can be calculated using vectors. Let vectors AC and BD form the diagonals from "A" to "C" and from "B" to "D". The area of the quadrilateral is then

which is half the magnitude of the cross product of vectors AC and BD. In two-dimensional Euclidean space, expressing vector AC as a free vector in Cartesian space equal to (x","y") and BD as (x","y"), this can be rewritten as:

In the following table it is listed if the diagonals in some of the most basic quadrilaterals bisect each other, if their diagonals are perpendicular, and if their diagonals have equal length. The list applies to the most general cases, and excludes named subsets.

"Note 1: The most general trapezoids and isosceles trapezoids do not have perpendicular diagonals, but there are infinite numbers of (non-similar) trapezoids and isosceles trapezoids that do have perpendicular diagonals and are not any other named quadrilateral."

"Note 2: In a kite, one diagonal bisects the other. The most general kite has unequal diagonals, but there is an infinite number of (non-similar) kites in which the diagonals are equal in length (and the kites are not any other named quadrilateral)."

The lengths of the diagonals in a convex quadrilateral "ABCD" can be calculated using the law of cosines on each triangle formed by one diagonal and two sides of the quadrilateral. Thus

and

Other, more symmetric formulas for the lengths of the diagonals, are

and

In any convex quadrilateral "ABCD", the sum of the squares of the four sides is equal to the sum of the squares of the two diagonals plus four times the square of the line segment connecting the midpoints of the diagonals. Thus

where "x" is the distance between the midpoints of the diagonals. This is sometimes known as Euler's quadrilateral theorem and is a generalization of the parallelogram law.

The German mathematician Carl Anton Bretschneider derived in 1842 the following generalization of Ptolemy's theorem, regarding the product of the diagonals in a convex quadrilateral

This relation can be considered to be a law of cosines for a quadrilateral. In a cyclic quadrilateral, where "A" + "C" = 180°, it reduces to "pq = ac + bd". Since cos ("A" + "C") ≥ −1, it also gives a proof of Ptolemy's inequality.

If "X" and "Y" are the feet of the normals from "B" and "D" to the diagonal "AC" = "p" in a convex quadrilateral "ABCD" with sides "a" = "AB", "b" = "BC", "c" = "CD", "d" = "DA", then

In a convex quadrilateral "ABCD" with sides "a" = "AB", "b" = "BC", "c" = "CD", "d" = "DA", and where the diagonals intersect at "E",

where "e" = "AE", "f" = "BE", "g" = "CE", and "h" = "DE".

The shape and size of a convex quadrilateral are fully determined by the lengths of its sides in sequence and of one diagonal between two specified vertices. The two diagonals "p, q" and the four side lengths "a, b, c, d" of a quadrilateral are related by the Cayley-Menger determinant, as follows:

The internal angle bisectors of a convex quadrilateral either form a cyclic quadrilateral (that is, the four intersection points of adjacent angle bisectors are concyclic) or they are concurrent. In the latter case the quadrilateral is a tangential quadrilateral.

In quadrilateral "ABCD", if the angle bisectors of "A" and "C" meet on diagonal "BD", then the angle bisectors of "B" and "D" meet on diagonal "AC".

The bimedians of a quadrilateral are the line segments connecting the midpoints of the opposite sides. The intersection of the bimedians is the centroid of the vertices of the quadrilateral.

The midpoints of the sides of any quadrilateral (convex, concave or crossed) are the vertices of a parallelogram called the Varignon parallelogram. It has the following properties:

The two bimedians in a quadrilateral and the line segment joining the midpoints of the diagonals in that quadrilateral are concurrent and are all bisected by their point of intersection.

In a convex quadrilateral with sides "a", "b", "c" and "d", the length of the bimedian that connects the midpoints of the sides "a" and "c" is

where "p" and "q" are the length of the diagonals. The length of the bimedian that connects the midpoints of the sides "b" and "d" is

Hence

This is also a corollary to the parallelogram law applied in the Varignon parallelogram.

The lengths of the bimedians can also be expressed in terms of two opposite sides and the distance "x" between the midpoints of the diagonals. This is possible when using Euler's quadrilateral theorem in the above formulas. Whence

and

Note that the two opposite sides in these formulas are not the two that the bimedian connects.

In a convex quadrilateral, there is the following dual connection between the bimedians and the diagonals:

The four angles of a simple quadrilateral "ABCD" satisfy the following identities:

and

Also,

In the last two formulas, no angle is allowed to be a right angle, since tan 90° is not defined.

If a convex quadrilateral has the consecutive sides "a", "b", "c", "d" and the diagonals "p", "q", then its area "K" satisfies

From Bretschneider's formula it directly follows that the area of a quadrilateral satisfies

with equality if and only if the quadrilateral is cyclic or degenerate such that one side is equal to the sum of the other three (it has collapsed into a line segment, so the area is zero).

The area of any quadrilateral also satisfies the inequality

Denoting the perimeter as "L", we have

with equality only in the case of a square.

The area of a convex quadrilateral also satisfies

for diagonal lengths "p" and "q", with equality if and only if the diagonals are perpendicular.

A corollary to Euler's quadrilateral theorem is the inequality

where equality holds if and only if the quadrilateral is a parallelogram.

Euler also generalized Ptolemy's theorem, which is an equality in a cyclic quadrilateral, into an inequality for a convex quadrilateral. It states that

where there is equality if and only if the quadrilateral is cyclic. This is often called Ptolemy's inequality.

In any convex quadrilateral the bimedians "m, n" and the diagonals "p, q" are related by the inequality

with equality holding if and only if the diagonals are equal. This follows directly from the quadrilateral identity formula_50

The sides "a", "b", "c", and "d" of any quadrilateral satisfy

and

Among all quadrilaterals with a given perimeter, the one with the largest area is the square. This is called the "isoperimetric theorem for quadrilaterals". It is a direct consequence of the area inequality

where "K" is the area of a convex quadrilateral with perimeter "L". Equality holds if and only if the quadrilateral is a square. The dual theorem states that of all quadrilaterals with a given area, the square has the shortest perimeter.

The quadrilateral with given side lengths that has the maximum area is the cyclic quadrilateral.

Of all convex quadrilaterals with given diagonals, the orthodiagonal quadrilateral has the largest area. This is a direct consequence of the fact that the area of a convex quadrilateral satisfies

where "θ" is the angle between the diagonals "p" and "q". Equality holds if and only if "θ" = 90°.

If "P" is an interior point in a convex quadrilateral "ABCD", then

From this inequality it follows that the point inside a quadrilateral that minimizes the sum of distances to the vertices is the intersection of the diagonals. Hence that point is the Fermat point of a convex quadrilateral.

The centre of a quadrilateral can be defined in several different ways. The "vertex centroid" comes from considering the quadrilateral as being empty but having equal masses at its vertices. The "side centroid" comes from considering the sides to have constant mass per unit length. The usual centre, called just centroid (centre of area) comes from considering the surface of the quadrilateral as having constant density. These three points are in general not all the same point.

The "vertex centroid" is the intersection of the two bimedians. As with any polygon, the "x" and "y" coordinates of the vertex centroid are the arithmetic means of the "x" and "y" coordinates of the vertices.

The "area centroid" of quadrilateral "ABCD" can be constructed in the following way. Let "G", "G", "G", "G" be the centroids of triangles "BCD", "ACD", "ABD", "ABC" respectively. Then the "area centroid" is the intersection of the lines "GG" and "GG".

In a general convex quadrilateral "ABCD", there are no natural analogies to the circumcenter and orthocenter of a triangle. But two such points can be constructed in the following way. Let "O", "O", "O", "O" be the circumcenters of triangles "BCD", "ACD", "ABD", "ABC" respectively; and denote by "H", "H", "H", "H" the orthocenters in the same triangles. Then the intersection of the lines "OO" and "OO" is called the quasicircumcenter, and the intersection of the lines "HH" and "HH" is called the "quasiorthocenter" of the convex quadrilateral. These points can be used to define an Euler line of a quadrilateral. In a convex quadrilateral, the quasiorthocenter "H", the "area centroid" "G", and the quasicircumcenter "O" are collinear in this order, and "HG" = 2"GO".

There can also be defined a "quasinine-point center" "E" as the intersection of the lines "EE" and "EE", where "E", "E", "E", "E" are the nine-point centers of triangles "BCD", "ACD", "ABD", "ABC" respectively. Then "E" is the midpoint of "OH".

Another remarkable line in a convex non-parallelogram quadrilateral is the Newton line, which connects the midpoints of the diagonals, the segment connecting these points being bisected by the vertex centroid. One more interesting line (in some sense dual to the Newton's one) is the line connecting the point of intersection of diagonals with the vertex centroid. The line is remarkable by the fact that it contains the (area) centroid. The vertex centroid divides the segment connecting the intersection of diagonals and the (area) centroid in the ratio 3:1.


A hierarchical taxonomy of quadrilaterals is illustrated by the figure to the right. Lower classes are special cases of higher classes they are connected to. Note that "trapezoid" here is referring to the North American definition (the British equivalent is a trapezium). Inclusive definitions are used throughout.

A non-planar quadrilateral is called a skew quadrilateral. Formulas to compute its dihedral angles from the edge lengths and the angle between two adjacent edges were derived for work on the properties of molecules such as cyclobutane that contain a "puckered" ring of four atoms. Historically the term gauche quadrilateral was also used to mean a skew quadrilateral. A skew quadrilateral together with its diagonals form a (possibly non-regular) tetrahedron, and conversely every skew quadrilateral comes from a tetrahedron where a pair of opposite edges is removed.



</doc>
<doc id="25280" url="https://en.wikipedia.org/wiki?curid=25280" title="Quantum teleportation">
Quantum teleportation

Quantum teleportation is a process by which quantum information (e.g. the exact state of an atom or photon) can be transmitted (exactly, in principle) from one location to another, with the help of classical communication and previously shared quantum entanglement between the sending and receiving location. Because it depends on classical communication, which can proceed no faster than the speed of light, it cannot be used for faster-than-light transport or communication of classical bits. While it has proven possible to teleport one or more qubits of information between two (entangled) atoms, this has not yet been achieved between anything larger than molecules.

Although the name is inspired by the teleportation commonly used in fiction, quantum teleportation is limited to the transfer of information rather than matter itself. Quantum teleportation is not a form of transportation, but of communication: it provides a way of transporting a qubit from one location to another without having to move a physical particle along with it.

The term was coined by physicist Charles Bennett. The seminal paper first expounding the idea of quantum teleportation was published by C. H. Bennett, G. Brassard, C. Crépeau, R. Jozsa, A. Peres and W. K. Wootters in 1993. Quantum teleportation was first realized in single photons, later being demonstrated in various material systems such as atoms, ions, electrons and superconducting circuits. The latest reported record distance for quantum teleportation is by the group of Jian-Wei Pan using the Micius satellite for space-based quantum teleportation.

In matters relating to quantum or classical information theory, it is convenient to work with the simplest possible unit of information, the two-state system. In classical information, this is a bit, commonly represented using one or zero (or true or false). The quantum analog of a bit is a quantum bit, or qubit. Qubits encode a type of information, called quantum information, which differs sharply from "classical" information. For example, quantum information can be neither copied (the no-cloning theorem) nor destroyed (the no-deleting theorem).

Quantum teleportation provides a mechanism of moving a qubit from one location to another, without having to physically transport the underlying particle to which that qubit is normally attached. Much like the invention of the telegraph allowed classical bits to be transported at high speed across continents, quantum teleportation holds the promise that one day, qubits could be moved likewise. However, , only photons and single atoms have been employed as information bearers.

The movement of qubits does require the movement of "things". The actual teleportation protocol requires that an entangled quantum state or Bell state be created, and its two parts shared between two locations (the source and destination, or Alice and Bob). In essence, a certain kind of quantum channel between two sites must be established first, before a qubit can be moved. Teleportation also requires a classical information channel to be established, as two classical bits must be transmitted to accompany each qubit. The reason for this is that the results of the measurements must be communicated, and this must be done over ordinary classical communication channels. The need for such classical channels may, at first, seem disappointing; however, this is not unlike ordinary communications, which requires wires, radios or lasers. What's more, Bell states are most easily shared using photons from lasers, and so teleportation could be done, in principle, through open space.

The quantum states of single atoms have been teleported. An atom consists of several parts: the qubits in the electronic state or electron shells surrounding the atomic nucleus, the qubits in the nucleus itself, and, finally, the electrons, protons and neutrons making up the atom. Physicists have teleported the qubits encoded in the electronic state of atoms; they have not teleported the nuclear state, nor the nucleus itself. It is therefore inaccurate to say "an atom has been teleported". The quantum state of an atom has. Thus, performing this kind of teleportation requires a stock of atoms at the receiving site, available for having qubits imprinted on them. The importance of teleporting nuclear state is unclear: nuclear state does affect the atom, e.g. in hyperfine splitting, but whether such state would need to be teleported in some futuristic "practical" application is debatable.

An important aspect of quantum information theory is entanglement, which imposes statistical correlations between otherwise distinct physical systems. These correlations hold even when measurements are chosen and performed independently, out of causal contact from one another, as verified in Bell test experiments. Thus, an observation resulting from a measurement choice made at one point in spacetime seems to instantaneously affect outcomes in another region, even though light hasn't yet had time to travel the distance; a conclusion seemingly at odds with special relativity (EPR paradox). However such correlations can never be used to transmit any information faster than the speed of light, a statement encapsulated in the no-communication theorem. Thus, teleportation, as a whole, can never be superluminal, as a qubit cannot be reconstructed until the accompanying classical information arrives.

Understanding quantum teleportation requires a good grounding in finite-dimensional linear algebra, Hilbert spaces and projection matrixes. A qubit is described using a two-dimensional complex number-valued vector space (a Hilbert space), which are the primary basis for the formal manipulations given below. A working knowledge of quantum mechanics is not absolutely required to understand the mathematics of quantum teleportation, although without such acquaintance, the deeper meaning of the equations may remain quite mysterious.

The prerequisites for quantum teleportation are a qubit that is to be teleported, a conventional communication channel capable of transmitting two classical bits (i.e., one of four states), and means of generating an entangled EPR pair of qubits, transporting each of these to two different locations, A and B, performing a Bell measurement on one of the EPR pair qubits, and manipulating the quantum state of the other pair. The protocol is then as follows:


Work in 1998 verified the initial predictions, and the distance of teleportation was increased in August 2004 to 600 meters, using optical fiber. Subsequently, the record distance for quantum teleportation has been gradually increased to 16 km, then to 97 km, and is now , set in open air experiments done between two of the Canary Islands. There has been a recent record set (as of September 2015) using superconducting nanowire detectors that reached the distance of over optical fiber. For material systems, the record distance is 21m.

A variant of teleportation called "open-destination" teleportation, with receivers located at multiple locations, was demonstrated in 2004 using five-photon entanglement. Teleportation of a composite state of two single photons has also been realized. In April 2011, experimenters reported that they had demonstrated teleportation of wave packets of light up to a bandwidth of 10 MHz while preserving strongly nonclassical superposition states. In August 2013, the achievement of "fully deterministic" quantum teleportation, using a hybrid technique, was reported. On 29 May 2014, scientists announced a reliable way of transferring data by quantum teleportation. Quantum teleportation of data had been done before but with highly unreliable methods. On 26 February 2015, scientists at the University of Science and Technology of China in Hefei, led by Chao-yang Lu and Jian-Wei Pan carried out the first experiment teleporting multiple degrees of freedom of a quantum particle. They managed to teleport the quantum information from ensemble of rubidium atoms to another ensemble of rubidium atoms over a distance of 150 metres using entangled photons

Researchers have also successfully used quantum teleportation to transmit information between clouds of gas atoms, notable because the clouds of gas are macroscopic atomic ensembles.

In 2018, physicists at Yale demonstrated a deterministic teleported CNOT operation between logically encoded qubits.

There are a variety of ways in which the teleportation protocol can be written mathematically. Some are very compact but abstract, and some are verbose but straightforward and concrete. The presentation below is of the latter form: verbose, but has the benefit of showing each quantum state simply and directly. Later sections review more compact notations.

The teleportation protocol begins with a quantum state or qubit formula_4, in Alice's possession, that she wants to convey to Bob. This qubit can be written generally, in bra–ket notation, as:

The subscript "C" above is used only to distinguish this state from "A" and "B", below.

Next, the protocol requires that Alice and Bob share a maximally entangled state. This state is fixed in advance, by mutual agreement between Alice and Bob, and can be any one of the four Bell states shown. It does not matter which one.

In the following, assume that Alice and Bob share the state formula_10
Alice obtains one of the particles in the pair, with the other going to Bob. (This is implemented by preparing the particles together and shooting them to Alice and Bob from a common source.) The subscripts "A" and "B" in the entangled state refer to Alice's or Bob's particle.

At this point, Alice has two particles ("C", the one she wants to teleport, and "A", one of the entangled pair), and Bob has one particle, "B". In the total system, the state of these three particles is given by

Alice will then make a local measurement in the Bell basis (i.e. the four Bell states) on the two particles in her possession. To make the result of her measurement clear, it is best to write the state of Alice's two qubits as superpositions of the Bell basis. This is done by using the following general identities, which are easily verified:

and

One applies these identities with "A" and "C" subscripts. The total three particle state, of "A", "B" and "C" together, thus becomes the following four-term superposition:

The above is just a change of basis on Alice's part of the system. No operation has been performed and the three particles are still in the same total state. The actual teleportation occurs when Alice measures her two qubits A,C, in the Bell basis

Experimentally, this measurement may be achieved via a series of laser pulses directed at the two particles. Given the above expression, evidently the result of Alice's (local) measurement is that the three-particle state would collapse to one of the following four states (with equal probability of obtaining each):


Alice's two particles are now entangled to each other, in one of the four Bell states, and the entanglement originally shared between Alice's and Bob's particles is now broken. Bob's particle takes on one of the four superposition states shown above. Note how Bob's qubit is now in a state that resembles the state to be teleported. The four possible states for Bob's qubit are unitary images of the state to be teleported.

The result of Alice's Bell measurement tells her which of the above four states the system is in. She can now send her result to Bob through a classical channel. Two classical bits can communicate which of the four results she obtained.

After Bob receives the message from Alice, he will know which of the four states his particle is in. Using this information, he performs a unitary operation on his particle to transform it to the desired state formula_22:


to recover the state.


to his qubit.


Teleportation is thus achieved. The above-mentioned three gates correspond to rotations of π radians (180°) about appropriate axes (X, Y and Z).
Some remarks:

Quantum circuit for a single qubit quantum teleportation is demonstrated using Q-Kit, a quantum circuit simulator with graphical interface. Alice's state in qubit 2 is transferred to Bob's qubit 0 using a priorly entangled pair of qubits between Alice and Bob, qubits 1 and 0.

There are a variety of different notations in use that describe the teleportation protocol. One common one is by using the notation of quantum gates. In the above derivation, the unitary transformation that is the change of basis (from the standard product basis into the Bell basis) can be written using quantum gates. Direct calculation shows that this gate is given by

where "H" is the one qubit Walsh-Hadamard gate and formula_31 is the Controlled NOT gate.

Teleportation can be applied not just to pure states, but also mixed states, that can be regarded as the state of a single subsystem of an entangled pair. The so-called entanglement swapping is a simple and illustrative example.

If Alice has a particle which is entangled with a particle owned by Bob, and Bob teleports it to Carol, then afterwards, Alice's particle is entangled with Carol's.

A more symmetric way to describe the situation is the following: Alice has one particle, Bob two, and Carol one. Alice's particle and Bob's first particle are entangled, and so are Bob's second and Carol's particle:

Now, if Bob does a projective measurement on his two particles in the Bell state basis and communicates the results to Carol, as per the teleportation scheme described above, the state of Bob's first particle can be teleported to Carol's. Although Alice and Carol never interacted with each other, their particles are now entangled.

A detailed diagrammatic derivation of entanglement swapping has been given by Bob Coecke, presented in terms of categorical quantum mechanics.

One can imagine how the teleportation scheme given above might be extended to "N"-state particles, i.e. particles whose states lie in the "N" dimensional Hilbert space. The combined system of the three particles now has an formula_32 dimensional state space. To teleport, Alice makes a partial measurement on the two particles in her possession in some entangled basis on the formula_33 dimensional subsystem. This measurement has formula_33 equally probable outcomes, which are then communicated to Bob classically. Bob recovers the desired state by sending his particle through an appropriate unitary gate.

International Journal of General Systems have published a research paper titled From n-qubit multi-particle quantum teleportation modelling to n-qudit contextuality based quantum teleportation and beyond, which has been authored by Prof Prem Saran Satsangi and his associates from Quantum-Nano Systems & Consciousness Centre at DEI, Agra India. This model uses graph theory to model a three particle entangled teleportation system. This paper became quite popular due to its holistic approach(Scientific Usage, Its applications And Limits that binds scientist from having a holistic understanding of the cosmic Universe.) and also featured as trending research on the home page of the publishers(Taylor and Francis) in June 2017.

In general, mixed states ρ may be transported, and a linear transformation ω applied during teleportation, thus allowing data processing of quantum information. This is one of the foundational building blocks of quantum information processing. This is demonstrated below.

A general teleportation scheme can be described as follows. Three quantum systems are involved. System 1 is the (unknown) state "ρ" to be teleported by Alice. Systems 2 and 3 are in a maximally entangled state "ω" that are distributed to Alice and Bob, respectively. The total system is then in the state

A successful teleportation process is a LOCC quantum channel Φ that satisfies

where Tr is the partial trace operation with respect systems 1 and 2, and formula_37 denotes the composition of maps. This describes the channel in the Schrödinger picture.

Taking adjoint maps in the Heisenberg picture, the success condition becomes

for all observable "O" on Bob's system. The tensor factor in formula_39 is formula_40 while that of formula_41 is formula_42.

The proposed channel Φ can be described more explicitly. To begin teleportation, Alice performs a local measurement on the two subsystems (1 and 2) in her possession. Assume the local measurement have "effects"

If the measurement registers the "i"-th outcome, the overall state collapses to

The tensor factor in formula_45 is formula_40 while that of formula_41 is formula_42. Bob then applies a corresponding local operation Ψ"" on system 3. On the combined system, this is described by

where "Id" is the identity map on the composite system formula_50.

Therefore, the channel Φ is defined by

Notice Φ satisfies the definition of LOCC. As stated above, the teleportation is said to be successful if, for all observable "O" on Bob's system, the equality

holds. The left hand side of the equation is:

where Ψ"*" is the adjoint of Ψ"" in the Heisenberg picture. Assuming all objects are finite dimensional, this becomes

The success criterion for teleportation has the expression

A local explanation of quantum teleportation is put forward by David Deutsch and Patrick Hayden, with respect to the many-worlds interpretation of quantum mechanics. Their paper asserts that the two bits that Alice sends Bob contain "locally inaccessible information" resulting in the teleportation of the quantum state. "The ability of quantum information to flow through a classical channel ..., surviving decoherence, is ... the
basis of quantum teleportation."





</doc>
<doc id="25284" url="https://en.wikipedia.org/wiki?curid=25284" title="Qubit">
Qubit

In quantum computing, a qubit () or quantum bit (sometimes qbit) is the basic unit of quantum information — the quantum version of the classical binary bit physically realized with a two-state device. A qubit is a two-state (or two-level) quantum-mechanical system, one of the simplest quantum systems displaying the weirdness of quantum mechanics. Examples include: the spin of the electron in which the two levels can be taken as spin up and spin down; or the polarization of a single photon in which the two states can be taken to be the vertical polarization and the horizontal polarization. In a classical system, a bit would have to be in one state or the other. However, quantum mechanics allows the qubit to be in a coherent superposition of both states/levels at the same time, a property that is fundamental to quantum mechanics and thus quantum computing.

The coining of the term “qubit” is attributed to Benjamin Schumacher. In the acknowledgments of his 1995 paper, Schumacher states that the term "qubit" was invented in jest during a conversation with William Wootters. The paper describes a way of compressing states emitted by a quantum source of information so that they require fewer physical resources to store. This procedure is now known as Schumacher compression.

A binary digit, characterized as 0 and 1, is used to represent information in classical computers. A binary digit can represent up to one bit of Shannon information, where a bit is the basic unit of information. 
However, in this article, the word bit is synonymous with binary digit.

In classical computer technologies, a "processed" bit is implemented by one of two levels of low DC voltage, and whilst switching from one of these two levels to the other, a so-called forbidden zone must be passed as fast as possible, as electrical voltage cannot change from one level to another "instantaneously".

There are two possible outcomes for the measurement of a qubit—usually taken to have the value “0” and “1”, like a bit or binary digit. However, whereas the state of a bit can only be either 0 or 1, the general state of a qubit according to quantum mechanics can be a coherent superposition of both. Moreover, whereas a measurement of a classical bit would not disturb its state, a measurement of a qubit would destroy its coherence and irrevocably disturb the superposition state. It is possible to fully encode one bit in one qubit. However, a qubit can hold more information, e.g. up to two bits using superdense coding.

For a system of "n" components, a complete description of its state in classical physics requires only "n" bits, whereas in quantum physics it requires 2−1 complex numbers.

In quantum mechanics, the general quantum state of a qubit can be represented by a linear superposition of its two orthonormal basis states (or basis vectors). These vectors are usually denoted as
formula_1
and 
formula_2. They are written in the conventional Dirac—or "bra–ket"—notation; the formula_3 and formula_4 are pronounced "ket 0" and "ket 1", respectively. These two orthonormal basis states, <math>\

In a paper entitled: "Solid-state quantum memory using the P nuclear spin", published in the October 23, 2008 issue of the journal "Nature", a team of scientists from the U.K. and U.S. reported the first relatively long (1.75 seconds) and coherent transfer of a superposition state in an electron spin "processing" qubit to a nuclear spin "memory" qubit. This event can be considered the first relatively consistent quantum data storage, a vital step towards the development of quantum computing. Recently, a modification of similar systems (using charged rather than neutral donors) has dramatically extended this time, to 3 hours at very low temperatures and 39 minutes at room temperature. Room temperature preparation of a qubit based on electron spins instead of nuclear spin was also demonstrated by a team of scientists from Switzerland and Australia.







</doc>
<doc id="25286" url="https://en.wikipedia.org/wiki?curid=25286" title="Quechuan languages">
Quechuan languages

Quechua (, ; ), usually called Runasimi ("people's language") in Quechuan languages, is an indigenous language family spoken by the Quechua peoples, primarily living in the Andes and highlands of South America. Derived from a common ancestral language, it is the most widely spoken language family of indigenous peoples of the Americas, with a total of probably some 8–10 million speakers. Approximately 25% (7.7 million) of Peruvians speak a Quechuan language.
It is perhaps most widely known for being the main language family of the Inca Empire. The Spanish colonisers initially encouraged its use, but from the middle of their reign they suppressed it. However, Quechua ultimately survived, and variants are still widely spoken today.

The Quechua had already expanded across wide ranges of the central Andes long before the expansion of the Inca Empire. The Inca were one among many peoples in present-day Peru who already spoke form of Quechua. In the Cusco region, Quechua was influenced by neighboring languages such as Aymara, which caused it to develop as distinct. In similar ways, diverse dialects developed in different areas, borrowing from local languages, when the Inca Empire ruled and imposed Quechua as the official language.

After the Spanish conquest of the Inca Empire in the 16th century, Quechua continued to be used widely by the indigenous peoples as the "common language". It was officially recognized by the Spanish administration and many Spanish learned it in order to communicate with local peoples. Clergy of the Catholic Church adopted Quechua to use as the language of evangelization. Given its use by the Catholic missionaries, the range of Quechua continued to expand in some areas.

In the late 18th century, colonial officials ended administrative and religious use of Quechua, banning it from public use in Peru after the Túpac Amaru II rebellion of indigenous peoples. The Crown banned even "loyal" pro-Catholic texts in Quechua, such as Garcilaso de la Vega's "Comentarios Reales."

Despite a brief revival of the language immediately after the Latin American nations achieved independence in the 19th century, the prestige of Quechua had decreased sharply. Gradually its use declined so that it was spoken mostly by indigenous people in the more isolated and conservative rural areas. Nevertheless, in the 21st century, Quechua language speakers number 8 to 10 million people across South America, the most speakers of any indigenous language.

The oldest written records of the language are by missionary Domingo de Santo Tomás, who arrived in Peru in 1538 and learned the language from 1540. He published his "Grammatica o arte de la lengua general de los indios de los reynos del Perú" (Grammar or Art of the General Language of the Indians of the Royalty of Peru) in 1560.

In 1975 Peru became the first country to recognize Quechua as one of its official languages. Ecuador conferred official status on the language in its 2006 constitution, and in 2009, Bolivia adopted a new constitution that recognized Quechua and several other indigenous languages as official languages of the country.

The major obstacle to the usage and teaching of Quechuan languages is the lack of written materials in the languages, such as books, newspapers, software, and magazines. The Bible has been translated into Quechua and is distributed by certain missionary groups. Quechua, along with Aymara and the minor indigenous languages, remains essentially a spoken language.

In recent years, Quechua has been introduced in intercultural bilingual education (IBE) in Bolivia, Ecuador and Peru. Even in these areas, the governments are reaching only a part of the Quechua-speaking populations. Some indigenous people in each of the countries are having their children study in Spanish for the purposes of social advancement.

Radio Nacional del Perú broadcasts news and agrarian programs in Quechua for periods in the mornings.

Quechua and Spanish are now heavily intermixed in much of the Andean region, with many hundreds of Spanish loanwords in Quechua. Similarly, Quechua phrases and words are commonly used by Spanish speakers. In southern rural Bolivia, for instance, many Quechua words such as "wawa" (infant), "misi" (cat), "waska" (strap or thrashing), are as commonly used as their Spanish counterparts, even in entirely Spanish-speaking areas. Quechua has also had a profound influence on other native languages of the Americas, such as Mapuche.

The number of speakers given varies widely according to the sources. The total in "Ethnologue" 16 is 10 million, mostly based on figures published 1987–2002, but with a few dating from the 1960s. The figure for Imbabura Highland Quechua in "Ethnologue", for example, is 300,000, an estimate from 1977.

The missionary organization FEDEPI, on the other hand, estimated one million Imbabura dialect speakers (published 2006). Census figures are also problematic, due to under-reporting. The 2001 Ecuador census reports only 500,000 Quechua speakers, compared to the estimate in most linguistic sources of more than 2 million. The censuses of Peru (2007) and Bolivia (2001) are thought to be more reliable.


Additionally, there are an unknown number of speakers in emigrant communities, including Queens, New York, and Paterson, New Jersey, in the United States.

There are significant differences among the varieties of Quechua spoken in the central Peruvian highlands and the peripheral varieties of Ecuador, as well as those of southern Peru and Bolivia. They can be labeled Quechua I (or Quechua B, central) and Quechua II (or Quechua A, peripheral). Within the two groups, there are few sharp boundaries, making them dialect continua.

But, there is a secondary division in Quechua II between the grammatically simplified northern varieties of Ecuador, Quechua II-B, known there as Kichwa, and the generally more conservative varieties of the southern highlands, Quechua II-C, which include the old Inca capital of Cusco. The closeness is at least in part because of the influence of Cusco Quechua on the Ecuadorean varieties in the Inca Empire. Because Northern nobles were required to educate their children in Cusco, this was maintained as the prestige dialect in the north.

Speakers from different points within any of the three regions can generally understand one another reasonably well. There are nonetheless significant local-level differences across each. (Wanka Quechua, in particular, has several very distinctive characteristics that make the variety more difficult to understand, even for other Central Quechua speakers.) Speakers from different major regions, particularly Central or Southern Quechua, are not able to communicate effectively.

The lack of mutual intelligibility among the dialects is the basic criterion that defines Quechua not as a single language, but as a language family. The complex and progressive nature of how speech varies across the dialect continua makes it nearly impossible to differentiate discrete varieties; "Ethnologue" lists 45 varieties which are then divided into 2 groups; Central and Peripheral. Due to the non-intelligibility among the 2 groups, they are all classified as separate languages..

As a reference point, the overall degree of diversity across the family is a little less than that of the Romance or Germanic families, and more of the order of Slavic or Arabic. The greatest diversity is within Central Quechua, or Quechua I, which is believed to lie close to the homeland of the ancestral Proto-Quechua language.

Alfredo Torero devised the traditional classification, the three divisions above, plus a fourth, a northern or Peruvian branch. The latter causes complications in the classification, however, as the northern dialects (Cajamarca–Cañaris, Pacaraos, and Yauyos–Chincha) have features of both Quechua I and Quechua II, and so are difficult to assign to either.

Torero classifies them as the following:

Willem Adelaar adheres to the Quechua I / Quechua II (central/peripheral) bifurcation. But, partially following later modifications by Torero, he reassigns part of Quechua II-A to Quechua I:

Landerman (1991) does not believe a truly genetic classification is possible and divides Quechua II so that the family has four geographical–typological branches: Northern, North Peruvian, Central, and Southern. He includes Chachapoyas and Lamas in North Peruvian Quechua so Ecuadorian is synonymous with Northern Quechua.

Quechua I (Central Quechua, "Waywash") is spoken in Peru's central highlands, from the Ancash Region to Huancayo. It is the most diverse branch of Quechua, to the extent that its divisions are commonly considered different languages.

Quechua II (Peripheral Quechua, "Wamp'una" "Traveler")

This is a sampling of words in several Quechuan languages:

Quechua shares a large amount of vocabulary, and some striking structural parallels, with Aymara, and the two families have sometimes been grouped together as a "Quechumaran family". That hypothesis is generally rejected by specialists, however. The parallels are better explained by mutual influence and borrowing through intensive and long-term contact. Many Quechua–Aymara cognates are close, often closer than intra-Quechua cognates, and there is little relationship in the affixal system.

A number of Quechua loanwords have entered English via Spanish, including "coca", "cockroach", "condor", "guano", "jerky", "llama", "pampa", "poncho", "puma", "quinine", "quinoa", "vicuña", and, possibly, "gaucho". The word "lagniappe" comes from the Quechuan word "yapay" ("to increase; to add") with the Spanish article "la" in front of it, "la yapa" or "la ñapa" in Spanish.

The influence on Latin American Spanish includes such borrowings as "papa" for "potato", "chuchaqui" for "hangover" in Ecuador, and diverse borrowings for "altitude sickness", in Bolivia from Quechuan "suruqch'i" to Bolivian "sorojchi", in Ecuador and Peru "soroche".

A rare instance of a Quechua word being taken into general Spanish use is given by "carpa" for "tent" (Quechua "karpa").

In Bolivia, particularly, Quechua words are used extensively even by non-Quechua speakers. These include wawa (baby, infant), ch'aki (hangover), misi (cat), juk'ucho (mouse), q'omer uchu (green pepper), jacu ("lets go"), chhiri and chhurco (curly haired), among many others. Quechua grammar also enters Bolivian Spanish, such as the use of the suffix -ri. In Bolivian Quechua, -ri is added to verbs to signify an action is performed with affection or, in the imperative, as a rough equivalent to please. In Bolivia -ri is often included in the Spanish imperative to imply "please" or to soften commands. For example, the standard "pásame" (pass me), becomes pasarime.

Quechua has borrowed a large number of Spanish words, such as "piru" (from "pero", but), "bwenu" (from "bueno", good), iskwila (from "escuela," school), waka (from "vaca," cow) and "burru" (from "burro", donkey).

At first, Spaniards referred to the language of the Inca empire as the "lengua general", the "general language". The name "quichua" was first used in 1560 by Domingo de Santo Tomás in his "Grammatica o arte de la lengua general de los indios de los reynos del Perú". It is not known what name the native speakers gave to their language before colonial times and whether it was Spaniards who called it "quechua".

There are two possible etymologies of Quechua as the name of the language. There is a possibility that the name Quechua was derived from "*qiĉ.wa", the native word which originally meant the "temperate valley" altitude ecological zone in the Andes (suitable for maize cultivation) and to its inhabitants.

Alternatively, Pedro Cieza de León and Inca Garcilaso de la Vega, the early Spanish chroniclers, mention the existence of a people called Quichua in the present Apurímac Region, and it could be inferred that their name was given to the entire language.

The Hispanicised spellings "Quechua" and "Quichua" have been used in Peru and Bolivia since the 17th century, especially after the Third Council of Lima. Today, the various local pronunciations of "Quechua Simi" include , , , and .

Another name that native speakers give to their own language is "runa simi", "language of man/people"; it also seems to have emerged during the colonial period.

The description below applies to Cusco Quechua; there are significant differences in other varieties of Quechua.

Quechua only has three vowel phonemes: and , as in Aymara (including Jaqaru). Monolingual speakers pronounce them as respectively, but Spanish realizations may also be found. When the vowels appear adjacent to uvular consonants (, , and ), they are rendered more like , and , respectively.

Voicing is not phonemic in the Quechua native vocabulary of the modern Cusco variety.

About 30% of the modern Quechua vocabulary is borrowed from Spanish, and some Spanish sounds (such as , , , ) may have become phonemic even among monolingual Quechua-speakers.

Cusco Quechua, North- and South-Bolivian Quechua are the only varieties of Quechua to have glottalized consonants, and they, along with certain kinds of Ecuadorian Kichwa, are the only varieties with aspirated consonants. Because reflexes of a given Proto-Quechua word may have different stops in neighboring dialects (Proto-Quechua "čaki" 'foot' becomes "č’aki" and "čaka" 'bridge' becomes "čaka"), they are thought to be innovations in Quechua from Aymara, borrowed independently after branching off from Proto-Quechua.

Gemination of the tap results in a trill .

Stress is penultimate in most dialects of Quechua. In some varieties, factors such as apocope of word-final vowels may cause exceptional final stress.

Quechua has been written using the Roman alphabet since the Spanish conquest of the Inca Empire. However, written Quechua is rarely used by Quechua speakers because of the lack of printed material in Quechua.

Until the 20th century, Quechua was written with a Spanish-based orthography, for example "Inca, Huayna Cápac, Collasuyo, Mama Ocllo, Viracocha, quipu, tambo, condor". This orthography is the most familiar to Spanish speakers and so has been used for most borrowings into English, which essentially always happen through Spanish.

In 1975, the Peruvian government of Juan Velasco Alvarado adopted a new orthography for Quechua. This is the system preferred by the Academia Mayor de la Lengua Quechua, which results innthe following spellings of the examples listed above: "Inka, Wayna Qhapaq, Qollasuyu, Mama Oqllo, Wiraqocha, khipu, tampu, kuntur". This orthography has the following features:

In 1985, a variation of this system was adopted by the Peruvian government that uses the Quechuan three-vowel system, resulting in the following spellings: "Inka, Wayna Qhapaq, Qullasuyu, Mama Uqllu, Wiraqucha, khipu, tampu, kuntur".

The different orthographies are still highly controversial in Peru. Advocates of the traditional system believe that the new orthographies look too foreign and believe that it makes Quechua harder to learn for people who have first been exposed to written Spanish. Those who prefer the new system maintain that it better matches the phonology of Quechua, and they point to studies showing that teaching the five-vowel system to children later causes reading difficulties in Spanish.

For more on this, see Quechuan and Aymaran spelling shift.

Writers differ in the treatment of Spanish loanwords. These are sometimes adapted to the modern orthography and sometimes left as in Spanish. For instance, "I am Roberto" could be written "Robertom kani" or "Ruwirtum kani". (The "-m" is not part of the name; it is an evidential suffix, showing how the information is known: firsthand, in this case.)

The Peruvian linguist Rodolfo Cerrón Palomino has proposed an orthographic norm for all of Southern Quechua: this Standard Quechua ("el Quechua estándar" or "Hanan Runasimi") conservatively integrates features of the two widespread dialects Ayacucho Quechua and Cusco Quechua. For instance:

The Spanish-based orthography is now in conflict with Peruvian law. According to article 20 of the decree "Decreto Supremo No 004-2016-MC", which approves regulations relative to Law 29735, published in the official newspaper El Peruano on July 22, 2016, adequate spellings of the toponyms in the normalized alphabets of the indigenous languages must progressively be proposed, with the aim of standardizing the spellings used by the National Geographic Institute "(Instituto Geográfico Nacional, IGN)" The IGN implements the necessary changes on the official maps of Peru.

Quechua is an agglutinating language. Words are built up from basic roots followed by several suffixes which each carry one meaning. All varieties of Quechua are very regular agglutinative languages, as opposed to isolating or fusional ones [Thompson]. Their normal sentence order is SOV (subject–object–verb). Their large number of suffixes changes both the overall significance of words and their subtle shades of meaning. Notable grammatical features include bipersonal conjugation (verbs agree with both subject and object), evidentiality (indication of the source and veracity of knowledge), a set of topic particles, and suffixes indicating who benefits from an action and the speaker's attitude toward it, but some languages and varieties may lack some of the characteristics.

In Quechua, there are seven pronouns. Quechua has two first-person plural pronouns ("we" in English). One is called the inclusive, which is used if the speaker wishes to include the addressee ("we and you"). The other form is called the exclusive, which is used when the addressee is excluded ("we without you"). Quechua also adds the suffix "-kuna" to the second and third person singular pronouns "qam" and "pay" to create the plural forms, "qam-kuna" and "pay-kuna".

Adjectives in Quechua are always placed before nouns. They lack gender and number and are not declined to agree with substantives.


Noun roots accept suffixes that indicate person (defining of possession, not identity), number, and case. In general, the personal suffix precedes that of number. In the Santiago del Estero variety, however, the order is reversed. From variety to variety, suffixes may change.

Adverbs can be formed by adding "-ta" or, in some cases, "-lla" to an adjective: "allin – allinta" ("good – well"), "utqay – utqaylla" ("quick – quickly"). They are also formed by adding suffixes to demonstratives: "chay" ("that") – "chaypi" ("there"), "kay" ("this") – "kayman" ("hither").

There are several original adverbs. For Europeans, it is striking that the adverb "qhipa" means both "behind" and "future" and "ñawpa" means "ahead, in front" and "past". Local and temporal concepts of adverbs in Quechua (as well as in Aymara) are associated to each other reversely, compared to European languages. For the speakers of Quechua, we are moving backwards into the future (we cannot see it: it is unknown), facing the past (we can see it: it is remembered).

The infinitive forms (unconjugated) have the suffix "-y" ("much'a"= "kiss"; "much'a-y" = "to kiss"). These are the endings for the indicative:
The suffixes shown in the table above usually indicate the subject; the person of the object is also indicated by a suffix ("-a-" for first person and "-su-" for second person), which precedes the suffixes in the table. In such cases, the plural suffixes from the table ("-chik" and "-ku") can be used to express the number of the object rather than the subject.

Various suffixes are added to the stem to change the meaning. For example, "-chi" is a causative and "-ku" is a reflexive (example: "wañuy" = "to die"; "wañuchiy" = to kill "wañuchikuy" = "to commit suicide"); "-naku" is used for mutual action (example: "marq'ay"= "to hug"; "marq'anakuy"= "to hug each other"), and "-chka" is a progressive, used for an ongoing action (e.g., "mikhuy" = "to eat"; "mikhuchkay" = "to be eating").

Particles are indeclinable: they do not accept suffixes. They are relatively rare, but the most common are "arí" ("yes") and "mana" ("no"), although "mana" can take some suffixes, such as "-n"/"-m" ("manan"/"manam"), "-raq" ("manaraq", not yet) and "-chu" ("manachu?", or not?), to intensify the meaning. Also used are "yaw" ("hey", "hi"), and certain loan words from Spanish, such as "piru" (from Spanish "pero" "but") and "sinuqa" (from "sino" "rather").

The Quechuan languages have three different morphemes that mark evidentiality. Evidentiality refers to a morpheme whose primary purpose is to indicate the source of information. In the Quechuan languages, evidentiality is a three-term system: there are three evidential morphemes that mark varying levels of source information. The markers can apply to the first, second, and third persons. The chart below depicts an example of these morphemes from Wanka Quechua:
The parentheses around the vowels indicate that the vowel can be dropped in when following an open vowel. For the sake of cohesiveness, the above forms are used to discuss the evidential morphemes. However, it should be noted that there are dialectal variations to the forms. The variations will be presented in the following descriptions.

The following sentences provide examples of the three evidentials and further discuss the meaning behind each of them.

Regional variations: In Cusco Quechua, the direct evidential presents itself as "–mi" and "–n".

The evidential "–mi" indicates that the speaker has a "strong personal conviction the veracity of the circumstance expressed." It has the basis of direct personal experience.

Wanka Quechua

I saw them with my own eyes.

In Quechuan languages, not specified by the source, the inference morpheme appears as "–ch(i), -ch(a), -chr(a)".

The "–chr(a)" evidential indicates that the utterance is an inference or form of conjecture. That inference relays the speaker’s non-commitment to the truth-value of the statement. It also appears in cases such as acquiescence, irony, interrogative constructions, and first person inferences. These uses constitute nonprototypical use and will be discussed later in the "changes in meaning and other uses" section.

Wanka Quechua

I think they will probably come back.

Regional variations: It can appear as "–sh(i)" or "–s(i)" depending on the dialect.

With the use of this morpheme, the speaker "serves as a conduit through which information from another source passes." The information being related is hearsay or revelatory in nature. It also works to express the uncertainty of the speaker regarding the situation. However, it also appears in other constructions that are discussed in the "changes in meaning" section.

Wanka Quechua

(I was told) Shanti borrowed it.

Hintz discusses an interesting case of evidential behavior found in the Sihaus dialect of Ancash Quechua. The author postulates that instead of three single evidential markers, that Quechuan language contains three pairs of evidential markers.

It may have been noted the evidential morphemes have been referred to as markers or morphemes. The literature seems to differ on whether or not the evidential morphemes are acting as affixes or clitics, in some cases, such as Wanka Quechua, enclitics. Lefebvre and Muysken (1998) discuss this issue in terms of case but remark the line between affix and clitic is not clear. Both terms are used interchangeably throughout these sections.

Evidentials in the Quechuan languages are "second position enclitics", which usually attach to the first constituent in the sentence, as shown in this example.

Once, there were an old man and an old woman.

They can, however, also occur on a focused constituent.

It is now that Pedro is building the house.

Sometimes, the affix is described as attaching to the focus, particularly in the Tarma dialect of Yaru Quechua, but this does not hold true for all varieties of Quechua. In Huanuco Quechua, the evidentials may follow any number of topics, marked by the topic marker "–qa", and the element with the evidential must precede the main verb or be the main verb.

However, there are exceptions to that rule, and the more topics there are in a sentence, the more likely the sentence is to deviate from the usual pattern.

When she (the witch) reached the peak, God had already taken the child up into heaven.

Evidentials can be used to relay different meanings depending on the context and perform other functions. The following examples are restricted to Wanka Quechua.

The direct evidential, -mi

The direct evidential appears in wh-questions and yes/no questions. By considering the direct evidential in terms of prototypical semantics, it seems somewhat counterintuitive to have a direct evidential, basically an evidential that confirms the speaker’s certainty about a topic, in a question. However, if one focuses less on the structure and more on the situation, some sense can be made. The speaker is asking the addressee for information so the speaker assumes the speaker knows the answer. That assumption is where the direct evidential comes into play. The speaker holds a certain amount of certainty that the addressee will know the answer. The speaker interprets the addressee as being in "direct relation" to the proposed content; the situation is the same as when, in regular sentences, the speaker assumes direct relation to the proposed information.

When did he come back from Huancayo?

The direct evidential affix is also seen in yes/no questions, similar to the situation with wh-questions. Floyd describes yes/no questions as being "characterized as instructions to the addressee to assert one of the propositions of a disjunction." Once again, the burden of direct evidence is being placed on the addressee, not on the speaker. The question marker in Wanka Quechua, "-chun", is derived from the negative –chu marker and the direct evidential (realized as –n in some dialects).

Is he going to Tarma?

While "–chr(a)" is usually used in an inferential context, it has some non-prototypical uses.

"Mild Exhortation"
In these constructions the evidential works to reaffirm and encourage the addressee’s actions or thoughts.

Yes, tell them, "I've gone farther."

This example comes from a conversation between husband and wife, discussing the reactions of their family and friends after they have been gone for a while. The husband says he plans to stretch the truth and tell them about distant places to which he has gone, and his wife (in the example above) echoes and encourages his thoughts.

"Acquiescence"
With these, the evidential is used to highlight the speaker’s assessment of inevitability of an event and acceptance of it. There is a sense of resistance, diminished enthusiasm, and disinclination in these constructions.

I suppose I'll pay you then.

This example comes from a discourse where a woman demands compensation from the man (the speaker in the example) whose pigs ruined her potatoes. He denies the pigs as being his but finally realizes he may be responsible and produces the above example.

"Interrogative"
Somewhat similar to the "–mi" evidential, the inferential evidential can be found in content questions. However, the salient difference between the uses of the evidentials in questions is that in the "–m(i)" marked questions, an answer is expected. That is not the case with "–chr(a)" marked questions.

I wonder what we will give our families when we arrive.

"Irony"
Irony in language can be a somewhat complicated topic in how it functions differently in languages, and by its semantic nature, it is already somewhat vague. For these purposes, it is suffice to say that when irony takes place in Wanka Quechua, the "–chr(a)" marker is used.
(I suppose) That's how you learn [that is the way in which you will learn].

This example comes from discourse between a father and daughter about her refusal to attend school. It can be interpreted as a genuine statement (perhaps one can learn by resisting school) or as an ironic statement (that is an absurd idea).

Aside from being used to express hearsay and revelation, this affix also has other uses.

"Folktales, myths, and legends"

Because folktales, myths, and legends are, in essence, reported speech, it follows that the hearsay marker would be used with them. Many of these types of stories are passed down through generations, furthering this aspect of reported speech. A difference between simple hearsay and folktales can be seen in the frequency of the "–sh(i)" marker. In normal conversation using reported speech, the marker is used less, to avoid redundancy.

"Riddles"

Riddles are somewhat similar to myths and folktales in that their nature is to be passed by word of mouth.

In certain grammatical structures, the evidential marker does not appear at all. In all Quechuan languages the evidential will not appear in a dependent clause. Sadly, no example was given to depict this omission.
Omissions occur in Quechua. The sentence is understood to have the same evidentiality as the other sentences in the context. Quechuan speakers vary as to how much they omit evidentials, but they occur only in connected speech.

An interesting contrast to omission of evidentials is overuse of evidentials. If a speaker uses evidentials too much with no reason, competence is brought into question. For example, the overuse of –m(i) could lead others to believe that the speaker is not a native speaker or, in some extreme cases, that one is mentally ill.

By using evidentials, the Quechua culture has certain assumptions about the information being relayed. Those who do not abide by the cultural customs should not be trusted. A passage from Weber (1986) summarizes them nicely below:

Evidentials also show that being precise and stating the source of one’s information is extremely important in the language and the culture. Failure to use them correctly can lead to diminished standing in the community. Speakers are aware of the evidentials and even use proverbs to teach children the importance of being precise and truthful. Precision and information source are of the utmost importance. They are a powerful and resourceful method of human communication.

Although the body of literature in Quechua is not as sizable as its historical and current prominence would suggest, it is nevertheless not negligible.

As in the case of the pre-Columbian Mesoamerica, there are a number of surviving Andean documents in the local language that were written down in Latin characters after the European conquest, but they express, to a great extent, the culture of pre-Conquest times. That type of Quechua literature is somewhat scantier, but nevertheless significant. It includes the so-called Huarochirí Manuscript (1598), describing the mythology and religion of the valley of Huarochirí as well as Quechua poems quoted within the Spanish-language texts of some chronicles dealing with the pre-Conquest period. There are a number of anonymous or signed Quechua dramas dating from the post-conquest period (starting from the 17th century), some of which deal with the Inca era, while most are on religious topics and of European inspiration. The most famous dramas is "Ollantay" and the plays describing the death of Atahualpa. For example, Juan de Espinosa Medrano wrote several dramas in the language. Poems in Quechua were also composed during the colonial period.

Dramas and poems continued to be written in the 19th and especially in 20th centuries as well; in addition, in the 20th century and more recently, more prose has been published. However, few literary forms were made present in the 19th century as European influences limited literary criticism. While some of that literature consists of original compositions (poems and dramas), the bulk of 20th century Quechua literature consists of traditional folk stories and oral narratives. Johnny Payne has translated two sets of Quechua oral short stories, one into Spanish and the other into English.

Many Andean musicians write and sing in their native languages, including Quechua and Aymara. Notable musical groups are Los Kjarkas, Kala Marka, J'acha Mallku, Savia Andina, Wayna Picchu, Wara and many others.




</doc>
<doc id="25291" url="https://en.wikipedia.org/wiki?curid=25291" title="Protein quaternary structure">
Protein quaternary structure

Protein quaternary structure is the number and arrangement of multiple folded protein subunits in a multi-subunit complex. It includes organisations from simple dimers to large homooligomers and complexes with defined or variable numbers of subunits. It can also refer to biomolecular complexes of proteins with nucleic acids and other cofactors.

Many proteins are actually assemblies of multiple polypeptide chains. The quaternary structure refers to the number and arrangement of the protein subunits with respect to one another. Examples of proteins with quaternary structure include hemoglobin, DNA polymerase, and ion channels.

Enzymes composed of subunits with diverse functions are sometimes called holoenzymes, in which some parts may be known as regulatory subunits and the functional core is known as the catalytic subunit. Other assemblies referred to instead as multiprotein complexes also possess quaternary structure. Examples include nucleosomes and microtubules. Changes in quaternary structure can occur through conformational changes within individual subunits or through reorientation of the subunits relative to each other. It is through such changes, which underlie cooperativity and allostery in "multimeric" enzymes, that many proteins undergo regulation and perform their physiological function.

The above definition follows a classical approach to biochemistry, established at times when the distinction between a protein and a functional, proteinaceous unit was difficult to elucidate. More recently, people refer to protein–protein interaction when discussing quaternary structure of proteins and consider all assemblies of proteins as protein complexes.

The number of subunits in an oligomeric complex is described using names that end in -mer (Greek for "part, subunit"). Formal and Greco-Latinate names are generally used for the first ten types and can be used for up to twenty subunits, whereas higher order complexes are usually described by the number of subunits, followed by -meric.

Although complexes higher than octamers are rarely observed for most proteins, there are some important exceptions. Viral capsids are often composed of multiples of 60 proteins. Several molecular machines are also found in the cell, such as the proteasome (four heptameric rings = 28 subunits), the transcription complex and the spliceosome. The ribosome is probably the largest molecular machine, and is composed of many RNA and protein molecules.

In some cases, proteins form complexes that then assemble into even larger complexes. In such cases, one uses the nomenclature, e.g., "dimer of dimers" or "trimer of dimers", to suggest that the complex might dissociate into smaller sub-complexes before dissociating into monomers.

Protein quaternary structure can be determined using a variety of experimental techniques that require a sample of protein in a variety of experimental conditions. The experiments often provide an estimate of the mass of the native protein and, together with knowledge of the masses and/or stoichiometry of the subunits, allow the quaternary structure to be predicted with a given accuracy. It is not always possible to obtain a precise determination of the subunit composition for a variety of reasons.

The number of subunits in a protein complex can often be determined by measuring the hydrodynamic molecular volume or mass of the intact complex, which requires native solution conditions. For "folded" proteins, the mass can be inferred from its volume using the partial specific volume of 0.73 ml/g. However, volume measurements are less certain than mass measurements, since "unfolded" proteins appear to have a much larger volume than folded proteins; additional experiments are required to determine whether a protein is unfolded or has formed an oligomer.

Some bioinformatics methods were developed for predicting the quaternary structural attributes of proteins based on their sequence information by using various modes of pseudo amino acid composition (see, e.g., refs.).




Methods that measure the mass or volume under unfolding conditions (such as 
MALDI-TOF mass spectrometry and SDS-PAGE) are generally not useful, since non-native conditions usually cause the complex to dissociate into monomers. However, these may sometimes be applicable; for example, the experimenter may apply SDS-PAGE after first treating the intact complex with chemical cross-link reagents.

Proteins are capable of forming very tight complexes. For example, ribonuclease inhibitor binds to ribonuclease A with a roughly 20 fM dissociation constant. Other proteins have evolved to bind specifically to unusual moieties on another protein, e.g., biotin groups (avidin), phosphorylated tyrosines (SH2 domains) or proline-rich segments (SH3 domains).




</doc>
<doc id="25292" url="https://en.wikipedia.org/wiki?curid=25292" title="Quest for Glory">
Quest for Glory

Quest for Glory is a series of hybrid adventure/role-playing video games, which were designed by Corey and Lori Ann Cole. The series was created in the Sierra Creative Interpreter, a toolset developed at Sierra specifically to assist with adventure game development. The series combines humor, puzzle elements, themes and characters borrowed from various legends, puns, and memorable characters, creating a 5-part series in the Sierra stable.

The series was originally titled "Hero's Quest". However, Sierra failed to trademark the name. The Milton Bradley Company successfully trademarked an electronic version of their unrelated joint Games Workshop board game, "HeroQuest", which forced Sierra to change the series' title to "Quest for Glory". This decision meant that all future games in the series (as well as newer releases of "Hero's Quest I") used the new name.

Lori Cole pitched Quest for Glory to Sierra as a: "rich, narrative-driven, role-playing experience".

The series consisted of five games, each of which followed directly upon the events of the last. New games frequently referred to previous entries in the series, often in the form of cameos by recurring characters. The objective of the series is to transform the player character from an average adventurer to a hero by completing non-linear quests.

The game also was revolutionary in its character import system. This allowed players to import their individual character, including the skills and wealth s/he had acquired, from one game to the next.

Hybrids by their gameplay and themes, the games feature serious stories leavened with humor throughout. There are real dangers to face, and true heroic feats to perform, but silly details and overtones creep in (when the drama of adventuring does not force them out). Cheap word play is particularly frequent, to the point that the second game's ending refers to itself as the hero's "latest set of adventures and miserable puns."

The games have recurring story elements. For example, each installment in the series requires the player to create a dispel potion.

The games include a number of easter eggs, including a number of allusions to other Sierra games. For example, if a player types "pick nose" in the first game, (or clicks the lockpick icon on the player in the new version), if their lock-picking skill is high enough, the game responds: "Success! You now have an open nose". If the skill is too low, the player could insert the lock pick too far, killing himself. Another example is Dr. Cranium, an allusion to "The Castle of Dr. Brain", in the fourth game.

Each game draws its inspiration from a different culture and mythology: (in order, Germanic/fairy tale; Middle Eastern/Arabian Nights; Egyptian/African; Slavic folklore; and finally Greco-Mediterranean) with the hero facing increasingly powerful opponents with help from characters who become more familiar from game to game.

Each game varies somewhat from the tradition it is derived from; for example, Baba Yaga, a character borrowed from Slavic folklore, appears in the first game which is based on German mythology. The second game, which uses Middle Eastern folklore, introduces several Arab and African-themed characters who reappear in the third game based on Egyptian mythology. Characters from every game and genre in the series reappear in the fourth and fifth games. In addition to deviating from the player's expectations of the culture represented in each game, the series also includes a number of intentional anachronisms, such as the pizza-loving, mad scientists in the later games.

Many CRPG enthusiasts consider the "Quest for Glory" series to be among the best in the genre, and the series is lauded for its non-linearity. The games are notable for blending the mechanics of adventure video games and roleplaying video games, their unique tone which combines pathos and humour, and the game systems which were ahead of their time, such as day-night cycles, non-playable characters which adhered to their own schedules within the games, and character improvement through both skill practice and point investiture. The website Polygon and the Kotaku blog have characterised the game as a precursor to modern day RPGs. Fraser Brown of the Destructoid blog considers the games: "one of the greatest adventure series of all time".

Rowan Kaizer of the blog Engadget credits the games' hybrid adventure and roleplaying systems for the series' success. "The binary succeed/fail form of adventure game puzzles tended to either make those games too easy or too hard," he wrote, "But most puzzles in "Quest For Glory" involved some kind of skill check for your hero. This meant that you could succeed at most challenges by practicing or exploring, instead of getting stuck on bizarre item-combination puzzles".

The first four games are hybrid Adventure/Role playing video games with real-time combat, while the fifth game switches to the Action/RPG genre.

The gameplay standards established in earlier Sierra adventure games are enhanced by the player's ability to choose his character's career path from among the three traditional role-playing game backgrounds: fighter, magic-user/wizard and thief. Further variation is added by the ability to customize the Hero's abilities, including the option of selecting skills normally reserved for another character class, leading to unique combinations often referred to as "hybrid characters". During the second or third games, a character can be initiated as a Paladin by performing honorable actions, changing his class and abilities, and receiving a unique sword. This applies when the character is exported into later games. Any character that finishes any game in the series (except "Dragon Fire", the last in the series) can be exported to a more recent game ("Shadows of Darkness" has a glitch which allows one to import characters from the same game), keeping the character's statistics and parts of its inventory. If the character received the paladin sword, he would keep the magic sword (Soulforge or Piotyr's sword) and special paladin magic abilities. A character imported into a later game in the series from any other game can be assigned any character class, including Paladin.

Each career path has its own strengths and weaknesses, and scenarios unique to the class because of the skills associated with it. Each class also has its own distinct way to solve various in-game puzzles, which encourage replay: some puzzles have up to four different solutions. For instance, if a door is closed, instead of lockpicking or casting an open spell, the fighter can simply knock down the door. The magic user and the thief are both non-confrontational characters, as they lack the close range ability of the fighter, but are better able to attack from a distance, using daggers or spells. An example of these separate paths can be seen early in the first game. A gold ring belonging to the healer rests in a nest on top of a tree; fighters might make it fall by hurling rocks, thieves may want to climb the tree, while a magic user can simply cast the fetch spell to retrieve the nest, and then, while the fighter and magic user return the ring for a reward, the thief can choose between returning or selling the same ring in the thieves' guild (which is not available for those not possessing the "thieving" skills). It is also possible to build, over the course of several games, a character that has points in every skill in the game, and can therefore perform nearly every task.

Each character class features special abilities unique to that class, as well as a shared set of attributes which can be developed by performing tasks and completing quests. In general, for a particular game the maximum value which can be reached for an ability is 100*[the number of that game]. "Quest for Glory V" allows stat bonuses which can push an attribute over the maximum and lets certain classes raise certain attributes beyond the normal limits. "Quest for Glory V" also features special kinds of equipment which lower some stats while raising others. At the beginning of each game, the player may assign points to certain attributes, and certain classes only have specific attributes enabled, although skills can be added for an extra cost.

General attributes influence all characters' classes and how they interact with objects and other people in the game; high values in strength allows movement of heavier objects and communication helps with bargaining goods with sellers. These attributes are changed by performing actions related to the skill; climbing a tree eventually increases the skill value in climb, running increases vitality, and so on. There are also complementing skills which are only of associated with some classes; parry (the ability to block a blow with the sword), for instance, is mainly used by fighters and paladins, lock picking and sneaking thief's hobby, and the ability to cast magic spells is usually associated with magic user.

Vital statistics are depleted by performing some actions. Health, (determined by strength and vitality), determines the hit points of the character, which decreases when the player is attacked or harms himself. Stamina, (based on agility and vitality), limits the number of actions (exercise, fighting, running, etc.) the character is able to perform before needing rest or risking injury. Mana is only required by characters with skill in magic, and is calculated according to the character's intelligence and magic attributes.

Puzzle and Experience points only show the development of the player and his progress in the game, though in the first game also affects the kind of random encounters a player faces, as some monsters only appear after a certain level of experience is reached.

In the valley barony of Spielburg, the evil ogress Baba Yaga has cursed the land and the baron who tried to drive her off. His children have disappeared, while the land is ravaged by monsters and brigands. The Valley of Spielburg is in need of a Hero able to solve these problems.

The original game was released in 1989 while a VGA remake was released in 1992.

"Quest for Glory II: Trial by Fire" takes place in the land of Shapeir, in the world of Gloriana. Directly following from the events of the first game, the newly proclaimed Hero of Spielburg travels by flying carpet with his friends Abdulla Doo, Shameen and Shema to the desert city of Shapeir. The city is threatened by magical elementals, while the Emir Arus al-Din of Shapeir's sister city Raseir is missing and his city fallen under tyranny.

"Quest for Glory II" is the only game in the series not to have originated or have been remade beyond the EGA graphics engine by Sierra, but AGD Interactive released a VGA fan remake of the game using the Adventure Game Studio engine on August 24, 2008.

Rakeesh the Paladin brings the Hero (and Prince of Shapeir) along with Uhura and her son Simba to his homeland, the town of Tarna in a jungle and savannah country called Fricana that resembles central African ecosystems.

Tarna is on the brink of war; the Simbani, the tribe of Uhura, are ready to do battle with the Leopardmen. Each tribe has stolen a sacred relic from the other, and both refuse to return it until the other side does. The Hero must prevent the war then thwart a demon who may be loosed upon the world.

Drawn without warning from his victory in Fricana, the Hero arrives without equipment or explanation in the middle of the hazardous Dark One Caves in the distant land of Mordavia. While struggling to survive in this land plagued with undead, the Hero must prevent a dark power from summoning eternal darkness into the world.

Erasmus introduces the player character, the Hero, to the Greece-like kingdom of Silmaria, whose king was recently assassinated. Thus, the traditional Rites of Rulership are due to commence, and the victor will be crowned king. The Hero enters the contest with the assistance of Erasmus, Rakeesh, and many old friends from previous entries in the series. The Hero competes against competitors, including the Silmarian guard Kokeeno Pookameeso, the warlord Magnum Opus, the hulking Gort, and the warrior Elsa Von Spielburg.


Originally, the series was to be a tetralogy, consisting of 4 games, with the following themes and cycles: the 4 cardinal directions, the 4 classical elements, the 4 seasons and 4 different mythologies.

This is what the creators originally had in mind:
However, when "" was designed, it was thought that it would be too difficult for the hero to go straight from Shapeir to Mordavia and defeat the Dark One. To solve the problem, a new game, "", was inserted into the canon, and resulting in a renumbering of the series. Evidence for this can be found in the end of "": the player is told that the next game will be "" and a fanged vampiric moon is shown, to hint at the next game's theme.

The developers discussed this in the Fall 1992 issue of Sierra's "InterAction" magazine, and an online chat room:
Somewhere between finishing "Trial by Fire" and cranking up the design process for "Shadows of Darkness", the husband-and-wife team realized a fifth chapter would have to be added to bridge the games. That chapter became "Wages of War".

The concept of seasons in the games represents the maturation of the Hero as he moves from story to story. It's a critical component in a series that – from the very beginning – was designed to be a defined quartet of stories, representing an overall saga with a distinct beginning, middle, and end.

In the first episode, the player is a new graduate of the Famous Adventurer's Correspondence School, ready to venture out into the springtime of his career and build a rep. It's a light-hearted, exhilarating journey into the unknown that can be replayed three times with three distinct outlooks at puzzle-solving.

In the second chapter – "Trial by Fire" – the Hero enters the summer of his experience, facing more difficult challenges with more highly developed skills. While the episode is more serious and dangerous than its predecessor, it retains the enchanting mixture of fantasy, challenge, and humor that made the first game a hit with so many fans.

Of all the reasons Lori and Corey found for creating a bridge between "Trial by Fire" and "Shadows of Darkenss", the most compelling was the feeling that the Hero character simply hadn't matured enough to face the very grim challenges awaiting him in Transylvania.

Along with the Hero, several recurring characters appear and re-appear throughout the series including: Rakeesh Sah Tarna, Baba Yaga, Abdullah Doo, Elsa von Spielburg, the evil Ad Avis, and others.

The fictional world in which the Quest for Glory series takes place includes the town of Spielburg (based on German folklore), the desert city of Shapeir (based on the Arabia of "One Thousand and One Nights"), the jungle city of Tarna (based on African mythology, especially Egypt), the hamlet of Mordavia (based on Slavic mythology) and Silmaria (based on Greek mythology). Adventures, monsters and story of the games are usually drawn from legends of the respective mythology on which a title is based, although there are several cross-over exceptions, like the Eastern European Baba Yaga also appearing in the first game, which is distinctly German.


</doc>
<doc id="25293" url="https://en.wikipedia.org/wiki?curid=25293" title="Quango">
Quango

A quango or QUANGO (less often QuANGO or QANGO) is a quasi-autonomous non-governmental organisation. The concept is most often applied in the United Kingdom and, to a lesser degree, Australia, Canada, Ireland, New Zealand, the United States, and other English-speaking countries. As its name suggests, a quango is a hybrid form of organization, with elements of both non-government organizations (NGOs) and public sector bodies. It is typically an organisation to which a government has devolved power, but which is still partly controlled and/or financed by government bodies.

In the UK, the term quango covers different "arm's-length" government bodies, including "non-departmental public bodies", non-ministerial government departments, and executive agencies. One UK example is the Forestry Commission, which is a non-ministerial government department responsible for forestry in England and Scotland.

The term "quasi-autonomous non-governmental organisation" was created in 1967 by Alan Pifer of the US-based Carnegie Foundation, in an essay on the independence and accountability of public-funded bodies that are incorporated in the private sector. Pifer's term was shortened to the acronym "" – later spelt quango – by Anthony Barker, a British participant during a follow-up conference on the subject.

It describes an ostensibly non-governmental organisation performing governmental functions, often in receipt of funding or other support from government, while mainstream NGOs mostly get their donations or funds from the public and other organisations that support their cause. Numerous quangos were created from the 1980s onwards. Examples in the United Kingdom include those engaged in the regulation of various commercial and service sectors, such as the Water Services Regulation Authority.

An essential feature of a quango in the original definition was that it should not be a formal part of the state structure. The term was then extended to apply to a range of organisations, such as executive agencies providing (from 1988) health, education and other services. Particularly in the UK, this occurred in a polemical atmosphere in which it was alleged that proliferation of such bodies was undesirable and should be reversed (see below). This spawned the related acronym "qualgo", a 'quasi-autonomous "local" government organisation'.

The less contentious term non-departmental public body (NDPB) is often employed to identify numerous organisations with devolved governmental responsibilities. The UK government's definition in 1997 of a non-departmental public body or quango was:
"The Times" has accused quangos of bureaucratic waste and excess. In 2005, Dan Lewis, author of "The Essential Guide to Quangos", claimed that the UK had 529 quangos, many of which were useless and duplicated the work of others.

In 2006 there were more than 800 quangos in Ireland, 482 at national and 350 at local level, with a total of 5,784 individual appointees and a combined annual budget of €13 billion.

The Cabinet Office 2009 report on non-departmental public bodies found that there are 766 NDPBs sponsored by the UK government.
The number has been falling: there were 790 in 2008 and 827 in 2007. The number of NDPBs has fallen by over 10% since 1997. Staffing and expenditure of NDPBs have increased. They employed 111,000 people in 2009 and spent £46.5 billion, of which £38.4 billion was directly funded by the Government.

Since the coalition government of Conservatives and Liberal Democrats was formed in May 2010, numerous NDPBs have been abolished under Conservative plans to reduce the overall budget deficit by reducing the size of the public sector. As of the end of July 2010, the government had abolished at least 80 NDPBs and warned many others that they faced mergers or deep cuts. In September 2010, "The Telegraph" published a leaked Cabinet Office list suggesting that a further 94 could be abolished, while four would be privatised and 129 merged. In August 2012, Cabinet Office minister Francis Maude said the government was on course to abolish 204 public bodies by 2015, and said this would create a net saving of at least £2.6 billion.

Use of the term quango is less common and therefore more controversial in the United States. However, Paul Krugman has stated that the US Federal Reserve is, effectively, "what the British call a quango... Its complex structure divides power between the federal government and the private banks that are its members, and in effect gives substantial autonomy to a governing board of long-term appointees."




</doc>
<doc id="25295" url="https://en.wikipedia.org/wiki?curid=25295" title="Quiver">
Quiver

A quiver is a container for holding arrows, bolts, or darts. It can be carried on an archer's body, the bow, or the ground, depending on the type of shooting and the archer's personal preference. Quivers were traditionally made of leather, wood, furs, and other natural materials, but are now often made of metal or plastic.

The most common style of quiver is a flat or cylindrical container suspended from the belt. They are found across many cultures from North America to China. Many variations of this type exist, such as being canted forwards or backwards, and being carried on the dominant hand side, off-hand side, or the small of the back. Some variants enclose almost the entire arrow, while minimalist "pocket quivers" consist of little more than a small stiff pouch that only covers the first few inches.

Back quivers are secured to the archer's back via straps, with the nock ends protruding above the dominant hand's shoulder. Arrows can be drawn over the shoulder rapidly by the nock. This style of quiver was used by native peoples of North America and Africa, and was also commonly depicted in bas-reliefs from ancient Assyria. While popular in cinema and 20th century art for depictions of medieval European characters (such as Robin Hood), this style of quiver was rarely used in medieval Europe. The Bayeux Tapestry shows that most bowmen in medieval Europe used belt quivers.

A ground quiver is used for both target shooting or warfare when the archer is shooting from a fixed location. They can be simply stakes in the ground with a ring at the top to hold the arrows, or more elaborate designs that hold the arrows within reach without the archer having to lean down to draw.

A modern invention, the bow quiver attaches directly to the bow's limbs and holds the arrows steady with a clip of some kind. They are popular with compound bow hunters as it allows one piece of equipment to be carried in the field without encumbering the hunter's body.

A style used by medieval English Longbowmen and several other cultures, an arrow bag is a simple drawstring cloth sack with a leather spacer at the top to keep the arrows divided. When not in use, the drawstring could be closed, completely covering the arrows so as to protect them from rain and dirt. Some had straps or rope sewn to them for carrying, but many either were tucked into the belt or set on the ground before battle to allow easier access.

Yebira refers to a variety of quiver designs. The Yazutsu is a different type, used in Kyudo. Arrows are removed from it before shooting, and held in the hand, so it is mainly used to transport and protect arrows.


Dr. Brian Marin, author of Ancient Warfare| Concordia Press| page 137



</doc>
<doc id="25296" url="https://en.wikipedia.org/wiki?curid=25296" title="Quid">
Quid

Quid may refer to:





</doc>
<doc id="25297" url="https://en.wikipedia.org/wiki?curid=25297" title="Quinine">
Quinine

Quinine is a medication used to treat malaria and babesiosis. This includes the treatment of malaria due to "Plasmodium falciparum" that is resistant to chloroquine when artesunate is not available. While used for restless legs syndrome, it is not recommended for this purpose due to the risk of side effects. It can be taken by mouth or used intravenously. Malaria resistance to quinine occurs in certain areas of the world. Quinine is also the ingredient in tonic water that gives it its bitter taste.
Common side effects include headache, ringing in the ears, trouble seeing, and sweating. More severe side effects include deafness, low blood platelets, and an irregular heartbeat. Use can make one more prone to sunburn. While it is unclear if use during pregnancy causes harm to the baby, use to treat malaria during pregnancy is still recommended. Quinine is an alkaloid, a naturally occurring chemical compound. How it works as a medicine is not entirely clear.
Quinine was first isolated in 1820 from the bark of a cinchona tree. Bark extracts have been used to treat malaria since at least 1632. It is on the World Health Organization's List of Essential Medicines, the most effective and safe medicines needed in a health system. The wholesale price in the developing world is about US$1.70 to $3.40 per course of treatment. In the United States a course of treatment is more than $200.

As of 2006, it is no longer recommended by the WHO (World Health Organization) as a first-line treatment for malaria, and it should be used only when artemisinins are not available. Quinine is also used to treat lupus and arthritis.

In the past, quinine was frequently prescribed as an off-label treatment for leg cramps at night, but this has become less common due to a Food and Drug Administration warning that this practice is associated with life-threatening side effects.

Quinine is a basic amine and is usually provided as a salt. Various existing preparations include the hydrochloride, dihydrochloride, sulfate, bisulfate and gluconate. In the United States, quinine sulfate is commercially available in 324-mg tablets under the brand name Qualaquin.

All quinine salts may be given orally or intravenously (IV); quinine gluconate may also be given intramuscularly (IM) or rectally (PR). The main problem with the rectal route is that the dose can be expelled before it is completely absorbed; in practice, this is corrected by giving a further half dose. No injectable preparation of quinine is licensed in the US; quinidine is used instead.

Quinine is a flavour component of tonic water and bitter lemon drink mixers. On the soda gun behind many bars, tonic water is designated by the letter "Q" representing quinine.

According to tradition, the bitter taste of anti-malarial quinine tonic led British colonials in India to mix it with gin, thus creating the iconic gin and tonic cocktail, which is still popular today. Nowadays, the amount of quinine in tonic is much lower and drinking it against malaria is useless. Quinine is an ingredient in both tonic water and bitter lemon. In the US, quinine is listed as an ingredient in some Diet Snapple flavors, including Cranberry-Raspberry.

In France, quinine is an ingredient of an "apéritif" known as "quinquina" or "Cap Corse", and the wine-based apéritif, Dubonnet. In Spain, quinine ("Peruvian bark") is sometimes blended into sweet Malaga wine, which is then called "Malaga Quina". In Italy, the traditional flavoured wine Barolo Chinato is infused with quinine and local herbs and is served as a "digestif". In Canada and Italy, quinine is an ingredient in the carbonated chinotto beverages Brio and San Pellegrino. In Scotland, the company A.G. Barr uses quinine as an ingredient in the carbonated and caffeinated beverage Irn-Bru. In Uruguay and Argentina, quinine is an ingredient of a PepsiCo tonic water named Paso de los Toros. In Denmark, it is used as an ingredient in the carbonated sports drink Faxe Kondi made by Royal Unibrew.

As a flavoring agent in beverages, quinine is limited to less than 83 parts per million in the United States, and 100 mg/l in the European Union.

Quinine (and quinidine) are used as the chiral moiety for the ligands used in Sharpless asymmetric dihydroxylation as well as for numerous other chiral catalyst backbones. Because of its relatively constant and well-known fluorescence quantum yield, quinine is used in photochemistry as a common fluorescence standard.

Quinine can cause abnormal heart rhythms, and should be avoided if possible in patients with atrial fibrillation, conduction defects, or heart block. Quinine can cause hemolysis in G6PD deficiency (an inherited deficiency), but this risk is small and the physician should not hesitate to use quinine in patients with G6PD deficiency when there is no alternative.

Quinine can cause unpredictable serious and life-threatening blood and cardiovascular reactions including low platelet count and hemolytic-uremic syndrome/thrombotic thrombocytopenic purpura (HUS/TTP), long QT syndrome and other serious cardiac arrhythmias including torsades de pointes, blackwater fever, disseminated intravascular coagulation, leukopenia, and neutropenia. Some people who have developed TTP due to quinine have gone on to develop kidney failure.

It can also cause serious hypersensitivity reactions include anaphylactic shock, urticaria, serious skin rashes, including Stevens-Johnson syndrome and toxic epidermal necrolysis, angioedema, facial edema, bronchospasm, granulomatous hepatitis, and itchiness.

The most common adverse effects involve a group of symptoms called cinchonism; almost everyone taking quinine has mild cinchonism , which can include headache, vasodilation and sweating, nausea, tinnitus, hearing impairment, vertigo or dizziness, blurred vision, and disturbance in color perception . More severe cinchonism includes vomiting, diarrhea, abdominal pain, deafness, blindness, and disturbances in heart rhythms. Cinchonism is much less common when quinine is given by mouth, but oral quinine is not well tolerated (quinine is exceedingly bitter and many patients will vomit after ingesting quinine tablets): Other drugs, such as Fansidar (sulfadoxine with pyrimethamine) or Malarone (proguanil with atovaquone), are often used when oral therapy is required. Quinine ethyl carbonate is tasteless and odourless, but is available commercially only in Japan. Blood glucose, electrolyte and cardiac monitoring are not necessary when quinine is given by mouth.

Quinine is theorized to be toxic to the Plasmodium falciparum by interfering with the parasite's ability to dissolve and metabolize hemoglobin. As with other quinoline antimalarial drugs, the mechanism of action of quinine has not been fully resolved. The most widely accepted hypothesis of its action is based on the well-studied and closely related quinoline drug, chloroquine. This model involves the inhibition of hemozoin biocrystallization in Heme Detoxification pathway, which facilitates the aggregation of cytotoxic heme. Free cytotoxic heme accumulates in the parasites, causing their deaths.

The UV absorption of quinine peaks around 350 nm (in UVA). Fluorescent emission peaks at around 460 nm (bright blue/cyan hue). Quinine is highly fluorescent (quantum yield ~0.58) in 0.1 M sulfuric acid solution.

Cinchona trees remain the only economically practical source of quinine. However, under wartime pressure, research towards its synthetic production was undertaken. A formal chemical synthesis was accomplished in 1944 by American chemists R.B. Woodward and W.E. Doering. Since then, several more efficient quinine total syntheses have been achieved, but none of them can compete in economic terms with isolation of the alkaloid from natural sources. The first synthetic organic dye, mauveine, was discovered by William Henry Perkin in 1856 while he was attempting to synthesize quinine.

The bark of "Remijia" contains 0.5–2% of quinine. The bark is cheaper than bark of "Cinchona", and as it has an intense taste. It is used for making tonic water.

Quinine was used as a muscle relaxant by the Quechua, who are indigenous to Peru, Bolivia and Ecuador, to halt shivering due to low temperatures. The Quechuas would mix the ground bark of cinchona trees with sweetened water to offset the bark's bitter taste, thus producing tonic water.

The Jesuits were the first to bring cinchona to Europe. The Spanish were aware of the medicinal properties of cinchona bark by the 1570s or earlier: Nicolás Monardes (1571) and Juan Fragoso (1572) both described a tree that was subsequently identified as the cinchona tree and whose bark was used to produce a drink to treat diarrhea. Quinine has been used in unextracted form by Europeans since at least the early 17th century. It was first used to treat malaria in Rome in 1631. During the 17th century, malaria was endemic to the swamps and marshes surrounding the city of Rome. Malaria was responsible for the deaths of several popes, many cardinals and countless common Roman citizens. Most of the priests trained in Rome had seen malaria victims and were familiar with the shivering brought on by the febrile phase of the disease. The Jesuit brother Agostino Salumbrino (1564–1642), an apothecary by training who lived in Lima, observed the Quechua using the bark of the cinchona tree for that purpose. While its effect in treating malaria (and hence malaria-induced shivering) was unrelated to its effect in controlling shivering from rigors, it was still a successful medicine for malaria. At the first opportunity, Salumbrino sent a small quantity to Rome to test as a malaria treatment. In the years that followed, cinchona bark, known as Jesuit's bark or Peruvian bark, became one of the most valuable commodities shipped from Peru to Europe. When King Charles II was cured of malaria at the end of the 17th Century with quinine, it became popular in London. It remained the antimalarial drug of choice until the 1940s, when other drugs took over.

The form of quinine most effective in treating malaria was found by Charles Marie de La Condamine in 1737. In 1820, French researchers Pierre Joseph Pelletier and Joseph Bienaimé Caventou first isolated quinine from the bark of a tree in the genus "Cinchona" - probably "Cinchona officinalis" - and subsequently named the substance. The name was derived from the original Quechua (Inca) word for the cinchona tree bark, "quina" or "quina-quina", which means "bark of bark" or "holy bark". Prior to 1820, the bark was first dried, ground to a fine powder, and then mixed into a liquid (commonly wine) which was then drunk. Large-scale use of quinine as a malaria prophylaxis started around 1850; for instance in 1853 Paul Briquet published a brief history and discussion of the literature on "quinquina".

Quinine also played a significant role in the colonization of Africa by Europeans. Quinine had been said to be the prime reason Africa ceased to be known as the "white man's grave". A historian has stated, "it was quinine's efficacy that gave colonists fresh opportunities to swarm into the Gold Coast, Nigeria and other parts of west Africa".

To maintain their monopoly on cinchona bark, Peru and surrounding countries began outlawing the export of cinchona seeds and saplings beginning in the early 19th century. The Dutch government persisted in its attempts to smuggle the seeds, and in the late 19th century the Dutch managed to grow the plants in their Indonesian plantations. Soon they became the main suppliers of the plant, and in 1913 they set up the Kina Bureau, a cartel of cinchona producers charged with controlling price and production. By the 1930s Dutch plantations in Java were producing 22 million pounds of cinchona bark, or 97% of the world's quinine production. U.S. attempts to prosecute the Kina Bureau proved unsuccessful. During World War II, Allied powers were cut off from their supply of quinine when the Germans conquered the Netherlands and the Japanese controlled the Philippines and Indonesia. The United States had managed to obtain four million cinchona seeds from the Philippines and began operating cinchona plantations in Costa Rica. Nonetheless, such supplies came too late; tens of thousands of US troops in Africa and the South Pacific died due to the lack of quinine. Despite controlling the supply, the Japanese did not make effective use of quinine, and thousands of Japanese troops in the southwest Pacific died as a result. Quinine remained the antimalarial drug of choice until after World War II, when other drugs, such as chloroquine, that have fewer side effects largely replaced it.

"Bromo Quinine" were brand name cold tablets containing quinine, manufactured by Grove Laboratories. They were first marketed in 1889 and available until at least the 1960s.

From 1969 to 1992, the US Food and Drug Administration (FDA) received 157 reports of health problems related to quinine use, including 23 which had resulted in death. In 1994, the FDA banned the marketing of over-the-counter quinine as a treatment for nocturnal leg cramps. Pfizer Pharmaceuticals had been selling the brand name Legatrin for this purpose. Also sold as a Softgel (by SmithKlineBeecham) as Q-vel. Doctors may still prescribe quinine, but the FDA has ordered firms to stop marketing unapproved drug products containing quinine. The FDA is also cautioning consumers about off-label use of quinine to treat leg cramps. Quinine is approved for treatment of malaria, but is also commonly prescribed to treat leg cramps and similar conditions. Because malaria is life-threatening, the risks associated with quinine use are considered acceptable when used to treat that affliction.

Though Legatrin was banned by the FDA for the treatment of leg cramps, the drug manufacturer URL Mutual has branded a quinine-containing drug named Qualaquin. It is marketed as a treatment for malaria and is sold in the United States only by prescription. In 2004, the CDC reported only 1,347 confirmed cases of malaria in the United States.

Quinine is sometimes detected as a cutting agent in street drugs such as cocaine and heroin.

Quinine is used as a treatment for "Cryptocaryon irritans" (commonly referred to as white spot, crypto or marine ich) infection of marine aquarium fish.





</doc>
<doc id="25298" url="https://en.wikipedia.org/wiki?curid=25298" title="Quincy">
Quincy

Quincy may refer to:









</doc>
<doc id="25301" url="https://en.wikipedia.org/wiki?curid=25301" title="Quimby">
Quimby

Quimby may refer to:



</doc>
<doc id="25302" url="https://en.wikipedia.org/wiki?curid=25302" title="Quail">
Quail

Quail is a collective name for several genera of mid-sized birds generally placed in the order Galliformes. 
Old World quail are placed in the family Phasianidae, and New World quail are placed in the family Odontophoridae. The species of buttonquail are named for their superficial resemblance to quail, and form the family Turnicidae in the order Charadriiformes. The king quail, an Old World quail, often is sold in the pet trade, and within this trade is commonly, though mistakenly, referred to as a "button quail". Many of the common larger species are farm-raised for table food or egg consumption, and are hunted on game farms or in the wild, where they may be released to supplement the wild population, or extend into areas outside their natural range. In 2007, 40 million quail were produced in the U.S. 

The collective noun for a group of quail is a flock, covey or bevy.


Quail that have fed on hemlock (e.g., during migration) may induce acute renal failure due to accumulation of toxic substances from the hemlock in the meat; this problem is referred to as "coturnism".




</doc>
<doc id="25303" url="https://en.wikipedia.org/wiki?curid=25303" title="Quagmire (disambiguation)">
Quagmire (disambiguation)

Quagmire may refer to:







</doc>
<doc id="25305" url="https://en.wikipedia.org/wiki?curid=25305" title="Crossbow bolt">
Crossbow bolt

A quarrel or bolt is the arrow used in a crossbow. The name "quarrel" is derived from the French "carré", meaning square, referring to their typically square heads. Although their lengths vary, they are typically shorter than traditional arrows.


</doc>
<doc id="25308" url="https://en.wikipedia.org/wiki?curid=25308" title="Quasispecies model">
Quasispecies model

The quasispecies model is a description of the process of the Darwinian evolution of certain self-replicating entities within the framework of physical chemistry. A quasispecies is a large group or "cloud" of related genotypes that exist in an environment of high mutation rate (at stationary state), where a large fraction of offspring are expected to contain one or more mutations relative to the parent. This is in contrast to a species, which from an evolutionary perspective is a more-or-less stable single genotype, most of the offspring of which will be genetically accurate copies.

It is useful mainly in providing a qualitative understanding of the evolutionary processes of self-replicating macromolecules such as RNA or DNA or simple asexual organisms such as bacteria or viruses (see also viral quasispecies), and is helpful in explaining something of the early stages of the origin of life. Quantitative predictions based on this model are difficult because the parameters that serve as its input are impossible to obtain from actual biological systems. The quasispecies model was put forward by Manfred Eigen and Peter Schuster based on initial work done by Eigen.

When evolutionary biologists describe competition between species, they generally assume that each species is a single genotype whose descendants are mostly accurate copies. (Such genotypes are said to have a high reproductive "fidelity".) Evolutionarily, we are interested in the behavior and fitness of that one species or genotype over time.

Some organisms or genotypes, however, may exist in circumstances of low fidelity, where most descendants contain one or more mutations. A group of such genotypes is constantly changing, so discussions of which single genotype is the most fit become meaningless. Importantly, if many closely related genotypes are only one mutation away from each other, then genotypes in the group can mutate back and forth into each other. For example, with one mutation per generation, a child of the sequence AGGT could be AGTT, and a grandchild could be AGGT again. Thus we can envision a "cloud" of related genotypes that is rapidly mutating, with sequences going back and forth among different points in the cloud. Though the proper definition is mathematical, that cloud, roughly speaking, is a quasispecies.

Quasispecies behavior exists for large numbers of individuals existing at a certain (high) range of mutation rates.

In a species, though reproduction may be mostly accurate, periodic mutations will give rise to one or more competing genotypes. If a mutation results in greater replication and survival, the mutant genotype may out-compete the parent genotype and come to dominate the species. Thus, the individual genotypes (or species) may be seen as the units on which selection acts and biologists will often speak of a single genotype's fitness.

In a quasispecies, however, mutations are ubiquitous and so the fitness of an individual genotype becomes meaningless: if one particular mutation generates a boost in reproductive success, it can't amount to much because that genotype's offspring are unlikely to be accurate copies with the same properties. Instead, what matters is the "connectedness" of the cloud. For example, the sequence AGGT has 12 (3+3+3+3) possible single point mutants AGGA, AGGG, and so on. If 10 of those mutants are viable genotypes that may reproduce (and some of whose offspring or grandchildren may mutate back into AGGT again), we would consider that sequence a well-connected node in the cloud. If instead only two of those mutants are viable, the rest being lethal mutations, then that sequence is poorly connected and most of its descendants will not reproduce. The analog of fitness for a quasispecies is the tendency of nearby relatives within the cloud to be well-connected, meaning that more of the mutant descendants will be viable and give rise to further descendants within the cloud.

When the fitness of a single genotype becomes meaningless because of the high rate of mutations, the cloud as a whole or quasispecies becomes the natural unit of selection.

Quasispecies represents the evolution of high-mutation-rate viruses such as HIV and sometimes single genes or molecules within the genomes of other organisms. Quasispecies models have also been proposed by Jose Fontanari and Emmanuel David Tannenbaum to model the evolution of sexual reproduction. Quasispecies was also shown in compositional replicators (based on the Gard model for abiogenesis) and was also suggested to be applicable to describe cell's replication, which amongst other things requires the maintenance and evolution of the internal composition of the parent and bud.

The model rests on four assumptions:

In the quasispecies model, mutations occur through errors made in the process of copying already existing sequences. Further, selection arises because different types of sequences tend to replicate at different rates, which leads to the suppression of sequences that replicate more slowly in favor of sequences that replicate faster. However, the quasispecies model does not predict the ultimate extinction of all but the fastest replicating sequence. Although the sequences that replicate more slowly cannot sustain their abundance level by themselves, they are constantly replenished as sequences that replicate faster mutate into them. At equilibrium, removal of slowly replicating sequences due to decay or outflow is balanced by replenishing, so that even relatively slowly replicating sequences can remain present in finite abundance.

Due to the ongoing production of mutant sequences, selection does not act on single sequences, but on mutational "clouds" of closely related sequences, referred to as "quasispecies". In other words, the evolutionary success of a particular sequence depends not only on its own replication rate, but also on the replication rates of the mutant sequences it produces, and on the replication rates of the sequences of which it is a mutant. As a consequence, the sequence that replicates fastest may even disappear completely in selection-mutation equilibrium, in favor of more slowly replicating sequences that are part of a quasispecies with a higher average growth rate. Mutational clouds as predicted by the quasispecies model have been observed in RNA viruses and in "in vitro" RNA replication.

The mutation rate and the general fitness of the molecular sequences and their neighbors is crucial to the formation of a quasispecies. If the mutation rate is zero, there is no exchange by mutation, and each sequence is its own species. If the mutation rate is too high, exceeding what is known as the error threshold, the quasispecies will break down and be dispersed over the entire range of available sequences.

A simple mathematical model for a quasispecies is as follows: let there be formula_1 possible sequences and let there be formula_2 organisms with sequence "i". Let's say that each of these organisms asexually gives rise to formula_3 offspring. Some are duplicates of their parent, having sequence "i", but some are mutant and have some other sequence. Let the mutation rate formula_4 correspond to the probability that a "j" type parent will produce an "i" type organism. Then the expected fraction of offspring generated by "j" type organisms that would be "i" type organisms is formula_5,

where formula_6.

Then the total number of "i"-type organisms after the first round of reproduction, given as formula_7, is

Sometimes a death rate term formula_9 is included so that:

where formula_11 is equal to 1 when i=j and is zero otherwise. Note that the "n-th" generation can be found by just taking the "n-th" power of W substituting it in place of W in the above formula.

This is just a system of linear equations. The usual way to solve such a system is to first diagonalize the W matrix. Its diagonal entries will be eigenvalues corresponding to certain linear combinations of certain subsets of sequences which will be eigenvectors of the W matrix. These subsets of sequences are the quasispecies. Assuming that the matrix W is a primitive matrix (irreducible and aperiodic), then after very many generations only the eigenvector with the largest eigenvalue will prevail, and it is this quasispecies that will eventually dominate. The components of this eigenvector give the relative abundance of each sequence at equilibrium.

W being primitive means that for some integer formula_12, that the formula_13 power of W is > 0, i.e. all the entries are positive. If W is primitive then each type can, through a sequence of mutations (i.e. powers of W) mutate into all the other types after some number of generations. W is not primitive if it is periodic, where the population can perpetually cycle through different disjoint sets of compositions, or if it is reducible, where the dominant species (or quasispecies) that develops can depend on the initial population, as is the case in the simple example given below.

The quasispecies formulae may be expressed as a set of linear differential equations. If we consider the difference between the new state formula_7 and the old state formula_2 to be the state change over one moment of time, then we can state that the time derivative of formula_2 is given by this difference, formula_17 we can write:

The quasispecies equations are usually expressed in terms of concentrations formula_19 where

The above equations for the quasispecies then become for the discrete version:

or, for the continuum version:

The quasispecies concept can be illustrated by a simple system consisting of 4 sequences. Sequences [0,0], [0,1], [1,0], and [1,1] are numbered 1, 2, 3, and 4, respectively. Let's say the [0,0] sequence never mutates and always produces a single offspring. Let's say the other 3 sequences all produce, on average, formula_24 replicas of themselves, and formula_25 of each of the other two types, where formula_26. The W matrix is then:

The diagonalized matrix is:

And the eigenvectors corresponding to these eigenvalues are:

Only the eigenvalue formula_29 is more than unity. For the n-th generation, the corresponding eigenvalue will be formula_30 and so will increase without bound as time goes by. This eigenvalue corresponds to the eigenvector [0,1,1,1], which represents the quasispecies consisting of sequences 2, 3, and 4, which will be present in equal numbers after a very long time. Since all population numbers must be positive, the first two quasispecies are not legitimate. The third quasispecies consists of only the non-mutating sequence 1. It's seen that even though sequence 1 is the most fit in the sense that it reproduces more of itself than any other sequence, the quasispecies consisting of the other three sequences will eventually dominate (assuming that the initial population was not homogeneous of the sequence 1 type).



</doc>
<doc id="25310" url="https://en.wikipedia.org/wiki?curid=25310" title="Qing dynasty">
Qing dynasty

The Qing dynasty, also known as the Qing Empire, officially the Great Qing (), was the last imperial dynasty of China, established in 1636 and ruling China from 1644 to 1912. It was preceded by the Ming dynasty and succeeded by the Republic of China. The Qing multi-cultural empire lasted almost three centuries and formed the territorial base for the modern Chinese state. It was the fourth largest empire in world history.

The dynasty was founded by the Jurchen Aisin Gioro clan in Manchuria. In the late sixteenth century, Nurhaci, originally a Ming vassal, began organizing "Banners", military-social units that included Jurchen, Han Chinese, and Mongol elements. Nurhaci formed the Jurchen clans into a unified entity, which he renamed as the Manchus. By 1636, his son Hong Taiji began driving Ming forces out of Liaodong and declared a new dynasty, the Qing. In 1644, peasant rebels led by Li Zicheng conquered the Ming capital, Beijing. Rather than serve them, Ming general Wu Sangui made an alliance with the Manchus and opened the Shanhai Pass to the Banner Armies led by the regent Prince Dorgon, who defeated the rebels and seized the capital. Resistance from the Southern Ming and the Revolt of the Three Feudatories led by Wu Sangui delayed the Qing conquest of China proper by nearly four decades. The conquest was only completed in 1683 under the Kangxi Emperor (r. 1661–1722). The Ten Great Campaigns of the Qianlong Emperor from the 1750s to the 1790s extended Qing control into Inner Asia. The early Qing rulers maintained their Manchu customs, and while their title was Emperor, they used "Bogd khaan" when dealing with the Mongols and they were patrons of Tibetan Buddhism. They governed using Confucian styles and institutions of bureaucratic government and retained the imperial examinations to recruit Han Chinese to work under or in parallel with Manchus. They also adapted the ideals of the tributary system in dealing with neighboring territories.

During the Qianlong reign (1735–96) the dynasty reached its apogee, but then began its initial decline in prosperity and imperial control. The population rose to some 400 million, but taxes and government revenues were fixed at a low rate, virtually guaranteeing eventual fiscal crisis. Corruption set in, rebels tested government legitimacy, and ruling elites failed to change their mindsets in the face of changes in the world system. Following the Opium War, European powers imposed "unequal treaties", free trade, extraterritoriality and treaty ports under foreign control. The Taiping Rebellion (1850–64) and the Dungan Revolt (1862–77) in Central Asia led to the deaths of some 20 million people, most of them due to famines caused by war. In spite of these disasters, in the Tongzhi Restoration of the 1860s, Han Chinese elites rallied to the defense of the Confucian order and the Qing rulers. The initial gains in the Self-Strengthening Movement were destroyed in the First Sino-Japanese War of 1895, in which the Qing lost its influence over Korea and the possession of Taiwan. New Armies were organized, but the ambitious Hundred Days' Reform of 1898 was turned back in a coup by Empress Dowager Cixi, a conservative leader. When the Scramble for Concessions by foreign powers triggered the violently anti-foreign "Boxers", the foreign powers invaded China, Cixi declared war on them, leading to defeat and the flight of the Imperial Court to Xi'an.

After agreeing to sign the Boxer Protocol, the government initiated unprecedented fiscal and administrative reforms, including elections, a new legal code, and abolition of the examination system. Sun Yat-sen and other revolutionaries competed with reformist monarchists such as Kang Youwei and Liang Qichao to transform the Qing Empire into a modern nation. After the deaths of Cixi and the Guangxu Emperor in 1908, the hardline Manchu court alienated reformers and local elites alike by obstructing social reform. The Wuchang Uprising on October 11, 1911, led to the Xinhai Revolution. General Yuan Shikai negotiated the abdication of Puyi, the last emperor, on February 12, 1912. The Qing Empire was briefly restored on July 1, 1917, before it was once again overthrown 12 days later.

Nurhaci declared himself the "Bright Khan" of the "Later Jin" (lit. "gold") state in honor both of the 12–13th century Jurchen Jin dynasty and of his Aisin Gioro clan ("Aisin" being Manchu for the Chinese ("jīn", "gold")). His son Hong Taiji renamed the dynasty "Great Qing" in 1636. There are competing explanations on the meaning of "Qīng" (lit. "clear" or "pure"). The name may have been selected in reaction to the name of the Ming dynasty (), which consists of the Chinese characters for "sun" () and "moon" (), both associated with the fire element of the Chinese zodiacal system. The character "Qīng" () is composed of "water" () and "azure" (), both associated with the water element. This association would justify the Qing conquest as defeat of fire by water. The water imagery of the new name may also have had Buddhist overtones of perspicacity and enlightenment and connections with the Bodhisattva Manjusri. The Manchu name "daicing", which sounds like a phonetic rendering of "Dà Qīng" or "Dai Ching", may in fact have been derived from a Mongolian word ", дайчин" that means "warrior". "Daicing gurun" may therefore have meant "warrior state", a pun that was only intelligible to Manchu and Mongol people. In the later part of the dynasty, however, even the Manchus themselves had forgotten this possible meaning.

After conquering "China proper", the Manchus identified their state as "China" (中國, "Zhōngguó"; "Middle Kingdom"), and referred to it as "Dulimbai Gurun" in Manchu ("Dulimbai" means "central" or "middle," "gurun" means "nation" or "state"). The emperors equated the lands of the Qing state (including present-day Northeast China, Xinjiang, Mongolia, Tibet and other areas) as "China" in both the Chinese and Manchu languages, defining China as a multi-ethnic state, and rejecting the idea that "China" only meant Han areas. The Qing emperors proclaimed that both Han and non-Han peoples were part of "China". They used both "China" and "Qing" to refer to their state in official documents, international treaties (as the Qing was known internationally as "China" or the "Chinese Empire") and foreign affairs, and "Chinese language" (Manchu: "Dulimbai gurun i bithe") included Chinese, Manchu, and Mongol languages, and "Chinese people" (中國之人 "Zhōngguó zhī rén"; Manchu: "Dulimbai gurun i niyalma") referred to all subjects of the empire. In the Chinese-language versions of its treaties and its maps of the world, the Qing government used "Qing" and "China" interchangeably.

The dynasty was sometimes referred to as the "Manchu dynasty" in foreign language sources.

The Qing dynasty was founded not by Han Chinese, who constitute the majority of the Chinese population, but by a sedentary farming people known as the Jurchen, a Tungusic people who lived around the region now comprising the Chinese provinces of Jilin and Heilongjiang. The Manchus are sometimes mistaken for a nomadic people, which they were not. What was to become the Manchu state was founded by Nurhaci, the chieftain of a minor Jurchen tribethe Aisin Gioroin Jianzhou in the early 17th century. Originally a vassal of the Ming emperors, Nurhaci embarked on an intertribal feud in 1582 that escalated into a campaign to unify the nearby tribes. By 1616, he had sufficiently consolidated Jianzhou so as to be able to proclaim himself Khan of the Great Jin in reference to the previous Jurchen dynasty.

Two years later, Nurhaci announced the "Seven Grievances" and openly renounced the sovereignty of Ming overlordship in order to complete the unification of those Jurchen tribes still allied with the Ming emperor. After a series of successful battles, he relocated his capital from Hetu Ala to successively bigger captured Ming cities in Liaodong: first Liaoyang in 1621, then Shenyang (Mukden) in 1625.

Relocating his court from Jianzhou to Liaodong provided Nurhaci access to more resources; it also brought him in close contact with the Khorchin Mongol domains on the plains of Mongolia. Although by this time the once-united Mongol nation had long since fragmented into individual and hostile tribes, these tribes still presented a serious security threat to the Ming borders. Nurhaci's policy towards the Khorchins was to seek their friendship and cooperation against the Ming, securing his western border from a powerful potential enemy.

Furthermore, the Khorchin proved a useful ally in the war, lending the Jurchens their expertise as cavalry archers. To guarantee this new alliance, Nurhaci initiated a policy of inter-marriages between the Jurchen and Khorchin nobilities, while those who resisted were met with military action. This is a typical example of Nurhaci's initiatives that eventually became official Qing government policy. During most of the Qing period, the Mongols gave military assistance to the Manchus.

Some other important contributions by Nurhaci include ordering the creation of a written Manchu script, based on Mongolian script, after the earlier Jurchen script was forgotten (it had been derived from Khitan and Chinese). Nurhaci also created the civil and military administrative system that eventually evolved into the Eight Banners, the defining element of Manchu identity and the foundation for transforming the loosely knitted Jurchen tribes into a nation.

There were too few ethnic Manchus to conquer China proper, so they gained strength by defeating and absorbing Mongols. More importantly, they added Han Chinese to the Eight Banners. The Manchus had to create an entire "Jiu Han jun" (Old Han Army) due to the massive number of Han Chinese soldiers who were absorbed into the Eight Banners by both capture and defection. Ming artillery was responsible for many victories against the Manchus, so the Manchus established an artillery corps made out of Han Chinese soldiers in 1641, and the swelling of Han Chinese numbers in the Eight Banners led in 1642 to all Eight Han Banners being created. Armies of defected Ming Han Chinese conquered southern China for the Qing.

Han defectors played a massive role in the Qing conquest of China. Han Chinese Generals who defected to the Manchu were often given women from the Imperial Aisin Gioro family in marriage while the ordinary soldiers who defected were often given non-royal Manchu women as wives. Jurchen (Manchu) women married Han Chinese defectors in Liaodong. Manchu Aisin Gioro princesses were also married to Han Chinese official's sons.

The unbroken series of military successes by Nurhaci came to an end in January 1626 when he was defeated by Yuan Chonghuan while laying siege to Ningyuan. He died a few months later and was succeeded by his eighth son, Hong Taiji, who emerged after a short political struggle amongst other potential contenders as the new Khan.

Although Hong Taiji was an experienced leader and the commander of two Banners at the time of his succession, his reign did not start well on the military front. The Jurchens suffered yet another defeat in 1627 at the hands of Yuan Chonghuan. As before, this defeat was, in part, due to the Ming's newly acquired Portuguese cannons.

To redress the technological and numerical disparity, Hong Taiji in 1634 created his own artillery corps, the "ujen cooha" (Chinese: ) from among his existing Han troops who cast their own cannons in the European design with the help of defector Chinese metallurgists. One of the defining events of Hong Taiji's reign was the official adoption of the name "Manchu" for the united Jurchen people in November 1635. In 1635, the Manchus' Mongol allies were fully incorporated into a separate Banner hierarchy under direct Manchu command. Hong Taiji conquered the territory north of Shanhai Pass by Ming Dynasty and Ligdan Khan in Inner Mongolia. In April 1636, Mongol nobility of Inner Mongolia, Manchu nobility and the Han mandarin held the Kurultai in Shenyang, recommended khan of Later Jin to be the emperor of Great Qing empire. One of the Yuan Dynasty's jade seal has also dedicated to the emperor (Qing Taizong) by nobility. When he was said to be presented with the imperial seal of the Yuan dynasty after the defeat of the last Khagan of the Mongols, Hong Taiji renamed his state from "Great Jin" to "Great Qing" and elevated his position from Khan to Emperor, suggesting imperial ambitions beyond unifying the Manchu territories. Hong Taiji then proceeded in 1636 to invade Korea again.

After the Second Manchu invasion of Korea, Joseon Korea was forced to give several of their royal princesses as concubines to the Qing Manchu regent Prince Dorgon. In 1650, Dorgon married the Korean Princess Uisun.

This was followed by the creation of the first two Han Banners in 1637 (increasing to eight in 1642). Together these military reforms enabled Hong Taiji to resoundingly defeat Ming forces in a series of battles from 1640 to 1642 for the territories of Songshan and Jinzhou. This final victory resulted in the surrender of many of the Ming dynasty's most battle-hardened troops, the death of Yuan Chonghuan at the hands of the Chongzhen Emperor (who thought Yuan had betrayed him), and the complete and permanent withdrawal of the remaining Ming forces north of the Great Wall.

Meanwhile, Hong Taiji set up a rudimentary bureaucratic system based on the Ming model. He established six boards or executive level ministries in 1631 to oversee finance, personnel, rites, military, punishments, and public works. However, these administrative organs had very little role initially, and it was not until the eve of completing the conquest ten years later that they fulfilled their government roles.

Hong Taiji's bureaucracy was staffed with many Han Chinese, including many newly surrendered Ming officials. The Manchus' continued dominance was ensured by an ethnic quota for top bureaucratic appointments. Hong Taiji's reign also saw a fundamental change of policy towards his Han Chinese subjects. Nurhaci had treated Han in Liaodong differently according to how much grain they had: those with less than 5 to 7 sin were treated badly, while those with more than that amount were rewarded with property. Due to a revolt by Han in Liaodong in 1623, Nurhaci, who previously gave concessions to conquered Han subjects in Liaodong, turned against them and ordered that they no longer be trusted. He enacted discriminatory policies and killings against them, while ordering that Han who assimilated to the Jurchen (in Jilin) before 1619 be treated equally, as Jurchens were, and not like the conquered Han in Liaodong. Hong Taiji recognized that Han defectors were needed by the Manchus to assist in the conquest of the Ming, explaining to other Manchus why he needed to treat the Ming defector General Hong Chengchou leniently. Hong Taiji instead incorporated them into the Jurchen "nation" as full (if not first-class) citizens, obligated to provide military service. By 1648, less than one-sixth of the bannermen were of Manchu ancestry. This change of policy not only increased Hong Taiji's manpower and reduced his military dependence on banners not under his personal control, it also greatly encouraged other Han Chinese subjects of the Ming dynasty to surrender and accept Jurchen rule when they were defeated militarily. Through these and other measures Hong Taiji was able to centralize power unto the office of the Khan, which in the long run prevented the Jurchen federation from fragmenting after his death.

Hong Taiji died suddenly in September 1643 without a designated heir. As the Jurchens had traditionally "elected" their leader through a council of nobles, the Qing state did not have in place a clear succession system until the reign of the Kangxi Emperor. The leading contenders for power at this time were Hong Taiji's oldest son Hooge and Hong Taiji' half brother Dorgon. A compromise candidate in the person of Hong Taiji's five-year-old son, Fulin, was installed as the Shunzhi Emperor, with Dorgon as regent and de facto leader of the Manchu nation.

Ming government officials fought against each other, against fiscal collapse, and against a series of peasant rebellions. They were unable to capitalise on the Manchu succession dispute and installation of a minor as emperor. In April 1644, the capital at Beijing was sacked by a coalition of rebel forces led by Li Zicheng, a former minor Ming official, who established a short-lived Shun dynasty. The last Ming ruler, the Chongzhen Emperor, committed suicide when the city fell, marking the official end of the dynasty.

Li Zicheng then led a coalition of rebel forces numbering 200,000 to confront Wu Sangui, the general commanding the Ming garrison at Shanhai Pass. Shanhai Pass is a pivotal pass of the Great Wall, located fifty miles northeast of Beijing, and for years its defenses kept the Manchus from directly raiding the Ming capital. Wu Sangui, caught between a rebel army twice his size and a foreign enemy he had fought for years, decided to cast his lot with the Manchus, with whom he was familiar. Wu Sangui may have been influenced by Li Zicheng's mistreatment of his family and other wealthy and cultured officials; it was said that Li also took Wu's concubine Chen Yuanyuan for himself. Wu and Dorgon allied in the name of avenging the death of the Chongzhen Emperor. Together, the two former enemies met and defeated Li Zicheng's rebel forces in battle on May 27, 1644.

The newly allied armies captured Beijing on June 6. The Shunzhi Emperor was invested as the "Son of Heaven" on October 30. The Manchus, who had positioned themselves as political heir to the Ming emperor by defeating the rebel Li Zicheng, completed the symbolic transition by holding a formal funeral for the Chongzhen Emperor. However the process of conquering the rest of China took another seventeen years of battling Ming loyalists, pretenders and rebels. The last Ming pretender, Prince Gui, sought refuge with the King of Burma, Pindale Min, but was turned over to a Qing expeditionary army commanded by Wu Sangui, who had him brought back to Yunnan province and executed in early 1662.

Han Chinese Banners were made up of Han Chinese who defected to the Qing up to 1644 and joined the Eight Banners, giving them social and legal privileges in addition to being acculturated to Manchu culture. So many Han defected to the Qing and swelled the ranks of the Eight Banners that ethnic Manchus became a minority, making up only 16% in 1648, with Han Bannermen dominating at 75% and Mongol Bannermen making up the rest. This multi-ethnic force in which Manchus were only a minority conquered China for the Qing.

Han Chinese Bannermen were responsible for the successful Qing conquest of China, as they made up the majority of governors in the early Qing, and they governed and administered China after the conquest, stabilizing Qing rule. Han Bannermen dominated the post of governor-general in the time of the Shunzhi and Kangxi Emperors, and also the post of governor, largely excluding ordinary Han civilians from these posts.

The Qing showed that the Manchus valued military skills in propaganda targeted towards the Ming military to get them to defect to the Qing, since the Ming civilian political system discriminated against the military. The three Liaodong Han Bannermen officers who played a massive role in the conquest of southern China from the Ming were Shang Kexi, Geng Zhongming, and Kong Youde and they governed southern China autonomously as viceroys for the Qing after their conquests. Normally the Manchu Bannermen acted only as reserve forces or in the rear and were used predominantly for quick strikes with maximum impact, so as to minimize ethnic Manchu losses; instead, the Qing used defected Han Chinese troops to fight as the vanguard during the entire conquest of China.

Among the Banners, gunpowder weapons like muskets and artillery were specifically wielded by the Chinese Banners.

To promote ethnic harmony, a 1648 decree from Shunzhi allowed Han Chinese civilian men to marry Manchu women from the Banners with the permission of the Board of Revenue if they were registered daughters of officials or commoners or the permission of their banner company captain if they were unregistered commoners, it was only later in the dynasty that these policies allowing intermarriage were done away with.

The southern cadet branch of Confucius' descendants who held the title Wujing boshi and the northern branch 65th generation descendant to hold the title Duke Yansheng had both their titles confirmed by the Qing Shunzhi Emperor upon the Qing conquest of the Ming and entry into Beijing on 31 October. The Kong's title of Duke was maintained by the Qing.

The first seven years of the Shunzhi Emperor's reign were dominated by the regent prince Dorgon. Because of his own political insecurity, Dorgon followed Hong Taiji's example by ruling in the name of the emperor at the expense of rival Manchu princes, many of whom he demoted or imprisoned under one pretext or another. Although the period of his regency was relatively short, Dorgon cast a long shadow over the Qing dynasty.

First, the Manchus had entered "China proper" because Dorgon responded decisively to Wu Sangui's appeal. Then, after capturing Beijing, instead of sacking the city as the rebels had done, Dorgon insisted, over the protests of other Manchu princes, on making it the dynastic capital and reappointing most Ming officials. Choosing Beijing as the capital had not been a straightforward decision, since no major Chinese dynasty had directly taken over its immediate predecessor's capital. Keeping the Ming capital and bureaucracy intact helped quickly stabilize the regime and sped up the conquest of the rest of the country. Dorgon drastically reduced the influence of the eunuchs, a major force in the Ming bureaucracy, and directed Manchu women not to bind their feet in the Chinese style.

However, not all of Dorgon's policies were equally popular nor easily implemented. The controversial July 1645 edict (the "haircutting order") forced adult Han Chinese men to shave the front of their heads and comb the remaining hair into the queue hairstyle which was worn by Manchu men, on pain of death. The popular description of the order was: "To keep the hair, you lose the head; To keep your head, you cut the hair." To the Manchus, this policy was a test of loyalty and an aid in distinguishing friend from foe. For the Han Chinese, however, it was a humiliating reminder of Qing authority that challenged traditional Confucian values. The "Classic of Filial Piety" ("Xiaojing") held that "a person's body and hair, being gifts from one's parents, are not to be damaged." Under the Ming dynasty, adult men did not cut their hair but instead wore it in the form of a top-knot. The order triggered strong resistance to Qing rule in Jiangnan and massive killing of Han Chinese. It was Han Chinese defectors who carried out massacres against people refusing to wear the queue. Li Chengdong, a Han Chinese general who had served the Ming but surrendered to the Qing, ordered his Han troops to carry out three separate massacres in the city of Jiading within a month, resulting in tens of thousands of deaths. At the end of the third massacre, there was hardly a living person left in this city. Jiangyin also held out against about 10,000 Han Chinese Qing troops for 83 days. When the city wall was finally breached on 9 October 1645, the Han Chinese Qing army led by the Han Chinese Ming defector Liu Liangzuo (劉良佐), who had been ordered to "fill the city with corpses before you sheathe your swords," massacred the entire population, killing between 74,000 and 100,000 people. The queue was the only aspect of Manchu culture which the Qing forced on the common Han population. The Qing required people serving as officials to wear Manchu clothing, but allowed non-official Han civilians to continue wearing Hanfu (Han clothing).

On December 31, 1650, Dorgon suddenly died during a hunting expedition, marking the official start of the Shunzhi Emperor's personal rule. Because the emperor was only 12 years old at that time, most decisions were made on his behalf by his mother, Empress Dowager Xiaozhuang, who turned out to be a skilled political operator.

Although his support had been essential to Shunzhi's ascent, Dorgon had centralised so much power in his hands as to become a direct threat to the throne. So much so that upon his death he was bestowed the extraordinary posthumous title of Emperor Yi (), the only instance in Qing history in which a Manchu "prince of the blood" () was so honored. Two months into Shunzhi's personal rule, however, Dorgon was not only stripped of his titles, but his corpse was disinterred and mutilated. to atone for multiple "crimes", one of which was persecuting to death Shunzhi's agnate eldest brother, Hooge. More importantly, Dorgon's symbolic fall from grace also led to the purge of his family and associates at court, thus reverting power back to the person of the emperor. After a promising start, Shunzhi's reign was cut short by his early death in 1661 at the age of twenty-four from smallpox. He was succeeded by his third son Xuanye, who reigned as the Kangxi Emperor.

The Manchus sent Han Bannermen to fight against Koxinga's Ming loyalists in Fujian. They removed the population from coastal areas in order to deprive Koxinga's Ming loyalists of resources. This led to a misunderstanding that Manchus were "afraid of water". Han Bannermen carried out the fighting and killing, casting doubt on the claim that fear of the water led to the coastal evacuation and ban on maritime activities. Even though a poem refers to the soldiers carrying out massacres in Fujian as "barbarians", both Han Green Standard Army and Han Bannermen were involved and carried out the worst slaughter. 400,000 Green Standard Army soldiers were used against the Three Feudatories in addition to the 200,000 Bannermen.

The sixty-one year reign of the Kangxi Emperor was the longest of any Chinese emperor. Kangxi's reign is also celebrated as the beginning of an era known as the "High Qing", during which the dynasty reached the zenith of its social, economic and military power. Kangxi's long reign started when he was eight years old upon the untimely demise of his father. To prevent a repeat of Dorgon's dictatorial monopolizing of power during the regency, the Shunzhi Emperor, on his deathbed, hastily appointed four senior cabinet ministers to govern on behalf of his young son. The four ministers – Sonin, Ebilun, Suksaha, and Oboi – were chosen for their long service, but also to counteract each other's influences. Most important, the four were not closely related to the imperial family and laid no claim to the throne. However, as time passed, through chance and machination, Oboi, the most junior of the four, achieved such political dominance as to be a potential threat. Even though Oboi's loyalty was never an issue, his personal arrogance and political conservatism led him into an escalating conflict with the young emperor. In 1669 Kangxi, through trickery, disarmed and imprisoned Oboi – a significant victory for a fifteen-year-old emperor over a wily politician and experienced commander.

The early Manchu rulers established two foundations of legitimacy that help to explain the stability of their dynasty. The first was the bureaucratic institutions and the neo-Confucian culture that they adopted from earlier dynasties. Manchu rulers and Han Chinese scholar-official elites gradually came to terms with each other. The examination system offered a path for ethnic Han to become officials. Imperial patronage of Kangxi Dictionary demonstrated respect for Confucian learning, while the Sacred Edict of 1670 effectively extolled Confucian family values. His attempts to discourage Chinese women from foot binding, however, were unsuccessful.

The second major source of stability was the Central Asian aspect of their Manchu identity, which allowed them to appeal to Mongol, Tibetan and Uighur constituents. The ways of the Qing legitimization were different for the Chinese, Mongolian and Tibetan peoples. This contradicted traditional Chinese worldview requiring acculturation of "barbarians". Qing emperors, on the contrary, sought to prevent this in regard to Mongols and Tibetans. The Qing used the title of Emperor (Huangdi) in Chinese, while among Mongols the Qing monarch was referred to as Bogda khan (wise Khan), and referred to as Gong Ma in Tibet. The Qianlong Emperor propagated the image of himself as a Buddhist sage ruler, a patron of Tibetan Buddhism. In the Manchu language, the Qing monarch was alternately referred to as either Huwangdi (Emperor) or Khan with no special distinction between the two usages. The Kangxi Emperor also welcomed to his court Jesuit missionaries, who had first come to China under the Ming. Missionaries including Tomás Pereira, Martino Martini, Johann Adam Schall von Bell, Ferdinand Verbiest and Antoine Thomas held significant positions as military weapons experts, mathematicians, cartographers, astronomers and advisers to the emperor. The relationship of trust was however lost in the later Chinese Rites controversy.

Yet controlling the "Mandate of Heaven" was a daunting task. The vastness of China's territory meant that there were only enough banner troops to garrison key cities forming the backbone of a defense network that relied heavily on surrendered Ming soldiers. In addition, three surrendered Ming generals were singled out for their contributions to the establishment of the Qing dynasty, ennobled as feudal princes (藩王), and given governorships over vast territories in Southern China. The chief of these was Wu Sangui, who was given the provinces of Yunnan and Guizhou, while generals Shang Kexi and Geng Jingzhong were given Guangdong and Fujian provinces respectively.

As the years went by, the three feudal lords and their extensive territories became increasingly autonomous. Finally, in 1673, Shang Kexi petitioned Kangxi for permission to retire to his hometown in Liaodong province and nominated his son as his successor. The young emperor granted his retirement, but denied the heredity of his fief. In reaction, the two other generals decided to petition for their own retirements to test Kangxi's resolve, thinking that he would not risk offending them. The move backfired as the young emperor called their bluff by accepting their requests and ordering that all three fiefdoms to be reverted to the crown.

Faced with the stripping of their powers, Wu Sangui, later joined by Geng Zhongming and by Shang Kexi's son Shang Zhixin, felt they had no choice but to revolt. The ensuing Revolt of the Three Feudatories lasted for eight years. Wu attempted, ultimately in vain, to fire the embers of south China Ming loyalty by restoring Ming customs, ordering that the resented queues be cut, and declaring himself emperor of a new dynasty. At the peak of the rebels' fortunes, they extended their control as far north as the Yangtze River, nearly establishing a divided China. Wu then hesitated to go further north, not being able to coordinate strategy with his allies, and Kangxi was able to unify his forces for a counterattack led by a new generation of Manchu generals. By 1681, the Qing government had established control over a ravaged southern China which took several decades to recover.

Manchu Generals and Bannermen were initially put to shame by the better performance of the Han Chinese Green Standard Army. Kangxi accordingly assigned generals Sun Sike, Wang Jinbao, and Zhao Liangdong to crush the rebels, since he thought that Han Chinese were superior to Bannermen at battling other Han people. Similarly, in north-western China against Wang Fuchen, the Qing used Han Chinese Green Standard Army soldiers and Han Chinese generals as the primary military forces. This choice was due to the rocky terrain, which favoured infantry troops over cavalry, to the desire to keep Bannermen in reserve, and, again, to the belief that Han troops were better at fighting other Han people. These Han generals achieved victory over the rebels. Also due to the mountainous terrain, Sichuan and southern Shaanxi were retaken by the Green Standard Army in 1680, with Manchus participating only in logistics and provisions. 400,000 Green Standard Army soldiers and 150,000 Bannermen served on the Qing side during the war. 213 Han Chinese Banner companies, and 527 companies of Mongol and Manchu Banners were mobilized by the Qing during the revolt. 400,000 Green Standard Army soldiers were used against the Three Feudatories besides 200,000 Bannermen.

The Qing forces were crushed by Wu from 1673–1674. The Qing had the support of the majority of Han Chinese soldiers and Han elite against the Three Feudatories, since they refused to join Wu Sangui in the revolt, while the Eight Banners and Manchu officers fared poorly against Wu Sangui, so the Qing responded with using a massive army of more than 900,000 Han Chinese (non-Banner) instead of the Eight Banners, to fight and crush the Three Feudatories. Wu Sangui's forces were crushed by the Green Standard Army, made out of defected Ming soldiers.

To extend and consolidate the dynasty's control in Central Asia, the Kangxi Emperor personally led a series of military campaigns against the Dzungars in Outer Mongolia. The Kangxi Emperor was able to successfully expel Galdan's invading forces from these regions, which were then incorporated into the empire. Galdan was eventually killed in the Dzungar–Qing War. In 1683, Qing forces received the surrender of Formosa (Taiwan) from Zheng Keshuang, grandson of Koxinga, who had conquered Taiwan from the Dutch colonists as a base against the Qing. Zheng Keshuang was awarded the title "Duke Haicheng" (海澄公) and was inducted into the Han Chinese Plain Red Banner of the Eight Banners when he moved to Beijing. Several Ming princes had accompanied Koxinga to Taiwan in 1661–1662, including the Prince of Ningjing Zhu Shugui and Prince Zhu Honghuan (朱弘桓), son of Zhu Yihai, where they lived in the Kingdom of Tungning. The Qing sent the 17 Ming princes still living on Taiwan in 1683 back to mainland China where they spent the rest of their lives in exile since their lives were spared from execution. Winning Taiwan freed Kangxi's forces for series of battles over Albazin, the far eastern outpost of the Tsardom of Russia. Zheng's former soldiers on Taiwan like the rattan shield troops were also inducted into the Eight Banners and used by the Qing against Russian Cossacks at Albazin. The 1689 Treaty of Nerchinsk was China's first formal treaty with a European power and kept the border peaceful for the better part of two centuries. After Galdan's death, his followers, as adherents to Tibetan Buddhism, attempted to control the choice of the next Dalai Lama. Kangxi dispatched two armies to Lhasa, the capital of Tibet, and installed a Dalai Lama sympathetic to the Qing.

By the end of the 17th century, China was at its greatest height of confidence and political control since the Ming dynasty.

The reigns of the Yongzheng Emperor (r. 1723–1735) and his son, the Qianlong Emperor (r. 1735–1796), marked the height of Qing power. During this period, the Qing Empire ruled over 13 million square kilometers of territory. Yet, as the historian Jonathan Spence puts it, the empire by the end of the Qianlong reign was "like the sun at midday". In the midst of "many glories," he writes, "signs of decay and even collapse were becoming apparent".

After the death of the Kangxi Emperor in the winter of 1722, his fourth son, Prince Yong (雍親王), became the Yongzheng Emperor. In the later years of Kangxi's reign, Yongzheng and his brothers had fought, and there were rumours that he had usurped the throne – most of the rumours held that Yongzheng's brother Yingzhen (Kangxi's 14th son) was the real successor of the Kangxi Emperor, and that Yongzheng and his confidant Keduo Long had tampered with the Kangxi's testament on the night when Kangxi died, though there was little evidence for these charges. In fact, his father had trusted him with delicate political issues and discussed state policy with him. When Yongzheng came to power at the age of 45, he felt a sense of urgency about the problems that had accumulated in his father's later years, and he did not need instruction on how to exercise power. In the words of one recent historian, he was "severe, suspicious, and jealous, but extremely capable and resourceful", and in the words of another, he turned out to be an "early modern state-maker of the first order".

Yongzheng moved rapidly. First, he promoted Confucian orthodoxy and reversed what he saw as his father's laxness by cracking down on unorthodox sects and by decapitating an anti-Manchu writer his father had pardoned. In 1723 he outlawed Christianity and expelled Christian missionaries, though some were allowed to remain in the capital. Next, he moved to control the government. He expanded his father's system of Palace Memorials, which brought frank and detailed reports on local conditions directly to the throne without being intercepted by the bureaucracy, and he created a small Grand Council of personal advisors, which eventually grew into the emperor's "de facto" cabinet for the rest of the dynasty. He shrewdly filled key positions with Manchu and Han Chinese officials who depended on his patronage. When he began to realize that the financial crisis was even greater than he had thought, Yongzheng rejected his father's lenient approach to local landowning elites and mounted a campaign to enforce collection of the land tax. The increased revenues were to be used for "money to nourish honesty" among local officials and for local irrigation, schools, roads, and charity. Although these reforms were effective in the north, in the south and lower Yangzi valley, where Kangxi had wooed the elites, there were long established networks of officials and landowners. Yongzheng dispatched experienced Manchu commissioners to penetrate the thickets of falsified land registers and coded account books, but they were met with tricks, passivity, and even violence. The fiscal crisis persisted.

In 1725 Yongzheng bestowed the hereditary title of Marquis on a descendant of the Ming dynasty Imperial family, Zhu Zhiliang, who received a salary from the Qing government and whose duty was to perform rituals at the Ming tombs, and was also inducted into the Chinese Plain White Banner in the Eight Banners. Later the Qianlong Emperor bestowed the title Marquis of Extended Grace posthumously on Zhu Zhuliang in 1750, and the title passed on through twelve generations of Ming descendants until the end of the Qing dynasty.

Yongzheng also inherited diplomatic and strategic problems. A team made up entirely of Manchus drew up the Treaty of Kyakhta (1727) to solidify the diplomatic understanding with Russia. In exchange for territory and trading rights, the Qing would have a free hand dealing with the situation in Mongolia. Yongzheng then turned to that situation, where the Zunghars threatened to re-emerge, and to the southwest, where local Miao chieftains resisted Qing expansion. These campaigns drained the treasury but established the emperor's control of the military and military finance.

The Yongzheng Emperor died in 1735. His 24-year-old son, Prince Bao (寶親王), then became the Qianlong Emperor. Qianlong personally led military campaigns near Xinjiang and Mongolia, putting down revolts and uprisings in Sichuan and parts of southern China while expanding control over Tibet.

The Qianlong Emperor launched several ambitious cultural projects, including the compilation of the "Siku Quanshu", or "Complete Repository of the Four Branches of Literature". With a total of over 3,400 books, 79,000 chapters, and 36,304 volumes, the "Siku Quanshu" is the largest collection of books in Chinese history. Nevertheless, Qianlong used Literary Inquisition to silence opposition. The accusation of individuals began with the emperor's own interpretation of the true meaning of the corresponding words. If the emperor decided these were derogatory or cynical towards the dynasty, persecution would begin. Literary inquisition began with isolated cases at the time of Shunzhi and Kangxi, but became a pattern under Qianlong's rule, during which there were 53 cases of literary persecution.

Beneath outward prosperity and imperial confidence, the later years of Qianlong's reign were marked by rampant corruption and neglect. Heshen, the emperor's handsome young favorite, took advantage of the emperor's indulgence to become one of the most corrupt officials in the history of the dynasty. Qianlong's son, the Jiaqing Emperor (r. 1796–1820), eventually forced Heshen to commit suicide.

China also began suffering from mounting overpopulation during this period. Population growth was stagnant for the first half of the 17th century due to civil wars and epidemics, but prosperity and internal stability gradually reversed this trend. The introduction of new crops from the Americas such as the potato and peanut allowed an improved food supply as well, so that the total population of China during the 18th century ballooned from 100 million to 300 million people. Soon all available farmland was used up, forcing peasants to work ever-smaller and more intensely worked plots. The Qianlong Emperor once bemoaned the country's situation by remarking, "The population continues to grow, but the land does not." The only remaining part of the empire that had arable farmland was Manchuria, where the provinces of Jilin and Heilongjiang had been walled off as a Manchu homeland. The emperor decreed for the first time that Han Chinese civilians were forbidden to settle. Mongols were forbidden by the Qing from crossing the borders of their banners, even into other Mongol Banners, and from crossing into neidi (the Han Chinese 18 provinces) and were given serious punishments if they did in order to keep the Mongols divided against each other to benefit the Qing.

Despite officially prohibiting Han Chinese settlement on the Manchu and Mongol lands, by the 18th century the Qing decided to settle Han refugees from northern China who were suffering from famine, floods, and drought into Manchuria and Inner Mongolia. Han Chinese then streamed into Manchuria, both illegally and legally, over the Great Wall and Willow Palisade. As Manchu landlords desired Han Chinese to rent their land and grow grain, most Han Chinese migrants were not evicted. During the eighteenth century Han Chinese farmed 500,000 hectares of privately owned land in Manchuria and 203,583 hectares of lands that were part of courrier stations, noble estates, and Banner lands. In garrisons and towns in Manchuria Han Chinese made up 80% of the population.

In 1796, open rebellion broke out by the White Lotus Society against the Qing government. The White Lotus Rebellion continued for eight years, until 1804, and marked a turning point in the history of the Qing dynasty.

At the start of the dynasty, the Chinese empire continued to be the hegemonic power in East Asia. Although there was no formal ministry of foreign relations, the Lifan Yuan was responsible for relations with the Mongol and Tibetans in Central Asia, while the tributary system, a loose set of institutions and customs taken over from the Ming, in theory governed relations with East and Southeast Asian countries. The Treaty of Nerchinsk (1689) stabilized relations with Czarist Russia.

In the Jahriyya revolt sectarian violence between two suborders of the Naqshbandi Sufis, the Jahriyya Sufi Muslims and their rivals, the Khafiyya Sufi Muslims, led to a Jahriyya Sufi Muslim rebellion which the Qing dynasty in China crushed with the help of the Khafiyya Sufi Muslims. The Eight Trigrams uprising of 1813 broke out in 1813.

However, during the 18th century European empires gradually expanded across the world, as European states developed economies built on maritime trade. The dynasty was confronted with newly developing concepts of the international system and state to state relations. European trading posts expanded into territorial control in nearby India and on the islands that are now Indonesia. The Qing response, successful for a time, was to establish the Canton System in 1756, which restricted maritime trade to that city (modern-day Guangzhou) and gave monopoly trading rights to private Chinese merchants. The British East India Company and the Dutch East India Company had long before been granted similar monopoly rights by their governments.

In 1793, the British East India Company, with the support of the British government, sent a delegation to China under Lord George Macartney in order to open free trade and put relations on a basis of equality. The imperial court viewed trade as of secondary interest, whereas the British saw maritime trade as the key to their economy. The Qianlong Emperor told Macartney "the kings of the myriad nations come by land and sea with all sorts of precious things," and "consequently there is nothing we lack..."

Demand in Europe for Chinese goods such as silk, tea, and ceramics could only be met if European companies funneled their limited supplies of silver into China. In the late 1700s, the governments of Britain and France were deeply concerned about the imbalance of trade and the drain of silver. To meet the growing Chinese demand for opium, the British East India Company greatly expanded its production in Bengal. Since China's economy was essentially self-sufficient, the country had little need to import goods or raw materials from the Europeans, so the usual way of payment was through silver. The Daoguang Emperor, concerned both over the outflow of silver and the damage that opium smoking was causing to his subjects, ordered Lin Zexu to end the opium trade. Lin confiscated the stocks of opium without compensation in 1839, leading Britain to send a military expedition the following year.

The First Opium War revealed the outdated state of the Chinese military. The Qing navy, composed entirely of wooden sailing junks, was severely outclassed by the modern tactics and firepower of the British Royal Navy. British soldiers, using advanced muskets and artillery, easily outmanoeuvred and outgunned Qing forces in ground battles. The Qing surrender in 1842 marked a decisive, humiliating blow to China. The Treaty of Nanjing, the first of the "unequal treaties", demanded war reparations, forced China to open up the Treaty Ports of Canton, Amoy, Fuchow, Ningpo and Shanghai to Western trade and missionaries, and to cede Hong Kong Island to Britain. It revealed weaknesses in the Qing government and provoked rebellions against the regime. In 1842, the Qing dynasty fought a war to the Sikh Empire (the last independent kingdom of India), resulting in a negotiated peace and a return to the status quo ante bellum.

The Taiping Rebellion in the mid-19th century was the first major instance of anti-Manchu sentiment. Amid widespread social unrest and worsening famine, the rebellion not only posed the most serious threat towards Qing rulers, it has also been called the "bloodiest civil war of all time"; during its fourteen-year course from 1850 to 1864 between 20 and 30 million people died. Hong Xiuquan, a failed civil service candidate, in 1851 launched an uprising in Guizhou province, and established the Taiping Heavenly Kingdom with Hong himself as king. Hong announced that he had visions of God and that he was the brother of Jesus Christ. Slavery, concubinage, arranged marriage, opium smoking, footbinding, judicial torture, and the worship of idols were all banned. However, success led to internal feuds, defections and corruption. In addition, British and French troops, equipped with modern weapons, had come to the assistance of the Qing imperial army. It was not until 1864 that Qing armies under Zeng Guofan succeeded in crushing the revolt. After the outbreak of this rebellion, there were also revolts by the Muslims and Miao people of China against the Qing dynasty, most notably in the Miao Rebellion (1854–73) in Guizhou, the Panthay Rebellion (1856–1873) in Yunnan and the Dungan Revolt (1862–77) in the northwest.

The Western powers, largely unsatisfied with the Treaty of Nanjing, gave grudging support to the Qing government during the Taiping and Nian Rebellions. China's income fell sharply during the wars as vast areas of farmland were destroyed, millions of lives were lost, and countless armies were raised and equipped to fight the rebels. In 1854, Britain tried to re-negotiate the Treaty of Nanjing, inserting clauses allowing British commercial access to Chinese rivers and the creation of a permanent British embassy at Beijing.

In 1856, Qing authorities, in searching for a pirate, boarded a ship, the "Arrow", which the British claimed had been flying the British flag, an incident which led to the Second Opium War. In 1858, facing no other options, the Xianfeng Emperor agreed to the Treaty of Tientsin, which contained clauses deeply insulting to the Chinese, such as a demand that all official Chinese documents be written in English and a proviso granting British warships unlimited access to all navigable Chinese rivers.

Ratification of the treaty in the following year led to a resumption of hostilities. In 1860, with Anglo-French forces marching on Beijing, the emperor and his court fled the capital for the imperial hunting lodge at Rehe. Once in Beijing, the Anglo-French forces looted the Old Summer Palace and, in an act of revenge for the arrest of several Englishmen, burnt it to the ground. Prince Gong, a younger half-brother of the emperor, who had been left as his brother's proxy in the capital, was forced to sign the Convention of Beijing. The humiliated emperor died the following year at Rehe.

Yet the dynasty rallied. Chinese generals and officials such as Zuo Zongtang led the suppression of rebellions and stood behind the Manchus. When the Tongzhi Emperor came to the throne at the age of five in 1861, these officials rallied around him in what was called the Tongzhi Restoration. Their aim was to adopt Western military technology in order to preserve Confucian values. Zeng Guofan, in alliance with Prince Gong, sponsored the rise of younger officials such as Li Hongzhang, who put the dynasty back on its feet financially and instituted the Self-Strengthening Movement. The reformers then proceeded with institutional reforms, including China's first unified ministry of foreign affairs, the Zongli Yamen; allowing foreign diplomats to reside in the capital; establishment of the Imperial Maritime Customs Service; the formation of modernized armies, such as the Beiyang Army, as well as a navy; and the purchase from Europeans of armament factories.

The dynasty lost control of peripheral territories bit by bit. In return for promises of support against the British and the French, the Russian Empire took large chunks of territory in the Northeast in 1860. The period of cooperation between the reformers and the European powers ended with the Tientsin Massacre of 1870, which was incited by the murder of French nuns set off by the belligerence of local French diplomats. Starting with the Cochinchina Campaign in 1858, France expanded control of Indochina. By 1883, France was in full control of the region and had reached the Chinese border. The Sino-French War began with a surprise attack by the French on the Chinese southern fleet at Fuzhou. After that the Chinese declared war on the French. A French invasion of Taiwan was halted and the French were defeated on land in Tonkin at the Battle of Bang Bo. However Japan threatened to enter the war against China due to the Gapsin Coup and China chose to end the war with negotiations. The war ended in 1885 with the Treaty of Tientsin (1885) and the Chinese recognition of the French protectorate in Vietnam.

In 1884, pro-Japanese Koreans in Seoul led the Gapsin Coup. Tensions between China and Japan rose after China intervened to suppress the uprising. Japanese Prime Minister Itō Hirobumi and Li Hongzhang signed the Convention of Tientsin, an agreement to withdraw troops simultaneously, but the First Sino-Japanese War of 1895 was a military humiliation. The Treaty of Shimonoseki recognized Korean independence and ceded Taiwan and the Pescadores to Japan. The terms might have been harsher, but when Japanese citizen attacked and wounded Li Hongzhang, an international outcry shamed the Japanese into revising them. The original agreement stipulated the cession of Liaodong Peninsula to Japan, but Russia, with its own designs on the territory, along with Germany and France, in what was known as the Triple Intervention, successfully put pressure on the Japanese to abandon the peninsula.

These years saw an evolution in the participation of Empress Dowager Cixi (Wade–Giles: Tz'u-Hsi) in state affairs. She entered the imperial palace in the 1850s as a concubine to the Xianfeng Emperor (r. 1850–1861) and came to power in 1861 after her five-year-old son, the Tongzhi Emperor ascended the throne. She, the Empress Dowager Ci'an (who had been Xianfeng's empress), and Prince Gong (a son of the Daoguang Emperor), staged a coup that ousted several regents for the boy emperor. Between 1861 and 1873, she and Ci'an served as regents, choosing the reign title "Tongzhi" (ruling together). Following the emperor's death in 1875, Cixi's nephew, the Guangxu Emperor, took the throne, in violation of the dynastic custom that the new emperor be of the next generation, and another regency began. In the spring of 1881, Ci'an suddenly died, aged only forty-three, leaving Cixi as sole regent.

From 1889, when Guangxu began to rule in his own right, to 1898, the Empress Dowager lived in semi-retirement, spending the majority of the year at the Summer Palace. On November 1, 1897, two German Roman Catholic missionaries were murdered in the southern part of Shandong Province (the Juye Incident). Germany used the murders as a pretext for a naval occupation of Jiaozhou Bay. The occupation prompted a "scramble for concessions" in 1898, which included the German lease of Jiazhou Bay, the Russian acquisition of Liaodong, and the British lease of the New Territories of Hong Kong.

In the wake of these external defeats, the Guangxu Emperor initiated the Hundred Days' Reform of 1898. Newer, more radical advisers such as Kang Youwei were given positions of influence. The emperor issued a series of edicts and plans were made to reorganize the bureaucracy, restructure the school system, and appoint new officials. Opposition from the bureaucracy was immediate and intense. Although she had been involved in the initial reforms, the Empress Dowager stepped in to call them off, arrested and executed several reformers, and took over day-to-day control of policy. Yet many of the plans stayed in place, and the goals of reform were implanted.

Widespread drought in North China, combined with the imperialist designs of European powers and the instability of the Qing government, created conditions that led to the emergence of the Righteous and Harmonious Fists, or "Boxers." In 1900, local groups of Boxers proclaiming support for the Qing dynasty murdered foreign missionaries and large numbers of Chinese Christians, then converged on Beijing to besiege the Foreign Legation Quarter. A coalition of European, Japanese, and Russian armies (the Eight-Nation Alliance) then entered China without diplomatic notice, much less permission. Cixi declared war on all of these nations, only to lose control of Beijing after a short, but hard-fought campaign. She fled to Xi'an. The victorious allies drew up scores of demands on the Qing government, including compensation for their expenses in invading China and execution of complicit officials.

By the early 20th century, mass civil disorder had begun in China, and it was growing continuously. To overcome such problems, Empress Dowager Cixi issued an imperial edict in 1901 calling for reform proposals from the governors-general and governors and initiated the era of the dynasty's "New Policies", also known as the "Late Qing Reform". The edict paved the way for the most far-reaching reforms in terms of their social consequences, including the creation of a national education system and the abolition of the imperial examinations in 1905.

The Guangxu Emperor died on November 14, 1908, and on November 15, 1908, Cixi also died. Rumors held that she or Yuan Shikai ordered trusted eunuchs to poison the Guangxu Emperor, and an autopsy conducted nearly a century later confirmed lethal levels of arsenic in his corpse. Puyi, the oldest son of Zaifeng, Prince Chun, and nephew to the childless Guangxu Emperor, was appointed successor at the age of two, leaving Zaifeng with the regency. This was followed by the dismissal of General Yuan Shikai from his former positions of power. In April 1911 Zaifeng created a cabinet in which there were two vice-premiers. Nonetheless, this cabinet was also known by contemporaries as "The Royal Cabinet" because among the thirteen cabinet members, five were members of the imperial family or Aisin Gioro relatives. This brought a wide range of negative opinions from senior officials like Zhang Zhidong.
The Wuchang Uprising of October 10, 1911, led to the creation of a new central government, the Republic of China, in Nanjing with Sun Yat-sen as its provisional head. Many provinces soon began "separating" from Qing control. Seeing a desperate situation unfold, the Qing government brought Yuan Shikai back to military power. He took control of his Beiyang Army to crush the revolution in Wuhan at the Battle of Yangxia. After taking the position of Prime Minister and creating his own cabinet, Yuan Shikai went as far as to ask for the removal of Zaifeng from the regency. This removal later proceeded with directions from Empress Dowager Longyu.

With Zaifeng gone, Yuan Shikai and his Beiyang commanders effectively dominated Qing politics. He reasoned that going to war would be unreasonable and costly, especially when noting that the Qing government had a goal for constitutional monarchy. Similarly, Sun Yat-sen's government wanted a republican constitutional reform, both aiming for the benefit of China's economy and populace. With permission from Empress Dowager Longyu, Yuan Shikai began negotiating with Sun Yat-sen, who decided that his goal had been achieved in forming a republic, and that therefore he could allow Yuan to step into the position of President of the Republic of China.

On 12 February 1912, after rounds of negotiations, Longyu issued an imperial edict bringing about the abdication of the child emperor Puyi. This brought an end to over 2,000 years of Imperial China and began an extended period of instability of warlord factionalism. The unorganized political and economic systems combined with a widespread criticism of Chinese culture led to questioning and doubt about the future. In July 1917, there was an abortive attempt to restore the Qing dynasty led by Zhang Xun, which was quickly reversed by republican troops. In the 1930s, the Empire of Japan invaded Northeast China and founded Manchukuo in 1932, with Puyi as its emperor. After the invasion by the Soviet Union, Manchukuo fell in 1945.

The early Qing emperors adopted the bureaucratic structures and institutions from the preceding Ming dynasty but split rule between Han Chinese and Manchus, with some positions also given to Mongols. Like previous dynasties, the Qing recruited officials via the imperial examination system, until the system was abolished in 1905. The Qing divided the positions into civil and military positions, each having nine grades or ranks, each subdivided into a and b categories. Civil appointments ranged from an attendant to the emperor or a Grand Secretary in the Forbidden City (highest) to being a prefectural tax collector, deputy jail warden, deputy police commissioner, or tax examiner. Military appointments ranged from being a field marshal or chamberlain of the imperial bodyguard to a third class sergeant, corporal or a first or second class private.

The formal structure of the Qing government centered on the Emperor as the absolute ruler, who presided over six Boards (Ministries), each headed by two presidents and assisted by four vice presidents. In contrast to the Ming system, however, Qing ethnic policy dictated that appointments were split between Manchu noblemen and Han officials who had passed the highest levels of the state examinations. The Grand Secretariat, which had been an important policy-making body under the Ming, lost its importance during the Qing and evolved into an imperial chancery. The institutions which had been inherited from the Ming formed the core of the Qing "Outer Court," which handled routine matters and was located in the southern part of the Forbidden City.

In order not to let the routine administration take over the running of the empire, the Qing emperors made sure that all important matters were decided in the "Inner Court," which was dominated by the imperial family and Manchu nobility and which was located in the northern part of the Forbidden City. The core institution of the inner court was the Grand Council. It emerged in the 1720s under the reign of the Yongzheng Emperor as a body charged with handling Qing military campaigns against the Mongols, but it soon took over other military and administrative duties and served to centralize authority under the crown. The Grand Councillors served as a sort of privy council to the emperor.

The Six Ministries and their respective areas of responsibilities were as follows:

Board of Civil Appointments

Board of Revenue

Board of Rites

Board of War

Board of Punishments

Board of Works

From the early Qing, the central government was characterized by a system of dual appointments by which each position in the central government had a Manchu and a Han Chinese assigned to it. The Han Chinese appointee was required to do the substantive work and the Manchu to ensure Han loyalty to Qing rule. The distinction between Han Chinese and Manchus extended to their court costumes. During the Qianlong Emperor's reign, for example, members of his family were distinguished by garments with a small circular emblem on the back, whereas Han officials wore clothing with a square emblem.

In addition to the six boards, there was a Lifan Yuan unique to the Qing government. This institution was established to supervise the administration of Tibet and the Mongol lands. As the empire expanded, it took over administrative responsibility of all minority ethnic groups living in and around the empire, including early contacts with Russia – then seen as a tribute nation. The office had the status of a full ministry and was headed by officials of equal rank. However, appointees were at first restricted only to candidates of Manchu and Mongol ethnicity, until later open to Han Chinese as well.

Even though the Board of Rites and Lifan Yuan performed some duties of a foreign office, they fell short of developing into a professional foreign service. It was not until 1861 – a year after losing the Second Opium War to the Anglo-French coalition – that the Qing government bowed to foreign pressure and created a proper foreign affairs office known as the Zongli Yamen. The office was originally intended to be temporary and was staffed by officials seconded from the Grand Council. However, as dealings with foreigners became increasingly complicated and frequent, the office grew in size and importance, aided by revenue from customs duties which came under its direct jurisdiction.

There was also another government institution called Imperial Household Department which was unique to the Qing dynasty. It was established before the fall of the Ming, but it became mature only after 1661, following the death of the Shunzhi Emperor and the accession of his son, the Kangxi Emperor. The department's original purpose was to manage the internal affairs of the imperial family and the activities of the inner palace (in which tasks it largely replaced eunuchs), but it also played an important role in Qing relations with Tibet and Mongolia, engaged in trading activities (jade, ginseng, salt, furs, etc.), managed textile factories in the Jiangnan region, and even published books. Relations with the Salt Superintendents and salt merchants, such as those at Yangzhou, were particularly lucrative, especially since they were direct, and did not go through absorptive layers of bureaucracy. The department was manned by "booi", or "bondservants," from the Upper Three Banners. By the 19th century, it managed the activities of at least 56 subagencies.

Qing China reached its largest extent during the 18th century, when it ruled China proper (eighteen provinces) as well as the areas of present-day Northeast China, Inner Mongolia, Outer Mongolia, Xinjiang and Tibet, at approximately 13 million km in size. There were originally 18 provinces, all of which in China proper, but later this number was increased to 22, with Manchuria and Xinjiang being divided or turned into provinces. Taiwan, originally part of Fujian province, became a province of its own in the 19th century, but was ceded to the Empire of Japan following the First Sino-Japanese War by the end of the century. In addition, many surrounding countries, such as Korea (Joseon dynasty), Vietnam frequently paid tribute to China during much of this period. The Katoor dynasty of Afghanistan also paid tribute to the Qing dynasty of China until the mid-19th century. During the Qing dynasty the Chinese claimed suzerainty over the Taghdumbash Pamir in the south west of Tashkurgan Tajik Autonomous County but permitted the Mir of Hunza to administer the region in return for a tribute. Until 1937 the inhabitants paid tribute to the Mir of Hunza, who exercised control over the pastures. Khanate of Kokand were forced to submit as protectorate and pay tribute to the Qing dynasty in China between 1774 and 1798.





The Qing organization of provinces was based on the fifteen administrative units set up by the Ming dynasty, later made into eighteen provinces by splitting for example, Huguang into Hubei and Hunan provinces. The provincial bureaucracy continued the Yuan and Ming practice of three parallel lines, civil, military, and censorate, or surveillance. Each province was administered by a governor (, "xunfu") and a provincial military commander (, "tidu"). Below the province were prefectures (, "fu") operating under a prefect (, "zhīfǔ"), followed by subprefectures under a subprefect. The lowest unit was the county, overseen by a county magistrate. The eighteen provinces are also known as "China proper". The position of viceroy or governor-general (, "zongdu") was the highest rank in the provincial administration. There were eight regional viceroys in China proper, each usually took charge of two or three provinces. The Viceroy of Zhili, who was responsible for the area surrounding the capital Beijing, is usually considered as the most honorable and powerful viceroy among the eight.


By the mid-18th century, the Qing had successfully put outer regions such as Inner and Outer Mongolia, Tibet and Xinjiang under its control. Imperial commissioners and garrisons were sent to Mongolia and Tibet to oversee their affairs. These territories were also under supervision of a central government institution called Lifan Yuan. Qinghai was also put under direct control of the Qing court. Xinjiang, also known as Chinese Turkestan, was subdivided into the regions north and south of the Tian Shan mountains, also known today as Dzungaria and Tarim Basin respectively, but the post of Ili General was established in 1762 to exercise unified military and administrative jurisdiction over both regions. Dzungaria was fully opened to Han migration by the Qianlong Emperor from the beginning. Han migrants were at first forbidden from permanently settling in the Tarim Basin but were the ban was lifted after the invasion by Jahangir Khoja in the 1820s. Likewise, Manchuria was also governed by military generals until its division into provinces, though some areas of Xinjiang and Northeast China were lost to the Russian Empire in the mid-19th century. Manchuria was originally separated from China proper by the Inner Willow Palisade, a ditch and embankment planted with willows intended to restrict the movement of the Han Chinese, as the area was off-limits to civilian Han Chinese until the government started colonizing the area, especially since the 1860s.

With respect to these outer regions, the Qing maintained imperial control, with the emperor acting as Mongol khan, patron of Tibetan Buddhism and protector of Muslims. However, Qing policy changed with the establishment of Xinjiang province in 1884. During The Great Game era, taking advantage of the Dungan revolt in northwest China, Yaqub Beg invaded Xinjiang from Central Asia with support from the British Empire, and made himself the ruler of the kingdom of Kashgaria. The Qing court sent forces to defeat Yaqub Beg and Xinjiang was reconquered, and then the political system of China proper was formally applied onto Xinjiang. The Kumul Khanate, which was incorporated into the Qing empire as a vassal after helping Qing defeat the Zunghars in 1757, maintained its status after Xinjiang turned into a province through the end of the dynasty in the Xinhai Revolution up until 1930. In the early 20th century, Britain sent an expedition force to Tibet and forced Tibetans to sign a treaty. The Qing court responded by asserting Chinese sovereignty over Tibet, resulting in the 1906 Anglo-Chinese Convention signed between Britain and China. The British agreed not to annex Tibetan territory or to interfere in the administration of Tibet, while China engaged not to permit any other foreign state to interfere with the territory or internal administration of Tibet. Furthermore, similar to Xinjiang which was converted into a province earlier, the Qing government also turned Manchuria into three provinces in the early 20th century, officially known as the "Three Northeast Provinces", and established the post of Viceroy of the Three Northeast Provinces to oversee these provinces, making the total number of regional viceroys to nine.

The early Qing military was rooted in the Eight Banners first developed by Nurhaci to organize Jurchen society beyond petty clan affiliations. There were eight banners in all, differentiated by color. The yellow, bordered yellow, and white banners were known as the "Upper Three Banners" and were under the direct command of the emperor. Only Manchus belonging to the Upper Three Banners, and selected Han Chinese who had passed the highest level of martial exams could serve as the emperor's personal bodyguards. The remaining Banners were known as the "Lower Five Banners". They were commanded by hereditary Manchu princes descended from Nurhachi's immediate family, known informally as the "Iron cap princes". Together they formed the ruling council of the Manchu nation as well as high command of the army. Nurhachi's son Hong Taiji expanded the system to include mirrored Mongol and Han Banners. After capturing Beijing in 1644, the relatively small Banner armies were further augmented by the Green Standard Army, made up of those Ming troops who had surrendered to the Qing, which eventually outnumbered Banner troops three to one. They maintained their Ming era organization and were led by a mix of Banner and Green Standard officers.

Banner Armies were organized along ethnic lines, namely Manchu and Mongol, but included non-Manchu bondservants registered under the household of their Manchu masters. The years leading up to the conquest increased the number of Han Chinese under Manchu rule, leading Hong Taiji to create the , and around the time of the Qing takeover of Beijing, their numbers rapidly swelled. Han Bannermen held high status and power in the early Qing period, especially immediately after the conquest during Shunzhi and Kangxi's reign where they dominated Governor-Generalships and Governorships across China at the expense of both Manchu Bannermen and Han civilians. Han also numerically dominated the Banners up until the mid 18th century. European visitors in Beijing called them "Tartarized Chinese" or "Tartarified Chinese".

The Qianlong Emperor, concerned about maintaining Manchu identity, re-emphasized Manchu ethnicity, ancestry, language, and culture in the Eight Banners and started a mass discharge of Han Bannermen from the Eight Banners, either asking them to voluntarily resign from the Banner rolls or striking their names off. This led to a change from Han majority to a Manchu majority within the Banner system,
and previous Han Bannermen garrisons in southern China such as at Fuzhou, Zhenjiang, Guangzhou, were replaced by Manchu Bannermen in the purge, which started in 1754. The turnover by Qianlong most heavily impacted Han banner garrisons stationed in the provinces while it less impacted Han Bannermen in Beijing, leaving a larger proportion of remaining Han Bannermen in Beijing than the provinces. Han Bannermen's status was decreased from that point on with Manchu Banners gaining higher status. Han Bannermen numbered 75% in 1648 Shunzhi's reign, 72% in 1723 Yongzheng's reign, but decreased to 43% in 1796 during the first year of Jiaqing's reign, which was after Qianlong's purge. The mass discharge was known as the . Qianlong directed most of his ire at those Han Bannermen descended from defectors who joined the Qing after the Qing passed through the Great Wall at Shanhai Pass in 1644, deeming their ancestors as traitors to the Ming and therefore untrustworthy, while retaining Han Bannermen who were descended from defectors who joined the Qing before 1644 in Liaodong and marched through Shanhai pass, also known as those who "followed the Dragon through the pass" ().

After a century of peace the Manchu Banner troops lost their fighting edge. Before the conquest, the Manchu banner had been a "citizen" army whose members were farmers and herders obligated to provide military service in times of war. The decision to turn the banner troops into a professional force whose every need was met by the state brought wealth, corruption, and decline as a fighting force. The Green Standard Army declined in a similar way.

Early during the Taiping Rebellion, Qing forces suffered a series of disastrous defeats culminating in the loss of the regional capital city of Nanjing in 1853. Shortly thereafter, a Taiping expeditionary force penetrated as far north as the suburbs of Tianjin, the imperial heartlands. In desperation the Qing court ordered a Chinese official, Zeng Guofan, to organize regional and village militias into an emergency army called tuanlian. Zeng Guofan's strategy was to rely on local gentry to raise a new type of military organization from those provinces that the Taiping rebels directly threatened. This new force became known as the Xiang Army, named after the Hunan region where it was raised. The Xiang Army was a hybrid of local militia and a standing army. It was given professional training, but was paid for out of regional coffers and funds its commanders – mostly members of the Chinese gentry – could muster. The Xiang Army and its successor, the Huai Army, created by Zeng Guofan's colleague and protégée Li Hongzhang, were collectively called the "Yong Ying" (Brave Camp).

Zeng Guofan had no prior military experience. Being a classically educated official, he took his blueprint for the Xiang Army from the Ming general Qi Jiguang, who, because of the weakness of regular Ming troops, had decided to form his own "private" army to repel raiding Japanese pirates in the mid-16th century. Qi Jiguang's doctrine was based on Neo-Confucian ideas of binding troops' loyalty to their immediate superiors and also to the regions in which they were raised. Zeng Guofan's original intention for the Xiang Army was simply to eradicate the Taiping rebels. However, the success of the Yongying system led to its becoming a permanent regional force within the Qing military, which in the long run created problems for the beleaguered central government.

First, the Yongying system signaled the end of Manchu dominance in Qing military establishment. Although the Banners and Green Standard armies lingered on as a drain on resources, henceforth the Yongying corps became the Qing government's de facto first-line troops. Second, the Yongying corps were financed through provincial coffers and were led by regional commanders, weakening central government's grip on the whole country. Finally, the nature of Yongying command structure fostered nepotism and cronyism amongst its commanders, who laid the seeds of regional warlordism in the first half of the 20th century.

By the late 19th century, the most conservative elements within the Qing court could no longer ignore China's military weakness. In 1860, during the Second Opium War, the capital Beijing was captured and the Summer Palace sacked by a relatively small Anglo-French coalition force numbering 25,000. The advent of modern weaponry resulting from the European Industrial Revolution had rendered China's traditionally trained and equipped army and navy obsolete. The government attempts to modernize during the Self-Strengthening Movement were initially successful, but yielded few lasting results because of the central government's lack of funds, lack of political will, and unwillingness to depart from tradition.

Losing the First Sino-Japanese War of 1894–1895 was a watershed. Japan, a country long regarded by the Chinese as little more than an upstart nation of pirates, annihilated the Qing government's modernized Beiyang Fleet, then deemed to be the strongest naval force in Asia. The Japanese victory occurred a mere three decades after the Meiji Restoration set a feudal Japan on course to emulate the Western nations in their economic and technological achievements. Finally, in December 1894, the Qing government took concrete steps to reform military institutions and to re-train selected units in Westernized drills, tactics and weaponry. These units were collectively called the New Army. The most successful of these was the Beiyang Army under the overall supervision and control of a former Huai Army commander, General Yuan Shikai, who used his position to build networks of loyal officers and eventually become President of the Republic of China.

The most significant fact of early and mid-Qing social history was population growth. The population doubled during the 18th century, and by 2000, the population had already quickly exceeded 1.25 billion. One of the main reasons for this growth was the increase in New World crops like peanuts, sweet potatoes, and potatoes, which helped to sustain the people during shortages of harvest for crops such as rice or wheat. This was because these crops were often easier to grow and thus cheaper as well, which led to them becoming staples for poorer farmers. Another reason, aided by the farming of the new crops, was the decrease in the number of deaths previously caused from the dangers of malnutrition. Diseases such as smallpox, which had been quite widespread in the seventeenth century, that had formerly terrorized the population were brought under control due to a new increase in vaccines. In addition, infant deaths were also greatly decreased due to improvements in birthing techniques and childcare performed by doctors and midwives and through an increase in medical books available to the public. There was also a great decrease in the practice of infanticide, which had previously been more prevalent towards but not limited to girls. Such a harsh process had previously been the result of fights for opportunities and livelihood and its drop was likely the most significant reason for China's rapid growth in population.

Also, according to one study, the homicide rate in Qing Chin "ranged between 0.35 and 1.47 per 100,000 inhabitants during the 1661–1898 period, a low level unmatched by Western Europe until the late 19th century. China's homicide rate rose steadily from 1661 to 1821 but declined gradually thereafter until the turn of the century."

People in this period were also remarkably on the move. There is evidence suggesting that the empire's rapidly expanding population was geographically mobile on a scale, which, in term of its volume and its protracted and routinized nature, was unprecedented in Chinese history. Indeed, the Qing government did far more to encourage mobility than to discourage it. Migration took several different forms, though might be divided in two varieties: permanent migration for resettlement, and relocation conceived by the party (in theory at least) as a temporary sojourn. Parties to the latter would include the empire's increasingly large and mobile manual workforce, as well as its densely overlapping internal diaspora of local-origin-based merchant groups. It would also included the patterned movement of Qing subjects overseas, largely to Southeastern Asia, in search of trade and other economic opportunities.

According to statute, Qing society was divided into relatively closed estates, of which in most general terms there were five. Apart from the estates of the officials, the comparatively minuscule aristocracy, and the degree-holding literati, there also existed a major division among ordinary Chinese between commoners and people with inferior status. They were divided into two categories: one of them, the good "commoner" people, the other "mean" people who were seen as debased and servile. The majority of the population belonged to the first category and were described as "liangmin", a legal term meaning good people, as opposed to "jianmin" meaning the mean (or ignoble) people. Qing law explicitly stated that the traditional four occupational groups of scholars, farmers, artisans and merchants were "good", or having a status of commoners. On the other hand, slaves or bondservants, entertainers (including prostitutes and actors), tattooed criminals, and those low-level employees of government officials were the "mean people". Mean people were considered legally inferior to commoners and suffered unequal treatments, forbidden to take the imperial examination. Furthermore, such people were usually not allowed to marry with free commoners and were even often required to acknowledge their abasement in society through actions such as bowing. However, throughout the Qing dynasty, the emperor and his court, as well as the bureaucracy, worked towards reducing the distinctions between the debased and free but did not completely succeed even at the end of its era in merging the two classifications together.

Although the Qing gentry did not hold hereditary noble rank, they, like their British counterparts, were a group of elites who held imperial privileges and managed local affairs. Their public role was that of an imperially acknowledged male scholar and civil servant who had succeeded in passing in at least the first level of civil service examinations, held a degree, could legally wear gentry robes, was qualified for official service and could talk to other officials as equals even though the gentry may not be an official himself. It was not uncommon in the Qing that some of the gentry were officials who had served for one or two short terms in their youth and then "retired" to enjoy the glory of their status during their mid to later years. In terms of a more private role, the gentry included not only the males holding degrees but also their wives, descendants, some of their relatives, as well as some patrilineages who had once or perhaps never had someone who had held a degree.

The Qing gentry were mostly prominently defined by their lifestyle. They lived more refined and comfortable lives than the commoners and were often known to use sedan-chairs as a mean to travel any significant distances. They were also usually quite literate and often tried to show off their intelligence, such as by wearing items like eyeglasses. Many were also known to have objects such as porcelain or pieces of art in their homes that served no purpose other than to be admired for their beauty. This was because not everyone could afford these luxuries and thus such unnecessary objects were seen as a form of class.

In Qing society, women did not enjoy the same rights as men. The Confucian moral system that was built by and thus favored men restrained their rights and were they often seen as a type of "merchandise" that could be traded away by their family. Once a woman married, she would essentially become the property of her husband's family and could not divorce her husband unless under very specific circumstances, such as severe physical harm or an attempt to sell her into prostitution. Men, on the other hand, could divorce their wives for trivial matters such as excessive talkativeness. Furthermore, women were extremely restricted in owning property and inheritance and were essentially confined to their homes and stripped of social interaction and mobility. Mothers would often bind their young daughters' feet, a practice that was seen as a standard of feminine beauty and a necessity to be marriageable, but was also in fact a way to restrict a woman's physical movement in society.

By early Qing, the romanticized courtesan culture that had been much more popular in the late-Ming with men who had sought after a model of a refinement and literacy that was missing from their marriage partners had mostly disappeared. Such a decline was the result of the Qing's reinforced defense of fundamental Confucian family values as well as an attempt to put a stop to the cultural revolution that was happening at the time. The court thus began to rain down heavily on such practices as prostitution, pornography, rape, and homosexuality. However, by the time of the Qianlong emperor, red light districts had once again become capitals of tasteful and trending courtesanship. In economically diverse port cities such as Tianjin, Chongqing, and Hankou, the sex trade became a large business, which helped supply a fine hierarchy of prostitutes to all classes of men. Shanghai, which had been rapidly growing in the late nineteenth century, became a city where prostitutes of different ranks whom male patrons fawned over and gossiped about, as some became recognized as national entities of femininity.

Another phenomenon that came into being, especially during the eighteenth century was the cult of widow chastity. The fact that many young women were betrothed during early adolescence coupled with the high rate of early mortality resulted in a significant number of young widows. This resulted in a problem, as most women had already moved into their husband's household and upon her husband's death would essentially become a burden who could never fulfill her original duty of producing a male heir. Widow chastity began to be seen as a form of devout filiality for other relationships including loyalty to the emperor, which resulted in the Qing court's attempt to reward those families who resisted selling off their unneeded daughter-in-laws in order to underline such women's virtuosity. However, this system began to die down when cases of families who attempted to "abuse" the system appeared for social competition and authorities speculated that some families coerced their young widows to commit suicide at the time of their husband's death to obtain more honors. Such corruption showed a lack of respect for human life, and was thus greatly disapproved of by the officials who then chose to award the families more sparingly.

One of the main reasons for a shift in gender roles was the unprecedented high incidences of men leaving their homes to travel, which in turn gave women more freedom for action. Wives of such men often became the ones to run the household, especially with financial matters. Elite women also began to pursue different kinds of fashionable activities, such as writing poetry, and a new frenzy of female sociability appeared. Women started to leave their households to attend local opera performances and temple festivals and some even began to form little societies to venture about famous sacred sites with other restless women, which helped to shape a new view of the conventional societal norms on how women should act.

One of the most important features of the Qing era was the power of patrilineal kinship socially and culturally, since people strongly believed that a person's success or failure had a direct correspondence with how he was guided by his parental figure. This was because such parental oversight was seen as a type of formula for a family's success and future prosperity. The main type of kinship structure was one known as the patrilineage, which has often been associated with a translation of "clan" and was the primary organizational device in society. This change began during the Song dynasty when the civil service examination became the standard for gaining status versus being nobility due to birth, which caused elite families to begin changing their marital practices, identity and loyalty. Instead of intermarrying within an aristocratic elite, they began to form marital alliances with other nearby wealthy families and established the local people's interests as first and foremost which helped to form intermarried townships. These kind of local lineages reached its peak during the Qing era and thus became the building blocks of society.

Although the Qing lineages could be said to be mainly based on biological descent, they were in fact purposefully "crafted". The first act usually was to carefully identify an older "founding ancestor" and once such a person had been agreed upon, specific generational characters would be given to succeeding male generations. A written genealogy was made to record the lineage's history, biographies of respected ancestors, a chart of all the family members of each generation, rules for the members to abide, and often copies of title contracts for collective property as well. Lastly, an ancestral hall was built to serve as the lineage's headquarters and a place for annual ancestral sacrifice. Members strongly believed that such worship would ensure that their ancestors remain content and benevolent spirits, known as "shen" who would thus keep watch over the family tradition and protect it in the process. Although such actions could be seen as mere superstition, perhaps it was the family’s guilt or grief for the unexpected or abnormal that resulted in them. Thus, the ancestral cult focuses on the family and lineage and its cohesion, rather than on more public matters such as community and nation, causing the family to be more bound together and result in the most powerful and prevalent element in Chinese society.

By the end of the 17th century, the Chinese economy had recovered from the devastation caused by the wars in which the Ming dynasty were overthrown, and the resulting breakdown of order. In the following century, markets continued to expand as in the late Ming period, but with more trade between regions, a greater dependence on overseas markets and a greatly increased population. By the end of the 18th century the population had risen to 300 million from approximately 150 million during the late Ming dynasty. The dramatic rise in population was due to several reasons, including the long period of peace and stability in the 18th century and the import of new crops China received from the Americas, including peanuts, sweet potatoes and maize. New species of rice from Southeast Asia led to a huge increase in production. Merchant guilds proliferated in all of the growing Chinese cities and often acquired great social and even political influence. Rich merchants with official connections built up huge fortunes and patronized literature, theater and the arts. Textile and handicraft production boomed.

The government broadened land ownership by returning land that had been sold to large landowners in the late Ming period by families unable to pay the land tax. To give people more incentives to participate in the market, they reduced the tax burden in comparison with the late Ming, and replaced the corvée system with a head tax used to hire laborers. The administration of the Grand Canal was made more efficient, and transport opened to private merchants. A system of monitoring grain prices eliminated severe shortages, and enabled the price of rice to rise slowly and smoothly through the 18th century. Wary of the power of wealthy merchants, Qing rulers limited their trading licenses and usually refused them permission to open new mines, except in poor areas. These restrictions on domestic resource exploration, as well as on foreign trade, are held by some scholars as a cause of the Great Divergence, by which the Western world overtook China economically.

During the Ming-Qing period (1368–1911) the biggest development in the Chinese economy was its transition from a command to a market economy, the latter becoming increasingly more pervasive throughout the Qing's rule. From roughly 1550 to 1800 China proper experienced a second commercial revolution, developing naturally from the first commercial revolution of the Song period which saw the emergence of long-distance inter-regional trade of luxury goods. During the second commercial revolution, for the first time, a large percentage of farming households began producing crops for sale in the local and national markets rather than for their own consumption or barter in the traditional economy. Surplus crops were placed onto the national market for sale, integrating farmers into the commercial economy from the ground up. This naturally led to regions specializing in certain cash-crops for export as China's economy became increasingly reliant on inter-regional trade of bulk staple goods such as cotton, grain, beans, vegetable oils, forest products, animal products, and fertilizer.

Perhaps the most important factor in the development of the second commercial revolution was the mass influx of silver that entered into the country from foreign trade. After the Spanish conquered the Philippines in the 1570s they mined for silver around the New World, greatly expanding the circulating supply of silver. Foreign trade stimulated the ubiquity of the silver standard, after the re-opening of the southeast coast, which had been closed in the late 17th century, foreign trade was quickly re-established, and was expanding at 4% per annum throughout the latter part of the 18th century. China continued to export tea, silk and manufactures, creating a large, favorable trade balance with the West. The resulting inflow of silver expanded the money supply, facilitating the growth of competitive and stable markets. During the mid-Ming China had gradually shifted to silver as the standard currency for large scale transactions and by the late Kangxi reign the assessment and collection of the land tax was done in silver. By standardizing the collection of the land tax in silver, landlords followed suit and began only accepting rent payments in silver rather than in crops themselves, which in turn incentivized farmers to produce crops for sale in local and national markets rather than for their own personal consumption or barter. Unlike the copper coins, "qian" or cash, used mainly for smaller peasant transactions, silver was not properly minted into a coin but rather was traded in designated units of weight: the "liang" or "tael", which equaled roughly 1.3 ounces of silver. Since it was never properly minted, a third-party had to be brought in to assess the weight and purity of the silver, resulting in an extra "meltage fee" added on to the price of transaction. Furthermore, since the "meltage fee" was unregulated until the reign of the Yongzheng emperor it was the source of much corruption at each level of the bureaucracy. The Yongzheng emperor cracked down on the corrupt "meltage fees," legalizing and regulating them so that they could be collected as a tax, "returning meltage fees to the public coffer." From this newly increased public coffer, the Yongzheng emperor increased the salaries of the officials who collected them, further legitimizing silver as the standard currency of the Qing economy.

The second commercial revolution also had a profound effect on the dispersion of the Qing populace. Up until the late-Ming there existed a stark contrast between the rural countryside and city metropoles and very few mid-sized cities existed. This was due to the fact that extraction of surplus crops from the countryside was traditionally done by the state and not commercial organizations. However, as commercialization expanded exponentially in the late-Ming and early-Qing, mid-sized cities began popping up to direct the flow of domestic, commercial trade. Some towns of this nature had such a large volume of trade and merchants flowing through them that they developed into full-fledged market-towns. Some of these more active market-towns even developed into small-cities and became home to the new rising merchant-class. The proliferation of these mid-sized cities was only made possible by advancements in long-distance transportation and methods of communication. As more and more Chinese-citizens were travelling the country conducting trade they increasingly found themselves in a far-away place needing a place to stay, in response the market saw the expansion of guild halls to house these merchants.

A key distinguishing feature of the Qing economy was the emergence of guild halls around the nation. As inter-regional trade and travel became ever more common during the Qing, guild halls dedicated to facilitating commerce, "huiguan", gained prominence around the urban landscape. The location where two merchants would meet to exchange commodities was usually mediated by a third-party broker who served a variety of roles for the market and local citizenry including bringing together buyers and sellers, guaranteeing the good faith of both parties, standardizing the weights, measurements, and procedures of the two parties, collecting tax for the government, and operating inns and warehouses. It was these broker's and their places of commerce that were expanded during the Qing into full-fledged trade guilds, which, among other things, issued regulatory codes and price schedules, and provided a place for travelling merchants to stay and conduct their business. The first recorded trade guild set up to facilitate inter-regional commerce was in Hankou in 1656. Along with the "huiguan" trade guilds, guild halls dedicated to more specific professions, "gongsuo", began to appear and to control commercial craft or artisanal industries such as carpentry, weaving, banking, and medicine. By the nineteenth century guild halls had much more impact on the local communities than simply facilitating trade, they transformed urban areas into cosmopolitan, multi-cultural hubs, staged theatre performances open to general public, developed real estate by pooling funds together in the style of a trust, and some even facilitated the development of social services such as maintaining streets, water supply, and sewage facilities.

In 1685 the Kangxi emperor legalized private maritime trade along the coast, establishing a series of customs stations in major port cities. The customs station at Canton became by far the most active in foreign trade and by the late Kangxi reign more than forty mercantile houses specializing in trade with the West had appeared. The Yongzheng emperor made a parent corporation comprising those forty individual houses in 1725 known as the Cohong system. Firmly established by 1757, the Canton Cohong was an association of thirteen business firms that had been awarded exclusive rights to conduct trade with Western merchants in Canton. Until its abolition after the Opium War in 1842, the Canton Cohong system was the only permitted avenue of Western trade into China, and thus became a booming hub of international trade by the early eighteenth century. By the eighteenth century the most significant export China had was tea. British demand for tea increased exponentially up until they figured out how to grow it for themselves in the hills of northern India in the 1880s. By the end of the eighteenth century tea exports going through the Canton Cohong system amounted to one-tenth of the revenue from taxes collected from the British and nearly the entire revenue of the British East India Company and until the early nineteenth century tea comprised ninety percent of exports leaving Canton.

Under the Qing, traditional forms of art flourished and innovations occurred at many levels and in many types. High levels of literacy, a successful publishing industry, prosperous cities, and the Confucian emphasis on cultivation all fed a lively and creative set of cultural fields.

The Qing emperors were generally adept at poetry and often skilled in painting, and offered their patronage to Confucian culture. The Kangxi and Qianlong Emperors, for instance, embraced Chinese traditions both to control them and to proclaim their own legitimacy. The Kangxi Emperor sponsored the "Peiwen Yunfu", a rhyme dictionary published in 1711, and the "Kangxi Dictionary" published in 1716, which remains to this day an authoritative reference. The Qianlong Emperor sponsored the largest collection of writings in Chinese history, the "Siku Quanshu," completed in 1782. Court painters made new versions of the Song masterpiece, Zhang Zeduan's "Along the River During the Qingming Festival" whose depiction of a prosperous and happy realm demonstrated the beneficence of the emperor. The emperors undertook tours of the south and commissioned monumental scrolls to depict the grandeur of the occasion. Imperial patronage also encouraged the industrial production of ceramics and Chinese export porcelain. Peking glassware became popular after European glass making processes were introduced by Jesuits to Beijing.

Yet the most impressive aesthetic works were done among the scholars and urban elite. Calligraphy and painting remained a central interest to both court painters and scholar-gentry who considered the Four Arts part of their cultural identity and social standing. The painting of the early years of the dynasty included such painters as the orthodox Four Wangs and the individualists Bada Shanren (1626–1705) and Shitao (1641–1707). The nineteenth century saw such innovations as the Shanghai School and the Lingnan School which used the technical skills of tradition to set the stage for modern painting.

Traditional learning flourished, especially among Ming loyalists such as Dai Zhen and Gu Yanwu, but scholars in the school of evidential learning made innovations in skeptical textual scholarship. Scholar-bureaucrats, including Lin Zexu and Wei Yuan, developed a school of practical statecraft which rooted bureaucratic reform and restructuring in classical philosophy.

Literature grew to new heights in the Qing period. Poetry continued as a mark of the cultivated gentleman, but women wrote in larger and larger numbers and came from all walks of life. The poetry of the Qing dynasty is a lively field of research, being studied (along with the poetry of the Ming dynasty) for its association with Chinese opera, developmental trends of Classical Chinese poetry, the transition to a greater role for vernacular language, and for poetry by women in Chinese culture. The Qing dynasty was a period of much literary collection and criticism, and many of the modern popular versions of Classical Chinese poems were transmitted through Qing dynasty anthologies, such as the "Quantangshi" and the "Three Hundred Tang Poems". Pu Songling brought the short story form to a new level in his "Strange Stories from a Chinese Studio", published in the mid-18th century, and Shen Fu demonstrated the charm of the informal memoir in "Six Chapters of a Floating Life", written in the early 19th century but published only in 1877. The art of the novel reached a pinnacle in Cao Xueqin's "Dream of the Red Chamber", but its combination of social commentary and psychological insight were echoed in highly skilled novels such as Wu Jingzi's "Rulin waishi" (1750) and Li Ruzhen's "Flowers in the Mirror" (1827).

In drama, Kong Shangren's Kunqu opera "The Peach Blossom Fan", completed in 1699, portrayed the tragic downfall of the Ming dynasty in romantic terms. The most prestigious form became the so-called Peking opera, though local and folk opera were also widely popular.

Cuisine aroused a cultural pride in the richness of a long and varied past. The gentleman gourmet, such as Yuan Mei, applied aesthetic standards to the art of cooking, eating, and appreciation of tea at a time when New World crops and products entered everyday life. Yuan's "Suiyuan Shidan" expounded culinary aesthetics and theory, along with a range of recipes. The Manchu Han Imperial Feast originated at the court. Although this banquet was probably never common, it reflected an appreciation of Manchu culinary customs. Nevertheless, culinary traditionalists such as Yuan Mei lambasted the opulence of the Manchu Han Feast. Yuan wrote that the feast was caused in part by the "vulgar habits of bad chefs" and that "displays this trite are useful only for welcoming new relations through one's gates or when the boss comes to visit." (皆惡廚陋習。只可用之於新親上門，上司入境)

By the end of the nineteenth century, national artistic and cultural worlds had begun to come to terms with the cosmopolitan culture of the West and Japan. The decision to stay within old forms or welcome Western models was now a conscious choice rather than an unchallenged acceptance of tradition. Classically trained Confucian scholars such as Liang Qichao and Wang Guowei read widely and broke aesthetic and critical ground later cultivated in the New Culture Movement.






</doc>
<doc id="25312" url="https://en.wikipedia.org/wiki?curid=25312" title="Quantum gravity">
Quantum gravity

Quantum gravity (QG) is a field of theoretical physics that seeks to describe gravity according to the principles of quantum mechanics, and where quantum effects cannot be ignored, such as near compact astrophysical objects where the effects of gravity are strong.

The current understanding of gravity is based on Albert Einstein's general theory of relativity, which is formulated within the framework of classical physics. On the other hand, the other three fundamental forces of physics are described within the framework of quantum mechanics and quantum field theory, radically different formalisms for describing physical phenomena. It is sometimes argued that a quantum mechanical description of gravity is necessary on the grounds that one cannot consistently couple a classical system to a quantum one.

While a quantum theory of gravity may be needed to reconcile general relativity with the principles of quantum mechanics, difficulties arise when applying the usual prescriptions of quantum field theory to the force of gravity via graviton bosons. The problem is that the theory one gets in this way is not renormalizable and therefore cannot be used to make meaningful physical predictions. As a result, theorists have taken up more radical approaches to the problem of quantum gravity, the most popular approaches being string theory and loop quantum gravity. Although some quantum gravity theories, such as string theory, try to unify gravity with the other fundamental forces, others, such as loop quantum gravity, make no such attempt; instead, they make an effort to quantize the gravitational field while it is kept separate from the other forces.

Strictly speaking, the aim of quantum gravity is only to describe the quantum behavior of the gravitational field and should not be confused with the objective of unifying all fundamental interactions into a single mathematical framework. A theory of quantum gravity that is also a grand unification of all known interactions is sometimes referred to as The Theory of Everything (TOE). While any substantial improvement into the present understanding of gravity would aid further work towards unification, the study of quantum gravity is a field in its own right with various branches having different approaches to unification.

One of the difficulties of formulating a quantum gravity theory is that quantum gravitational effects only appear at length scales near the Planck scale, around 10 meter, a scale far smaller, and equivalently far larger in energy, than those currently accessible by high energy particle accelerators. Therefore physicists lack experimental data which could distinguish between the competing theories which have been proposed.

Much of the difficulty in meshing these theories at all energy scales comes from the different assumptions that these theories make on how the universe works. General relativity models gravity as curvature of spacetime: in the slogan of John Archibald Wheeler, "Spacetime tells matter how to move; matter tells spacetime how to curve." On the other hand, quantum field theory is typically formulated in the "flat" spacetime used in special relativity. No theory has yet proven successful in describing the general situation where the dynamics of matter, modeled with quantum mechanics, affect the curvature of spacetime. If one attempts to treat gravity as simply another quantum field, the resulting theory is not renormalizable. Even in the simpler case where the curvature of spacetime is fixed "a priori," developing quantum field theory becomes more mathematically challenging, and many ideas physicists use in quantum field theory on flat spacetime are no longer applicable.

It is widely hoped that a theory of quantum gravity would allow us to understand problems of very high energy and very small dimensions of space, such as the behavior of black holes, and the origin of the universe.

Popularly harmonizing the theory of general relativity that describes gravitation, and applications to large-scale structures like stars, planets, and galaxies with quantum mechanics, that describes the other three fundamental forces acting on the atomic scale, quantum mechanics and general relativity can seem fundamentally incompatible. Also, demonstrations of the structure of general relativity essentially follows inevitably from the quantum mechanics of interacting theoretical spin-2 massless particles called gravitons.

No concrete proof of gravitons exists, but quantized theories of matter may necessitate their existence. The observation that all fundamental forces except gravity have one or more known messenger particles leads researchers to believe that at least one must exist. This hypothetical particle is known as the "graviton". The predicted find would result in the classification of the graviton as a force particle similar to the photon of the electromagnetic interaction. Many of the accepted notions of a unified theory of physics since the 1970s assume, and to some degree depend upon, the existence of the graviton. These include string theory, superstring theory, and M-theory. Detection of gravitons would validate these various lines of research to unify quantum mechanics and relativity theory.

The Weinberg–Witten theorem places some constraints on theories in which the graviton is a composite particle.

The dilaton made its first appearance in Kaluza–Klein theory, a five-dimensional theory that combined gravitation and electromagnetism. It appears in string theory. However, it's become central to the lower-dimensional many-bodied gravity problem based on the field theoretic approach of Roman Jackiw. The impetus arose from the fact that complete analytical solutions for the metric of a covariant "N"-body system have proven elusive in general relativity. To simplify the problem, the number of dimensions was lowered to "1+1" - one spatial dimension and one temporal dimension. This model problem, known as "R=T" theory, as opposed to the general "G=T" theory, was amenable to exact solutions in terms of a generalization of the Lambert W function. Also, the field equation governing the dilaton, derived from differential geometry, as the Schrödinger equation could be amenable to quantization.

This combines gravity, quantization, and even the electromagnetic interaction, promising ingredients of a fundamental physical theory. This outcome revealed a previously unknown and already existing natural link between general relativity and quantum mechanics. There lacks clarity in the generalization of this theory to "3+1" dimensions. However, a recent derivation in "3+1" dimensions under the right coordinate conditions yields a formulation similar to the earlier "1+1", a dilaton field governed by the logarithmic Schrödinger equation that is seen in condensed matter physics and superfluids. The field equations are amenable to such a generalization, as shown with the inclusion of a one-graviton process, and yield the correct Newtonian limit in "d" dimensions, but only with a dilaton. Furthermore, some speculate on the view of the apparent resemblance between the dilaton and the Higgs boson. However, there needs more experimentation to resolve the relationship between these two particles. Finally, since this theory can combine gravitational, electromagnetic, and quantum effects, their coupling could potentially lead to a means of testing the theory through cosmology and experimentation.

General relativity, like electromagnetism, is a classical field theory. One might expect that, as with electromagnetism, the gravitational force should also have a corresponding quantum field theory.

However, gravity is perturbatively nonrenormalizable. For a quantum field theory to be well-defined according to this understanding of the subject, it must be asymptotically free or asymptotically safe. The theory must be characterized by a choice of "finitely many" parameters, which could, in principle, be set by experiment. For example, in quantum electrodynamics these parameters are the charge and mass of the electron, as measured at a particular energy scale.

On the other hand, in quantizing gravity there are, in perturbation theory, "infinitely many independent parameters" (counterterm coefficients) needed to define the theory. For a given choice of those parameters, one could make sense of the theory, but since it is impossible to conduct infinite experiments to fix the values of every parameter, it has been argued that one does not, in perturbation theory, have a meaningful physical theory. At low energies, the logic of the renormalization group tells us that, despite the unknown choices of these infinitely many parameters, quantum gravity will reduce to the usual Einstein theory of general relativity. On the other hand, if we could probe very high energies where quantum effects take over, then "every one" of the infinitely many unknown parameters would begin to matter, and we could make no predictions at all.

It is conceivable that, in the correct theory of quantum gravity, the infinitely many unknown parameters will reduce to a finite number that can then be measured. One possibility is that normal perturbation theory is not a reliable guide to the renormalizability of the theory, and that there really "is" a UV fixed point for gravity. Since this is a question of non-perturbative quantum field theory, it is difficult to find a reliable answer, but some people still pursue this option. Another possibility is that there are new, undiscovered symmetry principles that constrain the parameters and reduce them to a finite set. This is the route taken by string theory, where all of the excitations of the string essentially manifest themselves as new symmetries.

In an effective field theory, all but the first few of the infinite set of parameters in a nonrenormalizable theory are suppressed by huge energy scales and hence can be neglected when computing low-energy effects. Thus, at least in the low-energy regime, the model is a predictive quantum field theory. Furthermore, many theorists argue that the Standard Model should be regarded as an effective field theory itself, with "nonrenormalizable" interactions suppressed by large energy scales and whose effects have consequently not been observed experimentally.

By treating general relativity as an effective field theory, one can actually make legitimate predictions for quantum gravity, at least for low-energy phenomena. An example is the well-known calculation of the tiny first-order quantum-mechanical correction to the classical Newtonian gravitational potential between two masses.

A fundamental lesson of general relativity is that there is no fixed spacetime background, as found in Newtonian mechanics and special relativity; the spacetime geometry is dynamic. While easy to grasp in principle, this is the hardest idea to understand about general relativity, and its consequences are profound and not fully explored, even at the classical level. To a certain extent, general relativity can be seen to be a relational theory, in which the only physically relevant information is the relationship between different events in space-time.

On the other hand, quantum mechanics has depended since its inception on a fixed background (non-dynamic) structure. In the case of quantum mechanics, it is time that is given and not dynamic, just as in Newtonian classical mechanics. In relativistic quantum field theory, just as in classical field theory, Minkowski spacetime is the fixed background of the theory.

String theory can be seen as a generalization of quantum field theory where instead of point particles, string-like objects propagate in a fixed spacetime background, although the interactions among closed strings give rise to space-time in a dynamical way.
Although string theory had its origins in the study of quark confinement and not of quantum gravity, it was soon discovered that the string spectrum contains the graviton, and that "condensation" of certain vibration modes of strings is equivalent to a modification of the original background. In this sense, string perturbation theory exhibits exactly the features one would expect of a perturbation theory that may exhibit a strong dependence on asymptotics (as seen, for example, in the AdS/CFT correspondence) which is a weak form of background dependence.

Loop quantum gravity is the fruit of an effort to formulate a background-independent quantum theory.

Topological quantum field theory provided an example of background-independent quantum theory, but with no local degrees of freedom, and only finitely many degrees of freedom globally. This is inadequate to describe gravity in 3+1 dimensions, which has local degrees of freedom according to general relativity. In 2+1 dimensions, however, gravity is a topological field theory, and it has been successfully quantized in several different ways, including spin networks.

Quantum field theory on curved (non-Minkowskian) backgrounds, while not a full quantum theory of gravity, has shown many promising early results. In an analogous way to the development of quantum electrodynamics in the early part of the 20th century (when physicists considered quantum mechanics in classical electromagnetic fields), the consideration of quantum field theory on a curved background has led to predictions such as black hole radiation.

Phenomena such as the Unruh effect, in which particles exist in certain accelerating frames but not in stationary ones, do not pose any difficulty when considered on a curved background (the Unruh effect occurs even in flat Minkowskian backgrounds). The vacuum state is the state with the least energy (and may or may not contain particles).
See Quantum field theory in curved spacetime for a more complete discussion.

A conceptual difficulty in combining quantum mechanics with general relativity arises from the contrasting role of time within these two frameworks. In quantum theories time acts as an independent background through which states evolve, with the Hamiltonian operator acting as the generator of infinitesimal translations of quantum states through time. In contrast, general relativity treats time as a dynamical variable which interacts directly with matter and moreover requires the Hamiltonian constraint to vanish, removing any possibility of employing a notion of time similar to that in quantum theory.

There are a number of proposed quantum gravity theories. Currently, there is still no complete and consistent quantum theory of gravity, and the candidate models still need to overcome major formal and conceptual problems. They also face the common problem that, as yet, there is no way to put quantum gravity predictions to experimental tests, although there is hope for this to change as future data from cosmological observations and particle physics experiments becomes available.

One suggested starting point is ordinary quantum field theories which are successful in describing the other three basic fundamental forces in the context of the standard model of elementary particle physics. However, while this leads to an acceptable effective (quantum) field theory of gravity at low energies, gravity turns out to be much more problematic at higher energies. For ordinary field theories such as quantum electrodynamics, a technique known as renormalization is an integral part of deriving predictions which take into account higher-energy contributions, but gravity turns out to be nonrenormalizable: at high energies, applying the recipes of ordinary quantum field theory yields models that are devoid of all predictive power.

One attempt to overcome these limitations is to replace ordinary quantum field theory, which is based on the classical concept of a point particle, with a quantum theory of one-dimensional extended objects: string theory. At the energies reached in current experiments, these strings are indistinguishable from point-like particles, but, crucially, different modes of oscillation of one and the same type of fundamental string appear as particles with different (electric and other) charges. In this way, string theory promises to be a unified description of all particles and interactions. The theory is successful in that one mode will always correspond to a graviton, the messenger particle of gravity; however, the price of this success are unusual features such as six extra dimensions of space in addition to the usual three for space and one for time.

In what is called the , it was conjectured that both string theory and a unification of general relativity and supersymmetry known as supergravity form part of a hypothesized eleven-dimensional model known as M-theory, which would constitute a uniquely defined and consistent theory of quantum gravity. As presently understood, however, string theory admits a very large number (10 by some estimates) of consistent vacua, comprising the so-called "string landscape". Sorting through this large family of solutions remains a major challenge.

Loop quantum gravity seriously considers general relativity's insight that spacetime is a dynamical field and is therefore a quantum object. Its second idea is that the quantum discreteness that determines the particle-like behavior of other field theories (for instance, the photons of the electromagnetic field) also affects the structure of space.

The main result of loop quantum gravity is the derivation of a granular structure of space at the Planck length. This is derived from following considerations: In the case of electromagnetism, the quantum operator representing the energy of each frequency of the field has a discrete spectrum. Thus the energy of each frequency is quantized, and the quanta are the photons. In the case of gravity, the operators representing the area and the volume of each surface or space region likewise have discrete spectrum. Thus area and volume of any portion of space are also quantized, where the quanta are elementary quanta of space. It follows, then, that spacetime has an elementary quantum granular structure at the Planck scale, which cuts off the ultraviolet infinities of quantum field theory.

The quantum state of spacetime is described in the theory by means of a mathematical structure called Spin Networks. Spin networks were initially introduced in 1964 by Roger Penrose in abstract form, as a way to set up an intrinsically quantum mechanical model of spacetime, and later shown by Carlo Rovelli and Lee Smolin to derive naturally from a non-perturbative quantization of general relativity. Spin networks do not represent quantum states of a field in spacetime: they represent directly quantum states of spacetime.

The theory is based on the reformulation of general relativity known as Ashtekar variables, which represent geometric gravity using mathematical analogues of electric and magnetic fields.
In the quantum theory, space is represented by a network structure called a spin network, evolving over time in discrete steps.

The dynamics of the theory is today constructed in several versions. One version starts with the canonical quantization of general relativity. The analogue of the Schrödinger equation is a Wheeler–DeWitt equation, which can be defined within the theory.
In the covariant, or spinfoam formulation of the theory, the quantum dynamics is obtained via a sum over discrete versions of spacetime, called spinfoams. These represent histories of spin networks.

There are a number of other approaches to quantum gravity. The approaches differ depending on which features of general relativity and quantum theory are accepted unchanged, and which features are modified. Examples include:

As was emphasized above, quantum gravitational effects are extremely weak and therefore difficult to test. For this reason, the possibility of experimentally testing quantum gravity had not received much attention prior to the late 1990s. However, in the past decade, physicists have realized that evidence for quantum gravitational effects can guide the development of the theory. Since theoretical development has been slow, the field of phenomenological quantum gravity, which studies the possibility of experimental tests, has obtained increased attention.

The most widely pursued possibilities for quantum gravity phenomenology include violations of Lorentz invariance, imprints of quantum gravitational effects in the cosmic microwave background (in particular its polarization), and decoherence induced by fluctuations in the space-time foam.

The BICEP2 experiment detected what was initially thought to be primordial B-mode polarization caused by gravitational waves in the early universe. Had the signal in fact been primordial in origin, it could have been an indication of quantum gravitational effects, but it soon transpired that the polarization was due to interstellar dust interference.




</doc>
<doc id="25315" url="https://en.wikipedia.org/wiki?curid=25315" title="Quality of service">
Quality of service

Quality of service (QoS) is the description or measurement of the overall performance of a service, such as a telephony or computer network or a cloud computing service, particularly the performance seen by the users of the network. To quantitatively measure quality of service, several related aspects of the network service are often considered, such as packet loss, bit rate, throughput, transmission delay, availability, jitter, etc.

In the field of computer networking and other packet-switched telecommunication networks, quality of service refers to traffic prioritization and resource reservation control mechanisms rather than the achieved service quality. Quality of service is the ability to provide different priority to different applications, users, or data flows, or to guarantee a certain level of performance to a data flow.

Quality of service is particularly important for the transport of traffic with special requirements. In particular, developers have introduced Voice over IP technology to allow computer networks to become as useful as telephone networks for audio conversations, as well as supporting new applications with even stricter network performance requirements.

In the field of telephony, quality of service was defined by the ITU in 1994. Quality of service comprises requirements on all the aspects of a connection, such as service response time, loss, signal-to-noise ratio, crosstalk, echo, interrupts, frequency response, loudness levels, and so on. A subset of telephony QoS is grade of service (GoS) requirements, which comprises aspects of a connection relating to capacity and coverage of a network, for example guaranteed maximum blocking probability and outage probability.

In the field of computer networking and other packet-switched telecommunication networks, teletraffic engineering refers to traffic prioritization and resource reservation control mechanisms rather than the achieved service quality. Quality of service is the ability to provide different priority to different applications, users, or data flows, or to guarantee a certain level of performance to a data flow. For example, a required bit rate, delay, delay variation, packet loss or bit error rates may be guaranteed. Quality of service is important for real-time streaming multimedia applications such as voice over IP, multiplayer online games and IPTV, since these often require fixed bit rate and are delay sensitive. Quality of service is especially important in networks where the capacity is a limited resource, for example in cellular data communication.

A network or protocol that supports QoS may agree on a traffic contract with the application software and reserve capacity in the network nodes, for example during a session establishment phase. During the session it may monitor the achieved level of performance, for example the data rate and delay, and dynamically control scheduling priorities in the network nodes. It may release the reserved capacity during a tear down phase.

A best-effort network or service does not support quality of service. An alternative to complex QoS control mechanisms is to provide high quality communication over a best-effort network by over-provisioning the capacity so that it is sufficient for the expected peak traffic load. The resulting absence of network congestion reduces or eliminates the need for QoS mechanisms.

QoS is sometimes used as a quality measure, with many alternative definitions, rather than referring to the ability to reserve resources. Quality of service sometimes refers to the level of quality of service, i.e. the guaranteed service quality. High QoS is often confused with a high level of performance, for example high bit rate, low latency and low bit error rate.

QoS is sometimes used in application layer services such as telephony and streaming video to describe a metric that reflects or predicts the subjectively experienced quality. In this context, QoS is the acceptable cumulative effect on subscriber satisfaction of all imperfections affecting the service. Other terms with similar meaning are the quality of experience (QoE), mean opinion score (MOS), perceptual speech quality measure (PSQM) and perceptual evaluation of video quality (PEVQ). See also Subjective video quality.

A number of attempts for layer 2 technologies that add QoS tags to the data have gained popularity in the past. Examples are frame relay, asynchronous transfer mode (ATM) and multiprotocol label switching (MPLS) (a technique between layer 2 and 3). Despite these network technologies remaining in use today, this kind of network lost attention after the advent of Ethernet networks. Today Ethernet is, by far, the most popular layer 2 technology. Conventional Internet routers and LAN switches operate on a best effort basis. This equipment is less expensive, less complex and faster and thus more popular than earlier more complex technologies that provide QoS mechanisms. 

Ethernet optionally uses 802.1p to signal the priority of a frame. 

There were four "type of service" bits and three "precedence" bits originally provided in each IP packet header, but they were not generally respected. These bits were later re-defined as Differentiated services code points (DSCP).

With the advent of IPTV and IP telephony, QoS mechanisms are increasingly available to the end user.

In packet-switched networks, quality of service is affected by various factors, which can be divided into human and technical factors. Human factors include: stability of service quality, availability of service, waiting times and user information. Technical factors include: reliability, scalability, effectiveness, maintainability and network congestion.

Many things can happen to packets as they travel from origin to destination, resulting in the following problems as seen from the point of view of the sender and receiver:


A defined quality of service may be desired or required for certain types of network traffic, for example:

These types of service are called "inelastic", meaning that they require a certain minimum bit rate and a certain maximum latency to function. By contrast, "elastic" applications can take advantage of however much or little bandwidth is available. Bulk file transfer applications that rely on TCP are generally elastic.

Circuit switched networks, especially those intended for voice transmission, such as Asynchronous Transfer Mode (ATM) or GSM, have QoS in the core protocol and do not need additional procedures to achieve it. Shorter data units and built-in QoS were some of the unique selling points of ATM for applications such as video on demand.

When the expense of mechanisms to provide QoS is justified, network customers and providers can enter into a contractual agreement termed a service level agreement (SLA) which specifies guarantees for the ability of a network/protocol to give guaranteed performance/throughput/latency bounds based on mutually agreed measures, usually by prioritizing traffic.
In other approaches, resources are reserved at each step on the network for the call as it is set up.

An alternative to complex QoS control mechanisms is to provide high quality communication by generously over-provisioning a network so that capacity is based on peak traffic load estimates. This approach is simple for networks with predictable peak loads. The performance is reasonable for many applications. This might include demanding applications that can compensate for variations in bandwidth and delay with large receive buffers, which is often possible for example in video streaming. Over-provisioning can be of limited use, however, in the face of transport protocols (such as TCP) that over time exponentially increase the amount of data placed on the network until all available bandwidth is consumed and packets are dropped. Such greedy protocols tend to increase latency and packet loss for all users.

Commercial VoIP services are often competitive with traditional telephone service in terms of call quality even though QoS mechanisms are usually not in use on the user's connection to their ISP and the VoIP provider's connection to a different ISP. Under high load conditions, however, VoIP may degrade to cell-phone quality or worse. The mathematics of packet traffic indicate that network requires just 60% more raw capacity under conservative assumptions.

The amount of over-provisioning in interior links required to replace QoS depends on the number of users and their traffic demands. This limits usability of over-provisioning. Newer more bandwidth intensive applications and the addition of more users results in the loss of over-provisioned networks. This then requires a physical update of the relevant network links which is an expensive process. Thus over-provisioning cannot be blindly assumed on the Internet.

Unlike single-owner networks, the Internet is a series of exchange points interconnecting private networks. Hence the Internet's core is owned and managed by a number of different network service providers, not a single entity. Its behavior is much more stochastic or unpredictable. Therefore, research continues on QoS procedures that are deployable in large, diverse networks.

There are two principal approaches to QoS in modern packet-switched IP networks, a parameterized system based on an exchange of application requirements with the network, and a prioritized system where each packet identifies a desired service level to the network.

Early work used the integrated services (IntServ) philosophy of reserving network resources. In this model, applications used the Resource reservation protocol (RSVP) to request and reserve resources through a network. While IntServ mechanisms do work, it was realized that in a broadband network typical of a larger service provider, Core routers would be required to accept, maintain, and tear down thousands or possibly tens of thousands of reservations. It was believed that this approach would not scale with the growth of the Internet, and in any event was antithetical to the notion of designing networks so that Core routers do little more than simply switch packets at the highest possible rates.

In response to these markings, routers and switches use various queuing strategies to tailor performance to requirements. At the IP layer, DSCP markings use the 6 bits in the IP packet header. At the MAC layer, VLAN IEEE 802.1Q and IEEE 802.1p can be used to carry essentially the same information.

Routers supporting DiffServ configure their network scheduler to use multiple queues for packets awaiting transmission from bandwidth constrained (e.g., wide area) interfaces. Router vendors provide different capabilities for configuring this behavior, to include the number of queues supported, the relative priorities of queues, and bandwidth reserved for each queue.

In practice, when a packet must be forwarded from an interface with queuing, packets requiring low jitter (e.g., VoIP or videoconferencing) are given priority over packets in other queues. Typically, some bandwidth is allocated by default to network control packets (such as Internet Control Message Protocol and routing protocols), while best effort traffic might simply be given whatever bandwidth is left over.

At the Media Access Control (MAC) layer, VLAN IEEE 802.1Q and IEEE 802.1p can be used to distinguish between Ethernet frames and classify them. Queueing theory models have been developed on performance analysis and QoS for MAC layer protocols.

Cisco IOS NetFlow and the Cisco Class Based QoS (CBQoS) Management Information Base (MIB) are marketed by Cisco Systems.
One compelling example of the need for QoS on the Internet relates to congestion collapse. The Internet relies on congestion avoidance protocols, as built into Transmission Control Protocol (TCP), to reduce traffic under conditions that would otherwise lead to "meltdown". QoS applications, such as VoIP and IPTV, require largely constant bitrates and low latency, therefore they cannot use TCP and cannot otherwise reduce their traffic rate to help prevent congestion. QoS contracts limit traffic that can be offered to the Internet and thereby enforce traffic shaping that can prevent it from becoming overloaded, and are hence an indispensable part of the Internet's ability to handle a mix of real-time and non-real-time traffic without meltdown.


End-to-end quality of service can require a method of coordinating resource allocation between one autonomous system and another.
The Internet Engineering Task Force (IETF) defined the Resource Reservation Protocol (RSVP) for bandwidth reservation, as a proposed standard in 1997.
RSVP is an end-to-end bandwidth reservation protocol. The traffic engineering version, RSVP-TE, is used in many networks to establish traffic-engineered Multiprotocol Label Switching (MPLS) label-switched paths.
The IETF also defined Next Steps in Signaling (NSIS) with QoS signalling as a target. NSIS is a development and simplification of RSVP.

Research consortia such as "end-to-end quality of service support over heterogeneous networks" (EuQoS, from 2004 through 2007) and fora such as the IPsphere Forum developed more mechanisms for handshaking QoS invocation from one domain to the next. IPsphere defined the Service Structuring Stratum (SSS) signaling bus in order to establish, invoke and (attempt to) assure network services.
EuQoS conducted experiments to integrate Session Initiation Protocol, Next Steps in Signaling and IPsphere's SSS with an estimated cost of about 15.6 million Euro and published a book.

A research project Multi Service Access Everywhere (MUSE) defined another QoS concept in a first phase from January 2004 through February 2006, and a second phase from January 2006 through 2007.
Another research project named PlaNetS was proposed for European funding circa 2005.
A broader European project called "Architecture and design for the future Internet" known as 4WARD had a budgest estimated at 23.4 million Euro and was funded from January 2008 through June 2010.
It included a "Quality of Service Theme" and published a book.
Another European project, called WIDENS (Wireless Deployable Network System) proposed a bandwidth reservation approach for mobile wireless multirate adhoc networks.

Strong cryptography network protocols such as Secure Sockets Layer, I2P, and virtual private networks obscure the data transferred using them. As all electronic commerce on the Internet requires the use of such strong cryptography protocols, unilaterally downgrading the performance of encrypted traffic creates an unacceptable hazard for customers. Yet, encrypted traffic is otherwise unable to undergo deep packet inspection for QoS.

Protocols like ICA and RDP may encapsulate other traffic (e.g. printing, video streaming) with varying requirements that can make optimization difficult.

The Internet2 project found, in 2001, that the QoS protocols were probably not deployable inside its Abilene Network with equipment available at that time.
Equipment available at the time relied on software to implement QoS. The group also predicted that “logistical, financial, and organizational barriers will block the way toward any bandwidth guarantees” by protocol modifications aimed at QoS.
They believed that the economics would encourage network providers to deliberately erode the quality of best effort traffic as a way to push customers to higher priced QoS services. Instead they proposed over-provisioning of capacity as more cost-effective at the time.

The Abilene network study was the basis for the testimony of Gary Bachula to the US Senate Commerce Committee's hearing on Network Neutrality in early 2006. He expressed the opinion that adding more bandwidth was more effective than any of the various schemes for accomplishing QoS they examined.

Bachula's testimony has been cited by proponents of a law banning quality of service as proof that no legitimate purpose is served by such an offering. This argument is dependent on the assumption that over-provisioning isn't a form of QoS and that it is always possible. Cost and other factors affect the ability of carriers to build and maintain permanently over-provisioned networks.

Mobile cellular service providers may offer mobile QoS to customers just as the fixed line PSTN services providers and Internet Service Providers (ISP) may offer QoS. QoS mechanisms are always provided for circuit switched services, and are essential for non-elastic services, for example streaming multimedia.

Mobility adds complication to the QoS mechanisms, for several reasons:

Quality of service in the field of telephony, was first defined in 1994 in the ITU-T Recommendation E.800. This definition is very broad, listing 6 primary components: Support, Operability, Accessibility, Retainability, Integrity and Security.
A 1995 recommendation X.902 included a definition is the OSI reference model.
In 1998 the ITU published a document discussing QoS in the field of data networking. X.641 offers a means of developing or enhancing standards related to QoS and provide concepts and terminology that will assist in maintaining the consistency of related standards.

Some QoS-related IETF Request For Comments (RFC)s are , and ; both these are discussed above. The IETF has also published two RFCs giving background on QoS: , and .

The IETF has also published as an informative or "best practices" document about the practical aspects of designing a QoS solution for a DiffServ network. They try to identify which types of applications are commonly run over an IP network to group them into traffic classes, study what treatment do each of these classes need from the network, and suggest which of the QoS mechanisms commonly available in routers can be used to implement those treatments.





</doc>
<doc id="25316" url="https://en.wikipedia.org/wiki?curid=25316" title="Quadrature amplitude modulation">
Quadrature amplitude modulation

Quadrature amplitude modulation (QAM) is the name of a family of digital modulation methods and a related family of analog modulation methods widely used in modern telecommunications to transmit information. It conveys two analog message signals, or two digital bit streams, by changing ("modulating") the amplitudes of two carrier waves, using the amplitude-shift keying (ASK) digital modulation scheme or amplitude modulation (AM) analog modulation scheme. The two carrier waves of the same frequency, usually sinusoids, are out of phase with each other by 90° and are thus called quadrature carriers or quadrature components — hence the name of the scheme. The modulated waves are summed, and the final waveform is a combination of both phase-shift keying (PSK) and amplitude-shift keying (ASK), or, in the analog case, of phase modulation (PM) and amplitude modulation. In the digital QAM case, a finite number of at least two phases and at least two amplitudes are used. PSK modulators are often designed using the QAM principle, but are not considered as QAM since the amplitude of the modulated carrier signal is constant. QAM is used extensively as a modulation scheme for digital telecommunication systems, such as in 802.11 Wi-Fi standards. Arbitrarily high spectral efficiencies can be achieved with QAM by setting a suitable constellation size, limited only by the noise level and linearity of the communications channel.

QAM is being used in optical fiber systems as bit rates increase; QAM16 and QAM64 can be optically emulated with a 3-path interferometer.

Like all modulation schemes, QAM conveys data by changing some aspect of a carrier signal, or carrier wave, (usually a sinusoid) in response to a data signal. In the case of QAM, the carrier wave is the sum of two sinusoidal waves of the same frequency, 90° out of phase with each other (in quadrature). These are often called the "I" or "in-phase" component, and the "Q" or "quadrature" component. Each component wave is amplitude modulated, that is its amplitude is varied to represent the data to be carried, before the two are added together. Amplitude modulating two carriers in quadrature can be equivalently viewed as both amplitude modulating and phase modulating a single carrier.

Phase modulation (analog PM) and phase-shift keying (digital PSK) can be regarded as a special case of QAM, where the magnitude of the modulating signal is a constant, with only the phase varying. This can also be extended to frequency modulation (FM) and frequency-shift keying (FSK), for these can be regarded as a special case of phase modulation.

When transmitting two signals by modulating them with QAM, the transmitted signal will be of the form:

where formula_2, formula_3 and formula_4 are the modulating signals, formula_5 is the carrier frequency and formula_6 is the real part.

At the receiver, these two modulating signals can be demodulated using a coherent demodulator. Such a receiver multiplies the received signal separately with both a cosine and sine signal to produce the received estimates of formula_3 and formula_4 respectively. Because of the orthogonality property of the carrier signals, it is possible to detect the modulating signals independently.

In the ideal case formula_3 is demodulated by multiplying the transmitted signal with a cosine signal:

Using standard trigonometric identities, we can write it as:

Low-pass filtering formula_12 removes the high frequency terms (containing formula_13), leaving only the formula_3 term. This filtered signal is unaffected by formula_4, showing that the in-phase component can be received independently of the quadrature component. Similarly, we may multiply formula_16 by a sine wave and then low-pass filter to extract formula_4.

Analog QAM suffers from the same problem as Single-sideband modulation: the exact phase of the carrier is required for correct demodulation at the receiver. If the demodulating phase is even a little off, it results in crosstalk between the modulated signals. This issue of carrier synchronization at the receiver must be handled somehow in QAM systems. The coherent demodulator needs to be exactly in phase with the received signal, or otherwise the modulated signals cannot be independently received. This is achieved typically by transmitting a burst subcarrier or a Pilot signal.

Analog QAM is used in:

In the frequency domain, QAM has a similar spectral pattern to DSB-SC modulation. Using the properties of the Fourier transform, we find that:

where "S"("f"), "M"("f") and "M"("f") are the Fourier transforms (frequency-domain representations) of "s"("t"), "I"("t") and "Q"("t"), respectively.

As in many digital modulation schemes, the constellation diagram is useful for QAM. In QAM, the constellation points are usually arranged in a square grid with equal vertical and horizontal spacing, although other configurations are possible (e.g. Cross-QAM). Since in digital telecommunications the data is usually binary, the number of points in the grid is usually a power of 2 (2, 4, 8, …). Since QAM is usually square, some of these are rare—the most common forms are 16-QAM, 64-QAM and 256-QAM. By moving to a higher-order constellation, it is possible to transmit more bits per symbol. However, if the mean energy of the constellation is to remain the same (by way of making a fair comparison), the points must be closer together and are thus more susceptible to noise and other corruption; this results in a higher bit error rate and so higher-order QAM can deliver more data less reliably than lower-order QAM, for constant mean constellation energy. Using higher-order QAM without increasing the bit error rate requires a higher signal-to-noise ratio (SNR) by increasing signal energy, reducing noise, or both.

If data-rates beyond those offered by 8-PSK are required, it is more usual to move to QAM since it achieves a greater distance between adjacent points in the I-Q plane by distributing the points more evenly. The complicating factor is that the points are no longer all the same amplitude and so the demodulator must now correctly detect both phase and amplitude, rather than just phase.

64-QAM and 256-QAM are often used in digital cable television and cable modem applications. In the United States, 64-QAM and 256-QAM are the mandated modulation schemes for digital cable (see QAM tuner) as standardised by the SCTE in the standard ANSI/SCTE 07 2013. Note that many marketing people will refer to these as QAM-64 and QAM-256. In the UK, 64-QAM is used for digital terrestrial television (Freeview) whilst 256-QAM is used for Freeview-HD.

Communication systems designed to achieve very high levels of spectral efficiency usually employ very dense QAM constellations. For example, current Homeplug AV2 500-Mbit powerline Ethernet devices use 1024-QAM and 4096-QAM, as well as future devices using ITU-T G.hn standard for networking over existing home wiring (coaxial cable, phone lines and power lines); 4096-QAM provides 12 bits/symbol. Another example is ADSL technology for copper twisted pairs, whose constellation size goes up to 32768-QAM (in ADSL terminology this is referred to as bit-loading, or bit per tone, 32768-QAM being equivalent to 15 bits per tone).

Ultra-high capacity Microwave Backhaul Systems also use 1024-QAM. With 1024-QAM, adaptive coding and modulation (ACM) and XPIC, vendors can obtain gigabit capacity in a single 56 MHz channel.

The following picture shows the ideal structure of a QAM transmitter, with a carrier center frequency formula_5 and the frequency response of the transmitter's filter formula_20:

First the flow of bits to be transmitted is split into two equal parts: this process generates two independent signals to be transmitted. They are encoded separately just like they were in an amplitude-shift keying (ASK) modulator. Then one channel (the one "in phase") is multiplied by a cosine, while the other channel (in "quadrature") is multiplied by a sine. This way there is a phase of 90° between them. They are simply added one to the other and sent through the real channel.

The sent signal can be expressed in the form:

where formula_22 and formula_23 are the voltages applied in response to the formula_24 symbol to the cosine and sine waves respectively.

The receiver simply performs the inverse operation of the transmitter. Its ideal structure is shown in the picture below with formula_25 the receive filter's frequency response :

Multiplying by a cosine (or a sine) and by a low-pass filter it is possible to extract the component in phase (or in quadrature). Then there is only an ASK demodulator and the two flows of data are merged back.

In practice, there is an unknown phase delay between the transmitter and receiver that must be compensated by "synchronization" of the receiver's local oscillator (i.e. the sine and cosine functions in the above figure). In mobile applications, there will often be an offset in the relative "frequency" as well, due to the possible presence of a Doppler shift proportional to the relative velocity of the transmitter and receiver. Both the phase and frequency variations introduced by the channel must be compensated by properly tuning the sine and cosine components, which requires a "phase reference", and is typically accomplished using a Phase-Locked Loop (PLL).

In any application, the low-pass filter and the receive formula_25 filter will be implemented as a single combined filter. Here they are shown as separate just to be clearer.

The following definitions are needed in determining error rates:

formula_37 is related to the complementary Gaussian error function by:
formula_38, which is the probability that "x" will be under the tail of the Gaussian PDF towards positive infinity.

The error rates quoted here are those in additive white Gaussian noise (AWGN).

Where coordinates for constellation points are given in this article, note that they represent a "non-normalised" constellation. That is, if a particular mean average energy were required (e.g. unit average energy), the constellation would need to be linearly scaled.

Rectangular QAM constellations are, in general, sub-optimal in the sense that they do not maximally space the constellation points for a given energy. However, they have the considerable advantage that they may be easily transmitted as two pulse amplitude modulation (PAM) signals on quadrature carriers, and can be easily demodulated. The non-square constellations, dealt with below, achieve marginally better bit-error rate (BER) but are harder to modulate and demodulate.

The first rectangular QAM constellation usually encountered is 16-QAM, the constellation diagram for which is shown here. A Gray coded bit-assignment is also given. The reason that 16-QAM is usually the first is that a brief consideration reveals that 2-QAM and 4-QAM are in fact binary phase-shift keying (BPSK) and quadrature phase-shift keying (QPSK), respectively. Also, the error-rate performance of 8-QAM is close to that of 16-QAM (only about 0.5 dB better), but its data rate is only three-quarters that of 16-QAM.

Expressions for the symbol-error rate of rectangular QAM are not hard to derive but yield rather unpleasant expressions. For M-ary square QAM exact expressions are available. They are most easily expressed in a "per carrier" sense:

so

The bit-error rate depends on the bit to symbol mapping, but for formula_41 and a Gray-coded assignment—so that we can assume each symbol error causes only one bit error—the bit-error rate is approximately

Since the carriers are independent, the overall bit error rate is the same as the per-carrier error rate, just like BPSK and QPSK:

The exact and general closed-form expression of the bit error rate for rectangular QAM of size formula_44, where formula_45 is the size of the in-phase/quadrature PAM forming the QAM, respectively, was derived analytically for the AWGN channel: 
where

with formula_48. M-ary square QAM is a special case with formula_49.

For odd formula_50, such as 8-QAM (formula_51) it is harder to obtain symbol-error rates, but a tight upper bound is:

Two rectangular 8-QAM constellations are shown below without bit assignments. These both have the same minimum distance between symbol points, and thus the same symbol-error rate (to a first approximation).

The exact bit-error rate, formula_32 will depend on the bit-assignment.

Note that both of these constellations are seldom used in practice, as the non-rectangular version of 8-QAM is optimal.

It is the nature of QAM that most orders of constellations can be constructed in many different ways and it is neither possible nor instructive to cover them all here. This article instead presents two, lower-order constellations.

Two diagrams of circular QAM constellation are shown, for 8-QAM and 16-QAM. The circular 8-QAM constellation is known to be the optimal 8-QAM constellation in the sense of requiring the least mean power for a given minimum Euclidean distance. The 16-QAM constellation is suboptimal although the optimal one may be constructed along the same lines as the 8-QAM constellation. The circular constellation highlights the relationship between QAM and PSK. Other orders of constellation may be constructed along similar (or very different) lines. It is consequently hard to establish expressions for the error rates of non-rectangular QAM since it necessarily depends on the constellation. Nevertheless, an obvious upper bound to the rate is related to the minimum Euclidean distance of the constellation (the shortest straight-line distance between two points):

Again, the bit-error rate will depend on the assignment of bits to symbols.

Although, in general, there is a non-rectangular constellation that is optimal for a particular formula_27, they are not often used since the rectangular QAMs are much easier to modulate and demodulate.

Hierarchical QAM is a form of hierarchical modulation. For example, hierarchical QAM is used in DVB, where the constellation points are grouped into a high-priority QPSK stream and a low-priority 16-QAM stream. The irregular distribution of constellation points improves the reception probability of the high-priority stream in low SNR conditions, at the expense of higher SNR requirements for the low-priority stream.

The mutual information of QAM can be evaluated in additive Gaussian noise by numerical integration of its definition. The curves of mutual information saturate to the number of bits carried by each symbol in the limit of infinite signal to noise ratio formula_56. On the contrary, in the limit of small signal to noise ratios the mutual information approaches the AWGN channel capacity, which is the supremum among all possible choices of symbol statistical distributions.

The mutual information of QAM is generally closer to the AWGN channel capacity than PSK modulation formats.

In moving to a higher order QAM constellation (higher data rate and mode) in hostile RF/microwave QAM application environments, such as in broadcasting or telecommunications, multipath interference typically increases. There is a spreading of the spots in the constellation, decreasing the separation between adjacent states, making it difficult for the receiver to decode the signal appropriately. In other words, there is reduced noise immunity. There are several test parameter measurements which help determine an optimal QAM mode for a specific operating environment. The following three are most significant:


5. Jonqyin (Russell) Sun "Linear diversity analysis for QAM in Rician fading channels", IEEE WOCC 2014

The notation used here has mainly (but not exclusively) been taken from



</doc>
<doc id="25317" url="https://en.wikipedia.org/wiki?curid=25317" title="QAM (disambiguation)">
QAM (disambiguation)

QAM may refer to:



</doc>
<doc id="25319" url="https://en.wikipedia.org/wiki?curid=25319" title="Quetzalcoatlus">
Quetzalcoatlus

Quetzalcoatlus northropi is a pterosaur known from the Late Cretaceous of North America (Maastrichtian stage) and one of the largest-known flying animals of all time. It is a member of the family Azhdarchidae, a family of advanced toothless pterosaurs with unusually long, stiffened necks. Its name comes from the Mesoamerican feathered serpent god, Quetzalcoatl.

When it was first named as a new species in 1975, scientists estimated that the largest "Quetzalcoatlus" fossils came from an individual with a wingspan as large as , choosing the middle of three extrapolations from the proportions of other pterosaurs that gave an estimate of 11 m, 15.5 m, and 21 m, respectively (36 ft, 50.85 ft, 68.9 ft). In 1981, further advanced studies lowered these estimates to .

More recent estimates based on greater knowledge of azhdarchid proportions place its wingspan at . Remains found in Texas in 1971 indicate that this reptile had a minimum wingspan of about . Generalized height in a bipedal stance, based on its wingspan, would have been at least high at the shoulder.

Weight estimates for giant azhdarchids are extremely problematic because no existing species share a similar size or body plan, and in consequence, published results vary widely. Generalized weight, based on some studies that have historically found extremely low weight estimates for "Quetzalcoatlus", was as low as for a individual. A majority of estimates published since the 2000s have been substantially higher, around .

Skull material (from smaller specimens, possibly a related species) shows that "Quetzalcoatlus" had a very sharp and pointed beak. That is contrary to some earlier reconstructions that showed a blunter snout, based on the inadvertent inclusion of jaw material from another pterosaur species, possibly a tapejarid or a form related to "Tupuxuara". A skull crest was also present but its exact form and size are still unknown.

The first "Quetzalcoatlus" fossils were discovered in Texas, United States, from the Maastrichtian Javelina Formation at Big Bend National Park (dated to around 68 million years ago) in 1971 by Douglas A. Lawson, a geology graduate student from the Jackson School of Geosciences at the University of Texas at Austin. The specimen consisted of a partial wing (in pterosaurs composed of the forearms and elongated fourth finger), from an individual later estimated at over in wingspan.

Lawson discovered a second site of the same age, about from the first, where between 1972 and 1974 he and Professor Wann Langston Jr. of the Texas Memorial Museum unearthed three fragmentary skeletons of much smaller individuals. Lawson in 1975 announced the find in an article in "Science". That same year, in a subsequent letter to the same journal, he made the original large specimen, TMM 41450-3, the holotype of a new genus and species, Quetzalcoatlus northropi. The genus name refers to the Aztec feathered serpent god, Quetzalcoatl. The specific name honors John Knudsen Northrop, the founder of Northrop, who was interested in large tailless flying wing aircraft designs resembling "Quetzalcoatlus".

At first it was assumed that the smaller specimens were juvenile or subadult forms of the larger type. Later, when more remains were found, it was realized they could have been a separate species. This possible second species from Texas was provisionally referred to as a "Quetzalcoatlus" sp. by Alexander Kellner and Langston in 1996, indicating that its status was too uncertain to give it a full new species name. The smaller specimens are more complete than the "Q. northropi" holotype, and include four partial skulls, though they are much less massive, with an estimated wingspan of .

The holotype specimen of "Q. northropi" has yet to be properly described and diagnosed, and the current status of the genus "Quetzalcoatlus" has been identified as problematic. Mark Witton and colleagues (2010) noted that the type species of the genus—the fragmentary wing bones comprising "Q. northropi"—represent elements which are typically considered undiagnostic to generic or specific level, and that this complicates interpretations of azhdarchid taxonomy. For instance, Witton et al. (2010) suggested that the "Q. northropi" type material is of generalised enough morphology to be near identical to that of other giant azhdarchids, such as the overlapping elements of the contemporary Romanian giant azhdarchid" Hatzegopteryx". This being the case, and assuming "Q. northropi" can be distinguished from other pterosaurs (i.e., if it is not a "nomen dubium"), perhaps "Hatzegopteryx" should be regarded as a European occurrence of "Quetzalcoatlus". However, Witton "et al." also noted that the skull material of "Hatzegopteryx" and "Q." sp. differ enough that they cannot be regarded as the same animal, but that the significance of this cannot be ascertained given uncertainty over the relationships of "Quetzalcoatlus" specimens. These issues can only be resolved by "Q. northropi" being demonstrated as a valid taxon and its relationships with "Q". sp. being investigated. An additional complication to these discussions are the likelihood that huge pterosaurs such as "Q. northropi" could have made long, transcontinental flights, suggesting that locations as disparate as North America and Europe could have shared giant azhdarchid species.

An azhdarchid neck vertebra, discovered in 2002 from the Maastrichtian age Hell Creek Formation, may also belong to "Quetzalcoatlus". The specimen (BMR P2002.2) was recovered accidentally when it was included in a field jacket prepared to transport part of a "Tyrannosaurus" specimen. Despite this association with the remains of a large carnivorous dinosaur, the vertebra shows no evidence that it was chewed on by the dinosaur. The bone came from an individual azhdarchid pterosaur estimated to have had a wingspan of .

Below is a cladogram showing the phylogenetic placement of "Quetzalcoatlus" within Neoazhdarchia from Andres and Myers (2013).

"Quetzalcoatlus" was abundant in Texas during the Lancian in a fauna dominated by "Alamosaurus". The "Alamosaurus"-"Quetzalcoatlus" association probably represents semi-arid inland plains. "Quetzalcoatlus" had precursors in North America and its apparent rise to widespreadness may represent the expansion of its preferred habitat rather than an immigration event, as some experts have suggested.

There have been a number of different ideas proposed about the lifestyle of "Quetzalcoatlus". Because the area of the fossil site was four hundred kilometers removed from the coastline and there were no indications of large rivers or deep lakes nearby at the end of the Cretaceous, Lawson in 1975 rejected a fish-eating lifestyle, instead suggesting that "Quetzalcoatlus" scavenged like the marabou stork (which will scavenge, but is more of a terrestrial predator of small animals), but then on the carcasses of titanosaur sauropods such as "Alamosaurus". Lawson had found the remains of the giant pterosaur while searching for the bones of this dinosaur, which formed an important part of its ecosystem.

In 1996, Lehman and Langston rejected the scavenging hypothesis, pointing out that the lower jaw bent so strongly downwards that even when it closed completely a gap of over five centimeters remained between it and the upper jaw, very different from the hooked beaks of specialized scavenging birds. They suggested that with its long neck vertebrae and long toothless jaws "Quetzalcoatlus" fed like modern-day skimmers, catching fish during flight while cleaving the waves with its beak. While this skim-feeding view became widely accepted, it was not subjected to scientific research until 2007 when a study showed that for such large pterosaurs it was not a viable method because the energy costs would be too high due to excessive drag. In 2008 pterosaur workers Mark Witton and Darren Naish published an examination of possible feeding habits and ecology of azhdarchids. Witton and Naish noted that most azhdarchid remains are found in inland deposits far from seas or other large bodies of water required for skimming. Additionally, the beak, jaw, and neck anatomy are unlike those of any known skimming animal. Rather, they concluded that azhdarchids were more likely terrestrial stalkers, similar to modern storks, and probably hunted small vertebrates on land or in small streams. Though "Quetzalcoatlus", like other pterosaurs, was a quadruped when on the ground, "Quetzalcoatlus" and other azhdarchids have fore and hind limb proportions more similar to modern running ungulate mammals than to their smaller cousins, implying that they were uniquely suited to a terrestrial lifestyle.

The nature of flight in "Quetzalcoatlus" and other giant azhdarchids was poorly understood until serious biomechanical studies were conducted in the 21st century. One early (1984) experiment by Paul MacCready used practical aerodynamics to test the flight of "Quetzalcoatlus". MacCready constructed a model flying machine or ornithopter with a simple computer functioning as an autopilot. The model successfully flew with a combination of soaring and wing flapping; the model was based on a then-current weight estimate of around , far lower than more modern estimates of over . The method of flight in these pterosaurs depends largely on weight, which has been controversial, and widely differing masses have been favored by different scientists. Some researchers have suggested that these animals employed slow, soaring flight, while others have concluded that their flight was fast and dynamic. In 2010, Donald Henderson argued that the mass of "Q. northropi" had been underestimated, even the highest estimates, and that it was too massive to have achieved powered flight. He estimated it in his 2010 paper as . Henderson argued that it may have been flightless.

Other flight capability estimates have disagreed with Henderson's research, suggesting instead an animal superbly adapted to long-range, extended flight. In 2010, Mike Habib, a professor of biomechanics at Chatham University, and Mark Witton, a British paleontologist, undertook further investigation into the claims of flightlessness in large pterosaurs. After factoring wingspan, body weight, and aerodynamics, computer modelling led the two researchers to conclude that "Q. northropi" was capable of flight up to for 7 to 10 days at altitudes of . Habib further suggested a maximum flight range of for "Q. northropi". Henderson's work was also further criticized by Witton and Habib in another study, who pointed out that although Henderson used excellent mass estimations, they were based on outdated pterosaur models, which caused Henderson's mass estimations to be more than double what Habib used in his estimations, and that anatomical study of "Q. northropi" and other large pterosaur forelimbs show a higher degree of robustness than would be expected if they were purely quadrupedal. This study proposed that large pterosaurs most likely utilized a short burst of powered flight in order to then transition to thermal soaring.

"Quetzalcoatlus" has been featured in documentaries, both in cinemas and on television, since the 1980s. The Smithsonian project to build a working model of "Q. northropi" was the subject of the 1986 IMAX documentary "On the Wing", shown at the National Air and Space Museum in Washington, D.C.. In the BBC's 1999 documentary "Walking with Dinosaurs", "Quetzalcoatlus" was erroneously shown as toothed piscivore with an inaccurate stub-like crest. In 2009, Dangerous Ltd.'s "Clash of the Dinosaurs" depicted it as having a number of fictional traits created by the producers to heighten entertainment value, including the ability to use ultraviolet vision to locate dinosaur urine when hunting in the air. "Quetzalcoatlus" was also erroneously depicted in the 2011 documentary "March of the Dinosaurs" as a clawless, bipedal scavenger. Conversely, in the 2009 series "Animal Armageddon" and the 2010 BBC film "Flying Monsters 3D", "Quetzalcoatlus" was accurately portrayed with pycnofibres. However, in both "Flying Monsters 3D" and the 2001 documentary "When Dinosaurs Roamed America", "Quetzalcoatlus" was shown as being a vulture-like scavenger. In the "Return to Jurassic Park" bonus feature of the 2011 Blu-ray release of the "Jurassic Park" film series, John R. Horner describes "Quetzalcoatlus" as the pterosaur that most accurately represented and matched the size of the pterosaurs featured in the films.

In June 2010, several life-sized models of "Q. northropi" were put on display on London's South Bank as the centerpiece exhibit for the Royal Society's 350th-anniversary exhibition. The models, which included both flying and standing individuals with wingspans of , were intended to help build public interest in science. The models were created by scientists from the University of Portsmouth, including David Martill, Bob Loveridge, and Mark Witton, and engineers Bob and Jack Rushton from Griffon Hoverwork. The display featured the most accurate pterosaur models constructed at the time; these models took into account the latest evidence based on skeletal and trace fossils from related pterosaurs.

In 1985, the US Defense Advanced Research Projects Agency (DARPA) and AeroVironment used "Quetzalcoatlus northropi" as the basis for an experimental ornithopter unmanned aerial vehicle (UAV). They produced a half-scale model weighing , with a wingspan of . Coincidentally, Douglas A. Lawson, who discovered "Q. northropi" in Texas in 1971, named it after John "Jack" Northrop, a developer of tailless flying wing aircraft in the 1940s. The replica of "Q. northropi" incorporates a "flight control system/autopilot which processes pilot commands and sensor inputs, implements several feedback loops, and delivers command signals to its various servo-actuators". It is on exhibit at the National Air and Space Museum.




</doc>
<doc id="25320" url="https://en.wikipedia.org/wiki?curid=25320" title="Quedlinburg">
Quedlinburg

Quedlinburg () is a town situated just north of the Harz mountains, in the district of Harz in the west of Saxony-Anhalt, Germany. In 1994, the castle, church and old town were added to the UNESCO World Heritage List.

Quedlinburg has a population of more than 24,000. The town was the capital of the district of Quedlinburg until 2007, when the district was dissolved. Several locations in the town are designated stops along a scenic holiday route, the Romanesque Road.

The town of Quedlinburg is known to have existed since at least the early 9th century, when there was a settlement known as "Gross Orden" on the eastern bank of the River Bode. It was first mentioned as a town in 922 as part of a donation by King Henry the Fowler ("Heinrich der Vogler"). The records of this donation were held by the abbey of Corvey.

According to legend, Henry had been offered the German crown at Quedlinburg in 919 by Franconian nobles, giving rise to the town being called the "cradle of the German Reich".

After Henry's death in 936, his widow Saint Matilda founded a religious community for women ("Frauenstift") on the castle hill, where daughters of the higher nobility were educated. The main task of this collegiate foundation, Quedlinburg Abbey, was to pray for the memory of King Henry and the rulers who came after him. The "Annals of Quedlinburg" were also compiled there. The first abbess was Matilda, a granddaughter of King Henry and St. Matilda.

The Quedlinburg castle complex, founded by King Henry I and built up by Emperor Otto I in 936, was an imperial "Pfalz" of the Saxon emperors. The "Pfalz", including the male convent, was in the valley, where today the Roman Catholic Church of "St. Wiperti" is situated, while the women's convent was located on the castle hill.

In 973, shortly before the death of Emperor Otto I, a "Reichstag" (Imperial Convention) was held at the imperial court in which Mieszko, duke of Polans, and Boleslav, duke of Bohemia, as well as numerous other nobles from as far away as Byzantium and Bulgaria, gathered to pay homage to the emperor. On the occasion, Otto the Great introduced his new daughter-in-law Theophanu, a Byzantine princess whose marriage to Otto II brought hope for recognition and continued peace between the rulers of the Eastern and Western empires.

In 994, Otto III granted the right of market, tax, and coining, and established the first market place to the north of the castle hill.

The town became a member of the Hanseatic League in 1426. Quedlinburg Abbey frequently disputed the independence of the town, which sought the aid of the Bishopric of Halberstadt. In 1477, Abbess Hedwig, aided by her brothers Ernest and Albert, broke the resistance of the town and expelled the bishop's forces. Quedlinburg was forced to leave the Hanseatic League and was subsequently protected by the Electorate of Saxony. Both town and abbey converted to Lutheranism in 1539 during the Protestant Reformation.

In 1697, Elector Frederick Augustus I of Saxony sold his rights to Quedlinburg to Elector Frederick III of Brandenburg for 240,000 thalers. Quedlinburg Abbey contested Brandenburg-Prussia's claims throughout the 18th century, however. The abbey was secularized in 1802 during the German Mediatisation, and Quedlinburg passed to the Kingdom of Prussia as part of the Principality of Quedlinburg. Part of the Napoleonic Kingdom of Westphalia from 1807–13, it was included within the new Prussian Province of Saxony in 1815. In all this time, ladies ruled Quedlinburg as abbesses without "taking the veil"; they were free to marry. The last of these ladies was a Swedish princess, an early fighter for women's rights, Sofia Albertina.

During the Nazi regime, the memory of Henry I became a sort of cult, as Heinrich Himmler saw himself as the reincarnation of the "most German of all German" rulers. The collegiate church and castle were to be turned into a shrine for Nazi Germany. The Nazi Party tried to create a new religion. The cathedral was closed from 1938 and during the war. The local crematory was kept busy burning the victims of the Langenstein-Zwieberge concentration camp. Georg Ay was local party chief from 1931 until the end of the war. Liberation in 1945 brought back the Protestant bishop and the church bells, and the Nazi-style eagle was taken down from the tower.

During the last months of World War II, the United States military had occupied Quedlinburg. In the 1980s, upon the death of one of the US military men, the theft of medieval art from Quedlinburg came to light.

Quedlinburg was administered within Bezirk Halle while part of the Communist East Germany from 1949 to 1990. It became part of the state of Saxony-Anhalt upon German reunification in 1990.

During Quedlinburg's Communist era, restoration specialists from Poland were called in during the 1980s to carry out repairs on the old architecture. Today, Quedlinburg is a center of restoration of "Fachwerk" houses.

The town is located north of the Harz mountains, about 123 m above NHN. The nearest mountains reach 181 m above NHN. The largest part of the town is located in the western part of the Bode river valley. This river comes from the Harz mountains and flows into the river Saale, a tributary of the river Elbe. The municipal area of Quedlinburg is . Before the incorporation of the two (previously independent) municipalities of Gernrode and Bad Suderode in January 2014 it was only .

Quedlinburg has a oceanic climate (Cfb) resulting from prevailing westerlies, blowing from the high-pressure area in the central Atlantic towards Scandinavia. Snowfall occurs almost every winter. January and February are the coldest months of the year, with an average temperature of 0.5 °C and 1.5 °C. July and August are the hottest months, with an average temperature of 17 °C (63 °F) and 18 °C (64 °F). The average annual precipitation is close to 438 mm with rain occurring usually from May to September. This precipitation is one of the lowest in Germany, which has an annual average close to 700 mm. In August 2010, Quedlinburg was the driest place in Germany, with only 72,4 l/m.

The mayor is Frank Ruch (CDU).

Quedlinburg is twinned with:

In the centre of the town, a wide selection of half-timbered buildings from at least five different centuries are to be found (including a 14th-century structure, one of Germany's oldest), while around the outer fringes of the old town are examples of "Jugendstil" buildings, dating from the late 19th and early 20th centuries.

Since December 1994, the old town of Quedlinburg and the castle mount with the "Stiftskirche" (collegiate church) are listed as one of UNESCO's World Heritage Sites. Quedlinburg is one of the best-preserved medieval and Renaissance towns in Europe, having escaped major damage in World War II.

In 2006, the Selke valley branch of the Harz Narrow Gauge Railways was extended to Quedlinburg from Gernrode, giving access to the historic steam narrow gauge railway, Alexisbad and the high Harz plateau.

The castle and "Stifstkirche St. Servatius" still dominate the town like in the early Middle Ages. The church is a prime example of German Romanesque style. The treasure of the church containing ancient Christian religious artifacts and books, was stolen by an American soldier but brought back to Quedlinburg in 1993 and is again on display here.

The former "Stiftskirche St. Wiperti" was established in 936 when the "Kanonikerstift St. Wigpertus" (of male canons) was moved from the castle hill to make way for what became Quedlinburg Abbey. The church was built at the location of the first, Ottonian, Royal palace at Quedlinburg. Around 1020, a three-aisled crypt was added to the basilica. The crypt, which survived all later alterations to the church, today is also a designated stop on the Romanesque Road.

The nearest airports to Quedlinburg are Hannover, northwest, and Leipzig/Halle Airport, southeast. Much closer, but only served by a few airlines, is Magdeburg-Cochstedt. An airfield is located at Ballenstedt-Assmussstedt for general aviation.

Regional trains opeated by Deutsche Bahn and the private Transdev company run on the standard-gauge Magdeburg–Thale line connecting Quedlinburg station with Magdeburg, Thale, and Halberstadt.

In 2006, the Selke Valley branch of the Harz Narrow Gauge Railways was extended into Quedlinburg from Gernrode, giving access via the historic steam-operated narrow-gauge railway to Alexisbad and the High Harz plateau.

Quedlinburg is connected by regional buses to the surrounding villages and small towns. Additionally, there are long distance buses to Berlin.







</doc>
<doc id="25321" url="https://en.wikipedia.org/wiki?curid=25321" title="Quantization">
Quantization

Quantization is the process of constraining an input from a continuous or otherwise large set of values (such as the real numbers) to a discrete set (such as the integers). The terms "quantization" and "discretization" are often denotatively synonymous but not always connotatively interchangeable.





</doc>
<doc id="25322" url="https://en.wikipedia.org/wiki?curid=25322" title="Quantum theory">
Quantum theory

Quantum theory may refer to:




</doc>
<doc id="25323" url="https://en.wikipedia.org/wiki?curid=25323" title="QRP operation">
QRP operation

In amateur radio, QRP operation refers to transmitting at reduced power while attempting to maximize one's effective range. The term QRP derives from the standard Q code used in radio communications, where "QRP" and "QRP?" are used to request, "Reduce power", and ask "Should I reduce power?" respectively. The opposite of QRP is QRO, or high-power operation.

Most amateur transceivers are capable of transmitting approximately 100 watts, but in some parts of the world, such as the U.S., amateurs can transmit up to 1,500 watts. QRP enthusiasts contend that this is not always necessary, and doing so wastes power, increases the likelihood of causing interference to nearby televisions, radios, and telephones and, for United States' amateurs, is incompatible with FCC Part 97 rule, which states that one must use "the minimum power necessary to carry out the desired communications".

The current record for a QRP connection is 1 µW for 2640 kilometers (1650 miles) on 10-meter band (28-29.7 MHz).

There is not complete agreement on what constitutes QRP power. Most amateur organizations agree that for CW, AM, FM, and data modes, the transmitter output power should be 5 watts (or less). The maximum output power for SSB (single sideband) is not always agreed upon. Some believe that the power should be no more than 10 watts peak envelope power (PEP), while others strongly hold that the power limit should be 5 watts. QRPers are known to use even less than five watts, sometimes operating with as little as 100 milliwatts or even less. Extremely low power—1 watt and below—is often referred to by hobbyists as QRPp.

Communicating using QRP can be difficult since the QRPer must face the same challenges of radio propagation faced by amateurs using higher power levels, but with the inherent disadvantages associated with having a weaker signal on the receiving end, all other things being equal. QRP aficionados try to make up for this through more efficient antenna systems and enhanced operating skills.

QRP is especially popular with CW operators and those using the newer digital modes. PSK31 is a highly efficient, narrow-band mode that is very suitable to QRP operation.

QRSS refers to transmitting extremely slowly. Some extreme QRP enthusiasts use QRSS to compensate for the decreased signal-to-noise ratio involved in QRP operation. QRSS derives from the standard Q code used in radio communications, where "QRS?" asks "Shall I send more slowly?" and "QRS" requests "Send more slowly".

Rather than directly listening to such slow transmissions, many QRSS enthusiasts record the transmission for later analysis, later decoding "by ear" while playing it back at much faster rates (time compression), or decoding "by eye" on the waterfall display of a spectrum analyzer.

QRSS enthusiasts typically use some form of Morse code, except much slower—rather than a typical second "dit" time, QRSS transmissions may use a full second for the "dit" time, or in extreme cases, a full minute for a single "dit" time.

A few people apply QRSS techniques to other narrow-band communication codes or protocols, such as the "Slowfeld" variant of Hellschreiber, slow-scan television, MT63, etc.

Many of the larger, more powerful commercial transceivers permit the operator to lower their output level to QRP levels. Commercial transceivers specially designed to operate at or near QRP power levels have been commercially available since the late 1960s. In 1969, American manufacturer, Ten-Tec, produced the Powermite-1. This radio was one of Ten-Tec's first assembled transceivers. (The MR-1 was available, and it was essentially the same radio, albeit in kit form.) This radio featured modular construction (all stages of the transceiver were on individual circuit boards): the transmitter was capable of about one or two watts of RF, and the receiver was a direct-conversion unit, similar to that found in the Heathkit HW-7 and HW-8 lines. Many amateurs became quite adept at QRP'ing through their use of these early, trend-setting radios . As QRP has become more popular in recent years , radio manufacturers have introduced radios specifically intended for the QRP enthusiast. Popular US models include Elecraft KX3, K2 and K1, the Yaesu FT-817, the Icom IC-703, and the 516 Argonaut V and the new 539 Argonaut VI from TenTec. Another popular source is Hendricks QRP Kits, which offers a variety of popular kits. Enthusiasts operate QRP radios on the HF bands in portable modes, usually carrying the radios in backpacks, with whip antennas. Some QRPers prefer to construct their equipment from kits or homebrew it from scratch. Many popular designs are based on the NE612 mixer IC, i.e. the K1, K2, ATS series and the Softrock SDR.

There are specific operating awards, contests, clubs, and conventions devoted to QRP enthusiasts.

In the United States, the November Sweepstakes, June and September VHF QSO Parties, January VHF Sweepstakes, and the ARRL International DX Contest, as well as many major international contests have designated special QRP categories. For example, during the annual ARRL's Field Day contest, making a QSO (ham-to-ham contact) using "QRP battery power" is worth five times as many points as a contact made by conventional means.

The QRP ARCI club sponsors 12 contests during the year specifically for QRP operators. QRP-ARCI Contests

Typical awards include the QRP ARCI club's "thousand-miles-per-watt" award, available to anyone presenting evidence of a qualifying contact. QRP ARCI also offers special awards for achieving the ARRL's Worked All States, Worked All Continents, and DX Century Club awards under QRP conditions. Other QRP clubs also offer similar versions of these awards, as well as general QRP operating achievement awards.






</doc>
<doc id="25327" url="https://en.wikipedia.org/wiki?curid=25327" title="QCD (disambiguation)">
QCD (disambiguation)

The initialism QCD may refer to:


</doc>
<doc id="25328" url="https://en.wikipedia.org/wiki?curid=25328" title="Quicksilver">
Quicksilver

Quicksilver may refer to:













</doc>
<doc id="25330" url="https://en.wikipedia.org/wiki?curid=25330" title="Quartet">
Quartet

In music, a quartet or quartette (, , , , ) is an ensemble of four singers or instrumental performers; or a musical composition for four voices or instruments.

In Classical music, the most important combination of four instruments in chamber music is the string quartet. String quartets most often consist of two violins, a viola, and a cello. The particular choice and number of instruments derives from the registers of the human voice: soprano, alto, tenor and bass. In the string quartet, two violins play the soprano and alto vocal registers, the viola plays the tenor register and the cello plays the bass register.

Composers of notable string quartets include Joseph Haydn (68 compositions), Wolfgang Amadeus Mozart (23), Ludwig van Beethoven (17), Felix Mendelssohn (6), Franz Schubert (15), Johannes Brahms (3), Antonín Dvořák (14), Alexander Borodin (2), Béla Bartók (6), and Dmitri Shostakovich (15). The Italian composer Luigi Boccherini (1743–1805), wrote more than 100 string quartets.

Less often, string quartets are written for other combinations of the standard string ensemble. These include quartets for one violin, two violas, and one cello, notably by Carl Stamitz (6 compositions) and others; and for one violin, one viola, and two cellos, by Johann Georg Albrechtsberger and others.

Another common standard classical quartet is the piano quartet, consisting of violin, viola, cello, and piano. Romantic composers Beethoven, Brahms, and Mendelssohn each wrote three important compositions in this form, and Mozart, Dvořák, and Gabriel Fauré each wrote two.

Wind quartets are scored either the same as a string quartet with the wind instrument replacing the first violin (i.e. scored for wind, violin, viola and cello) or are groups of four wind instruments. Among the latter, the SATB format woodwind quartet of flute, oboe, clarinet, and bassoon is relatively common.

An example of a wind quartet featuring four of the same types of wind instruments is the saxophone quartet, consisting of soprano saxophone, alto saxophone, tenor saxophone and baritone saxophone or (SATB). Often a second alto may be substituted for the soprano part (AATB) or a bass saxophone may be substituted for the baritone.

Compositions for four singers have been written for quartets a cappella; accompanied by instruments, such as a piano; and accompanied by larger vocal forces, such as a choir. Brahms and Schubert wrote numerous pieces for four voices that were once popular in private salons, although they are seldom performed today. Vocal quartets also feature within larger classical compositions, such as opera, choral works, and symphonic compositions. The final movement of Beethoven's Ninth Symphony and the Verdi Requiem are two examples of renowned concert works that include vocal quartets.

Typically, a vocal quartet is composed of:

The baroque quartet is a form of music composition similar to the trio sonata, but with four music parts performed by three solo melodic instruments and basso continuo. The solo instruments could be strings or wind instruments.

Examples of baroque quartets are Telemann's Paris quartets.

Quartets are popular in jazz and jazz fusion music. Jazz quartet ensembles are often composed of a horn, classically clarinet (or saxophone, trumpet, etc.), a chordal instrument (e.g., electric guitar, piano, Hammond organ, etc.), a bass instrument (e.g., double bass, tuba or bass guitar) and a drum kit. This configuration is sometimes modified by using a second horn replacing the chordal instrument, such as a trumpet and saxophone with string bass and drum kit, or by using two chordal instruments (e.g., piano and electric guitar).

In 20th century Western popular music, the term "vocal quartet" usually refers to ensembles of four singers of the same gender. This is particularly common for barbershop quartets and Gospel quartets.

Some well-known female US vocal quartets include The Carter Sisters; The Forester Sisters; The Chiffons; The Chordettes; The Lennon Sisters; and En Vogue. Some well-known male US vocal quartets include The Oak Ridge Boys; The Statler Brothers; The Ames Brothers; The Chi-Lites; Crosby Stills Nash & Young; The Dixie Hummingbirds; The Four Aces; Four Freshmen; The Four Seasons; The Four Tops; The Cathedral Quartet; Ernie Haase and Signature Sound; The Golden Gate Quartet; The Hilltoppers; The Jordanaires; and Mills Brothers. The only known U.S. drag quartet is The Kinsey Sicks. Some mixed-gender vocal quartets include The Pied Pipers; The Mamas & the Papas; The Merry Macs; and The Weavers.

The quartet lineup also is very common in pop and rock music. A standard quartet formation in pop and rock music is an ensemble consisting of two electric guitars, a bass guitar, and a drum kit. This configuration is sometimes modified by using a keyboard instrument (e.g., organ, piano, synthesizer) or a soloing instrument (e.g., saxophone) in place of the second electric guitar.

A Russian folk-instrument quartet commonly consists of a bayan, a prima balalaika, a prima or alto domra, and a contrabass balalaika. Configurations without a bayan include a prima domra, a prima balalaika, an alto domra, and a bass balalaika; or two prima domras, a prima balalaika, and a bass balalaika. 



</doc>
<doc id="25336" url="https://en.wikipedia.org/wiki?curid=25336" title="Quantum entanglement">
Quantum entanglement

Quantum entanglement is a physical phenomenon which occurs when pairs or groups of particles are generated, interact, or share spatial proximity in ways such that the quantum state of each particle cannot be described independently of the state of the other(s), even when the particles are separated by a large distance—instead, a quantum state must be described for the system as a whole.

Measurements of physical properties such as position, momentum, spin, and polarization, performed on entangled particles are found to be correlated. For example, if a pair of particles is generated in such a way that their total spin is known to be zero, and one particle is found to have clockwise spin on a certain axis, the spin of the other particle, measured on the same axis, will be found to be counterclockwise, as is to be expected due to their entanglement. However, this behavior gives rise to seemingly paradoxical effects: any measurement of a property of a particle performs an irreversible collapse on that particle and will change the original quantum state. In the case of entangled particles, such a measurement will be on the entangled system as a whole. Given that the statistics of these measurements cannot be replicated by models in which each particle has its own state independent of the other, it appears that one particle of an entangled pair "knows" what measurement has been performed on the other, and with what outcome, even though there is no known means for such information to be communicated between the particles, which at the time of measurement may be separated by arbitrarily large distances.

Such phenomena were the subject of a 1935 paper by Albert Einstein, Boris Podolsky, and Nathan Rosen, and several papers by Erwin Schrödinger shortly thereafter, describing what came to be known as the EPR paradox. Einstein and others considered such behavior to be impossible, as it violated the local realist view of causality (Einstein referring to it as "spooky action at a distance") and argued that the accepted formulation of quantum mechanics must therefore be incomplete. Later, however, the counterintuitive predictions of quantum mechanics were verified experimentally in tests where the polarization or spin of entangled particles were measured at separate locations, statistically violating Bell's inequality, demonstrating that the classical conception of "local realism" cannot be correct.

In earlier tests it couldn't be absolutely ruled out that the test result at one point (or which test was being performed) could have been subtly transmitted to the remote point, affecting the outcome at the second location. However so-called "loophole-free" Bell tests have been performed in which the locations were separated such that communications at the speed of light would have taken longer—in one case 10,000 times longer—than the interval between the measurements. Since faster-than-light signaling is impossible according to the special theory of relativity, any doubts about entanglement due to such a loophole have thereby been quashed.

According to "some" interpretations of quantum mechanics, the effect of one measurement occurs instantly. Other interpretations which don't recognize wavefunction collapse, dispute that there is any "effect" at all. After all, if the separation between two events is spacelike, then observers in different inertial frames will disagree about the order of events. Joe will see that the detection at point A occurred first, and could not have been caused by the measurement at point B, while Mary (moving at a different velocity) will be certain that the measurement at point B occurred first and could not have been caused by the A measurement. Of course both Joe and Mary are correct: there is no demonstrable cause and effect. However all interpretations agree that entanglement produces "correlation" between the measurements, and that the mutual information between the entangled particles can be exploited, but that any "transmission" of information at faster-than-light speeds is impossible.

In November 2016, researchers performed Bell test experiments in which further "loopholes" were closed.

Entanglement is considered fundamental to quantum mechanics, even though it wasn't recognized in the beginning. Quantum entanglement has been demonstrated experimentally with photons, neutrinos, electrons, molecules as large as buckyballs, and even small diamonds. The utilization of entanglement in communication and computation is a very active area of research.

The counterintuitive predictions of quantum mechanics about strongly correlated systems were first discussed by Albert Einstein in 1935, in a joint paper with Boris Podolsky and Nathan Rosen. 
In this study, the three formulated the EPR paradox, a thought experiment that attempted to show that quantum mechanical theory was incomplete. They wrote: "We are thus forced to conclude that the quantum-mechanical description of physical reality given by wave functions is not complete."

However, the three scientists did not coin the word "entanglement", nor did they generalize the special properties of the state they considered. Following the EPR paper, Erwin Schrödinger wrote a letter to Einstein in German in which he used the word "Verschränkung" (translated by himself as "entanglement") "to describe the correlations between two particles that interact and then separate, as in the EPR experiment."

Schrödinger shortly thereafter published a seminal paper defining and discussing the notion of "entanglement." In the paper he recognized the importance of the concept, and stated: "I would not call [entanglement] "one" but rather "the" characteristic trait of quantum mechanics, the one that enforces its entire departure from classical lines of thought."

Like Einstein, Schrödinger was dissatisfied with the concept of entanglement, because it seemed to violate the speed limit on the transmission of information implicit in the theory of relativity. Einstein later famously derided entanglement as ""spukhafte Fernwirkung"" or "spooky action at a distance."

The EPR paper generated significant interest among physicists and inspired much discussion about the foundations of quantum mechanics (perhaps most famously Bohm's interpretation of quantum mechanics), but produced relatively little other published work. So, despite the interest, the weak point in EPR's argument was not discovered until 1964, when John Stewart Bell proved that one of their key assumptions, the principle of locality, as applied to the kind of hidden variables interpretation hoped for by EPR, was mathematically inconsistent with the predictions of quantum theory.

Specifically, Bell demonstrated an upper limit, seen in Bell's inequality, regarding the strength of correlations that can be produced in any theory obeying local realism, and he showed that quantum theory predicts violations of this limit for certain entangled systems. His inequality is experimentally testable, and there have been numerous relevant experiments, starting with the pioneering work of Stuart Freedman and John Clauser in 1972 and Alain Aspect's experiments in 1982, all of which have shown agreement with quantum mechanics rather than the principle of local realism.

Until recently each had left open at least one loophole by which it was possible to question the validity of the results. However, in 2015 an experiment was performed that simultaneously closed both the detection and locality loopholes, and was heralded as "loophole-free"; this experiment ruled out a large class of local realism theories with certainty. Alain Aspect notes that the setting-independence loophole, which he refers to as "far-fetched" yet a "residual loophole" that "cannot be ignored" has yet to be closed, and the free-will, or superdeterminism, loophole is unclosable, saying "no experiment, as ideal as it is, can be said to be totally loophole-free."

A minority opinion holds that although quantum mechanics is correct, there is no superluminal instantaneous action-at-a-distance between entangled particles once the particles are separated.

Bell's work raised the possibility of using these super-strong correlations as a resource for communication. It led to the discovery of quantum key distribution protocols, most famously BB84 by Charles H. Bennett and Gilles Brassard and E91 by Artur Ekert. Although BB84 does not use entanglement, Ekert's protocol uses the violation of a Bell's inequality as a proof of security.

An entangled system is defined to be one whose quantum state cannot be factored as a product of states of its local constituents; that is to say, they are not individual particles but are an inseparable whole. In entanglement, one constituent cannot be fully described without considering the other(s). Note that the state of a composite system is always expressible as a "sum", or superposition, of products of states of local constituents; it is entangled if this sum necessarily has more than one term.

Quantum systems can become entangled through various types of interactions. For some ways in which entanglement may be achieved for experimental purposes, see the section below on methods. Entanglement is broken when the entangled particles decohere through interaction with the environment; for example, when a measurement is made.

As an example of entanglement: a subatomic particle decays into an entangled pair of other particles. The decay events obey the various conservation laws, and as a result, the measurement outcomes of one daughter particle must be highly correlated with the measurement outcomes of the other daughter particle (so that the total momenta, angular momenta, energy, and so forth remains roughly the same before and after this process). For instance, a spin-zero particle could decay into a pair of spin-½ particles. Since the total spin before and after this decay must be zero (conservation of angular momentum), whenever the first particle is measured to be spin up on some axis, the other, when measured on the same axis, is always found to be spin down. (This is called the "spin anti-correlated" case; and if the prior probabilities for measuring each spin are equal, the pair is said to be in the singlet state.)

The special property of entanglement can be better observed if we separate the said two particles. Let's put one of them in the White House in Washington and the other in Buckingham Palace (think about this as a thought experiment, not an actual one). Now, if we measure a particular characteristic of one of these particles (say, for example, spin), get a result, and then measure the other particle using the same criterion (spin along the same axis), we find that the result of the measurement of the second particle will match (in a complementary sense) the result of the measurement of the first particle, in that they will be opposite in their values.

The above result may or may not be perceived as surprising. A classical system would display the same property, and a hidden variable theory (see below) would certainly be "required" to do so, based on conservation of angular momentum in classical and quantum mechanics alike. The difference is that a classical system has definite values for all the observables all along, while the quantum system does not. In a sense to be discussed below, the quantum system considered here seems to "acquire" a probability distribution for the outcome of a measurement of the spin along "any" axis of the "other" particle upon measurement of the "first" particle. This probability distribution is in general "different" from what it would be "without" measurement of the first particle. This may certainly be perceived as surprising in the case of spatially separated entangled particles.

The paradox is that a measurement made on either of the particles apparently collapses the state of the entire entangled system—and does so instantaneously, before any information about the measurement result could have been communicated to the other particle (assuming that information cannot travel faster than light) and hence assured the "proper" outcome of the measurement of the other part of the entangled pair. In the Copenhagen interpretation, the result of a spin measurement on one of the particles is a collapse into a state in which each particle has a definite spin (either up or down) along the axis of measurement. The outcome is taken to be random, with each possibility having a probability of 50%. However, if both spins are measured along the same axis, they are found to be anti-correlated. This means that the random outcome of the measurement made on one particle seems to have been transmitted to the other, so that it can make the "right choice" when it too is measured.

The distance and timing of the measurements can be chosen so as to make the interval between the two measurements spacelike, hence, any causal effect connecting the events would have to travel faster than light. According to the principles of special relativity, it is not possible for any information to travel between two such measuring events. It is not even possible to say which of the measurements came first. For two spacelike separated events and there are inertial frames in which is first and others in which is first. Therefore, the correlation between the two measurements cannot be explained as one measurement determining the other: different observers would disagree about the role of cause and effect.

A possible resolution to the paradox is to assume that quantum theory is incomplete, and the result of measurements depends on predetermined "hidden variables". The state of the particles being measured contains some hidden variables, whose values effectively determine, right from the moment of separation, what the outcomes of the spin measurements are going to be. This would mean that each particle carries all the required information with it, and nothing needs to be transmitted from one particle to the other at the time of measurement. Einstein and others (see the previous section) originally believed this was the only way out of the paradox, and the accepted quantum mechanical description (with a random measurement outcome) must be incomplete. (In fact similar paradoxes can arise even without entanglement: the position of a single particle is spread out over space, and two widely separated detectors attempting to detect the particle in two different places must instantaneously attain appropriate correlation, so that they do not "both" detect the particle.)

The hidden variables theory fails, however, when we consider measurements of the spin of entangled particles along different axes (for example, along any of three axes that make angles of 120 degrees). If a large number of pairs of such measurements are made (on a large number of pairs of entangled particles), then statistically, if the local realist or hidden variables view were correct, the results would always satisfy Bell's inequality. A number of experiments have shown in practice that Bell's inequality is not satisfied. However, prior to 2015, all of these had loophole problems that were considered the most important by the community of physicists. When measurements of the entangled particles are made in moving relativistic reference frames, in which each measurement (in its own relativistic time frame) occurs before the other, the measurement results remain correlated.

The fundamental issue about measuring spin along different axes is that these measurements cannot have definite values at the same time―they are incompatible in the sense that these measurements' maximum simultaneous precision is constrained by the uncertainty principle. This is contrary to what is found in classical physics, where any number of properties can be measured simultaneously with arbitrary accuracy. It has been proven mathematically that compatible measurements cannot show Bell-inequality-violating correlations, and thus entanglement is a fundamentally non-classical phenomenon.
In experiments in 2012 and 2013, polarization correlation was created between photons that never coexisted in time. The authors claimed that this result was achieved by entanglement swapping between two pairs of entangled photons "after" measuring the polarization of one photon of the early pair, and that it proves that quantum non-locality applies not only to space but also to time.

In three independent experiments in 2013 it was shown that classically-communicated separable quantum states can be used to carry entangled states. The first loophole-free Bell test was held in TU Delft in 2015 confirming the violation of Bell inequality.

In August 2014, Brazilian researcher Gabriela Barreto Lemos and team were able to "take pictures" of objects using photons that had not interacted with the subjects, but were entangled with photons that did interact with such objects. Lemos, from the University of Vienna, is confident that this new quantum imaging technique could find application where low light imaging is imperative, in fields like biological or medical imaging.

There have been suggestions to look at the concept of time as an emergent phenomenon that is a side effect of quantum entanglement. 
In other words, time is an entanglement phenomenon, which places all equal clock readings (of correctly prepared clocks, or of any objects usable as clocks) into the same history. This was first fully theorized by Don Page and William Wootters in 1983. 
The Wheeler–DeWitt equation that combines general relativity and quantum mechanics – by leaving out time altogether – was introduced in the 1960s and it was taken up again in 1983, when the theorists Don Page and William Wootters made a solution based on the quantum phenomenon of entanglement. Page and Wootters argued that entanglement can be used to measure time.

In 2013, at the Istituto Nazionale di Ricerca Metrologica (INRIM) in Turin, Italy, researchers performed the first experimental test of Page and Wootters' ideas. Their result has been interpreted to confirm that time is an emergent phenomenon for internal observers but absent for external observers of the universe just as the Wheeler-DeWitt equation predicts.

Physicist Seth Lloyd says that quantum uncertainty gives rise to "entanglement", the putative source of the arrow of time. According to Lloyd; "The arrow of time is an arrow of increasing correlations." The approach to entanglement would be from the perspective of the causal arrow of time, with the assumption that the cause of the measurement of one particle determines the effect of the result of the other particle's measurement.

In the media and popular science, quantum non-locality is often portrayed as being equivalent to entanglement. While it is true that a pure bipartite quantum state must be entangled in order for it to produce non-local correlations, there exist entangled states that do not produce such correlations, and there exist non-entangled (separable) quantum states that present some non-local behaviour. A well-known example of the first case is the Werner state that is entangled for certain values of formula_1, but can always be described using local hidden variables. In short, entanglement of a state shared by two parties is necessary but not sufficient for that state to be non-local. Moreover, it was shown that, for arbitrary numbers of parties, there exist states that are genuinely entangled but admits a fully local strategy. It is important to recognize that entanglement is more commonly viewed as an algebraic concept, noted for being a precedent to non-locality as well as to quantum teleportation and to superdense coding, whereas non-locality is defined according to experimental statistics and is much more involved with the foundations and interpretations of quantum mechanics.

The following subsections are for those with a good working knowledge of the formal, mathematical description of quantum mechanics, including familiarity with the formalism and theoretical framework developed in the articles: bra–ket notation and mathematical formulation of quantum mechanics.

Consider two noninteracting systems and , with respective Hilbert spaces and . The Hilbert space of the composite system is the tensor product

If the first system is in state formula_3 and the second in state formula_4, the state of the composite system is

States of the composite system that can be represented in this form are called "separable states", or "product states".

Not all states are separable states (and thus product states). Fix a basis formula_6 for and a basis formula_7 for . The most general state in is of the form

This state is separable if there exist vectors formula_9 so that formula_10 yielding formula_11 and formula_12 It is inseparable if for any vectors formula_13 at least for one pair of coordinates formula_14 we have formula_15 If a state is inseparable, it is called an "entangled state".

For example, given two basis vectors formula_16 of and two basis vectors formula_17 of , the following is an entangled state:

If the composite system is in this state, it is impossible to attribute to either system or system a definite pure state. Another way to say this is that while the von Neumann entropy of the whole state is zero (as it is for any pure state), the entropy of the subsystems is greater than zero. In this sense, the systems are "entangled". This has specific empirical ramifications for interferometry. It is worthwhile to note that the above example is one of four Bell states, which are (maximally) entangled pure states (pure states of the space, but which cannot be separated into pure states of each and ).

Now suppose Alice is an observer for system , and Bob is an observer for system . If in the entangled state given above Alice makes a measurement in the formula_19 eigenbasis of , there are two possible outcomes, occurring with equal probability:


If the former occurs, then any subsequent measurement performed by Bob, in the same basis, will always return 1. If the latter occurs, (Alice measures 1) then Bob's measurement will return 0 with certainty. Thus, system has been altered by Alice performing a local measurement on system . This remains true even if the systems and are spatially separated. This is the foundation of the EPR paradox.

The outcome of Alice's measurement is random. Alice cannot decide which state to collapse the composite system into, and therefore cannot transmit information to Bob by acting on her system. Causality is thus preserved, in this particular scheme. For the general argument, see no-communication theorem.

As mentioned above, a state of a quantum system is given by a unit vector in a Hilbert space. More generally, if one has less information about the system, then one calls it an "ensemble" and describes it by a density matrix, which is a positive-semidefinite matrix, or a trace class when the state space is infinite-dimensional, and has trace 1. Again, by the spectral theorem, such a matrix takes the general form:

where the "w" are positive-valued probabilities (they sum up to 1), the vectors are unit vectors, and in the infinite-dimensional case, we would take the closure of such states in the trace norm. We can interpret as representing an ensemble where is the proportion of the ensemble whose states are formula_23. When a mixed state has rank 1, it therefore describes a "pure ensemble". When there is less than total information about the state of a quantum system we need density matrices to represent the state.

Experimentally, a mixed ensemble might be realized as follows. Consider a "black box" apparatus that spits electrons towards an observer. The electrons' Hilbert spaces are identical. The apparatus might produce electrons that are all in the same state; in this case, the electrons received by the observer are then a pure ensemble. However, the apparatus could produce electrons in different states. For example, it could produce two populations of electrons: one with state formula_24 with spins aligned in the positive direction, and the other with state formula_25 with spins aligned in the negative direction. Generally, this is a mixed ensemble, as there can be any number of populations, each corresponding to a different state.

Following the definition above, for a bipartite composite system, mixed states are just density matrices on . That is, it has the general form

where the "w" are positively valued probabilities, formula_27, and the vectors are unit vectors. This is self-adjoint and positive and has trace 1.

Extending the definition of separability from the pure case, we say that a mixed state is separable if it can be written as

where the are positively valued probabilities and the formula_29's and formula_30's are themselves mixed states (density operators) on the subsystems and respectively. In other words, a state is separable if it is a probability distribution over uncorrelated states, or product states. By writing the density matrices as sums of pure ensembles and expanding, we may assume without loss of generality that formula_29 and formula_30 are themselves pure ensembles. A state is then said to be "entangled" if it is not separable.

In general, finding out whether or not a mixed state is entangled is considered difficult. The general bipartite case has been shown to be NP-hard. For the and cases, a necessary and sufficient criterion for separability is given by the famous Positive Partial Transpose (PPT) condition.

The idea of a reduced density matrix was introduced by Paul Dirac in 1930. Consider as above systems and each with a Hilbert space . Let the state of the composite system be

As indicated above, in general there is no way to associate a pure state to the component system . However, it still is possible to associate a density matrix. Let

which is the projection operator onto this state. The state of is the partial trace of over the basis of system :

For example, the reduced density matrix of for the entangled state

discussed above is

This demonstrates that, as expected, the reduced density matrix for an entangled pure ensemble is a mixed ensemble. Also not surprisingly, the density matrix of for the pure product state formula_38 discussed above is

In general, a bipartite pure state ρ is entangled if and only if its reduced states are mixed rather than pure.

Reduced density matrices were explicitly calculated in different spin chains with unique ground state. An example is the one-dimensional AKLT spin chain: the ground state can be divided into a block and an environment. The reduced density matrix of the block is proportional to a projector to a degenerate ground state of another Hamiltonian.

The reduced density matrix also was evaluated for XY spin chains, where it has full rank. It was proved that in the thermodynamic limit, the spectrum of the reduced density matrix of a large block of spins is an exact geometric sequence in this case.

In this section, the entropy of a mixed state is discussed as well as how it can be viewed as a measure of quantum entanglement.

In classical information theory , the Shannon entropy, is associated to a probability distribution,formula_40, in the following way:

Since a mixed state is a probability distribution over an ensemble, this leads naturally to the definition of the von Neumann entropy:

In general, one uses the Borel functional calculus to calculate a non-polynomial function such as . If the nonnegative operator acts on a finite-dimensional Hilbert space and has eigenvalues formula_43, turns out to be nothing more than the operator with the same eigenvectors, but the eigenvalues formula_44. The Shannon entropy is then:

Since an event of probability 0 should not contribute to the entropy, and given that

the convention is adopted. This extends to the infinite-dimensional case as well: if has spectral resolution

assume the same convention when calculating

As in statistical mechanics, the more uncertainty (number of microstates) the system should possess, the larger the entropy. For example, the entropy of any pure state is zero, which is unsurprising since there is no uncertainty about a system in a pure state. The entropy of any of the two subsystems of the entangled state discussed above is (which can be shown to be the maximum entropy for mixed states).

Entropy provides one tool that can be used to quantify entanglement, although other entanglement measures exist. If the overall system is pure, the entropy of one subsystem can be used to measure its degree of entanglement with the other subsystems.

For bipartite pure states, the von Neumann entropy of reduced states is the unique measure of entanglement in the sense that it is the only function on the family of states that satisfies certain axioms required of an entanglement measure.

It is a classical result that the Shannon entropy achieves its maximum at, and only at, the uniform probability distribution {1/"n"...,1/"n"}. Therefore, a bipartite pure state is said to be a maximally entangled state if the reduced state of is the diagonal matrix

For mixed states, the reduced von Neumann entropy is not the only reasonable entanglement measure.

As an aside, the information-theoretic definition is closely related to entropy in the sense of statistical mechanics (comparing the two definitions, we note that, in the present context, it is customary to set the Boltzmann constant ). For example, by properties of the Borel functional calculus, we see that for any unitary operator ,

Indeed, without this property, the von Neumann entropy would not be well-defined.

In particular, could be the time evolution operator of the system, i.e.,

where is the Hamiltonian of the system. Here the entropy is unchanged.

The reversibility of a process is associated with the resulting entropy change, i.e., a process is reversible if, and only if, it leaves the entropy of the system invariant. Therefore, the march of the arrow of time towards thermodynamic equilibrium is simply the growing spread of quantum entanglement.
This provides a connection between quantum information theory and thermodynamics.

Rényi entropy also can be used as a measure of entanglement.

Entanglement measures quantify the amount of entanglement in a (often viewed as a bipartite) quantum state. As aforementioned, entanglement entropy is the standard measure of entanglement for pure states (but no longer a measure of entanglement for mixed states). For mixed states, there are some entanglement measures in the literature and no single one is standard.

Most (but not all) of these entanglement measures reduce for pure states to entanglement entropy, and are difficult (NP-hard) to compute.

The Reeh-Schlieder theorem of quantum field theory is sometimes seen as an analogue of quantum entanglement.

Entanglement has many applications in quantum information theory. With the aid of entanglement, otherwise impossible tasks may be achieved.

Among the best-known applications of entanglement are superdense coding and quantum teleportation.

Most researchers believe that entanglement is necessary to realize quantum computing (although this is disputed by some).

Entanglement is used in some protocols of quantum cryptography. This is because the "shared noise" of entanglement makes for an excellent one-time pad. Moreover, since measurement of either member of an entangled pair destroys the entanglement they share, entanglement-based quantum cryptography allows the sender and receiver to more easily detect the presence of an interceptor.

In interferometry, entanglement is necessary for surpassing the standard quantum limit and achieving the Heisenberg limit.

There are several canonical entangled states that appear often in theory and experiments.

For two qubits, the Bell states are

These four pure states are all maximally entangled (according to the entropy of entanglement) and form an orthonormal basis (linear algebra) of the Hilbert space of the two qubits. They play a fundamental role in Bell's theorem.

For M>2 qubits, the GHZ state is

which reduces to the Bell state formula_55 for formula_56. The traditional GHZ state was defined for formula_57. GHZ states are occasionally extended to "qudits", i.e., systems of "d" rather than 2 dimensions.

Also for M>2 qubits, there are spin squeezed states. Spin squeezed states are a class of squeezed coherent states satisfying certain restrictions on the uncertainty of spin measurements, and are necessarily entangled. Spin squeezed states are good candidates for enhancing precision measurements using quantum entanglement.

For two bosonic modes, a NOON state is

This is like a Bell state formula_55 except the basis kets 0 and 1 have been replaced with "the "N" photons are in one mode" and "the "N" photons are in the other mode".

Finally, there also exist twin Fock states for bosonic modes, which can be created by feeding a Fock state into two arms leading to a beam splitter. They are the sum of multiple of NOON states, and can used to achieve the Heisenberg limit.

For the appropriately chosen measure of entanglement, Bell, GHZ, and NOON states are maximally entangled while spin squeezed and twin Fock states are only partially entangled. The partially entangled states are generally easier to prepare experimentally.

Entanglement is usually created by direct interactions between subatomic particles. These interactions can take numerous forms. One of the most commonly used methods is spontaneous parametric down-conversion to generate a pair of photons entangled in polarisation. Other methods include the use of a fiber coupler to confine and mix photons, photons emitted from decay cascade of the bi-exciton in a quantum dot, the use of the Hong–Ou–Mandel effect, etc., In the earliest tests of Bell's theorem, the entangled particles were generated using atomic cascades.

It is also possible to create entanglement between quantum systems that never directly interacted, through the use of entanglement swapping. Two independently-prepared, identical particles may also be entangled if their wave functions merely spatially overlap, at least partially.

Systems which contain no entanglement are said to be separable. For 2-Qubit and Qubit-Qutrit systems (2 × 2 and 2 × 3 respectively) the simple Peres–Horodecki criterion provides both a necessary and a sufficient criterion for separability, and thus for detecting entanglement. However, for the general case, the criterion is merely a sufficient one for separability, as the problem becomes NP-hard. A numerical approach to the problem is suggested by Jon Magne Leinaas, Jan Myrheim and Eirik Ovrum in their paper "Geometrical aspects of entanglement". Leinaas et al. offer a numerical approach, iteratively refining an estimated separable state towards the target state to be tested, and checking if the target state can indeed be reached. An implementation of the algorithm (including a built-in Peres-Horodecki criterion testing) is brought in the "StateSeparator" web-app.

In 2016 China launched the world’s first quantum communications satellite. The $100m Quantum Experiments at Space Scale (QUESS) mission was launched on Aug 16, 2016, from the Jiuquan Satellite Launch Center in northern China at 01:40 local time.

For the next two years, the craft – nicknamed "Micius" after the ancient Chinese philosopher – will demonstrate the feasibility of quantum
communication between Earth and space, and test quantum entanglement over unprecedented distances.

In the June 16, 2017, issue of "Science", Yin et al. report setting a new quantum entanglement distance record of 1203 km, demonstrating the survival of a 2-photon pair and a violation of a Bell inequality, reaching a CHSH valuation of 2.37 ± 0.09, under strict Einstein locality conditions, from the Micius satellite to bases in Lijian, Yunnan and Delingha, Quinhai, increasing the efficiency of transmission over prior fiberoptic experiments by an order of magnitude.

The electron shell of multi-electron atoms always consists of entangled electrons. The correct ionization energy can be calculated only by consideration of electron entanglement.

It has been suggested that in the process of photosynthesis, entanglement is involved in the transfer of energy between light-harvesting complexes and photosynthetic reaction centers where the kinetic energy is harvested in the form of chemical energy. Without such a process, the efficient conversion of optical energy into chemical energy cannot be explained. Using femtosecond spectroscopy, the coherence of entanglement in the Fenna-Matthews-Olson complex was measured over hundreds of femtoseconds (a relatively long time in this regard) providing support to this theory.




</doc>
<doc id="25343" url="https://en.wikipedia.org/wiki?curid=25343" title="Quasi-War">
Quasi-War

The Quasi-War () was an undeclared war fought almost entirely at sea between the United States and France from 1798 to 1800. After the toppling of the French crown during the French Revolutionary Wars, the United States refused to continue repaying its debt to France on the grounds that it had been owed to a previous regime. French outrage led to a series of attacks on U.S. shipping, ultimately leading to retaliation from the U.S. 

The war was called "quasi" because it was undeclared. It involved two years of hostilities at sea, in which both navies attacked the other's shipping in the West Indies. The unexpected fighting ability of the U.S. Navy, which destroyed the French West Indian trade, together with the growing weaknesses and final overthrow of the ruling Directory in France, led Talleyrand to reopen negotiations. At the same time, President Adams feuded with Hamilton over control of the Adams administration. Adams took sudden and unexpected action, rejecting the anti-French hawks in his own party and offering peace to France. In 1800 he sent William Vans Murray to France to negotiate peace; the Federalists cried betrayal. Hostilities ended with the signing of the Convention of 1800.

The Kingdom of France, a crucial ally of the United States in the American Revolutionary War after early 1776, had loaned the U.S. large amounts of money and in 1778 signed a treaty of alliance against Great Britain. However, Louis XVI of France was overthrown in 1792 during the French Revolution, with the French monarchy being abolished. As a result, in 1794 the U.S. government reached an agreement with Great Britain in the Jay Treaty, ratified the following year. It resolved several points of contention between the United States and Britain that had lingered since the end of the American Revolutionary War. It also encouraged bilateral trade, but it outraged the Jeffersonian Democrat Republicans, who were pro-France. 

The United States had already declared neutrality in the conflict between Great Britain and revolutionary France, and U.S. legislation was being passed for a trade deal with Great Britain. When the U.S. refused to continue repaying its debt using the argument that the debt was owed to the previous government, not to the French First Republic, French outrage led to a series of responses. First, French privateers began seizing U.S. ships trading with Great Britain and bringing them in as prizes to be sold. Next, the French government refused to receive Charles Cotesworth Pinckney, the new U.S. Minister, when he arrived in Paris in December 1796. In his annual message to Congress at the close of 1797, President John Adams reported on France's refusal to negotiate a settlement and spoke of the need "to place our country in a suitable posture of defense." In April 1798, President Adams informed Congress of the "XYZ Affair", in which French agents demanded a large bribe before engaging in substantive negotiations with United States diplomats.

Meanwhile, French privateers inflicted substantial losses on U.S. shipping. On 21 February 1797, Secretary of State Timothy Pickering told Congress that during the previous eleven months, France had seized 316 U.S. merchant ships. French marauders cruised the length of the Atlantic seaboard virtually unopposed. The United States government had nothing to combat them, as the navy had been abolished at the end of the Revolutionary War and its last warship sold in 1785. The United States had only a flotilla of small revenue cutters and a few neglected coastal forts.

Increased depredations by French privateers led to the rebirth of the U.S. Navy and U.S. Marine Corps to defend the expanding U.S. merchant fleet. Congress authorized the president to acquire, arm, and man not more than twelve ships of up to twenty two guns each. Several merchantmen were immediately purchased and refitted as ships of war, and construction of the frigate resumed.

Congress rescinded the treaties with France on 7 July 1798. That date is now considered the beginning of the Quasi-War. This was followed two days later with the passage of the Congressional authorization of attacks on French warships in U.S. waters.

The U.S. Navy operated with a battle fleet of about twenty-five vessels, which patrolled the southern coast of the United States and throughout the Caribbean, hunting down French privateers. Captain Thomas Truxtun's insistence on the highest standards of crew training paid dividends when the frigate captured the French Navy's frigate "L'Insurgente" and severely damaged the frigate "La Vengeance". French privateers generally resisted, as did "La Croyable", which was captured on 7 July 1798, by outside of Egg Harbor, New Jersey. captured eight privateers and freed eleven U.S. merchant ships from captivity, while captured the French privateers "Deux Amis" and "Diane". Numerous U.S. merchantmen were recaptured by the "Experiment". forced "Le Berceau" into submission. Silas Talbot engineered an expedition to Puerto Plata harbor in Hispaniola. On 11 May 1800, sailors and marines from under Lieutenant Isaac Hull captured the French privateer "Sandwich" in the harbor and spiked the guns of the fort.

The U.S. Navy lost only one ship to the French, , which was later recaptured. She was the captured privateer "La Croyable", recently purchased by the U.S. Navy. "Retaliation" departed Norfolk on 28 October 1798, with and , and cruised in the West Indies protecting U.S. commerce. On 20 November 1798, the French frigates "L’Insurgente" and "Volontaire" overtook the "Retaliation" while her consorts were away and forced commanding officer Lieutenant William Bainbridge to surrender the out-gunned schooner. The "Montezuma" and "Norfolk" escaped after Bainbridge convinced the senior French commander that those U.S. warships were too powerful for his frigates and persuaded him to abandon the chase. Renamed the "Magicienne" by the French, the schooner again came into U.S. hands on 28 June, when a broadside from forced her to haul down her colors.

Revenue cutters in the service of the U.S. Revenue-Marine (the predecessor to the U.S. Coast Guard), also took part in the conflict. The cutter USRC "Pickering", commanded by Edward Preble, made two cruises to the West Indies and captured ten prizes. Preble turned command of the "Pickering" over to Benjamin Hillar, who captured the much larger and more heavily armed French privateer "lEgypte Conquise" after a nine-hour battle. In September 1800, Hillar, the "Pickering", and her entire crew were lost at sea in a storm. Preble next commanded the frigate "Essex", which he sailed around Cape Horn into the Pacific to protect U.S. merchantmen in the East Indies. He recaptured several U.S. ships that had been seized by French privateers.

U.S. naval losses may have been light, but the French had successfully seized many U.S. merchant ships by the war's end in 1800 — more than 2,000, according to one source.

Although they were fighting the same enemy, the Royal Navy and the United States Navy did not cooperate operationally or share operational plans. There were no mutual understandings about deployment between their forces. However, the British sold naval stores and munitions to the U.S. government, and the two navies shared a signal system so they could recognise the other's warships at sea and allowed their merchantmen to join each other's convoys for safety.

By late 1800, the United States Navy and the Royal Navy, combined with a more conciliatory diplomatic stance by the government of First Consul Napoleon Bonaparte, had reduced the activity of the French privateers and warships. The Convention of 1800, signed on 30 September, ended the Quasi-War. It was embodied in the Treaty of Mortefontaine of September 30, 1800. It affirmed the rights of Americans as neutrals upon the sea and abrogated the alliance with France of 1778. The treaty failed to provide compensation for the $20,000,000 "French Spoliation Claims" of the United States. The treaty and the Convention of 1800 between the two nations implicitly ensured that the United States would remain neutral toward France in the wars of Napoleon and ended the "entangling" French alliance. In truth, this alliance had only been viable between 1778 and 1783.





</doc>
<doc id="25345" url="https://en.wikipedia.org/wiki?curid=25345" title="Quality management system">
Quality management system

A quality management system (QMS) is a collection of business processes focused on consistently meeting customer requirements and enhancing their satisfaction. It is aligned with an organization's purpose and strategic direction (ISO9001:2015). It is expressed as the organizational goals and aspirations, policies, processes, documented information and resources needed to implement and maintain it. Early quality management systems emphasized predictable outcomes of an industrial product production line, using simple statistics and random sampling. By the 20th century, labor inputs were typically the most costly inputs in most industrialized societies, so focus shifted to team cooperation and dynamics, especially the early signaling of problems via a continual improvement cycle. In the 21st century, QMS has tended to converge with sustainability and transparency initiatives, as both investor and customer satisfaction and perceived quality is increasingly tied to these factors. Of QMS regimes, the ISO 9000 family of standards is probably the most widely implemented worldwide – the ISO 19011 audit regime applies to both, and deals with quality and sustainability and their integration.

Other QMS, e.g. Natural Step, focus on sustainability issues and assume that other quality problems will be reduced as result of the systematic thinking, transparency, documentation and diagnostic discipline.

The term "Quality Management System" and the acronym "QMS" were invented in 1991 by Ken Croucher, a British management consultant working on designing and implementing a generic model of a QMS within the IT industry.


The concept of a quality as we think of it now first emerged from the Industrial Revolution.. Previously goods had been made from start to finish by the same person or team of people, with handcrafting and tweaking the product to meet 'quality criteria'. Mass production brought huge teams of people together to work on specific stages of production where one person would not necessarily complete a product from start to finish. In the late 19th century pioneers such as Frederick Winslow Taylor and Henry Ford recognized the limitations of the methods being used in mass production at the time and the subsequent varying quality of output. Birland established Quality Departments to oversee the quality of production and rectifying of errors, and Ford emphasized standardization of design and component standards to ensure a standard product was produced. Management of quality was the responsibility of the Quality department and was implemented by Inspection of product output to 'catch' defects.

Application of statistical control came later as a result of World War production methods, which were advanced by the work done of W. Edwards Deming, a statistician, after whom the Deming Prize for quality is named. Joseph M. Juran focused more on managing for quality. The first edition of Juran's Quality Control Handbook was published in 1951. He also developed the "Juran's trilogy", an approach to cross-functional management that is composed of three managerial processes: quality planning, quality control, and quality improvement. These functions all play a vital role when evaluating quality.

Quality, as a profession and the managerial process associated with the quality function, was introduced during the second half of the 20th century and has evolved since then. Over this period, few other disciplines have seen as many changes as the quality profession.

The quality profession grew from simple control to engineering, to systems engineering. Quality control activities were predominant in the 1940s, 1950s, and 1960s. The 1970s were an era of quality engineering and the 1990s saw quality systems as an emerging field. Like medicine, accounting, and engineering, quality has achieved status as a recognized profession

As Lee and Dale (1998) state, there are many organizations that are striving to assess the methods and ways in which their overall productivity, the quality of their products and services and the required operations to achieve them are done.

The two primary , state of the art, guidelines for medical device manufacturer QMS and related services today are the ISO 13485 standards and the US FDA 21 CFR 820 regulations. The two have a great deal of similarity, and many manufacturers adopt QMS that is compliant with both guidelines.

ISO 13485 are harmonized with the European Union medical devices directive (93/42/EEC) as well as the IVD and AIMD directives. The ISO standard is also incorporated in regulations for other jurisdictions such as Japan (JPAL) and Canada (CMDCAS).

Quality System requirements for medical devices have been internationally recognized as a way to assure product safety and efficacy and customer satisfaction since at least 1983 and were instituted as requirements in a final rule published on October 7, 1996. The U.S. Food and Drug Administration (FDA) had documented design defects in medical devices that contributed to recalls from 1983 to 1989 that would have been prevented if Quality Systems had been in place. The rule is promulgated at 21 CFR 820.

According to current Good Manufacturing Practice (GMP), medical device manufacturers have the responsibility to use good judgment when developing their quality system and apply those sections of the FDA Quality System (QS) Regulation that are applicable to their specific products and operations, in Part 820 of the QS regulation. As with GMP, operating within this flexibility, it is the responsibility of each manufacturer to establish requirements for each type or family of devices that will result in devices that are safe and effective, and to establish methods and procedures to design, produce, and distribute devices that meet the quality system requirements.

The FDA has identified in the QS regulation the 7 essential subsystems of a quality system. These subsystems include:

all overseen by management and quality audits.

Because the QS regulation covers a broad spectrum of devices and production processes, it allows some leeway in the details of quality system elements. It is left to manufacturers to determine the necessity for, or extent of, some quality elements and to develop and implement procedures tailored to their particular processes and devices. For example, if it is impossible to mix up labels at a manufacturer because there is only one label to each product, then there is no necessity for the manufacturer to comply with all of the GMP requirements under device labeling.

Drug manufactures are regulated under a different section of the Code of Federal Regulations:

The International Organization for Standardization's ISO 9001:2015 series describes standards for a QMS addressing the principles and processes surrounding the design, development, and delivery of a general product or service. Organizations can participate in a continuing certification process to ISO 9001:2008 to demonstrate their compliance with the standard, which includes a requirement for continual (i.e. planned) improvement of the QMS, as well as more foundational QMS components such as failure mode and effects analysis (FMEA).

(ISO 9000:2005 provides information on the fundamentals and vocabulary used in quality management systems. ISO 9004:2009 provides guidance on quality management approach for the sustained success of an organization. Neither of these standards can be used for certification purposes as they provide guidance, not requirements).

The Baldrige Performance Excellence Program educates organizations in improving their performance and administers the Malcolm Baldrige National Quality Award. The Baldrige Award recognizes U.S. organizations for performance excellence based on the Baldrige Criteria for Performance Excellence. The Criteria address critical aspects of management that contribute to performance excellence: leadership; strategy; customers; measurement, analysis, and knowledge management; workforce; operations; and results.

The European Foundation for Quality Management's EFQM Excellence Model supports an award scheme similar to the Baldrige Award for European companies.

In Canada, the National Quality Institute presents the 'Canada Awards for Excellence' on an annual basis to organizations that have displayed outstanding performance in the areas of Quality and Workplace Wellness, and have met the Institute's criteria with documented overall achievements and results.

The European Quality in Social Service (EQUASS) is a sector-specific quality system designed for the social services sector and addresses quality principles that are specific to service delivery to vulnerable groups, such as empowerment, rights, and person-centredness. 

The Alliance for Performance Excellence is a network of state and local organizations that use the Baldrige Criteria for Performance Excellence at the grassroots level to improve the performance of local organizations and economies. browsers can find Alliance members in their state and get the latest news and events from the Baldrige community.

A QMS process is an element of an organizational QMS. The ISO9001:2000 standard requires organizations seeking compliance or certification to define the processes which form the QMS and the sequence and interaction of these processes. Butterworth-Heinemann and other publishers have offered several books which provide step-by-step guides to those seeking the quality certifications of their products 

Examples of such processes include:


ISO9001 requires that the performance of these processes be measured, analyzed and continually improved, and the results of this form an input into the management review process.




</doc>
<doc id="25346" url="https://en.wikipedia.org/wiki?curid=25346" title="Québécois (word)">
Québécois (word)

Québécois (pronounced ; feminine: Québécoise (pronounced ), ' (fem.: '), or (fem.: ) is a word used primarily to refer to a native or inhabitant of the Canadian province of Quebec, the majority of which speak French as a mother tongue. It can refer to French spoken in Quebec. It may also be used, with an upper or lower case initial, as an adjective relating to Quebec, or to the French culture of Quebec. A resident or native of Quebec is usually referred to in English as a Quebecer or Quebecker. In French, Québécois or Québécoise usually refers to any native or resident of Quebec. Its use became more prominent in the 1960s as French Canadians from Quebec increasingly self-identified as Québécois.

The name "Quebec" comes from a Mi'kmaq word "k'webeq" meaning "where the waters get narrow" and originally referred to the area around Quebec City, where the Saint Lawrence River narrows to a cliff-lined gap. French explorer Samuel de Champlain chose this name in 1608 for the colonial outpost he would use as the administrative seat for the French colony of Canada and New France. The Province of Quebec was first founded as a British colony in the Royal Proclamation of 1763 after the Treaty of Paris formally transferred the French colony of New France to Britain after the Seven Years' War. Quebec City remained the capital. In 1774, Guy Carleton obtained from the British Government the Quebec Act, which gave Canadiens most of the territory they held before 1763; the right of religion; and their right of language and culture. The British Government did this to in order to keep their loyalty, in the face of a growing menace of independence from the 13 original British colonies.

The term became more common in English as "Québécois" largely replacing "French Canadian" as an expression of cultural and national identity among French Canadians living in Quebec during the Quiet Revolution of the 1960s. The predominant French Canadian nationalism and identity of previous generations was based on the protection of the French language, the Roman Catholic Church, and Church-run institutions across Canada and in parts of the United States. In contrast, the modern Québécois identity is secular and based on a social democratic ideal of an active Quebec government promoting the French language and French-speaking culture in the arts, education, and business within the Province of Quebec. Politically, this resulted in a push towards more autonomy for Quebec and an internal debate on Quebec independence and identity that continues to this day. The emphasis on the French language and Quebec autonomy means that French-speakers across Canada now self-identify more specifically with provincial or regional identity-tags, such as "acadienne", or "franco-canadienne", "franco-manitobaine", "franco-ontarienne" or "fransaskoise". As a result, francophone and anglophones now borrow the French terms when discussing issues of francophone linguistic and cultural identity in English, though outside of Quebec terms such as Franco-Ontarian, acadian and Franco-Manitoban are still predominant.

The political shift towards a new Quebec nationalism in the 1960s led to Québécois increasingly referring to provincial institutions as being national. This was reflected in the change of the provincial "Legislative Assembly" to "National Assembly" in 1968. Nationalism reached an apex the 1970s and 1990s, with contentious constitutional debates resulting in close to half of all of French-speaking Québécois seeking recognition of nation status through tight referendums on Quebec sovereignty in 1980 and 1995. Having lost both referendums, the sovereigntist Parti Québécois government renewed the push for recognition as a nation through symbolic motions that gained the support of all parties in the National Assembly. They affirmed the right to determine the independent status of Quebec. They also renamed the area around Quebec City the "Capitale-Nationale" (national capital) region and renamed provincial parks "Parcs Nationaux" (national parks). In opposition in October 2003, the Parti Québécois tabled a motion that was unanimously adopted in the National Assembly affirming that the Quebec people formed a nation. Bloc Québécois leader Gilles Duceppe scheduled a similar motion in the House of Commons for November 23, 2006, that would have recognized "Quebecers as a nation". Conservative Prime Minister Stephen Harper tabled the "Québécois nation motion" the day before the Bloc Québécois resolution came to a vote. The English version changed the word "Quebecer" to "Québécois" and added "within a united Canada" at the end of the Bloc motion.

The "Québécois nation" was recognized by the House of Commons of Canada on November 27, 2006. The Prime Minister specified that the motion used the ""cultural"" and ""sociological"" as opposed to the ""legal"" sense of the word ""nation"". According to Harper, the motion was of a symbolic political nature, representing no constitutional change, no recognition of Quebec sovereignty, and no legal change in its political relations within the federation. The Prime Minister has further elaborated, stating that the motion's definition of Québécois relies on personal decisions to self-identify as Québécois, and therefore is a personal choice.

Despite near-universal support in the House of Commons, several important dissenters criticized the motion. Intergovernmental Affairs minister Michael Chong resigned from his position and abstained from voting, arguing that this motion was too ambiguous and had the potential of recognizing a destructive ethnic nationalism in Canada. Liberals were the most divided on the issue and represented 15 of the 16 votes against the motion. Liberal MP Ken Dryden summarized the view of many of these dissenters, maintaining that it was a game of semantics that cheapened issues of national identity. A survey by Leger Marketing in November 2006 showed that Canadians were deeply divided on this issue. When asked if Québécois are a nation, only 53 per cent of Canadians agreed, 47 per cent disagreed, with 33 per cent strongly disagreeing; 78 per cent of French-speaking Canadians agreed that Québécois are a nation, compared with 38 per cent of English-speaking Canadians. As well, 78 per cent of 1,000 Québécois polled thought that Québécois should be recognized as a nation.

The Québécois self-identify as an ethnic group in both the English and French versions of the Canadian census and in demographic studies of ethnicity in Canada. In the 2001 Census of Canada, 98,670 Canadians, or just over 1% of the population of Quebec identified "Québécois" as their ethnicity, ranking "Québécois" as the 37th most common response. These results were based on a question on residents in each household in Canada: ""To which ethnic or cultural group(s) did this person's ancestors belong?"", along with a list of sample choices ("Québécois" did not appear among the various sample choices). The most common ethnicity,""Canadien"" or Canadian, did appear as an example on the questionnaire, and was selected by 4.9 million people or 68.2% of the Quebec population.

In the more detailed "Ethnic Diversity Survey",
Québécois was the most common ethnic identity in Quebec, reported by 37% of
Quebec’s population aged 15 years and older, either as their only identity or alongside
other identities. The survey, based on interviews, asked the following questions: ""1) I would now like to ask you about your ethnic ancestry, heritage or background. What were the ethnic or cultural origins of your ancestors? 2) In addition to "Canadian", what were the other ethnic or cultural origins of your ancestors on first coming to North America?"" This survey did not list possible choices of ancestry and permitted multiple answers.
In census ethnic surveys, French-speaking Canadians identify their ethnicity most often as French, "Canadien", "Québécois", or French Canadian, with the latter three referred to by Jantzen (2005) as "French New World" ancestries because they originate in Canada. Jantzen (2005) distinguishes the English "Canadian", meaning "someone whose family has been in Canada for multiple generations", and the French "Canadien", used to refer to descendants of the original settlers of New France in the 17th and 18th centuries.

Those reporting "French New World" ancestries overwhelmingly had ancestors that went back at least 4 generations in Canada: specifically, 90% of "Québécois" traced their ancestry back this far. Fourth generation Canadiens and Québécois showed considerable attachment to their ethno-cultural group, with 70% and 61% respectively reporting a strong sense of belonging.

The generational profile and strength of identity of French New World ancestries contrast with those of British or Canadian ancestries, which represent the largest ethnic identities in Canada. Although deeply rooted Canadians express a deep attachment to their ethnic identity, most English-speaking Canadians of British ancestry generally cannot trace their ancestry as far back in Canada as French-speakers. As a result, their identification with their ethnicity is weaker tending to have a more broad based cultural identification: for example, only 50% of third generation "Canadians" strongly identify as such, bringing down the overall average. The survey report notes that 80% of Canadians whose families had been in Canada for three or more generations reported "Canadian and provincial or regional ethnic identities". These identities include "Québécois" (37% of Quebec population), "Acadian" (6% of Atlantic provinces) and "Newfoundlander" (38% of Newfoundland and Labrador).

English expressions employing the term may imply specific reference to francophones; such as "Québécois literature"

The dictionary "Le Petit Robert", published in France, states that the adjective "québécois", in addition to its territorial meaning, may refer specifically to francophone or French Canadian culture in Quebec. The dictionary gives as examples "cinéma québécois" and "littérature québécoise".

However, an ethnic or linguistic sense is absent from "Le Petit Larousse", also published in France, as well as from French dictionaries published in Canada such as "Le Dictionnaire québécois d'aujourd'hui" and "Le Dictionnaire du français Plus", which indicate instead "Québécois francophone" "francophone Quebecer" in the linguistic sense.

The online dictionary "Grand dictionnaire terminologique" of the Office québécois de la langue française mentions only a territorial meaning for "Québécois".

Newspaper editor Lysiane Gagnon has referred to an ethnic sense of the word "Québécois" in both English and French.

French expressions employing "Québécois" often appear in both French and English.





</doc>
<doc id="25348" url="https://en.wikipedia.org/wiki?curid=25348" title="Quantico, Virginia">
Quantico, Virginia

Quantico (formerly Potomac) is a town in Prince William County, Virginia, United States. The population was 480 at the 2010 census.

Quantico is bordered by the U.S. military installation of Marine Corps Base Quantico on three sides and the Potomac River on the fourth. Quantico is located south of the mouth of Quantico Creek on the Potomac. The word Quantico is a derivation of the name of a Doeg village recorded by English colonists as "Pamacocack".

Quantico is the site of one of the largest U.S. Marine Corps bases, MCB Quantico. The base is the site of the Marine Corps Combat Development Command and HMX-1 (the presidential helicopter squadron), Officer Candidate School, and The Basic School. The United States Drug Enforcement Administration's training academy, the FBI Academy, the FBI Laboratory, the Naval Criminal Investigative Service, the United States Army Criminal Investigation Command, and the Air Force Office of Special Investigations headquarters are on the base. A replica of the USMC War Memorial stands at the entrance to the base.

, the mayor is Kevin P. Brown.

Quantico is at 38°31'19" North, 77°17'23" West (38.521871, −77.289757). According to the United States Census Bureau, the town has a total area of , of which, of it is land and none of the area is covered with water.

Quantico has a humid subtropical climate (Köppen climate classification "Cfa").

As of the census of 2000, there were 561 people, 295 households, and 107 families living in the town. The population density was . There were 359 housing units at an average density of . The racial makeup was 61.32% White, 20.32% African American, 10.16% Asian, 0.36% Native American, 2.32% from other races, and 5.53% from two or more races. Hispanic or Latino of any race were 5.53% of the population.

There were 295 households out of which 19.7% had children under the age of 18 living with them, 21.4% were married couples living together, 11.2% had a female householder with no husband present, and 63.4% were non-families. 53.2% of all households were made up of individuals and 9.2% had someone living alone who was 65 years of age or older. The average household size was 1.90 and the average family size was 3.02.

In the town the population was spread out with 20.9% under the age of 18, 11.6% from 18 to 24, 39.8% from 25 to 44, 19.4% from 45 to 64, and 8.4% who were 65 years of age or older. The median age was 35 years. For every 100 females, there were 122.6 males. For every 100 females age 18 and over, there were 130.1 males.

The median income for a household in the town was $26,250, and the median income for a family was $27,596. Males had a median income of $29,615 versus $23,125 for females. The per capita income for the town was $19,087. About 22.4% of families and 21.4% of the population were below the poverty line, including 39.4% of those under the age of 18 and none of those ages 65 or older.

Amtrak and Virginia Railway Express trains stop at the Quantico station.


The FBI Academy in Quantico was the setting of fifteen episodes of "The X-Files" and several scenes from Thomas Harris' book "The Silence of the Lambs" and the film of the same name. "Criminal Minds" is based out of Quantico. The CBS drama "" is also set in Quantico. The drama-thriller series,"Quantico", set at the FBI Academy premiered in 2015 on the ABC network. Lee Child's character Major Jack Reacher makes numerous references to Quantico, especially in the novel "The Visitor". It is also mentioned in the new drama series Mindhunter, premiered in 2017 on Netflix.




</doc>
<doc id="25349" url="https://en.wikipedia.org/wiki?curid=25349" title="QSIG">
QSIG

QSIG is an ISDN based signaling protocol for signaling between private branch exchanges (PBXs) in a private integrated services network (PISN). It makes use of the connection-level Q.931 protocol and the application-level ROSE protocol. ISDN "proper" functions as the physical link layer.

QSIG was originally developed by Ecma International, adopted by ETSI and is defined by a set of ISO standard documents, so is not owned by any company. This allows interoperability between communications platforms provided by disparate vendors. 

QSIG has two layers, called BC (basic call) and GF (generic function). QSIG BC describes how to set up calls between PBXs. QSIG GF provides supplementary services for large-scale corporate, educational, and government networks, such as line identification, call intrusion and call forwarding. Thus for a large or very distributed company that requires multiple PBXs, users can receive the same services across the network and be unaware of the switch that their telephone is connected to. This greatly eases the problems of management of large networks.

QSIG will likely never rival each vendor's private network protocols, but it does provide an option for a higher level of integration than that of the traditional choices.

Note: This list is not complete. See the "source" after the list for more information.

Source : ECMA - list of standards (search the list for PISN to find all QSIG related standards at ECMA)

QSIG basically uses ROSE to invoke specific supplementary service at the remote PINX. These ROSE operations are coded in a Q.931 FACILITY info element. Here a list of QSIG opcodes:

Source : European Telecommunications Standards Institute (ETSI)

Source : International Telecommunications Union (ITU)



</doc>
<doc id="25350" url="https://en.wikipedia.org/wiki?curid=25350" title="Quasicrystal">
Quasicrystal

A quasiperiodic crystal, or quasicrystal, is a structure that is ordered but not periodic. A quasicrystalline pattern can continuously fill all available space, but it lacks translational symmetry. While crystals, according to the classical crystallographic restriction theorem, can possess only two, three, four, and six-fold rotational symmetries, the Bragg diffraction pattern of quasicrystals shows sharp peaks with other symmetry orders, for instance five-fold.

Aperiodic tilings were discovered by mathematicians in the early 1960s, and, some twenty years later, they were found to apply to the study of quasicrystals. The discovery of these aperiodic forms in nature has produced a paradigm shift in the fields of crystallography. Quasicrystals had been investigated and observed earlier, but, until the 1980s, they were disregarded in favor of the prevailing views about the atomic structure of matter. In 2009, after a dedicated search, a mineralogical finding, icosahedrite, offered evidence for the existence of natural quasicrystals.

Roughly, an ordering is non-periodic if it lacks translational symmetry, which means that a shifted copy will never match exactly with its original. The more precise mathematical definition is that there is never translational symmetry in more than "n" – 1 linearly independent directions, where "n" is the dimension of the space filled, e.g., the three-dimensional tiling displayed in a quasicrystal may have translational symmetry in two dimensions. Symmetrical diffraction patterns result from the existence of an indefinitely large number of elements with a regular spacing, a property loosely described as long-range order. Experimentally, the aperiodicity is revealed in the unusual symmetry of the diffraction pattern, that is, symmetry of orders other than two, three, four, or six. 
In 1982 materials scientist Dan Shechtman observed that certain aluminium-manganese alloys produced the unusual diffractograms which today are seen as revelatory of quasicrystal structures. Due to fear of the scientific community's reaction, it took him two years to publish the results for which he was awarded the Nobel Prize in Chemistry in 2011.

In 1961, Hao Wang asked whether determining if a set of tiles admits a tiling of the plane is an algorithmically unsolvable problem or not. He conjectured that it is solvable, relying on the hypothesis that every set of tiles that can tile the plane can do it "periodically" (hence, it would suffice to try to tile bigger and bigger patterns until obtaining one that tiles periodically). Nevertheless, two years later, his student Robert Berger constructed a set of some 20,000 square tiles (now called Wang tiles) that can tile the plane but not in a periodic fashion. As further aperiodic sets of tiles were discovered, sets with fewer and fewer shapes were found. In 1976 Roger Penrose discovered a set of just two tiles, now referred to as Penrose tiles, that produced only non-periodic tilings of the plane. These tilings displayed instances of fivefold symmetry. One year later Alan Mackay showed experimentally that the diffraction pattern from the Penrose tiling had a two-dimensional Fourier transform consisting of sharp 'delta' peaks arranged in a fivefold symmetric pattern. Around the same time Robert Ammann created a set of aperiodic tiles that produced eightfold symmetry.

Mathematically, quasicrystals have been shown to be derivable from a general method that treats them as projections of a higher-dimensional lattice. Just as circles, ellipses, and hyperbolic curves in the plane can be obtained as sections from a three-dimensional double cone, so too various (aperiodic or periodic) arrangements in two and three dimensions can be obtained from postulated hyperlattices with four or more dimensions. Icosahedral quasicrystals in three dimensions were projected from a six-dimensional hypercubic lattice by Peter Kramer and Roberto Neri in 1984. The tiling is formed by two tiles with rhombohedral shape.

Shechtman first observed ten-fold electron diffraction patterns in 1982, as described in his notebook. The observation was made during a routine investigation, by electron microscopy, of a rapidly cooled alloy of aluminium and manganese prepared at the US National Bureau of Standards (later NIST).

In the summer of the same year Shechtman visited Ilan Blech and related his observation to him. Blech responded that such diffractions had been seen before. Around that time, Shechtman also related his finding to John Cahn of NIST who did not offer any explanation and challenged him to solve the observation. Shechtman quoted Cahn as saying: "Danny, this material is telling us something and I challenge you to find out what it is".

The observation of the ten-fold diffraction pattern lay unexplained for two years until the spring of 1984, when Blech asked Shechtman to show him his results again. A quick study of Shechtman's results showed that the common explanation for a ten-fold symmetrical diffraction pattern, the existence of twins, was ruled out by his experiments. Since periodicity and twins were ruled out, Blech, unaware of the two-dimensional tiling work, was looking for another possibility: a completely new structure containing cells connected to each other by defined angles and distances but without translational periodicity. Blech decided to use a computer simulation to calculate the diffraction intensity from a cluster of such a material without long-range translational order but still not random. He termed this new structure multiple polyhedral.

The idea of a new structure was the necessary paradigm shift to break the impasse. The “Eureka moment” came when the computer simulation showed sharp ten-fold diffraction patterns, similar to the observed ones, emanating from the three-dimensional structure devoid of periodicity. The multiple polyhedral structure was termed later by many researchers as icosahedral glass but in effect it embraces "any arrangement of polyhedra connected with definite angles and distances" (this general definition includes tiling, for example).

Shechtman accepted Blech's discovery of a new type of material and it gave him the courage to publish his experimental observation. Shechtman and Blech jointly wrote a paper entitled "The Microstructure of Rapidly Solidified AlMn" and sent it for publication around June 1984 to the "Journal of Applied Physics" (JAP). The JAP editor promptly rejected the paper as being better fit for a metallurgical readership. As a result, the same paper was re-submitted for publication to the "Metallurgical Transactions A", where it was accepted. Although not noted in the body of the published text, the published paper was slightly revised prior to publication.

Meanwhile, on seeing the draft of the Shechtman–Blech paper in the summer of 1984, John Cahn suggested that Shechtman's experimental results merit a fast publication in a more appropriate scientific journal. Shechtman agreed and, in hindsight, called this fast publication "a winning move”. This paper, published in the "Physical Review Letters" (PRL), repeated Shechtman's observation and used the same illustrations as the original Shechtman–Blech paper in the "Metallurgical Transactions A". The PRL paper, the first to appear in print, caused considerable excitement in the scientific community.

Next year Ishimasa "et al." reported twelvefold symmetry in Ni-Cr particles. Soon, eightfold diffraction patterns were recorded in V-Ni-Si and Cr-Ni-Si alloys. Over the years, hundreds of quasicrystals with various compositions and different symmetries have been discovered. The first quasicrystalline materials were thermodynamically unstable—when heated, they formed regular crystals. However, in 1987, the first of many stable quasicrystals were discovered, making it possible to produce large samples for study and opening the door to potential applications. In 2009, following a 10-year systematic search, scientists reported the first natural quasicrystal, a mineral found in the Khatyrka River in eastern Russia. This natural quasicrystal exhibits high crystalline quality, equalling the best artificial examples. The natural quasicrystal phase, with a composition of AlCuFe, was named icosahedrite and it was approved by the International Mineralogical Association in 2010. Furthermore, analysis indicates it may be meteoritic in origin, possibly delivered from a carbonaceous chondrite asteroid.
A further study of Khatyrka meteorites revealed micron-sized grains of another natural quasicrystal, which has a ten-fold symmetry and a chemical formula of AlNiFe. This quasicrystal is stable in a narrow temperature range, from 1120 to 1200 K at ambient pressure, which suggests that natural quasicrystals are formed by rapid quenching of a meteorite heated during an impact-induced shock.
In 1972 de Wolf and van Aalst reported that the diffraction pattern produced by a crystal of sodium carbonate cannot be labeled with three indices but needed one more, which implied that the underlying structure had four dimensions in reciprocal space. Other puzzling cases have been reported, but until the concept of quasicrystal came to be established, they were explained away or denied. However, at the end of the 1980s the idea became acceptable, and in 1992 the International Union of Crystallography altered its definition of a crystal, broadening it as a result of Shechtman’s findings, reducing it to the ability to produce a clear-cut diffraction pattern and acknowledging the possibility of the ordering to be either periodic or aperiodic. Now, the symmetries compatible with translations are defined as "crystallographic", leaving room for other "non-crystallographic" symmetries. Therefore, aperiodic or quasiperiodic structures can be divided into two main classes: those with crystallographic point-group symmetry, to which the incommensurately modulated structures and composite structures belong, and those with non-crystallographic point-group symmetry, to which quasicrystal structures belong.

Originally, the new form of matter was dubbed "Shechtmanite". The term "quasicrystal" was first used in print by Steinhardt and Levine shortly after Shechtman's paper was published.
The adjective "quasicrystalline" had already been in use, but now it came to be applied to any pattern with unusual symmetry. 'Quasiperiodical' structures were claimed to be observed in some decorative tilings devised by medieval Islamic architects. For example, Girih tiles in a medieval Islamic mosque in Isfahan, Iran, are arranged in a two-dimensional quasicrystalline pattern. These claims have, however, been under some debate.

Shechtman was awarded the Nobel Prize in Chemistry in 2011 for his work on quasicrystals. "His discovery of quasicrystals revealed a new principle for packing of atoms and molecules," stated the Nobel Committee and pointed that "this led to a paradigm shift within chemistry." 

There are several ways to mathematically define quasicrystalline patterns. One definition, the "cut and project" construction, is based on the work of Harald Bohr (mathematician brother of Niels Bohr). The concept of an almost periodic function (also called a quasiperiodic function) was studied by Bohr, including work of Bohl and Escanglon.
He introduced the notion of a superspace. Bohr showed that quasiperiodic functions arise as restrictions of high-dimensional periodic functions to an irrational slice (an intersection with one or more hyperplanes), and discussed their Fourier point spectrum. These functions are not exactly periodic, but they are arbitrarily close in some sense, as well as being a projection of an exactly periodic function.

In order that the quasicrystal itself be aperiodic, this slice must avoid any lattice plane of the higher-dimensional lattice. De Bruijn showed that Penrose tilings can be viewed as two-dimensional slices of five-dimensional hypercubic structures. Equivalently, the Fourier transform of such a quasicrystal is nonzero only at a dense set of points spanned by integer multiples of a finite set of basis vectors (the projections of the primitive reciprocal lattice vectors of the higher-dimensional lattice).
The intuitive considerations obtained from simple model aperiodic tilings are formally expressed in the concepts of Meyer and Delone sets. The mathematical counterpart of physical diffraction is the Fourier transform and the qualitative description of a diffraction picture as 'clear cut' or 'sharp' means that singularities are present in the Fourier spectrum. There are different methods to construct model quasicrystals. These are the same methods that produce aperiodic tilings with the additional constraint for the diffractive property. Thus, for a substitution tiling the eigenvalues of the substitution matrix should be Pisot numbers. The aperiodic structures obtained by the cut-and-project method are made diffractive by choosing a suitable orientation for the construction; this is a geometric approach that has also a great appeal for physicists.

Classical theory of crystals reduces crystals to point lattices where each point is the center of mass of one of the identical units of the crystal. The structure of crystals can be analyzed by defining an associated group. Quasicrystals, on the other hand, are composed of more than one type of unit, so, instead of lattices, quasilattices must be used. Instead of groups, groupoids, the mathematical generalization of groups in category theory, is the appropriate tool for studying quasicrystals.

Using mathematics for construction and analysis of quasicrystal structures is a difficult task for most experimentalists. Computer modeling, based on the existing theories of quasicrystals, however, greatly facilitated this task. Advanced programs have been developed allowing one to construct, visualize and analyze quasicrystal structures and their diffraction patterns.

Interacting spins were also analyzed in quasicrystals: AKLT Model and 8-vertex model were solved in quasicrystals analytically.

Study of quasicrystals may shed light on the most basic notions related to quantum critical point observed in heavy fermion metals. Experimental measurements on the gold-aluminium-ytterbium quasicrystal have revealed a quantum critical point defining the divergence of the magnetic susceptibility as temperature tends to zero. It is suggested that the electronic system of some quasicrystals is located at quantum critical point without tuning, while quasicrystals exhibit the typical scaling behaviour of their thermodynamic properties and belong to the famous family of heavy-fermion metals.

Since the original discovery by Dan Shechtman, hundreds of quasicrystals have been reported and confirmed. Undoubtedly, the quasicrystals are no longer a unique form of solid; they exist
universally in many metallic alloys and some polymers. Quasicrystals are found most often in aluminium alloys (Al-Li-Cu, Al-Mn-Si, Al-Ni-Co, Al-Pd-Mn, Al-Cu-Fe, Al-Cu-V, etc.), but numerous other compositions are also known (Cd-Yb, Ti-Zr-Ni, Zn-Mg-Ho, Zn-Mg-Sc, In-Ag-Yb, Pd-U-Si, etc.).

Two types of quasicrystals are known. The first type, polygonal (dihedral) quasicrystals, have an axis of 8, 10, or 12-fold local symmetry (octagonal, decagonal, or dodecagonal quasicrystals, respectively). They are periodic along this axis and quasiperiodic in planes normal to it. The second type, icosahedral quasicrystals, are aperiodic in all directions.

Quasicrystals fall into three groups of different thermal stability:

Except for the Al–Li–Cu system, all the stable quasicrystals are almost free of defects and disorder, as evidenced by X-ray and electron diffraction revealing peak widths as sharp as those of perfect crystals such as Si. Diffraction patterns exhibit fivefold, threefold, and twofold symmetries, and reflections are arranged quasiperiodically in three dimensions.

The origin of the stabilization mechanism is different for the stable and metastable quasicrystals. Nevertheless, there is a common feature observed in most quasicrystal-forming liquid alloys or their undercooled liquids: a local icosahedral order. The icosahedral order is in equilibrium in the "liquid state" for the stable quasicrystals, whereas the icosahedral order prevails in the "undercooled liquid state" for the metastable quasicrystals.

A nanoscale icosahedral phase was formed in Zr-, Cu- and Hf-based bulk metallic glasses alloyed with noble metals.

Most quasicrystals have ceramic-like properties including high thermal and electrical resistance, hardness and brittleness, resistance to corrosion, and non-stick
properties. Many metallic quasicrystalline substances are impractical for most applications due to their thermal instability; the Al-Cu-Fe ternary system and the Al-Cu-Fe-Cr and Al-Co-Fe-Cr quaternary systems, thermally stable up to 700 °C, are notable exceptions.

Quasicrystalline substances have potential applications in several forms.

Metallic quasicrystalline coatings can be applied by plasma-coating or magnetron sputtering. A problem that must be resolved is the tendency for cracking due to the materials' extreme brittleness. The cracking could be suppressed by reducing sample dimensions or coating thickness. Recent studies show typically brittle quasicrystals can exhibit remarkable ductility of over 50% strains at room temperature and sub-micrometer scales (<500 nm).

An application was the use of low-friction Al-Cu-Fe-Cr quasicrystals as a coating for frying pans. Food did not stick to it as much as to stainless steel making the pan moderately non-stick and easy to clean; heat transfer and durability were better than PTFE non-stick cookware and the pan was free from perfluorooctanoic acid (PFOA); the surface was very hard, claimed to be ten times harder than stainless steel, and not harmed by metal utensils or cleaning in a dishwasher; and the pan could withstand temperatures of without harm. However, cooking with a lot of salt would etch the quasicrystalline coating used, and the pans were eventually withdrawn from production. Shechtman had one of these pans.

The Nobel citation said that quasicrystals, while brittle, could reinforce steel "like armor". When Shechtman was asked about potential applications of quasicrystals he said that a precipitation-hardened stainless steel is produced that is strengthened by small quasicrystalline particles. It does not corrode and is extremely strong, suitable for razor blades and surgery instruments. The small quasicrystalline particles impede the motion of dislocation in the material.

Quasicrystals were also being used to develop heat insulation, LEDs, diesel engines, and new materials that convert heat to electricity. Shechtman suggested new applications taking advantage of the low coefficient of friction and the hardness of some quasicrystalline materials, for example embedding particles in plastic to make strong, hard-wearing, low-friction plastic gears. The low heat conductivity of some quasicrystals makes them good for heat insulating coatings.

Other potential applications include selective solar absorbers for power conversion, broad-wavelength reflectors, and bone repair and prostheses applications where biocompatibility, low friction and corrosion resistance are required. Magnetron sputtering can be readily applied to other stable quasicrystalline alloys such as Al-Pd-Mn.

While saying that the discovery of icosahedrite, the first quasicrystal found in nature, was important, Shechtman saw no practical applications.




</doc>
<doc id="25381" url="https://en.wikipedia.org/wiki?curid=25381" title="Recreation">
Recreation

Recreation is an activity of leisure, leisure being discretionary time. The "need to do something for recreation" is an essential element of human biology and psychology. Recreational activities are often done for enjoyment, amusement, or pleasure and are considered to be "fun".

The term "recreation" appears to have been used in English first in the late 14th century, first in the sense of "refreshment or curing of a sick person", and derived turn from Latin ("re": "again", "creare": "to create, bring forth, beget").

Humans spend their time in activities of daily living, work, sleep, social duties, and leisure, the latter time being free from prior commitments to physiologic or social needs, a prerequisite of recreation. Leisure has increased with increased longevity and, for many, with decreased hours spent for physical and economic survival, yet others argue that time pressure has increased for modern people, as they are committed to too many tasks. Other factors that account for an increased role of recreation are affluence, population trends, and increased commercialization of recreational offerings. While one perception is that leisure is just "spare time", time not consumed by the necessities of living, another holds that leisure is a force that allows individuals to consider and reflect on the values and realities that are missed in the activities of daily life, thus being an essential element of personal development and civilization. This direction of thought has even been extended to the view that leisure is the purpose of work, and a reward in itself, and "leisure life" reflects the values and character of a nation. Leisure is considered a human right under the Universal Declaration of Human Rights.

Recreation is difficult to separate from the general concept of play, which is usually the term for children's recreational activity. Children may playfully imitate activities that reflect the realities of adult life. It has been proposed that play or recreational activities are outlets of or expression of excess energy, channeling it into socially acceptable activities that fulfill individual as well as societal needs, without need for compulsion, and providing satisfaction and pleasure for the participant. A traditional view holds that work is supported by recreation, recreation being useful to "recharge the battery" so that work performance is improved. Work, an activity generally performed out of economic necessity and useful for society and organized within the economic framework, however can also be pleasurable and may be self-imposed thus blurring the distinction to recreation. Many activities may be work for one person and recreation for another, or, at an individual level, over time recreational activity may become work, and vice versa. Thus, for a musician, playing an instrument may be at one time a profession, and at another a recreation. Similarly, it may be difficult to separate education from recreation as in the case of recreational mathematics.

Recreation is an essential part of human life and finds many different forms which are shaped naturally by individual interests but also by the surrounding social construction. Recreational activities can be communal or solitary, active or passive, outdoors or indoors, healthy or harmful, and useful for society or detrimental. A significant section of recreational activities are designated as hobbies which are activities done for pleasure on a regular basis. A list of typical activities could be almost endless including most human activities, a few examples being reading, playing or listening to music, watching movies or TV, gardening, hunting, sports, studies, and travel. Some recreational activities - such as gambling, recreational drug use, or delinquent activities - may violate societal norms and laws.

Public space such as parks and beaches are essential venues for many recreational activities. Tourism has recognized that many visitors are specifically attracted by recreational offerings. In support of recreational activities government has taken an important role in their creation, maintenance, and organization, and whole industries have developed merchandise or services. Recreation-related business is an important factor in the economy; it has been estimated that the outdoor recreation sector alone contributes $730 billion annually to the U.S. economy and generates 6.5 million jobs.

Many recreational activities are organized, typically by public institutions, voluntary group-work agencies, private groups supported by membership fees, and commercial enterprises. Examples of each of these are the National Park Service, the YMCA, the Kiwanis, and Disney World.

Recreation has many health benefits, and, accordingly, Therapeutic Recreation has been developed to take advantage of this effect. The National Council for Therapeutic Recreation Certification (NCTRC) is the nationally recognized credentialing organization for the profession of Therapeutic Recreation. Professionals in the field of Therapeutic Recreation who are certified by the NCTRC are called "Certified Therapeutic Recreation Specialists". The job title "Recreation Therapist" is identified in the U.S. Dept of Labor's Occupation Outlook. Such therapy is applied in rehabilitation, psychiatric facilities for youth and adults, and in the care of the elderly, the disabled, or people with chronic diseases. Recreational physical activity is important to reduce obesity, and the risk of osteoporosis and of cancer, most significantly in men that of colon and prostate, and in women that of the breast; however, not all malignancies are reduced as outdoor recreation has been linked to a higher risk of melanoma. Extreme adventure recreation naturally carries its own hazards.

A recreation specialist would be expected to meet the recreational needs of a community or assigned interest group. Educational institutions offer courses that lead to a degree as a Bachelor of Arts in recreation management. People with such degrees often work in parks and recreation centers in towns, on community projects and activities. Networking with instructors, budgeting, and evaluation of continuing programs are common job duties.

In the United States, most states have a professional organization for continuing education and certification in recreation management. The National Recreation and Park Association administers a certification program called the CPRP (Certified Park and Recreation Professional) that is considered a national standard for professional recreation specialist practices.



</doc>
<doc id="25382" url="https://en.wikipedia.org/wiki?curid=25382" title="Recession">
Recession

In economics, a recession is a business cycle contraction which results in a general slowdown in economic activity. Macroeconomic indicators such as GDP (gross domestic product), investment spending, capacity utilization, household income, business profits, and inflation fall, while bankruptcies and the unemployment rate rise. In the United Kingdom, it is defined as a negative economic growth for two consecutive quarters.

Recessions generally occur when there is a widespread drop in spending (an adverse demand shock). This may be triggered by various events, such as a financial crisis, an external trade shock, an adverse supply shock or the bursting of an economic bubble. Governments usually respond to recessions by adopting expansionary macroeconomic policies, such as increasing money supply, increasing government spending and decreasing taxation.

In a 1974 "The New York Times" article, Commissioner of the Bureau of Labor Statistics Julius Shiskin suggested several rules of thumb for defining a recession, one of which was two down consecutive quarters of GDP. In time, the other rules of thumb were forgotten. Some economists prefer a definition of a 1.5-2 percentage points rise in unemployment within 12 months.

In the United States, the Business Cycle Dating Committee of the National Bureau of Economic Research (NBER) is generally seen as the authority for dating US recessions. The NBER defines an economic recession as: "a significant decline in economic activity spread across the economy, lasting more than a few months, normally visible in real GDP, real income, employment, industrial production, and wholesale-retail sales." Almost universally, academics, economists, policy makers, and businesses defer to the determination by the NBER for the precise dating of a recession's onset and end.

In the United Kingdom, recessions are generally defined as two consecutive quarters of negative economic growth, as measured by the seasonal adjusted quarter-on-quarter figures for real GDP. The exact same recession definition applies for all member states of the European Union.
A recession has many attributes that can occur simultaneously and includes declines in component measures of economic activity (GDP) such as consumption, investment, government spending, and net export activity. These summary measures reflect underlying drivers such as employment levels and skills, household savings rates, corporate investment decisions, interest rates, demographics, and government policies.

Economist Richard C. Koo wrote that under ideal conditions, a country's economy should have the household sector as net savers and the corporate sector as net borrowers, with the government budget nearly balanced and net exports near zero. When these relationships become imbalanced, recession can develop within the country or create pressure for recession in another country. Policy responses are often designed to drive the economy back towards this ideal state of balance.

A severe (GDP down by 10%) or prolonged (three or four years) recession is referred to as an economic depression, although some argue that their causes and cures can be different. As an informal shorthand, economists sometimes refer to different recession shapes, such as V-shaped, U-shaped, L-shaped and W-shaped recessions.

The type and shape of recessions are distinctive. In the US, v-shaped, or short-and-sharp contractions followed by rapid and sustained recovery, occurred in 1954 and 1990–91; U-shaped (prolonged slump) in 1974–75, and W-shaped, or double-dip recessions in 1949 and 1980–82. Japan’s 1993–94 recession was U-shaped and its 8-out-of-9 quarters of contraction in 1997–99 can be described as L-shaped. Korea, Hong Kong and South-east Asia experienced U-shaped recessions in 1997–98, although Thailand’s eight consecutive quarters of decline should be termed L-shaped.

Recessions have psychological and confidence aspects. For example, if companies expect economic activity to slow, they may reduce employment levels and save money rather than invest. Such expectations can create a self-reinforcing downward cycle, bringing about or worsening a recession. Consumer confidence is one measure used to evaluate economic sentiment. The term animal spirits has been used to describe the psychological factors underlying economic activity. Economist Robert J. Shiller wrote that the term "...refers also to the sense of trust we have in each other, our sense of fairness in economic dealings, and our sense of the extent of corruption and bad faith. When animal spirits are on ebb, consumers do not want to spend and businesses do not want to make capital expenditures or hire people."

High levels of indebtedness or the bursting of a real estate or financial asset price bubble can cause what is called a "balance sheet recession." This is when large numbers of consumers or corporations pay down debt (i.e., save) rather than spend or invest, which slows the economy. The term balance sheet derives from an accounting identity that holds that assets must always equal the sum of liabilities plus equity. If asset prices fall below the value of the debt incurred to purchase them, then the equity must be negative, meaning the consumer or corporation is insolvent. Economist Paul Krugman wrote in 2014 that "the best working hypothesis seems to be that the financial crisis was only one manifestation of a broader problem of excessive debt--that it was a so-called "balance sheet recession." In Krugman's view, such crises require debt reduction strategies combined with higher government spending to offset declines from the private sector as it pays down its debt.

For example, economist Richard Koo wrote that Japan's "Great Recession" that began in 1990 was a "balance sheet recession." It was triggered by a collapse in land and stock prices, which caused Japanese firms to have negative equity, meaning their assets were worth less than their liabilities. Despite zero interest rates and expansion of the money supply to encourage borrowing, Japanese corporations in aggregate opted to pay down their debts from their own business earnings rather than borrow to invest as firms typically do. Corporate investment, a key demand component of GDP, fell enormously (22% of GDP) between 1990 and its peak decline in 2003. Japanese firms overall became net savers after 1998, as opposed to borrowers. Koo argues that it was massive fiscal stimulus (borrowing and spending by the government) that offset this decline and enabled Japan to maintain its level of GDP. In his view, this avoided a U.S. type Great Depression, in which U.S. GDP fell by 46%. He argued that monetary policy was ineffective because there was limited demand for funds while firms paid down their liabilities. In a balance sheet recession, GDP declines by the amount of debt repayment and un-borrowed individual savings, leaving government stimulus spending as the primary remedy.

Krugman discussed the balance sheet recession concept during 2010, agreeing with Koo's situation assessment and view that sustained deficit spending when faced with a balance sheet recession would be appropriate. However, Krugman argued that monetary policy could also affect savings behavior, as inflation or credible promises of future inflation (generating negative real interest rates) would encourage less savings. In other words, people would tend to spend more rather than save if they believe inflation is on the horizon. In more technical terms, Krugman argues that the private sector savings curve is elastic even during a balance sheet recession (responsive to changes in real interest rates) disagreeing with Koo's view that it is inelastic (non-responsive to changes in real interest rates).

A July 2012 survey of balance sheet recession research reported that consumer demand and employment are affected by household leverage levels. Both durable and non-durable goods consumption declined as households moved from low to high leverage with the decline in property values experienced during the subprime mortgage crisis. Further, reduced consumption due to higher household leverage can account for a significant decline in employment levels. Policies that help reduce mortgage debt or household leverage could therefore have stimulative effects.

A liquidity trap is a Keynesian theory that a situation can develop in which interest rates reach near zero (zero interest-rate policy) yet do not effectively stimulate the economy. In theory, near-zero interest rates should encourage firms and consumers to borrow and spend. However, if too many individuals or corporations focus on saving or paying down debt rather than spending, lower interest rates have less effect on investment and consumption behavior; the lower interest rates are like "pushing on a string." Economist Paul Krugman described the U.S. 2009 recession and Japan's lost decade as liquidity traps. One remedy to a liquidity trap is expanding the money supply via quantitative easing or other techniques in which money is effectively printed to purchase assets, thereby creating inflationary expectations that cause savers to begin spending again. Government stimulus spending and mercantilist policies to stimulate exports and reduce imports are other techniques to stimulate demand. He estimated in March 2010 that developed countries representing 70% of the world's GDP were caught in a liquidity trap.

Behavior that may be optimal for an individual (e.g., saving more during adverse economic conditions) can be detrimental if too many individuals pursue the same behavior, as ultimately one person's consumption is another person's income. Too many consumers attempting to save (or pay down debt) simultaneously is called the paradox of thrift and can cause or deepen a recession. Economist Hyman Minsky also described a "paradox of deleveraging" as financial institutions that have too much leverage (debt relative to equity) cannot all de-leverage simultaneously without significant declines in the value of their assets.

During April 2009, U.S. Federal Reserve Vice Chair Janet Yellen discussed these paradoxes: "Once this massive credit crunch hit, it didn’t take long before we were in a recession. The recession, in turn, deepened the credit crunch as demand and employment fell, and credit losses of financial institutions surged. Indeed, we have been in the grips of precisely this adverse feedback loop for more than a year. A process of balance sheet deleveraging has spread to nearly every corner of the economy. Consumers are pulling back on purchases, especially on durable goods, to build their savings. Businesses are cancelling planned investments and laying off workers to preserve cash. And, financial institutions are shrinking assets to bolster capital and improve their chances of weathering the current storm. Once again, Minsky understood this dynamic. He spoke of the paradox of deleveraging, in which precautions that may be smart for individuals and firms—and indeed essential to return the economy to a normal state—nevertheless magnify the distress of the economy as a whole."

There are no known completely reliable predictors, but the following are considered possible predictors.

Most mainstream economists believe that recessions are caused by inadequate aggregate demand in the economy, and favor the use of expansionary macroeconomic policy during recessions. Strategies favored for moving an economy out of a recession vary depending on which economic school the policymakers follow. Monetarists would favor the use of expansionary monetary policy, while Keynesian economists may advocate increased government spending to spark economic growth. Supply-side economists may suggest tax cuts to promote business capital investment. When interest rates reach the boundary of an interest rate of zero percent (zero interest-rate policy) conventional monetary policy can no longer be used and government must use other measures to stimulate recovery. Keynesians argue that fiscal policy—tax cuts or increased government spending—works when monetary policy fails. Spending is more effective because of its larger multiplier but tax cuts take effect faster.

For example, Paul Krugman wrote in December 2010 that significant, sustained government spending was necessary because indebted households were paying down debts and unable to carry the U.S. economy as they had previously: "The root of our current troubles lies in the debt American families ran up during the Bush-era housing bubble...highly indebted Americans not only can’t spend the way they used to, they’re having to pay down the debts they ran up in the bubble years. This would be fine if someone else were taking up the slack. But what’s actually happening is that some people are spending much less while nobody is spending more — and this translates into a depressed economy and high unemployment. What the government should be doing in this situation is spending more while the private sector is spending less, supporting employment while those debts are paid down. And this government spending needs to be sustained..."

Some recessions have been anticipated by stock market declines. In "Stocks for the Long Run", Siegel mentions that since 1948, ten recessions were preceded by a stock market decline, by a lead time of 0 to 13 months (average 5.7 months), while ten stock market declines of greater than 10% in the Dow Jones Industrial Average were not followed by a recession.

The real-estate market also usually weakens before a recession. However real-estate declines can last much longer than recessions.

Since the business cycle is very hard to predict, Siegel argues that it is not possible to take advantage of economic cycles for timing investments. Even the National Bureau of Economic Research (NBER) takes a few months to determine if a peak or trough has occurred in the US.

During an economic decline, high yield stocks such as fast-moving consumer goods, pharmaceuticals, and tobacco tend to hold up better. However, when the economy starts to recover and the bottom of the market has passed (sometimes identified on charts as a MACD), growth stocks tend to recover faster. There is significant disagreement about how health care and utilities tend to recover. Diversifying one's portfolio into international stocks may provide some safety; however, economies that are closely correlated with that of the U.S. may also be affected by a recession in the U.S.

There is a view termed the "halfway rule" according to which investors start discounting an economic recovery about halfway through a recession. In the 16 U.S. recessions since 1919, the average length has been 13 months, although the recent recessions have been shorter. Thus if the 2008 recession followed the average, the downturn in the stock market would have bottomed around November 2008. The actual US stock market bottom of the 2008 recession was in March 2009.

Generally an administration gets credit or blame for the state of economy during its time. This has caused disagreements about on actually started. In an economic cycle, a downturn can be considered a consequence of an expansion reaching an unsustainable state, and is corrected by a brief decline. Thus it is not easy to isolate the causes of specific phases of the cycle.

The 1981 recession is thought to have been caused by the tight-money policy adopted by Paul Volcker, chairman of the Federal Reserve Board, before Ronald Reagan took office. Reagan supported that policy. Economist Walter Heller, chairman of the Council of Economic Advisers in the 1960s, said that "I call it a Reagan-Volcker-Carter recession. The resulting taming of inflation did, however, set the stage for a robust growth period during Reagan's administration.

Economists usually teach that to some degree recession is unavoidable, and its causes are not well understood. Consequently, modern government administrations attempt to take steps, also not agreed upon, to soften a recession.

Unemployment is particularly high during a recession. Many economists working within the neoclassical paradigm argue that there is a natural rate of unemployment which, when subtracted from the actual rate of unemployment, can be used to calculate the negative GDP gap during a recession. In other words, unemployment never reaches 0 percent, and thus is not a negative indicator of the health of an economy unless above the "natural rate," in which case it corresponds directly to a loss in gross domestic product, or GDP.
The full impact of a recession on employment may not be felt for several quarters. Research in Britain shows that low-skilled, low-educated workers and the young are most vulnerable to unemployment in a downturn. After recessions in Britain in the 1980s and 1990s, it took five years for unemployment to fall back to its original levels. Many companies often expect employment discrimination claims to rise during a recession.

Productivity tends to fall in the early stages of a recession, then rises again as weaker firms close. The variation in profitability between firms rises sharply. Recessions have also provided opportunities for anti-competitive mergers, with a negative impact on the wider economy: the suspension of competition policy in the United States in the 1930s may have extended the Great Depression.

The living standards of people dependent on wages and salaries are not more affected by recessions than those who rely on fixed incomes or welfare benefits. The loss of a job is known to have a negative impact on the stability of families, and individuals' health and well-being. Fixed income benefits receive small cuts which make it tougher to survive.

According to the International Monetary Fund (IMF), "Global recessions seem to occur over a cycle lasting between eight and 10 years." The IMF takes many factors into account when defining a global recession. Until April 2009, IMF several times communicated to the press, that a global annual real GDP growth of 3.0 percent or less in their view was "...equivalent to a global recession."
By this measure, six periods since 1970 qualify: 1974–1975, 1980–1983, 1990–1993, 1998, 2001–2002, and 2008–2009. During what IMF in April 2002 termed the past three global recessions of the last three decades, global per capita output growth was zero or negative, and IMF argued—at that time—that because of the opposite being found for 2001, the economic state in this year by itself did not qualify as a "global recession".

In April 2009, IMF had changed their Global recession definition to: 
By this new definition, a total of four global recessions took place since World War II: 1975, 1982, 1991 and 2009. All of them only lasted one year, although the third would have lasted three years (1991–93) if IMF as criteria had used the normal exchange rate weighted percapita real World GDP rather than the purchase power parity weighted percapita real World GDP.

The worst recession Australia has ever suffered happened in the beginning of the 1930s. As a result of late 1920s profit issues in agriculture and cutbacks, 1931-1932 saw Australia’s biggest recession in its entire history. It fared better than other nations, that underwent depressions, but their poor economic states influenced Australia’s as well, that depended on them for export, as well as foreign investments. The nation also benefited from bigger productivity in manufacturing, facilitated by trade protection, which also helped with feeling the effects less.
Due to a credit squeeze, the economy had gone into a brief recession in 1961
Australia was facing a rising level of inflation in 1973, caused partially by the oil crisis happening in that same year, which brought inflation at a 13% increase. Economic recession hit by the middle of the year 1974, with no change in policy enacted by the government as a measure to counter the economic situation of the country. Consequently, the unemployment level rose and the trade deficit increased significantly.

Another recession – the most recent one to date – came in the 1990s, at the beginning of the decade. It was the result of a major stock collapse in 1987, in October, referred to now as Black Monday. Although the collapse was larger than the one in 1929, the global economy recovered quickly, but North America still suffered a decline in lumbering savings and loans, which led to a crisis. The recession wasn’t limited to only America, but it also affected partnering nations, such as Australia. The unemployment level increased to 10.8%, employment declined by 3.4% and the GDP also decreased as much as 1.7%. Inflation, however, was successfully reduced.

The most recent recession to affect the United Kingdom was the late-2000s recession.

According to economists, since 1854, the U.S. has encountered 32 cycles of expansions and contractions, with an average of 17 months of contraction and 38 months of expansion. However, since 1980 there have been only eight periods of negative economic growth over one fiscal quarter or more, and four periods considered recessions:

For the past three recessions, the NBER decision has approximately conformed with the definition involving two consecutive quarters of decline. While the 2001 recession did not involve two consecutive quarters of decline, it was preceded by two quarters of alternating decline and weak growth.

Official economic data shows that a substantial number of nations were in recession as of early 2009. The US entered a recession at the end of 2007, and 2008 saw many other nations follow suit. The US recession of 2007 ended in June 2009 as the nation entered the current economic recovery.

The United States housing market correction (a possible consequence of United States housing bubble) and subprime mortgage crisis significantly contributed to a recession.

The 2007–2009 recession saw private consumption fall for the first time in nearly 20 years. This indicates the depth and severity of the current recession. With consumer confidence so low, recovery takes a long time. Consumers in the U.S. have been hard hit by the current recession, with the value of their houses dropping and their pension savings decimated on the stock market. Not only have consumers watched their wealth being eroded – they are now fearing for their jobs as unemployment rises.

U.S. employers shed 63,000 jobs in February 2008, the most in five years. Former Federal Reserve chairman Alan Greenspan said on 6 April 2008 that "There is more than a 50 percent chance the United States could go into recession." On 1 October, the Bureau of Economic Analysis reported that an additional 156,000 jobs had been lost in September. On 29 April 2008, Moody's declared that nine US states were in a recession. In November 2008, employers eliminated 533,000 jobs, the largest single month loss in 34 years. For 2008, an estimated 2.6 million U.S. jobs were eliminated.

The unemployment rate in the US grew to 8.5 percent in March 2009, and there were 5.1 million job losses until March 2009 since the recession began in December 2007. That was about five million more people unemployed compared to just a year prior, which was the largest annual jump in the number of unemployed persons since the 1940s.

Although the US Economy grew in the first quarter by 1%, by June 2008 some analysts stated that due to a protracted credit crisis and "...rampant inflation in commodities such as oil, food, and steel," the country was nonetheless in a recession. The third quarter of 2008 brought on a GDP retraction of 0.5% the biggest decline since 2001. The 6.4% decline in spending during Q3 on non-durable goods, like clothing and food, was the largest since 1950.

A 17 November 2008 report from the Federal Reserve Bank of Philadelphia based on the survey of 51 forecasters, suggested that the recession started in April 2008 and would last 14 months. They project real GDP declining at an annual rate of 2.9% in the fourth quarter and 1.1% in the first quarter of 2009. These forecasts represent significant downward revisions from the forecasts of three months ago.

A 1 December 2008, report from the National Bureau of Economic Research stated that the U.S. has been in a recession since December 2007 (when economic activity peaked), based on a number of measures including job losses, declines in personal income, and declines in real GDP. By July 2009 a growing number of economists believed that the recession may have ended. The National Bureau of Economic Research announced on 20 September 2010 that the 2008/2009 recession ended in June 2009, making it the longest recession since World War II.

Many other countries, particularly in Europe, have undergone negative GDP growth. Some countries have been able to avoid a recession but have still experienced slower economic activity, such as China. India and Australia were able to maintain positive growth throughout the late-2000s recession.

China had their stock market crash, which began with the popping of the stock market bubble on 12 July 2015.



</doc>
<doc id="25385" url="https://en.wikipedia.org/wiki?curid=25385" title="RSA (cryptosystem)">
RSA (cryptosystem)

RSA (Rivest–Shamir–Adleman) is one of the first public-key cryptosystems and is widely used for secure data transmission. In such a cryptosystem, the encryption key is public and it is different from the decryption key which is kept secret (private). In RSA, this asymmetry is based on the practical difficulty of the factorization of the product of two large prime numbers, the "factoring problem". The acronym RSA is made of the initial letters of the surnames of Ron Rivest, Adi Shamir, and Leonard Adleman, who first publicly described the algorithm in 1978. Clifford Cocks, an English mathematician working for the British intelligence agency Government Communications Headquarters (GCHQ), had developed an equivalent system in 1973, but this was not declassified until 1997.

A user of RSA creates and then publishes a public key based on two large prime numbers, along with an auxiliary value. The prime numbers must be kept secret. Anyone can use the public key to encrypt a message, but with currently published methods, and if the public key is large enough, only someone with knowledge of the prime numbers can decode the message feasibly.
Breaking RSA encryption is known as the RSA problem. Whether it is as difficult as the factoring problem remains an open question.

RSA is a relatively slow algorithm, and because of this, it is less commonly used to directly encrypt user data. More often, RSA passes encrypted shared keys for symmetric key cryptography which in turn can perform bulk encryption-decryption operations at much higher speed.

The idea of an asymmetric public-private key cryptosystem is attributed to Whitfield Diffie and Martin Hellman, who published this concept in 1976. They also introduced digital signatures and attempted to apply number theory. Their formulation used a shared-secret-key created from exponentiation of some number, modulo a prime number. However, they left open the problem of realizing a one-way function, possibly because the difficulty of factoring was not well-studied at the time.

Ron Rivest, Adi Shamir, and Leonard Adleman at the Massachusetts Institute of Technology made several attempts, over the course of a year, to create a one-way function that was hard to invert. Rivest and Shamir, as computer scientists, proposed many potential functions, while Adleman, as a mathematician, was responsible for finding their weaknesses. They tried many approaches including "knapsack-based" and "permutation polynomials". For a time, they thought what they wanted to achieve was impossible due to contradictory requirements. In April 1977, they spent Passover at the house of a student and drank a good deal of Manischewitz wine before returning to their homes at around midnight. Rivest, unable to sleep, lay on the couch with a math textbook and started thinking about their one-way function. He spent the rest of the night formalizing his idea, and he had much of the paper ready by daybreak. The algorithm is now known as RSA – the initials of their surnames in same order as their paper.

Clifford Cocks, an English mathematician working for the British intelligence agency Government Communications Headquarters (GCHQ), described an equivalent system in an internal document in 1973. However, given the relatively expensive computers needed to implement it at the time, RSA was considered to be mostly a curiosity and, as far as is publicly known, was never deployed. His discovery, however, was not revealed until 1997 due to its top-secret classification.

Kid-RSA (KRSA) is a simplified public-key cipher published in 1997, designed for educational purposes. Some people feel that learning Kid-RSA gives insight into RSA and other public-key ciphers, analogous to simplified DES.

MIT was granted for a "Cryptographic communications system and method" that used the algorithm, on September 20, 1983. Though the patent was going to expire on September 21, 2000 (the term of patent was 17 years at the time), the algorithm was released to the public domain by RSA Security on September 6, 2000, two weeks earlier. Since a paper describing the algorithm had been published in August 1977, prior to the December 1977 filing date of the patent application, regulations in much of the rest of the world precluded patents elsewhere and only the US patent was granted. Had Cocks's work been publicly known, a patent in the United States would not have been legal either.

From the DWPI's abstract of the patent,
The RSA algorithm involves four steps: key generation, key distribution, encryption and decryption.

A basic principle behind RSA is the observation that it is practical to find three very large positive integers , and such that with modular exponentiation for all integer (with ):

and that even knowing and or even it can be extremely difficult to find .

In addition, for some operations it is convenient that the order of the two exponentiations can be changed and that this relation also implies:

RSA involves a "public key" and a "private key." The public key can be known by everyone, and it is used for encrypting messages. The intention is that messages encrypted with the public key can only be decrypted in a reasonable amount of time by using the private key. The public key is represented by the integers and ; and, the private key, by the integer (although is also used during the decryption process. Thus, it might be considered to be a part of the private key, too). represents the message (previously prepared with a certain technique explained below).

The keys for the RSA algorithm are generated the following way:

The "public key" consists of the modulus "n" and the public (or encryption) exponent "e". The "private key" consists of the private (or decryption) exponent "d", which must be kept secret. "p", "q", and "λ"("n") must also be kept secret because they can be used to calculate "d".

In the original RSA paper, the Euler totient function is used instead of "λ"("n") for calculating the private exponent "d". Since "φ"("n") is always divisible by "λ"("n") the algorithm works as well. That the Euler totient function can be used can also seen as consequence of the Lagrange's theorem applied to the multiplicative group of integers modulo pq. Thus any "d" satisfying also satisfies . However, computing "d" modulo "φ"("n") will sometimes yield a result that is larger than necessary (i.e. ). Most of the implementations of RSA will accept exponents generated using either method (if they use the private exponent "d" at all, rather than using the optimized decryption method based on the Chinese remainder theorem described below), but some standards like FIPS 186-4 may require that . Any "oversized" private exponents not meeting that criterion may always be reduced modulo "λ"("n") to obtain a smaller equivalent exponent.

Since any common factors of and are present in the factorisation of = = , it is recommended that and have only very small common factors, if any besides the necessary 2.

Note: The authors of the original RSA paper carry out the key generation by choosing "d" and then computing "e" as the modular multiplicative inverse of "d" (modulo "φ"("n")). Since it is beneficial to use a small value for "e" (e.g., 65,537) in order to speed up the encryption function, current implementations of RSA, such as PKCS#1 choose "e" and compute "d" instead.

Suppose that Bob wants to send information to Alice. If they decide to use RSA, Bob must know Alice's public key to encrypt the message and Alice must use her private key to decrypt the message.
To enable Bob to send his encrypted messages, Alice transmits her public key to Bob via a reliable, but not necessarily secret, route. Alice's private key is never distributed.

After Bob obtains Alice's public key, he can send a message to Alice.

To do it, he first turns (strictly speaking, the un-padded plaintext) into an integer (strictly speaking, the padded plaintext), such that by using an agreed-upon reversible protocol known as a padding scheme. He then computes the ciphertext , using Alice's public key , corresponding to

This can be done reasonably quickly, even for 500-bit numbers, using modular exponentiation. Bob then transmits to Alice.
Alice can recover from by using her private key exponent by computing

Given , she can recover the original message by reversing the padding scheme.

Here is an example of RSA encryption and decryption. The parameters used here are artificially small, but one can also .


The public key is (, ). For a padded plaintext message "m", the encryption function is

The private key is (, ). For an encrypted ciphertext "c", the decryption function is

For instance, in order to encrypt , we calculate

To decrypt , we calculate

Both of these calculations can be computed efficiently using the square-and-multiply algorithm for modular exponentiation. In real-life situations the primes selected would be much larger; in our example it would be trivial to factor "n", 3233 (obtained from the freely available public key) back to the primes "p" and "q". "e", also from the public key, is then inverted to get "d", thus acquiring the private key.

Practical implementations use the Chinese remainder theorem to speed up the calculation using modulus of factors (mod "pq" using mod "p" and mod "q").

The values "d", "d" and "q", which are part of the private key are computed as follows:

Here is how "d", "d" and "q" are used for efficient decryption. (Encryption is efficient by choice of a suitable "d" and "e" pair)

A working example in JavaScript using BigInteger.js. This code should not be used in production, as codice_1 uses codice_2, which is not a cryptographically secure pseudorandom number generator.
'use strict';
const RSA = {};
RSA.generate = function(keysize) {

};
RSA.encrypt = function(m, n, e) {

RSA.decrypt = function(c, d, n) {

Suppose Alice uses Bob's public key to send him an encrypted message. In the message, she can claim to be Alice but Bob has no way of verifying that the message was actually from Alice since anyone can use Bob's public key to send him encrypted messages. In order to verify the origin of a message, RSA can also be used to sign a message.

Suppose Alice wishes to send a signed message to Bob. She can use her own private key to do so. She produces a hash value of the message, raises it to the power of "d" (modulo "n") (as she does when decrypting a message), and attaches it as a "signature" to the message. When Bob receives the signed message, he uses the same hash algorithm in conjunction with Alice's public key. He raises the signature to the power of "e" (modulo "n") (as he does when encrypting a message), and compares the resulting hash value with the message's actual hash value. If the two agree, he knows that the author of the message was in possession of Alice's private key, and that the message has not been tampered with since.

The proof of the correctness of RSA is based on Fermat's little theorem, stating that and if "p" is prime and "p" does not divide an integer "a".

We want to show that
for every integer "m" when "p" and "q" are distinct prime numbers and "e" and "d" are positive integers satisfying .

Since is, by construction, divisible by both and , we can write
for some nonnegative integers "h" and "k".

To check whether two numbers, like "m" and "m", are congruent mod "pq", it suffices (and in fact is equivalent) to check that they are congruent mod "p" and mod "q" separately. 

To show , we consider two cases:

The verification that proceeds in a completely analogous way: 

This completes the proof that, for any integer "m", and integers "e", "d" such that ,

"Notes:"
Although the original paper of Rivest, Shamir, and Adleman used Fermat's little theorem to explain why RSA works, it is common to find proofs that rely instead on Euler's theorem.

We want to show that , where is a product of two different prime numbers and "e" and "d" are positive integers satisfying . Since "e" and "d" are positive, we can write for some non-negative integer "h". "Assuming" that "m" is relatively prime to "n", we have

where the second-last congruence follows from Euler's theorem.

More generally, for any "e" and "d" satisfying , the same conclusion follows from Carmichael's generalization of Euler's theorem, which states that for all "m" relatively prime to "n".

When "m" is not relatively prime to "n", the argument just given is invalid. This is highly improbable (only a proportion of numbers have this property), but even in this case the desired congruence is still true. Either or , and these cases can be treated using the previous proof.

There are a number of attacks against plain RSA as described below.


To avoid these problems, practical RSA implementations typically embed some form of structured, randomized padding into the value "m" before encrypting it. This padding ensures that "m" does not fall into the range of insecure plaintexts, and that a given message, once padded, will encrypt to one of a large number of different possible ciphertexts.

Standards such as PKCS#1 have been carefully designed to securely pad messages prior to RSA encryption. Because these schemes pad the plaintext "m" with some number of additional bits, the size of the un-padded message "M" must be somewhat smaller. RSA padding schemes must be carefully designed so as to prevent sophisticated attacks which may be facilitated by a predictable message structure. Early versions of the PKCS#1 standard (up to version 1.5) used a construction that appears to make RSA semantically secure. However, at Crypto 1998, Bleichenbacher showed that this version is vulnerable to a practical adaptive chosen ciphertext attack. Furthermore, at Eurocrypt 2000, Coron et al. showed that for some types of messages, this padding does not provide a high enough level of security. Later versions of the standard include Optimal Asymmetric Encryption Padding (OAEP), which prevents these attacks. As such, OAEP should be used in any new application, and PKCS#1 v1.5 padding should be replaced wherever possible. The PKCS#1 standard also incorporates processing schemes designed to provide additional security for RSA signatures, e.g. the Probabilistic Signature Scheme for RSA (RSA-PSS).

Secure padding schemes such as RSA-PSS are as essential for the security of message signing as they are for message encryption. Two US patents on PSS were granted (USPTO 6266771 and USPTO 70360140); however, these patents expired on 24 July 2009 and 25 April 2010, respectively. Use of PSS no longer seems to be encumbered by patents. Note that using different RSA key-pairs for encryption and signing is potentially more secure.

For efficiency many popular crypto libraries (like OpenSSL, Java and .NET) use the following optimization for decryption and signing based on the Chinese remainder theorem. The following values are precomputed and stored as part of the private key:

These values allow the recipient to compute the exponentiation more efficiently as follows:

This is more efficient than computing exponentiation by squaring even though two modular exponentiations have to be computed. The reason is that these two modular exponentiations both use a smaller exponent and a smaller modulus.

The security of the RSA cryptosystem is based on two mathematical problems: the problem of factoring large numbers and the RSA problem. Full decryption of an RSA ciphertext is thought to be infeasible on the assumption that both of these problems are hard, i.e., no efficient algorithm exists for solving them. Providing security against "partial" decryption may require the addition of a secure padding scheme.

The RSA problem is defined as the task of taking "e"th roots modulo a composite "n": recovering a value "m" such that , where is an RSA public key and "c" is an RSA ciphertext. Currently the most promising approach to solving the RSA problem is to factor the modulus "n". With the ability to recover prime factors, an attacker can compute the secret exponent "d" from a public key , then decrypt "c" using the standard procedure. To accomplish this, an attacker factors "n" into "p" and "q", and computes which allows the determination of "d" from "e". No polynomial-time method for factoring large integers on a classical computer has yet been found, but it has not been proven that none exists. "See integer factorization for a discussion of this problem".

Multiple polynomial quadratic sieve (MPQS) can be used to factor the public modulus "n". The time taken to factor 128-bit and 256-bit "n" on a desktop computer are respectively 2 seconds and 35 minutes.

A tool called YAFU can be used to optimize this process. It took about 5720s to factor "320bit-N" on the same computer.

In 2009, Benjamin Moody factored an RSA-512 bit key in 73 days using only public software (GGNFS) and his desktop computer (a dual-core Athlon64 with a 1,900 MHz cpu.). Just less than five gigabytes of disk storage was required and about 2.5 gigabytes of RAM for the sieving process. The first RSA-512 factorization in 1999 required the equivalent of 8,400 MIPS years, over an elapsed time of about seven months.

Rivest, Shamir, and Adleman noted that Miller has shown that – assuming the truth of the Extended Riemann Hypothesis – finding "d" from "n" and "e" is as hard as factoring "n" into "p" and "q" (up to a polynomial time difference). However, Rivest, Shamir, and Adleman noted, in section IX/D of their paper, that they had not found a proof that inverting RSA is equally as hard as factoring.

, the largest factored RSA number was 768 bits long (232 decimal digits, see RSA-768). Its factorization, by a state-of-the-art distributed implementation, took around fifteen hundred CPU years (two years of real time, on many hundreds of computers). No larger RSA key is known publicly to have been factored. In practice, RSA keys are typically 1024 to 4096 bits long. Some experts believe that 1024-bit keys may become breakable in the near future or may already be breakable by a sufficiently well-funded attacker, though this is disputable. Few people see any way that 4096-bit keys could be broken in the foreseeable future. Therefore, it is generally presumed that RSA is secure if "n" is sufficiently large. If "n" is 300 bits or shorter, it can be factored in a few hours in a personal computer, using software already freely available. Keys of 512 bits have been shown to be practically breakable in 1999 when RSA-155 was factored by using several hundred computers, and these are now factored in a few weeks using common hardware. Exploits using 512-bit code-signing certificates that may have been factored were reported in 2011. A theoretical hardware device named TWIRL, described by Shamir and Tromer in 2003, called into question the security of 1024 bit keys. It is currently recommended that "n" be at least 2048 bits long.

In 1994, Peter Shor showed that a quantum computer – if one could ever be practically created for the purpose – would be able to factor in polynomial time, breaking RSA; see Shor's algorithm.

Finding the large primes "p" and "q" is usually done by testing random numbers of the right size with probabilistic primality tests that quickly eliminate virtually all of the nonprimes.

The numbers "p" and "q" should not be "too close", lest the Fermat factorization for "n" be successful. If "p" − "q" is less than 2"n" (n = p * q, which for even small 1024-bit values of "n" is ) solving for "p" and "q" is trivial. Furthermore, if either "p" − 1 or "q" − 1 has only small prime factors, "n" can be factored quickly by Pollard's p − 1 algorithm, and such values of "p" or "q" should hence be discarded.

It is important that the private exponent "d" be large enough. Michael J. Wiener showed that if "p" is between "q" and 2"q" (which is quite typical) and , then "d" can be computed efficiently from "n" and "e".

There is no known attack against small public exponents such as , provided that the proper padding is used. Coppersmith's Attack has many applications in attacking RSA specifically if the public exponent "e" is small and if the encrypted message is short and not padded. 65537 is a commonly used value for "e"; this value can be regarded as a compromise between avoiding potential small exponent attacks and still allowing efficient encryptions (or signature verification). The NIST Special Publication on Computer Security (SP 800-78 Rev 1 of August 2007) does not allow public exponents "e" smaller than 65537, but does not state a reason for this restriction.

In October 2017 a team of researchers from Masaryk University announced the ROCA vulnerability, which affects RSA keys generated by an algorithm embodied in a library from Infineon. Large number of smart cards and TPMs were shown to be affected. Vulnerable RSA keys are easily identified using a test program the team released.

A cryptographically strong random number generator, which has been properly seeded with adequate entropy, must be used to generate the primes "p" and "q". An analysis comparing millions of public keys gathered from the Internet was carried out in early 2012 by Arjen K. Lenstra, James P. Hughes, Maxime Augier, Joppe W. Bos, Thorsten Kleinjung and Christophe Wachter. They were able to factor 0.2% of the keys using only Euclid's algorithm.

They exploited a weakness unique to cryptosystems based on integer factorization. If is one public key and is another, then if by chance (but q is not equal to q'), then a simple computation of factors both "n" and "n"′, totally compromising both keys. Lenstra et al. note that this problem can be minimized by using a strong random seed of bit-length twice the intended security level, or by employing a deterministic function to choose "q" given "p", instead of choosing "p" and "q" independently.

Nadia Heninger was part of a group that did a similar experiment. They used an idea of Daniel J. Bernstein to compute the GCD of each RSA key "n" against the product of all the other keys "n"′ they had found (a 729 million digit number), instead of computing each gcd("n","n"′) separately, thereby achieving a very significant speedup since after one large division the GCD problem is of normal size.

Heninger says in her blog that the bad keys occurred almost entirely in embedded applications, including "firewalls, routers, VPN devices, remote server administration devices, printers, projectors, and VOIP phones" from over 30 manufacturers. Heninger explains that the one-shared-prime problem uncovered by the two groups results from situations where the pseudorandom number generator is poorly seeded initially and then reseeded between the generation of the first and second primes. Using seeds of sufficiently high entropy obtained from key stroke timings or electronic diode noise or atmospheric noise from a radio receiver tuned between stations should solve the problem.

Strong random number generation is important throughout every phase of public key cryptography. For instance, if a weak generator is used for the symmetric keys that are being distributed by RSA, then an eavesdropper could bypass RSA and guess the symmetric keys directly.

Kocher described a new attack on RSA in 1995: if the attacker Eve knows Alice's hardware in sufficient detail and is able to measure the decryption times for several known ciphertexts, she can deduce the decryption key "d" quickly. This attack can also be applied against the RSA signature scheme. In 2003, Boneh and Brumley demonstrated a more practical attack capable of recovering RSA factorizations over a network connection (e.g., from a Secure Sockets Layer (SSL)-enabled webserver) This attack takes advantage of information leaked by the Chinese remainder theorem optimization used by many RSA implementations.

One way to thwart these attacks is to ensure that the decryption operation takes a constant amount of time for every ciphertext. However, this approach can significantly reduce performance. Instead, most RSA implementations use an alternate technique known as cryptographic blinding. RSA blinding makes use of the multiplicative property of RSA. Instead of computing , Alice first chooses a secret random value "r" and computes . The result of this computation after applying Euler's Theorem is and so the effect of "r" can be removed by multiplying by its inverse. A new value of "r" is chosen for each ciphertext. With blinding applied, the decryption time is no longer correlated to the value of the input ciphertext and so the timing attack fails.

In 1998, Daniel Bleichenbacher described the first practical adaptive chosen ciphertext attack, against RSA-encrypted messages using the PKCS #1 v1 padding scheme (a padding scheme randomizes and adds structure to an RSA-encrypted message, so it is possible to determine whether a decrypted message is valid). Due to flaws with the PKCS #1 scheme, Bleichenbacher was able to mount a practical attack against RSA implementations of the Secure Socket Layer protocol, and to recover session keys. As a result of this work, cryptographers now recommend the use of provably secure padding schemes such as Optimal Asymmetric Encryption Padding, and RSA Laboratories has released new versions of PKCS #1 that are not vulnerable to these attacks.

A side-channel attack using branch prediction analysis (BPA) has been described. Many processors use a branch predictor to determine whether a conditional branch in the instruction flow of a program is likely to be taken or not. Often these processors also implement simultaneous multithreading (SMT). Branch prediction analysis attacks use a spy process to discover (statistically) the private key when processed with these processors.

Simple Branch Prediction Analysis (SBPA) claims to improve BPA in a non-statistical way. In their paper, "On the Power of Simple Branch Prediction Analysis", the authors of SBPA (Onur Aciicmez and Cetin Kaya Koc) claim to have discovered 508 out of 512 bits of an RSA key in 10 iterations.

A power fault attack on RSA implementations has been described in 2010. The author recovered the key by varying the CPU power voltage outside limits; this caused multiple power faults on the server.





</doc>
<doc id="25389" url="https://en.wikipedia.org/wiki?curid=25389" title="Robert A. Heinlein">
Robert A. Heinlein

Robert Anson Heinlein (; July 7, 1907 – May 8, 1988) was an American science-fiction writer. Often called the "dean of science fiction writers", he wrote sometimes controversial works which continue to have an influential effect on the science-fiction genre, and on modern culture more generally.

Heinlein became one of the first American science-fiction writers to break into mainstream magazines such as "The Saturday Evening Post" in the late 1940s. He was one of the best-selling science-fiction novelists for many decades, and he, Isaac Asimov, and Arthur C. Clarke are often considered the "Big Three" of English-language science fiction authors. Notable Heinlein works include "Stranger in a Strange Land", "Starship Troopers" (which helped mould the space marine and mecha archetypes) and "The Moon Is a Harsh Mistress".

A writer also of numerous science-fiction short stories, Heinlein was one of a group of writers who came to prominence under the editorship (1937-1971) of John W. Campbell at "Astounding Science Fiction" magazine; though Heinlein denied that Campbell influenced his writing to any great degree.

Within the framework of his science-fiction stories, Heinlein repeatedly addressed certain social themes: the importance of individual liberty and self-reliance, the obligation individuals owe to their societies, the influence of organized religion on culture and government, and the tendency of society to repress nonconformist thought. He also speculated on the influence of space travel on human cultural practices.

Heinlein was named the first Science Fiction Writers Grand Master in 1974. Four of his novels won Hugo Awards, a different four won Nebula Awards. In addition, fifty years after publication, five of his works were awarded "Retro Hugos"—awards given retrospectively for works that were published before the Hugo Awards came into existence. In his fiction, Heinlein coined terms that have become part of the English language, including "grok", "waldo", and "speculative fiction", as well as popularizing existing terms like "TANSTAAFL", "pay it forward", and "space marine". He also anticipated mechanical computer-aided design with "Drafting Dan" and described a modern version of a waterbed in his novel "The Door into Summer", though he never patented nor built one. In the first chapter of the novel "Space Cadet" he anticipated the cell-phone, 35 years before Motorola invented the technology. Several of Heinlein's works have been adapted for film and television.

Heinlein was born on July 7, 1907 to Rex Ivar Heinlein (an accountant) and Bam Lyle Heinlein, in Butler, Missouri. He was a 6th-generation German-American: a family tradition had it that Heinleins fought in every American war starting with the War of Independence.

His childhood was spent in Kansas City, Missouri. The outlook and values of this time and place (in his own words, "The Bible Belt") had a definite influence on his fiction, especially his later works, as he drew heavily upon his childhood in establishing the setting and cultural atmosphere in works like "Time Enough for Love" and "To Sail Beyond the Sunset".

Heinlein's experience in the U.S. Navy exerted a strong influence on his character and writing. He graduated from the U.S. Naval Academy in Annapolis, Maryland, with the class of 1929 and went on to serve as an officer in the Navy. He was assigned to the new aircraft carrier in 1931, where he worked in radio communications, then in its earlier phases, with the carrier's aircraft. The captain of this carrier was Ernest J. King, who served as the Chief of Naval Operations and Commander-in-Chief, U.S. Fleet during World War II. Heinlein was frequently interviewed during his later years by military historians who asked him about Captain King and his service as the commander of the U.S. Navy's first modern aircraft carrier.

Heinlein also served aboard the destroyer in 1933 and 1934, reaching the rank of lieutenant. His brother, Lawrence Heinlein, served in the U.S. Army, the U.S. Air Force, and the Missouri National Guard, and reaching the rank of major general in the National Guard.

In 1929, Heinlein married Elinor Curry of Kansas City. However, their marriage only lasted about a year. His second marriage in 1932 to Leslyn MacDonald (1904–1981) lasted for 15 years. MacDonald was, according to the testimony of Heinlein's Navy friend, Rear Admiral Cal Laning, "astonishingly intelligent, widely read, and extremely liberal, though a registered Republican," while Isaac Asimov later recalled that Heinlein was, at the time, "a flaming liberal". "(See section: Politics of Robert Heinlein.)"

In 1934, Heinlein was discharged from the Navy due to pulmonary tuberculosis. During a lengthy hospitalization, he developed a design for a waterbed.

After his discharge, Heinlein attended a few weeks of graduate classes in mathematics and physics at the University of California at Los Angeles (UCLA), but he soon quit either because of his health or from a desire to enter politics.

Heinlein supported himself at several occupations, including real estate sales and silver mining, but for some years found money in short supply. Heinlein was active in Upton Sinclair's socialist End Poverty in California movement in the early 1930s. When Sinclair gained the Democratic nomination for Governor of California in 1934, Heinlein worked actively in the campaign. Heinlein himself ran for the California State Assembly in 1938, but was unsuccessful.

While not destitute after the campaign — he had a small disability pension from the Navy — Heinlein turned to writing to pay off his mortgage. His first published story, "Life-Line", was printed in the August 1939 issue of "Astounding Science Fiction". Originally written for a contest, he sold it to "Astounding" for significantly more than the contest's first-prize payoff. Another Future History story, "Misfit", followed in November. Others saw Heinlein's talent and stardom from his first story, and he was quickly acknowledged as a leader of the new movement toward "social" science fiction. In California he hosted the Mañana Literary Society, a 1940–41 series of informal gatherings of new authors. He was the guest of honor at Denvention, the 1941 Worldcon, held in Denver. During World War II, he did aeronautical engineering for the U.S. Navy, also recruiting Isaac Asimov and L. Sprague de Camp to work at the Philadelphia Naval Shipyard in Pennsylvania.

As the war wound down in 1945, Heinlein began to re-evaluate his career. The atomic bombings of Hiroshima and Nagasaki, along with the outbreak of the Cold War, galvanized him to write nonfiction on political topics. In addition, he wanted to break into better-paying markets. He published four influential short stories for "The Saturday Evening Post" magazine, leading off, in February 1947, with "The Green Hills of Earth". That made him the first science fiction writer to break out of the "pulp ghetto". In 1950, the movie "Destination Moon" — the documentary-like film for which he had written the story and scenario, co-written the script, and invented many of the effects — won an Academy Award for special effects. Also, he embarked on a series of juvenile novels for the Charles Scribner's Sons publishing company that went from 1947 through 1959, at the rate of one book each autumn, in time for Christmas presents to teenagers. He also wrote for "Boys' Life" in 1952.

At the Philadelphia Naval Shipyard he met and befriended a chemical engineer named Virginia "Ginny" Gerstenfeld. After the war, her engagement having fallen through, she moved to UCLA for doctoral studies in chemistry and made contact again.

As his second wife's alcoholism gradually spun out of control, Heinlein moved out and the couple filed for divorce. Heinlein's friendship with Virginia turned into a relationship and on October 21, 1948 — shortly after the decree nisi came through — they married in the town of Raton, New Mexico, shortly after setting up housekeeping in Colorado. They remained married until Heinlein's death.

As Heinlein's increasing success as a writer resolved their initial financial woes, they had a house custom built with various innovative features, later described in an article in "Popular Mechanics". In 1965, after various chronic health problems of Virginia's were traced back to altitude sickness, they moved to Santa Cruz, California, which is at sea level. They built a new residence in the adjacent village of Bonny Doon, California. Robert and Virginia designed and built their California house themselves, which is in a circular shape. Previously they had also designed and built their Colorado house.

Ginny undoubtedly served as a model for many of his intelligent, fiercely independent female characters. She was a chemist, rocket test engineer, and held a higher rank in the Navy than Heinlein himself. She was also an accomplished college athlete, earning four letters. In 1953–1954, the Heinleins voyaged around the world (mostly via ocean liners and cargo liners, as Ginny detested flying), which Heinlein described in "Tramp Royale", and which also provided background material for science fiction novels set aboard spaceships on long voyages, such as "Podkayne of Mars", "Friday" and , the latter initially being set on a cruise much as detailed in "Tramp Royale". Ginny acted as the first reader of his manuscripts. Isaac Asimov believed that Heinlein made a swing to the right politically at the same time he married Ginny.

The Heinleins formed the small "Patrick Henry League" in 1958, and they worked in the 1964 Barry Goldwater Presidential campaign.

Heinlein had used topical materials throughout his juvenile series beginning in 1947, but in 1959, his novel "Starship Troopers" was considered by the editors and owners of Scribner's to be too controversial for one of its prestige lines, and it was rejected.

Heinlein found another publisher (Putnam), feeling himself released from the constraints of writing novels for children. He had told an interviewer that he did not want to do stories that merely added to categories defined by other works. Rather he wanted to do his own work, stating that: "I want to do my own stuff, my own way". He would go on to write a series of challenging books that redrew the boundaries of science fiction, including "Stranger in a Strange Land" (1961) and "The Moon Is a Harsh Mistress" (1966).

Beginning in 1970, Heinlein had a series of health crises, broken by strenuous periods of activity in his hobby of stonemasonry. (In a private correspondence, he referred to that as his "usual and favorite occupation between books".) The decade began with a life-threatening attack of peritonitis, recovery from which required more than two years, and treatment of which required multiple transfusions of Heinlein's rare blood type, A2 negative. As soon as he was well enough to write again, he began work on "Time Enough for Love" (1973), which introduced many of the themes found in his later fiction.

In the mid-1970s, Heinlein wrote two articles for the "Britannica Compton Yearbook". He and Ginny crisscrossed the country helping to reorganize blood donation in the United States in an effort to assist the system which had saved his life. At science fiction conventions to receive his autograph, fans would be asked to co-sign with Heinlein a beautifully embellished pledge form he supplied stating that the recipient agrees that they will donate blood. He was the guest of honor at the Worldcon in 1976 for the third time at MidAmeriCon in Kansas City, Missouri. At that Worldcon, Heinlein hosted a blood drive and donors' reception to thank all those who had helped save lives. While vacationing in Tahiti in early 1978, he suffered a transient ischemic attack. Over the next few months, he became more and more exhausted, and his health again began to decline. The problem was determined to be a blocked carotid artery, and he had one of the earliest known carotid bypass operations to correct it. Heinlein and Virginia had been smokers, and smoking appears often in his fiction, as do fictitious strikable self-lighting cigarettes.

In 1980 Robert Heinlein was a member of the Citizens Advisory Council on National Space Policy, chaired by Jerry Pournelle, which met at the home of SF writer Larry Niven to write space policy papers for the incoming Reagan Administration. Members included such aerospace industry leaders as former astronaut Buzz Aldrin, General Daniel O. Graham, aerospace engineer Max Hunter and North American VP and Space Shuttle manager George Merrick. Policy recommendations from the Council included ballistic missile defense concepts which were later transformed into what was called the Strategic Defense Initiative by those who favored it, and "Star Wars" as a term of derision coined by Senator Ted Kennedy. Heinlein assisted with Council contribution to the Reagan "Star Wars" speech of Spring 1983.

Asked to appear before a Joint Committee of the U.S. House and Senate that year, he testified on his belief that spin-offs from space technology were benefiting the infirm and the elderly. Heinlein's surgical treatment re-energized him, and he wrote five novels from 1980 until he died in his sleep from emphysema and heart failure on May 8, 1988.

At that time, he had been putting together the early notes for another "World as Myth" novel. Several of his other works have been published posthumously.

After his death, his wife Virginia Heinlein issued a compilation of Heinlein's correspondence and notes into a somewhat autobiographical examination of his career, published in 1989 under the title "Grumbles from the Grave". Heinlein's archive is housed by the Special Collections department of McHenry Library at the University of California at Santa Cruz. The collection includes manuscript drafts, correspondence, photographs and artifacts. A substantial portion of the archive has been digitized and it is available online through the Robert A. and Virginia Heinlein Archives.

Heinlein published 32 novels, 59 short stories, and 16 collections during his life. Four films, two television series, several episodes of a radio series, and a board game have been derived more or less directly from his work. He wrote a screenplay for one of the films. Heinlein edited an anthology of other writers' SF short stories.

Three nonfiction books and two poems have been published posthumously. "For Us, The Living: A Comedy of Customs" was published posthumously in 2003; "Variable Star", written by Spider Robinson based on an extensive outline by Heinlein, was published in September 2006. Four collections have been published posthumously.

Over the course of his career Heinlein wrote three somewhat overlapping series.

Heinlein began his career as a writer of stories for "Astounding Science Fiction" magazine, which was edited by John Campbell. The science fiction writer Frederik Pohl has described Heinlein as "that greatest of Campbell-era sf writers". Isaac Asimov said that, from the time of his first story, the science fiction world accepted that Heinlein was the best science fiction writer in existence, adding that he would hold this title through his lifetime.

Alexei and Cory Panshin noted that Heinlein's impact was immediately felt. In 1940, the year after selling 'Life-Line' to Campbell, he wrote three short novels, four novelettes, and seven short stories. They went on to say that "No one ever dominated the science fiction field as Bob did in the first few years of his career." Alexei expresses awe in Heinlein's ability to show readers a world so drastically different from the one we live in now, yet have so many similarities. He says that "We find ourselves not only in a world other than our own, but identifying with a living, breathing individual who is operating within its context, and thinking and acting according to its terms."

The first novel that Heinlein wrote, "" (1939), did not see print during his lifetime, but Robert James tracked down the manuscript and it was published in 2003. Though some regard it as a failure as a novel, considering it little more than a disguised lecture on Heinlein's social theories, some readers took a very different view. In a review of it, John Clute wrote: "I'm not about to suggest that if Heinlein had been able to publish [such works] openly in the pages of Astounding in 1939, SF would have gotten the future right; I would suggest, however, that if Heinlein, and his colleagues, had been able to publish adult SF in Astounding and its fellow journals, then SF might not have done such a grotesquely poor job of prefiguring something of the flavor of actually living here at the onset of 2004."

"For Us, the Living" was intriguing as a window into the development of Heinlein's radical ideas about man as a social animal, including his interest in free love. The root of many themes found in his later stories can be found in this book. It also contained a large amount of material that could be considered background for his other novels. This included a detailed description of the protagonist's treatment to avoid being banned to Coventry (a lawless land in the Heinlein mythos where unrepentant law-breakers are exiled).
It appears that Heinlein at least attempted to live in a manner consistent with these ideals, even in the 1930s, and had an open relationship in his marriage to his second wife, Leslyn. He was also a nudist; nudism and body taboos are frequently discussed in his work. At the height of the Cold War, he built a bomb shelter under his house, like the one featured in "Farnham's Freehold".

After "For Us, The Living", Heinlein began selling (to magazines) first short stories, then novels, set in a Future History, complete with a time line of significant political, cultural, and technological changes. A chart of the future history was published in the May 1941 issue of "Astounding". Over time, Heinlein wrote many novels and short stories that deviated freely from the Future History on some points, while maintaining consistency in some other areas. The Future History was eventually overtaken by actual events. These discrepancies were explained, after a fashion, in his later World as Myth stories.

Heinlein's first novel published as a book, "Rocket Ship Galileo", was initially rejected because going to the moon was considered too far-fetched, but he soon found a publisher, Scribner's, that began publishing a Heinlein juvenile once a year for the Christmas season. Eight of these books were illustrated by Clifford Geary in a distinctive white-on-black scratchboard style. Some representative novels of this type are "Have Space Suit—Will Travel", "Farmer in the Sky", and "Starman Jones". Many of these were first published in serial form under other titles, e.g., "Farmer in the Sky" was published as "Satellite Scout" in the Boy Scout magazine "Boys' Life". There has been speculation that Heinlein's intense obsession with his privacy was due at least in part to the apparent contradiction between his unconventional private life and his career as an author of books for children. However, "For Us, The Living" explicitly discusses the political importance Heinlein attached to privacy as a matter of principle thus negating this line of reasoning.

The novels that Heinlein wrote for a young audience are commonly called "the Heinlein juveniles", and they feature a mixture of adolescent and adult themes. Many of the issues that he takes on in these books have to do with the kinds of problems that adolescents experience. His protagonists are usually very intelligent teenagers who have to make their way in the adult society they see around them. On the surface, they are simple tales of adventure, achievement, and dealing with stupid teachers and jealous peers. Heinlein was a vocal proponent of the notion that juvenile readers were far more sophisticated and able to handle more complex or difficult themes than most people realized. His juvenile stories often had a maturity to them that made them readable for adults. "Red Planet", for example, portrays some very subversive themes, including a revolution in which young students are involved; his editor demanded substantial changes in this book's discussion of topics such as the use of weapons by children and the misidentified sex of the Martian character. Heinlein was always aware of the editorial limitations put in place by the editors of his novels and stories, and while he observed those restrictions on the surface, was often successful in introducing ideas not often seen in other authors' juvenile SF.

In 1957, James Blish wrote that one reason for Heinlein's success "has been the high grade of machinery which goes, today as always, into his story-telling. Heinlein seems to have known from the beginning, as if instinctively, technical lessons about fiction which other writers must learn the hard way (or often enough, never learn). He does not always operate the machinery to the best advantage, but he always seems to be aware of it."

Heinlein decisively ended his juvenile novels with "Starship Troopers" (1959), a controversial work and his personal riposte to leftists calling for President Dwight D. Eisenhower to stop nuclear testing in 1958. "The "Patrick Henry" ad shocked 'em," he wrote many years later. ""Starship Troopers" outraged 'em." "Starship Troopers" is a coming-of-age story about duty, citizenship, and the role of the military in society. The book portrays a society in which suffrage is earned by demonstrated willingness to place society's interests before one's own, at least for a short time and often under onerous circumstances, in government service; in the case of the protagonist, this was military service.

Later, in "Expanded Universe", Heinlein said that it was his intention in the novel that service could include positions outside strictly military functions such as teachers, police officers, and other government positions. This is presented in the novel as an outgrowth of the failure of unearned suffrage government and as a very successful arrangement. In addition, the franchise was only awarded after leaving the assigned service, thus those serving their terms—in the military, or any other service—were excluded from exercising any franchise. Career military were completely disenfranchised until retirement.

The name "Starship Troopers" was licensed for an unrelated, B movie script called "Bug Hunt at Outpost Nine", which was then retitled to benefit from the book's credibility. The resulting film, entitled "Starship Troopers" (1997), which was written by Ed Neumeier and directed by Paul Verhoeven, had little relationship to the book, beyond the inclusion of character names, the depiction of space marines, and the concept of suffrage earned by military service. Fans of Heinlein were critical of the movie, which they considered a betrayal of Heinlein's philosophy, presenting the society in which the story takes place as fascist.

Likewise, the powered armor technology that is not only central to the book, but became a standard subgenre of science fiction thereafter, is completely absent in the movie, where the characters use World War II-technology weapons and wear light combat gear little more advanced than that. In Verhoeven's movie of the same name, there is no battle armor. Verhoeven commented that he had tried to read the book after he had bought the rights to it, in order to add it to his existing movie. However he read only the first two chapters, finding it too boring to continue. He thought it was a bad book and asked Ed Neumeier to tell him the story because he couldn't read it.

From about 1961 ("Stranger in a Strange Land") to 1973 ("Time Enough for Love"), Heinlein explored some of his most important themes, such as individualism, libertarianism, and free expression of physical and emotional love. Three novels from this period, "Stranger in a Strange Land", "The Moon Is a Harsh Mistress", and "Time Enough for Love", won the Libertarian Futurist Society's Prometheus Hall of Fame Award, designed to honor classic libertarian fiction. Jeff Riggenbach described "The Moon Is a Harsh Mistress" as "unquestionably one of the three or four most influential libertarian novels of the last century".

Heinlein did not publish "Stranger in a Strange Land" until some time after it was written, and the themes of free love and radical individualism are prominently featured in his long-unpublished first novel, "For Us, The Living: A Comedy of Customs".

"The Moon Is a Harsh Mistress" tells of a war of independence waged by the Lunar penal colonies, with significant comments from a major character, Professor La Paz, regarding the threat posed by government to individual freedom.

Although Heinlein had previously written a few short stories in the fantasy genre, during this period he wrote his first fantasy novel, "Glory Road", and in "Stranger in a Strange Land" and "I Will Fear No Evil", he began to mix hard science with fantasy, mysticism, and satire of organized religion. Critics William H. Patterson, Jr., and Andrew Thornton believe that this is simply an expression of Heinlein's longstanding philosophical opposition to positivism. Heinlein stated that he was influenced by James Branch Cabell in taking this new literary direction. The penultimate novel of this period, "I Will Fear No Evil", is according to critic James Gifford "almost universally regarded as a literary failure" and he attributes its shortcomings to Heinlein's near-death from peritonitis.

After a seven-year hiatus brought on by poor health, Heinlein produced five new novels in the period from 1980 ("The Number of the Beast") to 1987 ("To Sail Beyond the Sunset"). These books have a thread of common characters and time and place. They most explicitly communicated Heinlein's philosophies and beliefs, and many long, didactic passages of dialog and exposition deal with government, sex, and religion. These novels are controversial among his readers and one critic, David Langford, has written about them very negatively. Heinlein's four Hugo awards were all for books written before this period.

Most of the novels from this period are recognized by critics as forming an offshoot from the Future History series, and referred to by the term World as Myth.

The tendency toward authorial self-reference begun in "Stranger in a Strange Land" and "Time Enough for Love" becomes even more evident in novels such as "The Cat Who Walks Through Walls", whose first-person protagonist is a disabled military veteran who becomes a writer, and finds love with a female character.

The 1982 novel "Friday", a more conventional adventure story (borrowing a character and backstory from the earlier short story "Gulf", also containing suggestions of connection to "The Puppet Masters") continued a Heinlein theme of expecting what he saw as the continued disintegration of Earth's society, to the point where the title character is strongly encouraged to seek a new life off-planet. It concludes with a traditional Heinlein note, as in "The Moon Is a Harsh Mistress" or "Time Enough for Love", that freedom is to be found on the frontiers.

The 1984 novel "" is a sharp satire of organized religion. Heinlein himself was agnostic.

Several Heinlein works have been published since his death, including the aforementioned "" as well as 1989's "Grumbles from the Grave", a collection of letters between Heinlein and his editors and agent; 1992's "Tramp Royale", a travelogue of a southern hemisphere tour the Heinleins took in the 1950s; "Take Back Your Government", a how-to book about participatory democracy written in 1946; and a tribute volume called "Requiem: Collected Works and Tributes to the Grand Master", containing some additional short works previously unpublished in book form. "Off the Main Sequence", published in 2005, includes three short stories never before collected in any Heinlein book (Heinlein called them "stinkeroos").

Spider Robinson, a colleague, friend, and admirer of Heinlein, wrote "Variable Star", based on an outline and notes for a juvenile novel that Heinlein prepared in 1955. The novel was published as a collaboration, with Heinlein's name above Robinson's on the cover, in 2006.

A complete collection of Heinlein's published work has been published by the Heinlein Prize Trust as the "Virginia Edition", after his wife. See the Complete Works section of Robert A. Heinlein bibliography for details.

The primary influence on Heinlein's writing style may have been Rudyard Kipling. Kipling is the first known modern example of "indirect exposition", a writing technique for which Heinlein later became famous. In his famous text on "On the Writing of Speculative Fiction", Heinlein quotes Kipling:

"There are nine-and-sixty ways
"Of constructing tribal lays
"And every single one of them is right"

"Stranger in a Strange Land" actually originated as a modernized version of Kipling's "The Jungle Book", his wife suggesting that the child be raised by martians instead of wolves. Likewise, "Citizen of the Galaxy" can be seen as a reboot of Kipling's novel "Kim".

Even philosophically, the "Starship Troopers" idea of needing to serve in the military in order to vote, can be found in Kipling's "The Army of a Dream":

Poul Anderson once said of Kipling's science fiction story "As Easy as A.B.C.", "a wonderful science fiction yarn, showing the same eye for detail that would later distinguish the work of Robert Heinlein".

Heinlein described himself as also being influenced by George Bernard Shaw, having read most of his plays. Shaw is an example of an earlier author who used the competent man, a favorite Heinlein archetype. He denied, though, any direct influence of "Back to Methuselah" on "Methuselah's Children".

Heinlein's books probe a range of ideas about a range of topics such as sex, race, politics, and the military. Many were seen as radical or as ahead of their time in their social criticism. His books have inspired considerable debate about the specifics, and the evolution, of Heinlein's own opinions, and have earned him both lavish praise and a degree of criticism. He has also been accused of contradicting himself on various philosophical questions.

Brian Doherty cites William Patterson, saying that the best way to gain an understanding of Heinlein is as a "full-service iconoclast, the unique individual who decides that things do not have to be, and won't continue, as they are." He says this vision is "at the heart of Heinlein, science fiction, libertarianism, and America. Heinlein imagined how everything about the human world, from our sexual mores to our religion to our automobiles to our government to our plans for cultural survival, might be flawed, even fatally so."

The critic Elizabeth Anne Hull, for her part, has praised Heinlein for his interest in exploring fundamental life questions, especially questions about "political power—our responsibilities to one another" and about "personal freedom, particularly sexual freedom."

Heinlein's political positions shifted throughout his life. Heinlein's early political leanings were to the liberal. In 1934, he worked actively for the Democratic campaign of Upton Sinclair for Governor of California. After Sinclair lost, Heinlein became an anti-Communist Democratic activist. He made an unsuccessful bid for a California State Assembly seat in 1938. Heinlein's first novel, "For Us, The Living" (written 1939), consists largely of speeches advocating the Social Credit system, and the early story "Misfit" (1939) deals with an organization that seems to be Franklin D. Roosevelt's Civilian Conservation Corps translated into outer space.

Of this time in his life, Heinlein later said:

Heinlein's fiction of the 1940s and 1950s, however, began to espouse conservative views. After 1945, he came to believe that a strong world government was the only way to avoid mutual nuclear annihilation. His 1949 novel "Space Cadet" describes a future scenario where a military-controlled global government enforces world peace. Heinlein ceased considering himself a Democrat in 1954. He was among those who in 1968 signed a pro-Vietnam War ad in "Galaxy Science Fiction".

Heinlein considered himself a libertarian; in a letter to Judith Merril in 1967 (never sent) he said, "As for libertarian, I've been one all my life, a radical one. You might use the term "philosophical anarchist" or "autarchist" about me, but "libertarian" is easier to define and fits well enough."

"Stranger in a Strange Land" was embraced by the hippie counterculture, and libertarians have found inspiration in "The Moon Is a Harsh Mistress". Both groups found resonance with his themes of personal freedom in both thought and action.

Heinlein grew up in the era of racial segregation in the United States and wrote some of his most influential fiction at the height of the civil rights movement. His early novels were very much ahead of their time both in their explicit rejection of racism and in their inclusion of protagonists of color—in the context of science fiction before the 1960s, the mere existence of characters of color was a remarkable novelty, with green occurring more often than brown. For example, his 1948 novel "Space Cadet" explicitly uses aliens as a metaphor for minorities. In his novel "Star Beast", the "de facto" foreign minister of the Terran government is an undersecretary, a Mr. Kiku, who is from Africa. Heinlein explicitly states his skin is "ebony black", and that Kiku is in an arranged marriage that is happy.

In a number of his stories, Heinlein challenges his readers' possible racial preconceptions by introducing a strong, sympathetic character, only to reveal much later that he or she is of African or other ancestry; in several cases, the covers of the books show characters as being light-skinned, when in fact the text states, or at least implies, that they are dark-skinned or of African ancestry. Heinlein repeatedly denounced racism in his non-fiction works, including numerous examples in "Expanded Universe".

Heinlein reveals in "Starship Troopers" that the novel's protagonist and narrator, Johnny Rico, the formerly disaffected scion of a wealthy family, is Filipino, actually named "Juan Rico" and speaks Tagalog in addition to English.

Race was a central theme in some of Heinlein's fiction. The most prominent and controversial example is "Farnham's Freehold", which casts a white family into a future in which white people are the slaves of cannibalistic black rulers. In the 1941 novel "Sixth Column" (also known as "The Day After Tomorrow"), a white resistance movement in the United States defends itself against an invasion by an Asian fascist state (the "Pan-Asians") using a "super-science" technology that allows ray weapons to be tuned to specific races. The book is sprinkled with racist slurs against Asian people, and blacks and Hispanics are not mentioned at all. The idea for the story was pushed on Heinlein by editor John W. Campbell, and Heinlein wrote later that he had "had to re-slant it to remove racist aspects of the original story line" and that he did not "consider it to be an artistic success." However, the novel prompted a heated debate in the scientific community regarding the plausibility of developing ethnic bioweapons.

In keeping with his belief in individualism, his work for adults—and sometimes even his work for juveniles—often portrays both the oppressors and the oppressed with considerable ambiguity. Heinlein believed that individualism was incompatible with ignorance. He believed that an appropriate level of adult competence was achieved through a wide-ranging education, whether this occurred in a classroom or not. In his juvenile novels, more than once a character looks with disdain at a student's choice of classwork, saying, "Why didn't you study something useful?" In "Time Enough for Love", Lazarus Long gives a long list of capabilities that anyone should have, concluding, "Specialization is for insects." The ability of the individual to create himself is explored in stories such as "I Will Fear No Evil", "—All You Zombies—", and "By His Bootstraps".

Heinlein claimed to have written "Starship Stroopers" in response to "calls for the unilateral ending of nuclear testing by the United States." Heinlein suggests in the book that the Bugs are a good example of Communism being something that humans cannot successfully adhere to, since humans are strongly defined individuals, whereas the Bugs, being a collective, can all contribute to the whole without consideration of individual desire.

For Heinlein, personal liberation included sexual liberation, and free love was a major subject of his writing starting in 1939, with "For Us, The Living". During his early period, Heinlein's writing for younger readers needed to take account of both editorial perceptions of sexuality in his novels, and potential perceptions among the buying public; as critic William H. Patterson has put it, his dilemma was "to sort out what was really objectionable from what was only excessive over-sensitivity to imaginary librarians".
By his middle period, sexual freedom and the elimination of sexual jealousy were a major theme of "Stranger in a Strange Land" (1961), in which the progressively minded but sexually conservative reporter, Ben Caxton, acts as a dramatic foil for the less parochial characters, Jubal Harshaw and Valentine Michael Smith (Mike). Another of the main characters, Jill, is homophobic.

According to Gary Westfahl, "Heinlein is a problematic case for feminists; on the one hand, his works often feature strong female characters and vigorous statements that women are equal to or even superior to men; but these characters and statements often reflect hopelessly stereotypical attitudes about typical female attributes. It is disconcerting, for example, that in "Expanded Universe" Heinlein calls for a society where all lawyers and politicians are women, essentially on the grounds that they possess a mysterious feminine practicality that men cannot duplicate."

In books written as early as 1956, Heinlein dealt with incest and the sexual nature of children. Many of his books including "Time for the Stars", "Glory Road", "Time Enough for Love", and "The Number of the Beast" dealt explicitly or implicitly with incest, sexual feelings and relations between adults and children, or both. The treatment of these themes include the romantic relationship and eventual marriage, once the girl becomes an adult via time-travel, of a 30-year-old engineer and an 11-year-old girl in "The Door into Summer" or the more overt intra-familial incest in "To Sail Beyond the Sunset" and "Farnham's Freehold". Peers such as L. Sprague de Camp and Damon Knight have commented critically on Heinlein's portrayal of incest and pedophilia in a lighthearted and even approving manner.

In "To Sail Beyond the Sunset", Heinlein has the main character, Maureen, state that the purpose of metaphysics is to ask questions: Why are we here? Where are we going after we die? (and so on), and that you are not allowed to answer the questions. "Asking" the questions is the point of metaphysics, but "answering" them is not, because once you answer this kind of question, you cross the line into religion. Maureen does not state a reason for this; she simply remarks that such questions are "beautiful" but lack answers. Maureen's son/lover Lazarus Long makes a related remark in "Time Enough for Love". In order for us to answer the "big questions" about the universe, Lazarus states at one point, it would be necessary to stand "outside" the universe.

During the 1930s and 1940s, Heinlein was deeply interested in Alfred Korzybski's General Semantics and attended a number of seminars on the subject. His views on epistemology seem to have flowed from that interest, and his fictional characters continue to express Korzybskian views to the very end of his writing career. Many of his stories, such as "Gulf", "If This Goes On—", and "Stranger in a Strange Land", depend strongly on the premise, related to the well-known Sapir–Whorf hypothesis, that by using a correctly designed language, one can change or improve oneself mentally, or even realize untapped potential (as in the case of Joe Green in "Gulf").

When Ayn Rand's novel "The Fountainhead" was published, Heinlein was very favorably impressed, as quoted in "Grumbles ..." and mentioned John Galt—the hero in Rand's "Atlas Shrugged"—as a heroic archetype in "The Moon Is a Harsh Mistress". He was also strongly affected by the religious philosopher P. D. Ouspensky. Freudianism and psychoanalysis were at the height of their influence during the peak of Heinlein's career, and stories such as "Time for the Stars" indulged in psychological theorizing.

However, he was skeptical about Freudianism, especially after a struggle with an editor who insisted on reading Freudian sexual symbolism into his juvenile novels. Heinlein was fascinated by the social credit movement in the 1930s. This is shown in "Beyond This Horizon" and in his 1938 novel "", which was finally published in 2003, long after his death.

The term "pay it forward", though it was already in occasional use as a quotation, was popularized by Robert A. Heinlein in his book "Between Planets", published in 1951:

Heinlein was a mentor to Ray Bradbury, giving him help and quite possibly passing on the concept, made famous by the publication of a letter from him to Heinlein thanking him. In Bradbury's novel "Dandelion Wine", published in 1957, when the main character Douglas Spaulding is reflecting on his life being saved by Mr. Jonas, the Junkman:

Bradbury has also advised that writers he has helped thank him by helping other writers.

Heinlein both preached and practiced this philosophy; now the Heinlein Society, a humanitarian organization founded in his name, does so, attributing the philosophy to its various efforts, including Heinlein for Heroes, the Heinlein Society Scholarship Program, and Heinlein Society blood drives.
Author Spider Robinson made repeated reference to the doctrine, attributing it to his spiritual mentor Heinlein.

Heinlein is usually identified, along with Isaac Asimov and Arthur C. Clarke, as one of the three masters of science fiction to arise in the so-called Golden Age of science fiction, associated with John W. Campbell and his magazine "Astounding".
In the 1950s he was a leader in bringing science fiction out of the low-paying and less prestigious "pulp ghetto". Most of his works, including short stories, have been continuously in print in many languages since their initial appearance and are still available as new paperbacks decades after his death.

He was at the top of his form during, and himself helped to initiate, the trend toward social science fiction, which went along with a general maturing of the genre away from space opera to a more literary approach touching on such adult issues as politics and human sexuality. In reaction to this trend, hard science fiction began to be distinguished as a separate subgenre, but paradoxically Heinlein is also considered a seminal figure in hard science fiction, due to his extensive knowledge of engineering and the careful scientific research demonstrated in his stories. Heinlein himself stated—with obvious pride—that in the days before pocket calculators, he and his wife Virginia once worked for several days on a mathematical equation describing an Earth-Mars rocket orbit, which was then subsumed in a single sentence of the novel "Space Cadet".

Heinlein is often credited with bringing serious writing techniques to the genre of science fiction.

For example, when writing about fictional worlds, previous authors were often limited by the reader's existing knowledge of a typical "space opera" setting, leading to a relatively low creativity level: The same starships, death rays, and horrifying rubbery aliens becoming ubiquitous. This was necessary unless the author was willing to go into long expositions about the setting of the story, at a time when the word count was at a premium in SF.

But Heinlein utilized a technique called "indirect exposition", perhaps first introduced by Rudyard Kipling in his own science fiction venture, the Aerial Board of Control stories. Kipling had picked this up during his time in India. This technique — mentioning details in a way that lets the reader infer more about the universe than is actually spelled out became a trademark rhetorical technique of both Heinlein and generation of writers influenced by him. Heinlein was significantly influenced by Kipling beyond this, for example quoting him in On the Writing of Speculative Fiction.
Likewise, Heinlein's name is often associated with the competent hero, a character archetype who, though he or she may have flaws and limitations, is a strong, accomplished person able to overcome any soluble problem set in their path. They tend to feel confident overall, have a broad life experience and set of skills, and not give up when the going gets tough. This style influenced not only the writing style of a generation of authors, but even their personal character. Harlan Ellison once said, "Very early in life when I read Robert Heinlein I got the thread that runs through his stories—the notion of the competent man...I've always held that as my ideal. I've tried to be a very competent man."

While Heinlein used this style, in part, as a role model to the reader, it also has appeal to the self-image of general competence among many Science Fiction readers, who may see themselves as having technical ability, wide-ranging knowledge, an understanding of science, and great problem-solving skill, all of which feel unappreciated in school and work.

When fellow writers, or fans, wrote Heinlein asking for writing advice, he famously gave out his own list of rules for becoming a successful writer:

About which he said:
Heinlein later published an entire article, "On the Writing of Speculative Fiction", which included his rules, and from which the above quote is taken. When he says "anything said above them", he refers to his other guidelines. For example, he describes most stories as fitting into one of a handful of basic categories:

In the article, Heinlein credits L. Ron Hubbard as having identified "The Man-Who-Learned-Better".

Heinlein has had a pervasive influence on other science fiction writers. In a 1953 poll of leading science fiction authors, he was cited more frequently as an influence than any other modern writer. Critic James Gifford writes that "Although many other writers have exceeded Heinlein's output, few can claim to match his broad and seminal influence. Scores of science fiction writers from the prewar Golden Age through the present day loudly and enthusiastically credit Heinlein for blazing the trails of their own careers, and shaping their styles and stories."

Heinlein gave Larry Niven and Jerry Pournelle extensive advice on a draft manuscript of "The Mote in God's Eye". He contributed a cover blurb "Possibly the finest science fiction novel I have ever read." Writer David Gerrold, responsible for creating the tribbles in "Star Trek", also credited Heinlein as the inspiration for his "Dingilliad" series of novels. Gregory Benford refers to his novel "Jupiter Project" as a Heinlein tribute. Similarly, Charles Stross says his Hugo Award-nominated novel "Saturn's Children" is "a space opera and late-period Robert A. Heinlein tribute", referring to Heinlein's "Friday".

Outside the science fiction community, several words and phrases coined or adopted by Heinlein have passed into common English usage:

In 1962, Oberon Zell-Ravenheart (then still using his birth name, Tim Zell) founded the Church of All Worlds, a Neopagan religious organization modeled in many ways (including its name) after the treatment of religion in the novel "Stranger in a Strange Land". This spiritual path included several ideas from the book, including non-mainstream family structures, social libertarianism, water-sharing rituals, an acceptance of all religious paths by a single tradition, and the use of several terms such as "grok", "Thou art God", and "Never Thirst". Though Heinlein was neither a member nor a promoter of the Church, there was a frequent exchange of correspondence between Zell and Heinlein, and he was a paid subscriber to their magazine, "Green Egg". This Church still exists as a 501(C)(3) religious organization incorporated in California, with membership worldwide, and it remains an active part of the neopagan community today.

Heinlein was influential in making space exploration seem to the public more like a practical possibility. His stories in publications such as "The Saturday Evening Post" took a matter-of-fact approach to their outer-space setting, rather than the "gee whiz" tone that had previously been common. The documentary-like film "Destination Moon" advocated a Space Race with an unspecified foreign power almost a decade before such an idea became commonplace, and was promoted by an unprecedented publicity campaign in print publications. Many of the astronauts and others working in the U.S. space program grew up on a diet of the Heinlein juveniles, best evidenced by the naming of a crater on Mars after him, and a tribute interspersed by the Apollo 15 astronauts into their radio conversations while on the moon.

Heinlein was also a guest commentator for Walter Cronkite during Neil Armstrong and Buzz Aldrin's Apollo 11 moon landing. He remarked to Cronkite during the landing that, "This is the greatest event in human history, up to this time. This is—today is New Year's Day of the Year One." Businessman and entrepreneur Elon Musk says that Heinlein's books have helped inspire his career.

The Heinlein Society was founded by Virginia Heinlein on behalf of her husband, to "pay forward" the legacy of the writer to future generations of "Heinlein's Children." The foundation has programs to:

The Heinlein society also established the Robert A. Heinlein Award in 2003 "for outstanding published works in science fiction and technical writings to inspire the human exploration of space."



In his lifetime, Heinlein received four Hugo Awards, for "Double Star", "Starship Troopers", "Stranger in a Strange Land", and "The Moon Is a Harsh Mistress", and was nominated for four Nebula Awards, for "Stranger in a Strange Land", "Friday", "Time Enough for Love", and "Job: A Comedy of Justice". He was also given five posthumous Hugos, for "Farmer in the Sky", "Destination Moon", "If This Goes On", "The Roads Must Roll", and "The Man Who Sold the Moon".

The Science Fiction Writers of America named Heinlein its first Grand Master in 1974, presented 1975. Officers and past presidents of the Association select a living writer for lifetime achievement (now annually and including fantasy literature).

Main-belt asteroid 6312 Robheinlein (1990 RH4), discovered on September 14, 1990 by H. E. Holt, at Palomar was named after him.

There is no lunar feature named explicitly for Heinlein, but in 1994 the International Astronomical Union named Heinlein crater on Mars in his honor.

The Science Fiction and Fantasy Hall of Fame inducted Heinlein in 1998, its third class of two deceased and two living writers and editors.

In 2001 the United States Naval Academy created the Robert A. Heinlein Chair In Aerospace Engineering.

In 2016, after an intensive online campaign to win a vote for the opening, Heinlein was inducted into the Hall of Famous Missourians. His bronze bust, created by Kansas City sculptor E. Spencer Schubert, is on permanent display in the Missouri State Capitol in Jefferson City.

The Libertarian Futurist Society has honored five of Heinlein's novels and two short stories with their Hall of Fame award. The first two were given during his lifetime for "The Moon Is a Harsh Mistress" and "Stranger in a Strange Land". Five more were awarded posthumously for "Red Planet", "Methuselah's Children", "Time Enough for Love", and the short stories "Requiem" and "Coventry".







</doc>
