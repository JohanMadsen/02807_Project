<doc id="24408" url="https://en.wikipedia.org/wiki?curid=24408" title="Polar bear">
Polar bear

The polar bear ("Ursus maritimus") is a hypercarnivorous bear whose native range lies largely within the Arctic Circle, encompassing the Arctic Ocean, its surrounding seas and surrounding land masses. It is a large bear, approximately the same size as the omnivorous Kodiak bear ("Ursus arctos middendorffi"). A boar (adult male) weighs around , while a sow (adult female) is about half that size. Although it is the sister species of the brown bear, it has evolved to occupy a narrower ecological niche, with many body characteristics adapted for cold temperatures, for moving across snow, ice and open water, and for hunting seals, which make up most of its diet. Although most polar bears are born on land, they spend most of their time on the sea ice. Their scientific name means "maritime bear" and derives from this fact. Polar bears hunt their preferred food of seals from the edge of sea ice, often living off fat reserves when no sea ice is present. Because of their dependence on the sea ice, polar bears are classified as marine mammals.

Because of expected habitat loss caused by climate change, the polar bear is classified as a vulnerable species, and at least three of the nineteen polar bear subpopulations are currently in decline. However, at least two of the nineteen subpopulations are currently increasing, while another six are considered stable. For decades, large-scale hunting raised international concern for the future of the species, but populations rebounded after controls and quotas began to take effect. For thousands of years, the polar bear has been a key figure in the material, spiritual, and cultural life of circumpolar peoples, and polar bears remain important in their cultures. Historically, the polar bear has also been known as the white bear.

Constantine John Phipps was the first to describe the polar bear as a distinct species in 1774. He chose the scientific name "Ursus maritimus", the Latin for 'maritime bear', due to the animal's native habitat. The Inuit refer to the animal as "nanook" (transliterated as "nanuq" in the Inupiat language). The Yupik also refer to the bear as "nanuuk" in Siberian Yupik. The bear is "umka" in the Chukchi language. In Russian, it is usually called бе́лый медве́дь ("bélyj medvédj", the white bear), though an older word still in use is ошку́й ("Oshkúj", which comes from the Komi "oski", "bear"). In Quebec, the polar bear is referred to as "ours blanc" ("white bear") or "ours polaire" ("polar bear"). In the Norwegian-administered Svalbard archipelago, the polar bear is referred to as "Isbjørn" ("ice bear").

The polar bear was previously considered to be in its own genus, "Thalarctos". However, evidence of hybrids between polar bears and brown bears, and of the recent evolutionary divergence of the two species, does not support the establishment of this separate genus, and the accepted scientific name is now therefore "Ursus maritimus", as Phipps originally proposed.

The bear family, Ursidae, is thought to have split from other carnivorans about 38 million years ago. The Ursinae subfamily originated approximately 4.2 million years ago. The oldest known polar bear fossil is a 130,000 to 110,000-year-old jaw bone, found on Prince Charles Foreland in 2004. Fossils show that between 10,000 and 20,000 years ago, the polar bear's molar teeth changed significantly from those of the brown bear. Polar bears are thought to have diverged from a population of brown bears that became isolated during a period of glaciation in the Pleistocene from the eastern part of Siberia, (from Kamchatka and the Kolym Peninsula).

The evidence from DNA analysis is more complex. The mitochondrial DNA (mtDNA) of the polar bear diverged from the brown bear, "Ursus arctos", roughly 150,000 years ago. Further, some clades of brown bear, as assessed by their mtDNA, are more closely related to polar bears than to other brown bears, meaning that the polar bear might not be considered a species under some species concepts. The mtDNA of extinct Irish brown bears is particularly close to polar bears. A comparison of the nuclear genome of polar bears with that of brown bears revealed a different pattern, the two forming genetically distinct clades that diverged approximately 603,000 years ago, although the latest research is based on analysis of the complete genomes (rather than just the mitochondria or partial nuclear genomes) of polar and brown bears, and establishes the divergence of polar and brown bears at 400,000 years ago.

However, the two species have mated intermittently for all that time, most likely coming into contact with each other during warming periods, when polar bears were driven onto land and brown bears migrated northward. Most brown bears have about 2 percent genetic material from polar bears, but one population, the ABC Islands bears has between 5 percent and 10 percent polar bear genes, indicating more frequent and recent mating. Polar bears can breed with brown bears to produce fertile grizzly–polar bear hybrids, rather than indicating that they have only recently diverged, the new evidence suggests more frequent mating has continued over a longer period of time, and thus the two bears remain genetically similar. However, because neither species can survive long in the other's ecological niche, and because they have different morphology, metabolism, social and feeding behaviours, and other phenotypic characteristics, the two bears are generally classified as separate species.

When the polar bear was originally documented, two subspecies were identified: "Ursus maritimus maritimus" by Constantine J. Phipps in 1774, and "Ursus maritimus marinus" by Peter Simon Pallas in 1776. This distinction has since been invalidated. One alleged fossil subspecies has been identified: "Ursus maritimus tyrannus" became extinct during the Pleistocene. "U.m. tyrannus" was significantly larger than the living subspecies. However, recent reanalysis of the fossil suggests that it was actually a type of brown bear.

The polar bear is found in the Arctic Circle and adjacent land masses as far south as Newfoundland. Due to the absence of human development in its remote habitat, it retains more of its original range than any other extant carnivore. While they are rare north of 88°, there is evidence that they range all the way across the Arctic, and as far south as James Bay in Canada. Their southernmost range is near the boundary between the subarctic and humid continental climate zones. They can occasionally drift widely with the sea ice, and there have been anecdotal sightings as far south as Berlevåg on the Norwegian mainland and the Kuril Islands in the Sea of Okhotsk. It is difficult to estimate a global population of polar bears as much of the range has been poorly studied; however, biologists use a working estimate of about 20–25,000 or 22–31,000 polar bears worldwide.

There are 19 generally recognized, discrete subpopulations, though polar bears are thought to exist only in low densities in the area of the Arctic Basin. The subpopulations display seasonal fidelity to particular areas, but DNA studies show that they are not reproductively isolated. The thirteen North American subpopulations range from the Beaufort Sea south to Hudson Bay and east to Baffin Bay in western Greenland and account for about 54% of the global population.
The range includes the territory of five nations: Denmark (Greenland), Norway (Svalbard), Russia, the United States (Alaska) and Canada. These five nations are the signatories of the International Agreement on the Conservation of Polar Bears, which mandates cooperation on research and conservation efforts throughout the polar bear's range. Bears sometimes swim to Iceland from Greenland—about 600 sightings since the country's settlement in the 9th century AD, and five in the 21st century —and are always killed because of their danger, and the cost and difficulty of repatriation.

Modern methods of tracking polar bear populations have been implemented only since the mid-1980s, and are expensive to perform consistently over a large area. The most accurate counts require flying a helicopter in the Arctic climate to find polar bears, shooting a tranquilizer dart at the bear to sedate it, and then tagging the bear. In Nunavut, some Inuit have reported increases in bear sightings around human settlements in recent years, leading to a belief that populations are increasing. Scientists have responded by noting that hungry bears may be congregating around human settlements, leading to the illusion that populations are higher than they actually are. The Polar Bear Specialist Group of the IUCN Species Survival Commission takes the position that "estimates of subpopulation size or sustainable harvest levels should not be made solely on the basis of traditional ecological knowledge without supporting scientific studies."

Of the 19 recognized polar bear subpopulations, three are declining, six are stable, one is increasing, and nine have insufficient data, as of 2014.

The polar bear is a marine mammal because it spends many months of the year at sea. However, it is the only living marine mammal with powerful, large limbs and feet that allow them to cover miles on foot and run on land. Its preferred habitat is the annual sea ice covering the waters over the continental shelf and the Arctic inter-island archipelagos. These areas, known as the "Arctic ring of life", have high biological productivity in comparison to the deep waters of the high Arctic. The polar bear tends to frequent areas where sea ice meets water, such as polynyas and leads (temporary stretches of open water in Arctic ice), to hunt the seals that make up most of its diet. Freshwater is limited in these environments because it is either locked up in snow or saline. Polar bears are able to produce water through the metabolism of fats found in seal blubber. Polar bears are therefore found primarily along the perimeter of the polar ice pack, rather than in the Polar Basin close to the North Pole where the density of seals is low.

Annual ice contains areas of water that appear and disappear throughout the year as the weather changes. Seals migrate in response to these changes, and polar bears must follow their prey. In Hudson Bay, James Bay, and some other areas, the ice melts completely each summer (an event often referred to as "ice-floe breakup"), forcing polar bears to go onto land and wait through the months until the next freeze-up. In the Chukchi and Beaufort seas, polar bears retreat each summer to the ice further north that remains frozen year-round.

The only other bear similar in size to the polar bear is the Kodiak bear, which is a subspecies of brown bear. Adult male polar bears weigh and measure in total length. Around the Beaufort Sea, however, mature males reportedly average . Adult females are roughly half the size of males and normally weigh , measuring in length. Elsewhere, a slightly larger estimated average weight of was claimed for adult females. When pregnant, however, females can weigh as much as . The polar bear is among the most sexually dimorphic of mammals, surpassed only by the pinnipeds such as elephant seals. The largest polar bear on record, reportedly weighing , was a male shot at Kotzebue Sound in northwestern Alaska in 1960. This specimen, when mounted, stood tall on its hindlegs. The shoulder height of an adult polar bear is . While all bears are short-tailed, the polar bear's tail is relatively the shortest amongst living bears, ranging from in length.

Compared with its closest relative, the brown bear, the polar bear has a more elongated body build and a longer skull and nose. As predicted by Allen's rule for a northerly animal, the legs are stocky and the ears and tail are small. However, the feet are very large to distribute load when walking on snow or thin ice and to provide propulsion when swimming; they may measure across in an adult. The pads of the paws are covered with small, soft papillae (dermal bumps), which provide traction on the ice. The polar bear's claws are short and stocky compared to those of the brown bear, perhaps to serve the former's need to grip heavy prey and ice. The claws are deeply scooped on the underside to assist in digging in the ice of the natural habitat. Research of injury patterns in polar bear forelimbs found injuries to the right forelimb to be more frequent than those to the left, suggesting, perhaps, right-handedness. Unlike the brown bear, polar bears in captivity are rarely overweight or particularly large, possibly as a reaction to the warm conditions of most zoos.

The 42 teeth of a polar bear reflect its highly carnivorous diet. The cheek teeth are smaller and more jagged than in the brown bear, and the canines are larger and sharper. The dental formula is 

Polar bears are superbly insulated by up to of adipose tissue, their hide and their fur; they overheat at temperatures above , and are nearly invisible under infrared photography. Polar bear fur consists of a layer of dense underfur and an outer layer of guard hairs, which appear white to tan but are actually transparent. Two genes that are known to influence melanin production, LYST and AIM1, are both mutated in polar bears, possibly leading to the absence on this pigment in their fur. The guard hair is over most of the body. Polar bears gradually moult from May to August, but, unlike other Arctic mammals, they do not shed their coat for a darker shade to provide camouflage in summer conditions. The hollow guard hairs of a polar bear coat were once thought to act as fiber-optic tubes to conduct light to its black skin, where it could be absorbed; however, this hypothesis was disproved by a study in 1998.

The white coat usually yellows with age. When kept in captivity in warm, humid conditions, the fur may turn a pale shade of green due to algae growing inside the guard hairs. Males have significantly longer hairs on their forelegs, which increase in length until the bear reaches 14 years of age. The male's ornamental foreleg hair is thought to attract females, serving a similar function to the lion's mane.

The polar bear has an extremely well developed sense of smell, being able to detect seals nearly away and buried under of snow. Its hearing is about as acute as that of a human, and its vision is also good at long distances.

The polar bear is an excellent swimmer and often will swim for days. One bear swam continuously for 9 days in the frigid Bering Sea for to reach ice far from land. She then travelled another . During the swim, the bear lost 22% of her body mass and her yearling cub died. With its body fat providing buoyancy, the bear swims in a dog paddle fashion using its large forepaws for propulsion. Polar bears can swim . When walking, the polar bear tends to have a lumbering gait and maintains an average speed of around . When sprinting, they can reach up to .

Unlike brown bears, polar bears are not territorial. Although stereotyped as being voraciously aggressive, they are normally cautious in confrontations, and often choose to escape rather than fight. Satiated polar bears rarely attack humans unless severely provoked. However, due to their lack of prior human interaction, hungry polar bears are extremely unpredictable, fearless towards people and are known to kill and sometimes eat humans. Many attacks by brown bears are the result of surprising the animal, which is not the case with the polar bear. Polar bears are stealth hunters, and the victim is often unaware of the bear's presence until the attack is underway. Whereas brown bears often maul a person and then leave, polar bear attacks are more likely to be predatory and are almost always fatal. However, due to the very small human population around the Arctic, such attacks are rare. Michio Hoshino, a Japanese wildlife photographer, was once pursued briefly by a hungry male polar bear in northern Alaska. According to Hoshino, the bear started running but Hoshino made it to his truck. The bear was able to reach the truck and tore one of the doors off the truck before Hoshino was able to drive off.

In general, adult polar bears live solitary lives. Yet, they have often been seen playing together for hours at a time and even sleeping in an embrace, and polar bear zoologist Nikita Ovsianikov has described adult males as having "well-developed friendships." Cubs are especially playful as well. Among young males in particular, play-fighting may be a means of practicing for serious competition during mating seasons later in life. Polar bears are usually quiet but do communicate with various sounds and vocalizations. Females communicate with their young with moans and chuffs, and the distress calls of both cubs and subadults consists of bleats. Cubs may hum while nursing. When nervous, bears produce huffs, chuffs and snorts while hisses, growls and roars are signs of aggression. Chemical communication can also be important: bears leave behind their scent in their tracks which allow individuals to keep track of one another in the vast Arctic wilderness.

In 1992, a photographer near Churchill took a now widely circulated set of photographs of a polar bear playing with a Canadian Eskimo Dog ("Canis lupus familiaris") a tenth of its size. The pair wrestled harmlessly together each afternoon for ten days in a row for no apparent reason, although the bear may have been trying to demonstrate its friendliness in the hope of sharing the kennel's food. This kind of social interaction is uncommon; it is far more typical for polar bears to behave aggressively towards dogs.

The polar bear is the most carnivorous member of the bear family, and throughout most of its range, its diet primarily consists of ringed ("Pusa hispida") and bearded seals ("Erignathus barbatus"). The Arctic is home to millions of seals, which become prey when they surface in holes in the ice in order to breathe, or when they haul out on the ice to rest. Polar bears hunt primarily at the interface between ice, water, and air; they only rarely catch seals on land or in open water.

The polar bear's most common hunting method is called "still-hunting": the bear uses its excellent sense of smell to locate a seal breathing hole, and crouches nearby in silence for a seal to appear. The bear may lay in wait for several hours. When the seal exhales, the bear smells its breath, reaches into the hole with a forepaw, and drags it out onto the ice. The polar bear kills the seal by biting its head to crush its skull. The polar bear also hunts by stalking seals resting on the ice: upon spotting a seal, it walks to within , and then crouches. If the seal does not notice, the bear creeps to within of the seal and then suddenly rushes forth to attack. A third hunting method is to raid the birth lairs that female seals create in the snow.

A widespread legend tells that polar bears cover their black noses with their paws when hunting. This behaviour, if it happens, is rare – although the story exists in the oral history of northern peoples and in accounts by early Arctic explorers, there is no record of an eyewitness account of the behaviour in recent decades.
Mature bears tend to eat only the calorie-rich skin and blubber of the seal, which are highly digestible, whereas younger bears consume the protein-rich red meat. Studies have also photographed polar bears scaling near-vertical cliffs, to eat birds' chicks and eggs. For subadult bears, which are independent of their mother but have not yet gained enough experience and body size to successfully hunt seals, scavenging the carcasses from other bears' kills is an important source of nutrition. Subadults may also be forced to accept a half-eaten carcass if they kill a seal but cannot defend it from larger polar bears. After feeding, polar bears wash themselves with water or snow.

Although polar bears are extraordinarily powerful, its primary prey species, the ringed seal, is much smaller than itself, and many of the seals hunted are pups rather than adults. Ringed seals are born weighing and grown to an estimated average weight of only . They also in places prey heavily upon the harp seal ("Pusa groenlandica") or the harbor seal. The bearded seal, on the other hand, can be nearly the same size as the bear itself, averaging . Adult male bearded seals, at are too large for a female bear to overtake, and so are potential prey only for mature male bears. Large males also occasionally attempt to hunt and kill even larger prey items. It can kill an adult walrus ("Odobenus rosmarus"), although this is rarely attempted. At up to and a typical adult mass range of , a walrus can be more than twice the bear's weight, and has up to -long ivory tusks that can be used as formidable weapons. A polar bear may charge a group of walruses, with the goal of separating a young, infirm, or injured walrus from the pod. They will even attack adult walruses when their diving holes have frozen over or intercept them before they can get back to the diving hole in the ice. Yet, polar bears will very seldom attack full-grown adult walruses, with the largest male walrus probably invulnerable unless otherwise injured or incapacitated. Since an attack on a walrus tends to be an extremely protracted and exhausting venture, bears have been known to back down from the attack after making the initial injury to the walrus. Polar bears have also been seen to prey on beluga whales ("Delphinapterus leucas") and narwhals ("Monodon monoceros"), by swiping at them at breathing holes. The whales are of similar size to the walrus and nearly as difficult for the bear to subdue. Most terrestrial animals in the Arctic can outrun the polar bear on land as polar bears overheat quickly, and most marine animals the bear encounters can outswim it. In some areas, the polar bear's diet is supplemented by walrus calves and by the carcasses of dead adult walruses or whales, whose blubber is readily devoured even when rotten.
Polar bears sometimes swim underwater to catch fish like the Arctic charr or the fourhorn sculpin.
With the exception of pregnant females, polar bears are active year-round, although they have a vestigial hibernation induction trigger in their blood. Unlike brown and black bears, polar bears are capable of fasting for up to several months during late summer and early fall, when they cannot hunt for seals because the sea is unfrozen. When sea ice is unavailable during summer and early autumn, some populations live off fat reserves for months at a time, as polar bears do not 'hibernate' any time of the year.

Being both curious animals and scavengers, polar bears investigate and consume garbage where they come into contact with humans. Polar bears may attempt to consume almost anything they can find, including hazardous substances such as styrofoam, plastic, car batteries, ethylene glycol, hydraulic fluid, and motor oil. The dump in Churchill, Manitoba was closed in 2006 to protect bears, and waste is now recycled or transported to Thompson, Manitoba.

Although seal predation is the primary and an indispensable way of life for most polar bears, when alternatives are present they are quite flexible. Polar bears consume a wide variety of other wild foods, including muskox ("Ovibos moschatus"), reindeer ("Rangifer tarandus"), birds, eggs, rodents, crabs, other crustaceans and other polar bears. They may also eat plants, including berries, roots, and kelp; however, none of these have been a significant part of their diet, except for beachcast marine mammal carcasses. Given the change in climate, with ice breaking up in areas such as the Hudson Bay earlier than it used to, polar bears are exploiting food resources such as snow geese and eggs, and plants such as lyme grass in increased quantities.
When stalking land animals, such as muskox, reindeer, and even willow ptarmigan ("Lagopus lagopus"), polar bears appear to make use of vegetative cover and wind direction to bring them as close to their prey as possible before attacking. Polar bears have been observed to hunt the small Svalbard reindeer ("R. t. platyrhynchus"), which weigh only as adults, as well as the barren-ground caribou ("R. t. groenlandicus"), which is about twice as heavy as that. Adult muskox, which can weigh or more, are a more formidable quarry. Although ungulates are not typical prey, the killing of one during the summer months can greatly increase the odds of survival during that lean period. Like the brown bear, most ungulate prey of polar bears is likely to be young, sickly or injured specimens rather than healthy adults. The polar bear's metabolism is specialized to require large amounts of fat from marine mammals, and it cannot derive sufficient caloric intake from terrestrial food.

In their southern range, especially near Hudson Bay and James Bay, Canadian polar bears endure all summer without sea ice to hunt from. Here, their food ecology shows their dietary flexibility. They still manage to consume some seals, but they are food-deprived in summer as only marine mammal carcasses are an important alternative without sea ice, especially carcasses of the beluga whale. These alternatives may reduce the rate of weight loss of bears when on land. One scientist found that 71% of the Hudson Bay bears had fed on seaweed (marine algae) and that about half were feeding on birds such as the dovekie and sea ducks, especially the long-tailed duck (53%) and common eider, by swimming underwater to catch them. They were also diving to feed on blue mussels and other underwater food sources like the green sea urchin. 24% had eaten moss recently, 19% had consumed grass, 34% had eaten black crowberry and about half had consumed willows. This study illustrates the polar bear's dietary flexibility but it does not represent its life history elsewhere. Most polar bears elsewhere will never have access to these alternatives, except for the marine mammal carcasses that are important wherever they occur.

In Svalbard, polar bears were observed to kill white-beaked dolphins during spring, when the dolphins were trapped in the sea ice. The bears then proceeded to cache the carcasses, which remained and were eaten during the ice-free summer and autumn.

Courtship and mating take place on the sea ice in April and May, when polar bears congregate in the best seal hunting areas. A male may follow the tracks of a breeding female for or more, and after finding her engage in intense fighting with other males over mating rights, fights that often result in scars and broken teeth. Polar bears have a generally polygynous mating system; recent genetic testing of mothers and cubs, however, has uncovered cases of litters in which cubs have different fathers. Partners stay together and mate repeatedly for an entire week; the mating ritual induces ovulation in the female.

After mating, the fertilized egg remains in a suspended state until August or September. During these four months, the pregnant female eats prodigious amounts of food, gaining at least and often more than doubling her body weight.

When the ice floes are at their minimum in the fall, ending the possibility of hunting, each pregnant female digs a "maternity den" consisting of a narrow entrance tunnel leading to one to three chambers. Most maternity dens are in snowdrifts, but may also be made underground in permafrost if it is not sufficiently cold yet for snow. In most subpopulations, maternity dens are situated on land a few kilometers from the coast, and the individuals in a subpopulation tend to reuse the same denning areas each year. The polar bears that do not den on land make their dens on the sea ice. In the den, she enters a dormant state similar to hibernation. This hibernation-like state does not consist of continuous sleeping; however, the bear's heart rate slows from 46 to 27 beats per minute. Her body temperature does not decrease during this period as it would for a typical mammal in hibernation.

Between November and February, cubs are born blind, covered with a light down fur, and weighing less than , but in captivity they might be delivered in the earlier months. The earliest recorded birth of polar bears in captivity was on 11 October 2011 in the Toronto Zoo. On average, each litter has two cubs. The family remains in the den until mid-February to mid-April, with the mother maintaining her fast while nursing her cubs on a fat-rich milk. By the time the mother breaks open the entrance to the den, her cubs weigh about . For about 12 to 15 days, the family spends time outside the den while remaining in its vicinity, the mother grazing on vegetation while the cubs become used to walking and playing. Then they begin the long walk from the denning area to the sea ice, where the mother can once again catch seals. Depending on the timing of ice-floe breakup in the fall, she may have fasted for up to eight months. During this time, cubs playfully imitate the mother's hunting methods in preparation for later life.
Female polar bears are noted for both their affection towards their offspring, and their valor in protecting them. Multiple cases of adoption of wild cubs have been confirmed by genetic testing. Adult male bears occasionally kill and eat polar bear cubs. As of 2006, in Alaska, 42% of cubs were reaching 12 months of age, down from 65% in 1991. In most areas, cubs are weaned at two and a half years of age, when the mother chases them away or abandons them. The Western Hudson Bay subpopulation is unusual in that its female polar bears sometimes wean their cubs at only one and a half years. This was the case for 40% of cubs there in the early 1980s; however by the 1990s, fewer than 20% of cubs were weaned this young. After the mother leaves, sibling cubs sometimes travel and share food together for weeks or months.

Females begin to breed at the age of four years in most areas, and five years in the Beaufort Sea area. Males usually reach sexual maturity at six years; however, as competition for females is fierce, many do not breed until the age of eight or ten. A study in Hudson Bay indicated that both the reproductive success and the maternal weight of females peaked in their mid-teens.

Polar bears appear to be less affected by infectious diseases and parasites than most terrestrial mammals. Polar bears are especially susceptible to "Trichinella", a parasitic roundworm they contract through cannibalism, although infections are usually not fatal. Only one case of a polar bear with rabies has been documented, even though polar bears frequently interact with Arctic foxes, which often carry rabies. Bacterial leptospirosis and "Morbillivirus" have been recorded. Polar bears sometimes have problems with various skin diseases that may be caused by mites or other parasites.

Polar bears rarely live beyond 25 years. The oldest wild bears on record died at age 32, whereas the oldest captive was a female who died in 1991, age 43. The causes of death in wild adult polar bears are poorly understood, as carcasses are rarely found in the species's frigid habitat. In the wild, old polar bears eventually become too weak to catch food, and gradually starve to death. Polar bears injured in fights or accidents may either die from their injuries or become unable to hunt effectively, leading to starvation.

The polar bear is the apex predator within its range, and is a keystone species for the Arctic. Several animal species, particularly Arctic foxes ("Vulpes lagopus") and glaucous gulls ("Larus hyperboreus"), routinely scavenge polar bear kills.

The relationship between ringed seals and polar bears is so close that the abundance of ringed seals in some areas appears to regulate the density of polar bears, while polar bear predation in turn regulates density and reproductive success of ringed seals. The evolutionary pressure of polar bear predation on seals probably accounts for some significant differences between Arctic and Antarctic seals. Compared to the Antarctic, where there is no major surface predator, Arctic seals use more breathing holes per individual, appear more restless when hauled out on the ice, and rarely defecate on the ice. The baby fur of most Arctic seal species is white, presumably to provide camouflage from predators, whereas Antarctic seals all have dark fur at birth.

Brown bears tend to dominate polar bears in disputes over carcasses, and dead polar bear cubs have been found in brown bear dens. Wolves are rarely encountered by polar bears, though there are two records of Arctic wolf ("Canis lupus arctos") packs killing polar bear cubs. Adult polar bears are occasionally vulnerable to predation by orcas ("Orcinus orca") while swimming, but they are rarely reported as taken and bears are likely to avoid entering the water if possible if they detect an orca pod in the area. The melting sea ice in the Arctic may be causing an increase of orcas in the Arctic sea, which may increase the risk of predation on polar bears but also may benefit the bears by providing more whale carcasses that they can scavenge. The remains of polar bears have found in the stomachs of large Greenland sharks ("Somniosus microcephalus"), although it certainly cannot be ruled out that the bears were merely scavenged by this slow-moving, unusual shark. A rather unlikely killer of a grown polar bear has reportedly included a wolverine ("Gulo gulo"), anecedotely reported to have suffocated a bear in a zoo with a bite to the throat during a conflict. This report may well be dubious, however. Polar bears are sometimes the host of arctic mites such as "Alaskozetes antarcticus".

Researchers tracked 52 sows in the southern Beaufort Sea off Alaska with GPS system collars; no boars were involved in the study due to males' necks being too thick for the GPS-equipped collars. Fifty long-distance swims were recorded; the longest at , with an average of . The length of these swims ranged from most of a day to ten days. Ten of the sows had a cub swim with them and after a year, six cubs survived. The study did not determine if the others lost their cubs before, during, or some time after their long swims. Researchers do not know whether or not this is a new behaviour; before polar ice shrinkage, they opined that there was probably neither the need nor opportunity to swim such long distances.

The polar bear may swim underwater for up to three minutes to approach seals on shore or on ice floes.

Polar bears have long provided important raw materials for Arctic peoples, including the Inuit, Yupik, Chukchi, Nenets, Russian Pomors and others. Hunters commonly used teams of dogs to distract the bear, allowing the hunter to spear the bear or shoot it with arrows at closer range. Almost all parts of captured animals had a use. The fur was used in particular to make trousers and, by the Nenets, to make galoshes-like outer footwear called "tobok"; the meat is edible, despite some risk of trichinosis; the fat was used in food and as a fuel for lighting homes, alongside seal and whale blubber; sinews were used as thread for sewing clothes; the gallbladder and sometimes heart were dried and powdered for medicinal purposes; the large canine teeth were highly valued as talismans. Only the liver was not used, as its high concentration of vitamin A is poisonous. Hunters make sure to either toss the liver into the sea or bury it in order to spare their dogs from potential poisoning. Traditional subsistence hunting was on a small enough scale to not significantly affect polar bear populations, mostly because of the sparseness of the human population in polar bear habitat.

In Russia, polar bear furs were already being commercially traded in the 14th century, though it was of low value compared to Arctic fox or even reindeer fur. The growth of the human population in the Eurasian Arctic in the 16th and 17th century, together with the advent of firearms and increasing trade, dramatically increased the harvest of polar bears. However, since polar bear fur has always played a marginal commercial role, data on the historical harvest is fragmentary. It is known, for example, that already in the winter of 1784/1785 Russian Pomors on Spitsbergen harvested 150 polar bears in Magdalenefjorden. In the early 20th century, Norwegian hunters were harvesting 300 bears per year at the same location. Estimates of total historical harvest suggest that from the beginning of the 18th century, roughly 400 to 500 animals were being harvested annually in northern Eurasia, reaching a peak of 1,300 to 1,500 animals in the early 20th century, and falling off as the numbers began dwindling.

In the first half of the 20th century, mechanized and overpoweringly efficient methods of hunting and trapping came into use in North America as well. Polar bears were chased from snowmobiles, icebreakers, and airplanes, the latter practice described in a 1965 "New York Times" editorial as being "about as sporting as machine gunning a cow." Norwegians used "self-killing guns", comprising a loaded rifle in a baited box that was placed at the level of a bear's head, and which fired when the string attached to the bait was pulled. The numbers taken grew rapidly in the 1960s, peaking around 1968 with a global total of 1,250 bears that year.

Concerns over the future survival of the species led to the development of national regulations on polar bear hunting, beginning in the mid-1950s. The Soviet Union banned all hunting in 1956. Canada began imposing hunting quotas in 1968. Norway passed a series of increasingly strict regulations from 1965 to 1973, and has completely banned hunting since then. The United States began regulating hunting in 1971 and adopted the Marine Mammal Protection Act in 1972. In 1973, the International Agreement on the Conservation of Polar Bears was signed by all five nations whose territory is inhabited by polar bears: Canada, Denmark, Norway, the Soviet Union, and the United States. Member countries agreed to place restrictions on recreational and commercial hunting, ban hunting from aircraft and icebreakers, and conduct further research. The treaty allows hunting "by local people using traditional methods". Norway is the only country of the five in which all harvest of polar bears is banned. The agreement was a rare case of international cooperation during the Cold War. Biologist Ian Stirling commented, "For many years, the conservation of polar bears was the only subject in the entire Arctic that nations from both sides of the Iron Curtain could agree upon sufficiently to sign an agreement. Such was the intensity of human fascination with this magnificent predator, the only marine bear."

Agreements have been made between countries to co-manage their shared polar bear subpopulations. After several years of negotiations, Russia and the United States signed an agreement in October 2000 to jointly set quotas for indigenous subsistence hunting in Alaska and Chukotka. The treaty was ratified in October 2007. In September 2015, the polar bear range states agreed upon a "circumpolar action plan" describing their conservation strategy for polar bears.

Although the United States government has proposed that polar bears be transferred to Appendix I of CITES, which would ban all international trade in polar bear parts, polar bears currently remain listed under Appendix II. This decision was approved of by members of the IUCN and TRAFFIC, who determined that such an uplisting was unlikely to confer a conservation benefit.

Polar bears were designated "Not at Risk" in April 1986 and uplisted to "Special Concern" in April 1991. This status was re-evaluated and confirmed in April 1999, November 2002, and April 2008. Polar bears continue to be listed as a species of special concern in Canada because of their sensitivity to overharvest and because of an expected range contraction caused by loss of Arctic sea ice.

More than 600 bears are killed per year by humans across Canada, a rate calculated by scientists to be unsustainable for some areas, notably Baffin Bay. Canada has allowed sport hunters accompanied by local guides and dog-sled teams since 1970, but the practice was not common until the 1980s. The guiding of sport hunters provides meaningful employment and an important source of income for northern communities in which economic opportunities are few. Sport hunting can bring CDN$20,000 to $35,000 per bear into northern communities, which until recently has been mostly from American hunters.

The territory of Nunavut accounts for the location 80% of annual kills in Canada. In 2005, the government of Nunavut increased the quota from 400 to 518 bears, despite protests from the IUCN Polar Bear Specialist Group. In two areas where harvest levels have been increased based on increased sightings, science-based studies have indicated declining populations, and a third area is considered data-deficient. While most of that quota is hunted by the indigenous Inuit people, a growing share is sold to recreational hunters. (0.8% in the 1970s, 7.1% in the 1980s, and 14.6% in the 1990s) Nunavut polar bear biologist, Mitchell Taylor, who was formerly responsible for polar bear conservation in the territory, has insisted that bear numbers are being sustained under current hunting limits. In 2010, the 2005 increase was partially reversed. Government of Nunavut officials announced that the polar bear quota for the Baffin Bay region would be gradually reduced from 105 per year to 65 by the year 2013. The Government of the Northwest Territories maintain their own quota of 72 to 103 bears within the Inuvialuit communities of which some are set aside for sports hunters. Environment Canada also banned the export from Canada of fur, claws, skulls and other products from polar bears harvested in Baffin Bay as of 1 January 2010.

Because of the way polar bear hunting quotas are managed in Canada, attempts to discourage sport hunting would actually increase the number of bears killed in the short term. Canada allocates a certain number of permits each year to sport and subsistence hunting, and those that are not used for sport hunting are re-allocated to indigenous subsistence hunting. Whereas northern communities kill all the polar bears they are permitted to take each year, only half of sport hunters with permits actually manage to kill a polar bear. If a sport hunter does not kill a polar bear before his or her permit expires, the permit cannot be transferred to another hunter.

In August 2011, Environment Canada published a national polar bear conservation strategy.

In Greenland, hunting restrictions were first introduced in 1994 and expanded by executive order in 2005. Until 2005 Greenland placed no limit on hunting by indigenous people. However, in 2006 it imposed a limit of 150, while also allowed recreational hunting for the first time. Other provisions included year-round protection of cubs and mothers, restrictions on weapons used and various administrative requirements to catalogue kills.

Polar bear were hunted heavily in Svalbard, Norway throughout the 19th century and to as recently as 1973, when the conservation treaty was signed. 900 bears a year were harvested in the 1920s and after World War II, there were as many as 400–500 harvested annually. Some regulations of hunting did exist. In 1927, poisoning was outlawed while in 1939, certain denning sights were declared off limits. The killing of females and cubs was made illegal in 1965. Killing of polar bears decreased somewhat 25–30 years before the treaty. Despite this, the polar bear population continued to decline and by 1973, only around 1000 bears were left in Svalbard. Only with the passage of the treaty did they begin to recover.

The Soviet Union banned the harvest of polar bears in 1956; however, poaching continued and is estimated to pose a serious threat to the polar bear population. In recent years, polar bears have approached coastal villages in Chukotka more frequently due to the shrinking of the sea ice, endangering humans and raising concerns that illegal hunting would become even more prevalent. In 2007, the Russian government made subsistence hunting legal for indigenous Chukotkan peoples only, a move supported by Russia's most prominent bear researchers and the World Wide Fund for Nature as a means to curb poaching.

Polar bears are currently listed as "Rare", of "Uncertain Status", or "Rehabilitated and rehabilitating" in the Red Data Book of Russia, depending on population. In 2010, the Ministry of Natural Resources and Environment published a strategy for polar bear conservation in Russia.

The Marine Mammal Protection Act of 1972 afforded polar bears some protection in the United States. It banned hunting (except by indigenous subsistence hunters), banned importing of polar bear parts (except polar bear pelts taken legally in Canada), and banned the harassment of polar bears. On 15 May 2008, the United States Department of the Interior listed the polar bear as a threatened species under the Endangered Species Act, citing the melting of Arctic sea ice as the primary threat to the polar bear. It banned all importing of polar bear trophies. Importing products made from polar bears had been prohibited from 1972 to 1994 under the Marine Mammal Protection Act, and restricted between 1994 and 2008. Under those restrictions, permits from the United States Fish and Wildlife Service were required to import sport-hunted polar bear trophies taken in hunting expeditions in Canada. The permit process required that the bear be taken from an area with quotas based on sound management principles. Since 1994, hundreds of sport-hunted polar bear trophies have been imported into the U.S. In 2015, the U.S. Fish and Wildlife Service published a draft conservation management plan for polar bears to improve their status under the Endangered Species Act and the Marine Mammal Protection Act.

Polar bear population sizes and trends are difficult to estimate accurately because they occupy remote home ranges and exist at low population densities. Polar bear fieldwork can also be hazardous to researchers. As of 2015, the International Union for Conservation of Nature (IUCN) reports that the global population of polar bears is 22,000 to 31,000, and the current population trend is unknown. Nevertheless, polar bears are listed as "Vulnerable" under criterion A3c, which indicates an expected population decrease of ≥30% over the next three generations (~34.5 years) due to "decline in area of occupancy, extent of occurrence and/or quality of habitat". Risks to the polar bear include climate change, pollution in the form of toxic contaminants, conflicts with shipping, oil and gas exploration and development, and human-bear interactions including harvesting and possible stresses from recreational polar-bear watching.

According to the World Wildlife Fund, the polar bear is important as an indicator of Arctic ecosystem health. Polar bears are studied to gain understanding of what is happening throughout the Arctic, because at-risk polar bears are often a sign of something wrong with the Arctic marine ecosystem.

The International Union for Conservation of Nature, Arctic Climate Impact Assessment, United States Geological Survey and many leading polar bear biologists have expressed grave concerns about the impact of climate change, including the belief that the current warming trend imperils the survival of the polar bear.

The key danger posed by climate change is malnutrition or starvation due to habitat loss. Polar bears hunt seals from a platform of sea ice. Rising temperatures cause the sea ice to melt earlier in the year, driving the bears to shore before they have built sufficient fat reserves to survive the period of scarce food in the late summer and early fall. Reduction in sea-ice cover also forces bears to swim longer distances, which further depletes their energy stores and occasionally leads to drowning. Thinner sea ice tends to deform more easily, which appears to make it more difficult for polar bears to access seals. Insufficient nourishment leads to lower reproductive rates in adult females and lower survival rates in cubs and juvenile bears, in addition to poorer body condition in bears of all ages.
In addition to creating nutritional stress, a warming climate is expected to affect various other aspects of polar bear life: Changes in sea ice affect the ability of pregnant females to build suitable maternity dens. As the distance increases between the pack ice and the coast, females must swim longer distances to reach favored denning areas on land. Thawing of permafrost would affect the bears who traditionally den underground, and warm winters could result in den roofs collapsing or having reduced insulative value. For the polar bears that currently den on multi-year ice, increased ice mobility may result in longer distances for mothers and young cubs to walk when they return to seal-hunting areas in the spring. Disease-causing bacteria and parasites would flourish more readily in a warmer climate.

Problematic interactions between polar bears and humans, such as foraging by bears in garbage dumps, have historically been more prevalent in years when ice-floe breakup occurred early and local polar bears were relatively thin. Increased human-bear interactions, including fatal attacks on humans, are likely to increase as the sea ice shrinks and hungry bears try to find food on land.
The effects of climate change are most profound in the southern part of the polar bear's range, and this is indeed where significant degradation of local populations has been observed. The Western Hudson Bay subpopulation, in a southern part of the range, also happens to be one of the best-studied polar bear subpopulations. This subpopulation feeds heavily on ringed seals in late spring, when newly weaned and easily hunted seal pups are abundant. The late spring hunting season ends for polar bears when the ice begins to melt and break up, and they fast or eat little during the summer until the sea freezes again.

Due to warming air temperatures, ice-floe breakup in western Hudson Bay is currently occurring three weeks earlier than it did 30 years ago, reducing the duration of the polar bear feeding season. The body condition of polar bears has declined during this period; the average weight of lone (and likely pregnant) female polar bears was approximately in 1980 and in 2004. Between 1987 and 2004, the Western Hudson Bay population declined by 22%, although the population is currently listed as "stable". As the climate change melts sea ice, the U.S. Geological Survey projects that two-thirds of polar bears will disappear by 2050.

In Alaska, the effects of sea ice shrinkage have contributed to higher mortality rates in polar bear cubs, and have led to changes in the denning locations of pregnant females. In recent years, polar bears in the Arctic have undertaken longer than usual swims to find prey, possibly resulting in four recorded drownings in the unusually large ice pack regression of 2005.

A new development is that polar bears have begun ranging to new territory. While not unheard of but still uncommon, polar bears have been sighted increasingly in larger numbers ashore, staying on the mainland for longer periods of time during the summer months, particularly in North Canada, traveling farther inland. This may cause an increased reliance on terrestrial diets, such as goose eggs, waterfowl and caribou, as well as increased human–bear conflict.

Polar bears accumulate high levels of persistent organic pollutants such as polychlorinated biphenyl (PCBs) and chlorinated pesticides. Due to their position at the top of the ecological pyramid, with a diet heavy in blubber in which halocarbons concentrate, their bodies are among the most contaminated of Arctic mammals. Halocarbons are known to be toxic to other animals, because they mimic hormone chemistry, and biomarkers such as immunoglobulin G and retinol suggest similar effects on polar bears. PCBs have received the most study, and they have been associated with birth defects and immune system deficiency.

Many chemicals, such as PCBs and DDT, have been internationally banned due to the recognition of their harm on the environment. Their concentrations in polar bear tissues continued to rise for decades after being banned as these chemicals spread through the food chain. Since then, the trend seems to have discontinued, with tissue concentrations of PCBs declining between studies performed from 1989 to 1993 and studies performed from 1996 to 2002. During the same time periods, DDT was notably lower in the Western Hudson Bay population only.

Oil and gas development in polar bear habitat can affect the bears in a variety of ways. An oil spill in the Arctic would most likely concentrate in the areas where polar bears and their prey are also concentrated, such as sea ice leads. Because polar bears rely partly on their fur for insulation and soiling of the fur by oil reduces its insulative value, oil spills put bears at risk of dying from hypothermia. Polar bears exposed to oil spill conditions have been observed to lick the oil from their fur, leading to fatal kidney failure. Maternity dens, used by pregnant females and by females with infants, can also be disturbed by nearby oil exploration and development. Disturbance of these sensitive sites may trigger the mother to abandon her den prematurely, or abandon her litter altogether.

Steven Amstrup and other U.S. Geological Survey scientists have predicted two-thirds of the world's polar bears may disappear by 2050, based on moderate projections for the shrinking of summer sea ice caused by climate change, though the validity of this study has been debated. The bears could disappear from Europe, Asia, and Alaska, and be depleted from the Canadian Arctic Archipelago and areas off the northern Greenland coast. By 2080, they could disappear from Greenland entirely and from the northern Canadian coast, leaving only dwindling numbers in the interior Arctic Archipelago. However, in the short term, some polar bear populations in historically colder regions of the Arctic may temporarily benefit from a milder climate, as multiyear ice that is too thick for seals to create breathing holes is replaced by thinner annual ice.

Polar bears diverged from brown bears 400,000–600,000 years ago and have survived past periods of climate fluctuation. It has been claimed that polar bears will be able to adapt to terrestrial food sources as the sea ice they use to hunt seals disappears. However, most polar bear biologists think that polar bears will be unable to completely offset the loss of calorie-rich seal blubber with terrestrial foods, and that they will be outcompeted by brown bears in this terrestrial niche, ultimately leading to a population decline.

Warnings about the future of the polar bear are often contrasted with the fact that worldwide population estimates have increased over the past 50 years and are relatively stable today. Some estimates of the global population are around 5,000 to 10,000 in the early 1970s; other estimates were 20,000 to 40,000 during the 1980s. Current estimates put the global population at between 20,000 and 25,000 or 22,000 and 31,000.

There are several reasons for the apparent discordance between past and projected population trends: estimates from the 1950s and 1960s were based on stories from explorers and hunters rather than on scientific surveys. Second, controls of harvesting were introduced that allowed this previously overhunted species to recover. Third, the recent effects of climate change have affected sea ice abundance in different areas to varying degrees.

Debate over the listing of the polar bear under endangered species legislation has put conservation groups and Canada's Inuit at opposing positions; the Nunavut government and many northern residents have condemned the U.S. initiative to list the polar bear under the Endangered Species Act. Many Inuit believe the polar bear population is increasing, and restrictions on commercial sport-hunting are likely to lead to a loss of income to their communities.

For the indigenous peoples of the Arctic, polar bears have long played an important cultural and material role. Polar bear remains have been found at hunting sites dating to 2,500 to 3,000 years ago and 1,500-year-old cave paintings of polar bears have been found in the Chukchi Peninsula. Indeed, it has been suggested that Arctic peoples' skills in seal hunting and igloo construction has been in part acquired from the polar bears themselves.

The Inuit and Alaska Natives have many folk tales featuring the bears including legends in which bears are humans when inside their own houses and put on bear hides when going outside, and stories of how the constellation that is said to resemble a great bear surrounded by dogs came into being. These legends reveal a deep respect for the polar bear, which is portrayed as both spiritually powerful and closely akin to humans. The human-like posture of bears when standing and sitting, and the resemblance of a skinned bear carcass to the human body, have probably contributed to the belief that the spirits of humans and bears were interchangeable.

Among the Chukchi and Yupik of eastern Siberia, there was a longstanding shamanistic ritual of "thanksgiving" to the hunted polar bear. After killing the animal, its head and skin were removed and cleaned and brought into the home, and a feast was held in the hunting camp in its honor. To appease the spirit of the bear, traditional song and drum music was played, and the skull was ceremonially fed and offered a pipe. Only once the spirit was appeased was the skull be separated from the skin, taken beyond the bounds of the homestead, and placed in the ground, facing north.

The Nenets of north-central Siberia placed particular value on the talismanic power of the prominent canine teeth. These were traded in the villages of the lower Yenisei and Khatanga rivers to the forest-dwelling peoples further south, who would sew them into their hats as protection against brown bears. It was believed that the "little nephew" (the brown bear) would not dare to attack a man wearing the tooth of its powerful "big uncle", the polar bear. The skulls of killed polar bears were buried at sacred sites, and altars, called "sedyangi", were constructed out of the skulls. Several such sites have been preserved on the Yamal Peninsula.

Their distinctive appearance and their association with the Arctic have made polar bears popular icons, especially in those areas where they are native. The Canadian two-dollar coin carries an image of a lone polar bear on its reverse side, while a special millennium edition featured three. Vehicle license plates in the Northwest Territories and Nunavut in Canada are in the shape of a polar bear. The polar bear is the mascot of Bowdoin College, Maine, the University of Alaska Fairbanks, and the 1988 Winter Olympics held in Calgary. The Eisbären Berlin hockey team uses a roaring polar bear as their logo.

Companies such as Coca-Cola, Polar Beverages, Nelvana, Bundaberg Rum, and Good Humor-Breyers have used images of the polar bear in advertising, while Fox's Glacier Mints have featured a polar bear named Peppy as the brand mascot since 1922.

Polar bears are popular in fiction, particularly in books for children or teenagers. For example, "The Polar Bear Son" is adapted from a traditional Inuit tale. The animated television series "Noah's Island" features a polar bear named Noah as the protagonist. Polar bears feature prominently in "East" (also released as "North Child") by Edith Pattou, "The Bear" by Raymond Briggs (adapted into an animated short in 1998), and Chris d'Lacey's "The Fire Within" series. The "panserbjørne" of Philip Pullman's fantasy trilogy "His Dark Materials" are sapient, dignified polar bears who exhibit anthropomorphic qualities, and feature prominently in the 2007 film adaptation of "The Golden Compass". The television series "Lost" features polar bears living on the tropical island setting.





</doc>
<doc id="24411" url="https://en.wikipedia.org/wiki?curid=24411" title="Pagan (disambiguation)">
Pagan (disambiguation)

Pagan is an adept of Paganism, a large group of non-Abrahamic religions.

Pagan or Pagans may also refer to:








</doc>
<doc id="24412" url="https://en.wikipedia.org/wiki?curid=24412" title="Phalanx (disambiguation)">
Phalanx (disambiguation)

The phalanx is a rectangular mass military formation.

Phalanx may also refer to:










</doc>
<doc id="24413" url="https://en.wikipedia.org/wiki?curid=24413" title="Penguin Island">
Penguin Island

Penguin Island may refer to:




</doc>
<doc id="24416" url="https://en.wikipedia.org/wiki?curid=24416" title="Pommern (disambiguation)">
Pommern (disambiguation)

Pommern is the German language name for Pomerania, a historical region divided between Germany and Poland.

Pommern may also refer to:




</doc>
<doc id="24417" url="https://en.wikipedia.org/wiki?curid=24417" title="Punic Wars">
Punic Wars

The Punic Wars were a series of three wars fought between Rome and Carthage from 264 BC to 146 BC. At the time, they were some of the largest wars that had ever taken place. The term "Punic" comes from the Latin word "Punicus" (or "Poenicus"), meaning "Carthaginian", with reference to the Carthaginians' Phoenician ancestry.

The main cause of the Punic Wars was the conflicts of interest between the existing Carthaginian Empire and the expanding Roman Republic. The Romans were initially interested in expansion via Sicily (which at that time was a cultural melting pot), part of which lay under Carthaginian control. At the start of the First Punic War (264-241 BC), Carthage was the dominant power of the Western Mediterranean, with an extensive maritime empire. Rome was a rapidly ascending power in Italy, but it lacked the naval power of Carthage. The Second Punic War (218-201 BC) witnessed Hannibal's crossing of the Alps in 218 BC, followed by a prolonged but ultimately failed campaign of Carthage's Hannibal in mainland Italy. By the end of the Third Punic War (149-146 BC), after more than a hundred years and the loss of many hundreds of thousands of soldiers from both sides, Rome had conquered Carthage's empire, completely destroyed the city, and became the most powerful state of the Western Mediterranean.

With the end of the Macedonian Wars – which ran concurrently with the Punic Wars – and the defeat of the Seleucid King Antiochus III the Great in the Roman–Seleucid War (Treaty of Apamea, 188 BC) in the eastern sea, Rome emerged as the dominant Mediterranean power and one of the most powerful cities in classical antiquity. The Roman victories over Carthage in these wars gave Rome a preeminent status it would retain until the 5th century AD.

During the mid-3rd century BC, Carthage was a large city located on the coast of modern Tunisia. Founded by the Phoenicians in the mid-9th century BC, it was a powerful thalassocratic city-state with a vast commercial network. Of the great city-states in the western Mediterranean, only Rome rivaled it in power, wealth, and population. While Carthage's navy was the largest in the ancient world at the time, it did not maintain a large, permanent, standing army. Instead, Carthage relied mostly on mercenaries, especially the indigenous Numidians, to fight its wars. These mercenaries were primarily led by officers who were Carthaginian citizens. The Carthaginians were famed for their abilities as sailors, and many Carthaginians from the lower classes served in their navy, which provided them with a stable income and career.

In 200 BC, the Roman Republic had gained control of the Italian peninsula south of the Po River. Unlike Carthage, Rome had a large and disciplined army, but lacked a navy at the start of the First Punic War. This left the Romans at a disadvantage until the construction of large fleets during the war.

The First Punic War (264–241 BC) was fought partly on land in Sicily and Africa, but was largely a naval war. It began as a local conflict in Sicily between Hiero II of Syracuse and the Mamertines of Messina. The Mamertines enlisted the aid of the Carthaginian navy, and subsequently betrayed them by entreating the Roman Senate for aid against Carthage. The Romans sent a garrison to secure Messina, so the outraged Carthaginians then lent aid to Syracuse. Tensions quickly escalated into a full-scale war between Carthage and Rome for the control of Sicily.

After a harsh defeat at the Battle of Agrigentum in 262 BC, the Carthaginian leadership resolved to avoid further direct land-based engagements with the powerful Roman legions, and concentrate on the sea where they believed Carthage's large navy had the advantage. Initially the Carthaginian navy prevailed. In 260 BC, they defeated the fledgling Roman navy at the Battle of the Lipari Islands. Rome responded by drastically expanding its navy in a very short time. Within two months, the Romans had a fleet of over one hundred warships.

Aware that they could not defeat the Carthaginians in traditional ramming combat, the Romans used the "corvus", an assault bridge, to leverage their superior infantry. The hinged bridge would be swung down onto enemy vessels with a sharp spike to secure the two ships together. Roman legionaries could then board and capture Carthaginian ships. This innovative Roman tactic reduced the Carthaginian navy's advantage in ship-to-ship engagements.

However, the "corvus" was also cumbersome and dangerous, and was eventually phased out as the Roman navy became more experienced and tactically proficient. Save for the disastrous defeat at the Battle of Tunis in Africa, and the early naval defeats, the First Punic War was a nearly unbroken string of Roman victories. In 241 BC, Carthage signed a peace treaty under the terms of which they evacuated Sicily and paid Rome a large war indemnity. The long war was costly to both powers, but Carthage was more seriously destabilized.

According to Polybius, there had been several trade agreements between Rome and Carthage, even a mutual alliance against king Pyrrhus of Epirus. When Rome and Carthage made peace in 241 BC, Rome secured the release of all 8,000 prisoners of war without ransom and, furthermore, received a considerable amount of silver as a war indemnity. However, Carthage refused to deliver to Rome the Roman deserters serving among their troops. A first issue for dispute was that the initial treaty, agreed upon by Hamilcar Barca and the Roman commander in Sicily, had a clause stipulating that the Roman popular assembly had to accept the treaty in order for it to be valid. The assembly not only rejected the treaty but increased the indemnity Carthage had to pay.

Carthage had a liquidity problem and attempted to gain financial help from Egypt, a mutual ally of Rome and Carthage, but failed. This resulted in delay of payments owed to the mercenary troops that had served Carthage in Sicily, leading to a climate of mutual mistrust and, finally, a revolt supported by the Libyan natives, known as the Mercenary War (240–238 BC). During this war, Rome and Syracuse both aided Carthage, although traders from Italy seem to have done business with the insurgents. Some of them were caught and punished by Carthage, aggravating the political climate, which had started to improve in recognition of the old alliance and treaties.

During the uprising in the Punic mainland, the mercenary troops in Corsica and Sardinia toppled Punic rule and briefly established their own, but were expelled by a native uprising. After securing aid from Rome, the exiled mercenaries then regained authority on the island of Sicily. For several years, a brutal campaign was fought to quell the insurgent natives. Like many Sicilians, they would ultimately rise again in support of Carthage during the Second Punic War.

Eventually, Rome annexed Corsica and Sardinia by revisiting the terms of the treaty that ended the first Punic War. As Carthage was under siege and engaged in a difficult civil war, they grudgingly accepted the loss of these islands and the subsequent Roman conditions for ongoing peace, which also increased the war indemnity levied against Carthage after the first Punic War. This eventually plunged relations between the two powers to a new low point.

After Carthage emerged victorious from the Mercenary War there were two opposing factions: the reformist party was led by Hamilcar Barca while the other, more conservative, faction was represented by Hanno the Great and the old Carthaginian aristocracy. Hamilcar had led the initial Carthaginian peace negotiations and was blamed for the clause that allowed the Roman popular assembly to increase the war indemnity and annex Corsica and Sardinia, but his superlative generalship was instrumental in enabling Carthage to ultimately quell the mercenary uprising, ironically fought against many of the same mercenary troops he had trained. Hamilcar ultimately left Carthage for the Iberian peninsula where he captured rich silver mines and subdued many tribes who fortified his army with levies of native troops.

Hanno had lost many elephants and soldiers when he became complacent after a victory in the Mercenary War. Further, when he and Hamilcar were supreme commanders of Carthage's field armies, the soldiers had supported Hamilcar when his and Hamilcar's personalities clashed. On the other hand, he was responsible for the greatest territorial expansion of Carthage's hinterland during his rule as "strategus" and wanted to continue such expansion. However, the Numidian king of the relevant area was now a son-in-law of Hamilcar and had supported Carthage during a crucial moment in the Mercenary War. While Hamilcar was able to obtain the resources for his aim, the Numidians in the Atlas Mountains were not conquered, like Hanno suggested, but became vassals of Carthage.

The Iberian conquest was begun by Hamilcar Barca and his other son-in-law, Hasdrubal the Fair, who ruled relatively independently of Carthage and signed the Ebro Treaty with Rome. Hamilcar died in battle in 228 BC. Around this time, Hasdrubal became Carthaginian commander in Iberia (229 BC). He maintained this post for some eight years until 221 BC. Soon the Romans became aware of a burgeoning alliance between Carthage and the Celts of the Po river valley in northern Italy. The latter were amassing forces to invade Italy, presumably with Carthaginian backing. Thus, the Romans preemptively invaded the Po region in 225 BC. By 220 BC, the Romans had annexed the area as Gallia Cisalpina. Hasdrubal was assassinated around the same time (221 BC), bringing Hannibal to the fore. It seems that, having apparently dealt with the threat of a Gallo-Carthaginian invasion of Italy (and perhaps with the original Carthaginian commander killed), the Romans lulled themselves into a false sense of security. Thus, Hannibal took the Romans by surprise a mere two years later (218 BC) by merely reviving and adapting the original Gallo-Carthaginian invasion plan of his brother-in-law Hasdrubal.
After Hasdrubal's assassination by a Celtic assassin, Hamilcar's young sons took over, with Hannibal becoming the "strategus" of Iberia, although this decision was not undisputed in Carthage. The output of the Iberian silver mines allowed for the financing of a standing army and the payment of the war indemnity to Rome. The mines also served as a tool for political influence, creating a faction in Carthage's magistrate that was called the "Barcino".

In 219 BC, Hannibal attacked the town of Saguntum, which stood under the special protection of Rome. According to Roman tradition, Hannibal had been made to swear by his father never to be a friend of Rome, and he certainly did not take a conciliatory attitude when the Romans berated him for crossing the river Iberus (Ebro), which Carthage was bound by treaty not to cross. Hannibal did not cross the Ebro River (Saguntum was near modern Valencia – well south of the river) in arms, and the Saguntines provoked his attack by attacking their neighboring tribes who were Carthaginian protectorates and by massacring pro-Punic factions in their city. Rome had no legal protection pact with any tribe south of the Ebro River. Nonetheless, they asked Carthage to hand Hannibal over, and when the Carthaginian oligarchy refused, Rome declared war on Carthage.

The 'Barcid Empire' consisted of the Punic territories in Iberia. According to the historian Pedro Barceló, it can be described as a private military-economic hegemony backed by the two independent powers, Carthage and Gades (modern Cádiz). These shared the profits of the silver mines in southern Iberia with the Barcas family and closely followed Hellenistic diplomatic customs. Gades played a supporting role in this field, but Hannibal visited the local temple to conduct ceremonies before launching his campaign against Rome. The Barcid Empire was strongly influenced by the Hellenistic kingdoms of the time and for example, contrary to Carthage, it minted silver coins in its short time of existence.

The Second Punic War (218 BC – 201 BC) is most remembered for the Carthaginian Hannibal's crossing of the Alps. His army invaded Italy from the north and resoundingly defeated the Roman army in several battles, but never achieved the ultimate goal of causing a political break between Rome and its allies.

While fighting Hannibal in Italy, Hispania, and Sicily, Rome simultaneously fought against Macedon in the First Macedonian War. Eventually, the war was taken to Africa, where Carthage was defeated at the Battle of Zama (201 BC) by Scipio Africanus. The end of the war saw Carthage's control reduced to only the city itself.

There were three military theaters in this war: Italy, where Hannibal defeated the Roman legions repeatedly; Hispania, where Hasdrubal, a younger brother of Hannibal, defended the Carthaginian colonial cities with mixed success until eventually retreating into Italy; and Sicily, where the Romans held military supremacy.

After assaulting Saguntum in Hispania (219 BC), Hannibal attacked Italy in 218 BC by leading the Iberians and three dozen elephants through the Alps. Although Hannibal surprised the Romans and thoroughly beat them on the battlefields of Italy, he lost his only siege engines and most of his elephants to the cold temperatures and icy mountain paths. In the end he could defeat the Romans in the field, but not in the strategically crucial city of Rome itself, thus leaving him unable to win the war.

Hannibal defeated the Roman legions in several major engagements, including the Battle of the Trebia (December 218 BC), the Battle of Lake Trasimene (217 BC) and most famously the Battle of Cannae (216 BC), but his long-term strategy failed. Lacking siege engines and sufficient manpower to take the city of Rome itself, he had planned to turn the Italian allies against Rome and to starve the city out through a siege. However, with the exception of a few of the southern city-states, the majority of the Roman allies remained loyal and continued to fight alongside Rome, despite Hannibal's near-invincible army devastating the Italian countryside. Rome also exhibited an impressive ability to draft army after army of conscripts after each crushing defeat by Hannibal, allowing them to recover from the defeats at Cannae and elsewhere and to keep Hannibal cut off from aid.

Hannibal never successfully received any significant reinforcements from Carthage. Despite his many pleas, Carthage only ever sent reinforcements successfully to Hispania. This lack of reinforcements prevented Hannibal from decisively ending the conflict by conquering Rome through force of arms.

The Roman army under Quintus Fabius Maximus Verrucosus intentionally deprived Hannibal of open battle in Italy for the rest of the war, while making it difficult for Hannibal to forage for supplies. Nevertheless, Rome was also incapable of bringing the conflict in the Italian theatre to a decisive close. Not only did Roman legions contend with Hannibal in Italy and with Hannibal's brother Hasdrubal in Hispania, but Rome had embroiled itself in yet another foreign war, the first of its Macedonian wars against Carthage's ally Philip V, at the same time.

Through Hannibal's inability to take strategically important Italian cities, through the general loyalty Italian allies showed to Rome, and through Rome's own inability to counter Hannibal as a master general, Hannibal's campaign continued in Italy inconclusively for sixteen years. Though he managed to sustain his forces for 15 years, Hannibal did so only by ravaging farm-lands, keeping his army healthy, which brought anger among the Romans' subject states. Realizing that Hannibal's army was outrunning its supply lines quickly, Rome took countermeasures against Hannibal's home base in Africa by sea command and stopped the flow of supplies. Hannibal quickly turned back and rushed to home defense, but suffered defeat in the Battle of Zama (202 BC).

In Hispania, a young Roman commander, Publius Cornelius Scipio (later to be given the agnomen "Africanus" because of his feats during this war), eventually defeated the larger but divided Carthaginian forces under Hasdrubal and two other Carthaginian generals. Abandoning Hispania, Hasdrubal moved to bring his mercenary army into Italy to reinforce Hannibal, but never made it and was defeated by Roman forces near the Alps.

The Third Punic War (149–146 BC) involved an extended siege of Carthage, ending in the city's thorough destruction. The resurgence of the struggle can be explained by growing anti-Roman agitations in Hispania and Greece, and the visible improvement of Carthaginian wealth and martial power in the fifty years since the Second War.

With no military, Carthage suffered raids from its neighbor Numidia. Under the terms of the treaty with Rome, such disputes were arbitrated by the Roman Senate. Because Numidia was a favored client state of Rome, Roman rulings were slanted heavily in favor of the Numidians. After some fifty years of this condition, Carthage had managed to discharge its war indemnity to Rome, and considered itself no longer bound by the restrictions of the treaty, although Rome believed otherwise. Carthage mustered an army to repel Numidian forces. It immediately lost the war with Numidia, placing itself in debt yet again, this time to Numidia.

This new-found Punic militarism alarmed many Romans, including Cato the Elder who, after a voyage to Carthage, ended all his speeches, no matter what the topic, by saying: "Ceterum censeo Carthaginem esse delendam" – "And I also think that Carthage must be destroyed".

In 149 BC, in an attempt to draw Carthage into open conflict, Rome made a series of escalating demands, one being the surrender of three hundred children of the nobility as hostages, and finally ending with the near-impossible demand that the city be demolished and rebuilt away from the coast, deeper into Africa. When the Carthaginians refused this last demand, Rome declared the Third Punic War. Having previously relied on mercenaries to fight their wars for them, the Carthaginians were now forced into a more active role in the defense of their city. They made thousands of makeshift weapons in a short time, even using women's hair for catapult strings, and were able to hold off the initial Roman attack. A second offensive under the command of Scipio Aemilianus resulted in a three-year siege before he breached the walls, sacked the city, and systematically burned Carthage to the ground in 146 BC. When the war ended, the remaining 50,000 Carthaginians, a small part of the original pre-war population, were sold into slavery by the victors – the normal fate in antiquity of inhabitants of sacked cities. Carthage was systematically burned for 17 days; the city's walls and buildings were utterly destroyed. The remaining Carthaginian territories were annexed by Rome and reconstituted to become the Roman province of Africa.

After Rome emerged as victorious, significant Carthaginian settlements, such as those in Mauretania, were taken over and aggrandized by the Romans. Volubilis, for example, was an important Roman town situated near the westernmost border of the Roman conquests. It was built on the site of the previous Carthaginian settlement that overlies an earlier neolithic habitation.



</doc>
<doc id="24419" url="https://en.wikipedia.org/wiki?curid=24419" title="Peter Carey (novelist)">
Peter Carey (novelist)

Peter Philip Carey AO (born 7 May 1943) is an Australian novelist. Carey has won the Miles Franklin Award three times and is frequently named as Australia's next contender for the Nobel Prize in Literature. Carey is one of only four writers to have won the Booker Prize twice—the others being J. G. Farrell, J. M. Coetzee and Hilary Mantel. Carey won his first Booker Prize in 1988 for "Oscar and Lucinda", and won for the second time in 2001 with "True History of the Kelly Gang". In May 2008 he was nominated for the Best of the Booker Prize.

In addition to writing fiction, he collaborated on the screenplay of the film "Until the End of the World" with Wim Wenders and is executive director of the Master of Fine Arts in Creative Writing program at Hunter College, part of the City University of New York.

Peter Carey was born in Bacchus Marsh, Victoria, in 1943. His parents ran a General Motors dealership, Carey Motors. He attended Bacchus Marsh State School from 1948 to 1953, then boarded at Geelong Grammar School between 1954 and 1960. In 1961, Carey enrolled in a science degree at the new Monash University in Melbourne, majoring in chemistry and zoology, but cut his studies short because of a car accident and a lack of interest. It was at university that he met his first wife, Leigh Weetman, who was studying German and philosophy, and who also dropped out.

In 1962, he began to work in advertising. He was employed by various Melbourne agencies between 1962 and 1967, including on campaigns for Volkswagen and Lindeman's Wine. His advertising work brought him into contact with older writers who introduced him to recent European and American fiction: "I didn't really start getting an education until I worked in advertising with people like Barry Oakley and Morris Lurie—and Bruce Petty had an office next door."

During this time, he read widely, particularly the works of Samuel Beckett, William Faulkner, James Joyce, Franz Kafka, and Gabriel García Márquez, and began writing on his own, receiving his first rejection slip in 1964, the same year he married Weetman. Over the next few years he wrote five novels—"Contacts" (1964–1965), "Starts Here, Ends Here" (1965–1967), "The Futility Machine" (1966–1967), "Wog" (1969), and "Adventures on Board the Marie" [sic] "Celeste" (1971). None of them were published. Sun Books accepted "The Futility Machine" but did not proceed with publication, and "Adventures on Board the Marie Celeste" was accepted by Outback Press before being withdrawn by Carey himself. These and other unpublished manuscripts from the period—including twenty-one short stories—are now held by the Fryer Library at the University of Queensland.

Carey's only publications during the 1960s were "Contacts" (a short extract from the unpublished novel of the same name, in "Under Twenty-Five: An Anthology", 1966) and "She Wakes" (a short story, in "Australian Letters", 1967). Towards the end of the decade, Carey and Weetman abandoned Australia with "a certain degree of self-hatred", travelling through Europe and Iran before settling in London in 1968, where Carey continued to write highly regarded advertising copy and unpublished fiction.

Returning to Australia in 1970, Carey once again did advertising work in Melbourne and Sydney. He also kept writing, and gradually broke through with editors, publishing short stories in magazines and newspapers such as "Meanjin" and "Nation Review". Most of these were collected in his first book, "The Fat Man In History", which appeared in 1974. In the same year Carey moved to Balmain in Sydney to work for Grey Advertising.

In 1976, Carey moved to Queensland and joined an alternative community named Starlight in Yandina, north of Brisbane, with his new partner, the painter Margot Hutcheson, with whom he lived in the 1970s and 1980s. He remained with Grey, writing in Yandina for three weeks, then spending the fourth week at the agency in Sydney. It was during this time that he produced most of the stories collected in "War Crimes" (1979), as well as "Bliss" (1981), his first published novel.

Carey started his own advertising agency in 1980, the Sydney-based McSpedden Carey Advertising Consultants, in partnership with Bani McSpedden. After many years of separation, Leigh Weetman asked for a divorce in 1980 so that she could remarry and Peter agreed. In 1981, he moved to Bellingen in northern New South Wales. There he wrote "Illywhacker", published in 1985. In the same year he married theatre director Alison Summers. "Illusion", a stage musical Carey wrote with Mike Mullins and composer Martin Armiger, was performed at the 1986 Adelaide Festival of the Arts and a studio cast recording of the musical was nominated for a 1987 ARIA Award (for which Carey as lyricist was nominated).

The decade—and the Australian phase of Carey's career—culminated with the publication of "Oscar and Lucinda" (1988), which won the Booker McConnell Prize (as it was then known) and brought the author international recognition. Carey explained that the novel was inspired, in part, by his time in Bellingen:

Carey sold his share of McSpedden Carey and in 1990 moved with Alison Summers and their son to New York, where he took a job teaching creative writing at New York University. He later said that New York would not have been his first choice of place to live, and that moving there was his wife's idea. Carey and Summers divorced in 2005 after a four-year separation. Carey is now married to the British-born publisher Frances Coady.

"The Tax Inspector" (1991), begun in Australia, was the first book he completed in the United States. It was followed by "The Unusual Life of Tristan Smith" (1994), a fable in which he explored the relationship between Australia and America, disguised in the novel as "Efica" and "Voorstand". This is a relationship that has preoccupied him throughout his career, going back to "Bliss" (1981), "Illywhacker" (1985), and the early short stories. Nevertheless, Carey continued to set his fiction primarily in Australia and remained diffident about writing explicitly on American themes. In a piece on "True History of the Kelly Gang" (2001), Mel Gussow reported that:

It was only after nearly two decades in the United States that he embarked on "Parrot and Olivier in America" (2010), loosely based on events in the life of Alexis de Tocqueville. Carey says "Tocqueville opened a door I could enter. I saw the present in the past. It was accessible, imaginable." Carey continues to extend his canvas; in his most recent novel, "The Chemistry of Tears" (2012), "contemporary London is brought intimately in touch with ... a 19th-century Germany redolent of the Brothers Grimm".

In 1998, Carey was accused of snubbing Queen Elizabeth II by declining an invitation to meet her after winning the Commonwealth Writers Prize for "Jack Maggs" (1997). While Carey is a republican, in the Australian sense, he insists that no offence was intended:

The meeting did eventually take place, with the Queen remarking, according to Carey, "I believe you had a little trouble getting here."

The unhappy circumstances of Carey's break-up with Alison Summers received publicity (largely in Australia) in 2006 when "" appeared, depicting the toxic relationship between its protagonist, Butcher Bones, and his ex-wife, known only as "the Plaintiff".

In April 2015 he, alongside Michael Ondaatje, Francine Prose, Teju Cole, Rachel Kushner and Taiye Selasi, withdrew from the PEN American Center gala honouring the French satirical magazine "Charlie Hebdo" with its "Freedom of Expression Courage" award. He stated that one of his reasons for doing so was "PEN’s seeming blindness to the cultural arrogance of the French nation, which does not recognise its moral obligation to a large and disempowered segment of their population.". In addition, 204 PEN members, including Teju Cole and Deborah Eisenberg, wrote to PEN, objecting to its decision to give the award to Charlie Hebdo.

Carey has been awarded three honorary degrees. He has been elected a Fellow of the Royal Society of Literature (1989), an Honorary Fellow of the Australian Academy of the Humanities (2001), a Member of the American Academy of Arts and Sciences (2003), and a Member of the American Academy of Arts and Letters (2016), which has also awarded him its Harold D Vursell Memorial Award (2012). In 2010, he appeared on two Australian postage stamps in a series dedicated to "Australian Legends". On 11 June 2012, Carey was named an Officer of the Order of Australia for "distinguished service to literature as a novelist, through international promotion of the Australian identity, as a mentor to emerging writers." And in 2014, Carey was awarded an honorary Doctor of Letters (honoris causa) by Sydney University.

Carey has won numerous literary awards, including:


Stories from Carey's first two collections have been repackaged in "The Fat Man in History and Other Stories" (1980), "Exotic Pleasures" (1990), and "Collected Stories" (1994); the last also includes three previously uncollected stories: "Joe" ("Australian New Writing", 1973), "A Million Dollars Worth of Amphetamines" ("Nation Review", 1975), and "Concerning the Greek Tyrant" ("The Tabloid Story Pocket Book", 1978).








</doc>
<doc id="24420" url="https://en.wikipedia.org/wiki?curid=24420" title="Punched card">
Punched card

A punched card or punch card is a piece of stiff paper that can be used to contain digital data represented by the presence or absence of holes in predefined positions. Digital data can be for data processing applications or, in earlier examples, used to directly control automated machinery. 

Punched cards were widely used through much of the 20th century in the data processing industry, where specialized and increasingly complex unit record machines, organized into semiautomatic data processing systems, used punched cards for data input, output, and storage. Many early digital computers used punched cards, often prepared using keypunch machines, as the primary medium for input of both computer programs and data.

While punched cards are now obsolete as a storage medium, as of 2012, some voting machines still use punched cards to record votes.

Basile Bouchon developed the control of a loom by punched holes in paper tape in 1725. The design was improved by his assistant Jean-Baptiste Falcon and Jacques Vaucanson (1740) Although these improvements controlled the patterns woven, they still required an assistant to operate the mechanism. In 1804 Joseph Marie Jacquard demonstrated a mechanism to automate loom operation. A number of punched cards were linked into a chain of any length. Each card held the instructions for shedding (raising and lowering the warp) and selecting the shuttle for a single pass. It is considered an important step in the history of computing hardware.
Semyon Korsakov was reputedly the first to use the punched cards in informatics for information store and search. Korsakov announced his new method and machines in September 1832; rather than seeking patents, he offered the machines for public use.

Charles Babbage proposed the use of "Number Cards", "pierced with certain holes and stand opposite levers connected with a set of figure wheels ... advanced they push in those levers opposite to which there are no holes on the card and thus transfer that number" in his description of the Calculating Engine's Store.

In 1881 Jules Carpentier developed a method of recording and playing back performances on a harmonium using punched cards. The system was called the "Mélographe Répétiteur" and “writes down ordinary music played on the keyboard dans la langage de Jacquard”, that is as holes punched in a series of cards. By 1887 Carpentier had separated the mechanism into the "Melograph" which recorded the player's key presses and the "Melotrope" which played the music.

At the end of the 1800s Herman Hollerith invented the recording of data on a medium that could then be read by a machine. "After some initial trials with paper tape, he settled on punched cards...", developing punched card data processing technology for the 1890 US census. His tabulating machines read and summarized data stored on punched cards and they began use for government and commercial data processing. Initially, these electromechanical machines only counted holes, but by the 1920s they had units for carrying out basic arithmetic operations. 

Hollerith founded the "Tabulating Machine Company" (1896) which was one of four companies that were amalgamated (via stock acquisition) to form a fifth company, Computing-Tabulating-Recording Company (CTR) (1911), later renamed International Business Machines Corporation (IBM) (1924). Other companies entering the punched card business included The Tabulator Limited (1902) (later renamed the British Tabulating Machine Company), Deutsche Hollerith-Maschinen Gesellschaft mbH (Dehomag) (1911), Powers Accounting Machine Company (1911), Remington Rand (1927), and H.W. Egli Bull (1931). These companies, and others, manufactured and marketed a variety of punched cards and unit record machines for creating, sorting, and tabulating punched cards, even after the development of electronic computers in the 1950s.

Both IBM and Remington Rand tied punched card purchases to machine leases, a violation of the 1914 Clayton Antitrust Act. In 1932, the US government took both to court on this issue. Remington Rand settled quickly. IBM viewed its business as providing a service and that the cards were part of the machine. IBM fought all the way to the Supreme Court and lost in 1936; the court ruling that IBM could only set card specifications.

"By 1937... IBM had 32 presses at work in Endicott, N.Y., printing, cutting and stacking five to 10 million punched cards every day." Punched cards were even used as legal documents, such as U.S. Government checks and savings bonds.

During WW II punched card equipment was used by the Allies in some of their efforts to decrypt Axis communications. See, for example, Central Bureau in Australia. At Bletchley Park in England, 2,000,000 punched cards were used each week for storing decrypted German messages.

Punched card technology developed into a powerful tool for business data-processing. By 1950 punched cards had become ubiquitous in industry and government. "Do not fold, spindle or mutilate," a generalized version of the warning that appeared on some punched cards (generally on those distributed as paper documents to be later returned for further machine processing, checks for example), became a motto for the post-World War II era.

In 1955 IBM signed a consent decree requiring, amongst other things, that IBM would by 1962 have no more than one-half of the punched card manufacturing capacity in the United States. Tom Watson Jr.'s decision to sign this decree, where IBM saw the punched card provisions as the most significant point, completed the transfer of power to him from Thomas Watson, Sr.

The UNITYPER introduced magnetic tape for data entry in the 1950s. During the 1960s, the punched card was gradually replaced as the primary means for data storage by magnetic tape, as better, more capable computers became available. Mohawk Data Sciences introduced a magnetic tape encoder in 1965, a system marketed as a keypunch replacement which was somewhat successful. Punched cards were still commonly used for entering both data and computer programs until the mid-1980s when the combination of lower cost magnetic disk storage, and affordable interactive terminals on less expensive minicomputers made punched cards obsolete for these roles as well. However, their influence lives on through many standard conventions and file formats. The terminals that replaced the punched cards, the IBM 3270 for example, displayed 80 columns of text in text mode, for compatibility with existing software. Some programs still operate on the convention of 80 text columns, although fewer and fewer do as newer systems employ graphical user interfaces with variable-width type fonts.

The terms "punched card", "punch card", and "punchcard" were all commonly used, as were "IBM card" and "Hollerith card" (after Herman Hollerith). IBM used "IBM card" or, later, "punched card" at first mention in its documentation and thereafter simply "card" or "cards". Specific formats were often indicated by the number of character positions available, e.g. "80-column card". A sequence of cards that is input to or output from some step in an application's processing is called a "card deck" or simply "deck". The rectangular, round, or oval bits of paper punched out were called chad ("chads") or "chips" (in IBM usage). Sequential card columns allocated for a specific use, such as names, addresses, multi-digit numbers, etc., are known as a "field". The first card of a group of cards, containing fixed or indicative information for that group, is known as a "master card". Cards that are not master cards are "detail cards".

The Hollerith punched cards used for the US 1890 census were blank. Following that, cards commonly had printing such that the row and column position of a hole could be easily seen. Printing could include having fields named and marked by vertical lines, logos, and more. "General purpose" layouts (see, for example, the IBM 5081 below) were also available. For applications requiring master cards to be separated from following detail cards, the respective cards had different upper corner diagonal cuts and thus could be separated by a sorter. Other cards typically had one upper corner diagonal cut so that cards not oriented correctly, or cards with different corner cuts, could be identified.

Herman Hollerith was awarded a series of patents in 1889 for electromechanical tabulating machines. These patents described both paper tape and rectangular cards as possible recording media. The card shown in of June 8 was printed with a template and had hole positions arranged close to the edges so they could be reached by a railroad conductor's ticket punch, with the center reserved for written descriptions. Hollerith was originally inspired by railroad tickets that let the conductor encode a rough description of the passenger:

When use of the ticket punch proved tiring and error prone Hollerith developed the pantograph "keyboard punch". It featured an enlarged diagram of the card, indicating the positions of the holes to be punched. A printed reading board could be placed under a card that was to be read manually.

Hollerith envisioned a number of card sizes. In an article he wrote describing his proposed system for tabulating the 1890 U.S. Census, Hollerith suggested a card 3 inches by 5½ inches of Manila stock "would be sufficient to answer all ordinary purposes." The cards used in the 1890 census had round holes, 12 rows and 24 columns. A reading board for these cards can be seen at the Columbia University Computing History site. At some point, became the standard card size. These are the dimensions of the then current paper currency of 1862–1923.

Hollerith's original system used an ad-hoc coding system for each application, with groups of holes assigned specific meanings, e.g. sex or marital status. His tabulating machine had up to 40 counters, each with a dial divided into 100 divisions, with two indicator hands; one which stepped one unit with each counting pulse, the other which advanced one unit every time the other dial made a complete revolution. This arrangement allowed a count up to 9,999. During a given tabulating run counters were assigned specific holes or, using relay logic, combination of holes.

Later designs led to a card with ten rows, each row assigned a digit value, 0 through 9, and 45 columns. 
This card provided for fields to record multi-digit numbers that tabulators could sum, instead of their simply counting cards. Hollerith's 45 column punched cards are illustrated in Comrie's "The application of the Hollerith Tabulating Machine to Brown's Tables of the Moon".

By the late 1920s customers wanted to store more data on each punched card. Thomas J. Watson Sr., IBM’s head, asked two of his top inventors, Clair D. Lake and J. Royden Peirce, to independently develop ways to increase data capacity without increasing the size of the punched card. Pierce wanted to keep round holes and 45 columns, but allow each column to store more data. Lake suggested rectangular holes, which could be spaced more tightly, allowing 80 columns per punched card, thereby nearly doubling the capacity of the older format. Watson picked the latter solution, introduced as "The IBM Computer Card," in part because it was compatible with existing tabulator designs and in part because it could be protected by patents and give the company a distinctive advantage.

This IBM card format, introduced in 1928,
has rectangular holes, 80 columns, and 12 rows. Card size is exactly by inches (187.325 mm × 82.55 mm). The cards are made of smooth stock, thick. There are about 143 cards to the inch (143/2.54round0/cm). In 1964, IBM changed from square to round corners. They come typically in boxes of 2000 cards or as continuous form cards. Continuous form cards could be both pre-numbered and pre-punched for document control (checks, for example). 

Initially designed to record responses to Yes–no questions, support for numeric, alphabetic and special characters was added through the use of columns and zones. The top three positions of a column are called zone punching positions, 12 (top), 11, and 0 (0 may be either a zone punch or a digit punch). For decimal data the lower ten positions are called digit punching positions, 0 (top) through 9. An arithmetic sign can be specified for a decimal field by overpunching the field's rightmost column with a zone punch: 12 for plus, 11 for minus (CR). For Pound sterling pre-decimalization currency a penny column represents the values zero through eleven; 10 (top), 11, then 0 through 9 as above. An arithmetic sign can be punched in the adjacent shilling column. Zone punches had other uses in processing, such as indicating a master card.
　Reference: Note: The 11 and 12 zones were also called the X and Y zones, respectively.

In 1931 IBM began introducing upper-case letters and special characters (Powers-Samas had developed the first commercial alphabetic punched card representation in 1921). The 26 letters have two punches (zone [12,11,0] + digit [1–9]). The languages of Germany, Sweden, Denmark, Norway, Spain, Portugal and Finland require up to three additional letters; their punching is not shown here. Most special characters have two or three punches (zone [12,11,0, or none] + digit [2–7] + 8); a few special characters were exceptions: "&" is 12 only, "-" is 11 only, and "/" is 0 + 1). The Space character has no punches. The information represented in a column by a combination of zones [12, 11, 0] and digits [0–9] is dependent on the use of that column. For example, the combination "12-1" is the letter "A" in an alphabetic column, a plus signed digit "1" in a signed numeric column, or an unsigned digit "1" in a column where the "12" has some other use. The introduction of EBCDIC in 1964 defined columns with as many as six punches (zones [12,11,0,8,9] + digit [1–7]). IBM and other manufacturers used many different 80-column card character encodings. A 1969 American National Standard defined the punches for 128 characters and was named the "Hollerith Punched Card Code" (often referred to simply as "Hollerith Card Code"), honoring Hollerith. 
For some computer applications, binary formats were used, where each hole represented a single binary digit (or "bit"), every column (or row) is treated as a simple bit field, and every combination of holes is permitted.
As a prank, in binary mode, punched cards could be made where every possible punch position had a hole. Such "lace cards" lacked structural strength, and would frequently buckle and jam inside the machine.

The IBM 80-column punched card format dominated the industry, becoming known as just IBM cards, even though other companies made cards and equipment to process them.
One of the most common punched card formats is the IBM 5081 card format, a general purpose layout with no field divisions. This format has digits printed on it corresponding to the punch positions of the digits in each of the 80 columns. Other punched card vendors manufactured cards with this same layout and number.

The 80-column card could be scored, on either end, creating a stub that could be torn off, leaving a "stub card" or "short card". A common length for stub cards was 51-columns. Stub cards were used in applications requiring tags, labels, or carbon copies.

According to the IBM Archive: "IBM's Supplies Division introduced the Port-A-Punch in 1958 as a fast, accurate means of manually punching holes in specially scored IBM punched cards. Designed to fit in the pocket, Port-A-Punch made it possible to create punched card documents anywhere. The product was intended for "on-the-spot" recording operations—such as physical inventories, job tickets and statistical surveys—because it eliminated the need for preliminary writing or typing of source documents."

In the late 1960s, IBM introduced a new, smaller, round-hole, 96-column card format along with the IBM System/3 computer. These cards have tiny (1 mm), circular holes, smaller than those in paper tape. Data is stored in 6-bit BCD, with three rows of 32 characters each, or 8-bit EBCDIC. In this format, each column of the top tiers are combined with two punch rows from the bottom tier to form an 8-bit byte, and the middle tier is combined with two more punch rows, so that each card contains 64 bytes of 8-bit-per-byte binary coded data.

The Powers/Remington Rand card format was initially the same as Hollerith's; 45 columns and round holes. In 1930, Remington Rand leap-frogged IBM's 80 column format from 1928 by coding two characters in each of the 45 columns – producing what is now commonly called the 90-column card. There are two sets of six rows across each card. The rows in each set are labeled 0, 1/2, 3/4, 5/6, 7/8 and 9. The even numbers in a pair are formed by combining that punch with a 9 punch. Alphabetic and special characters use 3 or more punches

The Powers-Samas card formats began with 45 columns and round holes. Later 36, 40 and 65 column cards were provided. A 130 column card was also available - formed by dividing the card into two rows, each row with 65 columns and each character space with 5 punch positions. A 21 column card was comparable to the IBM Stub card.



IBM's Fred M. Carroll developed a series of rotary presses that were used to produce punched cards, including a 1921 model that operated at 460 cards per minute (cpm). In 1936 he introduced a completely different press that operated at 850 cpm. Carroll's high-speed press, containing a printing cylinder, revolutionized the company's manufacturing of punched cards. It is estimated that between 1930 and 1950, the Carroll press accounted for as much as 25 percent of the company's profits.

Discarded printing plates from these card presses, each printing plate the size of an IBM card and formed into a cylinder, often found use as desk pen/pencil holders, and even today are collectible IBM artifacts (every card layout had its own printing plate).

While punched cards have not been widely used for a generation, the impact was so great for most of the 20th century that they still appear from time to time in popular culture. For example:

metaphor... symbol of the "system"—first the registration system and then bureaucratic systems more generally ... a symbol of alienation ... Punched cards were the symbol of information machines, and so they became the symbolic point of attack. Punched cards, used for class registration, were first and foremost a symbol of uniformity. ... A student might feel "he is one of out of 27,500 IBM cards" ... The president of the Undergraduate Association criticized the University as "a machine ... IBM pattern of education."... Robert Blaumer explicated the symbolism: he referred to the "sense of impersonality... symbolized by the IBM technology."...
––Steven Lubar



Processing of punched cards was handled by a variety of machines, including:






</doc>
<doc id="24421" url="https://en.wikipedia.org/wiki?curid=24421" title="Profiler">
Profiler

Profiler may refer to:



</doc>
<doc id="24422" url="https://en.wikipedia.org/wiki?curid=24422" title="Pasteur (disambiguation)">
Pasteur (disambiguation)

Pasteur may refer to:





</doc>
<doc id="24423" url="https://en.wikipedia.org/wiki?curid=24423" title="Pope Innocent I">
Pope Innocent I

Pope Innocent I (; d. 12 March 417) served as the Pope of the Catholic Church from 401 to his death in 417. From the beginning of his papacy, he was seen as the general arbitrator of ecclesiastical disputes in both the East and the West. He confirmed the prerogatives of the Archbishop of Thessalonica, and issued a decretal on disciplinary matters referred to him by the Bishop of Rouen. He defended the exiled John Chrysostom and consulted with the bishops of Africa concerning the Pelagian controversy, confirming the decisions of the African synods. The Catholic priest-scholar, Johann Peter Kirsch, described Innocent as a very energetic and highly gifted individual, "...who fulfilled admirably the duties of his office".

According to his biographer in the "Liber Pontificalis", Innocent was a native of Albano Laziale and the son of a man called Innocentius, but his contemporary Jerome referred to him as the son of the previous pope, Anastasius I, probably a unique case of a son succeeding his father in the papacy. According to Urbano Cerri, Pope Innocent was a native of Albania.

Innocent I lost no opportunity in maintaining and extending the authority of the Roman apostolic See, which was seen as the ultimate resort for the settlement of all ecclesiastical disputes. His communications with Victricius of Rouen, Exuperius of Toulouse, Alexander of Antioch and others, as well as his actions on the appeal made to him by John Chrysostom against Theophilus of Alexandria, show that opportunities of this kind were numerous and varied. He took a decided view on the Pelagian controversy, confirming the decisions of the synod of the province of proconsular Africa, held in Carthage in 416, confirming the condemnation which had been pronounced in 411 against Cælestius, who shared the views of Pelagius. He also wrote in the same year in a similar sense to the fathers of the Numidian synod of Mileve who had addressed him. Soon after this, five African bishops, among them St. Augustine, wrote a personal letter to Innocent regarding their own position in the matter of Pelagianism. In addition he acted as metropolitan over the bishops of Italia Suburbicaria.

The historian Zosimus in his "Historia Nova" suggests that during the sack of Rome in 410 by Alaric I, Innocent I was willing to permit private pagan practices as a temporary measure. However, Zosimus also suggests that this attempt by pagans to restore public worship failed due to lack of public interest, suggesting that Rome had been successfully Christianized in the last century.

Among Innocent I's letters is one to Jerome and another to John II, Bishop of Jerusalem, regarding annoyances to which the former had been subjected by the Pelagians at Bethlehem.

He died on 12 March 417. Accordingly, his feast day is now celebrated on 12 March, though from the thirteenth to the twentieth century he was commemorated on 28 July. His successor was Zosimus.

It is accepted that the canon of the Bible was closed c. 405 AD by Pope Innocent, when he sent a list of the sacred books to a Gallic bishop, Exsuperius of Toulouse, identical with that of Trent, except for some uncertainty in the manuscript tradition about whether the letters ascribed to Paul were 14 or only 13, in the latter case possibly implying omission of the Epistle to the Hebrews.

In 846, Pope Sergius II gave approval for the relics of St. Innocent to be moved by Duke Liudolf of Saxony, along with those of his father and predecessor Anastasius, to the crypt of the former collegiate church of Gandersheim, now Gandersheim Abbey, where they rest until this day.




</doc>
<doc id="24425" url="https://en.wikipedia.org/wiki?curid=24425" title="Philippi">
Philippi

Philippi (; , "Philippoi") was a city in eastern Macedonia, in the Edonis region. Its original name was Crenides (, "Krenides" "Fountains") after its establishment by Thasian colonists in 360/359 BC. The city was renamed by Philip II of Macedon in 356 BC and abandoned in the 14th century after the Ottoman conquest. The present municipality, Filippoi, is located near the ruins of the ancient city and is part of the region of East Macedonia and Thrace in Kavalla, Greece. It was classified as a UNESCO World Heritage Site in 2016.

Philippi was established by Thasian colonists in 360/359 BC with the name Crenides. In 356 BC Philip II of Macedon, conquered the city and renamed it to Philippi. It was sited near the head of the Aegean Sea and at the foot of Mt. Orbelos, now called Mt. Lekani, about north-west of Kavalla, on the northern border of the marsh that, in antiquity, covered the entire plain separating it from the Pangaion hills to the south of Greece.

The objective of conquering the town was to take control of the neighbouring gold mines and to establish a garrison at a strategic passage: the site controlled the route between Amphipolis and Neapolis, part of the great royal route which crosses Macedonia from the east to the west and which was reconstructed later by the Roman Empire as the "Via Egnatia". Philip II endowed the city with important fortifications, which partially blocked the passage between the swamp and Mt. Orbelos, and sent colonists to occupy it. Philip also had the marsh partially drained, as is attested by the writer Theophrastus. Philippi preserved its autonomy within the kingdom of Macedon and had its own political institutions (the "Assembly" of the "demos"). The discovery of new gold mines near the city, at Asyla, contributed to the wealth of the kingdom and Philip established a mint there. The city was fully integrated into the kingdom under Philip V.

The city contained 2,000 people.

When the Romans destroyed the Antigonid dynasty of Macedon in the Third Macedonian War (168 BC), they divided the kingdom into four separate states ("merides"). It was Amphipolis and not Philippi that became the capital of the eastern Macedonian state.

Almost nothing is known about the city in this period, aside from the walls, the Greek theatre, the foundations of a house under the Roman forum and a little temple dedicated to a hero cult. This monument covers the tomb of a certain Exekestos, is possibly situated on the agora and is dedicated to the κτίστης ("ktistēs"), the foundation hero of the city.

The city reappears in the sources during the Roman civil war that followed the assassination of Julius Caesar. His heirs Mark Antony and Octavian confronted the assassins of Caesar, Marcus Junius Brutus and Gaius Cassius Longinus, at the Battle of Philippi on the plain to the west of the city during October in 42 BC. Antony and Octavian were victorious in this final battle against the partisans of the Republic. They released some of their veteran soldiers, probably from Legion XXVIII and colonized them in the city, which was refounded as "Colonia Victrix Philippensium". In 30 BC, Octavian became Roman emperor, reorganized the colony, and established more settlers there, veterans possibly from the Praetorian Guard and other Italians. The city was renamed "Colonia Iulia Philippensis", and then "Colonia Augusta Iulia Philippensis" after January, 27 BC, when Octavian received the title Augustus from the Roman Senate.

Following this second renaming, and perhaps after the first, the territory of Philippi was centuriated (divided into squares of land) and distributed to the colonists. The city kept its Macedonian walls, and its general plan was modified only partially by the construction of a forum, a little to the east of the site of Greek agora. It was a "miniature Rome," under the municipal law of Rome and governed by two military officers, the "duumviri", who were appointed directly from Rome.

The colony recognized its dependence on the mines that brought it its privileged position on the "Via Egnatia". This wealth was shown by the many monuments that were particularly imposing considering the relatively small size of the urban area: the forum, laid out in two terraces on both sides of the main road, was constructed in several phases between the reigns of Claudius and Antoninus Pius, and the theatre was enlarged and expanded in order to hold Roman games. There is an abundance of Latin inscriptions testifying to the prosperity of the city.

The New Testament states that in AD 49 or 50, the city was visited by the apostle Paul (). From the Acts of the Apostles and the letter to the Philippians, early Christians concluded that Paul had founded their community. Accompanied by Silas, Timothy and possibly Luke, the author of the Acts of the Apostles, Paul is believed to have preached for the first time on European soil in Philippi. According to the New Testament, Paul visited the city on two other occasions, in 56 and 57. The Epistle to the Philippians dates from around 61-62 and is believed to show the immediate effects of Paul's instruction.

The development of Christianity in Philippi is indicated by a letter from Polycarp of Smyrna addressed to the community in Philippi around AD 160 and by funerary inscriptions.

The first church described in the city is a small building that was probably originally a small prayer house. This "Basilica of Paul", identified by a mosaic inscription on the pavement, is dated around 343 from a mention by the bishop Porphyrios, who was present at the Council of Serdica that year.

Despite being one of the oldest congregations in Europe a bishopric was not established until the 4th century.

The prosperity of the city in the 5th and 6th centuries was attributed to Paul and to his ministry. As in other cities, many new ecclesiastical buildings were constructed at this time. Seven different churches were constructed in Philippi between the mid-4th century and the end of the 6th, some of which competed in size and decoration with the most beautiful buildings in Thessalonica, or those of Constantinople. The relationship of the plan and of the architectural decoration of Basilica B with Hagia Sophia and Saint Irene in Constantinople accorded a privileged place to this church in the history of early Christian art. The complex cathedral which took the place of the Basilica of Paul at the end of the 5th century, constructed around an octagonal church, also rivaled the churches of Constantinople.

In the same age, the fortifications of the city were rebuilt in order to better defend against the growing instability in the Balkans. In 473, the city was besieged by the Ostrogoths, who were unable to take it but burned down the surrounding villages.

Already weakened by the Slavic invasions at the end of the 6th century, which ruined the agrarian economy of Macedonia and probably also by the Plague of Justinian in 547, the city was almost totally destroyed by an earthquake around 619, from which it never recovered. There was a small amount of activity there in the 7th century, but the city was now hardly more than a village.

The Byzantine Empire possibly maintained a garrison there, but in 838 the city was taken by the Bulgarians under "kavhan" Isbul, who celebrated their victory with a monumental inscription on the stylobate in Basilica B, now partially in ruins. The site of Philippi was so strategically sound that the Byzantines attempted very soon to recapture it around 850. Several seals of civil servants and other Byzantine officials, dated to the first half of the 9th century, prove the presence of Byzantine armies in the city.

Around 969, Emperor Nicephorus II Phocas rebuilt the fortifications on the acropolis and in part of the city. These gradually helped weaken Bulgar power and strengthen the Byzantine presence in the area. In 1077, Bishop Basil Kartzimopoulos rebuilt part of the defenses inside the city. The city began to prosper once more, as witnessed by the Arab geographer Al Idrisi, who mentions it as a centre of business and wine production around 1150.

After a brief occupation by the Franks after the Fourth Crusade and the capture of Constantinople in 1204, the city was captured by the Serbs. Still, it remained a notable fortification on the route of the ancient "Via Egnatia"; in 1354, the pretender to the Byzantine throne, Matthew Cantacuzenus, was captured there by the Serbs.

The city was abandoned at an unknown date, but when the French traveller Pierre Belon visited it in the 16th century, there were nothing but ruins, used by the Turks as a quarry. The name of the city was preserved at first by a Turkish village on the nearby plain, Philibedjik (Filibecik, "Little Filibe" in Turkish), which has since disappeared and then by a Greek village in the mountains.

Noted or briefly described by 16th century travellers, the first archaeological description of the city was made in 1856 by Perrot, then in 1861 by Léon Heuzey and Henri Daumet in their famous "Mission archéologique de Macédoine". The first excavations did not begin until the summer of 1914, and were soon interrupted by the First World War. The excavations, carried out by the École française d'Athènes, were renewed in 1920 and continued until 1937. During this time the Greek theatre, the forum, Basilicas A and B, the baths and the walls were excavated. After the Second World War, Greek archaeologists returned to the site. From 1958 to 1978, the Société Archéologique, then the Service archéologique and the University of Thessalonica uncovered the bishop's quarter and the octagonal church, large private residences, a new basilica near the Museum and two others in the necropolis to the east of the city.


Translated from the , retrieved February 11, 2005. That article, in turn, gives the following references:



</doc>
<doc id="24427" url="https://en.wikipedia.org/wiki?curid=24427" title="Victoria, Crown Princess of Sweden">
Victoria, Crown Princess of Sweden

Victoria, Crown Princess of Sweden, Duchess of Västergötland (Victoria Ingrid Alice Désirée; born 14 July 1977) is the heir apparent to the Swedish throne, as the eldest child of King Carl XVI Gustaf. If she ascends to the throne as expected, she will be Sweden's fourth queen regnant (after Margaret, Christina and Ulrika Eleonora) and the first since 1720.

Victoria was born on 14 July 1977 at 21:45 CET at the Karolinska University Hospital in Solna, Stockholm County, Sweden, and is the oldest child of King Carl XVI Gustaf and Queen Silvia. She is a member of the Royal House of Bernadotte. Born as a Princess of Sweden, she was designated Crown Princess in 1979 (SFS 1979:932) ahead of her younger brother. Her place as first in the line of succession formally went into effect on 1 January 1980 with the parliamentary change to the Act of Succession that introduced absolute primogeniture.

Her given names honour various relatives. Her first name comes primarily from her great-great-grandmother, Victoria of Baden, the queen-consort of Sweden as wife of King Gustaf V, and her great-great-great-grandmother Victoria, queen of the United Kingdom (the Queen's granddaughter, Margaret of Connaught, Crown Princess of Sweden, was Victoria's great-grandmother). Her other names honour her great-aunt Ingrid of Denmark; her maternal grandmother, the Brazilian Alice Sommerlath (born Alice Soares de Toledo); and her ancestor Désirée Clary, the queen-consort of Charles XIV John and a former fiancée of Napoleon I of France as well as her paternal aunt and godmother, Princess Désirée.

She was baptised at The Royal Palace Church on 27 September 1977. Her godparents were Crown Prince Harald of Norway (later king of Norway), her maternal uncle, Ralf Sommerlath, Princess Beatrix of the Netherlands (later queen of the Netherlands, 1980–2013), and her aunt Princess Désirée, Baroness Silfverschiöld. The Crown Princess was confirmed in the summer of 1992 at Räpplinge church on the island of Öland.

Victoria studied for a year (1996/97) at the Université Catholique de l'Ouest at Angers in France, and in the fall term of 1997 participated in a special program following the work of the "Riksdag". From 1998 to 2000, Victoria resided in the United States, where she studied various subjects at Yale University, New Haven, Connecticut.

In May 1999, she was an intern at the Swedish Embassy in Washington, D.C. Victoria completed a study program at the Government Offices in 2001. In 2003, Victoria's education continued with visits to Swedish businesses, a study and intern program in agriculture and forestry, as well as completion of the basic soldier training at SWEDINT (the Swedish Armed Forces International Centre).

In 2006, Victoria enrolled in the Ministry for Foreign Affairs' Diplomat Program, running from September 2006 to June 2007. The program is a training program for young future diplomats and gives an insight to the ministry's work, Swedish foreign and security policies and Sweden's relations with the rest of the world. In June 2009, she graduated with a Bachelor of Arts degree from Uppsala University.

She speaks Swedish, English, French and German.

She was made Crown Princess and heir apparent on 1 January 1980 by the 1979 change to the Act of Succession of 1810 ("Successionsordningen"). This constitutional reform meant that the throne would be inherited by the monarch's eldest child without regard to gender. King Carl XVI Gustaf objected to the reform after it occurred—not because he objected to women entering the line of succession, but because he was upset about his son being stripped of the Crown Princely status he had held since birth.

When she became heir, she also was made titular Duchess of Västergötland, one of the historical provinces of Sweden. Prior to this constitutional change, the heir apparent to the throne was her younger brother, the then-Crown Prince Carl Philip, Duke of Värmland. He is now fourth in line to the throne, behind the Crown Princess's daughter and son.

She is one of only three female heirs apparent in the world, the other two being her goddaughter Catharina-Amalia, Princess of Orange, and Princess Elisabeth, Duchess of Brabant. 

Victoria's declaration of majority took place in the Hall of State at the Royal Palace of Stockholm on 14 July 1995. As of the day she turned 18, she became eligible to act as Head of State when the King is not in country. Victoria made her first public speech on this occasion. Located on the dais in the background was the same silver throne on which her father was seated at his enthronement, in actual use from 1650 and up until this ceremony.

As heir apparent to the throne, Victoria is a working member of the Swedish Royal Family with her own agenda of official engagements. Victoria attends the regular Advisory Council on Foreign Affairs and the information councils with Government ministers headed by the King, and steps in as a temporary regent (Riksföreståndare) when needed.

Victoria has made many official trips abroad as a representative of Sweden. Her first major official visit on her own was to Japan in 2001, where she promoted Swedish tourism, design, music, gastronomy and environmental sustainability during the "Swedish Style" event. That same year, Victoria also travelled to the West Coast of the United States, where she participated in the celebrations of the Nobel centenary.

In 2002, she paid official visits to United States, Spain, Uganda, Ethiopia, and Kosovo where she visited Camp Victoria. In 2003, she made official visits to Egypt and the United States. In early 2004, she paid an official visit to Saudi Arabia, as a part of a large official business delegation from Sweden, and in October 2004, she travelled to Hungary.

Crown Princess Victoria was given her own household in October 2004. It is headed by the Marshal of the Court, and serves to coordinate the official engagements of The Crown Princess.

In January 2005, Victoria made a long official visit to Australia, promoting Swedish style and businesses, and in April she visited Bangladesh and Sri Lanka to follow aid work and become informed about the work in the aftermath of the tsunami. In April 2005, Victoria made an official visit to Japan where she visited the Expo 2005 in Aichi, laid the foundation for a new IKEA store in Yokohama together with Princess Takamado and met with Emperor Akihito, Empress Michiko, Crown Prince Naruhito and Sayako Kuroda. In June 2005, Victoria travelled to Turkey on an official visit where she participated in the Swedish Business Seminar and Sweden Day celebrations in Ankara during a historic visit, which was organised by the Swedish Embassy in Ankara and Swedish Trade Council in Istanbul. Victoria also visited the historic sights such as the Blue Mosque, Topkapı Palace and Hagia Sophia. This was the first official Royal visit from Sweden to Turkey since 1934. In September 2005, she made an official visit to China.

In March 2006, Victoria made an official visit to Brazil where she followed the Volvo Ocean Race and visited projects supported by the World Childhood Foundation, such as the Abrigo Rainha Sílvia. In December, she paid a four-day official visit to Paris where she attended a French-Swedish soirée arranged by the Swedish Chamber of Commerce, the Swedish Trade Council and the Swedish Embassy, during which she also awarded the Prix d’Excellence 2006. The visit to Paris also included events with the Swedish Club in Paris, attendance at a church service in the Sofia Church (the Swedish church in Paris), a study visit to the OECD headquarters and meetings with the Secretary-General José Ángel Gurría, the Swedish Ambassador to the OECD, Gun-Britt Andersson, and other senior officials. She also attended a gala dinner hosted by La Fondation Pour L’Enfance at Versailles.

She is a member of the Honorary Board of the International Paralympic Committee.

In 2011, it was announced that Victoria would continue working throughout her pregnancy. In 2012, she took her maternity leave one day prior to the birth of her daughter Estelle and her husband Daniel revealed that he would take his paternity leave and switch parental roles with Victoria when Estelle began preschool.

The Crown Princess Victoria's Fund was set up in 1997 and is run as a part of Radiohjälpen, the fundraising branch of Sveriges Television and Sveriges Radio. The fund's aim is to provide support for leisure and recreational activities for children and young people with functional disabilities or chronic illnesses.

The Crown Princess Victoria Fund's means mainly derive from donations by the public, but large companies such as Arla Foods, Swedbank and AB Svenska Returpack are constant sponsor partners. Additional support comes from The Association of Swedish Bakers & Confectioners who every year arrange a national “princess cake week” during which the participating cafés and bakeries give 2,50 SEK per sold princess pastry and 10 SEK per sold princess cake to the fund. The result of this fund-raising drive is usually presented to Victoria herself on her name day on 12 March every year; in 2007, the total amount was 200,000 SEK. Congratulatory and memorial cards are also issued by Radiohjälpen benefitting the fund, a simple way to pay respects and do a good deed in one act. In 2006, The Crown Princess Victoria Fund raised a total of 5,5 million SEK.

Every year Victoria visits one or several clubs or projects that have been granted money. These visits are not announced via the official royal diary but kept private, instead Sveriges Television often accompanies her and airs short programs from these visits at some time during the year.

Victoria’s first boyfriend was Daniel Collert. They socialised in the same circles, went to the same school and were already friends when their romance developed in the mid-1990s. When Victoria moved to the United States in 1998 to study and recover from her eating disorders, Collert moved with her across the Atlantic and settled in New York. In September 2000, Victoria's relationship with Collert was confirmed in an interview with her at Expo 2000. They broke up in 2001.

In May 2002, Swedish newspaper "Expressen" reported that Victoria had a new boyfriend, her personal trainer at Master Training, Daniel Westling. When the news broke and the media turned its attention on him, it was obvious that he did not like being in the public eye. Once Westling was photographed crossing a street against a red light in order to avoid a camera. In July 2002, Victoria and Daniel Westling were pictured kissing for the first time at a birthday party for Caroline Kreuger, a close friend of Victoria's.

In a popular personal report called "Tre dagar med Victoria", which profiled her work during a three-day period that aired on TV4 in December 2004, Victoria commented on criticism directed at Westling, “Many unfair things are written. I understand that there is speculation, but some day justice will be done there, too.” Victoria also gave her opinion that happiness is important, and that these days it is not so much about background and pedigree but about two people who have to live with each other. She said that if they are not happy and comfortable with each other, it is impossible to do a good job.

Swedish media often speculated about upcoming engagements and marriages for Victoria. On 24 February 2009, rumours that wedding plans were imminent became particularly intense preceding an information council between the King and Prime Minister Fredrik Reinfeldt. Under the terms of the Swedish Act of Succession, the Government, upon the request of the King, gives the final consent for a dynastic marriage of a Prince or Princess of Sweden. The prince or princess otherwise loses their right to the throne. Later that day, it was confirmed that permission had been granted and that Victoria would marry Daniel Westling in the summer of 2010. The wedding date was set in Stockholm Cathedral for 19 June 2010, the 34th anniversary of her parents' marriage. Her engagement ring features a solitaire round brilliant-cut diamond mounted on white gold.

The wedding took place on 19 June 2010. Guests including royalty and ambassadors from various countries were invited to the wedding ceremony which took place at Stockholm Cathedral. After the wedding the newlyweds were driven through Stockholm in a coach and then rowed in the antique royal barge "Vasaorden" to the royal palace where the wedding banquet was held. On the evening before the wedding, there was a gala concert dedicated to the couple in the Stockholm Concert Hall.

On 17 August 2011, the Swedish royal court announced that Crown Princess Victoria was pregnant and expecting the couple's first child in March 2012. On 23 February 2012 at 04:26 CET, Victoria gave birth to Princess Estelle, Duchess of Östergötland, in the Karolinska University Hospital. Princess Estelle is second-in-line to the Swedish throne.

On 4 September 2015, the royal court announced that Crown Princess Victoria was expecting her second child in March 2016. On 2 March 2016, she gave birth to a son, Prince Oscar, Duke of Skåne.

In 1996, it was established that Victoria suffered from anorexia; this was not confirmed until the next year. Already at that time she was getting professional help, but given her public position in Sweden it was getting increasingly difficult to handle the situation. Victoria had planned to study at Uppsala University, but after intense media speculation and public discussion when pictures of an evidently emaciated Victoria in sleeveless dresses at the Order of the Innocence’s ball and the gala dinner for the incoming state visit from Austria surfaced in April 1997, the Royal Court decided to confirm what was feared.

After a press release from the Royal Court in November 1997 announced that Victoria had eating disorders, plans changed for her and she moved to the United States where she received professional help and studied at Yale University. By making this drastic decision, Victoria lived an anonymous life while getting professional help and recovering without having to worry about media speculations or if people were recognizing her on the streets.

In June 1999, Victoria said, "It was a really hard time. This kind of illness is hard, not only for the individual but also for the people close to him or her. Today I'm fine."

In November 2002, the book "Victoria, Victoria!" came out, speaking further about her eating disorder. Victoria said: "I felt like an accelerating train, going right down... during the whole period. I had eating disorders and was aware of it, my anguish was enormous. I really hated how I looked like, how I was... I, Victoria, didn’t exist. It felt like everything in my life and around me was controlled by others. The one thing I could control was the food I put in me". She further said that "What happened cost and I was the one who stood for the payments. Now I’m feeling well and with the insights I’ve acquired through this I can hopefully help someone else".

In 2008, she also spoke about her face blindness.




On her father's side, Victoria is descended from the House of Bernadotte (Swedish royal family); the House of Windsor (British royal family), including her namesake Queen Victoria; and the House of Schleswig-Holstein-Sonderburg-Glücksburg (Danish royal family). Her mother,
Queen Silvia, is descended from German Brazilians.




</doc>
<doc id="24428" url="https://en.wikipedia.org/wiki?curid=24428" title="Pope Innocent II">
Pope Innocent II

Pope Innocent II (; died 23 September 1143), born Gregorio Papareschi, was Pope from 14 February 1130 to his death in 1143. His election was controversial and the first eight years of his reign were marked by a struggle for recognition against the supporters of Antipope Anacletus II. He reached an understanding with Lothair II, Holy Roman Emperor who supported him against Anacletus and whom he crowned King of the Romans. Innocent went on to preside over the Second Lateran council.

Papareschi came from a Roman family, probably of the "rione" Trastevere. He was probably one of the clergy in personal attendance on the Antipope Clement III (Guibert of Ravenna).

Pope Paschal II made him a cardinal deacon. In this capacity, he accompanied Pope Gelasius II when he was driven into France. He was selected by Pope Callixtus II for various important and difficult missions, such as the one to Worms for the conclusion of the Concordat of Worms, the peace accord made with Holy Roman Emperor Henry V in 1122, and also the one to France in 1123 that made peace with King Louis VI.

In 1130, as Pope Honorius II lay dying, the cardinals decided to entrust the election to a commission of eight men led by papal chancellor Haimeric, who had his candidate Cardinal Gregory Papareschi hastily elected as Pope Innocent II. He was consecrated on 14 February, the day after Honorius' death. The other cardinals announced that Innocent had not been canonically elected and chose Cardinal Pietro Pierleoni, a Roman whose family were the enemy of Haimeric's supporters, the Frangipani; Pierleoni took the name Pope Anacletus II. Anacletus' mixed group of supporters were powerful enough to take control of Rome while Innocent was forced to flee north. Based on a simple majority of the entire college of cardinals, Anacletus was the canonically elected pope, and Innocent was the anti-Pope. However, the legislation of Pope Nicholas II pre-empted the choice of the majority of the cardinal priests and cardinal deacons. This rule was changed by the Second Lateran council of 1139.

Anacletus had control of Rome, so Innocent II took ship for Pisa, and thence sailed by way of Genoa to France, where the influence of Bernard of Clairvaux readily secured his cordial recognition by the clergy and the court. In October of the same year he was duly acknowledged by Holy Roman Emperor Lothair III and his bishops at the synod of Würzburg. In January 1131, he had also a favourable interview with Henry I of England, and in August 1132 Lothar III undertook an expedition to Italy for the double purpose of setting aside Anacletus as antipope and of being crowned by Innocent. Anacletus and his supporters being in secure control of St. Peter's Basilica, the coronation ultimately took place in the Lateran Church (4 June 1133), but otherwise the expedition proved abortive. At the investiture of Lothair as Emperor he gained the territories belonging to Matilda of Tuscany in return for an annuity to be paid to the pope, in consequence of which the curial party based the contention that the Emperor was a vassal of the Papal see.

A second expedition by Lothar III in 1136 was not more decisive in its results, and the protracted struggle between the rival pontiffs was terminated only by the death of Anacletus II on 25 January 1138.

Innocent took as cardinal-nephew first his nephew, Gregorio Papareschi, whom he elevated to cardinal in 1134, and then his brother Pietro Papareschi, whom he elevated to cardinal in 1142. Another nephew, Cinzio Papareschi (died 1182), was also a cardinal, raised to the cardinalate in 1158, after Innocent's death.

By the Second Lateran council of 1139, at which King Roger II of Sicily, Innocent II's most uncompromising foe, was excommunicated, peace was at last restored to the Church. Aside from the complete rebuilding of the ancient church of Santa Maria in Trastevere, which boldly features Ionic capitals from former colonnades in the Baths of Caracalla and other richly detailed "spolia" from Roman monuments, the remaining years of this Pope's life were almost as barren of permanent political results as the first had been. His efforts to undo the mischief wrought in Rome by the long schism were almost entirely neutralized by a quarrel with his erstwhile supporter, Louis VII of France over the candidate for archbishop of Bourges, in the course of which that kingdom was laid under an interdict to press for the papal candidate, and by a struggle with the town of Tivoli in which he became involved. As a result, Roman factions that wished Tivoli annihilated took up arms against Innocent.

It was also in 1139 that, in the "Omne Datum Optimum", Innocent II declared that the Knights Templar—a religious and military organization then twenty-one years old—should in the future be answerable only to the papacy. This was a keystone in the Templars' ever increasing power and wealth, and ironically helped to bring about their violent suppression in October 1307.

Can. 29 of the Second Lateran Council under Pope Innocent II in 1139 banned the use of crossbows, as well as slings and bows, against Christians.
On 22 July 1139, at Galluccio, Roger II's son Roger III, Duke of Apulia, ambushed the papal troops with a thousand knights and captured Innocent. On 25 July 1139, Innocent was forced to acknowledge the kingship and possessions of Roger with the Treaty of Mignano. In 1143, Innocent refused to recognise the Treaty of Mignano with Roger of Sicily, who sent Robert of Selby to march on papal Benevento. The terms agreed upon at Mignano were then recognised. Innocent II died on 24 September 1143 and was succeeded by Pope Celestine II.

The doctrinal questions which he was called on to decide were those that condemned the opinions of Pierre Abélard and of Arnold of Brescia.

In 1143, as the Pope lay dying, the Commune of Rome, to resist papal power, began deliberations that officially reinstated the Roman Senate the following year. The Pope was interred in a porphyry sarcophagus that contemporary tradition asserted had been the Emperor Hadrian's.



</doc>
<doc id="24429" url="https://en.wikipedia.org/wiki?curid=24429" title="Pope Zosimus">
Pope Zosimus

Pope Zosimus (died 26 December 418) reigned from 18 March 417 to his death in 418. He was born in Mesoraca, Calabria.

He succeeded Innocent I and was followed by Boniface I. Zosimus took a decided part in the protracted dispute in Gaul as to the jurisdiction of the See of Arles over that of Vienne, giving energetic decisions in favour of the former, but without settling the controversy. His fractious temper coloured all the controversies in which he took part, in Gaul, Africa and Italy, including Rome, where at his death the clergy were very much divided.

According to the "Liber Pontificalis", Zosimus was a Greek and his father's name was Abramius. Historian Adolf von Harnack deduced from this that the family was of Jewish origin, but this has been rejected by Louis Duchesne.

Nothing is known of the life of Zosimus before his elevation to the Papal See. His consecration as Bishop of Rome took place on 18 March 417. The festival was attended by Patroclus, Bishop of Arles, who had been raised to that See in place of Bishop Heros of Arles, who had been deposed by Constantius III. Patroclus gained the confidence of the new pope at once; as early as 22 March he received a papal letter which conferred upon him the rights of a metropolitan over all the bishops of the Gallic provinces of Viennensis and Narbonensis I and II. In addition, he was made a kind of papal vicar for the whole of Gaul, with no Gallic ecclesiastic being permitted to journey to Rome without bringing with him a certificate of identity from Patroclus.

In the year 400, Arles had been substituted for Trier as the residence of the chief government official of the civil Diocese of Gaul, the "Prefectus Praetorio Galliarum". Patroclus, who enjoyed the support of the commander Constantine, used this opportunity to procure for himself the position of supremacy above mentioned, by winning over Zosimus to his ideas. The bishops of Vienne, Narbonne, and Marseille regarded this elevation of the See of Arles as an infringement of their rights, and raised objections which occasioned several letters from Zosimus. The dispute, however, was not settled until the pontificate of Pope Leo I.

Not long after the election of Zosimus, Caelestius, a proponent of Pelagianism who had been condemned by the preceding pope Innocent I, came to Rome to appeal to the new pope, having been expelled from Constantinople. In the summer of 417, Zosimus held a meeting of the Roman clergy in the Basilica of St. Clement before which Caelestius appeared. The propositions drawn up by the deacon Paulinus of Milan, on account of which Caelestius had been condemned at Carthage in 411, were laid before him. Caelestius refused to condemn these propositions, at the same time declaring in general that he accepted the doctrine expounded in the letters of Pope Innocent and making a confession of faith which was approved. The pope was won over by the conduct of Caelestius, and said that it was not certain whether he had really maintained the false doctrine rejected by Innocent, and therefore Zosimus considered the action of the African bishops against Caelestius too hasty. He wrote at once in this sense to the bishops of the African province, and called upon those who had anything to bring against Caelestius to appear at Rome within two months.

Soon after this, Zosimus received from Pelagius a confession of faith, together with a new treatise on free will. The pope held a new synod of the Roman clergy, before which both these writings were read; the assembly held the statements to be orthodox, and Zosimus again wrote to the African bishops defending Pelagius and reproving his accusers, among whom were the Gallic bishops Hero and Lazarus. Archbishop Aurelius of Carthage quickly called a synod, which sent a reply to Zosimus in which it was argued that the pope had been deceived by heretics. In his answer Zosimus declared that he had settled nothing definitely, and wished to settle nothing without consulting the African bishops. After the new synodal letter of the African council of 1 May 418 to the pope, and after the steps taken by the emperor Honorius against the Pelagians, Zosimus issued his "Tractoria", in which Pelagianism and its authors were finally condemned.

Shortly after this, Zosimus became involved in a dispute with the African bishops in regard to the right of clerics who had been condemned by their bishops to appeal to the Roman See. When the priest Apiarius of Sicca had been excommunicated by his bishop on account of his crimes, he appealed directly to the pope, without regard to the regular course of appeal in Africa, which was exactly prescribed. The pope at once accepted the appeal, and sent legates with credentials to Africa to investigate the matter. Another, potentially wiser, course would have been to have first referred the case of Apiarius to the ordinary course of appeal in Africa itself. Zosimus next made the further mistake of basing his action on a reputed canon of the First Council of Nicaea, which was in reality a canon of the Council of Sardica. In the Roman manuscripts the canons of Sardica followed those of Nicaea immediately, without an independent title, while the African manuscripts contained only the genuine canons of Nicaea, so that the canon appealed to by Zosimus was not contained in the African copies of the Nicene canons. This mistake ignited a serious disagreement over the appeal, which continued after the death of Zosimus.

Besides the writings of the pope already mentioned, there are extant other letters to the bishops of the Byzantine province in Africa, in regard to a deposed bishop, and to the bishops of Gaul and Spain in respect to Priscillianism and ordination to the different grades of the clergy. The "Liber Pontificalis" attributes to Zosimus a decree on the wearing of the maniple by deacons, and on the dedication of Easter candles in the country parishes; also a decree forbidding clerics to visit taverns. Zosimus was buried in the sepulchral Basilica of Saint Lawrence outside the Walls.





</doc>
<doc id="24430" url="https://en.wikipedia.org/wiki?curid=24430" title="Pope Innocent IV">
Pope Innocent IV

Pope Innocent IV (; c. 1195 – 7 December 1254), born Sinibaldo Fieschi, was Pope of the Catholic Church from 25 June 1243 to his death in 1254.

Born in Genoa (although some sources say Manarola) in an unknown year, Sinibaldo was the son of Beatrice Grillo and Ugo Fieschi, Count of Lavagna. The Fieschi were a noble merchant family of Liguria. Sinibaldo received his education at the universities of Parma and Bologna and, for a time, taught canon law at Bologna. It is pointed out by Agostino Paravicini-Bagliani, however, that there is no "documentary" evidence of such a professorship. From 1216-1227 he was Canon of the Cathedral of Parma. He was considered one of the best canonists of his time, and was called to serve Pope Honorius III in the Roman Curia as "Auditor causarum", from 11 November 1226 to 30 May 1227. He was then promoted to the office of Vice-Chancellor of the Holy Roman Church (from 31 May to 23 September 1227), though he retained the office and the title for a time after he was named Cardinal.

Vice-Chancellor Sinibaldo Fieschi was created Cardinal Priest of San Lorenzo in Lucina on 18 September 1227 by Pope Gregory IX (1227-1241). He later served as papal governor of the March of Ancona, from 17 October 1235 until 1240.

It is widely repeated, from the 17th century on, that he became bishop of Albenga in 1235, but there is no foundation to this claim.

Innocent's immediate predecessor was Pope Celestine IV, elected 25 October 1241, whose reign lasted a mere fifteen days. The events of Innocent IV's pontificate are therefore inextricably linked to the policies dominating the reigns of popes Innocent III, Honorius III and Gregory IX.

Gregory had been demanding the return of portions of the Papal States taken over by Holy Roman Emperor Frederick II when he died. The Pope had called a general council so he could depose the emperor with the support of Europe's spiritual leaders, but Frederick had seized two cardinals traveling to the council in hopes of intimidating the curia. The two prelates remained incarcerated and missed the conclave that immediately elected Celestine. The conclave that reconvened after his death fell into camps supporting contradictory policies about how to treat with the emperor.

After a year and a half of contentious debate and coercion, a papal election finally reached a unanimous decision. Cardinal de' Fieschi very reluctantly accepted election as Pope 25 June 1243, taking the name Innocent IV. As Cardinal de' Fieschi, Sinibaldo had been on friendly terms with Frederick, even after his excommunication. The Emperor also greatly admired the cardinal's wisdom, having enjoyed discussions with him from time to time.

Following the election the witty Frederick remarked that he had lost the friendship of a cardinal but made up for it by gaining the enmity of a pope.

His jest notwithstanding, Frederick's letter to the new pontiff was couched in respectful terms, offering Innocent congratulations and success, also expressing hope for an amicable settlement of the differences between the empire and the papacy. Negotiations leading to this objective began shortly afterwards, but proved abortive. Innocent refused to back down from his demands, Frederick II refused to acquiesce, and the dispute continued, its major point of contention being the reinstatement of Lombardy to the Patrimony of St Peter.

The Emperor's machinations caused a good deal of anti-papal feeling to rise in Italy, particularly in the Papal States, and imperial agents encouraged plots against papal rule. Realizing how untenable his position in Rome was growing, Innocent IV secretly and hurriedly withdrew, fleeing Rome on 7 June 1244. Traveling in disguise, Innocent made his way to Sutri and Civitavecchia, to Genoa, his birthplace, where he arrived on 7 July. From there, on 5 October, he fled to France, where he was joyously welcomed. Making his way to Lyon, where he arrived on November 29, 1244, Innocent was happily greeted by the magistrates of the city.

Finding himself now in secure surroundings and out of the reach of Frederic II, Innocent summoned, in a sermon preached on December 27, 1244, as many bishops as could get to Lyon (140 bishops were present), to attend what became the 13th General (Ecumenical) Council of the Church, the first to be held in Lyon. The bishops met for three public sessions: 28 June, 5 July, and 17 July 1245. Their principal business was to subjugate the Emperor Frederick II.

An earlier pope, Gregory IX (1227-1241), had issued letters on 9 June 1239, ordering all the bishops of France to confiscate all Talmuds in the possession of the Jews. Agents were to raid each synagogue on the first Saturday of Lent of 1240, and seize the books, placing them in the custody of the Dominicans or the Franciscans. The Bishop of Paris was ordered to see to it that copies of the Pope's mandate reached all the bishops of France, England, Aragon, Navarre, Castile and León, and Portugal. On 20 June 1239, there was another letter, addressed to the Bishop of Paris, the Prior of the Dominicans and the Minister of the Franciscans, calling for the burning of all copies of the Talmud, and any obstructionists to be visited with ecclesiastical censures. On the same day he wrote to the King of Portugal ordering him to see to it that all copies of the Talmud be seized and turned over to the Dominicans or Franciscans. Louis IX, King of France, on account of these letters held a trial in Paris in 1240, which ultimately found the Talmud guilty of 35 alleged charges. Twenty-four cartloads of the Talmud were burned.

Initially, Innocent IV continued Gregory IX's policy. In a letter of 9 May 1244, he wrote to King Louis IX, ordering the Talmud and any books with Talmudic glosses to be examined by the Regent Doctors of the University of Paris, and if condemned by them, to be burned. However, an argument was presented that this policy was a negation of the Church’s traditional stance of tolerance toward Judaism. On 5 July 1247, Pope Innocent wrote to the bishops of Germany and the Bishops of Gallia (France) that, because both ecclesiastical and lay persons were lawlessly plundering the property of the Jews, and falsely stating that at Eastertime they sacrificed and ate the heart of a little child, the bishops should see to it that the Jews not be attacked or molested because of these or other reasons. In the year 1247, in a letter of 2 August to King Louis of France, he reversed his stance on the Talmud, and wrote letters to the effect that the Talmud should be censored rather than burned. Innocent IV’s words were met with the disapproval of Odo of Châteauroux, Cardinal Bishop of Tusculum and former Chancellor of the University of Paris. Nonetheless, Pope Innocent IV’s policy was continued by subsequent popes.

The First Council of Lyon of 1245 had the fewest participants of any General Council before it. However three patriarchs and the Latin emperor of Constantinople attended, along with about 150 bishops, most of them prelates from France and Spain. They were able to come quickly, and Innocent could rely on their help. Bishops from the rest of Europe outside Spain and France feared retribution from Frederick, while many other bishops were prevented from attending either by the invasions of the Mongols (Tartars) in the Far East or Muslim incursions in the Middle East.

In session, Frederick II's position was defended by Taddeo of Suessa, who renewed in his master's name all the promises made before, but refused to give the guarantees the pope demanded. Unable to end the impasse Taddeo was horrified to hear the fathers of the Council solemnly depose and excommunicate the Emperor on 17 July, while absolving all his subjects from allegiance.

The political agitation over these acts convulsed Europe. The turmoil relaxed only with Frederick's death in December 1250, which removed the proximate threat to Innocent's life and permitted his return to Italy. He departed Lyon on 19 April 1251, and arrived in Genoa on 18 May. On 1 July, he was at Milan, accompanied by only three cardinals and the Latin Patriarch of Constantinople. He stayed there until mid-September, when he began an inspection tour of Lombardy, heading for Bologna. On 5, November, he reached Perugia. From 1251–53 the Pope stayed at Perugia until it was safe for him to bring the papal court back to Rome. He finally saw Rome again in the first week of October, 1253. He left Rome on 27 April 1254, for Assisi and then Anagni. He immediately threw himself into the problems surrounding the succession to the possessions of Frederick II, both as German Emperor and as King of Sicily. In both cases, Innocent continued Pope Gregory IX's policy of opposition to the Hohenstaufen, supporting whatever opposition there could be found to that House. This papal stance embroiled Italy in one conflict after another for the next three decades. Innocent IV himself, following after the papal army which was seeking to destroy Frederick's son Manfred, died in Naples on 7 December 1254.

While in Perugia, on 15 May 1252, Innocent IV issued the papal bull "Ad extirpanda", composed of thirty-eight 'laws', and (in Lex 25) permitting the torture of heretics.

As Innocent III had before him, Innocent IV saw himself as the Vicar of Christ, whose power was above earthly kings. Innocent, therefore, had no objection to intervening in purely secular matters. He appointed Afonso III administrator of Portugal, and lent his protection to Ottokar, the son of the King of Bohemia. The Pope even sided with King Henry III against both nobles and bishops of England, despite the king's harassment of Edmund Rich, the Archbishop of Canterbury and Primate of All England, and the royal policy of having the income of a vacant bishopric or benefice delivered to the royal coffers, rather than handed over to a papal Administrator (usually a member of the Curia) or a Papal collector of revenue, or delivered directly to the Pope.

The warlike tendencies of the Mongols also concerned the Pope, and he sent a papal nuncio to the Mongol Empire in an attempt to strike an agreement. Innocent decreed that he, as Vicar of Christ, could make non-Christians accept his dominion and even exact punishment should they violate the non-God centered commands of the Ten Commandments. This policy was held more in theory than in practice and was eventually repudiated centuries later.

The papal preoccupation with imperial matters and secular princes caused the spirituality of the Church to suffer. Taxation increased in the Papal States and the complaints of the inhabitants grew considerably.

In August 1253, after much worry about the order's insistence on absolute poverty, Innocent finally approved the rule of the 2nd Order of the Franciscans, the Poor Clares, founded by St. Clare of Assisi, the great friend of St Francis.

In 1246 Edmund Rich, former Archbishop of Canterbury (died 1240), was named a saint. In 1250 Innocent proclaimed the pious Queen Margaret (died 1093), wife of King Malcolm III of Scotland, a saint. The Dominican priest Peter of Verona, martyred by Albigensian heretics in 1252, was canonized, as was Stanislaus of Szczepanów, the Polish Archbishop of Cracow, both in 1253.

Innocent IV is often credited as helping to create the idea of legal personality, "persona ficta" as it was originally written, which has led to the idea of corporate personhood. This allowed monasteries and universities the ability to act as a single legal entity, allowing for their existence to be more continuous and for monks pledged to poverty to nonetheless be part of an organization that could own infrastructure, but as "fictional people" they could not be excommunicated or considered guilty of delict, that is, negligence to action that is not contractually required. This meant that punishment of individuals within an organization would reflect less on the organization itself than it would if the person running such an organization was said to own it rather than be a constituent of it, and was meant to provide stability.

Innocent IV was responsible for the eventual deposition of King Sancho II of Portugal at the request of his brother Afonso (later King Afonso III of Portugal). One of the arguments he used against Sancho II in his "Grandi non immerito" text was his status as a minor upon inheriting the throne from his father Afonso II.

In 1245, Innocent IV issued bulls and sent an envoy in the person of Giovanni da Pian del Carpine (accompanied by Benedict the Pole) to the "Emperor of the Tartars". The message asked the Mongol ruler to become a Christian and stop his aggression against Europe. The Khan Güyük replied in 1246 in a letter written in Persian that is still preserved in the Vatican Library, demanding the submission of the Pope and the other rulers of Europe.
In 1245 Innocent had sent another mission, through another route, led by Ascelin of Lombardia, also bearing letters. The mission met with the Mongol ruler Baichu near the Caspian Sea in 1247. The reply of Baichu was in accordance with that of Güyük, but it was accompanied by two Mongolian envoys to the Papal seat in Lyon, Aïbeg and Serkis. In the letter Guyuk demanded that the Pope appear in person at the Mongol imperial headquarters, Karakorum in order that “we might cause him to hear every command that there is of the jasaq”. The envoys met with Innocent IV in 1248, who again appealed to the Mongols to stop their killing of Christians.

Innocent IV would also send other missions to the Mongols in 1245: the mission of André de Longjumeau and the possibly aborted mission of Laurent de Portugal.

The remainder of Innocent's life was largely directed to schemes for compassing the overthrow of Manfred of Sicily, the natural son of Frederick II, whom the towns and the nobility had for the most part received as his father's successor. Innocent aimed to incorporate the whole Kingdom of Sicily into the Papal States, but he lacked the necessary economic and political power. Therefore, after a failed agreement with Charles of Anjou, he invested Edmund, the nine-year-old son of King Henry III of England, with that kingdom on 14 May 1254.

In the same year, Innocent excommunicated Frederick II's other son, Conrad IV, King of Germany, but the latter died a few days after the investiture of Edmund. At the beginning of June, 1254, Innocent moved to Anagni, where he awaited Manfred's reaction to the event, especially considering that Conrad's heir, Conradin, had been entrusted to Papal tutelage by King Conrad's testament. Manfred submitted, although probably only to gain time and counter the menace from Edmund, and accepted the title of Papal vicar for southern Italy. Innocent could therefore enjoy a moment in which he was the acknowledged sovereign, in theory at least, of most of the peninsula. Innocent overplayed his hand, however, by accepting the fealty of Amalfi directly to the Papacy instead of to the Kingdom of Sicily on 23 October. Manfred immediately, on October 26, fled from Teano, where he had established his headquarters, and headed to Lucera to his Saracen troops.
Manfred had not lost his nerve, and organized resistance to papal aggression. Supported by his faithful Saracen troops, he began using military force to make rebellious barons and towns submit to his authority as Regent for his nephew. Realizing that Manfred had no intention of submitting to the Papacy or to anyone else, Innocent and his papal army headed south from his summer residence at Anagni on October 8, intending to confront Manfred's forces. On 27 October 1254 the Pope entered the city of Naples. It was on a sick bed at Naples that Innocent IV heard of Manfred's victory at Foggia on December 2 against the Papal forces, led by the new Papal Legate, Cardinal Guglielmo Fiesch, the Pope's nephew. The tidings are said to have precipitated Pope Innocent's death on 7 December 1254 in Naples. From triumph to disaster had taken only a few months.

Innocent's learning gave to the world an "Apparatus in quinque libros decretalium", a commentary on papal decrees. He is also remembered for issuing the papal bull "Ad extirpanda", which authorized the use of torture by the Inquisition for eliciting confessions from heretics.

Shortly after Innocent's election as Pope, his nephew Opizzo was elevated to the Latin Patriarchate of Antioch. In December, 1251, Innocent IV appointed another nephew, Ottobuono, Cardinal Deacon of S. Andriano. Ottobuono was elected Pope Adrian V in 1276.

Innocent was succeeded by Pope Alexander IV (Rinaldo de' Conti).




</doc>
<doc id="24434" url="https://en.wikipedia.org/wiki?curid=24434" title="Pope Innocent V">
Pope Innocent V

Pope Innocent V (; c. 1225 – 22 June 1276), born Pierre de Tarentaise, was pope from 21 January to 22 June 1276. He was a member of the Order of Preachers and was a close collaborator of Pope Gregory X during his pontificate. He was beatified in 1898 by Pope Leo XIII.

He was born around 1225 near Moûtiers in the Tarentaise region of the County of Savoy. An alternative popular hypothesis, however, suggests that he was born in La Salle in the Aosta valley in Italy. Both places were then part of the Kingdom of Arles in the Holy Roman Empire, but now the first is in southeastern France and the second in northwestern Italy. Another hypothesis, favored by some French scholars, is that Peter originated in a Tarantaise in Burgundy, or Tarantaise in the Department of the Loire in the Arrondisement of S. Etienne. In early life, around 1240, he joined the Dominican Order, at their convent in Lyons. In the summer of 1255, he was transferred to the "studium generale" of the Convent of S. Jacques in Paris. This move was essential for someone who was likely to study at the University of Paris. He obtained the degree of Master of Theology, and quickly acquired great fame as a preacher.

Between 1259 and 1264 he held the "Chair of the French", one of the two chairs (professorships) that were allocated to the Dominicans.

In 1259, Peter took part, perhaps because of his status as a Master at Paris, perhaps as an elected "Definitor" (delegate) for the Province of France, in the General Chapter of the Dominican Order at Valenciennes, under the leadership of the Master General, Humbertus de Romans. Peter participated together with Albert the Great, Thomas Aquinas, Bonushomo Britto, and Florentius. This General Chapter established a "ratio studiorum", or program of studies, which was to be implemented for the entire Dominican Order, that featured the study of philosophy as a preparative for those not sufficiently trained to study theology. This innovation initiated the tradition of Dominican scholastic philosophy which was to be put into practice in every Dominican convent, if possible, for example, in 1265 at the Order's "studium provinciale" at the convent of Santa Sabina in Rome. Each convent was expected to have an elected "Lector" to supervise the preparative studies and an elected Master for theological studies. In the next year he was assigned the title of Preacher General.

In 1264 a new Master General of the Order of Preachers was elected, John of Vercelli. It was taken as an opportunity to engage in some academic politics, since Humbertus de Romans, Peter's patron, was dead. One hundred and eight of Peter's statements in his "Commentary on the Sentences of Peter Lombard" were denounced as heretical. But, though Peter withdrew from his professorship, John of Vercelli appointed Thomas Aquinas to write a defense of the 108 propositions. Peter's reputation was such that he was immediately elected Provincial of the French Province for a three-year term (1264-1267). He was granted his release from office at the General Chapter, which was held in Bologna in May, 1267. At the conclusion of his term, and after Thomas of Aquinas' rejoinder to his critics was circulated, Peter returned to his Chair at the University of Paris (1267). In 1269 he was reelected to the office of Provincial of the French Province, and he held the post until he was named Archbishop of Lyons.

On 6 June 1272, Pope Gregory X himself named Peter of Tarantaise to be Archbishop of Lyons, a post he held until he was appointed to be Bishop of Ostia. It is said, however, that Peter was never consecrated. He did, however, take the oath of fealty in early December, 1272, to King Philip III of France. Pope Gregory himself arrived in Lyons in mid-November, 1273, intent upon bringing as many prelates as possible to his planned ecumenical council. He met immediately with King Philip III of France. Their conversations were obviously harmonious, since Philip ceded to the Church the Comtat Venaissin, which he had inherited from his uncle Alphonse, Count of Toulouse. The Second Council of Lyons opened on 1 May 1274. The first session was held on Monday, 7 May. The principal items on the agenda were the Crusade, and the reunion of the Eastern and Western Churches.

Peter of Tarantaise was elevated to the cardinalate on 3 June 1273, in a Consistory held at Orvieto by Pope Gregory X, and named Bishop of the suburbicarian See of Ostia. He participated in the Second Ecumenical Council of Lyons. During the Council, he sang the Funeral Mass and delivered the sermon at the funeral of Cardinal Bonaventure, Bishop of Albano, who had died on 15 July 1274, and was buried on the same day in the Church of the Franciscans in Lyons. Pope Gregory, the Fathers of the Council and the Roman Curia all attended. After the conclusion of the Council, Pope Gregory spent the Autumn and Winter in Lyons. He and his suite departed Lyons in May, 1275; he left Vienne shortly after 30 September 1275, and arrived in Lausanne on 6 October. There he met with the Emperor-elect Rudolph, King of the Romans, and on October 20 received his oath of fealty. There were seven cardinals with the Pope at the time, and their names are mentioned in the record of the oath-taking: Petrus Ostiensis, Ancherus Pantaleone of S. Prassede, Guglelmus de Bray of S. Marco, Ottobono Fieschi of S. Adriano, Giacomo Savelli of S. Maria in Cosmedin, Gottifridus de Alatri of S. Giorgio in Velabro, and Mattheus Rosso Orsini of S. Maria in Porticu. The party reached Milan on Tuesday, 12 November 1275, and Florence on 18 December. The papal party reached Arezzo in time for Christmas, but the Pope was weak and ill. The stay in Arezzo was prolonged until Gregory X died, on 10 January 1276. Only three cardinals were at his deathbed: Peter of Tarantaise, Peter Juliani of Tusculum, and Bertrand de Saint-Martin of Sabina, all cardinal-bishops. According to the Constitution "Ubi Periculum" which had been approved by the Council of Lyons, the Conclave to elect his successor should begin ten days after the pope's death.

After the required ten days had passed, the Cardinals assembled on the Vigil of St. Agnes (20 January) to hear the customary Mass of the Holy Spirit. There were twelve cardinals present. Two cardinals, Simon de Brion, who was Papal Legate in France, and Giovanni Gaetano Orsini, did not attend. The next morning, 21 January, Cardinal Petrus was the unanimous choice of the electors, on the first ballot (scrutiny). Peter of Tarantaise was the first Dominican to become Pope. He chose the pontifical name of "Innocent". His decision was to be crowned in Rome, which had not seen a pope since the departure of Gregory X in the third week of June, 1272. By 7 February the Papal Curia had reached Viterbo. King Charles of Naples rode up to Viterbo to meet the new Pope and escort him to Rome. On 22 February 1276, the Feast of S. Peter's Chair, he was crowned at the Vatican Basilica by Cardinal Giovanni Gaetano Orsini.

On 2 March 1276, Pope Innocent granted to King Charles I of Sicily the privilege of retaining the Senatorship of Rome, the government of the city, and the Rectorship of Tuscia. In a letter of 4 March, the Pope testifies that King Charles had sworn fealty for the Kingdom of Naples and of Sicily. On 9 March, he wrote to Rudolf, King of the Romans, begging him not to come to Italy, and if he had already started his journey, to break it off, until an agreement between him and the Papacy could be finalized. This meant that Rudolf's coronation, which had been agreed to by Gregory X, would not immediately take place. On the 17th, he wrote to the King of the Romans again, advising him to meet with the papal nuncios, and that, in their negotiations, he should by no means introduce the topic of the Exarchate of Ravenna, the Pentapolis, and the Romandiola. This looked like extortion. The French Innocent's favoritism toward King Charles, the brother of Louis IX and uncle of Philip III, and his harshness toward Rudolf was beginning to change the balance of power in Italy again, and was pointing in the direction of war. Pope Gregory's efforts to bring about peace had been ruined.

On the 26th he ordered the Bishops of Parma and Comacchio to see to it that Boniface de Lavania (Lavagna) be installed as Archbishop of Ravenna, as Pope Gregory X had decided. Innocent was able to arrange a peace treaty between Genoa and King Charles I, which was signed on 18 June 1276.

On 18 May 1276, Pope Innocent V notified King Philip III of France that he had appointed his friend Fr. Guy de Sully, OP, the Dominican Provincial of Paris (a post that Innocent himself had held until 1272, when he was appointed Archbishop of Lyon), to the See of Bourges.
A noteworthy feature of his brief pontificate was the practical form assumed by his desire for reunion with the Eastern Church. He wrote to Michael Palaeologus, informing him of the death of Gregory X, and apologizing for the fact that the Emperor's representatives, George, the Archdeacon of Constantinople, and Theodore, the Dispensator of the Imperial Curia, had not yet been released to return to Constantinople. He was proceeding to send legates to Michael VIII Palaeologus, the Byzantine emperor, in connection with the recent decisions of the Second Council of Lyons, hoping to broker a peace between Constantinople and King Charles I of Sicily. King Charles, however, was interested in conquest, not in concord. Innocent was interested in sending people to negotiate the reunion. He appointed Fr. Bartolommeo, O.Min., of Bologna, a Doctor of Sacred Scripture, to travel to the East, but he ordered him to come to Rome first, so that a suitable suite could be chosen for him.

Death intervened. Pope Innocent V died at Rome on 22 June 1276, after a reign of five months and one (or two) days. He was buried in the Lateran Basilica, in a magnificent tomb built by King Charles. Unfortunately, the tomb was destroyed by the two fourteenth century fires at the Basilica, in 1307 and 1361.

Innocent V had created no new cardinals at all, and therefore the cast of characters at the Conclave of July, 1276, was the same as in January. King Charles, however, was in Rome the entire time, and was in the position as Senator of Rome, to be the Governor of the Conclave. His wishes could not be ignored.

Pope Innocent V was the author of several works of philosophy, theology, and canon law, including commentaries on the Pauline epistles, and on the "Sentences" of Peter Lombard. He is sometimes referred to as "famosissimus doctor".

Pope Leo XIII beatified Peter of Tarantaise (Innocent V) on 9 March 1898, on account of his reputation for holiness and saintliness.




 


</doc>
<doc id="24437" url="https://en.wikipedia.org/wiki?curid=24437" title="Paranasal sinuses">
Paranasal sinuses

Paranasal sinuses are a group of four paired air-filled spaces that surround the nasal cavity. The maxillary sinuses are located under the eyes; the frontal sinuses are above the eyes; the ethmoidal sinuses are between the eyes and the sphenoidal sinuses are behind the eyes. The sinuses are named for the facial bones in which they are located.

Humans possess four paired paranasal sinuses, divided into subgroups that are named according to the bones within which the sinuses lie:


The paranasal air sinuses are lined with respiratory epithelium (ciliated pseudostratified columnar epithelium).

Paranasal sinuses form developmentally through excavation of bone by air-filled sacs (pneumatic diverticula) from the nasal cavity. This process begins prenatally (intrauterine life), and it continues through the course of an organism's lifetime.

The results of experimental studies suggest that the natural ventilation-rate of a sinus with a single sinus ostium (opening), is extremely slow. Such limited ventilation may be protective for the sinus, as it would help prevent drying of its mucosal surface and maintain a near-sterile environment with high carbon dioxide concentrations and minimal pathogen access. Thus composition of gas content in the maxillary sinus is similar to venous blood, with high carbon dioxide and lower oxygen levels compared to breathing air.

The paranasal sinuses are joined to the nasal cavity via small orifices called ostia. These become blocked easily by allergic inflammation, or by swelling in the nasal lining that occurs with a cold. If this happens, normal drainage of mucus within the sinuses is disrupted, and sinusitis may occur. Because the maxillary posterior teeth are close to the maxillary sinus, this can also cause clinical problems if any disease processes are present, such as an infection in any of these teeth. These clinical problems can include secondary sinusitis, the inflammation of the sinuses from another source such as an infection of the adjacent teeth.

These conditions may be treated with drugs such as decongestants, which cause vasoconstriction in the sinuses; reducing inflammation; by traditional techniques of nasal irrigation; or by corticosteroid.

Malignancies of the paranasal sinuses comprise approximately 0.2% of all malignancies. About 80% of these malignancies arise in the maxillary sinus. Men are much more often affected than women. They most often occur in the age group between 40 and 70 years. Carcinomas are more frequent than sarcomas. Metastases are rare. Tumours of the sphenoid and frontal sinuses are extremely rare.

Sinus is a Latin word meaning a "fold", "curve", or "bay". Compare "sine".

Paranasal sinuses occur in many other animals, including most mammals, birds, non-avian dinosaurs, and crocodilians. The bones occupied by sinuses are quite variable in these other species.


</doc>
<doc id="24438" url="https://en.wikipedia.org/wiki?curid=24438" title="PAL">
PAL

Phase Alternating Line (PAL) is a color encoding system for analogue television used in broadcast television systems in most countries broadcasting at 625-line / 50 field (25 frame) per second (576i). Other common colour encoding systems are NTSC and SECAM.

All the countries using PAL are currently in process of conversion or have already converted standards to DVB, ISDB or DTMB.

This page primarily discusses the PAL colour encoding system. The articles on broadcast television systems and analogue television further describe frame rates, image resolution and audio modulation.

In the 1950s, the Western European countries started planning to introduce colour television, and were faced with the problem that the NTSC standard demonstrated several weaknesses, including colour tone shifting under poor transmission conditions, which became a major issue considering Europe's geographical and weather-related particularities. To overcome NTSC's shortcomings, alternative standards were devised, resulting in the development of the PAL and SECAM standards. The goal was to provide a colour TV standard for the European picture frequency of 50 fields per second (50 hertz), and finding a way to eliminate the problems with NTSC.

PAL was developed by Walter Bruch at Telefunken in Hanover, Germany, with important input from Dr. Kruse and . The format was patented by Telefunken in 1962, citing Bruch as inventor, and unveiled to members of the European Broadcasting Union (EBU) on 3 January 1963. When asked, why the system was named "PAL" and not "Bruch" the inventor answered that a "Bruch system" would probably not have sold very well ("Bruch" lit. means "break"). The first broadcasts began in the United Kingdom in June 1967, followed by West Germany late that year. The one BBC channel initially using the broadcast standard was BBC2, which had been the first UK TV service to introduce "625-lines" in 1964. Telefunken PALcolor 708T was the first PAL commercial TV set. It was followed by Loewe-Farbfernseher S 920 & F 900.

Telefunken was later bought by the French electronics manufacturer Thomson. Thomson also bought the "Compagnie Générale de Télévision" where Henri de France developed SECAM, the first European Standard for colour television. Thomson, now called Technicolor SA, also owns the RCA brand and licenses it to other companies; Radio Corporation of America, the originator of that brand, created the NTSC colour TV standard before Thomson became involved.

The term PAL was often used informally and somewhat imprecisely to refer to the 625-line/50 Hz (576i) television system in general, to differentiate from the 525-line/60 Hz (480i) system generally used with NTSC. Accordingly, DVDs were labelled as PAL or NTSC (referring to the line count and frame rate) even though technically the discs carry neither PAL nor NTSC encoded signal. CCIR 625/50 and EIA 525/60 are the proper names for these (line count and field rate) standards; PAL and NTSC on the other hand are methods of encoding color information in the signal.

Both the PAL and the NTSC system use a quadrature amplitude modulated subcarrier carrying the chrominance information added to the luminance video signal to form a composite video baseband signal. The frequency of this subcarrier is 4.43361875 MHz for PAL and NTSC 4.43, compared to 3.579545 MHz for NTSC 3.58. The SECAM system, on the other hand, uses a frequency modulation scheme on its two line alternate colour subcarriers 4.25000 and 4.40625 MHz.

The name "Phase Alternating Line" describes the way that the phase of part of the colour information on the video signal is reversed with each line, which automatically corrects phase errors in the transmission of the signal by cancelling them out, at the expense of vertical frame colour resolution. Lines where the colour phase is reversed compared to NTSC are often called PAL or phase-alternation lines, which justifies one of the expansions of the acronym, while the other lines are called NTSC lines. Early PAL receivers relied on the human eye to do that cancelling; however, this resulted in a comb-like effect known as Hanover bars on larger phase errors. Thus, most receivers now use a chrominance analog delay line, which stores the received colour information on each line of display; an average of the colour information from the previous line and the current line is then used to drive the picture tube. The effect is that phase errors result in saturation changes, which are less objectionable than the equivalent hue changes of NTSC. A minor drawback is that the vertical colour resolution is poorer than the NTSC system's, but since the human eye also has a colour resolution that is much lower than its brightness resolution, this effect is not visible. In any case, NTSC, PAL, and SECAM all have chrominance bandwidth (horizontal colour detail) reduced greatly compared to the luminance signal.

The 4.43361875 MHz frequency of the colour carrier is a result of 283.75 colour clock cycles per line plus a 25 Hz offset to avoid interferences. Since the line frequency (number of lines per second) is 15625 Hz (625 lines × 50 Hz ÷ 2), the colour carrier frequency calculates as follows: 4.43361875 MHz = 283.75 × 15625 Hz + 25 Hz.

The original colour carrier is required by the colour decoder to recreate the colour difference signals. Since the carrier is not transmitted with the video information it has to be generated locally in the receiver. In order that the phase of this locally generated signal can match the transmitted information, a 10 cycle burst of colour subcarrier is added to the video signal shortly after the line sync pulse, but before the picture information, during the so-called back porch. This colour burst is not actually in phase with the original colour subcarrier, but leads it by 45 degrees on the odd lines and lags it by 45 degrees on the even lines. This swinging burst enables the colour decoder circuitry to distinguish the phase of the R-Y vector which reverses every line.

PAL usually has 576 visible lines compared with 480 lines with NTSC, meaning that PAL has a 20% higher resolution, in fact it even has a higher resolution than Enhanced Definition standard (854x480). Most TV output for PAL and NTSC use interlaced frames meaning that even lines update on one field and odd lines update on the next field. Interlacing frames gives a smoother motion with half the frame rate. NTSC is used with a frame rate of 60i or 30p whereas PAL generally uses 50i or 25p; both use a high enough frame rate to give the illusion of fluid motion. This is due to the fact that NTSC is generally used in countries with a utility frequency of 60 Hz and PAL in countries with 50 Hz, although there are many exceptions. Both PAL and NTSC have a higher frame rate than film which uses 24 frames per second. PAL has a closer frame rate to that of film, so most films are sped up 4% to play on PAL systems, shortening the runtime of the film and, without adjustment, slightly raising the pitch of the audio track. Film conversions for NTSC instead use 3:2 pull down to spread the 24 frames of film across 60 interlaced fields. This maintains the runtime of the film and preserves the original audio, but may cause worse interlacing artifacts during fast motion.

NTSC receivers have a tint control to perform colour correction manually. If this is not adjusted correctly, the colours may be faulty. The PAL standard automatically cancels hue errors by phase reversal, so a tint control is unnecessary yet Saturation control can be more useful. Chrominance phase errors in the PAL system are cancelled out using a 1H delay line resulting in lower saturation, which is much less noticeable to the eye than NTSC hue errors.

However, the alternation of colour information—Hanover bars—can lead to picture grain on pictures with extreme phase errors even in PAL systems, if decoder circuits are misaligned or use the simplified decoders of early designs (typically to overcome royalty restrictions). In most cases such extreme phase shifts do not occur. This effect will usually be observed when the transmission path is poor, typically in built up areas or where the terrain is unfavourable. The effect is more noticeable on UHF than VHF signals as VHF signals tend to be more robust.

In the early 1970s some Japanese set manufacturers developed decoding systems to avoid paying royalties to Telefunken. The Telefunken licence covered any decoding method that relied on the alternating subcarrier phase to reduce phase errors. This included very basic PAL decoders that relied on the human eye to average out the odd/even line phase errors. One solution was to use a 1H analog delay line to allow decoding of only the odd or even lines. For example, the chrominance on odd lines would be switched directly through to the decoder and also be stored in the delay line. Then, on even lines, the stored odd line would be decoded again. This method effectively converted PAL to NTSC. Such systems suffered hue errors and other problems inherent in NTSC and required the addition of a manual hue control.

PAL and NTSC have slightly divergent colour spaces, but the colour decoder differences here are ignored.

The SECAM patents predate those of PAL by several years (1956 vs 1962). Its creator, Henri de France, in search of a response to known NTSC hue problems, came up with ideas that were to become fundamental to both European systems, namely:
1) colour information on two successive TV lines is very similar and vertical resolution can be halved without serious impact on perceived visual quality
2) more robust colour transmission can be achieved by spreading information on two TV lines instead of just one
3) information from the two TV lines can be recombined using a delay line.

SECAM applies those principles by transmitting alternately only one of the U and V components on each TV line, and getting the other from the delay line. QAM is not required, and frequency modulation of the subcarrier is used instead for additional robustness (sequential transmission of U and V was to be reused much later in Europe's last "analog" video systems: the MAC standards).

SECAM is free of both hue and saturation errors. It is not sensitive to phase shifts between the color burst and the chrominance signal, and for this reason was sometimes used in early attempts at color video recording, where tape speed fluctuations could get the other systems into trouble. In the receiver, it did not require a quartz crystal (which was an expensive component at the time) and generally could do with lower accuracy delay lines and components.

SECAM transmissions are more robust over longer distances than NTSC or PAL. However, owing to their FM nature, the color signal remains present, although at reduced amplitude, even in monochrome portions of the image, thus being subject to stronger cross color.

One serious drawback for studio work is that the addition of two SECAM signals does not yield valid colour information, due to its use of frequency modulation. It was necessary to demodulate the FM and handle it as AM for proper mixing, before finally remodulating as FM, at the cost of some added complexity and signal degradation. In its later years, this was no longer a problem, due to the wider use of component and digital equipment.

PAL can work without a delay line, but this configuration, sometimes referred to as "poor man's PAL", could not match SECAM in terms of picture quality. To compete with it at the same level, it had to make use of the main ideas outlined above, and as a consequence PAL had to pay license fees to SECAM. Over the years, this contributed significantly to the estimated 500 million francs gathered by the SECAM patents (for an initial 100 million francs invested in research).

Hence, PAL could be considered as a hybrid system, with its signal structure closer to NTSC, but its decoding borrowing much from SECAM.

There were initial specifications to use color with the French 819 line format (system E). However, "SECAM E" only ever existed in development phases. Actual deployment used the 625 line format. This made for easy interchange and conversion between PAL and SECAM in Europe. Conversion was often not even needed, as more and more receivers and VCRs became compliant with both standards, helped in this by the common decoding steps and components. And furthermore, when the SCART plug became standard, it could take RGB as an input, effectively bypassing all the color coding formats' peculiarities.

When it comes to home VCRs, all video standards use what is called "color under" format. Color is extracted from the high frequencies of the video spectrum, and moved to the lower part of the spectrum available from tape. Luminance then uses what remains of it, above the color frequency range. This is usually done by heterodyning for PAL (as well as NTSC). But the FM nature of color in SECAM allows for a cheaper trick: division by 4 of the subcarrier frequency (and multiplication on replay). This became the standard for SECAM VHS recording in France. Most other countries kept using the same heterodyning process as for PAL or NTSC and this is known as MESECAM recording (as it was more convenient for some Middle East countries that used both PAL and SECAM broadcasts).

Regarding early (analog) videodiscs, the established Laserdisc standard supported only NTSC and PAL. However, a different optical disc format, the Thomson transmissive optical disc made a brief appearance on the market. At some point, it used a modified SECAM signal (single FM subcarrier at 3.6 MHz). The media's flexible and transmissive material allowed for direct access to both sides without flipping the disc, a concept that reappeared in multi-layered DVDs about fifteen years later.

For PAL-B/G the signal has these characteristics.

After 0.9 µs a colorburst of cycles is sent. Most rise/fall times are in range. Amplitude is 100% for white level, 30% for black, and 0% for sync.
The CVBS electrical amplitude is Vpp and impedance of 75 Ω.

The vertical timings are:

As PAL is interlaced, every two fields are summed to make a complete picture frame.

Luminance, formula_1, is derived from red, green, and blue (formula_2) signals:

formula_4 and formula_5 are used to transmit chrominance. Each has a typical bandwidth of 1.3 MHz.

Composite PAL signal formula_8timing where formula_9.

Subcarrier frequency formula_10 is 4.43361875 MHz (±5 Hz) for PAL-B/D/G/H/I/N.

This table illustrates the differences:
<nowiki>*</nowiki> System I has never been used on VHF in the UK.

Many countries have turned off analog transmissions, so the following does not apply, except for using devices which output broadcast signals, such as video recorders. The resolution that PAL gave may or may not still be used, but HD or full HD are most commonly used in digital transmissions.

The majority of countries using PAL have television standards with 625 lines and 50 fields per second, differences concern the audio carrier frequency and channel bandwidths. The variants are:


Systems B and G are similar. System B specifies 7 MHz channel bandwidth, while System G specifies 8 MHz channel bandwidth. Australia used System B for VHF and UHF channels. Similarly, Systems D and K are similar except for the bands they use: System D is only used on VHF (except in mainland China), while System K is only used on UHF. Although System I is used on both bands, it has only been used on UHF in the United Kingdom.

In Brazil, PAL is used in conjunction with the 525 line, 59.94 field/s system M, using (very nearly) the NTSC colour subcarrier frequency. Exact colour subcarrier frequency of PAL-M is 3.575611 MHz. Almost all other countries using system M use NTSC.

The PAL colour system (either baseband or with any RF system, with the normal 4.43 MHz subcarrier unlike PAL-M) can also be applied to an NTSC-like 525-line (480i) picture to form what is often known as "PAL-60" (sometimes "PAL-60/525", "Quasi-PAL" or "Pseudo PAL"). PAL-M (a broadcast standard) however should not be confused with "PAL-60" (a video playback system—see below).

In Argentina, Paraguay and Uruguay the PAL-N variant is used. It employs the 625 line/50 field per second waveform of PAL-B/G, D/K, H, and I, but on a 6 MHz channel with a chrominance subcarrier frequency of 3.582056 MHz very similar to NTSC.

VHS tapes recorded from a PAL-N or a PAL-B/G, D/K, H, or I broadcast are indistinguishable because the downconverted subcarrier on the tape is the same. A VHS recorded off TV (or released) in Europe will play in colour on any PAL-N VCR and PAL-N TV in Argentina, Paraguay and Uruguay. Likewise, any tape recorded in Argentina, Paraguay or Uruguay off a PAL-N TV broadcast can be sent to anyone in European countries that use PAL (and Australia/New Zealand, etc.) and it will display in colour. This will also play back successfully in Russia and other SECAM countries, as the USSR mandated PAL compatibility in 1985—this has proved to be very convenient for video collectors.

People in Argentina, Paraguay and Uruguay usually own TV sets that also display NTSC-M, in addition to PAL-N. Direct TV also conveniently broadcasts in NTSC-M for North, Central, and South America. Most DVD players sold in Argentina, Paraguay and Uruguay also play PAL discs—however, this is usually output in the European variant (colour subcarrier frequency 4.433618 MHz), so people who own a TV set which only works in PAL-N (plus NTSC-M in most cases) will have to watch those PAL DVD imports in black and white (unless the TV supports RGB SCART) as the colour subcarrier frequency in the TV set is the PAL-N variation, 3.582056 MHz.

In the case that a VHS or DVD player works in PAL (and not in PAL-N) and the TV set works in PAL-N (and not in PAL), there are two options:


Some DVD players (usually lesser known brands) include an internal transcoder and the signal can be output in NTSC-M, with some video quality loss due to the system's conversion from a 625/50 PAL DVD to the NTSC-M 525/60 output format. A few DVD players sold in Argentina, Paraguay and Uruguay also allow a signal output of NTSC-M, PAL, or PAL-N. In that case, a PAL disc (imported from Europe) can be played back on a PAL-N TV because there are no field/line conversions, quality is generally excellent.

Extended features of the PAL specification, such as Teletext, are implemented quite differently in PAL-N. PAL-N supports a modified 608 closed captioning format that is designed to ease compatibility with NTSC originated content carried on line 18, and a modified teletext format that can occupy several lines.

Some special VHS video recorders are available which can allow viewers the flexibility of enjoying PAL-N recordings using a standard PAL ( 625/50 Hz ) colour TV, or even through multi-system TV sets. Video recorders like Panasonic NV-W1E (AG-W1 for the US), AG-W2, AG-W3, NV-J700AM, Aiwa HV-M110S, HV-M1U, Samsung SV-4000W and SV-7000W feature a digital TV system conversion circuitry.

The PAL L (Phase Alternating Line with L-sound system) standard uses the same video system as PAL-B/G/H (625 lines, 50 Hz field rate, 15.625 kHz line rate), but with 6 MHz video bandwidth rather than 5.5 MHz. This requires the audio subcarrier to be moved to 6.5 MHz. An 8 MHz channel spacing is used for PAL-L.

The BBC tested their pre-war 405 line monochrome system with all three colour standards including PAL, before the decision was made to abandon 405 and transmit colour on 625/System I only.

The PAL colour system is usually used with a video format that has 625 lines per frame (576 visible lines, the rest being used for other information such as sync data and captioning) and a refresh rate of 50 interlaced fields per second (compatible with 25 full frames per second), such systems being B, G, H, I, and N (see broadcast television systems for the technical details of each format).

This ensures video interoperability. However, as some of these standards (B/G/H, I and D/K) use different sound carriers (5.5 MHz, 6.0 MHz 6.5 MHz respectively), it may result in a video image without audio when viewing a signal broadcast over the air or cable. Some countries in Eastern Europe which formerly used SECAM with systems D and K have switched to PAL while leaving other aspects of their video system the same, resulting in the different sound carrier. Instead, other European countries have changed completely from SECAM-D/K to PAL-B/G.

The PAL-N system has a different sound carrier, and also a different colour subcarrier, and decoding on incompatible PAL systems results in a black-and-white image without sound. 
The PAL-M system has a different sound carrier and a different colour subcarrier, and does not use 625 lines or 50 frames/second. This would result in no video or audio at all when viewing a European signal.

Recently manufactured PAL television receivers can typically decode all of these systems except, in some cases, PAL-M and PAL-N. Many of receivers can also receive Eastern European and Middle Eastern SECAM, though rarely French-broadcast SECAM (because France used a quasi-unique positive video modulation, system L) unless they are manufactured for the French market. They will correctly display plain CVBS or S-video SECAM signals. Many can also accept baseband NTSC-M, such as from a VCR or game console, and RF modulated NTSC with a PAL standard audio subcarrier (i.e., from a modulator), though not usually broadcast NTSC (as its 4.5 MHz audio subcarrier is not supported). Many sets also support NTSC with a 4.43 MHz subcarrier.

Many 1990s-onwards video recorders sold in Europe can play back NTSC tapes. When operating in this mode most of them do not output a true (625/25) PAL signal, but rather a hybrid consisting of the original NTSC line standard (525/30), but with colour converted to PAL 4.43 MHz—this is known as "PAL 60" (also "quasi-PAL" or "pseudo PAL") with "60" standing for 60 Hz (for 525/30), instead of 50 Hz (for 625/25). Some video game consoles also output a signal in this mode. Most newer television sets can display such a signal correctly, but some will only do so (if at all) in black and white and/or with flickering/foldover at the bottom of the picture, or picture rolling (however, many old TV sets can display the picture properly by means of adjusting the V-Hold and V-Height knobs—assuming they have them). Some TV tuner cards or video capture cards will support this mode (although software/driver modification can be required and the manufacturers' specs may be unclear). A "PAL 60" signal is similar to an NTSC (525/30) signal, but with the usual PAL chrominance subcarrier at 4.43 MHz (instead of 3.58 as with NTSC and South American PAL variants) and with the PAL-specific phase alternation of the red colour difference signal between the lines.

Most European DVD players output a true NTSC-M signal when playing NTSC discs, which many modern European TV sets can resolve.

Below countries and territories currently use or once used the PAL system. Many of these have converted or are currently converting PAL to DVB-T (most countries), DVB-T2 (most countries), DTMB (China, Hong Kong and Macau) or ISDB (Sri Lanka, Maldives, Botswana and part of South America).





The following countries no longer use PAL for terrestrial broadcasts, and are in process of converting from PAL (cable) to DVB-C.




</doc>
<doc id="24442" url="https://en.wikipedia.org/wiki?curid=24442" title="Philemon">
Philemon

Philemon may refer to:







</doc>
<doc id="24443" url="https://en.wikipedia.org/wiki?curid=24443" title="Polo">
Polo

Polo is a team sport played on horseback. The objective is to score goals against an opposing team. Players score by driving a small hard white ball into the opposing team's goal using a long-handled wooden mallet. The modern sport of polo is played on a grass field of . Each polo team consists of four riders and their polo ponies.

Arena polo has three players per team and the game usually involves more maneuvering and shorter plays at lower speeds due to space limitations of arenas. Arena polo is played with a small air-filled ball, similar to a small football.

The modern game usually lasts one to two hours and is divided into periods called chukkas (or "chukkers"). Polo is played professionally in 16 countries. Polo was an Olympic sport from 1900 to 1936.

The invention of polo is dated variously from the 6th century BC to the 1st century AD. Its exact origins are unknown, although China, India, Iran, Mongolia and Pakistan all claim to be the birthplace of polo. Valuable for training cavalry, the game was played from Constantinople to Japan by the Middle Ages. It likely began as a simple game played by mounted nomads of Iranian and Turkic origin in Central Asia, from where it spread to Persia and beyond. An archaic variation of polo, regionally referred to as "buzkashi" or "kokpar", is still played in parts of Asia.

The sport entered Persia during the period of the Parthian Empire (247 BC to 224 AD). In Persia, polo enjoyed great patronage under kings and noblemen and became known as "chovgan". The game continued to be supported by Mongol rulers of Persia in the 11th century, as well as under the Safavid dynasty. Emperor Shapur II learnt to play polo when he was seven years old in 316 AD, and in the 17th century, Naqsh-i Jahan Square in Isfahan was built as a polo field by King Abbas I. The game was also learnt by the neighbouring Byzantine Empire at an early date. A "tzykanisterion" (stadium for playing "tzykanion", the Byzantine name for polo) was built by emperor Theodosius II (r. 408–450) inside the Great Palace of Constantinople. Emperor Basil I (r. 867–886) excelled at it; Emperor Alexander (r. 912–913) died from exhaustion while playing and John I of Trebizond (r. 1235–1238) died from a fatal injury during a game. After the Muslim conquests to the Ayyubid and Mameluke dynasties of Egypt and the Levant, their elites favoured it above all other sports. Notable sultans such as Saladin and Baybars were known to play it and encourage it in their court. Polo sticks were features on the Mameluke precursor to modern day playing cards.

From Persia, the game spread to South Asia where it has had a strong presence in the north western areas of present-day Pakistan (including Gilgit, Chitral, Hunza and Baltistan) since at least the 15th-16th century. The name "polo" is said to have been derived from the Balti word "pulu", meaning ball. Qutubuddin Aibak, the Turkic slave from Central Asia who later became the Sultan of Delhi in Northern India, ruled as a Sultan for only four years, from 1206 to 1210, dying an accidental death during a game of polo when his horse fell and he was impaled on the pommel of his saddle. Polo likely travelled via the Silk Road to China where it was popular in the Chinese Tang dynasty capital of Chang'an, and also played by women, who wore male dress to do so; many Tang dynasty tomb figures of female players survive.

The modern game of polo is derived from Manipur, India, where the game was known as 'Sagol Kangjei', 'Kanjai-bazee', or 'Pulu'.
It was the anglicised form of the last, referring to the wooden ball that was used, which was adopted by the sport in its slow spread to the west. The first polo club was established in the town of Silchar in Assam, India, in 1833.

The origins of the game in Manipur are traced to early precursors of Sagol Kangjei. This was one of three forms of hockey in Manipur, the other ones being field hockey (called Khong Kangjei) and wrestling-hockey (called Mukna Kangjei). Local rituals such as those connected to the Marjing, the Winged-Pony God of Polo and the creation-ritual episodes of the Lai Haraoba festival enacting the life of his son, Khori-Phaba, the polo-playing god of sports. These may indicate an origin earlier than the historical records of Manipur. Later, according to Chaitharol-Kumbaba, a Royal Chronicle of Manipur King Kangba who ruled Manipur much earlier than Nongda Lairen Pakhangba (33 AD) introduced Sagol Kangjei (Kangjei on horse back). Further regular playing of this game commenced in 1605 during the reign of King Khagemba under newly framed rules of the game. However it was the first Mughal emperor, Babur, who popularised the sport in India and ultimately made a significant influence on England.
In Manipur, polo is traditionally played with seven players to a side. The players are mounted on the indigenous Manipuri pony, which stands less than . There are no goal posts, and a player scores simply by hitting the ball out of either end of the field. Players strike the ball with the long side of the mallet head, not the end. Players are not permitted to carry the ball, although blocking the ball with any part of the body except the open hand is permitted. The sticks are made of cane, and the balls are made from the roots of bamboo. Players protected their legs by attaching leather shields to their saddles and girths.

In Manipur, the game was played even by commoners who owned a pony. The kings of Manipur had a royal polo ground within the ramparts of their Kangla Fort. Here they played Manung Kangjei Bung (literally, "Inner Polo Ground"). Public games were held, as they are still today, at the Mapan Kangjei Bung (literally "Outer Polo Ground"), a polo ground just outside the Kangla. Weekly games called Hapta Kangjei (Weekly Polo) were also played in a polo ground outside the current Palace.

The oldest polo ground in the world is the Imphal Polo Ground in Manipur State. The history of this pologround is contained in the royal chronicle "Cheitharol Kumbaba" starting from AD 33. Lieutenant (later Major General) Joseph Ford Sherer, the father of modern polo visited the state and played on this polo ground in the 1850s. Lord Curzon, the Viceroy of India visited the state in 1901 and measured the polo ground as "225 yards long and 110 yards wide" .

In 1862 the oldest polo club still in existence, Calcutta Polo Club, was established by two British soldiers, Sherer and Captain Robert Stewart. Later they spread the game to their peers in England. The British are credited with spreading polo worldwide in the late 19th century and the early 20th century at the height of its empire. Military officers imported the game to Britain in the 1860s. The establishment of polo clubs throughout England and western Europe followed after the formal codification of rules. The 10th Hussars at Aldershot, Hants, introduced polo to England in 1834. The game's governing body in the United Kingdom is the Hurlingham Polo Association, which drew up the first set of formal British rules in 1874, many of which are still in existence.

This version of polo played in the 19th century was different from the faster form that was played in Manipur. The game was slow and methodical, with little passing between players and few set plays that required specific movements by participants without the ball. Neither players nor horses were trained to play a fast, nonstop game. This form of polo lacked the aggressive methods and equestrian skills to play. From the 1800s to the 1910s, a host of teams representing Indian principalities dominated the international polo scene.

The Champions polo league was launched in Jaipur in 2016. It is a new version of polo, similar to the T20 format of cricket. The pitch was made smaller and accommodated a huge audience. The First Event of the World Champions Polo League took place in Bhavnagar, Gujarat, with room for 10,000 spectators. The rules were changed and the duration was made shorter. Officially played 7–9 April in Bhavnagar, including India's most decorated polo player Samir Suhag, Shamsher Ali, foreign players Richard Henriques from Ireland and South Africa and others participated. Six teams were launched and Iscon Hemvijaya emerged the winner, while IPCL were runners up.

British settlers in the Argentine pampas started practicing polo during their free time. Among them, David Shennan is credited with having organised the first formal polo game of the country in 1875, at Estancia El Negrete, located in the province of Buenos Aires.

The sport spread quickly between the skilful gauchos, and several clubs opened in the following years in the towns of Venado Tuerto, Cañada de Gómez, Quilmes, Flores and later (1888) Hurlingham. In 1892 The River Plate Polo Association was founded and constituted the basis for the current Asociación Argentina de Polo. In the Olympic Games held in Paris in 1924 a team composed by Juan Miles, Enrique Padilla, Juan Nelson, Arturo Kenny, G. Brooke Naylor and A. Peña obtained the first gold medal for the country's olympic history; this also occurred in Berlín 1936 with players Manuel Andrada, Andrés Gazzotti, Roberto Cavanagh, Luis Duggan, Juan Nelson, Diego Cavanagh and Enrique Alberdi.

The game spread across the country, and Argentina is credited globally as the capital of polo, and Argentina is notably the country with the largest number ever of 10 handicap players in the world.

Five teams were able to gather four 10 handicap players each, to make 40 handicap teams: Coronel Suárez, 1975, 1977–1979 (Alberto Heguy, Juan Carlos Harriott, Alfredo Harriot and Horacio Heguy); La Espadaña, 1989–1990 (Carlos Gracida, Gonzalo Pieres, Alfonso Pieres y Ernesto Trotz Jr.); Indios Chapaleufú, 1992–1993 (Bautista Heguy, Gonzalo Heguy, Horacio Heguy Jr. and Marcos Heguy); La Dolfina, 2009–2010 (Adolfo Cambiaso Jr., Lucas Monteverde, Mariano Aguerre y Bartolomé Castagnola); Ellerstina, 2009 (Facundo Pieres, Gonzalo Pieres Jr., Pablo Mac Donough and Juan Martín Nero).

The three major polo tournaments in Argentina, known as "Triple Corona" ("Triple Crown"), are Hurlingham Polo Open, Tortugas Polo Open, Palermo Polo Open. Polo season usually last from October to December.

Polo has found popularity throughout the rest of the Americas, including Brazil, Chile, Mexico, and the United States of America.

James Gordon Bennett Jr. on 6 May 1876 organised what was billed as the first polo match in the United States at Dickel's Riding Academy at 39th Street and Fifth Avenue in New York City. The historical record states that James Gordon Bennett established the Westchester Polo Club on 6 May 1876 and on 13 May 1876 the Jerome Park Racetrack in Westchester County was the site of the "first" American outdoor polo match.

H.L. Herbert, James Gordon Bennett and August Belmont financed the original New York Polo Grounds. Herbert stated in a 1913 article that they formed the Westchester Club "after" the "first" outdoor game was played on 13 May 1876. This contradicts the historical record of the club being established before the Jerome Park game..

There is ample evidence that the first to play polo in America was actually the English Texans. The Galveston News reported on 2 May 1876 that Denison Texas had a Polo Club which was before James Gordon Bennett established his Westchester Club or attempted to play the "first" game. The Denison team sent a letter to James Gordon Bennett challenging him to a match game. The challenge was published 2 June 1876 in The Galveston Daily News. By the time the article came out on 2 June the Denison Club had already received a letter from Bennett indicating the challenge was offered before the "first" games in New York.

There is also an urban legend that the first game of polo in America was played in Boerne, Texas at retired British officer Captain Glynn Turquand's famous Balcones Ranch The Boerne, Texas legend also has plenty of evidence pointing to the fact that polo was played in Boerne before James Gordon Bennett Jr. ever picked up a polo mallet.

During the early part of the 20th century, under the leadership of Harry Payne Whitney, polo changed to become a high-speed sport in the United States, differing from the game in England, where it involved short passes to move the ball towards the opposition's goal. Whitney and his teammates used the fast break, sending long passes downfield to riders who had broken away from the pack at a full gallop.

In the late 1950s, champion polo player and Director of the Long Island Polo Association, Walter Scanlon, introduced the "short form", or "European" style, four period match, to the game of polo.
The rules of polo are written and used to provide for the safety of both players and horses. The rules are enforced in the game by the umpires who blow whistles when a penalty occurs. Strategic plays in polo are based on the "line of the ball", an imaginary line created by the ball as it travels down the field. This line traces the ball's path and extends past the ball along that trajectory. The line of the ball defines rules for players to approach the ball safely. The "line of the ball" changes each time the ball changes direction. The player who hit the ball generally has the right of way, and other players cannot cross the line of the ball in front of that player. As players approach the ball, they ride on either side of the line of the ball giving each access to the ball. A player can cross the line of the ball when it does not create a dangerous situation. Most fouls and penalty shots are related to players improperly crossing the line of the ball or the right of way. When a player has the line of the ball on his right, he has the right of way. A "ride-off" is when a player moves another player off the line of the ball by making shoulder-to-shoulder contact with the other players' horses.

The defending player has a variety of opportunities for his team to gain possession of the ball. He can push the opponent off the line or steal the ball from the opponent. Another common defensive play is called "hooking." While a player is taking a swing at the ball, his opponent can block the swing by using his mallet to hook the mallet of the player swinging at the ball. A player may hook only if he is on the side where the swing is being made or directly behind an opponent. A player may not purposely touch another player, his tack or pony with his mallet. Unsafe hooking is a foul that will result in a penalty shot being awarded. For example, it is a foul for a player to reach over an opponent's mount in an attempt to hook.

The other basic defensive play is called the bump or ride-off. It's similar to a body check in hockey. In a ride-off, a player rides his pony alongside an opponent's mount in order to move an opponent away from the ball or to take him out of a play. It must be executed properly so that it does not endanger the horses or the players. The angle of contact must be safe and can not knock the horses off balance, or harm the horses in any way. Two players following the line of the ball and riding one another off have the right of way over a single man coming from any direction.

Like in hockey or basketball, fouls are potentially dangerous plays that infringe on the rules of the game. To the novice spectator, fouls may be difficult to discern. There are degrees of dangerous and unfair play and penalty shots are awarded depending based on the severity of the foul and where the foul was committed on the polo field. White lines on the polo field indicate where the mid-field, sixty, forty and thirty yard penalties are taken.

The official set of rules and rules interpretations are reviewed and published annually by each country's polo association. Most of the smaller associations follow the rules of the Hurlingham Polo Association, the national governing body of the sport of polo in the United Kingdom, and the United States Polo Association.

Outdoor or field polo consists of four to eight 7-minute chukkas, between or during which players change mounts. At the end of each 7 minute chukka, play continues for an additional 30 seconds or until a stoppage in play, whichever comes first. There is a four-minute interval between chukkas and a ten-minute halftime. Play is continuous and is only stopped for penalties, broken tack (equipment) or injury to horse or player. The object is to score goals by hitting the ball between the goal posts, no matter how high in the air. If the ball goes wide of the goal, the defending team is allowed a free 'knock-in' from the place where the ball crossed the goal line, thus getting ball back into play.

Arena polo has rules similar to the field version, and is less strenuous for the player. It is played in a enclosed arena, much like those used for other equestrian sports; the minimum size is . There are many arena clubs in the United States, and most major polo clubs, including the Santa Barbara Polo & Raquet Club, have active arena programmes. The major differences between the outdoor and indoor games are: speed (outdoor being faster), physicality/roughness (indoor/arena is more physical), ball size (indoor is larger), goal size (because the arena is smaller the goal is smaller), and some penalties. In the United States and Canada, collegiate polo is arena polo; in the UK, collegiate polo is both.

Forms of arena polo include beach polo, played in many countries between teams of three riders on a sand surface, and cowboy polo, played almost exclusively in the western United States by teams of five riders on a dirt surface.

Another modern variant is snow polo, which is played on compacted snow on flat ground or a frozen lake. The format of snow polo varies depending on the space available. Each team generally consists of three players and a bright coloured light plastic ball is preferred.

Snow polo is not the same sport as ice polo, which was popular in the US in the late 1890s. The sport resembled ice hockey and bandy but died out entirely in favour of the Canadian ice hockey rules.

A popular combination of the sports of polo and lacrosse is the game of polocrosse, which was developed in Australia in the late 1930s.

These sports are considered as separate sports because of the differences in the composition of teams, equipment, rules, game facilities etc.

Polo is not played exclusively on horseback. Such polo variants are mostly played for recreational or tourist purposes; they include canoe polo, cycle polo, camel polo, elephant polo, golfcart polo, Segway polo and yak polo. In the early 1900s in the United States, cars were used instead of horses in the sport of Auto polo. Hobby Horse Polo is using hobby horses instead of ponies. It uses parts of the polo rules but has its own specialities, as e.g. 'punitive sherries'. The Hobby Horse variant started 1998 as a fun sport in south western Germany and lead 2002 to the foundation of the First Kurfürstlich-Kurpfälzisch Polo-Club in Mannheim. In the meantime it gained further interest in other German cities.

All tournaments and levels of play and players are organized within and between polo clubs, including membership, rules, safety, fields and arenas.

Club Polo (or County Polo in the UK) is usually overseen by qualified mounted instructors or umpires. In the UK the original County Polo Association was formed in 1898* to look after the interests of the country clubs and to run the County Cup Tournaments, the three London polo clubs—Hurlingham, Ranelagh and Roehampton—and from all associations within the Empire where polo was being played.

The mounts used are called 'polo ponies', although the term pony is purely traditional and the mount is actually a full-sized horse. They range from high at the withers, and weigh . The polo pony is selected carefully for quick bursts of speed, stamina, agility and manoeuvrability. Temperament is critical; the horse must remain responsive under pressure and not become excited or difficult to control. Many are Thoroughbreds or Thoroughbred crosses. They are trained to be handled with one hand on the reins, and to respond to the rider's leg and weight cues for moving forward, turning and stopping. A well trained horse will carry its rider smoothly and swiftly to the ball and can account for 60 to 75 percent of the player's skill and net worth to his team.

Polo pony training generally begins at age three and lasts from about six months to two years. Most horses reach full physical maturity at about age five, and ponies are at their peak of athleticism and training at around age 6 or 7. However, without any accidents, polo ponies may have the ability to play until they are 18 to 20 years of age.

The Argentine Polo Breeders Association (which is the organization that registers this Argentine Polo Pony breed) has indicated that on average, this race has a height of 1.56 meters and an average weight between 400 and 500 kilos.

Each player must have more than one horse, to allow for tired mounts to be replaced by fresh ones between or even during chukkas. A player's "string" of polo ponies may number 2 or 3 in Low Goal matches (with ponies being rested for at least a chukka before reuse), 4 or more for Medium Goal matches (at least one per chukka), and even more for the highest levels of competition.

Each team consists of four mounted players, which can be mixed teams of both men and women.

Each position assigned to a player has certain responsibilities:


Polo must be played right-handed in order to prevent head-on collisions.

The rules for equipment vary in details between the hosting authorities, but are always for the safety of the players and mounts.

Mandatory equipment includes a protective helmet with chinstrap worn at all times by all players and mounted grooms. They must be to the locally accepted safety standard, "PAS015" (UK), "NOCSAE" (USA).
A faceguard is commonly integral with a helmet.

Polo boots and kneeguards are mandatory in the UK during official play, and boots are recommended for all play everywhere. The UK also recommends goggles, elbow pads and gum shields.
A shirt or jersey is required that distinguishes the player's team, and is not black and white stripes like an umpire shirt.

White polo pants or trousers are worn during official play. Polo gloves are commonly worn to protect from working the reins and mallet.

Not permitted is any equipment that may harm horses, like certain spurs or whips.

The modern outdoor polo ball is made of a high-impact plastic. Historically they have been made of bamboo, leather covered cork, hard rubber, and for many years willow root. Originally the British used a white painted leather covered cricket ball.

The regulation outdoor polo ball is to in diameter and weighs to .

Plastic balls were introduced in the 1970s. They are less prone to breakage and much cheaper.

The indoor and arena polo ball is leather-covered and inflated, and is about in diameter.

It must be not less than or more than in circumference. The weight must be not less than or more than . In a bounce test from on concrete at , the rebound should be a minimum of and a maximum of at the inflation rate specified by the manufacturer. This provides for a hard and lively ball.

The polo mallet comprises a cane shaft with a rubber-wrapped grip, a webbed thong, called a sling, for wrapping around the thumb, and a wooden cigar-shaped head. The shaft is made of manau-cane (not bamboo, which is hollow) although a small number of mallets today are made from composite materials. Composite materials are usually not preferred by top players because the shaft of composite mallets can't absorb vibrations as well as traditional cane mallets. The mallet head is generally made from a hardwood called tipa, approximately 9" inches long. The mallet head weighs from to , depending on player preference and the type of wood used, and the shaft can vary in weight and flexibility depending on the player's preference. The weight of the mallet head is of important consideration for the more seasoned players. Female players often use lighter mallets than male players. For some polo players, the length of the mallet depends on the size of the horse: the taller the horse, the longer the mallet. However, some players prefer to use a single length of mallet regardless of the height of the horse. Either way, playing horses of differing heights requires some adjustment by the rider. Variable lengths of the mallet typically range from to . The term "mallet" is used exclusively in US English; British English prefers the term "polo stick". The ball is struck with the broad sides of the mallet head rather than its round and flat tips.

Polo saddles are English-style, close contact, similar to jumping saddles; although most polo saddles lack a flap under the billets. Some players will not use a saddle blanket. The saddle has a flat seat and no knee support; the rider adopting a forward-leaning seat and closed knees dissimilar to a classical dressage seat. A breastplate is added, usually attached to the front billet. A standing martingale must be used: so, a breastplate is a necessity for safety. The tie-down is usually supported by a neck strap. Many saddles also have an overgirth. The stirrup irons are heavier than most, and the stirrup leathers are wider and thicker, for added safety when the player stands in the stirrups. The legs of the pony are wrapped with polo wraps from below the knee to the fetlock to minimize pain. Jumping (open front) or gallop boots are sometimes used along with the polo wraps for added protection. Often, these wraps match the team colours. The pony's mane is most often roached (hogged), and its tail is docked or braided so that it will not snag the rider's mallet.

Polo is ridden with double reins for greater accuracy of signals. The bit is frequently a gag bit or Pelham bit. In both cases, the gag or shank rein will be the bottom rein in the rider's hands, while the snaffle rein will be the top rein. If a gag bit is used, there will be a drop noseband in addition to the cavesson, supporting the tie-down. One of the rein sets may alternately be draw reins.

The playing field is , the area of approximately six soccer fields or 9 football fields (10 acres)., while arena polo is 96 x 46 metres. The playing field is carefully maintained with closely mowed turf providing a safe, fast playing surface. Goals are posts which are set eight yards apart, centred at each end of the field. The surface of a polo field requires careful and constant grounds maintenance to keep the surface in good playing condition. During half-time of a match, spectators are invited to go onto the field to participate in a polo tradition called "divot stamping", which was developed not only to help replace the mounds of earth (divots) that are torn up by the horses' hooves, but also to afford spectators the opportunity to walk about and socialise.

Polo is played professionally in many countries, notably Argentina, Australia, Brazil, Canada, Chile, Dominican Republic, France, Germany, Iran, India, New Zealand, Mexico, Pakistan, Jamaica, Spain, Switzerland, the United Kingdom, and the United States, and is now an active sport in 77 countries, and although its tenure as an Olympic sport was limited to 1900–1939, in 1998 the International Olympic Committee recognised it as a sport with a bona fide international governing body, the Federation of International Polo. The World Polo Championship is held every three years by the Federation of International Polo.

Polo is unique among team sports in that amateur players, often the team patrons, routinely hire and play alongside the sport's top professionals.

The most important tournaments of the world, in a clubs level, are Abierto de Tortugas, Abierto de Hurlingham and Abierto Argentino de Polo, all of them in Argentina (la "Triple Corona").

The United States Polo Association (USPA) is the governing body for polo in the U.S. The U.S. is the only country that has separate women's polo, run by the United States Women's Polo Federation.

Polo has been played in Malaysia and Singapore, both of which are former British colonies, since being introduced to Malaya in during the late 19th century. Royal Johor Polo Club was formed in 1884 and Singapore Polo Club was formed in 1886. The oldest polo club in the modern country of Malaysia is Selangor Polo Club, founded in 1902. It was largely played by royalty and the political and business elite.

Polo was played at the 2007 Southeast Asian Games and 2017 Southeast Asian Games. Nations that competed in the tournament were Indonesia, Singapore, Malaysia, Thailand and Philippines (2007) and Brunei, Malaysia, Singapore and Thailand (2017). The 2007 tournament's gold medal was won by the Malaysian team, followed by Singapore with silver and Thailand with bronze while the 2017 tournament's gold medal was won by Malaysia, followed by Thailand with silver and Brunei with bronze.

The traditional or 'free style' "Polo" or "Pulu" of Northern Pakistan is still played very avidly in its native region, and the annual Shandur Polo Festival at Shandur Top in Chitral District. It is an internationally famed event attended by many enthusiasts from all over the world. The Shandur polo ground is said to be the highest polo ground in the world, at approximately 3,734 metres,

The recent resurgence in south-east Asia has resulted in its popularity in cities such as Pattaya, Kuala Lumpur and Jakarta. In Pattaya alone, there are 3 active polo clubs: Polo Escape, Siam Polo Park and the Thai Polo and Equestrian Club. Indonesia has a polo club (Nusantara Polo Club). More recently, Janek Gazecki and Australian professional Jack "Ruki" Baillieu have organised polo matches in parks "around metropolitan Australia, backed by wealthy sponsors."

A Chinese Equestrian Association has been formed with two new clubs in China itself: the Beijing Sunny Time Polo Club, founded by Xia Yang in 2004 and the Nine Dragons Hill Polo Club in Shanghai, founded in 2005.

Polo is not widely spread in West Asia, but still counts 5 active clubs in Iran, 4 active polo clubs in the UAE, one club in Bahrain and The Royal Jordanian Polo Club, in Amman, Jordan.

Polo in Iran is governed by the Polo Federation of Iran. There are five polo clubs in Iran: Ghasr-e Firoozeh, Nowroozabad, Army Ground Forces, Kanoon-e Chogan and Nesf-e Jahan. Iran possesses some of the best grass polo fields in the region. The country currently has over 100 registered players of which approximately 15% are women. Historically, Kurdish and Persian Arabian horses were the most widely used for polo. This was probably also the case in ancient times. Today Thoroughbreds are being increasingly used alongside the Kurdish and Persian Arabian horses. Some players have also been experimenting with Anglo-Arabians. Iranians still refer to the game of polo by its original Persian name of "Chogan", which means mallet. Iranians still maintain some of the ancient rituals of the game in official polo matches.

Polo first began its Irish history in 1870 with the first official game played on Gormanstown Strand, Co. Meath. Three years later the All Ireland Polo Club was founded by Mr. Horace Rochford in the Phoenix Park. Since then the sport has continued to grow with a further seven clubs opening around the country. The sport has also been made more accessible by these clubs by the creation of more affordable training programmes such as from beginner to pro programme at Polo Wicklow.

Sagol Kangjei, discussed above, is arguably a version of polo though it can also be seen as the precursor of modern outdoor polo.






</doc>
<doc id="24444" url="https://en.wikipedia.org/wiki?curid=24444" title="Page description language">
Page description language

In digital printing, a page description language (PDL) is a computer language that describes the appearance of a printed page in a higher level than an actual output bitmap. An overlapping term is printer control language, which includes Hewlett-Packard's Printer Command Language (PCL). PostScript is one of the most noted page description languages. The markup language adaptation of the PDL is the page description markup language.

Page description languages are text (human-readable) or binary data streams, usually intermixed with text or graphics to be printed. They are distinct from graphics application programming interfaces (APIs) such as GDI and OpenGL that can be called by software to generate graphical output.

Various page description languages exist:





</doc>
<doc id="24445" url="https://en.wikipedia.org/wiki?curid=24445" title="Pope Felix I">
Pope Felix I

Pope Felix I (died 30 December 274) was the Bishop of Rome or Pope from 5 January 269 to his death in 274.

A Roman by birth, Felix was chosen as Pope on 5 January 269, in succession to Pope Dionysius, who had died on 26 December 268.

Felix was the author of an important dogmatic letter on the unity of Christ's Person. He received the emperor Aurelian's aid in settling a theological dispute between the anti-Trinitarian Paul of Samosata, who had been deprived of the bishopric Antioch by a council of bishops for heresy and the orthodox Domnus, Paul's successor. Paul refused to give way, and in 272 the emperor Aurelian was asked to decide between the rivals. He ordered the church building to be given to the bishop who was "recognized by the bishops of Italy and of the city of Rome" (Felix). See Eusebius, Hist. Ecc. vii. 30.

The text of that letter was later interpolated by a follower of Apollinaris in the interests of his sect.

The notice about Felix in the "Liber Pontificalis" ascribes to him a decree that Masses should be celebrated on the tombs of martyrs ("Hic constituit supra memorias martyrum missas celebrare"). The author of this entry was evidently alluding to the custom of celebrating Mass privately at the altars near or over the tombs of the martyrs in the crypts of the catacombs (missa ad corpus), while the solemn celebration always took place in the basilicas built over the catacombs. This practice, still in force at the end of the fourth century, dates apparently from the period when the great cemeterial basilicas were built in Rome, and owes its origin to the solemn commemoration services of martyrs, held at their tombs on the anniversary of their burial, as early as the third century. Felix probably issued no such decree, but the compiler of the "Liber Pontificalis" attributed it to him because he made no departure from the custom in force in his time.

The acts of the Council of Ephesus give Pope Felix as a martyr; but this detail, which occurs again in the biography of the pope in the "Liber Pontificalis", is unsupported by any authentic earlier evidence and is manifestly due to a confusion of names. According to the notice in the "Liber Pontificalis", Felix erected a basilica on the Via Aurelia; the same source also adds that he was buried there. The latter detail is evidently an error, for the fourth-century Roman calendar of feasts says that Pope Felix was interred in the Catacomb of Callixtus on the Via Appia. The statement of the "Liber Pontificalis" concerning the pope's martyrdom results obviously from a confusion with a Roman martyr of the same name buried on the Via Aurelia, and over whose grave a church was built. In the Roman "Feriale" or calendar of feasts, referred to above, the name of Felix occurs in the list of Roman bishops ("Depositio episcoporum"), and not in that of the martyrs.

According to the above-mentioned detail of the "Depositio episcoporum", Felix was interred in the catacomb of Callixtus on 30 December, "III Kal. Jan." (third day to the calends of January) in the Roman dating system. Saint Felix I is mentioned as Pope and Martyr, with a simple feast, on 30 May. This date, given in the "Liber Pontificalis" as that of his death (III Kal. Jun.), is probably an error which could easily occur through a transcriber writing "Jun." for "Jan." This error persisted in the General Roman Calendar until 1969 (see General Roman Calendar of 1960), by which time the mention of Saint Felix I was reduced to a commemoration in the weekday Mass by decision of Pope Pius XII (see General Roman Calendar of Pope Pius XII). Thereafter, the feast of Saint Felix I, no longer mentioned in the General Roman Calendar, is celebrated on his true day of death, 30 December, and without the qualification of "martyr".

According to more recent studies, the oldest liturgical books indicate that the saint honoured on 30 May was a little-known martyr buried on the Via Aurelia, who was mistakenly identified with Pope Felix I, an error similar to but less curious than the identification in the liturgical books, until the mid-1950s, of the martyr saint celebrated on 30 July with the antipope Felix II.




</doc>
<doc id="24446" url="https://en.wikipedia.org/wiki?curid=24446" title="Peptide bond">
Peptide bond

A peptide bond is a covalent chemical bond linking two consecutive amino acid monomers along a peptide or protein chain.

When two amino acids form a "dipeptide" through a "peptide bond" it is called condensation. In condensation, two amino acids approach each other, with the acid moiety of one coming near the amino moiety of the other. One loses a hydrogen and oxygen from its carboxyl group (COOH) and the other loses a hydrogen from its amino group (NH). This reaction produces a molecule of water (HO) and two amino acids joined by a peptide bond (-CO-NH-). The two joined amino acids are called a dipeptide.

The amide bond is synthesized when the carboxyl group of one amino acid molecule reacts with the amino group of the other amino acid molecule, causing the release of a molecule of water (HO), hence the process is a dehydration synthesis reaction (also known as a condensation reaction).

The formation of the peptide bond consumes energy, which, in living systems, is derived from ATP. Polypeptides and proteins are chains of amino acids held together by peptide bonds. Living organisms employ enzymes to produce polypeptides, and ribosomes to produce proteins. Peptides are synthesized by specific enzymes. For example, the tripeptide glutathione is synthesized in two steps from free amino acids, by two enzymes: gamma-glutamylcysteine synthetase and glutathione synthetase.

A peptide bond can be broken by hydrolysis (the addition of water). In the presence of water they will break down and release 8–16 kilojoule/mol (2–4 kcal/mol) of free energy. This process is extremely slow, with the half life at 25C of between 350 and 600 years per bond.

In living organisms, the process is normally catalyzed by enzymes known as peptidases or proteases, although there are reports of peptide bond hydrolysis caused by conformational strain as the peptide/protein folds into the native structure. This non-enzymatic process is thus not accelerated by transition state stabilization, but rather by ground state destabilization.

The wavelength of absorption A for a peptide bond is 190–230 nm (which makes it particularly susceptible to UV radiation).

Significant delocalisation of the lone pair of electrons on the nitrogen atom gives the group a partial double bond character. The partial double bond renders the amide group planar, occurring in either the cis or trans isomers. In the unfolded state of proteins, the peptide groups are free to isomerize and adopt both isomers; however, in the folded state, only a single isomer is adopted at each position (with rare exceptions). The trans form is preferred overwhelmingly in most peptide bonds (roughly 1000:1 ratio in trans:cis populations). However, X-Pro peptide groups tend to have a roughly 30:1 ratio, presumably because the symmetry between the formula_1 and formula_2 atoms of proline makes the cis and trans isomers nearly equal in energy (See figure, below).

The dihedral angle associated with the peptide group (defined by the four atoms formula_3) is denoted formula_4; formula_5 for the cis isomer (synperiplanar conformation) and formula_6 for the trans isomer (antiperiplanar conformation). Amide groups can isomerize about the C'-N bond between the cis and trans forms, albeit slowly (formula_720 seconds at room temperature). The transition states formula_8 requires that the partial double bond be broken, so that the activation energy is roughly 80 kilojoule/mol (20 kcal/mol) (See Figure below). However, the activation energy can be lowered (and the isomerization catalyzed) by changes that favor the single-bonded form, such as placing the peptide group in a hydrophobic environment or donating a hydrogen bond to the nitrogen atom of an X-Pro peptide group. Both of these mechanisms for lowering the activation energy have been observed in peptidyl prolyl isomerases (PPIases), which are naturally occurring enzymes that catalyze the cis-trans isomerization of X-Pro peptide bonds.

Conformational protein folding is usually much faster (typically 10–100 ms) than cis-trans isomerization (10–100 s). A nonnative isomer of some peptide groups can disrupt the conformational folding significantly, either slowing it or preventing it from even occurring until the native isomer is reached. However, not all peptide groups have the same effect on folding; nonnative isomers of other peptide groups may not affect folding at all.

Due to its resonance stabilization, the peptide bond is relatively unreactive under physiological conditions, even less than similar compounds such as esters. Nevertheless, peptide bonds can undergo chemical reactions, usually through an attack of an electronegative atom on the carbonyl carbon, breaking the carbonyl double bond and forming a tetrahedral intermediate. This is the pathway followed in proteolysis and, more generally, in N-O acyl exchange reactions such as those of inteins. When the functional group attacking the peptide bond is a thiol, hydroxyl or amine, the resulting molecule may be called a cyclol or, more specifically, a thiacyclol, an oxacyclol or an azacyclol, respectively.



</doc>
<doc id="24449" url="https://en.wikipedia.org/wiki?curid=24449" title="Pumping lemma">
Pumping lemma

In the theory of formal languages, the pumping lemma may refer to:



</doc>
<doc id="24451" url="https://en.wikipedia.org/wiki?curid=24451" title="Privy Council of the United Kingdom">
Privy Council of the United Kingdom

Her Majesty's Most Honourable Privy Council, usually known simply as the Privy Council, is a formal body of advisers to the Sovereign of the United Kingdom. Its membership mainly comprises senior politicians, who are current or former members of either the House of Commons or the House of Lords.

The Privy Council formally advises the sovereign on the exercise of the Royal Prerogative, and corporately (as Queen-in-Council) it issues executive instruments known as Orders in Council, which among other powers enact Acts of Parliament. The Council also holds the delegated authority to issue Orders of Council, mostly used to regulate certain public institutions. The Council advises the sovereign on the issuing of Royal Charters, which are used to grant special status to incorporated bodies, and city or borough status to local authorities. Otherwise, the Privy Council's powers have now been largely replaced by its executive committee, the Cabinet of the United Kingdom.

Certain judicial functions are also performed by the Queen-in-Council, although in practice its actual work of hearing and deciding upon cases is carried out day-to-day by the Judicial Committee of the Privy Council. The Judicial Committee consists of senior judges appointed as Privy Counsellors: predominantly Justices of the Supreme Court of the United Kingdom and senior judges from the Commonwealth. The Privy Council formerly acted as the High Court of Appeal for the entire British Empire (other than for the United Kingdom itself), and continues to hear appeals from the Crown Dependencies, the British Overseas Territories, and some independent Commonwealth states.

The Privy Council of the United Kingdom was preceded by the Privy Council of Scotland and the Privy Council of England. The key events in the formation of the modern Privy Council are given below:

In Anglo-Saxon England, Witenagemot was an early equivalent to the Privy Council of England. During the reigns of the Norman monarchs, the English Crown was advised by a royal court or "curia regis", which consisted of magnates, ecclesiastics and high officials. The body originally concerned itself with advising the sovereign on legislation, administration and justice. Later, different bodies assuming distinct functions evolved from the court. The courts of law took over the business of dispensing justice, while Parliament became the supreme legislature of the kingdom. Nevertheless, the Council retained the power to hear legal disputes, either in the first instance or on appeal. Furthermore, laws made by the sovereign on the advice of the Council, rather than on the advice of Parliament, were accepted as valid. Powerful sovereigns often used the body to circumvent the Courts and Parliament. For example, a committee of the Council—which later became the Court of the Star Chamber—was during the 15th century permitted to inflict any punishment except death, without being bound by normal court procedure. During Henry VIII's reign, the sovereign, on the advice of the Council, was allowed to enact laws by mere proclamation. The legislative pre-eminence of Parliament was not restored until after Henry VIII's death. Though the royal Council retained legislative and judicial responsibilities, it became a primarily administrative body. The Council consisted of forty members in 1553, but the sovereign relied on a smaller committee, which later evolved into the modern Cabinet.

By the end of the English Civil War, the monarchy, House of Lords, and Privy Council had been abolished. The remaining parliamentary chamber, the House of Commons, instituted a Council of State to execute laws and to direct administrative policy. The forty-one members of the Council were elected by the House of Commons; the body was headed by Oliver Cromwell, "de facto" military dictator of the nation. In 1653, however, Cromwell became Lord Protector, and the Council was reduced to between thirteen and twenty-one members, all elected by the Commons. In 1657, the Commons granted Cromwell even greater powers, some of which were reminiscent of those enjoyed by monarchs. The Council became known as the Protector's Privy Council; its members were appointed by the Lord Protector, subject to Parliament's approval.

In 1659, shortly before the restoration of the monarchy, the Protector's Council was abolished. Charles II restored the Royal Privy Council, but he, like previous Stuart monarchs, chose to rely on a small group of advisers. Under George I even more power transferred to this committee. It now began to meet in the absence of the sovereign, communicating its decisions to him after the fact.

Thus, the British Privy Council, as a whole, ceased to be a body of important confidential advisers to the sovereign; the role passed to a committee of the Council, now known as the Cabinet.

According to the "Oxford English Dictionary", the definition of the word "privy" in "Privy Council" is obsolete meaning "of or pertaining exclusively to a particular person or persons, one's own"; hence the Council is personal to the sovereign. It is closely related to the word "private", and derives from the French word "privé".

The sovereign, when acting on the Council's advice, is known as the "King-in-Council" or "Queen-in-Council". The members of the Council are collectively known as "The Lords of Her Majesty's Most Honourable Privy Council" (sometimes "The Lords and others of ..."). The chief officer of the body is the Lord President of the Council, who is the fourth highest Great Officer of State, a Cabinet member and normally, either the Leader of the House of Lords or of the House of Commons. Another important official is the Clerk, whose signature is appended to all orders made in the Council.

Both "Privy Counsellor" and "Privy Councillor" may be correctly used to refer to a member of the Council. The former, however, is preferred by the Privy Council Office, emphasising English usage of the term "Counsellor" as "one who gives counsel", as opposed to "one who is a member of a council". A Privy Counsellor is traditionally said to be ""sworn of"" the Council after being received by the sovereign.

The sovereign may appoint anyone a Privy Counsellor, but in practice appointments are made only on the advice of Her Majesty's Government. The majority of appointees are senior politicians, including Ministers of the Crown, the few most senior figures of Loyal Opposition, the Parliamentary leader of the third-largest party, a couple of the most senior figures in the devolved British governments and senior politicians from Commonwealth countries. Besides these, the Council includes a very few members of the Royal Family (usually the consort and heir apparent only), a few dozen judges from British and Commonwealth countries, a few clergy and a small number of senior civil servants.

There is no statutory limit to its membership: at January 2012, there were about 600 members; they had risen in number to over 650 by June 2015.

However, the members have no automatic right to attend all Privy Council meetings, and only some are summoned regularly to meetings (in practice at the Prime Minister's discretion).

The Church of England's three senior bishops—the Archbishop of Canterbury, the Archbishop of York and the Bishop of London—become Privy Counsellors upon appointment. Senior members of the Royal Family may also be appointed, but this is confined to the current consort and heir apparent and consort. Prince Philip is at present the most senior member by length of service, and he is the only current Privy Counsellor not appointed by the reigning monarch, having been sworn of the Council by her father. The Private Secretary to the Sovereign is always appointed a Privy Counsellor, as are the Lord Chamberlain, the Speaker of the House of Commons, and the Lord Speaker. Justices of the Supreme Court of the United Kingdom, judges of the Court of Appeal of England and Wales, senior judges of the Inner House of the Court of Session (Scotland's highest law court) and the Lord Chief Justice of Northern Ireland also join the Privy Council "ex officio".

The balance of Privy Counsellors is largely made up of politicians. The Prime Minister, Cabinet ministers and the Leader of HM Opposition are traditionally sworn of the Privy Council upon appointment. Leaders of major parties in the House of Commons, First Ministers of the devolved assemblies, some senior Ministers outside Cabinet, and on occasion other respected senior parliamentarians are appointed Privy Counsellors.

Because Privy Counsellors are bound by oath to keep matters discussed at Council meetings secret, the appointment of the Leaders of Opposition Parties as Privy Counsellors allows the Government to share confidential information with them "on Privy Council terms". This usually only happens in special circumstances, such as in matters of national security. For example, Tony Blair met Iain Duncan Smith (then Leader of HM Opposition) and Charles Kennedy (then Leader of the Liberal Democrats) "on Privy Council terms" to discuss the evidence for Iraq's weapons of mass destruction.

Although the Privy Council is primarily a British institution, officials from some other Commonwealth realms are also appointed. By 2000, the most notable instance was New Zealand, whose Prime Minister, senior politicians, Chief Justice and Court of Appeal Justices were traditionally appointed Privy Counsellors. However, appointments of New Zealand members have since been discontinued. The Prime Minister, the Speaker, the Governor-General and the Chief Justice of New Zealand are still accorded the style "Right Honourable", but without membership of the Council. Until the late 20th century, the Prime Ministers and Chief Justices of Canada and Australia were also appointed Privy Counsellors. Canada also has its own Privy Council, the Queen's Privy Council for Canada ("see" below). Prime Ministers of some other Commonwealth countries that retain the Queen as their sovereign continue to be sworn of the Council.

It was formerly regarded by the Privy Council as criminal, and possibly treasonous, to disclose the oath administered to Privy Counsellors as they take office. However, the oath was officially made public by the Blair Government in a written parliamentary answer in 1998, as follows. It had also been read out in full in the House of Lords during debate by Lord Rankeillour on 21 December 1932.

A form of this oath dates back to at least 1570.

Privy counsellors can choose to affirm their allegiance in similar terms should they prefer not to take a religious oath. At the induction ceremony, the order of precedence places Anglicans (being those of the established church) before others.

The initiation ceremony for newly appointed privy counsellors is held in private and typically requires kneeling on a stool before the sovereign and then kissing hands. According to "The Royal Encyclopaedia": "The new privy counsellor or minister will extend his or her right hand, palm upwards, and, taking the Queen's hand lightly, will kiss it with no more than a touch of the lips." The ceremony has caused difficulties for privy counsellors who advocate republicanism; Tony Benn said in his diaries that he kissed his own thumb, rather than the Queen's hand, while Jeremy Corbyn reportedly did not kneel. Not all members of the privy council go through the initiation ceremony; appointments are frequently made by an Order in Council, although it is "rare for a party leader to use such a course."

Membership is conferred for life. Formerly, the death of a monarch ("demise of the Crown") brought an immediate dissolution of the Council, as all Crown appointments automatically lapsed. By the 18th century, it was enacted that the Council would not be dissolved until up to six months after the demise of the Crown. By convention, however, the sovereign would reappoint all members of the Council after its dissolution. In practice, therefore, membership continued without a break. In 1901 the law was changed to ensure that Crown Appointments became wholly unaffected by any succession of monarch.

The sovereign however may remove an individual from the Privy Council. On 8 June 2011, the former MP Elliot Morley was expelled following his conviction on charges of false accounting in connection with the British parliamentary expenses scandal. Before this, the last individual to be expelled from the Council against his will was Sir Edgar Speyer, , who was removed on 13 December 1921 for collaborating with the enemy German Empire, during the First World War.

Individuals can choose to resign, sometimes to avoid expulsion. Three members voluntarily left the Privy Council in the 20th century: John Profumo, who resigned on 26 June 1963;, John Stonehouse, who resigned on 17 August 1976 and Jonathan Aitken, who resigned on 25 June 1997 following allegations of perjury.

So far, three Privy Counsellors have resigned in the 21st century, coincidentally all in the same year. On 4 February 2013, Chris Huhne announced that he would voluntarily leave the Privy Council after pleading guilty to perverting the course of justice. Lord Prescott stood down on 6 July 2013 in protest against delays in the introduction of press regulation, expecting others to follow. Denis MacShane resigned on 9 October 2013 before a High Court hearing at which he pleaded guilty of false accounting and was subsequently imprisoned.

Meetings of the Privy Council are normally held once each month wherever the sovereign may be in residence at the time. The quorum, according to the Privy Council Office, is three, though some statutes provide for other quorums (for example, section 35 of the Opticians Act 1989 provides for a lower quorum of two).

The sovereign attends the meeting, though his or her place may be taken by two or more Counsellors of State. Under the Regency Acts 1937 to 1953, Counsellors of State may be chosen from among the sovereign's spouse and the four individuals next in the line of succession who are over 21 years of age (18 for the heir to the throne). Customarily the sovereign remains standing at meetings of the Privy Council, so that no other members may sit down, thereby keeping meetings short. The Lord President reads out a list of Orders to be made, and the sovereign merely says "Approved".

Few Privy Counsellors are required to attend regularly. The settled practice is that day-to-day meetings of the Council are attended by four Privy Counsellors, usually the relevant Minister to the matters pertaining. The Cabinet Minister holding the office of Lord President of the Council, currently the Rt Hon. Andrea Leadsom , invariably presides. Under Britain's modern conventions of parliamentary government and constitutional monarchy, every order made in Council is drafted by a Government Department and has already been approved by the Minister responsible—thus actions taken by the Queen-in-Council are formalities required for validation of each measure.

Full meetings of the Privy Council are held only when the reigning sovereign announces his or her own engagement (which last happened on 23 November 1839, in the reign of Queen Victoria); or when there is a demise of the Crown, either by the death or abdication of the monarch. A full meeting of the Privy Council was also held on 6 February 1811, when George, Prince of Wales was sworn in as Prince Regent by Act of Parliament. The current statutes regulating the establishment of a regency in the case of minority or incapacity of the sovereign also require any regents to swear their oaths before the Privy Council.

In the case of a demise of the Crown, the Privy Council—together with the Lords Spiritual, the Lords Temporal, the Lord Mayor and Aldermen of the City of London as well as representatives of Commonwealth realms—makes a proclamation declaring the accession of the new sovereign and receives an oath from the new monarch relating to the security of the Church of Scotland, as required by law. It is also customary for the new sovereign to make an allocution to the Privy Council on that occasion, and this Sovereign's Speech is formally published in "The London Gazette". Any such Special Assembly of the Privy Council, convened to proclaim the accession of a new sovereign and witness the monarch's statutory oath, is known as an Accession Council. The last such meetings were held on 6 and 8 February 1952: as Elizabeth II was abroad when the last demise of the Crown took place, the Accession Council met twice, once to proclaim the sovereign (meeting of 6 February 1952), and then again after the new queen had returned to Britain, to receive from her the oath required by statute (meeting of 8 February 1952).

The sovereign exercises executive authority by making Orders in Council upon the advice of the Privy Council. Orders-in-Council, which are drafted by the government rather than by the sovereign, are secondary legislation and are used to make government regulations and to make government appointments. Furthermore, Orders-in-Council are used to grant Royal Assent for Measures of the National Assembly for Wales, and laws passed by the legislatures of British Crown dependencies.

Distinct from Orders-in-Council are Orders of Council: the former are issued by the sovereign upon the advice of the Privy Council, whereas the latter are made by members of the Privy Council without requiring the sovereign's approval. They are issued under the specific authority of Acts of Parliament, and most commonly are used for the regulation of public institutions.

The sovereign also grants Royal Charters on the advice of the Privy Council. Charters bestow special status to incorporated bodies; they are used to grant "chartered" status to certain professional, educational or charitable bodies, and sometimes also city and borough status to towns. The Privy Council therefore deals with a wide range of matters, which also includes university and livery company statutes, the grant of academic degree-awarding powers (e.g. taught degree awarding powers (TDAP)), churchyards, coinage and dates of bank holidays.

The Privy Council comprises a number of Standing Committees:


The Baronetage Committee was established by a 1910 Order in Council, during Edward VII's reign, to scrutinise all succession claims (and thus reject doubtful ones) to be placed on the Roll of Baronets.

The Committee for the Affairs of Jersey and Guernsey recommends approval of Channel Islands legislation.

The Committee for the purposes of the Crown Office Act 1877 consists of the Lord Chancellor and Lord Privy Seal as well as a Secretary of State. The Committee which last met in 1988 is concerned with the design and usage of wafer seals.

The Scottish Universities Committee considers proposed amendments to the statutes of Scotland's four ancient universities. The Universities Committee, which last met in 1995, considers petitions against statutes
made by Oxford and Cambridge Universities and their colleges.

The Judicial Committee of the Privy Council, consists of senior judges who are Privy Counsellors. The decision of the Committee is presented in the form of "advice" to the monarch, but in practice it is always followed by the sovereign (as Crown-in-Council), who formally approves the recommendation of the Judicial Committee.

Within the United Kingdom, the Judicial Committee hears appeals from ecclesiastical courts, the Court of Admiralty of the Cinque Ports, prize courts and the Disciplinary Committee of the Royal College of Veterinary Surgeons, appeals against schemes of the Church Commissioners and appeals under certain Acts of Parliament (e.g., the House of Commons Disqualification Act 1975). The Crown-in-Council was formerly the Supreme Appeal Court for the entire British Empire, but a number of Commonwealth countries have now abolished the right to such appeals. The Judicial Committee continues to hear appeals from several Commonwealth countries, from British Overseas Territories, Sovereign Base Areas and Crown dependencies. The Judicial Committee had direct jurisdiction in cases relating to the Scotland Act 1998, the Government of Wales Act 1998 and the Northern Ireland Act 1998, but this was transferred to the new Supreme Court of the United Kingdom in 2009.

In addition to the Standing Committees, "ad hoc" Committees are notionally set up to consider and report on Petitions for Royal Charters of Incorporation and to approve changes to the bye-laws of bodies created by Royal Charter.

Committees of Privy Counsellors are occasionally established to examine specific issues. Such Committees are independent of the
Privy Council Office and therefore do not report directly to the Lord President of the Council. Examples of such Committees include:

The Civil Service is formally governed by Privy Council Orders, as an exercise of the Royal prerogative. One such order implemented HM Government's ban of GCHQ staff from joining a Trade Union. Another, the Civil Service (Amendment) Order in Council 1997, permitted the Prime Minister to grant up to three political advisers management authority over some Civil Servants.

In the 1960s, the Privy Council made an order to evict the 2,000 inhabitants of the 65-island Chagos Archipelago in the Indian Ocean, in preparation for the establishment of a joint United States–United Kingdom military base on the largest outlying island, Diego Garcia, some distant. In 2000 the Court of Appeal ruled the 1971 Immigration Ordinance preventing resettlement unlawful. In 2004, the Privy Council, under Jack Straw's tenure, overturned the ruling. In 2006 the High Court of Justice found the Privy Council's decision to be unlawful. Sir Sydney Kentridge described the treatment of the Chagossians as "outrageous, unlawful and a breach of accepted moral standards": Justice Kentridge stated that there was no known precedent "for the lawful use of prerogative powers to remove or exclude an entire population of British subjects from their homes and place of birth", and the Court of Appeal were persuaded by this argument, but the Law Lords (at that time the UK's highest law court) found its decision to be flawed and overturned the ruling by a 3–2 decision thereby upholding the terms of the Ordinance.

The Privy Council as a whole is termed "The Most Honourable" whilst its members individually, the Privy Counsellors, are entitled to be styled "The Right Honourable".

Each Privy Counsellor has the right of personal access to the sovereign. Peers were considered to enjoy this right individually; members of the House of Commons possess the right collectively. In each case, personal access may only be used to tender advice on public affairs.

Only Privy Counsellors can signify royal consent to the examination of a Bill affecting the rights of the Crown.

Members of the Privy Council are privileged to be given advance notice of any prime ministerial decision to commit HM Armed Forces in enemy action.

Privy Counsellors have the right to sit on the steps of the Sovereign's Throne in the Chamber of the House of Lords during debates, a privilege which was shared with heirs apparent of those hereditary peers who were to become members of the House of Lords before Labour's partial Reform of the Lords in 1999, diocesan bishops of the Church of England yet to be Lords Spiritual, retired bishops who formerly sat in the House of Lords, the Dean of Westminster, Peers of Ireland, the Clerk of the Crown in Chancery, and the Gentleman Usher of the Black Rod. While Privy Counsellors have the right to sit on the steps of the Sovereign's Throne they do so only as observers and are not allowed to participate in any of the workings of the House of Lords. Nowadays this privilege is rarely exercised. A notable recent instance of the exercising of this privilege was used by the Prime Minister, Theresa May, who watched the opening of the debate of the European Union (Notification of Withdrawal) Bill 2017 in the House of Lords.

Privy Counsellors are accorded a formal rank of precedence, if not already having a higher one. At the beginning of each new Parliament, and at the discretion of the Speaker, those members of the House of Commons who are Privy Counsellors usually take the oath of allegiance before all other members except the Speaker and the Father of the House, who is the most senior member of the House. Should a Privy Counsellor rise to speak in the House of Commons at the same time as another Honourable Member, the Speaker usually gives priority to the "Right Honourable" Member. This parliamentary custom, however, was discouraged under New Labour after 1998, despite the Government not being supposed to exert influence over the Speaker.

All those sworn of the Privy Council below the rank of marquess, including barons, viscounts and earls, are accorded the style "The Right Honourable"; privy counsellor non-royal dukes are styled "The Most Noble" and marquesses, "The Most Honourable". Modern custom as recommended by "Debrett's" is to use the post-nominal letters "PC" in a social style of address for peers who are Privy Counsellors. For commoners, "The Right Honourable" is sufficient identification of their status as a Privy Counsellor and they do not use the post-nominal letters "PC". The Ministry of Justice revises current practice of this convention from time to time.

The Privy Council is one of the four principal councils of the sovereign. The other three are the courts of law, the "Commune Concilium" (Common Council, or Parliament) and the "Magnum Concilium" (Great Council, or the assembly of all the Peers of the Realm). All are still in existence, or at least have never been formally abolished, but the "Magnum Concilium" has not been summoned since 1640 and was considered defunct even then.

Several other Privy Councils have advised the sovereign. England and Scotland once had separate Privy Councils (the Privy Council of England and Privy Council of Scotland). The Acts of Union 1707 united the two countries into the Kingdom of Great Britain and in 1708 the Parliament of Great Britain abolished the Privy Council of Scotland. Thereafter there was one Privy Council of Great Britain sitting in London. Ireland, on the other hand, continued to have a separate Privy Council even after the Act of Union 1800. The Privy Council of Ireland was abolished in 1922, when the southern part of Ireland separated from the United Kingdom; it was succeeded by the Privy Council of Northern Ireland, which became dormant after the suspension of the Parliament of Northern Ireland in 1972. No further appointments have been made since then, and only three appointees were still living as of November 2017.

Canada has had its own Privy Council—the Queen's Privy Council for Canada—since 1867. While the Canadian Privy Council is specifically "for Canada", the Privy Council discussed above is not "for the United Kingdom"; in order to clarify the ambiguity where necessary, the latter was traditionally referred to as the Imperial Privy Council. Equivalent organs of state in other Commonwealth realms, such as Australia and New Zealand, are called Executive Councils.




</doc>
<doc id="24452" url="https://en.wikipedia.org/wiki?curid=24452" title="Prime Minister of India">
Prime Minister of India

The Prime Minister of India is the leader of the executive of the Government of India. The prime minister is also the chief adviser to the President of India and head of the Council of Ministers. They can be a member of any of the two houses of the Parliament of India — the Lok Sabha (House of the People) and the Rajya Sabha (Council of the States) — but has to be a member of the political party or coalition, having a majority in the Lok Sabha.

The prime minister is the senior-most member of cabinet in the executive of government in a parliamentary system. The prime minister selects and can dismiss members of the cabinet; allocates posts to members within the government; and is the presiding member and chairperson of the cabinet.

The union cabinet headed by the prime minister is appointed by the President of India to assist the latter in the administration of the affairs of the executive. Union cabinet is collectively responsible to the Lok Sabha as per of the Constitution of India. The prime minister has to enjoy the confidence of a majority in the Lok Sabha and shall resign if they are unable to prove majority when instructed by the president.

India follows a parliamentary system in which the prime minister is the presiding head of the government and chief of the executive of the government. In such systems, the head of state, or, the head of state's official representative (i.e., the monarch, president, or governor-general) usually holds a purely ceremonial position and acts — on most matters — only on the advice of the prime minister.

The prime minister — if they are not already — shall become a member of parliament within six months of beginning his/her tenure. A prime minister is expected to work with other central ministers to ensure the passage of bills by the parliament.

Since 1947, there have been 14 different prime ministers. The first few decades after 1947 saw the Indian National Congress' (INC) almost complete domination over the political map of India. India's first prime minister — Jawaharlal Nehru — took oath on 15 August 1947. Nehru went on to serve as prime minister for 17 consecutive years, winning four general elections in the process. His tenure ended in May 1964, on his death. After the death of Nehru, Lal Bahadur Shastri — a former home minister and a leader of the Congress party — ascended to the position of prime minister. Shastri's tenure saw the Indo-Pakistani War of 1965. Shashtri subsequently died of a reported heart attack in Tashkent, after signing the Tashkent Declaration.

After Shastri, Indira Gandhi — Nehru's daughter — was elected as the country's first woman prime minister. Indira's first term in office lasted 11 years, in which she took steps such as nationalization of banks; end of allowances and political posts, which were received by members of the royal families of the erstwhile princely states of British India. In addition, events such as the Indo-Pakistani War of 1971; the establishment of a sovereign Bangladesh; accession of Sikkim to India, through a referendum in 1975; and India's first nuclear test in Pokhran occurred during Indira's first term. In 1975, President Fakhruddin Ali Ahmed — on Indira's advice — imposed a state of emergency, therefore, bestowing the government with the power to rule by decree, the period is known for human right violations.

After widespread protests, the emergency was lifted in 1977, and a general election was to be held. All of the political parties of the opposition — after the conclusion of the emergency — fought together against the Congress, under the umbrella of the Janata Party, in the general election of 1977, and were successful in defeating the Congress. Subsequently, Morarji Desai — a former deputy prime minister — became the first non-Congress prime minister of the country. The government of Prime Minister Desai was composed of groups with opposite ideologies, in which unity and coordination were difficult to maintain. Ultimately, after two and a half years as PM; on 28 July 1979, Morarji tendered his resignation to the president; and his government fell. Thereafter, Charan Singh — a deputy prime minister in Desai's cabinet — with outside, conditional support from Congress, proved a majority in Lok Sabha and took oath as prime minister. However, Congress pulled its support shortly after, and Charan Singh had to resign; he had a tenure of 5 months, the shortest in the history of the office.

In 1980, after a three-year absence, the Congress returned to power with an absolute majority. Indira Gandhi was elected prime minister a second time. During her second tenure, Operation Blue Star — an Indian Army operation inside the Golden Temple, the most sacred site in Sikhism — was conducted, resulting in reportedly thousands of deaths. Subsequently, on 31 October 1984, Indira was shot dead by Satwant Singh and Beant Singh — two of her bodyguards — in the garden of her residence at 1, Safdarjung Road, New Delhi.

After Indira, the Rajiv Gandhi — her eldest son and 40 years old at the time — was sworn in on the evening of 31 October 1984, becoming the youngest person ever to hold the office of prime minister. Rajiv immediately called for a general election. In the subsequent general election, the Congress secured an absolute majority, winning 401 of 552 seats in the Lok Sabha, the maximum number received by any party in the history of India. Vishwanath Pratap Singh — first finance minister and then later defence minister in Rajiv's cabinet — uncovered irregularities, in what became to be known as the Bofors scandal, during his stint at the Ministry of Defence; V. P. Singh was subsequently expelled from Congress and formed the Janata Dal and — with the help of several anti-Congress parties — also formed the National Front, a coalition of many political parties.

In the general election of 1989, the National Front — with outside support from the Bharatiya Janata Party (BJP) and the Left Front — came to power. V. P. Singh was elected prime minister. During a tenure of less than a year, V. P. Singh and his government accepted the Mandal Commission's recommendations. V. P. Singh's tenure came to an end after he ordered the arrest of BJP leader Lal Krishna Advani, as a result, BJP withdrew its outside support to the government, V. P. Singh lost the subsequent vote-of-no-confidence 146-320 and had to resign. After V. P. Singh's resignation, Chandra Shekhar — home minister in V. P. Singh's cabinet — along with 64 members of parliament (MPs) floated the Samajwadi Janata Party (Rashtriya) (SJP(R)), and proved a majority in the Lok Sabha with support from Congress. But his premiership did not last long, Congress proceeded to withdraw its support; Chandra Shekhar's government fell as a result, and new elections were announced.

In the general election of 1991, Congress — under the leadership of P. V. Narasimha Rao — formed a minority government; Rao became the first PM of South Indian origin. After the dissolution of the Soviet Union, India was on the brink of bankruptcy, so, Rao took steps to liberalise the economy, and appointed Manmohan Singh — an economist and a former governor of the Reserve Bank of India — as finance minister. Rao and Manmohan then took various steps to liberalise the economy, these resulted in an unprecedented economic growth in India. His premiership, however, was also a witness to the demolition of the Babri Masjid, which resulted in the death of about 2,000 people. Rao, however, did complete five continuous years in office, becoming the first prime minister outside of the Nehru—Gandhi family to do so.

After the end of Rao's tenure in May 1996, the nation saw four prime ministers in a span of three years, "", two tenures of Atal Bihari Vajpayee; one tenure of H. D. Deve Gowda from 1 June 1996 to 21 April 1997; and one tenure of I. K. Gujral from 21 April 1997 to 19 March 1998. The government of Prime Minister Vajpayee — elected in 1998 — took some concrete steps. In May 1998 — after a month in power — the government announced the conduct of five underground nuclear explosions in Pokhran. In response to these tests, many western countries, including the United States, imposed economic sanctions on India, but, due to the support received from Russia, France, the Gulf countries and some other nations, the sanctions — were largely — not considered successful. A few months later in response to the Indian nuclear tests, Pakistan also conducted nuclear tests. Given the deteriorating situation between the two countries, the governments tried to improve bilateral relations. In February 1999, the India and Pakistan signed the Lahore Declaration, in which the two countries announced their intention to annul mutual enmity, increase trade and use their nuclear capabilities for peaceful purposes. In May 1999, All India Anna Dravida Munnetra Kazhagam (AIADMK) withdrew from the ruling National Democratic Alliance (NDA) coalition; Vajpayee's government — hence — became a caretaker one after losing a motion-of-no-confidence 269-270, this coincided with the Kargil War with Pakistan. In the subsequent October 1999 election, the BJP-led NDA and its affiliated parties secured a comfortable majority in the Lok Sabha, winning 299 of 543 seats in the lower house.

Vajpayee continued the process of economic liberalization during his reign, resulting in economic growth. In addition to the development of infrastructure and basic facilities, the government took several steps to improve the infrastructure of the country, such as, the National Highways Development Project (NHDP) and the "Pradhan Mantri Gram Sadak Yojana" (PMGSY; IAST: ; Prime Minister Rural Road Scheme), for the development of roads. But during his reign, the 2002 Gujarat communal riots in the state of Gujarat took place; resulting in the death of about 2,000 deaths. Prime Minister Vajpayee's tenure as prime minister came to an end in May 2004, making him the first non-Congress PM to complete a full five-year tenure.

In the 2004 election, the Congress emerged as the largest party in a hung parliament; Congress-led United Progressive Alliance (UPA) — with outside support from the Left Front, the Samajwadi Party (SP) and Bahujan Samaj Party (BSP) among others — proved a majority in the Lok Sabha, and Manmohan Singh was elected prime minister; becoming the first Sikh prime minister of the nation. During his tenure, the country retained the economic momentum gained during Prime Minister Vajpayee's tenure. Apart from this, the government succeeded in getting the "National Rural Employment Guarantee Act, 2005", and the "Right to Information Act, 2005" passed in the parliament. Further, the government strengthened India's relations with nations like Afghanistan; Russia; and the United States, culminating with the ratification of India–United States Civil Nuclear Agreement near the end of Manmohan's first term. At the same time, the November 2008 Mumbai terrorist attacks also happened during Manmohan's first term in office. In the general election of 2009, the mandate of UPA increased. Prime Minister Manmohan's second term, however, was surrounded by accusations of high-level scandals and corruption. Manmohan resigned as prime minister on 17 May 2014, after Congress' defeat in the 2014 general election.

In the general election of 2014, the BJP-led NDA got an absolute majority, winning 336 out of 543 Lok Sabha seats; the BJP itself became the first party since 1984 to get a majority in the Lok Sabha. Narendra Modi — the Chief Minister of Gujarat — was elected prime minister; becoming the first prime minister to have been born in an independent India.

The Constitution envisions a scheme of affairs in which the President of India is the head of state; in terms of Article 53 with office of the prime minister being the head of Council of Ministers to assist and advise the president in the discharge of his/her constitutional functions. To quote, Article 53, and 75 provide as under;

Like most parliamentary democracies, the president's duties are mostly ceremonial as long as the constitution and the rule of law is obeyed by the cabinet and the legislature. The Prime Minister of India is the head of government and has the responsibility for executive power. The president's constitutional duty is to preserve, protect and defend the Constitution and the law per . In the constitution of India, the prime minister is mentioned in only four of its articles (articles 74, 75, 78 and 366), however he/she plays a crucial role in the Government of India by enjoying majority in the Lok Sabha.

According to Article 84 of the Constitution of India, which sets the principle qualification for member of Parliament, and Article 75 of the Constitution of India, which sets the qualifications for the minister in the Union Council of Minister, and the argument that the position of prime minister has been described as "primus inter pares" (the first among equals), A prime minister must:


If however a candidate is elected as the prime minister they must vacate their post from any private or government company and may take up the post only on completion of their term.

The prime minister is required to make and subscribe in the presence of President of India before entering office, the oath of office and secrecy, as per the Third Schedule of the Constitution of India.

Oath of office:
Oath of secrecy:
The prime minister serves on 'the pleasure of the president', hence, a prime minister may remain in office indefinitely, so long as the president has confidence in him/her. However, a prime minister must have the confidence of Lok Sabha, the lower house of the Parliament of India.

However, the term of a prime minister can end before the end of a Lok Sabha's term, if a simple majority of its members no longer have confidence in him/her, this is called a vote-of-no-confidence. Three prime ministers, I. K. Gujral , H. D. Deve Gowda and Atal Bihari Vajpayee have been voted out from office this way. In addition, a prime minister can also resign from office; Morarji Desai was the first prime minister to resign while in office.

Upon ceasing to possess the requisite qualifications to be a member of Parliament subject to the "Representation of the People Act, 1951".

The prime minister leads the functioning and exercise of authority of the Government of India. The President of India — subject to eligibility — invites a person who is commanding support of majority members of Lok Sabha to form the Government of India — also known as the central government or Union government — at the national level and exercise its powers. In practice the prime minister nominates the members of their council of ministers to the president. They also work upon to decide a core group of ministers (known as the cabinet), as in charge of the important functions and ministries of the Government of India.

The prime minister is responsible for aiding and advising the president in distribution of work of the government to various ministries and offices and in terms of the "Government of India (Allocation of Business) Rules, 1961". The coordinating work is generally allocated to the Cabinet Secretariat. While the work of the government is generally divided into various Ministries, the prime minister may retain certain portfolios if they are not allocated to any member of the cabinet.

The prime minister — in consultation with the cabinet — schedules and attends the sessions of the houses of parliament and is required to answer the question from the Members of Parliament to them as the in-charge of the portfolios in the capacity as Prime Minister of India.

Some specific ministries/department are not allocated to anyone in the cabinet but the prime minister themself. The prime minister is usually always in charge/head of:

The prime minister represents the country in various delegations, high level meetings and international organisations that require the attendance of the highest government office, and also addresses to the nation on various issues of national or other importance.

Per of the constitution, the official communication between the union cabinet and the president are through the prime minister. Other wise constitution recognises the prime minister as a member of the union cabinet only outside the sphere of union cabinet.

The prime minister recommends to the president — among others — names for the appointment of:


As the chairperson of Appointments Committee of the Cabinet (ACC), the prime minister — on the non-binding advice of the Cabinet Secretary of India led-Senior Selection Board (SSB) — decides the postings of top civil servants, such as, secretaries, additional secretaries and joint secretaries in the Government of India. Further, in the same capacity, the PM decides the assignments of top military personnel such as the Chief of the Army Staff, Chief of the Air Staff, Chief of the Naval Staff and commanders of operational and training commands. In addition, the ACC also decides the posting of Indian Police Service officers — the All India Service for policing, which staffs most of the higher level law enforcement positions at federal and state level — in the Government of India.

Also, as the Minister of Personnel, Public Grievances and Pensions, the PM also exercises control over the Indian Administrative Service (IAS), the country's premier civil service, which staffs most of the senior civil service positions; the Public Enterprises Selection Board (PESB); and the Central Bureau of Investigation (CBI), except for the selection of its director, who is chosen by a committee of: (a) the prime minister, as chairperson; (b) the leader of the opposition in Lok Sabha; (c) and the chief justice.

Unlike most other countries, the prime minister does not have much influence over the selection of judges, that is done by a collegium of judges consisting of the Chief Justice of India, four senior-most judges of the Supreme Court of India and the chief justice — or the senior-most judge — of the concerned state high court. The executive as a whole, however, has the right to send back a recommended name to the collegium for reconsideration, this, however, is not a full veto power, and the collegium can still put forward rejected name.

The prime minister acts as the leader of the house of the chamber of parliament — generally the Lok Sabha — he/she belongs to. In this role, the prime minister is tasked with representing the executive in the legislature, he/she is also expected to announce important legislation, and is further expected to respond to the opposition's concerns. Article 85 of the Indian constitution confers the president with the power to convene and end extraordinary sessions of the parliament, this power, however, is exercised only on the advise of prime minister and his/her council, so, in practice, the prime minister does exercise some control over affairs of the parliament.

Article 75 of the Constitution of India confers the parliament with the power to decide the remuneration and other benefits of the prime minister and other ministers are to be decided by the Parliament. and is renewed from time to time. The original remuneration for prime minister and other ministers were specified in the Part B of the second schedule of the constitution, which was later removed by an amendment.

In 2010, the prime minister's office reported that he/she does not receive a formal salary, but was only entitled to monthly allowances. That same year "The Economist" reported that, on a purchasing power parity basis, the prime minister received an equivalent of $4106 per year. As a percentage of the country's per-capita GDP (Gross Domestic Product), this is the lowest of all countries "The Economist" surveyed.

The 7, Lok Kalyan Marg — previously called the 7, Race Course Road — in New Delhi, serves as the official place of residence for the Prime Minister of India. For ground travel, the prime minister uses a highly modified, armoured version of a Range Rover, while for air travel, Boeing 777-300ERs — designated by the call sign Air India One (AI-1 or AIC001), and maintained by the Indian Air Force — are used. The Special Protection Group (SPG) is charged with protecting the sitting prime minister and his/her family.

The Prime Minister's Office (PMO) acts as the principal workplace of the prime minister. The office is located at South Block, and is a 20-room complex, and has the Cabinet Secretariat, the Ministry of Defence and the Ministry of External Affairs adjacent to it. The office is headed by the Principal Secretary to the Prime Minister, generally a former civil servant either from the Indian Administrative Service (IAS) or the Indian Foreign Service (IFS).

Former prime ministers are entitled to a bungalow, former prime ministers are also entitled the same facilities as those given to a serving cabinet minister, this includes a fourteen-member secretarial staff, for a period of five years; reimbursement of office expenses; six domestic executive-class air tickets each year and; and security cover from the Special Protection Group. In addition, former prime ministers rank seventh on the Indian order of precedence, equivalent to chief ministers of states (within their respective states) and cabinet ministers As a former member of the parliament, the prime minister receives a minimum pension of per month, plus — if he/she served as an MP for more than five years — for every year served.

The prime minister acts as the presides over various funds.

The National Defence Fund (NDF) was set up the Indian government in 1962, in the aftermath of 1962 Sino-Indian War. The prime minister acts as chairperson of the fund's executive committee, while, the ministers of defence, finance and home act as the members of the executive committee, the finance minister also acts the treasurer of the committee. The secretary of the fund's executive committee is a joint secretary in the Prime Minister's Office, dealing with the subject of NDF. The fund — according to its website — is “entirely dependent on voluntary contributions from the public and does not get any budgetary support.”. Donations to the fund are 100% tax-deductible under section 80G of the "Income Tax Act, 1961".

The Prime Minister's National Relief Fund (PMNRF) was set up by the first Prime Minister of India — Jawaharlal Nehru — in 1948, to assist displaced people from Pakistan. The fund, now, is primarily used to assist the families of those who are killed during natural disasters such as earthquakes, cyclones and flood and secondarily to reimburse medical expenses of people with chronic and deadly diseases. Donations to the PMNRF are 100% tax-deductible under section 80G of the "Income Tax Act, 1961".

The post of Deputy Prime Minister of India is not technically a constitutional post, nor is there any mention of it in an Act of the parliament. But historically, on various occasions, different governments have assigned one of their senior ministers as the 'deputy prime minister'. There is neither constitutional requirement for filling the post of deputy PM, nor does the post provide any kind of special powers. Typically, senior cabinet ministers like the finance minister or the home minister are appointed as deputy prime minister. The post is considered to be the senior most in the cabinet after the prime minister and represents the government in his/her absence. Generally, deputy prime ministers have been appointed to strengthen the coalition governments. The first holder of this post was Vallabhbhai Patel, who was also the home minister in Jawaharlal Nehru's cabinet.




</doc>
<doc id="24454" url="https://en.wikipedia.org/wiki?curid=24454" title="Paraphyly">
Paraphyly

In taxonomy, a group is paraphyletic if it consists of the group's last common ancestor and all descendants of that ancestor excluding a few—typically only one or two—monophyletic subgroups. The group is said to be paraphyletic "with respect to" the excluded subgroups. The arrangement of the members of a paraphyletic group is called a paraphyly. The term is commonly used in phylogenetics (a subfield of biology) and in linguistics. 

The term was coined to apply to well-known taxa like Reptilia (reptiles) which, as commonly named and traditionally defined, is paraphyletic with respect to mammals and birds. Reptilia contains the last common ancestor of reptiles and all descendants of that ancestor—including all extant reptiles as well as the extinct synapsids—except for mammals and birds. Other commonly recognized paraphyletic groups include fish, monkeys, and lizards.

If many subgroups are missing from the named group, it is said to be polyparaphyletic. A paraphyletic group cannot be a clade, or monophyletic group, which is any group of species that includes only a common ancestor and all of its descendants. Formally, a paraphyletic group is the relative complement of one or more subclades within a clade: removing one or more subclades leaves a paraphyletic group.

The term "paraphyly", or "paraphyletic", derives from the two Ancient Greek words (), meaning "beside, near", and (), meaning "genus, species", and refers to the situation in which one or several monophyletic subgroups of organisms (e.g., genera, species) are "left apart" from all other descendants of a unique common ancestor.

Conversely, the term "monophyly", or "monophyletic", builds on the Ancient Greek prefix (), meaning "alone, only, unique", and refers to the fact that a monophyletic group includes organisms consisting of "all" the descendants of a "unique" common ancestor.

By comparison, the term "polyphyly", or "polyphyletic", uses the ancient greek prefix (), meaning "many, a lot of", and refers to the fact that a polyphyletic group includes organisms arising from "multiple" ancestral sources.

Groups that include all the descendants of a common ancestor are said to be "monophyletic". A paraphyletic group is a monophyletic group from which one or more subsidiary clades (monophyletic groups) are excluded to form a separate group. Ereshefsky has argued that paraphyletic taxa are the result of anagenesis in the excluded group or groups.

A group whose identifying features evolved convergently in two or more lineages is "polyphyletic" (Greek πολύς ["polys"], "many"). More broadly, any taxon that is not paraphyletic or monophyletic can be called polyphyletic.

These terms were developed during the debates of the 1960s and 1970s accompanying the rise of cladistics.

The prokaryotes (single-celled life forms without cell nuclei), because they exclude the eukaryotes, a descendant group. Bacteria and Archaea are prokaryotes, but archaea and eukaryotes share a common ancestor that is not ancestral to the bacteria. The prokaryote/eukaryote distinction was proposed by Edouard Chatton in 1937 and was generally accepted after being adopted by Roger Stanier and C.B. van Niel in 1962. The botanical code (the ICBN, now the ICN) abandoned consideration of bacterial nomenclature in 1975; currently, prokaryotic nomenclature is regulated under the ICNB with a starting date of January 1, 1980 (in contrast to a 1753 start date under the ICBN/ICN).

Among plants, dicotyledons (in the traditional sense) are paraphyletic because the group excludes monocotyledons. "Dicotyledon" has not been used as an ICBN classification for decades, but is allowed as a synonym of Magnoliopsida. Phylogenetic analysis indicates that the monocots are a development from a dicot ancestor. Excluding monocots from the dicots makes the latter a paraphyletic group.

Among animals, several familiar groups are not in fact clades. The order Artiodactyla (even-toed ungulates), because it excludes Cetaceans (whales, dolphins, etc.). In the ICZN Code, the two taxa are orders of equal rank. Molecular studies, however, have shown that the Cetacea descend from Artiodactyl ancestors, although the precise phylogeny within the order remains uncertain. Without the Cetacean descendants the Artiodactyls must be paraphyletic.
The class Reptilia "as traditionally defined", is paraphyletic because it excludes birds (class Aves) and mammals. In the ICZN Code, the three taxa are classes of equal rank. However, mammals hail from the synapsids (which were once described as "mammal-like reptiles") and birds are descended from the dinosaurs (a group of Diapsida), both of which are reptiles. Alternatively, reptiles are paraphyletic because they gave rise to (only) birds. Birds and reptiles together make Sauropsids.
Osteichthyes, bony fish, are paraphyletic when they include only Actinopterygii (ray-finned fish) and Sarcopterygii (lungfish, etc.), excluding tetrapods; more recently, Osteichthyes is treated as a clade, including the tetrapods.
The wasps are paraphyletic, consisting of the narrow-waisted Apocrita without the ants and bees. The sawflies (Symphyta) are similarly paraphyletic, forming all of the Hymenoptera except for the Apocrita, a clade deep within the sawfly tree.
Crustaceans are not a clade because the Hexapoda (insects) are excluded. The modern clade that spans all of them is the Tetraconata.

Species have a special status in systematics as being an observable feature of nature itself and as the basic unit of classification. The phylogenetic species concept requires species to be monophyletic, but paraphyletic species are common in nature. Paraphyly is common in speciation, whereby a mother species (a paraspecies) gives rise to a daughter species without itself becoming extinct. Research indicates as many as 20 percent of all animal species and between 20 and 50 percent of plant species are paraphyletic. Accounting for these facts, some taxonomists argue that paraphyly is a trait of nature that should be acknowledged at higher taxonomic levels.

When the appearance of significant traits has led a subclade on an evolutionary path very divergent from that of a more inclusive clade, it often makes sense to study the paraphyletic group that remains without considering the larger clade. For example, the Neogene evolution of the Artiodactyla (even-toed ungulates, like deer) has taken place in an environment so different from that of the Cetacea (whales, dolphins, and porpoises) that the Artiodactyla are often studied in isolation even though the cetaceans are a descendant group. The prokaryote group is another example; it is paraphyletic because it excludes many of its descendant organisms (the eukaryotes), but it is very useful because it has a clearly defined and significant distinction (absence of a cell nucleus, a plesiomorphy) from its excluded descendants.

Also, paraphyletic groups are involved in evolutionary transitions, the development of the first tetrapods from their ancestors for example. Any name given to these ancestors to distinguish them from tetrapods—"fish", for example—necessarily picks out a paraphyletic group, because the descendant tetrapods are not included.

The term "evolutionary grade" is sometimes used for paraphyletic groups.

Viviparity, the production of offspring without the laying of a fertilized egg, developed independently in the lineages that led to humans ("Homo sapiens") and southern water skinks ("Eulampus tympanum", a kind of lizard). Put another way, at least one of the lineages that led to these species from their last common ancestor contains nonviviparous animals, the pelycosaurs ancestral to mammals; vivipary appeared subsequently in the mammal lineage.

Independently-developed traits like these cannot be used to distinguish paraphyletic groups because paraphyly requires the excluded groups to be monophyletic. Pelycosaurs were descended from the last common ancestor of skinks and humans, so vivipary could be paraphyletic only if the pelycosaurs were part of an excluded monophyletic group. Because this group is monophyletic, it contains all descendants of the pelycosaurs; because it is excluded, it contains no viviparous animals. This does not work, because humans are among these descendants. Vivipary in a group that includes humans and skinks cannot be paraphyletic.


The following list recapitulates a number of paraphyletic groups proposed in the literature, and provides the corresponding monophyletic taxa.

The concept of paraphyly has also been applied to historical linguistics, where the methods of cladistics have found some utility in comparing languages. For instance, the Formosan languages form a paraphyletic group of the Austronesian languages because they consist of the nine branches of the Austronesian family that are not Malayo-Polynesian and are restricted to the island of Taiwan.



</doc>
<doc id="24455" url="https://en.wikipedia.org/wiki?curid=24455" title="Pope Innocent III">
Pope Innocent III

Pope Innocent III (; 1160 or 1161 – 16 July 1216), born Lotario dei Conti di Segni (anglicized as Lothar of Segni) reigned from 8 January 1198 to his death in 1216. 

Pope Innocent was one of the most powerful and influential of the medieval popes. He exerted a wide influence over the Christian states of Europe, claiming supremacy over all of Europe's kings. 
He was central in supporting the Catholic Church's reforms of ecclesiastical affairs through his decretals and the Fourth Lateran Council. This resulted in a considerable refinement of Western canon law.
He is furthermore notable for using interdict and other censures to compel princes to obey his decisions, although these measures were not uniformly successful. 

Innocent greatly extended the scope of the crusades, directing crusades against Muslim Spain and the Holy Land as well as the Albigensian Crusade against the Cathars in southern France.
He organized the Fourth Crusade of 1202–1204, which ended in the disastrous sack of Constantinople. Although the attack on Constantiople went against his explicit orders, and the Crusaders were subsequently excommunicated, Innocent reluctantly accepted this result, seeing it as the will of God to reunite the Latin and Orthodox Churches. 

In the event, the sack of Constantinople and the subsequent period of "Frankokratia" led to an increase in the hostility between the Latin and Greek churches. The Byzantine empire was restored in 1261 but it never regained its former strength until its final destruction in 1453.

Lotario de' Conti was born in Gavignano, Italy, near Anagni. His father was Count Trasimund of Segni and was a member of a famous house, Conti di Segni (Earl of Segni), which produced nine popes, including Gregory IX, Alexander IV and Innocent XIII. Lotario was the nephew of Pope Clement III; his mother, Claricia Scotti (Romani de Scotti), was from the same noble Roman family.

Lotario received his early education in Rome, probably at the Benedictine abbey of St Andrea al Celio, under Peter Ismael; he studied theology in Paris under the theologians Peter of Poitiers, Melior of Pisa, and Peter of Corbeil, and (possibly) jurisprudence in Bologna, according to the "Gesta" (between 1187 and 1189). As Pope, Lotario was to play a major role in the shaping of canon law through conciliar canons and decretal letters.

Shortly after the death of Alexander III (30 August 1181) Lotario returned to Rome and held various ecclesiastical offices during the short reigns of Lucius III, Urban III, Gregory VIII, and Clement III, reaching the rank of Cardinal-Deacon in 1190.

As a cardinal, Lotario wrote "De miseria humanae conditionis" (On the Misery of the Human Condition). The work was very popular for centuries, surviving in more than 700 manuscripts. Although he never returned to the complementary work he intended to write, "On the Dignity of Human Nature", Bartolomeo Facio (1400–1457) took up the task writing "De excellentia ac praestantia hominis".

Celestine III died on 8 January 1198. Before his death he had urged the College of Cardinals to elect Giovanni di San Paolo as his successor, but Lotario de' Conti was elected pope in the ruins of the ancient Septizodium, near the Circus Maximus in Rome after only two ballots on the very day on which Celestine III died. He was only thirty-seven years old at the time. He took the name Innocent III, maybe as a reference to his predecessor Innocent II (1130–1143), who had succeeded in asserting the Papacy's authority over the emperor (in contrast with Celestine III's recent policy).

As pope, Innocent III began with a very wide sense of his responsibility and of his authority. The Muslim recapture of Jerusalem in 1187 was to him a divine judgment on the moral lapses of Christian princes. He was also determined to protect what he called "the liberty of the Church" from inroads by secular princes. This determination meant, among other things, that princes should not be involved in the selection of bishops, and it was focused especially on the "patrimonium" of the papacy, the section of central Italy claimed by the popes and later called the Papal States. The patrimonium was routinely threatened by Hohenstaufen German kings who, as Roman emperors, claimed it for themselves. The Holy Roman Emperor Henry VI expected to be succeeded by his infant son Frederick as king of Sicily, king of the Germans, and Roman Emperor, a combination that would have brought Germany, Italy, and Sicily under a single ruler and left the patrimonium exceedingly vulnerable.

The early death of Henry VI left his 4-year-old son Frederick II as king. Henry VI's widow Constance of Sicily ruled over Sicily for her young son before he reached the age of majority. She was as eager to remove German power from the kingdom of Sicily as was Innocent III. Before her death in 1198, she named Innocent as guardian of the young Frederick until he reached his maturity. In exchange, Innocent was also able to recover papal rights in Sicily that had been surrendered decades earlier to King William I of Sicily by Pope Adrian IV. The Pope invested the young Frederick II as King of Sicily in November 1198. He also later induced Frederick II to marry the widow of King Emeric of Hungary in 1209.

In 1209, Francis of Assisi led his first eleven followers to Rome to seek permission from Pope Innocent III to found a new religious Order, which was ultimately granted. Upon entry to Rome, the brothers encountered Bishop Guido of Assisi, who had in his company Giovanni di San Paolo, the Cardinal Bishop of Sabina. The Cardinal, who was the confessor of Pope Innocent III, was immediately sympathetic to Francis and agreed to represent Francis to the pope. Reluctantly, Pope Innocent agreed to meet with Francis and the brothers the next day. After several days, the pope agreed to admit the group informally, adding that when God increased the group in grace and number, they could return for an official admittance. The group was tonsured. This was important in part because it recognized Church authority and prevented his following from possible accusations of heresy, as had happened to the Waldensians decades earlier. Though Pope Innocent initially had his doubts, following a dream in which he saw Francis holding up the Basilica of St. John Lateran (the cathedral of Rome, thus the 'home church' of all Christendom), he decided to endorse Francis's Order. This occurred, according to tradition, on April 16, 1210, and constituted the official founding of the Franciscan Order. The group, then the "Lesser Brothers" ("Order of Friars Minor" also known as the "Franciscan Order"), preached on the streets and had no possessions. They were centered in Porziuncola, and preached first in Umbria, before expanding throughout Italy.

Papal power was based on more than scriptures. The popes acquired large amounts of land, and bishops and clergy were, in theory, agents of papal programs. Pope Innocent III's increased involvement in Imperial elections took historically documented form when he called the Fourth Lateran Council in 1215 during which time he beckoned about 1200 bishops, abbots and nobles from around Europe to assist in either tweaking current laws or creating new ones to further influence the masses in supporting the Pope as the universal authority of the Empire.

In order to define fundamental doctrines, the council reviewed the nature of the Eucharist, the ordered annual confession of sins, and prescribed detailed procedures for the election of bishops. The council also mandated a strict lifestyle for clergy, banning their participation in judicial procedures involving extremely painful punishments by which the accused would either atone for their sins or prove themselves innocent of often frivolous charges. One doctrine that confirmed the "power over the spirit" theory was the implementation by the council mandating that Jews wear special identifying markings on their clothing – a sign of the increased hostility felt by Christians towards Jews in the region.

Another tool Innocent III used to attempt to gain universal authority and have more involvement in Imperial elections was letters he wrote to power brokers in the region. The most explicit one concerns the theory of the sun and the moon.

Other letters that Innocent III sent during this attempt to mandate and secure the papal proprietor as the universal authority by demeaning and attempting to minimize the authority of the emperors were written under the title "Papal Policies":


One of the most direct public notices of the universal authority of the pope came in Innocent III's "Papal Decree on the choice of a German King, 1201". It was his opportunity to force the acceptance of his decree amidst a chaotic election of three men for emperor:

It is the business of the pope to look after the interests of the Roman empire, since the empire derives its origin and its final authority from the papacy; its origin, because it was originally transferred from Greece by and for the sake of the papacy...its final authority, because the emperor is raised to his position by the pope who blesses him, crowns him and invests him with the empire...Therefore, since three persons have lately been elected king by different parties, namely the youth [Frederick, son of Henry VI], Philip [of Hohenstaufen, brother of Henry VI], and Otto [of Brunswick, of the Welf family], so also three things must be taken into account in regard to each one, namely: the legality, the suitability and the expediency of his election...Far be it from us that we should defer to man rather than to God, or that we should fear the countenance of the powerful...On the foregoing grounds, then, we decide that the youth should not at present be given the empire; we utterly reject Philip for his manifest unfitness and we order his usurpation to be resisted by all...since Otto is not only himself devoted to the church, but comes from devout ancestors on both sides...therefore we decree that he ought to be accepted and supported as king, and ought to be given the crown of empire, after the rights of the Roman church have been secured.
During the reign of Pope Innocent III, the papacy was at the height of its powers. He was considered to be the most powerful person in Europe at the time. His papacy asserted the absolute spiritual authority of his office, while still respecting the temporal authority of kings.

After the death of Emperor Henry VI, who had recently also conquered the Kingdom of Sicily, the succession became disputed: as Henry's son Frederick was still a small child, the partisans of the Staufen dynasty elected Henry's brother, Philip, Duke of Swabia, king in March 1198, whereas the princes opposed to the Staufen dynasty elected Otto, Duke of Brunswick, of the House of Welf. King Philip II of France supported Philip's claim, whereas King Richard I of England supported his nephew Otto.

Pope Innocent was determined to prevent the continued unification of Sicily and the Holy Roman Empire under one monarch and seized the opportunity to extend his influence. In 1201, the pope openly espoused the side of Otto IV, whose family had always been opposed to the house of Hohenstaufen. Otto himself also seemed willing to grant any demands that Innocent would make. The confusion in the Empire allowed Innocent to drive out the imperial feudal lords from Ancona, Spoleto and Perugia, who had been installed by Emperor Henry VI. On 3 July 1201, the papal legate, Cardinal-Bishop Guido of Palestrina announced to the people, in the cathedral of Cologne, that Otto IV had been approved by the pope as Roman king and threatened with excommunication all those who refused to acknowledge him. At the same time, Innocent encouraged the cities in Tuscany to form a league, called the League of San Genesio against German imperial interests in Italy, and they placed themselves under Innocent's protection.

In May 1202, Innocent issued the decree "Venerabilem", addressed to the Duke of Zähringen, in which he explained the relation he considered the Empire to stand to the papacy. This decree, which has become famous, was afterwards embodied in the "Corpus Juris Canonici", contained the following major items:


Despite papal support, Otto could not oust his rival Philip until the latter was murdered in a private feud. His rule now undisputed, Otto reneged on his earlier promises and now set his sights on reestablishing Imperial power in Italy and claiming even the Kingdom of Sicily. Given the papal interest to keep Germany and Sicily apart, Innocent now supported his ward, King Frederick of Sicily, to resist Otto's advances and restore the Staufen dynasty to the Holy Roman Empire. Frederick was duly elected by the Staufen partisans.

The conflict was decided by the Battle of Bouvines on 27 July 1214, which pitted Otto, allied to King John of England against Philip II Augustus. Otto was defeated by the French and thereafter lost all influence. He died on 19 May 1218, leaving Frederick II the undisputed emperor. Meanwhile, King John was forced to acknowledge the Pope as his feudal lord and accept Stephen Langton as Archbishop of Canterbury.

Innocent III played further roles in the politics of Norway, France, Sweden, Bulgaria, Spain and England. At the request of England's King John, Pope Innocent III declared the Magna Carta annulled, resulting in a rebellion by the English Barons who did not accept this action.

Innocent called the Fourth Crusade, which was diverted to Constantinople. The pope excommunicated the Crusaders who attacked Christian cities, but was unable to halt or overturn their actions. Erroneously, he felt that the Latin presence would bring about a reconciliation between the Eastern and Western Churches. Innocent also ordered an Albigensian Crusade, which successfully subdued the Cathar heresy in France.

Innocent III was a vigorous opponent of religious dissent, perceived as heresy, and undertook campaigns against it. At the beginning of his pontificate, he focused on the Albigenses, also known as the Cathars, a sect that had become widespread in southwestern France, then under the control of local princes, such as the Counts of Toulouse. The Cathars rejected the authority and the teachings of the Catholic Church, and what they viewed in it as corrupt.

In 1198, Innocent III dispatched a monk named Rainier to visit France with the power to excommunicate heretics, and orders to local temporal authorities to confiscate the lands of heretics or to "as became Christians to deal with them more severely."

The murder of Pierre de Castelnau – Innocent's legate – in 1208, by unknown assailants commonly believed to be friends of Count Raymond of Toulouse (who was not a Cathar), caused Innocent to change his methods from words to weapons. Innocent called upon King Philip II Augustus of France to suppress the Albigenses. Under the leadership of Simon de Montfort, 5th Earl of Leicester, a campaign was launched. The Albigensian Crusade, which led to the slaughter of approximately 20,000 men, women and children, Cathar and Catholic alike and brought the region firmly under the control of the king of France. It was directed not only against heretical Christians, but also the nobility of Toulouse and vassals of the Crown of Aragon. King Peter II of Aragon was directly involved in the conflict, and was killed in the course of the Battle of Muret in 1213. The conflict largely ended with the Treaty of Paris of 1229, in which the integration of the Occitan territory in the French crown was agreed upon. Military action ceased in 1255.
Pope Innocent III spent a majority of his tenure as Pope (1198–1216) preparing for a great crusade on the Holy Land. His first attempt was the Fourth Crusade (1202–1204) which he decreed in 1198. Unlike past popes, Innocent III displayed interest in leading the crusade himself, rather than simply instigating it and allowing secular leaders to organize the expedition according to their own aspirations.

Innocent III's first order of business in preaching the crusade was to send missionaries to every Catholic state to endorse the campaign. Innocent III sent Peter of Capua to the kings France and England with specific instructions to convince them to settle their differences. As a result, in 1199, Innocent was successful in forging a truce of five years between the two nations. The intent of the truce between the kings was not to allow them to lead the crusade, but rather to improve the likelihood that they would provide assistance. For the army's leadership, Innocent aimed his pleas at the knights and nobles of Europe. The pleadings were successful in France, where many lords answered the pope's call, including the army's two eventual leaders, Theobald of Champagne and Boniface, marquis of Montferrat. Innocent III's calls to action were not received with as much enthusiasm in England or Germany. For this reason, the Fourth Crusade became mainly a French affair.

The Fourth Crusade was an expensive endeavor. Innocent III chose to raise funds by doing something previously unheard of in popes. He forced the entire clergy under his leadership to give one fortieth of their income in support of the Crusade. This marked the first time a pope ever imposed a direct tax on his clerical subjects. The pope faced many difficulties with collecting this tax, including corruption of his own officials and disregard of his subjects in England. He continued in his attempt to garner funds for his crusade by sending envoys to King John of England and King Philip of France. Both men pledged to contribute one fortieth of their own salaries to the campaign. John also declared that the tax would be collected throughout England as well. The other source of funds for the crusade was the crusaders themselves. Innocent declared that those who took the vow to become crusaders but could no longer perform the tasks that they had promised to complete, could be released of their oaths by a contribution of funds to the original cause. The pope put Archbishop Hubert Walter in charge of collecting these dues.

At the onset of the crusade, the intended destination was Egypt, as the Christians and Muslims were under a truce at the time. An agreement was made between the French Crusaders and the Venetians. The Venetians would supply vessels and supplies for the crusaders and in return, the crusaders would pay 85,000 marks (£200,000). Innocent gave his approval of this agreement under two conditions: a representative of the pope must accompany the crusade, and the attack of any other Christians was strictly forbidden. The French failed to raise sufficient funds for payment of the Venetians. As a result, the Crusaders diverted the crusade to the Christian city of Zara at the will of the Venetians to subsidize the debt. This diversion was adopted without the consent of Innocent III, who threatened excommunication to any who took part in the attack. A majority of the French ignored the threat and attacked Zara, and were excommunicated by Innocent III, but soon were forgiven so as to continue the crusade. A second diversion then occurred when the crusaders decided to conquer Constantinople, the capital of the Byzantine Empire. This diversion was taken without any knowledge by Innocent III, and he did not learn of it until after the city had been captured.

Innocent viewed the capture of Constantinople as a way to reunite the schismatic Catholic and Eastern Orthodox Churches. His goal was to install the Latin (Western) ideals into the main center of the Greek (Eastern) Church. He saw the invasion as a way of making the Greek Church submit to the views of those that occupied their city. His tactics ultimately failed due to the significant differences between the two churches. The crusade did lead to the start of the Latin Empire's rule of Constantinople, which lasted for the next sixty years.

On 15 November 1215 Innocent opened the Fourth Lateran Council, considered the most important church council of the Middle Ages. By its conclusion it issued seventy reformatory decrees. Among other things, it encouraged creating schools and holding clergy to a higher standard than the laity. It also forbade clergymen to participate in the practice of the judicial ordeal, effectively banning its use.

At the Fourth Lateran Council, Innocent III and his prelates legislated against subordination of Christians to Jews. Canon 69 forbade "that Jews be given preferment in public office since this offers them the pretext to vent their wrath against the Christians."
Canon 69 assumes that Jews blaspheme Christ, and therefore, as it would be "too absurd for a blasphemer of Christ to exercise power over Christians", Jews should not be appointed to public offices.

The Council had set the beginning of the Fifth Crusade for 1217, under the direct leadership of the Church. After the Council, in the spring of 1216, Innocent moved to northern Italy in an attempt to reconcile the maritime cities of Pisa and Genoa by removing the excommunication cast over Pisa by his predecessor Celestine III and concluding a pact with Genoa.

Innocent III, however, died suddenly at Perugia on 16 July 1216. He was buried in the cathedral of Perugia, where his body remained until Pope Leo XIII had it transferred to the Lateran in December 1891.

His Latin works include "De miseria humanae conditionis", a tract on asceticism that Innocent III wrote before becoming pope, and "De sacro altaris mysterio", a description and exegesis of the liturgy.






</doc>
<doc id="24458" url="https://en.wikipedia.org/wiki?curid=24458" title="Polyvinyl chloride">
Polyvinyl chloride

Polyvinyl chloride (; colloquial: polyvinyl, vinyl; abbreviated: PVC) is the world's third-most widely produced synthetic plastic polymer, after polyethylene and polypropylene.

PVC comes in two basic forms: rigid (sometimes abbreviated as RPVC) and flexible. The rigid form of PVC is used in construction for pipe and in profile applications such as doors and windows. It is also used in making bottles, non-food packaging, and cards (such as bank or membership cards). It can be made softer and more flexible by the addition of plasticizers, the most widely used being phthalates. In this form, it is also used in plumbing, electrical cable insulation, imitation leather, flooring, signage, phonograph records, inflatable products, and many applications where it replaces rubber. With cotton or linen, it is used to make canvas.

Pure polyvinyl chloride is a white, brittle solid. It is insoluble in alcohol but slightly soluble in tetrahydrofuran.

PVC was accidentally synthesized in 1872 by German chemist Eugen Baumann. The polymer appeared as a white solid inside a flask of vinyl chloride that had been left exposed to sunlight. In the early 20th century the Russian chemist Ivan Ostromislensky and Fritz Klatte of the German chemical company Griesheim-Elektron both attempted to use PVC in commercial products, but difficulties in processing the rigid, sometimes brittle polymer thwarted their efforts. Waldo Semon and the B.F. Goodrich Company developed a method in 1926 to plasticize PVC by blending it with various additives. The result was a more flexible and more easily processed material that soon achieved widespread commercial use.

Polyvinyl chloride is produced by polymerization of the vinyl chloride monomer (VCM), as shown.
About 80% of production involves suspension polymerization. Emulsion polymerization accounts for about 12%, and bulk polymerization accounts for 8%. Suspension polymerization affords particles with average diameters of 100–180 μm, whereas emulsion polymerization gives much smaller particles of average size around 0.2 μm. VCM and water are introduced into the reactor along with a polymerization initiator and other additives. The contents of the reaction vessel are pressurized and continually mixed to maintain the suspension and ensure a uniform particle size of the PVC resin. The reaction is exothermic and thus requires cooling. As the volume is reduced during the reaction (PVC is denser than VCM), water is continually added to the mixture to maintain the suspension.

The polymerization of VCM is started by compounds called initiators that are mixed into the droplets. These compounds break down to start the radical chain reaction. Typical initiators include dioctanoyl peroxide and dicetyl peroxydicarbonate, both of which have fragile O-O bonds. Some initiators start the reaction rapidly but decay quickly, and other initiators have the opposite effect. A combination of two different initiators is often used to give a uniform rate of polymerization. After the polymer has grown by about 10 times, the short polymer precipitates inside the droplet of VCM, and polymerization continues with the precipitated, solvent-swollen particles. The weight average molecular weights of commercial polymers range from 100,000 to 200,000, and the number average molecular weights range from 45,000 to 64,000.

Once the reaction has run its course, the resulting PVC slurry is degassed and stripped to remove excess VCM, which is recycled. The polymer is then passed through a centrifuge to remove water. The slurry is further dried in a hot air bed, and the resulting powder is sieved before storage or pelletization. Normally, the resulting PVC has a VCM content of less than 1 part per million. Other production processes, such as micro-suspension polymerization and emulsion polymerization, produce PVC with smaller particle sizes (10 μm vs. 120–150 μm for suspension PVC) with slightly different properties and with somewhat different sets of applications.

PVC may be manufactured from either naphtha or ethylene feedstock.

The polymers are linear and are strong. The monomers are mainly arranged head-to-tail, meaning that there are chlorides on alternating carbon centres. PVC has mainly an atactic stereochemistry, which means that the relative stereochemistry of the chloride centres are random. Some degree of syndiotacticity of the chain gives a few percent crystallinity that is influential on the properties of the material. About 57% of the mass of PVC is chlorine. The presence of chloride groups gives the polymer very different properties from the structurally related material polyethylene.

About half of the world's PVC production capacity is in China, but many Chinese PVC plants have been shut down due to issues complying with environmental regulations. The largest single producer of PVC as of 2018 is Shin-Etsu Chemical of Japan, with a global share of around 30%.

The product of the polymerization process is unmodified PVC. Before PVC can be made into finished products, it always requires conversion into a compound by the incorporation of additives (but not necessarily all of the following) such as heat stabilizers, UV stabilizers, plasticizers, processing aids, impact modifiers, thermal modifiers, fillers, flame retardants, biocides, blowing agents and smoke suppressors, and, optionally, pigments. The choice of additives used for the PVC finished product is controlled by the cost performance requirements of the end use specification (underground pipe, window frames, intravenous tubing and flooring all have very different ingredients to suit their performance requirements). Previously, polychlorinated biphenyls (PCBs) were added to certain PVC products as flame retardants and stabilizers.

Most vinyl products contain plasticizers which dramatically improve their performance characteristic. The most common plasticizers are derivatives of phthalic acid. The materials are selected on their compatibility with the polymer, low volatility levels, and cost. These materials are usually oily colourless substances that mix well with the PVC particles. About 90% of the plasticizer market, estimated to be millions of tons per year worldwide, is dedicated to PVC.

Liquid mixed metal stabilisers are used in several PVC flexible applications such as calendered films, extruded profiles, injection moulded soles and footwear, extruded hoses and plastisols where PVC paste is spread on to a backing (flooring, wall covering, artificial leather). Liquid mixed metal stabiliser systems are primarily based on barium, zinc and calcium carboxylates. In general liquid mixed metals like BaZn, CaZn require the addition of co-stabilisers, antioxidants and organo-phosphites to provide optimum performance.

BaZn stabilisers have successfully replaced cadmium-based stabilisers in Europe in many PVC semi-rigid and flexible applications.

In Europe, particularly Belgium, there has been a commitment to eliminate the use of cadmium (previously used as a part component of heat stabilizers in window profiles) and phase out lead-based heat stabilizers (as used in pipe and profile areas) such as liquid autodiachromate and calcium polyhydrocummate by 2015. According to the final report of Vinyl 2010 cadmium was eliminated across Europe by 2007. The progressive substitution of lead-based stabilizers is also confirmed in the same document showing a reduction of 75% since 2000 and ongoing. This is confirmed by the corresponding growth in calcium-based stabilizers, used as an alternative to lead-based stabilizers, more and more, also outside Europe.

Tin-based stabilizers are mainly used in Europe for rigid, transparent applications due to the high temperature processing conditions used. The situation in North America is different where tin systems are used for almost all rigid PVC applications.
Tin stabilizers can be divided into two main groups, the first group containing those with tin-oxygen bonds and the second group with tin-sulphur bonds.

One of the most crucial additives are heat stabilizers. These agents minimize loss of HCl, a degradation process that starts above 70 °C. Once dehydrochlorination starts, it is autocatalytic. Many diverse agents have been used including, traditionally, derivatives of heavy metals (lead, cadmium). Increasingly, metallic soaps (metal "salts" of fatty acids) are favored, species such as calcium stearate. Addition levels vary typically from 2% to 4%.
The choice of the best heat stabilizer depends on its cost effectiveness in the end use application, performance specification requirements, processing technology and regulatory approvals.

Di-2ethylhexylphthalate (DEHP) has been medically approved for many years for use in medical devices; the PVC-DEHP combination proving to be very suitable for making blood bags because DEHP stabilises red blood cells, minimising haemolysis (red blood cell rupture). However, DEHP is coming under increasing pressure in Europe. The assessment of potential risks related to phthalates, and in particular the use of DEHP in PVC medical devices, was subject to scientific and policy review by the European Union authorities, and on 21 March 2010, a specific labeling requirement was introduced across the EU for all devices containing phthalates that are classified as CMR (carcinogenic, mutagenic or toxic to reproduction). The label aims to enable healthcare professionals to use this equipment safely, and, where needed, take appropriate precautionary measures for patients at risk of over-exposure.

DEHP alternatives, which are gradually replacing it, are Adipates, Butyryltrihexylcitrate (BTHC), Cyclohexane-1,2-dicarboxylic acid, diisononylester (DINCH), Di(2-ethylhexyl)terephthalate, polymerics and trimellitic acid, 2-ethylhexylester (TOTM).

PVC is a thermoplastic polymer. Its properties are usually categorized based on rigid and flexible PVCs.
PVC has high hardness and mechanical properties. The mechanical properties enhance with the molecular weight increasing but decrease with the temperature increasing. The mechanical properties of rigid PVC (uPVC) are very good; the elastic modulus can reach 1500-3,000 MPa. The soft PVC (flexible PVC) elastic limit is 1.5–15 MPa.

The heat stability of raw PVC is very poor, so the addition of a heat stabilizer during the process is necessary in order to ensure the product's properties. Traditional product PVC has a maximum operating temperature around 140°F (60°C) when heat distortion begins to occur. Melting temperatures range from 212°F to 500°F (100°C to 260°C) depending upon manufacture additives to the PVC. The linear expansion coefficient of rigid PVC is small and has good flame retardancy, the limiting oxygen index (LOI) being up to 45 or more. The LOI is the minimum concentration of oxygen, expressed as a percentage, that will support combustion of a polymer and noting that air has 20% content of oxygen.

As a thermoplastic, PVC has an inherent insulation that aids in reducing condensation formation and resisting internal temperature changes for hot and cold liquids.

PVC is a polymer with good insulation properties, but because of its higher polar nature the electrical insulating property is inferior to non polar polymers such as polyethylene and polypropylene.

Since the dielectric constant, dielectric loss tangent value, and volume resistivity are high, the corona resistance is not very good, and it is generally suitable for medium or low voltage and low frequency insulation materials.

PVC is chemically resistant to acids, salts, bases, fats, and alcohols, making it resistant to the corrosive effects of sewage, which is why it is so extensively utilized in sewer piping systems. It is also resistant to some solvents, this, however, is reserved mainly for uPVC (unplasticized PVC). Plasticized PVC, also known as PVC-P, is in some cases less resistant to solvents. For example, PVC is resistant to fuel and some paint thinners. Some solvents may only swell it or deform it but not dissolve it, but some, like tetrahydrofuran or acetone, may damage it.

Roughly half of the world's polyvinyl chloride resin manufactured annually is used for producing pipes for municipal and industrial applications. In the water distribution market, it accounts for 66% of the market in the U.S., and in sanitary sewer pipe applications, it accounts for 75%. Buried PVC pipes in both water and sanitary sewer applications that are in diameter and larger are typically joined by means of a gasket-sealed joint. The most common type of gasket utilized in North America is a metal reinforced elastomer, commonly referred to as a Rieber sealing system. Its light weight, low cost, and low maintenance make it attractive. However, it must be carefully installed and bedded to ensure longitudinal cracking and overbelling does not occur. Additionally, PVC pipes can be fused together using various solvent cements, or heat-fused (butt-fusion process, similar to joining high-density polyethylene (HDPE) pipe), creating permanent joints that are virtually impervious to leakage.

In February 2007 the California Building Standards Code was updated to approve the use of chlorinated polyvinyl chloride (CPVC) pipe for use in residential water supply piping systems. CPVC has been a nationally accepted material in the U.S. since 1982; California, however, has permitted only limited use since 2001. The Department of Housing and Community Development prepared and certified an environmental impact statement resulting in a recommendation that the commission adopt and approve the use of CPVC. The commission's vote was unanimous, and CPVC has been placed in the 2007 California Plumbing Code.

PVC is commonly used as the insulation on electrical cables such as teck; PVC used for this purpose needs to be plasticized.
Flexible PVC coated wire and cable for electrical use has traditionally been stabilised with lead, but these are being replaced with calcium-based systems.

In a fire, PVC-coated wires can form hydrogen chloride fumes; the chlorine serves to scavenge free radicals and is the source of the material's fire retardance. While hydrogen chloride fumes can also pose a health hazard in their own right, it dissolves in moisture and breaks down onto surfaces, particularly in areas where the air is cool enough to breathe, and is not available for inhalation. Frequently in applications where smoke is a major hazard (notably in tunnels and communal areas), PVC-free cable insulation is preferred, such as low smoke zero halogen (LSZH) insulation.

PVC is a common, strong but lightweight plastic used in construction. It is made softer and more flexible by the addition of plasticizers. If no plasticizers are added, it is known as uPVC (unplasticized polyvinyl chloride) or rigid PVC.

uPVC is extensively used in the building industry as a low-maintenance material, particularly in Ireland, the United Kingdom, in the United States and Canada. In the U.S. and Canada it is known as vinyl or vinyl siding. The material comes in a range of colors and finishes, including a photo-effect wood finish, and is used as a substitute for painted wood, mostly for window frames and sills when installing insulated glazing in new buildings; or to replace older single-glazed windows, as it does not decompose and is weather-resistant. Other uses include fascia, and siding or weatherboarding. This material has almost entirely replaced the use of cast iron for plumbing and drainage, being used for waste pipes, drainpipes, gutters and downspouts. uPVC is known as having strong resistance against chemicals, sunlight, and oxidation from water.
Polyvinyl chloride is formed in flat sheets in a variety of thicknesses and colors. As flat sheets, PVC is often expanded to create voids in the interior of the material, providing additional thickness without additional weight and minimal extra cost (see Closed-cell PVC foamboard). Sheets are cut using saws and rotary cutting equipment. Plasticized PVC is also used to produce thin, colored, or clear, adhesive-backed films referred to simply as vinyl. These films are typically cut on a computer-controlled plotter (see Vinyl cutter) or printed in a wide-format printer. These sheets and films are used to produce a wide variety of commercial signage products, including car body stripes and stickers.

PVC fabric is water-resistant, used for its weather-resistant qualities in coats, skiing equipment, shoes, jackets, aprons, and sports bags.

PVC fabric has a niche role in specialty clothing, to either create a artificial leather material or at times simply for its effect. PVC clothing is common in Goth, Punk, clothing fetish and alternative fashions. PVC is less expensive than rubber, leather, and latex which it is used to simulate.

The two main application areas for single-use medically approved PVC compounds are flexible containers and tubing: containers used for blood and blood components, for urine collection or for ostomy products and tubing used for blood taking and blood giving sets, catheters, heart-lung bypass sets, hemodialysis sets etc. In Europe the consumption of PVC for medical devices is approximately 85,000 tons each year. Almost one third of plastic-based medical devices are made from PVC.
The reasons for using flexible PVC in these applications for over 50 years are numerous and based on cost effectiveness linked to transparency, light weight, softness, tear strength, kink resistance, suitability for sterilization and biocompatibility.

Flexible PVC flooring is inexpensive and used in a variety of buildings, including homes, hospitals, offices, and schools. Complex and 3D designs are possible, which are then protected by a clear wear layer. A middle vinyl foam layer also gives a comfortable and safe feel. The smooth, tough surface of the upper wear layer prevents the buildup of dirt, which prevents microbes from breeding in areas that need to be kept sterile, such as hospitals and clinics.

PVC may be extruded under pressure to encase wire rope and aircraft cable used for general purpose applications. PVC coated wire rope is easier to handle, resists corrosion and abrasion, and may be color-coded for increased visibility. It is found in a variety of industries and environments both indoor and out.

PVC has been used for a host of consumer products. One of its earliest mass-market consumer applications was vinyl record production. More recent examples include wallcovering, greenhouses, home playgrounds, foam and other toys, custom truck toppers (tarpaulins), ceiling tiles and other kinds of interior cladding.

PVC piping is cheaper than metals used in musical instrument making; it is therefore a common alternative when making instruments, often for leisure or for rarer instruments such as the contrabass flute.

PVC can be usefully modified by chlorination, which increases its chlorine content to 67%. CPVC, as it is called, is produced by chlorination of aqueous solution of suspension PVC particles followed by exposure to UV light which initiates the free-radical chlorination. An extensive market for CPVC is in pipe for use in office building, apartment and condominium fire protection. CPVC also has a higher heat resistance so is primarily used for hot water pipe and fittings, but it is more expensive and it is found only in niche applications, such as some water heaters and specialized clothing.

Degradation during service life, or after careless disposal, is a chemical change that drastically reduces the average molecular weight of the polyvinyl chloride polymer. Since the mechanical integrity of a plastic depends on its high average molecular weight, wear and tear inevitably weakens the material. Weathering degradation of plastics results in their surface embrittlement and microcracking, yielding microparticles that continue on in the environment. Also known as microplastics, these particles act like sponges and soak up persistent organic pollutants (POPs) around them. Thus laden with high levels of POPs, the microparticles are often ingested by organisms in the biosphere.

However, there is evidence that three of the polymers (HDPE, LDPE, and PP) consistently soaked up POPs at concentrations an order of magnitude higher than did the remaining two (PVC and PET). After 12 months of exposure, for example, there was a 34-fold difference in average total POPs amassed on LDPE compared to PET at one location. At another site, average total POPs adhered to HDPE was nearly 30 times that of PVC. The researchers think that differences in the size and shape of the polymer molecules can explain why some accumulate more pollutants than others. 
The fungus Aspergillus fumigatus effectively degrades plasticized PVC. Phanerochaete chrysosporium was grown on PVC in a mineral salt agar. Phanerochaete chrysosporium, Lentinus tigrinus, Aspergillus niger, and Aspergillus sydowii can effectively degrade PVC.

Phthalates, which are incorporated into plastics as plasticizers, comprise approximately 70% of the U.S. plasticizer market; phthalates are by design not covalently bound to the polymer matrix, which makes them highly susceptible to leaching. Phthalates are contained in plastics at high percentages. For example, they can contribute up to 40% by weight to intravenous medical bags and up to 80% by weight in medical tubing. Vinyl products are pervasive—including toys, car interiors, shower curtains, and flooring—and initially release chemical gases into the air. Some studies indicate that this outgassing of additives may contribute to health complications, and have resulted in a call for banning the use of DEHP on shower curtains, among other uses. Japanese car companies Toyota, Nissan, and Honda eliminated the use of PVC in car interiors since 2007.

In 2004 a joint Swedish-Danish research team found a statistical association between allergies in children and indoor air levels of DEHP and BBzP (butyl benzyl phthalate), which is used in vinyl flooring. In December 2006, the European Chemicals Bureau of the European Commission released a final draft risk assessment of BBzP which found "no concern" for consumer exposure including exposure to children.

Risk assessments have led to the classification of low molecular weight and labeling as Category 1B Reproductive agents. Three of these phthalates, DBP, BBP and DEHP were included on annex XIV of the REACH regulation in February 2011 and will be phased out by the EU by February 2015 unless an application for authorisation is made before July 2013 and an authorisation granted. DIBP is still on the REACH Candidate List for Authorisation. Environmental Science & Technology, a peer reviewed journal published by the American Chemical Society states DEHP poses a serious risk to human health.

In 2008 the European Union's Scientific Committee on Emerging and Newly Identified Health Risks (SCENIHR) reviewed the safety of DEHP in medical devices. The SCENIHR report states that certain medical procedures used in high risk patients result in a significant exposure to DEHP and concludes there is still a reason for having some concerns about the exposure of prematurely born male babies to medical devices containing DEHP. The Committee said there are some alternative plasticizers available for which there is sufficient toxicological data to indicate a lower hazard compared to DEHP but added that the functionality of these plasticizers should be assessed before they can be used as an alternative for DEHP in PVC medical devices. Risk assessment results have shown positive results regarding the safe use of High Molecular Weight Phthalates. They have all been registered for REACH and do not require any classification for health and environmental effects, nor are they on the Candidate List for Authorisation. High phthalates are not CMR (carcinogenic, mutagenic or toxic for reproduction), neither are they considered endocrine disruptors.

In the EU Risk Assessment the European Commission has confirmed that Di-isononyl phthalate (DINP) and Di-isodecyl phthalate (DIDP) pose no risk to either human health or the environment from any current use.
The European Commission's findings (published in the EU Official Journal on 13 April 2006) confirm the outcome of a risk assessment involving more than 10 years of extensive scientific evaluation by EU regulators.
Following the recent adoption of EU legislation with the regard to the marketing and use of DINP in toys and childcare articles, the risk assessment conclusions clearly state that there is no need for any further measures to regulate the use of DINP.
In Europe and in some other parts of the world, the use of DINP in toys and childcare items has been restricted as a precautionary measure. In Europe, for example, DINP can no longer be used in toys and childcare items that can be put in the mouth even though the EU scientific risk assessment concluded that its use in toys does not pose a risk to human health or the environment.
The rigorous EU risk assessments, which include a high degree of conservatism and built-in safety factors, have been carried out under the strict supervision of the European Commission and provide a clear scientific evaluation on which to judge whether or not a particular substance can be safely used.

The FDA Paper titled "Safety Assessment of Di(2-ethylhexyl)phthalate (DEHP)Released from PVC Medical Devices" states that [3.2.1.3] Critically ill or injured patients may be at increased risk of developing adverse health effects from DEHP, not only by virtue of increased exposure relative to the general population, but also because of the physiological and pharmacodynamic changes that occur in these patients compared to healthy individuals.

Lead had previously been frequently added to PVC to improve workability and stability. Lead has been shown to leach into drinking water from PVC pipes.

In Europe (EU 28) the use of lead-based stabilizers was gradually replaced by the end of 2015, under the VinylPlus voluntary commitment, ESPA members completed the replacement of Pb-based stabilisers.

In the early 1970s, the carcinogenicity of vinyl chloride (usually called vinyl chloride monomer or VCM) was linked to cancers in workers in the polyvinyl chloride industry. Specifically workers in polymerization section of a B.F. Goodrich plant near Louisville, Kentucky, were diagnosed with liver angiosarcoma also known as hemangiosarcoma, a rare disease. Since that time, studies of PVC workers in Australia, Italy, Germany, and the UK have all associated certain types of occupational cancers with exposure to vinyl chloride, and it has become accepted that VCM is a carcinogen. Technology for removal of VCM from products has become stringent, commensurate with the associated regulations.

PVC produces HCl upon combustion almost quantitatively related to its chlorine content. Extensive studies in Europe indicate that the chlorine found in emitted dioxins is not derived from HCl in the flue gases. Instead, most dioxins arise in the condensed solid phase by the reaction of inorganic chlorides with graphitic structures in char-containing ash particles. Copper acts as a catalyst for these reactions.

Studies of household waste burning indicate consistent increases in dioxin generation with increasing PVC concentrations. According to the EPA dioxin inventory, landfill fires are likely to represent an even larger source of dioxin to the environment. A survey of international studies consistently identifies high dioxin concentrations in areas affected by open waste burning and a study that looked at the homologue pattern found the sample with the highest dioxin concentration was "typical for the pyrolysis of PVC". Other EU studies indicate that PVC likely "accounts for the overwhelming majority of chlorine that is available for dioxin formation during landfill fires."

The next largest sources of dioxin in the EPA inventory are medical and municipal waste incinerators. Various studies have been conducted that reach contradictory results. For instance a study of commercial-scale incinerators showed no relationship between the PVC content of the waste and dioxin emissions. Other studies have shown a clear correlation between dioxin formation and chloride content and indicate that PVC is a significant contributor to the formation of both dioxin and PCB in incinerators.

In February 2007, the Technical and Scientific Advisory Committee of the US Green Building Council (USGBC) released its report on a PVC avoidance related materials credit for the LEED Green Building Rating system. The report concludes that "no single material shows up as the best across all the human health and environmental impact categories, nor as the worst" but that the "risk of dioxin emissions puts PVC consistently among the worst materials for human health impacts."

In Europe the overwhelming importance of combustion conditions on dioxin formation has been established by numerous researchers. The single most important factor in forming dioxin-like compounds is the temperature of the combustion gases. Oxygen concentration also plays a major role on dioxin formation, but not the chlorine content.

The design of modern incinerators minimises PCDD/F formation by optimising the stability of the thermal process. To comply with the EU emission limit of 0.1 ng I-TEQ/m3 modern incinerators operate in conditions minimising dioxin formation and are equipped with pollution control devices which catch the low amounts produced. Recent information is showing for example that dioxin levels in populations near incinerators in Lisbon and Madeira have not risen since the plants began operating in 1999 and 2002 respectively.

Several studies have also shown that removing PVC from waste would not significantly reduce the quantity of dioxins emitted. The European Union Commission published in July 2000 a Green Paper on the Environmental Issues of PVC. " The Commission states (page 27) that it has been suggested that the reduction of the chlorine content in the waste can contribute to the reduction of dioxin formation, even though the actual mechanism is not fully understood. The influence on the reduction is also expected to be a second or third order relationship. It is most likely that the main incineration parameters, such as the temperature and the oxygen concentration, have a major influence on the dioxin formation". The Green Paper states further that at the current levels of chlorine in municipal waste, there does not seem to be a direct quantitative relationship between chlorine content and dioxin formation.

A study commissioned by the European Commission on "Life Cycle Assessment of PVC and of principal competing materials" states that "Recent studies show that the presence of PVC has no significant effect on the amount of dioxins released through incineration of plastic waste."

The European waste hierarchy refers to the five steps included in the article 4 of the Waste Framework Directive:


The European Commission has set new rules to promote the recovery of PVC waste for use in a number of construction products. It says: "The use of recovered PVC should be encouraged in the manufacture of certain construction products because it allows the reuse of old PVC ... This avoids PVC being discarded in landfills or incinerated causing release of carbon dioxide and cadmium in the environment".

In Europe, developments in PVC waste management have been monitored by Vinyl 2010, established in 2000. Vinyl 2010's objective was to recycle 200,000 tonnes of post-consumer PVC waste per year in Europe by the end of 2010, excluding waste streams already subject to other or more specific legislation (such as the European Directives on End-of-Life Vehicles, Packaging and Waste Electric and Electronic Equipment).

Since June 2011, it is followed by VinylPlus, a new set of targets for sustainable development. Its main target is to recycle 800,000 tonnes/year of PVC by 2020 including 100,000 tonnes of "difficult to recycle" waste. One facilitator for collection and recycling of PVC waste is Recovinyl. The reported and audited recycled PVC tonnage in 2016 was 568,695 tonnes.
One approach to address the problem of waste PVC is also through the process called Vinyloop. It is a mechanical recycling process using a solvent to separate PVC from other materials. This solvent turns in a closed loop process in which the solvent is recycled. Recycled PVC is used in place of virgin PVC in various applications: coatings for swimming pools, shoe soles, hoses, diaphragms tunnel, coated fabrics, PVC sheets. This recycled PVC's primary energy demand is 46 percent lower than conventional produced PVC. So the use of recycled material leads to a significant better ecological footprint. The global warming potential is 39 percent lower.

In November 2005 one of the largest hospital networks in the U.S., Catholic Healthcare West, signed a contract with B. Braun Melsungen for vinyl-free intravenous bags and tubing.

In January 2012 a major U.S. West Coast healthcare provider, Kaiser Permanente, announced that it will no longer buy intravenous (IV) medical equipment made with polyvinyl chloride (PVC) and DEHP (di-2-ethyl hexyl phthalate) type plasticizers.

In 1998, the U.S. Consumer Product Safety Commission (CPSC) arrived at a voluntary agreement with manufacturers to remove phthalates from PVC rattles, teethers, baby bottle nipples and pacifiers.

Plasticized PVC is a common material for medical gloves. Due to vinyl gloves having less flexibility and elasticity, several guidelines recommend either latex or nitrile gloves for clinical care and procedures that require manual dexterity and/or that involve patient contact for more than a brief period. Vinyl gloves show poor resistance to many chemicals, including glutaraldehyde-based products and alcohols used in formulation of disinfectants for swabbing down work surfaces or in hand rubs. The additives in PVC are also known to cause skin reactions such as allergic contact dermatitis. These are for example the antioxidant bisphenol A, the biocide benzisothiazolinone, propylene glycol/adipate polyester and ethylhexylmaleate.

PVC is made from petroleum. The production process also uses sodium chloride. Recycled PVC is broken down into small chips, impurities removed, and the product refined to make pure white PVC. It can be recycled roughly seven times and has a lifespan of around 140 years.

In the UK, approximately 400 tonnes of PVC are recycled every month. Property owners can recycle it through nationwide collection depots. The Olympic Delivery Authority (ODA), for example, after initially rejecting PVC as material for different temporary venues of the London Olympics 2012, has reviewed its decision and developed a policy for its use. This policy highlighted that the functional properties of PVC make it the most appropriate material in certain circumstances while taking into consideration the environmental and social impacts across the whole life cycle, e.g. the rate for recycling or reuse and the percentage of recycled content. Temporary parts, like roofing covers of the Olympic Stadium, the Water Polo Arena, and the Royal Artillery Barracks, would be deconstructed and a part recycled in the Vinyloop process.



</doc>
<doc id="24460" url="https://en.wikipedia.org/wiki?curid=24460" title="Profession">
Profession

A profession is a vocation founded upon specialized educational training, the purpose of which is to supply disinterested objective counsel and service to others, for a direct and definite compensation, wholly apart from expectation of other business gain. The term is a truncation of the term "liberal profession", which is, in turn, an Anglicization of the French term "profession libérale". Originally borrowed by English users in the 19th century, it has been re-borrowed by international users from the late 20th, though the (upper-middle) class overtones of the term do not seem to survive retranslation: "liberal professions" are, according to the European Union's Directive on Recognition of Professional Qualifications (2005/36/EC) "those practiced on the basis of relevant professional qualifications in a personal, responsible and professionally independent capacity by those providing intellectual and conceptual services in the interest of the client and the public".

It has been said that a profession is not a trade and not an industry. 

Medieval and early modern tradition recognized only three professions: divinity, medicine, and law 

Major milestones which may mark an occupation being identified as a profession include:


Applying these milestones to the historical sequence of development in the United States shows surveying achieving professional status first (note that George Washington, Thomas Jefferson, and Abraham Lincoln all worked as land surveyors before entering politics), followed by medicine, actuarial science, law, dentistry, civil engineering, logistics, architecture and accounting.

With the rise of technology and occupational specialization in the 19th century, other bodies began to claim professional status: mechanical engineering, pharmacy, veterinary medicine, psychology, nursing, teaching, librarianship, optometry and social work, each of which could claim, using these milestones, to have become professions by 1900.

Just as some professions rise in status and power through various stages, others may decline. Disciplines formalized more recently, such as architecture, now have equally long periods of study associated with them.

Although professions may enjoy relatively high status and public prestige, not all professionals earn high salaries, and even within specific professions there exist significant inequalities of compensation; in law, for example, a corporate/insurance defense lawyer working on a billable-hour basis may earn several times what a prosecutor or public defender earns.

A profession arises when any trade or occupation transforms itself through ""the development of formal qualifications based upon education, apprenticeship, and examinations, the emergence of regulatory bodies with powers to admit and discipline members, and some degree of monopoly rights.""

Originally, any regulation of the professions was self-regulation through bodies such as the College of Physicians or the Inns of Court. With the growing role of government, statutory bodies have increasingly taken on this role, their members being appointed either by the profession or (increasingly) by government. Proposals for the introduction or enhancement of statutory regulation may be welcomed by a profession as protecting clients and enhancing its quality and reputation, or as restricting access to the profession and hence enabling higher fees to be charged. It may be resisted as limiting the members' freedom to innovate or to practice as in their professional judgement they consider best.

An example was in 12, when the British government proposed wide statutory regulation of psychologists. The inspiration for the change was a number of problems in the psychotherapy field, but there are various kinds of psychologist including many who have no clinical role and where the case for regulation was not so clear. Work psychology brought especial disagreement, with the British Psychological Society favoring statutory regulation of "occupational psychologists" and the Association of Business Psychologists resisting the statutory regulation of "business psychologists" – descriptions of professional activity which it may not be easy to distinguish.

Besides regulating access to a profession, professional bodies may set examinations of competence and enforce adherence to an ethical code. There may be several such bodies for one profession in a single country, an example being the accountancy bodies of the United Kingdom (ACCA, CAI, CIMA, CIPFA, ICAEW and ICAS), all of which have been given a Royal Charter, although their members are not necessarily considered to hold equivalent qualifications, and which operate alongside further bodies (AAPA, IFA, CPAA). Another example of a regulatory body that governs a profession is the Hong Kong Professional Teachers Union, which governs the conduct, rights, obligations and duties of salaried teachers working in educational institutions in Hong Kong.

The engineering profession is highly regulated in some countries (Canada and USA) with a strict licensing system for Professional Engineer that controls the practice but not in others (UK) where titles and qualifications are regulated Chartered Engineer but practice is not regulated. 

Typically, individuals are required by law to be qualified by a local professional body before they are permitted to practice in that profession. However, in some countries, individuals may not be required by law to be qualified by such a professional body in order to practice, as is the case for accountancy in the United Kingdom (except for auditing and insolvency work which legally require qualification by a professional body). In such cases, qualification by the professional bodies is effectively still considered a prerequisite to practice as most employers and clients stipulate that the individual hold such qualifications before hiring their services. For example, in order to become a fully qualified teaching professional in Hong Kong working in a state or government-funded school, one needs to have successfully completed a Postgraduate Diploma in Education ("PGDE") or a bachelor's degree in Education ("BEd") at an approved tertiary educational institution or university. This requirement is set out by the Educational Department Bureau of Hong Kong, which is the governmental department that governs the Hong Kong education sector.

Professions tend to be autonomous, which means they have a high degree of control of their own affairs: ""professionals are autonomous insofar as they can make independent judgments about their work"". This usually means ""the freedom to exercise their professional judgement.""

However, it also has other meanings. ""Professional autonomy is often described as a claim of professionals that has to serve primarily their own interests...this professional autonomy can only be maintained if members of the profession subject their activities and decisions to a critical evaluation by other members of the profession "" The concept of autonomy can therefore be seen to embrace not only judgement, but also self-interest and a continuous process of critical evaluation of ethics and procedures from within the profession itself.

One major implication of professional autonomy is the traditional ban on corporate practice of the professions, especially accounting, architecture, medicine, and law. This means that in many jurisdictions, these professionals cannot do business through regular for-profit corporations and raise capital rapidly through initial public offerings or flotations. Instead, if they wish to practice collectively they must form special business entities such as partnerships or professional corporations, which feature (1) reduced protection against liability for professional negligence and (2) severe limitations or outright prohibitions on ownership by non-professionals. The obvious implication of this is that all equity owners of the professional business entity must be professionals themselves. This avoids the possibility of a non-professional owner of the firm telling a professional how to do his or her job and thereby protects professional autonomy. The idea is that the "only" non-professional person who should be telling the professional what to do is the "client"; in other words, professional autonomy preserves the integrity of the two-party professional-client relationship. But because professional business entities are effectively locked out of the stock market, they tend to grow relatively slowly compared to public corporations.

Professions enjoy a high social status, regard and esteem conferred upon them by society. This high esteem arises primarily from the higher social function of their work, which is regarded as vital to society as a whole and thus of having a special and valuable nature. All professions involve technical, specialized and highly skilled work often referred to as ""professional expertise."" Training for this work involves obtaining degrees and professional qualifications (see Licensure) without which entry to the profession is barred (occupational closure). Updating skills through continuing education is required through training.

All professions have power. This power is used to control its own members, and also its area of expertise and interests. A profession tends to dominate, police and protect its area of expertise and the conduct of its members, and exercises a dominating influence over its entire field which means that professions can act monopolist, rebuffing competition from ancillary trades and occupations, as well as subordinating and controlling lesser but related trades. A profession is characterized by the power and high prestige it has in society as a whole. It is the power, prestige and value that society confers upon a profession that more clearly defines it. The power of professions has led to them being referred to as conspiracies against the laity.
On the other hand, professionals acquire some of their power and authority in organizations from their expertise and knowledge. As such they can bend rules, reduce bureaucratic inertia and increase problem solving and adaptability.

There is considerable agreement about defining the characteristic features of a profession. They have a "professional association, cognitive base, institutionalized training, licensing, work autonomy, colleague control... (and) code of ethics", to which Larson then also adds, "high standards of professional and intellectual excellence," (Larson, p. 221) that "professions are occupations with special power and prestige", (Larson, p.x) and that they comprise "an exclusive elite group," (Larson, p. 20) in all societies. Members of a profession have also been defined as "workers whose qualities of detachment, autonomy, and group allegiance are more extensive than those found among other groups...their attributes include a high degree of systematic knowledge; strong community orientation and loyalty; self-regulation; and a system of rewards defined and administered by the community of workers."

A profession has been further defined as: "a special type of occupation...(possessing) corporate solidarity...prolonged specialized training in a body of abstract knowledge, and a collectivity or service orientation...a vocational sub-culture which comprises implicit codes of behavior, generates an esprit de corps among members of the same profession, and ensures them certain occupational advantages...(also) bureaucratic structures and monopolistic privileges to perform certain types of work...professional literature, legislation, etc."

A critical characteristic of a profession is the need to cultivate and exercise professional "discretion" - that is, the ability to make case by case "judgements" that cannot be determined by an absolute rule or instruction.




</doc>
<doc id="24464" url="https://en.wikipedia.org/wiki?curid=24464" title="Philip Henry Gosse">
Philip Henry Gosse

Philip Henry Gosse FRS (; 6 April 1810 – 23 August 1888), known to his friends as Henry, was an English naturalist and popularizer of natural science, virtually the inventor of the seawater aquarium, and a painstaking innovator in the study of marine biology. The aquarium craze was launched in early Victorian England by Gosse who created and stocked the first public aquarium at the London Zoo in 1853, and coined the term "aquarium" when he published the first manual, "The Aquarium: An Unveiling of the Wonders of the Deep Sea", in 1854.

Gosse was also the author of "Omphalos", an attempt to reconcile the geological ages presupposed by Charles Lyell with the biblical account of creation. After his death, Gosse was portrayed as a despotic father of uncompromising religious views in "Father and Son" (1907), a memoir written by his son, the poet and critic Edmund Gosse.

Gosse was born in Worcester in 1810 of an itinerant painter of miniature portraits and a lady's maid. He spent his childhood mostly in Poole, Dorset, where his aunt, Susan Bell, taught him to draw and introduced him to zoology as she had her own son, Thomas Bell, twenty years older and later to be a great friend to Henry.

At fifteen he began work as a clerk in the counting house of George Garland and Sons in Poole, and in 1827 he sailed to Newfoundland to serve as a clerk in the Carbonear premises of Slade, Elson and Co., where he became a dedicated, self-taught student of Newfoundland entomology, "the first person systematically to investigate and to record the entomology" of the island. In 1832 Gosse experienced a religious conversion—as he said, "solemnly, deliberately and uprightly, took God for my God."

In 1835 he left Newfoundland for Compton, Lower Canada where he farmed unsuccessfully for three years, originally in an attempt to establish a commune with two of his religious friends. Nevertheless, the experience deepened his love for natural history, and locals referred to him as "that crazy Englishman who goes about picking up bugs." During this time he became a member of the Natural History Society of Montreal and submitted specimens to its museum.

In 1838 Gosse taught eight months for Reuben Saffold, the owner of Belvoir plantation, near Pleasant Hill, Alabama. Gosse studied and drew the local flora and fauna, assembling an unpublished volume, "Entomologia Alabamensis", on insect life in the state. He also recorded his negative impressions of slavery, later published as "Letters from Alabama" (1859).

Returning to England in 1839, Gosse was hard pressed to make a living, subsisting on eightpence a day ("one herring eaten as slowly as possible, and a little bread"). His fortunes began to improve when John Van Voorst, the leading publisher of naturalist writing, agreed, on the recommendation of Thomas Bell, to publish his "Canadian Naturalist" (1840). The book, set as a conversation between a father and his son (a son Gosse did not yet have), was widely praised and demonstrated that Gosse "had a practical grasp of the importance of conservation, far ahead of his time."

Gosse opened a "Classical and Commercial School for Young Gentlemen" while keeping detailed records of his microscopic investigations of pond life, especially cyclopidae and rotifera. He also began to preach to the Wesleyan Methodists and lead a Bible class. Nevertheless, in 1842, he became so captivated by the doctrine of the Second Coming of Christ that he severed his connection with the Methodists and joined the Plymouth Brethren. These dissenters emphasized the Second Coming while rejecting liturgy and an ordained ministry—although they otherwise endorsed the traditional doctrines of Christianity as represented by the creeds of the Methodist and the Anglican Church.

In 1843, Gosse gave up the school to write "An Introduction to Zoology" for the Society for Promoting Christian Knowledge (SPCK) and to draw some of the illustrations. Writing the work inspired him to further his interest in the flora and fauna of the seashore and also revealed him to be a determined creationist, although this position was typical of pre-Darwinian naturalists.
In October 1844 Gosse sailed to Jamaica, where he served as a professional collector for the churlish dealer Hugh Cuming. Although Gosse worked hard during his eighteen months on the island, he later called this period his ""holiday" in Jamaica." Gosse's study specialized in birds, and Gosse has been called "the father of Jamaican ornithology." With no racial prejudice, he easily hired black youths as his assistants, and his Jamaican books are full of praise for one of them, Samuel Campbell. For Christian companionship he enjoyed the company of Moravian missionaries and their black converts and preached regularly to the Moravian congregation.

On his return to London in 1846, he wrote a trilogy on the natural history of Jamaica including "A Naturalist's Sojourn in Jamaica" (1851), which was "written in a congenial style and firmly established his reputation both as a naturalist and a writer."

In the field of herpetology he described several new species of reptiles endemic to Jamaica.

Back in England, Gosse wrote books in his field and out; one quick volume for the SPCK was "Monuments of Ancient Egypt", a land he had never visited and never would. As his financial situation stabilized, Gosse courted Emily Bowes, a forty-one-year-old member of the Brethren, who was both a strong personality and a gifted writer of evangelical tracts. They were married in November 1848, and their union was an extremely happy one. As D. J. Taylor has written, "the word 'uxorious' seems to have minted to define" Gosse. Gosse's only son was born on 21 September 1849, an event Gosse noted in his diary with the words, "E. delivered of a son. Received green swallow from Jamaica"—an amusing conjunction which Edmund later described as demonstrating only the order of events: the boy had arrived first.

Gosse penned a succession of books and articles on natural history, some of which were (in his own words) "pot-boilers" for religious publications. (At the time, accounts of God's creation were considered appropriate Sabbath reading for children.) As L. C. Croft has written, "Much of Gosse's success was due to the fact that he was essentially a field naturalist who was able to impart to his readers something of the thrill of studying living animals at first hand rather than the dead disjointed ones of the museum shelf. In addition to this he was a skilled scientific draughtsman who was able to illustrate his books himself."

Suffering from headaches, perhaps the result of overwork, Gosse, with his family, began to spend more time away from London on the Devon coast. Here along the sea shore Gosse began serious experimentation with ways to sustain sea creatures so that they could be examined "without diving to gaze on them." Although there had been attempts to construct what had previously been called an "aquatic vivarium" (a name Gosse found "awkward and uncouth"), Gosse published "The Aquarium" in 1854 and set off a mid-Victorian craze for household aquariums. The book was financially profitable for Gosse, and "the reviews were full of praise" even though Gosse used natural science to point to the necessity of salvation through the blood of Christ. In 1856 Gosse was elected a Fellow of the Royal Society, which, because he had no university position or inherited wealth, gave him "a standing he otherwise lacked."

A few months before Gosse was honoured, his wife discovered that she had breast cancer. Rather than undergo surgery (a risky procedure in 1856), the Gosses decided to submit to the ointments of an American doctor, Jesse Weldon Fell, who if not a charlatan, was certainly on the fringe of contemporary medical practice. After much suffering, Emily Gosse died on 9 February 1857, entrusting her husband with their son's salvation and thus perhaps driving Gosse into "strange severities and eccentric prohibitions."

In the months following Emily's death, Gosse worked with remarkable diligence on a book that he may have viewed as the most important of his career. Although a failure both financially and intellectually, it is the book by which he is best remembered. Gosse believed that he had discovered a theory that might neatly resolve the seeming contradiction in the age of the earth between the evidence of God's Word and the evidence of His creation as expounded by such contemporary geologists as Charles Lyell. In 1857, two years before the publication of Charles Darwin's "Origin of Species", Gosse published "Omphalos: an Attempt to Untie the Geological Knot" and thereby created what has been called the Omphalos hypothesis.

In what Stephen Jay Gould has called "glorious purple prose," Gosse argued that if one assumed creation "ex nihilo", there would necessarily be traces of previous existence that had never actually occurred. ""Omphalos"" is Greek for ""navel"," and Gosse argued that the first man, Adam, did not require a navel because he was never born; nevertheless he must have had one, as do all complete human beings, just as God must have created trees with rings that they never grew. Thus, Gosse argued that the fossil record—even coprolites—might also be evidence of life that had never actually existed but which may have been instantly formed by God at the moment of creation.

The general response was "as the "Westminster Review" put it, that Gosse's theory was 'too monstrous for belief.'" Even his friend, the novelist Charles Kingsley, wrote that he had read "no other book which so staggered and puzzled" him, that he could not believe that God had "written on the rocks one enormous and superfluous lie for all mankind." Journalists later sniggered that God had apparently hidden fossils in the rocks to tempt geologists to infidelity.

"Omphalos" sold poorly and was eventually rebound with a new title, "Creation", "in case the obscure one had had an effect on sales." The problem was not with the title, and in 1869 most of the edition was sold as waste paper.

According to Edmund Gosse, his father's career was destroyed by his "strange act of wilfulness" in publishing "Omphalos"; Edmund claimed his father had "closed the doors upon himself forever." In fact, during the next three years Gosse published more than thirty scientific papers and four books.

By this time Gosse and his son had moved permanently from London to St Marychurch, Devon. (Gosse refused to use the "St" and even gave his address as Torquay so as not to have anything to do with the "so-called Church of England.") He soon became the pastor and overseer of the Brethren meeting, at first over a stable but shortly, under Gosse's preaching and peacemaking, in finer quarters—which he perhaps financed himself. His son said "he soon lost confidence in the Plymouth Brethren also, and for the last thirty years of his life he was really unconnected with any Christian body whatsoever."

During this period, Gosse made a special study of sea anemone (Actiniae) and in 1860 published "Actinologia Britannica". Reviewers especially praised the colour lithographs made from Gosse's watercolours. The "Literary Gazette" said that Gosse now stood "alone and unrivalled in the extremely difficult art of drawing objects of zoology so as to satisfy the requirements of science" as well as providing "vivid aesthetic impressions."

In 1860 he also met and married Eliza Brightwen (1813–1900), a kindly, tolerant Quaker who shared Gosse's intense interest in both natural history and the well-being of his son. Gosse's second marriage was as happy as his first. In 1862 he wrote that Eliza was "a true yoke-fellow, in love, in spirit and in service."

By this time Gosse was "very comfortably off" with the earnings from his books and dividends from his investments, and in 1864 Eliza received a substantial legacy which allowed Gosse to retire from his career as a professional writer and live in "congenial obscurity." The Gosses lived simply, invested some of their income and gave more away to charity, especially to foreign missionaries, including ones sent to the "Popish, priest-ridden Irish."

To Gosse's great grief, his son rejected Christianity—though almost certainly not as early or as dramatically as Edmund portrayed the break in "Father and Son". Nevertheless, Henry sponsored the publication of Edmund's early poetry, which gave the younger man entrée to new friends of literary importance, and the two men "came out of the years of conflict with their relationship wary but intact." Henry and Eliza welcomed Edmund's wife to the family and enjoyed visits with their three grandchildren.
Meanwhile, the ever active Gosse had taken up the study of orchids and exchanged a number of letters on the subject with Darwin, though he never published on it himself. His penultimate enthusiasm was with the genitalia of butterflies about which he published a paper in the "Transactions of the Linnean Society" But before his death he returned to rotifera, much of his research appearing in a two-volume study with another zoologist, Charles Thomas Hudson.

His wife recalled that Gosse's final illness was triggered by his enthusiasm to adjust his telescope at an open window on a winter night. Gosse had prayed regularly that he might not taste death but meet Christ in the air at his Second Coming, and he was bitterly disappointed when he realized that he would die like everyone else.

After his father's death, Edmund Gosse published a typical Victorian biography, "The Life of Philip Henry Gosse" (1890). Nevertheless, after reading the latter, the writer George Moore suggested to Edmund that it contained "the germ of a great book," which Edmund Gosse first published anonymously as "Father and Son" in 1907. It has never gone out of print in more than a hundred years. The reaction of readers to Henry's personality and character as represented in "Father and Son" has included phrases such as "scientific crackpot," "bible-soaked romantic," "a stern and repressive father," and a "pulpit-thumping Puritan throwback to the seventeenth century."

Even a modern editor of "Father and Son" has rejected this portrait of Philip Henry Gosse on the grounds that his "writings reveal a genuinely sweet character." The biographer of both Gosses, Ann Thwaite, has established just how inaccurate Edmund's recollections of his childhood were, that Edmund indeed, as Henry James remarked, had "a genius for inaccuracy." Although Edmund went out of his way to declare that the story of "Father and Son" was "scrupulously true," Thwaite cites a dozen occasions on which either Edmund's "memory betray[ed] him—he admitted it was 'like a colander'"—or he "changed things deliberately to make a better story." Thwaite argues that Edmund could only preserve his self-respect, in comparison to his father's superior abilities, by demolishing the latter's character.

Dennis Potter dramatised "Father and Son" in the television play "Where Adam Stood", first broadcast on BBC One in 1976. Gosse was played by Alan Badel and portrayed more sympathetically than in Edmund Gosse's book.
"Father and Son" was also adapted for BBC Radio 4 in 2005 by Nick Warburton. Roger Allam played Gosse and Derek Jacobi, Edmund.
Ann Lingard's novel Seaside Pleasures views the relationship between Gosse and his wife Emily from the point of view of one of the female students on his shore-class.






</doc>
<doc id="24466" url="https://en.wikipedia.org/wiki?curid=24466" title="List of Polish composers">
List of Polish composers

This is a list of notable and representative Polish composers.

Note: This list should contain notable composers, best with an existing article on Wikipedia. If a notable Polish composer is , please add the name .









</doc>
<doc id="24468" url="https://en.wikipedia.org/wiki?curid=24468" title="President of the European Commission">
President of the European Commission

The President of the European Commission is the head of the European Commission, the executive branch of the :European Union. The President of the Commission leads a cabinet of Commissioners, referred to as the "college", collectively accountable to the European Parliament, which is directly elected by EU citizens. The President is empowered to allocate portfolios amongst, reshuffle or dismiss Commissioners as necessary. The college directs the Commission's civil service, sets the policy agenda and determines the legislative proposals it produces (the Commission is the only body that can propose EU laws).

The President of the Commission also represents the EU abroad, together with the President of the European Council and the High Representative of the Union for Foreign Affairs and Security Policy.

The post was established in 1958. Each new President is nominated by the European Council and formally elected by the European Parliament, for a five-year term. , the current President is Jean-Claude Juncker, who took office on 1 November 2014. He is a member of the European People's Party (EPP) and is the former Prime Minister of Luxembourg. Juncker is the twelfth President and his First Vice-President is Frans Timmermans.

The present Commission was established by the Treaty of Rome in 1957; it also replaced the High Authority and the Commission of Euratom in 1967. The Commission's first president was Walter Hallstein (see Hallstein Commission) who started consolidating European law and began to impact on national legislation. National governments at first took little heed of his administration, with the President having to stamp the Commission's authority early on. With the aid of the European Court of Justice, the Commission began to be taken more seriously.

In 1965 Hallstein put forward his proposals for the Common Agricultural Policy, which would give the Community its own financial resources while giving more power to the Commission and Parliament and removing the veto power over Agriculture in the Council. These proposals led to an immediate backlash from France. Hallstein knew the proposals would be contentious, and took personal charge of drafting them, over-riding the Agriculture Commissioner. However he did gain the support of Parliament through his proposals to increase its powers, and he also presented his policy to Parliament a week before he submitted them to the Council. He aimed to demonstrate how he thought the Community ought to be run, in the hopes of generating a wave of pro-Europeanism big enough to get past the objections of member states. However, in this it proved that, despite its past successes, Hallstein was overconfident in his risky proposals.

In reaction to Hallstein's proposals and actions, then-French President Charles de Gaulle, who was sceptical of the rising supranational power of the Commission, accused Hallstein of acting as if he were a head of state. France eventually withdrew its representative from the Council, triggering the notorious "empty chair crisis". Although this was resolved under the "Luxembourg compromise", Hallstein became the scapegoat for the crisis. The Council refused to renew his term, despite his being the most 'dynamic' leader until Jacques Delors.

Hallstein's work did position the Commission as a substantial power. The presidents were involved in the major political projects of the day in the 1970s, such as the European Monetary Union. In 1970, President Jean Rey secured the Community's own financial resources and in 1977, President Roy Jenkins became the first Commission President to attend a G7 summit on behalf of the Community.

However, owing to problems such as the 1973 oil crisis and the 1979 energy crisis, economic hardship reduced the priority of European integration, with only the President trying to keep the idea alive. The member states had the upper hand, and they created the European Council to discuss topical problems, yet the Council was unable to keep the major projects on track such as the Common Agricultural Policy. The Community entered a period of eurosclerosis, owing to economic difficulties and disagreements on the Community budget, and by the time of the Thorn Commission the President was unable to exert his influence to any significant extent.

However, the Commission began to recover under President Jacques Delors' Commission. He is seen as the most successful President, being credited with having given the Community a sense of direction and dynamism. The "International Herald Tribune" noted the work of Delors at the end of his second term in 1992: "Mr. Delors rescued the European Community from the doldrums. He arrived when Europessimism was at its worst. Although he was a little-known (outside France) finance minister and former MEP, he breathed life and hope into the EC and into the dispirited Brussels Commission. In his first term, from 1985 to 1988, he rallied Europe to the call of the single market, and when appointed to a second term he began urging Europeans toward the far more ambitious goals of economic, monetary and political union."

But Delors not only turned the Community around, he signalled a change in the Presidency. Before he came to power, the Commission President still was a position of first among equals; when he left office, he was the undisputed icon and leader of the Community. His tenure had produced a strong Presidency and a strong Commission as the President became more important. Following treaties cemented this change, with the President being given control over the allocation of portfolios and being able to force the resignation of Commissioners. When President Romano Prodi took office with the new powers of the Treaty of Amsterdam, he was dubbed by the press as Europe's first Prime Minister. President Delors' work had increased the powers of Parliament, whose support he had enjoyed. However, later Commissions did not enjoy the same support, and in 1999, the European Parliament used its powers to force the Santer Commission to resign.

Historically, the Council appointed the Commission President and the whole body by unanimity without input from Parliament. However, with the Treaty on European Union in 1993, the European Parliament, the body elected directly by the citizens of the European Union, gained the right to be 'consulted' on the appointment of the President and to veto the Commission as a whole. Parliament decided to interpret its right to be consulted as a right to veto the President, which the Council reluctantly accepted This right of veto was formalised in the Amsterdam Treaty. The Treaty of Nice changed the Council's vote from a unanimous choice to one that merely needed a qualified majority. This meant that the weight of the Parliament in the process increased resulting in a quasi-parliamentary system where one group could be 'in government'. This became evident when numerous candidates were put forward in 2004, and a centre-right vote won out over left-wing groups, France and Germany. José Manuel Barroso, elected Commission President that year, was then forced to back down over his choice of Commissioners, owing to Parliament's threat that it would not approve his Commission.

In 2009, the European People's Party endorsed Barroso as its candidate for Commission President, and the EPP subsequently retained its position as largest party in that year's election. The Socialists responded by pledging to put forward a rival candidate at future elections. Once again, Barroso was forced by Parliament to make a change to his proposed Commission, but eventually received assent. However, in exchange for approval, Parliament forced some concessions from Barroso in terms of Parliamentary representation at Commission and international meetings. On 7 September 2010, Barroso gave the first US-style State of the Union address to Parliament; which focused primarily on the EU's economic recovery and human rights. The speech was to be annual.

Article 17 of the Treaty on European Union, as amended by the Treaty of Lisbon, lays out the procedure for appointing the President and his team. The European Council votes by qualified majority for a nominee for the post of President, taking account of the latest European elections. This proposal is then put before Parliament which must approve or veto the appointment. If an absolute majority of MEPs support the nominee, he/she is elected. The President then, together with the Council, puts forward his team to the Parliament to be scrutinised. The Parliament normally insists that each one of them appear before the parliamentary committee that corresponds to their prospective portfolio for a public hearing. The Parliament then votes on the Commission as a whole; if approved, the European Council, acting by a qualified majority, appoints the President and his team to office.

Qualified majority in the Council has led to more candidates being fielded while there has been greater politicisation due to the involvement of Parliament and the change of policy direction in the EU from the creation of the single market to reform of it. However, despite this, the choice within the Council remains largely behind closed doors. During the appointment of Santer, discussions were kept in private with the media relying on insider leaks. MEPs were angry with the process, against the spirit of consultation that the new EU treaty brought in. Pauline Green MEP, leader of the Socialist group, stated that her group thought "Parliament should refuse to condone a practice which so sullies the democratic process". There were similar deals in 1999 and 2004 saw a repeat of Santer's appointment when Barroso was appointed through a series of secret meetings between leaders with no press releases on the negotiations being released. This was sharply criticised by MEPs such as the ALDE group leader Graham Watson who described the procedure as a "Justus Lipsius carpet market" producing only the "lowest common denominator"; while Green-EFA co-leader Daniel Cohn-Bendit asked Barroso after his first speech "If you are the best candidate, why were you not the first?"

The candidate selected by the Council has often been a leading national politician but this is not a requirement. The choice of President must take into account the result of the latest Parliamentary elections (i.e. the candidate supported by the victorious European political party in particular, or at least someone from that political family). That provision was not in force in the nomination in 2004, but the centre-right European People's Party (EPP), which won the elections, pressured for a candidate from its own ranks. In the end, the EPP candidate, José Manuel Barroso was chosen. On the same basis, the EPP endorsed again Barroso for a second term during the 2009 European elections campaign and, after winning again the elections, was able to secure his nomination by the European Council.

Further criteria seen to be influencing the choice of the Council include: which area of Europe the candidate comes from, favoured as Southern Europe in 2004; the candidate's political influence, credible yet not overpowering members; language, proficiency in French considered necessary by France; and degree of integration, their state being a member of both the eurozone and the Schengen Agreement.

There is an assumption that there is a rolling agreement along these lines that a president from a large state is followed by a president from a small state, and one from the political left will be followed by one from the political right: Roy Jenkins (British socialist) was followed by Gaston Thorn (Luxembourg liberal), Jacques Delors (French socialist), Jacques Santer (Luxembourg Christian democrat), Romano Prodi (Italian left wing Christian democrat) and Jose Barroso (Portuguese Christian democrat). However, despite these assumptions these presidents have usually been chosen during political battles and coalition building. Delors was chosen following a Franco-British disagreement over Claude Cheysson, Santer was a compromise after Britain vetoed Jean-Luc Dehaene and Prodi was backed by a coalition of thirteen states against the Franco-German preference for Guy Verhofstadt. 

In February 2008, President Barroso admitted that despite the President having in theory as much legitimacy as heads of governments, in practice it was not the case. The low voter turnout creates a problem for the President's legitimacy, with the lack of a "European political sphere", but analysts claim that if citizens were voting for a list of candidates for the post of President, turn out would be much higher than that seen in recent years.

Under the Treaty of Lisbon the European Council has to take into account the results of the latest European elections and, furthermore, the Parliament elects, rather than simply approve, the Council's proposed candidate. This was taken as the parliament's cue to have its parties run with candidates for the President of the Commission with the candidate of the winning party being proposed by the Council. This was partly put into practice in 2004 when the European Council selected a candidate from the political party which secured a plurality of votes in that year's election. However at that time only a minor party had run with a specific candidate: the then fourth placed European Green Party, who had the first true pan-European political party with a common campaign, put forward Daniel Cohn-Bendit and lost even their fourth place in the following election becoming only the fifth largest group in 2009 and diminishing their candidate's chances further. However the winning European People's Party only mentioned four or five people as candidates for President.

There have been plans to strengthen the European political parties in order for them to propose candidate for future elections. The European Liberal Democrat and Reform Party indicated, in its October 2007 congress, its intention to forward a candidate for the post as part of a common campaign but failed to do so. However the European People's Party did select Barroso as their candidate and, as the largest party, Barroso's turn was renewed.

The Socialists, disappointed at the 2009 election, agreed to put forward a candidate for Commission President at all subsequent elections. After a campaign within that party to have open primaries for said candidate, the PES Congress gathering in Brussels in November 2011 decided that PES would designate its candidate for Commission president through primaries took place in January 2014 in each of its member parties and organisations, before a ratification of the results by an Extraordinary PES Congress in February 2014.

The "Spitzenkandidat" (German for "Lead Candidate") process is the method of linking European Parliament elections by having each major political group in Parliament nominating their candidate for Commission President prior to the Parliamentary elections (the Spitzenkandidaten). The Spitzenkandidat of the largest party would then have a mandate to assume the Commission Presidency. This process was first run in 2014 and its legitimacy was contested by the Council.

According to the treaties, the President of the European Commission is nominated by the European Council. Until 2004, this nomination was based on an informal consensus for a common candidate. But in 2004 the centre-right European People's Party rejected the consensus approach ahead of the European Council meeting and pushed through their own candidate, Barroso. The approach of national governments was to appoint the various high-profile jobs in EU institutions (European Council President, High Representative and so on) dividing them according along geographic, political and gender lines. This also led to fairly low-profile figures in some cases, as it avoided candidates who had either made enemies of some national governments or who were seen as potentially challenging the Council or certain member states.

Unease had built up around the way the secretive, power play that was involved in these appointments leading to a desire for a more democratic process. At the end of 2009, the Treaty of Lisbon entered into force. It amended the appointment of the Commission President in the Treaty on European Union Article 17.7 to add the wording "taking into account the elections to the European Parliament", so that Article 17.7 now included the wording 

In 2013, in preparation for the European elections of 2014, Martin Schulz, then President of the European Parliament campaigned for the parliamentary party groups to name lead candidates for the post of President of the European Commission; his own party group, the centre-left Progressive Alliance of Socialists and Democrats named Schulz as their lead candidate (). The Alliance of Liberals and Democrats for Europe Group and The Greens–European Free Alliance then named their own lead candidates, as did the European People's Party. The German term for lead candidates caught on, and they became known informally as '. The European Conservatives and Reformists, the third largest of the political groups, did not name a candidate, objecting to the principle of ' and its "tenuous" basis in law.

The People's Party won a plurality (29%) in the 2014 elections, and Jean-Claude Juncker, its lead candidate, was appointed as president by the European Council. British Prime Minister David Cameron and Hungarian Prime Minister Viktor Orbán were the only members of the council to object to his selection.

Some commentators argued that this amendment did not entitle the political parties of the Parliament to nominate candidates for the president of the Commission, and that such an interpretation would amount to a "power grab" at the expense of the European Council. The Council found itself taken off guard by how the process took off, and had backed themselves into a corner in having to approve the Parliament's candidate. Following the appointment, leaders vowed to review the process.

It has also been argued that it is still insufficiently democratic and needs to be replaced with a more direct system. Some suggestions toward this have been electing the President via a transnational list, having a direct election, and holding primary elections. Parliamentary proposals to enact some of these in advance of the 2019 election have been opposed by some in the Council.

The President is elected for a renewable five-year term starting five months after the elections to the European Parliament. These were brought into alignment via the Maastricht Treaty (prior to which the Commission had a four-year term of office) and the elections take place in June every five years (in years ending in 4 and 9). This alignment has led to a closer relationship between the elections and the President himself with the above-mentioned proposals for political parties running with candidates.

The President and his Commission may be removed from office by a vote of censure from Parliament. Parliament has never done this to date, however the imminence of such a vote in 1999, due to allegations of financial mismanagement, led to the Santer Commission resigning on its own accord, before the Parliamentary vote.

The President of the European Commission is the most powerful position in the European Union, controlling the Commission which collectively has the right of initiative on Union legislation (only on matters delegated to it by member states for collective action, as determined by the treaties) and is responsible for ensuring its enforcement. The President controls the policy agenda of the Commission for his term and in practice no policy can be proposed without the President's agreement. 

The role of the President is to lead the Commission, and give direction to the Commission and the Union as a whole. The treaties state that "the Commission shall work under the political guidance of its President" (Article 219 TEC), this is conducted through his calling and chairing of meetings of the college of Commissioners, his personal cabinet and the meetings of the heads of each commissioner's cabinet (the Hebdo). The president may also force a Commissioner to resign. The work of the Commission as a body is based on the principle of Cabinet collective responsibility, however in his powers he acts as more than a first among equals. The role of the President is similar to that of a national Prime Minister chairing a cabinet. 

The President also has responsibility for representing the Commission in the Union and beyond. For example, he is a member of the European Council and takes part in debates in Parliament and the Council of Ministers. Outside the Union he attends the meetings of the G8 to represent the Union. However, in foreign affairs, he does have to compete with several Commissioners with foreign affairs related portfolios: the High Representative for the Common Foreign and Security Policy and the President of the European Council.

The Presidential system had started to develop since Jacques Delors and has since been cemented. However, externally he is still dependent on support from the Council and Parliament. Delors had enjoyed the Parliament's and the Council's support for his whole term, during which, through treaty changes, the Parliament increased in powers and, through the accession of new Member States, the Council increased in membership. The membership is now so large the President is increasingly unable to garner the support of all the states, even though the job is supposed to try to keep everyone happy. The Parliament now has more powers over the Commission and can reject its proposals, although the Commission has little power over Parliament, such as the ability to dissolve it to call new elections.

The President's office is on the top, 13th, floor of the Berlaymont building in Brussels. The president receives his political guidance from his cabinet, the head of which acts as a political bodyguard for the President. Such factors can lead to an isolation of the President from outside events. For the European Civil Service the President has a very high status, due to his immense authority and symbolism within the body. The President exercises further authority through the legal service and Secretariat-General of the Commission. The former has the power to strike down proposals on legal technicalities while the latter organises meetings, agendas and minutes. His control over these areas gives the President further political tools when directing the work of the Commission. This has also increased the Presidential style of the Commission President.

With the reorganisation of leading EU posts under the Lisbon Treaty, there was some criticism of each posts vague responsibilities. Ukrainian ambassador to the EU Andriy Veselovsky praised the framework and clarified it in his own terms: The Commission President speaks as the EU's "government" while the President of the European Council is a "strategist". The High Representative specialises in "bilateral relations" while the European Commissioner for Enlargement and European Neighbourhood Policy deals in technical matters such as the free trade agreement with Ukraine. The President of the European Parliament meanwhile articulates the EU's values.

The MEP and author of several EU text books Richard Corbett has suggested that, instead of every EU institution having a "President", it would have been clearer if they had been named differently, with a "Speaker" of the Parliament, a "Governor" of the Central Bank, a "Chairman" of the (ordinary) Council of Ministers, a "President" of the European Council - and a "Prime Commissioner".

Despite the recent presidential style, the president has also begun to lose ground to the larger member states as countries such as France, Italy, the UK and Germany seek to sideline its role. This may increase with the recent creation of the permanent President of the European Council. There has been disagreement and concern over competition between the President of the European Council Van Rompuy and the Commission President Barroso due to the vague language of the treaty. Some clarifications see Van Rompuy as the "strategist" and Barroso as a head of government. In terms of economic planning Van Rompuy saw the Commission as dealing with the content of the plan and the European Council as dealing with the means and implementing it. Despite weekly breakfasts together there was a certain extent of rivalry between the two, as well as with the High Representative. At international summits, both Presidents go at the same time to represent the Union, with, in principle, the Commission President speaking on economic questions and the European Council President on political questions, although this division is often hard to maintain in practice.

Although there are concerns that this competition with the new European Council President would lead to increased infighting, there are provisions for combining the two offices. The European Council President may not hold a national office, such as a Prime Minister of a member state, but there is no such restraint on European offices. So the Commission President, who already sits in the European Council, could also be appointed as its president. This would allow the European Council to combine the position, with its powers, of both executive bodies into a single President of the European Union.

The basic monthly salary of the President is fixed at 138% of the top civil service grade which, in 2013, amounted to €25,351 per month or €304,212 per year plus an allowance for a residence equal to 15% of salary as well as other allowances including for children's schooling and household expenses.

This section firstly presents a lists over presidents of the three executives that were merged in 1967 following the Merger Treaty, namely the High Authority of the European Coal and Steel Community (from 1952), and the commissions of the European Atomic Energy Community and the European Economic Community (both from 1958). Secondly, a list is given over the presidents after the merger, when the single position presided over the Commission of the European Communities, until 2009 when the Treaty of Lisbon renamed of the institution, creating the President of the European Commission.

The European Economic Community was established by the Treaty of Rome, presently known as the Treaty on the Functioning of the European Union; a founding treaty of the union, which explains that the enumeration of presidents which ends with the present position starts with the first President of the Commission of the European Economic Community. The European Union is also the legal successor of the European Economic Community, or the European Community as it was named between 1993 and 2009. The establishment of the European Union in 1993 upon the entry into force of the Maastricht Treaty (formally the Treaty on European Union) did not affect the name of the position.

Upon its entry into force in 2009, the Treaty of Lisbon renamed the Commission of the European Communities the European Commission, reflecting the "de facto" name as well as the fact that the European Communities pillar was abolished along with the rest of the pillar system.





</doc>
<doc id="24471" url="https://en.wikipedia.org/wiki?curid=24471" title="Phonograph">
Phonograph

The phonograph is a device for the mechanical recording and reproduction of sound. In its later forms, it is also called a gramophone (as a trademark since 1887, as a generic name in the UK since 1910), or, since the 1940s, a record player. The sound vibration waveforms are recorded as corresponding physical deviations of a spiral groove engraved, etched, incised, or impressed into the surface of a rotating cylinder or disc, called a "record" or "vinyl". To recreate the sound, the surface is similarly rotated while a playback stylus traces the groove and is therefore vibrated by it, very faintly reproducing the recorded sound. In early acoustic phonographs, the stylus vibrated a diaphragm which produced sound waves which were coupled to the open air through a flaring horn, or directly to the listener's ears through stethoscope-type earphones. 

The phonograph was invented in 1877 by Thomas Edison. While other inventors had produced devices that could record sounds, Edison's phonograph was the first to be able to reproduce the recorded sound. His phonograph originally recorded sound onto a tinfoil sheet wrapped around a rotating cylinder. A stylus responding to sound vibrations produced an up and down or hill-and-dale groove in the foil. Alexander Graham Bell's Volta Laboratory made several improvements in the 1880s and introduced the "graphophone", including the use of wax-coated cardboard cylinders and a cutting stylus that moved from side to side in a zigzag groove around the record. In the 1890s, Emile Berliner initiated the transition from phonograph cylinders to flat discs with a spiral groove running from the periphery to near the center, coining the term "gramophone" for disc record players, which is predominantly used in many languages. Later improvements through the years included modifications to the turntable and its drive system, the stylus or needle, and the sound and equalization systems.

The disc phonograph record was the dominant audio recording format throughout most of the 20th century. From the mid-1980s on, phonograph use on a standard record player declined sharply because of the rise of the cassette tape, compact disc and other digital recording formats. Records are still a favorite format for some audiophiles and by DJs and turntablists (particularly in hip hop and electronic dance music), and has recently seen a somewhat revival in the 2010s. The original recordings of musicians, which may have been recorded on tape or digital methods, are sometimes re-issued on vinyl.

Usage of terminology is not uniform across the English-speaking world (see below). In more modern usage, the playback device is often called a "turntable", "record player", or "record changer". When used in conjunction with a mixer as part of a DJ setup, turntables are often colloquially called "decks". In later electric phonographs (more often known since the 1940s as record players or, most recently, turntables), the motions of the stylus are converted into an analogous electrical signal by a transducer, then converted back into sound by a loudspeaker.

The term "phonograph" ("sound writing") was derived from the Greek words ("phonē", "sound" or "voice") and ("graphē", "writing"). The similar related terms "gramophone" (from the Greek γράμμα "gramma" "letter" and φωνή "phōnē" "voice") and "graphophone" have similar root meanings. The roots were already familiar from existing 19th-century words such as "photograph" ("light writing"), "telegraph" ("distant writing"), and "telephone" ("distant sound"). The new term may have been influenced by the existing words "phonographic" and "phonography", which referred to a system of phonetic shorthand; in 1852 "The New York Times" carried an advertisement for "Professor Webster's phonographic class", and in 1859 the New York State Teachers Association tabled a motion to "employ a phonographic recorder" to record its meetings.

Arguably, any device used to record sound or reproduce recorded sound could be called a type of "phonograph", but in common practice the word has come to mean historic technologies of sound recording, involving audio-frequency modulations of a physical trace or groove. In the late-19th and early-20th centuries, "Phonograph", "Gramophone", "Graphophone", "Zonophone" and the like were still brand names specific to various makers of sometimes very different (i.e. cylinder and disc) machines; so considerable use was made of the generic term "talking machine", especially in print. "Talking machine" had earlier been used to refer to complicated devices which produced a crude imitation of speech, by simulating the workings of the vocal cords, tongue, and lips – a potential source of confusion both then and now.

In British English, "gramophone" may refer to any sound-reproducing machine using disc records, which were introduced and popularized in the UK by the Gramophone Company. Originally, "gramophone" was a proprietary trademark of that company and any use of the name by competing makers of disc records was vigorously prosecuted in the courts, but in 1910 an English court decision decreed that it had become a generic term; it has been so used in the UK and most Commonwealth countries ever since. The term "phonograph" was usually restricted to machines that used cylinder records.

"Gramophone" generally referred to a wind-up machine. After the introduction of the softer vinyl records, -rpm LPs (long-playing records) and 45-rpm "single" or two-song records, and EPs (extended-play recordings), the common name became "record player" or "turntable". Often the home record player was part of a system that included a radio ("radiogram") and, later, might also play audiotape cassettes. From about 1960, such a system began to be described as a "hi-fi" (high-fidelity, monophonic) or a "stereo" (most systems being stereophonic by the mid-1960s).

In American English, "phonograph", properly specific to machines made by Edison, was sometimes used in a generic sense as early as the 1890s to include cylinder-playing machines made by others. But it was then considered strictly incorrect to apply it to Emile Berliner's upstart Gramophone, a very different machine which played discs. "Talking machine" was the comprehensive generic term, but in the early 20th century the general public was increasingly applying the word "phonograph" indiscriminately to both cylinder and disc machines and to the records they played. By the time of the First World War, the mass advertising and popularity of the Victor Talking Machine Company's Victrolas (a line of disc-playing machines characterized by their concealed horns) was leading to widespread generic use of the word "victrola" for any machine that played discs, which were however still called "phonograph records" or simply "records", almost never "victrola records".

After electrical disc-playing machines started appearing on the market during the second half of the 1920s, usually sharing the same cabinet with a radio receiver, the term "record player" was increasingly favored by users when referring to the device. Manufacturers, however, typically advertised such combinations as "radio-phonographs". Portable record players (no radio included), with a latched cover and an integrated power amplifier and loudspeaker, were fairly common as well, especially in schools and for use by children and teenagers.

In the years following the Second World War, as "hi-fi" (high-fidelity, monophonic) and, later, "stereo" (stereophonic) component sound systems slowly evolved from an exotic specialty item into a common feature of American homes, the description of the record-spinning component as a "record changer" (which could automatically play through a stacked series of discs) or a "turntable" (which could hold only one disc at a time) entered common usage. By about 1980 the use of a "record changer", which might damage the stacked discs, was widely disparaged. So, the "turntable" emerged triumphant and retained its position to the end of the 20th century and beyond. Through all these changes, however, the discs have continued to be known as "phonograph records" or, much more commonly, simply as "records".

The brand name "Gramophone" was not used in the USA after 1901, and the word fell out of use there, although it has survived in its nickname form, "Grammy", as the name of the Grammy Awards. The Grammy trophy itself is a small rendering of a gramophone, resembling a Victor disc machine with a taper arm.

Modern amplifier-component manufacturers continue to label the input jack which accepts the output from a modern magnetic pickup cartridge as the "phono" input, abbreviated from "phonograph".

In Australian English, "record player" was the term; "turntable" was a more technical term; "gramophone" was restricted to the old mechanical (i.e., wind-up) players; and "phonograph" was used as in British English.

Several inventors devised machines to record sound prior to Thomas Edison's phonograph, Edison being the first to invent a device that could both record and reproduce sound. The phonograph's predecessors include Édouard-Léon Scott de Martinville's phonautograph, and Charles Cros's paleophone. Recordings made with the phonautograph were intended to be visual representations of the sound, but were never sonically reproduced until 2008. Cros's paleophone was intended to both record and reproduce sound but had not been developed beyond a basic concept at the time of Edison's successful demonstration of the Phonograph in 1877.

Direct tracings of the vibrations of sound-producing objects such as tuning forks had been made by English physician Thomas Young in 1807, but the first known device for recording airborne speech, music and other sounds is the phonautograph, patented in 1857 by French typesetter and inventor Édouard-Léon Scott de Martinville. In this device, sound waves travelling through the air vibrated a parchment diaphragm which was linked to a bristle, and the bristle traced a line through a thin coating of soot on a sheet of paper wrapped around a rotating cylinder. The sound vibrations were recorded as undulations or other irregularities in the traced line. Scott's phonautograph was intended purely for the visual study and analysis of the tracings. Reproduction of the recorded sound was not possible with the original phonautograph.

In 2008, phonautograph recordings made by Scott were played back as sound by American audio historians, who used optical scanning and computer processing to convert the traced waveforms into digital audio files. These recordings, made circa 1860, include fragments of two French songs and a recitation in Italian.

Charles Cros, a French poet and amateur scientist, is the first person known to have made the conceptual leap from recording sound as a traced line to the theoretical possibility of reproducing the sound from the tracing and then to devising a definite method for accomplishing the reproduction. On April 30, 1877, he deposited a sealed envelope containing a summary of his ideas with the French Academy of Sciences, a standard procedure used by scientists and inventors to establish priority of conception of unpublished ideas in the event of any later dispute.

Cros proposed the use of photoengraving, a process already in use to make metal printing plates from line drawings, to convert an insubstantial phonautograph tracing in soot into a groove or ridge on a metal disc or cylinder. This metal surface would then be given the same motion and speed as the original recording surface. A stylus linked to a diaphragm would be made to ride in the groove or on the ridge so that the stylus would be moved back and forth in accordance with the recorded vibrations. It would transmit these vibrations to the connected diaphragm, and the diaphragm would transmit them to the air, reproducing the original sound.

An account of his invention was published on October 10, 1877, by which date Cros had devised a more direct procedure: the recording stylus could scribe its tracing through a thin coating of acid-resistant material on a metal surface and the surface could then be etched in an acid bath, producing the desired groove without the complication of an intermediate photographic procedure. The author of this article called the device a "phonographe", but Cros himself favored the word "paleophone", sometimes rendered in French as "voix du passé" (voice of the past) but more literally meaning "ancient sound", which accorded well with his vision of his invention's potential for creating an archive of sound recordings that would be available to listeners in the distant future.

Cros was a poet of meager means, not in a position to pay a machinist to build a working model, and largely content to bequeath his ideas to the public domain free of charge and let others reduce them to practice, but after the earliest reports of Edison's presumably independent invention crossed the Atlantic he had his sealed letter of April 30 opened and read at the December 3, 1877 meeting of the French Academy of Sciences, claiming due scientific credit for priority of conception.

Throughout the first decade (1890–1900) of commercial production of the earliest crude disc records, the direct acid-etch method first invented by Cros was used to create the metal master discs, but Cros was not around to claim any credit or to witness the humble beginnings of the eventually rich phonographic library he had foreseen. He had died in 1888 at the age of 45.

Thomas Alva Edison conceived the principle of recording and reproducing sound between May and July 1877 as a byproduct of his efforts to "play back" recorded telegraph messages and to automate speech sounds for transmission by telephone.
He announced his invention of the first "phonograph", a device for recording and replaying sound, on November 21, 1877 (early reports appear in Scientific American and several newspapers in the beginning of November, and an even earlier announcement of Edison working on a 'talking-machine' can be found in the Chicago Daily Tribune on May 9), and he demonstrated the device for the first time on November 29 (it was patented on February 19, 1878 as US Patent 200,521). "In December, 1877, a young man came into the office of the SCIENTIFIC AMERICAN, and placed before the editors a small, simple machine about which very few preliminary remarks were offered. The visitor without any ceremony whatever turned the crank, and to the astonishment of all present the machine said: "Good morning. How do you do? How do you like the phonograph?" The machine thus spoke for itself, and made known the fact that it was the phonograph..."

Edison presented his own account of inventing the phonograph. "I was experimenting," he said, "on an automatic method of recording telegraph messages on a disk of paper laid on a revolving platen, exactly the same as the disk talking-machine of to-day. The platen had a spiral groove on its surface, like the disk. Over this was placed a circular disk of paper; an electromagnet with the embossing point connected to an arm traveled over the disk; and any signals given through the magnets were embossed on the disk of paper. If this disc was removed from the machine and put on a similar machine provided with a contact point, the embossed record would cause the signals to be repeated into another wire. The ordinary speed of telegraphic signals is thirty-five to forty words a minute; but with this machine several hundred words were possible."

"From my experiments on the telephone I knew of how to work a pawl connected to the diaphragm; and this engaging a ratchet-wheel served to give continuous rotation to a pulley. This pulley was connected by a cord to a little paper toy representing a man sawing wood. Hence, if one shouted: ' Mary had a little lamb,' etc., the paper man would start sawing wood. I reached the conclusion that if I could record the movements of the diaphragm properly, I could cause such records to reproduce the original movements imparted to the diaphragm by the voice, and thus succeed in recording and reproducing the human voice."

"Instead of using a disk I designed a little machine using a cylinder provided with grooves around the surface. Over this was to be placed tinfoil, which easily received and recorded the movements of the diaphragm. A sketch was made, and the piece-work price, $18, was marked on the sketch. I was in the habit of marking the price I would pay on each sketch. If the workman lost, I would pay his regular wages; if he made more than the wages, he kept it. The workman who got the sketch was John Kruesi. I didn't have much faith that it would work, expecting that I might possibly hear a word or so that would give hope of a future for the idea. Kruesi, when he had nearly finished it, asked what it was for. I told him I was going to record talking, and then have the machine talk back. He thought it absurd. However, it was finished, the foil was put on; I then shouted 'Mary had a little lamb', etc. I adjusted the reproducer, and the machine reproduced it perfectly. I was never so taken aback in my life. Everybody was astonished. I was always afraid of things that worked the first time. Long experience proved that there were great drawbacks found generally before they could be got commercial; but here was something there was no doubt of."

The music critic Herman Klein attended an early demonstration (1881–2) of a similar machine. On the early phonograph's reproductive capabilities he writes "It sounded to my ear like someone singing about half a mile away, or talking at the other end of a big hall; but the effect was rather pleasant, save for a peculiar nasal quality wholly due to the mechanism, though there was little of the scratching which later was a prominent feature of the flat disc. Recording for that primitive machine was a comparatively simple matter. I had to keep my mouth about six inches away from the horn and remember not to make my voice too loud if I wanted anything approximating to a clear reproduction; that was all. When it was played over to me and I heard my own voice for the first time, one or two friends who were present said that it sounded rather like mine; others declared that they would never have recognised it. I daresay both opinions were correct."

Edison's early phonographs recorded onto a thin sheet of metal, normally tinfoil, which was temporarily wrapped around a helically grooved cylinder mounted on a correspondingly threaded rod supported by plain and threaded bearings. While the cylinder was rotated and slowly progressed along its axis, the airborne sound vibrated a diaphragm connected to a stylus that indented the foil into the cylinder's groove, thereby recording the vibrations as "hill-and-dale" variations of the depth of the indentation.

Playback was accomplished by exactly repeating the recording procedure, the only difference being that the recorded foil now served to vibrate the stylus, which transmitted its vibrations to the diaphragm and onward into the air as audible sound. Although Edison's very first experimental tinfoil phonograph used separate and somewhat different recording and playback assemblies, in subsequent machines a single diaphragm and stylus served both purposes. One peculiar consequence was that it was possible to overdub additional sound onto a recording being played back. The recording was heavily worn by each playing, and it was nearly impossible to accurately remount a recorded foil after it had been removed from the cylinder. In this form, the only practical use that could be found for the phonograph was as a startling novelty for private amusement at home or public exhibitions for profit.
Edison's early patents show that he was aware that sound could be recorded as a spiral on a disc, but Edison concentrated his efforts on cylinders, since the groove on the outside of a rotating cylinder provides a constant velocity to the stylus in the groove, which Edison considered more "scientifically correct".

Edison's patent specified that the audio recording be embossed, and it was not until 1886 that vertically modulated engraved recording using wax-coated cylinders was patented by Chichester Bell and Charles Sumner Tainter. They named their version the Graphophone.

The use of a flat recording surface instead of a cylindrical one was an obvious alternative which thought-experimenter Charles Cros initially favored and which practical experimenter Thomas Edison and others actually tested in the late 1870s and early 1880s. The oldest surviving example is a copper electrotype of a recording cut into a wax disc in 1881. The commercialization of sound recording technology was initially aimed at use for business correspondence and transcription into writing, in which the cylindrical form offered certain advantages, the storage of large numbers of records seemed unlikely, and the ease of producing multiple copies was not a consideration.

In 1887, Emile Berliner patented a variant of the phonograph which he named the Gramophone. Berliner's approach was essentially the same one proposed, but never implemented, by Charles Cros in 1877. The diaphragm was linked to the recording stylus in a way that caused it to vibrate laterally (side to side) as it traced a spiral onto a zinc disc very thinly coated with a compound of beeswax. The zinc disc was then immersed in a bath of chromic acid; this etched a groove into the disc where the stylus had removed the coating, after which the recording could be played. With some later improvements the flat discs of Berliner could be produced in large quantities at much lower cost than the cylinders of Edison's system.

In May 1889, in San Francisco, the first "phonograph parlor" opened. It featured a row of coin-operated machines, each supplied with a different wax cylinder record. The customer selected a machine according to the title that it advertised, inserted a nickel, then heard the recording through stethoscope-like listening tubes. By the mid-1890s, most American cities had at least one phonograph parlor. The coin-operated mechanism was invented by Louis T. Glass and William S. Arnold. The cabinet contained an Edison Class M or Class E phonograph. The Class M was powered by a wet-cell glass battery that would spill dangerous acid if it tipped over or broke. The Class E sold for a lower price and ran on 120V DC. 
The phenomenon of phonograph parlors peaked in Paris around 1900: in Pathé's luxurious salon, patrons sat in plush upholstered chairs and chose from among many hundreds of available cylinders by using speaking tubes to communicate with attendants on the floor below. 
By 1890, record manufacturers had begun using a rudimentary duplication process to mass-produce their product. While the live performers recorded the master phonograph, up to ten tubes led to blank cylinders in other phonographs. Until this development, each record had to be custom-made. Before long, a more advanced pantograph-based process made it possible to simultaneously produce 90–150 copies of each record. However, as demand for certain records grew, popular artists still needed to re-record and re-re-record their songs. Reportedly, the medium's first major African-American star George Washington Johnson was obliged to perform his "The Laughing Song" (or the separate "The Whistling Coon") literally thousands of times in a studio during his recording career. Sometimes he would sing "The Laughing Song" more than fifty times in a day, at twenty cents per rendition. (The average price of a single cylinder in the mid-1890s was about fifty cents.)

Frank Lambert's lead cylinder recording for an experimental talking clock is often identified as the oldest surviving playable sound recording, although the evidence advanced for its early date is controversial. Wax phonograph cylinder recordings of Handel's choral music made on June 29, 1888, at The Crystal Palace in London were thought to be the oldest-known surviving musical recordings, until the recent playback by a group of American historians of a phonautograph recording of "Au clair de la lune" made on April 9, 1860. The 1860 phonautogram had not until then been played, as it was only a transcription of sound waves into graphic form on paper for visual study. Recently developed optical scanning and image processing techniques have given new life to early recordings by making it possible to play unusually delicate or physically unplayable media without physical contact.

A recording made on a sheet of tinfoil at an 1878 demonstration of Edison's phonograph in St. Louis, Missouri has been played back by optical scanning and digital analysis. A few other early tinfoil recordings are known to survive, including a slightly earlier one which is believed to preserve the voice of U.S. President Rutherford B. Hayes, but as of May 2014 they have not yet been played by this means. These antique tinfoil recordings, which have typically been stored folded, are too fragile to be played back with a stylus without seriously damaging them. Edison's 1877 tinfoil recording of "Mary Had a Little Lamb", not preserved, has been called the first instance of recorded verse. On the occasion of the 50th anniversary of the phonograph, Edison recounted reciting "Mary Had a Little Lamb" to test his first machine. The 1927 event was filmed by an early sound-on-film newsreel camera, and an audio clip from that film's soundtrack is sometimes mistakenly presented as the original 1877 recording. Wax cylinder recordings made by 19th century media legends such as P. T. Barnum and Shakespearean actor Edwin Booth are amongst the earliest verified recordings by the famous that have survived to the present.

Alexander Graham Bell and his two associates took Edison's tinfoil phonograph and modified it considerably to make it reproduce sound from wax instead of tinfoil. They began their work at Bell's Volta Laboratory in Washington, D. C., in 1879, and continued until they were granted basic patents in 1886 for recording in wax.

Although Edison had invented the phonograph in 1877 the fame bestowed on him for this invention was not due to its efficiency. Recording with his tinfoil phonograph was too difficult to be practical, as the tinfoil tore easily, and even when the stylus was properly adjusted, its reproduction of sound was distorted, and good for only a few playbacks; nevertheless Edison had hit upon the secret of sound recording. However immediately after his discovery he did not improve it, allegedly because of an agreement to spend the next five years developing the New York City electric light and power system.

Meanwhile, Bell, a scientist and experimenter at heart, was looking for new worlds to conquer after his invention of the telephone. According to Sumner Tainter, it was through Gardiner Green Hubbard that Bell took up the phonograph challenge. Bell had married Hubbard's daughter Mabel in 1879 while Hubbard was president of the Edison Speaking Phonograph Co., and his organization, which had purchased the Edison patent, was financially troubled because people did not want to buy a machine which seldom worked well and proved difficult for the average person to operate.

In 1879 Hubbard got Bell interested in improving the phonograph, and it was agreed that a laboratory should be set up in Washington. Experiments were also to be conducted on the transmission of sound by light, which resulted in the selenium-celled Photophone.

By 1881, the Volta associates had succeeded in improving an Edison tinfoil machine to some extent. Wax was put in the grooves of the heavy iron cylinder, and no tinfoil was used. Rather than apply for a patent at that time, however, they deposited the machine in a sealed box at the Smithsonian, and specified that it was not to be opened without the consent of two of the three men.

The sound vibrations had been indented in the wax which had been applied to the Edison phonograph. The following was the text of one of their recordings: "There are more things in heaven and earth, Horatio, than are dreamed of in your philosophy. I am a Graphophone and my mother was a phonograph." Most of the disc machines designed at the Volta Lab had their disc mounted on vertical turntables. The explanation is that in the early experiments, the turntable, with disc, was mounted on the shop lathe, along with the recording and reproducing heads. Later, when the complete models were built, most of them featured vertical turntables.

One interesting exception was a horizontal seven inch turntable. The machine, although made in 1886, was a duplicate of one made earlier but taken to Europe by Chichester Bell. Tainter was granted on July 10, 1888. The playing arm is rigid, except for a pivoted vertical motion of 90 degrees to allow removal of the record or a return to starting position. While recording or playing, the record not only rotated, but moved laterally under the stylus, which thus described a spiral, recording 150 grooves to the inch.

The preserved Bell and Tainter records are of both the lateral cut and the Edison-style "hill-and-dale" (up-and-down) styles. Edison for many years used the "hill-and-dale" method on both his cylinders and Diamond Disc records, and Emile Berliner is credited with the invention of the lateral cut, acid-etched Gramophone record in 1887. The Volta associates, however, had been experimenting with both formats and directions of groove modulation as early as 1881.

The basic distinction between the Edison's first phonograph patent and the Bell and Tainter patent of 1886 was the method of recording. Edison's method was to indent the sound waves on a piece of tin foil, while Bell and Tainter's invention called for cutting, or "engraving", the sound waves into a wax record with a sharp recording stylus.

In 1885, when the Volta Associates were sure that they had a number of practical inventions, they filed patent applications and began to seek out investors. The Volta Graphophone Company of Alexandria, Virginia, was created on January 6, 1886 and incorporated on February 3, 1886. It was formed to control the patents and to handle the commercial development of their sound recording and reproduction inventions, one of which became the first Dictaphone.

After the Volta Associates gave several demonstrations in the City of Washington, businessmen from Philadelphia created the American Graphophone Company on March 28, 1887, in order to produce and sell the machines for the budding phonograph marketplace. The Volta Graphophone Company then merged with American Graphophone, which itself later evolved into Columbia Records.

Shortly after American Graphophone's creation, Jesse H. Lippincott used nearly $1 million of an inheritance to gain control of it, as well as the rights to the Graphophone and the Bell and Tainter patents. Not long later Lippincott purchased the Edison Speaking Phonograph Company. He then created the North American Phonograph Company to consolidate the national sales rights of both the Graphophone and the Edison Speaking Phonograph. In the early 1890s Lippincott fell victim to the unit's mechanical problems and also to resistance from stenographers. 

A coin-operated version of the Graphophone, , was developed by Tainter in 1893 to compete with "nickel-in-the-slot" entertainment phonograph demonstrated in 1889 by Louis T. Glass, manager of the Pacific Phonograph Company.

The work of the Volta Associates laid the foundation for the successful use of dictating machines in business, because their wax recording process was practical and their machines were durable. But it would take several more years and the renewed efforts of Edison and the further improvements of Emile Berliner and many others, before the recording industry became a major factor in home entertainment.

Discs are not inherently better than cylinders at providing audio fidelity. Rather, the advantages of the format are seen in the manufacturing process: discs can be stamped; cylinders could not be until 1901–1902 when the gold moulding process was introduced by Edison.

Recordings made on a cylinder remain at a constant linear velocity for the entirety of the recording, while those made on a disc have a higher linear velocity at the outer portion of the groove compared to the inner portion.

Edison's patented recording method recorded with vertical modulations in a groove. Berliner utilized a laterally modulated groove.
Though Edison's recording technology was better than Berliner's, there were commercial advantages to a disc system since the disc could be easily mass-produced by molding and stamping and it required less storage space for a collection of recordings.

Berliner successfully argued that his technology was different enough from Edison's that he did not need to pay royalties on it, which reduced his business expenses.

Through experimentation, in 1892 Berliner began commercial production of his disc records, and "gramophones" or "talking-machines". His "gramophone record" was the first disc record to be offered to the public. They were five inches (12.7 cm) in diameter and recorded on one side only. Seven-inch (17.5 cm) records followed in 1895. Also in 1895 Berliner replaced the hard rubber used to make the discs with a shellac compound. Berliner's early records had poor sound quality, however. Work by Eldridge R. Johnson improved the sound fidelity to a point where it was as good as the cylinder. By 1901, ten-inch (25 cm) records were marketed by Johnson and Berliner's Victor Talking Machine Company, and Berliner had sold his interests. By 1908, a majority of the public demanded double-sided disc recordings, and cylinders fell into disfavor. Edison felt the commercial pressure for disc records, and by 1912, though reluctant at first, his movement to disc records was in full swing. This was the Edison Disc Record.

From the mid-1890s until the early 1920s both phonograph cylinder and disc recordings and machines to play them on were widely mass-marketed and sold. The disc system gradually became more popular because of its cheaper price and better marketing by disc record companies. Edison ceased cylinder manufacture in the autumn of 1929, and the history of disc and cylinder rivalry was concluded.

Berliner's lateral disc record was the ancestor of the 78 rpm, 45 rpm, 33⅓ rpm, and all other analogue disc records popular for use in sound recording through the 20th century. See gramophone record.

The 1920s brought improved radio technology and radio sales, bringing many phonograph dealers to near financial ruin. With efforts at improved audio fidelity, the big record companies succeeded in keeping business booming through the end of the decade, but the record sales plummeted during the Great Depression, with many companies merging or going out of business.

Record sales picked up appreciably by the late 30s and early 40s, with greater improvements in fidelity and more money to be spent. By this time home phonographs had become much more common, though it wasn't until the 1940s that console radio/phono set-ups with automatic record changers became more common.

In the 1930s, vinyl (originally known as vinylite) was introduced as a record material for radio transcription discs, and for radio commercials. At that time, virtually no discs for home use were made from this material. Vinyl was used for the popular 78-rpm V-discs issued to US soldiers during World War II. This significantly reduced breakage during transport. The first commercial vinylite record was the set of five 12" discs "Prince Igor" (Asch Records album S-800, dubbed from Soviet masters in 1945). Victor began selling some home-use vinyl 78s in late 1945; but most 78s were made of a shellac compound until the 78-rpm format was completely phased out. (Shellac records were heavier and more brittle.) 33s and 45s were, however, made exclusively of vinyl, with the exception of some 45s manufactured out of polystyrene.

Booms in record sales returned after the Second World War, as industry standards changed from 78s to vinyl, long-playing records (commonly called record albums), which could contain an entire symphony, and 45s which usually contained one hit song popularized on the radio – thus the term "single" record – plus another song on the back or "flip" side. An "extended play" version of the 45 was also available, designated 45 EP, which provided capacity for longer musical selections, or for two regular-length songs per side.

In 1955, Philco developed and produced the world's first all-transistor phonograph models TPA-1 and TPA-2, which were announced in the June 28, 1955 edition of the "Wall Street Journal". Philco started to sell these all-transistor phonographs in the fall of 1955, for the price of $59.95. The October 1955 issue of "Radio & Television News" magazine (page 41), had a full page detailed article on Philco's new consumer product. The all-transistor portable phonograph TPA-1 and TPA-2 models played only 45rpm records and used four 1.5 volt "D" batteries for their power supply. The "TPA" stands for "Transistor Phonograph Amplifier". Their circuitry used three Philco germanium PNP alloy-fused junction audio frequency transistors. After the 1956 season had ended, Philco decided to discontinue both models, for transistors were too expensive compared to vacuum tubes, but by 1961 a $49.95 ($ in ) portable, battery-powered radio-phonograph with seven transistors was available.

By the 1960s, cheaper portable record players and record changers which played stacks of records in wooden console cabinets were popular, usually with heavy and crude tonearms in the portables. The consoles were often equipped with better quality pick-ups. Even pharmacies stocked 45 rpm records at their front counters. Rock music played on 45s became the soundtrack to the 1960s as people bought the same songs that were played free of charge on the radio. Some record players were even tried in automobiles, but were quickly displaced by 8-track and cassette tapes.

High fidelity made great advances during the 1970s, as turntables became very precise instruments with belt or direct drive, jewel-balanced tonearms, some with electronically controlled linear tracking and magnetic cartridges. Some cartridges had frequency response above 30 kHz for use with CD-4 quadraphonic 4 channel sound. A high fidelity component system which cost under $1,000 could do a very good job of reproducing very accurate frequency response across the human audible spectrum from 20 Hz to 20,000 Hz with a $200 turntable which would typically have less than 0.05% wow and flutter and very low rumble (low frequency noise). A well-maintained record would have very little surface noise, though it was difficult to keep records completely free from scratches, which produced popping noises. Another characteristic failure mode was skipping, or groove lock, causing a section of music to repeat, separated by a popping noise. This was so common that a saying was coined: "you sound like a broken record", referring to someone who is being annoyingly repetitious.

A novelty variation on the standard format was the use of multiple concentric spirals with different recordings. Thus when the record was played multiple times, different recordings would play, seemingly at random.

Records themselves became an art form because of the large surface onto which graphics and books could be printed, and records could be molded into unusual shapes, colors, or with images (picture discs). The turntable remained a common element of home audio systems well after the introduction of other media, such as audio tape and even the early years of the compact disc as a lower-priced music format. However, even though the cost of producing CDs fell below that of records, CDs remained a higher-priced music format than either cassettes or records. Thus, records were not uncommon in home audio systems into the early 1990s.

By the turn of the 21st century, the turntable had become a niche product, as the price of CD players, which reproduce music free of pops and scratches, fell far lower than high-fidelity tape players or turntables. Nevertheless, there is some increase in interest; many big-box media stores carry turntables, as do professional DJ equipment stores. Most low-end and mid-range amplifiers omit the phono input; but on the other hand, low-end turntables with built-in phono pre-amplifiers are widely available. Some combination systems include a basic turntable, a CD player, a cassette deck. and a radio, in a retro-styled cabinet. Records also continue to be manufactured and sold today, albeit in smaller quantities than in the disc phonograph's heyday.

Inexpensive record players typically used a flanged steel stamping for the turntable structure. A rubber disc would be secured to the top of the stamping to provide traction for the record, as well as a small amount of vibration isolation. The spindle bearing usually consisted of a bronze bushing. The flange on the stamping provided a convenient place to drive the turntable by means of an "idler wheel" (see below). While light and cheap to manufacture, these mechanisms had low inertia, making motor speed instabilities more pronounced.

Costlier turntables made from heavy aluminium castings have greater balanced mass and inertia, helping minimize vibration at the stylus, and maintaining constant speed without wow or flutter, even if the motor exhibits cogging effects. Like stamped steel turntables, they were topped with rubber. Because of the increased mass, they usually employed ball bearings or roller bearings in the spindle to reduce friction and noise. Most are belt or direct drive, but some use an idler wheel. A specific case was the Swiss "Lenco" drive, which possessed a very heavy turntable coupled via an idler wheel to a long, tapered motor drive shaft. This enabled stepless rotation or speed control on the drive. Because of this feature the Lenco became popular in the late 1950s with dancing schools, because the dancing instructor could lead the dancing exercises at different speeds.

By the early 1980s, some companies started producing very inexpensive turntables that displaced the products of companies like BSR. Commonly found in "all-in-one" stereos from assorted far-east manufacturers, they used a thin plastic table set in a plastic plinth, no mats, belt drive, weak motors, and often, lightweight plastic tonearms with no counterweight. Most used sapphire pickups housed in ceramic cartridges, and they lacked several features of earlier units, such as auto-start and record-stacking. While not as common now that turntables are absent from the cheap "all-in-one" units, this type of turntable has made a strong resurgence in nostalgia-marketed record players.

From the earliest phonograph designs, many of which were powered by spring-wound mechanisms, a speed governor was essential. Most of these employed some type of flywheel-friction disc to control the speed of the rotating cylinder or turntable; as the speed increased, centrifugal force caused a brake—often a felt pad—to rub against a smooth metal surface, slowing rotation. Electrically powered turntables, whose rotational speed was governed by other means, eventually made their mechanical counterparts obsolete. The mechanical governor was, however, still employed in some toy phonographs (such as those found in talking dolls) until they were replaced by digital sound generators in the late 20th century.

Many modern players have platters with a continuous series of strobe markings machined or printed around their edge. Viewing these markings in artificial light at mains frequency produces a stroboscopic effect, which can be used to verify proper rotational speed. Additionally, the edge of the turntable can contain magnetic markings to provide feedback pulses to an electronic speed-control system.

Earlier designs used a rubberized idler-wheel drive system. However, wear and decomposition of the wheel, as well as the direct mechanical coupling to a vibrating motor, introduced low-frequency noise ("rumble") and speed variations ("wow and flutter") into the sound. These systems generally used a synchronous motor which ran at a speed synchronized to the frequency of the AC power supply. Portable record players typically used an inexpensive shaded-pole motor. At the end of the motor shaft there was a stepped driving capstan; to obtain different speeds, the rubber idler wheel was moved to contact different steps of this capstan. The idler was pinched against the bottom or inside edge of the platter to drive it.

Until the 1970s, the idler-wheel drive was the most common on turntables, except for higher-end audiophile models. However, even some higher-end turntables, such as the Lenco, Garrard, EMT, and Dual turntables, used idler-wheel drive.

Belt drives brought improved motor and platter isolation compared to idler-wheel designs. Motor noise, generally heard as low-frequency rumble, is greatly reduced. The design of the belt drive turntable allows for a less expensive motor than the direct-drive turntable to be used. The elastomeric belt absorbs motor vibrations and noise which could otherwise be picked up by the stylus. It also absorbs small, fast speed variations, caused by "cogging", which in other designs are heard as "flutter."

The "Acoustical professional" turntable (earlier marketed under Dutch ""Jobo prof"") of the 1960s however possessed an expensive German drive motor, the ""Pabst Aussenläufer"" ("Pabst outrunner"). As this motor name implied, the rotor was on the outside of the motor and acted as a flywheel ahead of the belt-driven turntable itself. In combination with a steel to nylon turntable bearing (with molybdenum disulfide inside for lifelong lubrication) very low wow, flutter and rumble figures were achieved.

Direct-drive turntables drive the platter directly without utilizing intermediate wheels, belts, or gears as part of a drive train. The platter functions as a motor armature. This requires good engineering, with advanced electronics for acceleration and speed control. Matsushita's Technics division introduced the first commercially successful direct drive platter, model SP10, in 1969, which was joined by the Technics SL-1200 turntable, in 1972. Its updated model, SL-1200MK2, released in 1978, had a stronger motor, a convenient pitch control slider for beatmatching and a stylus illuminator, which made it the long-standing favourite among disc jockeys ("see "Turntablism""). By the beginnings of the 80s, lowering of costs in microcontroller electronics made direct drive turntables more affordable.

The evaluation of the "best" drive technology is not clear and more depending on the implementation than on the drive technology itself. Technical measurements show that similarly low flutter (0.025% WRMS) and rumble (−78 dB weighed) figures are possible for high quality turntables, be they belt drive or direct drive.

Audiophile grade turntables start at a few hundred dollars and range upwards of $100,000, depending on the complexity and quality of design and manufacture. The common view is that there are diminishing returns with an increase in price – a turntable costing $1,000 would not sound significantly better than a turntable costing $500; nevertheless, there exists a large choice of expensive turntables.

The tone arm (or tonearm) holds the pickup cartridge over the groove, the stylus tracking the groove with the desired force to give the optimal compromise between good tracking and minimizing wear of the stylus and record groove. At its simplest, a tone arm is a pivoted lever, free to move in two axes (vertical and horizontal) with a counterbalance to maintain tracking pressure.
However, the requirements of high-fidelity reproduction place more demands upon the arm design. In a perfect world:


These demands are contradictory and impossible to realize (massless arms and zero-friction bearings do not exist in the real world), so tone arm designs require engineering compromises. Solutions vary, but all modern tonearms are at least relatively lightweight and stiff constructions, with precision, very low friction pivot bearings in both the vertical and horizontal axes. Most arms are made from some kind of alloy (the cheapest being aluminium), but some manufacturers use balsa wood, while others use carbon fiber or graphite. The latter materials favor a straight arm design; alloys' properties lend themselves to S-type arms.

The tone arm got its name before the age of electronics. It originally served to conduct actual sound waves from a purely mechanical "pickup" called a "sound box" or "reproducer" to a so-described "amplifying" horn. The earliest electronic record players, introduced at the end of 1925, had massive electromagnetic pickups that contained a horseshoe magnet, used disposable steel needles, and weighed several ounces. Their full weight rested on the record, providing ample "tracking force" to overcome their low compliance but causing rapid record wear. The tone arms were rudimentary and remained so even after lighter crystal pickups appeared about ten years later. When fine-grooved vinyl records were introduced in the late 1940s, still smaller and lighter crystal (later, ceramic) cartridges with semi-permanent jewel styluses became standard. In the mid-1950s these were joined by a new generation of magnetic cartridges that bore little resemblance to their crude ancestors. Far smaller tracking forces became possible and the "balanced arm" came into use.

Prices varied widely. The well-known and extremely popular high-end S-type SME arm of the 1970–1980 era not only had a complicated design, it was also very costly. On the other hand, even some cheaper arms could be of professional quality: the "All Balance" arm, made by the now-defunct Dutch company Acoustical, was only €30 [equivalent]. It was used during that period by all official radio stations in the Dutch Broadcast studio facilities of the NOS, as well as by the pirate radio station Veronica. Playing records from a boat in international waters, the arm had to withstand sudden ship movements. Anecdotes indicate this low-cost arm was the only one capable of keeping the needle firmly in the groove during heavy storms at sea.

Quality arms employ an adjustable counterweight to offset the mass of the arm and various cartridges and headshells. On this counterweight, a calibrated dial enables easy adjustment of stylus force. After perfectly balancing the arm, the dial itself is "zeroed"; the stylus force can then be dialed in by screwing the counterweight towards the fulcrum. (Sometimes a separate spring or smaller weight provides fine tuning.) Stylus forces of 10 to 20 mN (1 to 2 grams-force) are typical for modern consumer turntables, while forces of up to 50 mN (5 grams) are common for the tougher environmental demands of party deejaying or turntablism. 
Of special adjustment consideration, Stanton cartridges of the 681EE(E) series [and others like them] feature a small record brush ahead of the cartridge. The upforce of this brush, and its added drag require compensation of both tracking force (add 1 gram) and anti-skating adjustment values (see next paragraph for description).
Even on a perfectly flat LP, tonearms are prone to two types of tracking errors that affect the sound. As the tonearm tracks the groove, the stylus exerts a frictional force tangent to the arc of the groove, and since this force does not intersect the tone arm pivot, a clockwise rotational force (moment) occurs and a reaction "skating force" is exerted on the stylus by the record groove wall away from center of the disc. Modern arms provide an "anti-skate" mechanism, using springs, hanging weights, or magnets to produce an offsetting counter-clockwise force at the pivot, making the net lateral force on the groove walls near zero. 
The second error occurs as the arm sweeps in an arc across the disc, causing the angle between the cartridge head and groove to change slightly. A change in angle, albeit small, will have a detrimental effect (especially with stereo recordings) by creating different forces on the two groove walls, as well as a slight timing shift between left/right channels. Making the arm longer to reduce this angle is a partial solution, but less than ideal. A longer arm weighs more, and only an infinitely long [pivoted] arm would reduce the error to zero. Some designs (Burne-Jones, and Garrard "Zero" series) use dual arms in a parallelogram arrangement, pivoting the cartridge head to maintain a constant angle as it moves across the record. Unfortunately this "solution" creates more problems than it solves, compromising rigidity and creating sources of unwanted noise.

The pivoted arm produces yet another problem which is unlikely to be significant to the audiophile, though. As the master was originally cut in a linear motion from the edge towards the center, but the stylus on the pivoted arm always draws an arc, this causes a timing drift that is most significant when digitizing music and beat mapping the data for synchronization with other songs in a DAW or DJ software unless the software allows building a non-linear beat map. As the contact point of the stylus on the record wanders farther from the linear path between the starting point and center hole, the tempo and pitch tend to decrease towards the middle of the record, until the arc reaches its apex. After that the tempo and pitch increase towards the end as the contact point comes closer to the linear path again. Because the surface speed of the record is lower at the end, the relative speed error from the same absolute distance error is higher at the end, and the increase in tempo is more notable towards the end than the decrease towards the middle. This can be somewhat reduced by a curved arm pivoted so that the end point of the arc stays farther from the linear path than the starting point, or by a long straight arm that pivots perpendicularly to the linear path in the middle of the record. However the tempo droop at the middle can only be completely avoided by a linear tracking arm.

If the arm is not pivoted, but instead carries the stylus along a radius of the disc, there is no skating force and little to no cartridge angle error. Such arms are known as "linear tracking" or "tangential" arms. These are driven along a track by various means, from strings and pulleys, to worm gears or electromagnets. The cartridge's position is usually regulated by an electronic servomechanism or mechanical interface, moving the stylus properly over the groove as the record plays, or for song selection.

There are long-armed and short-armed linear arm designs. On a perfectly flat record a short arm will do, but once the record is even slightly warped, a short arm will be troublesome. Any vertical motion of the record surface at the stylus contact point will cause the stylus to considerably move longitudinally in the groove. This will cause the stylus to ride non-tangentially in the groove and cause a stereo phase error as well as pitch error every time the stylus rides over the warp. Also the arm track can come into touch with the record. A long arm will not completely eliminate this problem but will tolerate warped records much better.

Early developments in linear turntables were from Rek-O-Kut (portable lathe/phonograph) and Ortho-Sonic in the 1950s, and Acoustical in the early 1960s. These were eclipsed by more successful implementations of the concept from the late 1960s through the early 1980s.
Of note are Rabco's SL-8, followed by Bang & Olufsen with its Beogram 4000 model in 1972. These models positioned the track outside the platter's edge, as did turntables by Harman Kardon, Mitsubishi, Pioneer, Yamaha, Sony, etc. A 1970s design from Revox harkened back to the 1950s attempts (and, record lathes), positioning the track directly over the record. An enclosed bridge-like assembly is swung into place from the platter's right edge to its middle. Once in place, a short tonearm under this "bridge" plays the record, driven across laterally by a motor. The Sony PS-F5/F9 (1983) uses a similar, miniaturized design, and can operate in a vertical or horizontal orientation. The Technics SL-10, introduced in 1981, was the first direct drive linear tracking turntable, and placed the track and arm on the underside of the rear-hinged dust cover, to fold down over the record, similar to the SL-Q6 pictured.

The earliest Edison phonographs used horizontal, spring-powered drives to carry the stylus across the recording at a pre-determined rate. But, historically as a whole, the linear tracking systems never gained wide acceptance, due largely to their complexity and associated production/development costs. The resources it takes to produce one incredible linear turntable could produce several excellent ones. Some of the most sophisticated and expensive tonearms and turntable units ever made are linear trackers, from companies such as Rockport and Clearaudio. In theory, it seems nearly ideal; a stylus replicating the motion of the recording lathe used to cut the "master" record could result in minimal wear and maximum sound reproduction. In practice, in vinyl's heyday it was generally too much too late.

Since the early 1980s, an elegant solution has been the near-frictionless air bearing linear arm that requires no tracking drive mechanism other than the record groove. This provides a similar benefit as the electronic linear tonearm without the complexity and necessity of servo-motor correction for tracking error. In this case the trade-off is the introduction of pneumatics in the form of audible pumps and tubing. A more elegant solution is the mechanically driven low-friction design, also driven by the groove. Examples include Souther Engineering (U.S.A.), Clearaudio (Germany), and Aura (Czech Republic). This design places an exceeding demand upon precision engineering due to the lack of pneumatics.

Historically, most high-fidelity "component" systems (preamplifiers or receivers) that accepted input from a phonograph turntable had separate inputs for both ceramic and magnetic cartridges (typically labeled "CER" and "MAG"). One piece systems often had no additional phono inputs at all, regardless of type.

Most systems today, if they accept input from a turntable at all, are configured for use only with magnetic cartridges. Manufacturers of high-end systems often have in-built moving coil amplifier circuitry, or outboard head-amplifiers supporting either moving magnet or moving coil cartridges that can be plugged into the line stage.

Early electronic phonographs used a piezo-electric "crystal" for pickup (though the earliest electronic phonographs used crude magnetic pick-ups), where the mechanical movement of the stylus in the groove generates a proportional electrical voltage by creating stress within a crystal (typically Rochelle salt). Crystal pickups are relatively robust, and produce a substantial signal level which requires only a modest amount of further amplification. The output is not very linear however, introducing unwanted distortion. It is difficult to make a crystal pickup suitable for quality stereo reproduction, as the stiff coupling between the crystal and the long stylus prevents close tracking of the needle to the groove modulations. This tends to increase wear on the record, and introduces more distortion. Another problem is the hygroscopic nature of the crystal itself: it absorbs moisture from the air and may dissolve. The crystal was protected by embedding it in other materials, without hindering the movement of the pickup mechanism itself. After a number of years, the protective jelly often deteriorated or leaked from the cartridge case and the full unit needed replacement.

The next development was the "ceramic" cartridge, a piezoelectric device that used newer, and better, materials. These were more sensitive, and offered greater "compliance", that is, lack of resistance to movement and so increased ability to follow the undulations of the groove without gross distorting or jumping out of the groove. Higher compliance meant lower tracking forces and reduced wear to both the disc and stylus. It also allowed ceramic stereo cartridges to be made.

During the 1950s to 1970s, ceramic cartridge became common in low quality phonographs, but better high-fidelity (or "hi-fi") systems used magnetic cartridges, and the availability of low cost magnetic cartridges from the 1970s onwards made ceramic cartridges obsolete for essentially all purposes. At the seeming end of the market lifespan of ceramic cartridges, someone accidentally discovered that by terminating a specific ceramic mono cartridge (the Ronette TX88) not with the prescribed 47 kΩ resistance, but with approx. 10 kΩ, it could be connected to the moving magnet (MM) input too. The result, a much smoother frequency curve extended the lifetime for this popular and very cheap type.

Another popular ceramic stereo cartridge was the Audio Technica model AT66, which because of its price performance ratio was favoured by many as an alternative to more expensive magnetic cartridges.

There are two common designs for magnetic cartridges, moving magnet (MM) and moving coil (MC) (originally called "dynamic"). Both operate on the same physics principle of electromagnetic induction. The moving magnet type was by far the most common and more robust of the two, though audiophiles often claim that the moving coil system yields higher fidelity sound.

In either type, the stylus itself, usually of diamond, is mounted on a tiny metal strut called a cantilever, which is suspended using a collar of highly compliant plastic. This gives the stylus the freedom to move in any direction. On the other end of the cantilever is mounted a tiny permanent magnet (moving magnet type) or a set of tiny wound coils (moving coil type). The magnet is close to a set of fixed pick-up coils, or the moving coils are held within a magnetic field generated by fixed permanent magnets. In either case, the movement of the stylus as it tracks the grooves of a record causes a fluctuating magnetic field, which causes a small electric current to be induced in the coils. This current closely follows the sound waveform cut into the record, and may be transmitted by wires to an electronic amplifier where it is processed and amplified in order to drive a loudspeaker. Depending upon the amplifier design, a phono-preamplifier may be necessary.

In most moving magnet designs, the stylus itself is detachable from the rest of the cartridge so it can easily be replaced. There are three primary types of cartridge mounts. The most common type is attached using two small screws to a headshell that then plugs into the tonearm, while another is a standardized "P-mount" or "T4P" cartridge (invented by Technics in 1980 and adopted by other manufacturers) that plugs directly into the tonearm. Many P-mount cartridges come with adapters to allow them to be mounted to a headshell. The third type is used mainly in cartridges designed for DJ use and it has a standard round headshell connector. Some mass market turntables use a proprietary integrated cartridge that cannot be upgraded.

An alternative design is the "moving iron" variation on moving magnet used by ADC, Grado, Stanton/Pickering 681 series, Ortofon OM and VMS series, and the MMC cartridge of Bang & Olufsen. In these units, the magnet itself sits behind the four coils and magnetises the cores of all four coils. The moving iron cross at the other end of the coils varies the gaps between itself and each of these cores, according to its movements. These variations lead to voltage variations as described above.

Famous brands for magnetic cartridges are: Grado, Stanton/Pickering (681EE/EEE), B&O (MM types for its two, non-compatible generations of parallel arm design), Shure (V15 Type I to V), Audio-Technica, Nagaoka, Dynavector, Koetsu, Ortofon, Technics, Denon and ADC.

Strain gauge or "semiconductor" cartridges do not generate a voltage, but act like a variable resistor, whose resistance directly depends on the movement of the stylus. Thus, the cartridge "modulates" an external voltage supplied by the (special) preamplifier. These pickups were marketed by Euphonics, Sao Win, and Panasonic/Technics, amongst others.

The main advantages (compared to magnetic carts are):


The main disadvantage is the need of a special preamplifier that supplies a steady current (typically 5mA) to the semiconductor elements and handles a special equalization than the one needed for magnetic cartridges.

A high-end strain-gauge cartridge is currently sold by an audiophile company, with special preamplifiers available.

Electrostatic cartridges were marketed by Stax in the 1950 and 1960 years. They needed individual operating electronics or preamplifiers.

A few specialist laser turntables read the groove optically using a laser pickup. Since there is no physical contact with the record, no wear is incurred. However, this "no wear" advantage is debatable, since vinyl records have been tested to withstand even 1200 plays with no significant audio degradation, provided that it is played with a high quality cartridge and that the surfaces are clean.

An alternative approach is to take a high-resolution photograph or scan of each side of the record and interpret the image of the grooves using computer software. An amateur attempt using a flatbed scanner lacked satisfactory fidelity. A professional system employed by the Library of Congress produces excellent quality.

A smooth-tipped "stylus" (in popular usage often called a "needle" due to the former use of steel needles for the purpose) is used to play the recorded groove. A special chisel-like stylus is used to engrave the groove into the "master record".

The stylus is subject to hard wear as it is the only small part that comes into direct contact with the spinning record. In terms of the pressure imposed on its minute areas of actual contact, the forces it must bear are enormous. There are three desired qualities in a stylus: first, that it faithfully follows the contours of the recorded groove and transmits its vibrations to the next part in the chain; second, that it does not damage the recorded disc; and third, that it is resistant to wear. A worn-out, damaged or defective stylus tip will degrade audio quality and injure the groove.

Different materials for the stylus have been used over time. Thomas Edison introduced the use of sapphire in 1892 and the use of diamond in 1910 for his cylinder phonographs. The Edison Diamond Disc players (1912–1929), when properly played, hardly ever required the stylus to be changed. The styli for vinyl records were also made out of sapphire or diamond. A specific case is the specific stylus type of Bang & Olufsen's (B&O) moving magnet cartridge MMC 20CL, mostly used in parallel arm B&O turntables in the 4002/6000 series. It uses a sapphire stem on which a diamond tip is fixed by a special adhesive. A stylus tip mass as low as 0.3 milligram is the result and full tracking only requires 1 gram of stylus force, reducing record wear even further. Maximum distortion (2nd harmonic) fell below 0.6%.

Other than the Edison and European Pathé disc machines, early disc players, both external horn and internal horn "Victrola" style models, normally used very short-lived disposable needles. The most common material was steel, although other materials such as copper, tungsten, bamboo and cactus were used. Steel needles needed to be replaced frequently, preferably after each use, due to their very rapid wear from bearing down heavily on the mildly abrasive shellac record. Rapid wear was an essential feature so that their imprecisely formed tips would be quickly worn into compliance with the groove's contours. Advertisements implored customers to replace their steel needles after each record side. Steel needles were inexpensive, e.g., a box of 500 for 50 US cents, and were widely sold in packets and small tins. They were available in different thicknesses and lengths. Thick, short needles produced strong, loud tones while thinner, longer needles produces softer, muted tones. In 1916, in the face of a wartime steel shortage, Victor introduced their "Tungs-Tone" brand extra-long-playing needle, which was advertised to play between 100 and 300 records. It consisted of a brass shank into which a very hard and strong tungsten wire, somewhat narrower than the standard record groove, had been fitted. The protruding wire wore down, but not out, until it was worn too short to use. Later in the 78 rpm era, hardened steel and chrome-plated needles came on the market, some of which were claimed to play 10 to 20 record sides each.

When sapphires were introduced for the 78 rpm disc and the LP, they were made by tapering a stem and polishing the tip to a sphere with a radius of around 70 and 25 micrometers respectively. A sphere is not equal to the form of the cutting stylus and by the time diamond needles came to the market, a whole discussion was started on the effect of circular forms moving through a non-circular cut groove. It can be easily shown that vertical, so called "pinching" movements were a result and when stereophonic LPs were introduced, unwanted vertical modulation was recognized as a problem. Also, the needle started its life touching the groove on a very small surface, giving extra wear on the walls.
Another problem is in the tapering along a straight line, while the side of the groove is far from straight. Both problems were attacked together: by polishing the diamond in a certain way that it could be made doubly elliptic. 1) the side was made into one ellipse as seen from behind, meaning the groove touched along a short line and 2) the ellipse form was also polished as seen from above and curvature in the direction of the groove became much smaller than 25 micrometers e.g. 13 micrometers. With this approach a number of irregularities were eliminated. Furthermore, the angle of the stylus, which used to be always sloping backwards, was changed into the forward direction, in line with the slope the original cutting stylus possessed. These styli were expensive to produce, but the costs were effectively offset by their extended lifespans.

The next development in stylus form came about by the attention to the CD-4 quadraphonic sound modulation process, which requires up to 50 kHz frequency response, with cartridges like Technics EPC-100CMK4 capable of playback on frequencies up to 100 kHz. This requires a stylus with a narrow side radius, such as 5 µm (or 0.2 mil). A narrow-profile elliptical stylus is able to read the higher frequencies (greater than 20 kHz), but at an increased wear, since the contact surface is narrower. For overcoming this problem, the Shibata stylus was invented around 1972 in Japan by Norio Shibata of JVC, fitted as standard on quadraphonic cartridges, and marketed as an extra on some high-end cartridges.

The Shibata-designed stylus offers a greater contact surface with the groove, which in turn means less pressure over the vinyl surface and thus less wear. A positive side effect is that the greater contact surface also means the stylus will read sections of the vinyl that were not touched (or "worn") by the common spherical stylus. In a demonstration by JVC records "worn" after 500 plays at a relatively very high 4.5 gf tracking force with a spherical stylus, played "as new" with the Shibata profile.

Other advanced stylus shapes appeared following the same goal of increasing contact surface, improving on the Shibata. Chronologically: "Hughes" Shibata variant (1975), "Ogura" (1978), Van den Hul (1982). Such a stylus may be marketed as "Hyperelliptical" (Shure), "Alliptic", "Fine Line" (Ortofon), "Line contact" (Audio Technica), "Polyhedron", "LAC", or "Stereohedron" (Stanton).

A keel-shaped diamond stylus appeared as a byproduct of the invention of the CED Videodisc. This, together with laser-diamond-cutting technologies, made possible the "ridge" shaped stylus, such as the Namiki (1985) design, and Fritz Gyger (1989) design. This type of stylus is marketed as "MicroLine" (Audio technica), "Micro-Ridge" (Shure), or "Replicant" (Ortofon).

It is important to point out that most of those stylus profiles are still being manufactured and sold, together with the more common spherical and elliptical profiles. This is despite the fact that production of CD-4 quadraphonic records ended by the late 1970s.

Early materials in the 19th century were hardened rubber, wax, and celluloid, but early in the 20th century a shellac compound became the standard. Since shellac is not hard enough to withstand the wear of steel needles on heavy tone arms, filler made of pulverized shale was added. Shellac was also fragile, and records often shattered or cracked. This was a problem for home records, but it became a bigger problem in the late 1920s with the Vitaphone sound-on-disc motion picture "talkie" system, developed in 1927. 

To solve this problem, in 1930, RCA Victor made unbreakable records by mixing polyvinyl chloride with plasticisers, in a proprietary formula they called Victrolac, which was first used in 1931, in motion picture discs, and experimentally, in home records, the same year. However, with Sound-on-film achieving supremacy over sound-on-disc by 1931, the need for unbreakable records diminished and the production of vinyl home recordings was dropped as well, for the time being.

The Victrolac formula improved throughout the 30's, and by the late-30s the material, by then called vinylite, was being used in records sent to radio stations for radio program records, radio commercials, and later, DJ copies of phonograph records, because vinyl records could be sent through the mail to radio stations without breaking. During WWII, there was a shortage of shellac, which had to be imported from Asia, and the U.S. government banned production of shellac records for the duration of the war. Vinylite was made domestically, though, and was being used for V-discs during the war. Record company engineers took a much closer look at the possibilities of vinyl, possibly that it might even replace shellac as the basic record material. After the war, RCA Victor and Columbia, by far the two leading records companies in America, perfected two new vinyl formats, which were both introduced in 1948, when the 33 RPM LP was introduced by Columbia and the 45 RPM single was introduced by RCA Victor. For a few years thereafter, however, 78 RPM records continued to be made in shellac until that format was phased out around 1958.

Early "acoustical" record players used the stylus to vibrate a diaphragm that radiated the sound through a horn. Several serious problems resulted from this:


The introduction of electronic amplification allowed these issues to be addressed. Records are made with boosted high frequencies and reduced low frequencies, which allow for different ranges of sound to be produced.. This reduces the effect of background noise, including clicks or pops, and also conserves the amount of physical space needed for each groove, by reducing the size of the low-frequency undulations.

During playback, the high frequencies must be rescaled to their original, flat frequency response—known as "equalization"—as well as being amplified. A phono input of an amplifier incorporates such equalization as well as amplification to suit the very low level output from a modern cartridge. Most hi-fi amplifiers made between the 1950s and the 1990s and virtually all DJ mixers are so equipped. 

The widespread adoption of digital music formats, such as CD or satellite radio, has displaced phonograph records and resulted in phono inputs being omitted in most modern amplifiers. Some newer turntables include built-in preamplifiers to produce line-level outputs. Inexpensive and moderate performance discrete phono preamplifiers with RIAA equalization are available, while high-end audiophile units costing thousands of dollars continue to be available in very small numbers. Phono inputs are starting to reappear on amplifiers in the 2010s due to the vinyl revival.

Since the late 1950s, almost all phono input stages have used the RIAA equalization standard. Before settling on that standard, there were many different equalizations in use, including EMI, HMV, Columbia, Decca FFRR, NAB, Ortho, BBC transcription, etc. Recordings made using these other equalization schemes will typically sound odd if they are played through a RIAA-equalized preamplifier. High-performance (so-called "multicurve disc") preamplifiers, which include multiple, selectable equalizations, are no longer commonly available. However, some vintage preamplifiers, such as the LEAK varislope series, are still obtainable and can be refurbished. Newer preamplifiers like the Esoteric Sound Re-Equalizer or the K-A-B MK2 Vintage Signal Processor are also available. These kinds of adjustable phono equalizers are used by consumers wishing to play vintage record collections (often the only available recordings of musicians of the time) with the equalization used to make them.

Turntables continue to be manufactured and sold in the 2010s, although in small numbers. While some audiophiles still prefer the sound of vinyl records over that of digital music sources (mainly compact discs), they represent a minority of listeners. As of 2015 the sale of vinyl LP's has increased 49–50% percent from the previous year although small in comparison to the sale of other formats which although more units were sold (Digital Sales, CDs) the more modern formats experienced a decline in sales. The quality of available record players, tonearms, and cartridges has continued to improve, despite diminishing demand, allowing turntables to remain competitive in the high-end audio market. Vinyl enthusiasts are often committed to the refurbishment and sometimes tweaking of vintage systems. The chart on the right illustrates that users of one enthusiasts' forum post as many pictures of discontinued gear as they do of current models.
Updated versions of the 1970s era Technics SL-1200 (production ceased in 2010) have remained an industry standard for DJs to the present day. Turntables and vinyl records remain popular in mixing (mostly dance-oriented) forms of electronic music, where they allow great latitude for physical manipulation of the music by the DJ.

In hip hop music and occasionally in other genres, the turntable is used as a musical instrument by DJs, who use turntables along with a DJ mixer to create unique rhythmic sounds. Manipulation of a record as part of the music, rather than for normal playback or mixing, is called turntablism. The basis of turntablism, and its best known technique, is "scratching", pioneered by Grand Wizzard Theodore. It was not until Herbie Hancock's "Rockit" in 1983 that the turntablism movement was recognized in popular music outside of a hip hop context. In the 2010s, many hip hop DJs use DJ CD players or digital record emulator devices to create scratching sounds; nevertheless, some DJs still scratch with vinyl records.

The laser turntable uses a laser as the pickup instead of a stylus in physical contact with the disk. It was conceived of in the late 1980s, although early prototypes were not of usable audio quality. Practical laser turntables are now being manufactured by ELPJ. They are favoured by record libraries and some audiophiles since they eliminate physical wear completely.
Experimentation is in progress in retrieving the audio from old records by scanning the disc and analysing the scanned image, rather than using any sort of turntable.

Although largely replaced since the introduction of the compact disc in 1982, record albums still sell in small numbers and are available through numerous sources. In 2008, LP sales grew by 90% over 2007, with 1.9 million records sold.

USB turntables have a built-in audio interface, which transfers the sound directly to the connected computer. Some USB turntables transfer the audio without equalization, but are sold with software that allows the EQ of the transferred audio file to be adjusted. There are also many turntables on the market designed to be plugged into a computer via a USB port for needle dropping purposes.

Responding to longtime calls by fans and disc jockeys, Panasonic Corp. said it is reviving Technics turntables–the series that remains a de facto standard player supporting nightclub music scenes.
The new analog turntable, which would come with new direct-drive motor technologies that Panasonic says would improve the quality of sound, would be released sometime between April 2016 and March 2017, the Japanese electronics company announced on September 2, 2015.





</doc>
<doc id="24472" url="https://en.wikipedia.org/wiki?curid=24472" title="Paul Cézanne">
Paul Cézanne

Paul Cézanne ( or ; ; 19 January 1839 – 22 October 1906) was a French artist and Post-Impressionist painter whose work laid the foundations of the transition from the 19th-century conception of artistic endeavor to a new and radically different world of art in the 20th century. Cézanne's often repetitive, exploratory brushstrokes are highly characteristic and clearly recognizable. He used planes of colour and small brushstrokes that build up to form complex fields. The paintings convey Cézanne's intense study of his subjects.

Cézanne is said to have formed the bridge between late 19th-century Impressionism and the early 20th century's new line of artistic enquiry, Cubism. Both Matisse and Picasso are said to have remarked that Cézanne "is the father of us all."

The Cézannes came from the commune of Saint-Sauveur (Hautes-Alpes, Occitania). Paul Cézanne was born on 19 January 1839 in Aix-en-Provence. On 22 February, he was baptized in the Église de la Madeleine, with his grandmother and uncle Louis as godparents, and became a devout Catholic later in life. His father, Louis Auguste Cézanne (1798–1886), a native of Saint-Zacharie (Var), was the co-founder of a banking firm (Banque Cézanne et Cabassol) that prospered throughout the artist's life, affording him financial security that was unavailable to most of his contemporaries and eventually resulting in a large inheritance.
His mother, Anne Elisabeth Honorine Aubert (1814–1897), was "vivacious and romantic, but quick to take offence". It was from her that Cézanne got his conception and vision of life. He also had two younger sisters, Marie and Rose, with whom he went to a primary school every day.

At the age of ten Cézanne entered the Saint Joseph school in Aix. In 1852 Cézanne entered the Collège Bourbon (now Collège Mignet), where he became friends with Émile Zola, who was in a less advanced class, as well as Baptistin Baille—three friends who came to be known as "les trois inséparables" (the three inseparables). He stayed there for six years, though in the last two years he was a day scholar. In 1857, he began attending the Free Municipal School of Drawing in Aix, where he studied drawing under Joseph Gibert, a Spanish monk. From 1858 to 1861, complying with his father's wishes, Cézanne attended the law school of the University of Aix, while also receiving drawing lessons.

Going against the objections of his banker father, he committed himself to pursuing his artistic development and left Aix for Paris in 1861. He was strongly encouraged to make this decision by Zola, who was already living in the capital at the time. Eventually, his father reconciled with Cézanne and supported his choice of career. Cézanne later received an inheritance of 400,000 francs from his father, which rid him of all financial worries.

In Paris, Cézanne met the Impressionist Camille Pissarro. Initially the friendship formed in the mid-1860s between Pissarro and Cézanne was that of master and disciple, in which Pissarro exerted a formative influence on the younger artist. Over the course of the following decade their landscape painting excursions together, in Louveciennes and Pontoise, led to a collaborative working relationship between equals.

Cézanne's early work is often concerned with the figure in the landscape and includes many paintings of groups of large, heavy figures in the landscape, imaginatively painted. Later in his career, he became more interested in working from direct observation and gradually developed a light, airy painting style. Nevertheless, in Cézanne's mature work there is the development of a solidified, almost architectural style of painting. Throughout his life he struggled to develop an authentic observation of the seen world by the most accurate method of representing it in paint that he could find. To this end, he structurally ordered whatever he perceived into simple forms and colour planes. His statement "I want to make of impressionism something solid and lasting like the art in the museums", and his contention that he was recreating Poussin "after nature" underscored his desire to unite observation of nature with the permanence of classical composition. 

Cézanne was interested in the simplification of naturally occurring forms to their geometric essentials: he wanted to "treat nature by the cylinder, the sphere, the cone" (a tree trunk may be conceived of as a cylinder, an apple or orange a sphere, for example). Additionally, Cézanne's desire to capture the truth of perception led him to explore binocular vision graphically, rendering slightly different, yet simultaneous visual perceptions of the same phenomena to provide the viewer with an aesthetic experience of depth different from those of earlier ideals of perspective, in particular single-point perspective. His interest in new ways of modelling space and volume derived from the stereoscopy obsession of his era and from reading Hippolyte Taine’s Berkelean theory of spatial perception. Cézanne's innovations have prompted critics to suggest such varied explanations as sick retinas, pure vision, and the influence of the steam railway.

Cézanne's paintings were shown in the first exhibition of the Salon des Refusés in 1863, which displayed works not accepted by the jury of the official Paris Salon. The Salon rejected Cézanne's submissions every year from 1864 to 1869. He continued to submit works to the Salon until 1882. In that year, through the intervention of fellow artist Antoine Guillemet, he exhibited "Portrait de M. L. A.", probably "Portrait of Louis-Auguste Cézanne, The Artist's Father, Reading "L'Événement"", 1866 (National Gallery of Art, Washington, D.C.), his first and last successful submission to the Salon.

Before 1895 Cézanne exhibited twice with the Impressionists (at the first Impressionist exhibition in 1874 and the third Impressionist exhibition in 1877). In later years a few individual paintings were shown at various venues, until 1895, when the Parisian dealer, Ambroise Vollard, gave the artist his first solo exhibition. Despite the increasing public recognition and financial success, Cézanne chose to work in increasing artistic isolation, usually painting in the south of France, in his beloved Provence, far from Paris.

He concentrated on a few subjects and was equally proficient in each of these genres: still lifes, portraits, landscapes and studies of bathers. For the last, Cézanne was compelled to design from his imagination, due to a lack of available nude models. Like the landscapes, his portraits were drawn from that which was familiar, so that not only his wife and son but local peasants, children and his art dealer served as subjects. His still lifes are at once decorative in design, painted with thick, flat surfaces, yet with a weight reminiscent of Gustave Courbet. The 'props' for his works are still to be found, as he left them, in his studio (atelier), in the suburbs of modern Aix.
Cézanne's paintings were not well received among the petty bourgeoisie of Aix. In 1903 Henri Rochefort visited the auction of paintings that had been in Zola's possession and published on 9 March 1903 in L'Intransigeant a highly critical article entitled "Love for the Ugly". Rochefort describes how spectators had supposedly experienced laughing fits, when seeing the paintings of "an ultra-impressionist named Cézanne". The public in Aix was outraged, and for many days, copies of L'Intransigeant appeared on Cézanne's door-mat with messages asking him to leave the town "he was dishonouring".

One day, Cézanne was caught in a storm while working in the field. After working for two hours he decided to go home; but on the way he collapsed. He was taken home by a passing driver. His old housekeeper rubbed his arms and legs to restore the circulation; as a result, he regained consciousness. On the following day, he intended to continue working, but later on he fainted; the model with whom he was working called for help; he was put to bed, and he never left it. He died a few days later, on 22 October 1906 of pneumonia and was buried at the Saint-Pierre Cemetery in his hometown of Aix-en-Provence.

Various periods in the work and life of Cézanne have been defined.

In 1863 Napoleon III created by decree the Salon des Refusés, at which paintings rejected for display at the Salon of the Académie des Beaux-Arts were to be displayed. The artists of the refused works included the young Impressionists, who were considered revolutionary. Cézanne was influenced by their style but his social relations with them were inept—he seemed rude, shy, angry, and given to depression. His works of this period are characterized by dark colours and the heavy use of black. They differ sharply from his earlier watercolours and sketches at the École Spéciale de dessin at Aix-en-Provence in 1859, and their violence of expression is in contrast to his subsequent works.

In 1866–67, inspired by the example of Courbet, Cézanne painted a series of paintings with a palette knife. He later called these works, mostly portraits, "une couillarde" ("a coarse word for ostentatious virility"). Lawrence Gowing has written that Cézanne's palette knife phase "was not only the invention of modern expressionism, although it was incidentally that; the idea of art as emotional ejaculation made its first appearance at this moment".

Among the "couillarde" paintings are a series of portraits of his uncle Dominique in which Cézanne achieved a style that "was as unified as Impressionism was fragmentary".
Later works of the dark period include several erotic or violent subjects, such as "Women Dressing" (), "The Rape" (), and "The Murder" (), which depicts a man stabbing a woman who is held down by his female accomplice.

After the start of the Franco-Prussian War in July 1870, Cézanne and his mistress, Marie-Hortense Fiquet, left Paris for L'Estaque, near Marseilles, where he changed themes to predominantly landscapes. He was declared a draft dodger in January 1871, but the war ended the next month, in February, and the couple moved back to Paris, in the summer of 1871. After the birth of their son Paul in January 1872, in Paris, they moved to Auvers in Val-d'Oise near Paris. Cézanne's mother was kept a party to family events, but his father was not informed of Hortense for fear of risking his wrath. The artist received from his father a monthly allowance of 100 francs.
Camille Pissarro lived in Pontoise. There and in Auvers he and Cézanne painted landscapes together. For a long time afterwards, Cézanne described himself as Pissarro's pupil, referring to him as "God the Father", as well as saying: "We all stem from Pissarro." Under Pissarro's influence Cézanne began to abandon dark colours and his canvases grew much brighter.

Leaving Hortense in the Marseille region, Cézanne moved between Paris and Provence, exhibiting in the first (1874) and third Impressionist shows (1877). In 1875, he attracted the attention of the collector , whose commissions provided some financial relief. But Cézanne's exhibited paintings attracted hilarity, outrage, and sarcasm. Reviewer Louis Leroy said of Cézanne's portrait of Chocquet: "This peculiar looking head, the colour of an old boot might give [a pregnant woman] a shock and cause yellow fever in the fruit of her womb before its entry into the world."

In March 1878, Cézanne's father found out about Hortense and threatened to cut Cézanne off financially, but, in September, he relented and decided to give him 400 francs for his family. Cézanne continued to migrate between the Paris region and Provence until Louis-Auguste had a studio built for him at his home, Bastide du Jas de Bouffan, in the early 1880s. This was on the upper floor, and an enlarged window was provided, allowing in the northern light but interrupting the line of the eaves. This feature remains today. Cézanne stabilized his residence in L'Estaque. He painted with Renoir there in 1882 and visited Renoir and Monet in 1883.

In the early 1880s the Cézanne family stabilized their residence in Provence where they remained, except for brief sojourns abroad, from then on. The move reflects a new independence from the Paris-centered impressionists and a marked preference for the south, Cézanne's native soil. Hortense's brother had a house within view of Montagne Sainte-Victoire at Estaque. A run of paintings of this mountain from 1880 to 1883 and others of Gardanne from 1885 to 1888 are sometimes known as "the Constructive Period".

The year 1886 was a turning point for the family. Cézanne married Hortense. In that year also, Cézanne's father died, leaving him the estate purchased in 1859; he was 47. By 1888 the family was in the former manor, Jas de Bouffan, a substantial house and grounds with outbuildings, which afforded a new-found comfort. This house, with much-reduced grounds, is now owned by the city and is open to the public on a restricted basis.

For many years it was believed that Cézanne broke off his friendship with Émile Zola, after the latter used him, in large part, as the basis for the unsuccessful and ultimately tragic fictitious artist Claude Lantier, in the novel "L'Œuvre".

Recently letters have been discovered that refute this. A letter from 1887 demonstrates that their friendship endured.

Cézanne's idyllic period at Jas de Bouffan was temporary. From 1890 until his death he was beset by troubling events and he withdrew further into his painting, spending long periods as a virtual recluse. His paintings became well-known and sought after and he was the object of respect from a new generation of painters.
The problems began with the onset of diabetes in 1890, destabilizing his personality to the point where relationships with others were again strained. He traveled in Switzerland, with Hortense and his son, perhaps hoping to restore their relationship. Cézanne, however, returned to Provence to live; Hortense and Paul junior, to Paris. Financial need prompted Hortense's return to Provence but in separate living quarters. Cézanne moved in with his mother and sister. In 1891 he turned to Catholicism.

Cézanne alternated between painting at Jas de Bouffan and in the Paris region, as before. In 1895, he made a germinal visit to Bibémus Quarries and climbed Montagne Sainte-Victoire. The labyrinthine landscape of the quarries must have struck a note, as he rented a cabin there in 1897 and painted extensively from it. The shapes are believed to have inspired the embryonic "Cubist" style. Also in that year, his mother died, an upsetting event but one which made reconciliation with his wife possible. He sold the empty nest at Jas de Bouffan and rented a place on Rue Boulegon, where he built a studio.

The relationship, however, continued to be stormy. He needed a place to be by himself. In 1901 he bought some land along the Chemin des Lauves, an isolated road on some high ground at Aix, and commissioned a studio to be built there (now open to the public). He moved there in 1903. Meanwhile, in 1902, he had drafted a will excluding his wife from his estate and leaving everything to his son. The relationship was apparently off again; she is said to have burned the mementos of his mother.

From 1903 to the end of his life he painted in his studio, working for a month in 1904 with Émile Bernard, who stayed as a house guest. After his death it became a monument, Atelier Paul Cézanne, or les Lauves.

Cézanne's stylistic approaches and beliefs regarding how to paint were analyzed and written about by the French philosopher Maurice Merleau-Ponty who is primarily known for his association with phenomenology and existentialism. In his 1945 essay entitled "Cézanne's Doubt", Merleau-Ponty discusses how Cézanne gave up classic artistic elements such as pictorial arrangements, single view perspectives, and outlines that enclosed color in an attempt to get a "lived perspective" by capturing all the complexities that an eye observes. He wanted to see and sense the objects he was painting, rather than think about them. Ultimately, he wanted to get to the point where "sight" was also "touch". He would take hours sometimes to put down a single stroke because each stroke needed to contain "the air, the light, the object, the composition, the character, the outline, and the style". A still life might have taken Cézanne one hundred working sessions while a portrait took him around one hundred and fifty sessions. Cèzanne believed that while he was painting, he was capturing a moment in time, that once passed, could not come back. The atmosphere surrounding what he was painting was a part of the sensational reality he was painting. Cèzanne claimed: "Art is a personal apperception, which I embody in sensations and which I ask the understanding to organize into a painting."

Cézanne's works were rejected numerous times by the official Salon in Paris and ridiculed by art critics when exhibited with the Impressionists. Yet during his lifetime Cézanne was considered a master by younger artists who visited his studio in Aix.

After Cézanne died in 1906, his paintings were exhibited in a large museum-like retrospective in Paris, September 1907. The 1907 Cézanne retrospective at the Salon d'Automne greatly affected the direction that the avant-garde in Paris took, lending credence to his position as one of the most influential artists of the 19th century and to the advent of Cubism.

Inspired by Cézanne, two of the younger artists wrote:
Cézanne is one of the greatest of those who changed the course of art history . . . From him we have learned that to alter the coloring of an object is to alter its structure. His work proves without doubt that painting is not—or not any longer—the art of imitating an object by lines and colors, but of giving plastic [solid, but alterable] form to our nature. (Albert Gleizes and Jean Metzinger in "Du "Cubisme"", 1912)

Cézanne's explorations of geometric simplification and optical phenomena inspired Picasso, Braque, Metzinger, Gleizes, Gris and others to experiment with ever more complex multiple views of the same subject and eventually to the fracturing of form. Cézanne thus sparked one of the most revolutionary areas of artistic enquiry of the 20th century, one which was to affect profoundly the development of modern art. Picasso referred to Cézanne as "the father of us all" and claimed him as "my one and only master!" Other painters such as Edgar Degas, Pierre-Auguste Renoir, Paul Gauguin, Kasimir Malevich, Georges Rouault, Paul Klee, and Henri Matisse acknowledged Cézanne's genius.

A prize in his memory, called the Cézanne medal, is granted by the city of Aix en Provence, in France for special achievement in the arts.

Cézanne's painting "The Boy in the Red Vest" was stolen from a Swiss museum in 2008. It was recovered in a Serbian police raid in 2012.

The 2016 film "Cézanne and I" explores the friendship between the artist and Émile Zola.






</doc>
<doc id="24473" url="https://en.wikipedia.org/wiki?curid=24473" title="Pope Innocent VI">
Pope Innocent VI

Pope Innocent VI (; 1282 or 1295 – 12 September 1362), born Étienne Aubert, was Pope from 18 December 1352 to his death in 1362. He was the fifth Avignon Pope and the only one with the pontifical name of "Innocent".

Étienne's father was Adhemar Aubert (1260-?), seigneur de Montel-de-Gelat in Limousin province. He was a native of the hamlet of Les Monts, Diocese of Limoges (today part of the commune of Beyssac, "département" of Corrèze), and, after having taught civil law at Toulouse, he became successively Bishop of Noyon in 1338 and Bishop of Clermont in 1340. On 20 September 1342, he was raised to the position of Cardinal Priest of SS. John and Paul. He was made cardinal-bishop of Ostia and Velletri on 13 February 1352, by Pope Clement VI, whom he succeeded.

Etienne was crowned pope on 30 December 1352 by Cardinal Gaillard de la Mothe after the papal conclave of 1352. Upon his election, he revoked a signed agreement stating the college of cardinals was superior to the pope. His subsequent policy compares favourably with that of the other Avignon Popes. He introduced many needed reforms in the administration of church affairs, and through his legate, Cardinal Albornoz, who was accompanied by Rienzi, he sought to restore order in Rome. In 1355, Charles IV, Holy Roman Emperor, was crowned in Rome with Innocent's permission, after having made an oath that he would quit the city on the day of the ceremony.

It was largely through the exertions of Innocent VI that the Treaty of Brétigny (1360) between France and England was brought about. During his pontificate, the Byzantine emperor John V Palaeologus offered to submit the Greek Orthodox Church to the Roman See in return for assistance against John VI Cantacuzenus. The resources at the disposal of the Pope, however, were all required for exigencies nearer home, and the offer was declined.

Most of the wealth accumulated by John XXII and Benedict XII had been lost during the extravagant pontificate of Clement VI. Innocent VI economised by cutting the chapel staff ("capellani capelle") from twelve to eight. Works of art were sold rather than commissioned. His pontificate was dominated by the war in Italy and by Avignon's recovery from the plague, both of which made draining demands on his treasury. By 1357, he was complaining of poverty.

Innocent VI was a liberal patron of letters. If the extreme severity of his measures against the Fraticelli is ignored, he retains a high reputation for justice and mercy. However, St. Bridget of Sweden denounced him as a persecutor of Christians. He died on 12 September 1362 and was succeeded by Urban V. Today his tomb can be found in the Chartreuse du Val de Bénédiction, the Carthusian monastery in Villeneuve-les-Avignon.


 
 


</doc>
<doc id="24474" url="https://en.wikipedia.org/wiki?curid=24474" title="Polyandry">
Polyandry

Polyandry (; from "poly-", "many" and ἀνήρ "anēr", "man") is a form of polygamy in which a woman takes two or more husbands at the same time. Polyandry is contrasted with polygyny, involving one male and two or more females. If a marriage involves a plural number of "husbands and wives" participants of each gender, then it can be called polyamory, group or conjoint marriage. In its broadest use, polyandry refers to sexual relations with multiple males within or without marriage.

Of the 1,231 societies listed in the 1980 Ethnographic Atlas, 186 were found to be monogamous; 453 had occasional polygyny; 588 had more frequent polygyny; and 4 had polyandry. Polyandry is less rare than this figure suggests, as it considered only those examples found in the Himalayan mountains (28 societies). More recent studies have found more than 50 other societies practicing polyandry.<ref name="Starkweather/Hames 2012"></ref>

Fraternal polyandry is practiced among Tibetans in Nepal, parts of China and part of northern India, in which two or more brothers are married to the same wife, with the wife having equal "sexual access" to them. It is associated with "partible paternity", the cultural belief that a child can have more than one father.

Polyandry is believed to be more likely in societies with scarce environmental resources. It is believed to limit human population growth and enhance child survival. It is a rare form of marriage that exists not only among peasant families but also among the elite families. For example, polyandry in the Himalayan mountains is related to the scarcity of land. The marriage of all brothers in a family to the same wife allows family land to remain intact and undivided. If every brother married separately and had children, family land would be split into unsustainable small plots. In contrast, very poor persons not owning land were less likely to practice polyandry in Buddhist Ladakh and Zanskar. In Europe, the splitting up of land was prevented through the social practice of impartible inheritance. With most siblings disinherited, many of them became celibate monks and priests.

Polyandrous mating systems are also a common phenomenon in the animal kingdom.

In the Indian Himalayas, polyandry may be combined with polygyny to produce a system termed "polygynandry". The system results in less land fragmentation, a diversification of domestic economic activities, and lower population growth.

Fraternal polyandry (from the Latin "frater"—brother), also called adelphic polyandry, is a form of polyandry in which a woman is married to two or more men who are brothers. Fraternal polyandry was (and sometimes still is) found in certain areas of Tibet, Nepal, and Northern India, where polyandry was accepted as a social practice. The Toda people of southern India practice fraternal polyandry, but monogamy has become prevalent recently. In contemporary Hindu society, polyandrous marriages in agrarian societies in the Malwa region of Punjab seem to occur to avoid division of farming land.

Fraternal polyandry achieves a similar goal to that of primogeniture in 19th-century England. Primogeniture dictated that the eldest son inherited the family estate, while younger sons had to leave home and seek their own employment. Primogeniture maintained family estates intact over generations by permitting only one heir per generation. Fraternal polyandry also accomplishes this, but does so by keeping all the brothers together with just one wife so that there is only one set of heirs per generation. This strategy appears less successful the larger the fraternal sibling group is.

Some forms of polyandry appear to be associated with a perceived need to retain aristocratic titles or agricultural lands within kin groups, and/or because of the frequent absence, for long periods, of a man from the household. In Tibet the practice was particularly popular among the priestly Sakya class.

The female equivalent of fraternal polyandry is sororate marriage.

Anthropologist Stephen Beckerman points out that at least 20 tribal societies accept that a child could, and ideally should, have more than one father, referring to it as "partible paternity". This often results in the shared nurture of a child by multiple fathers in a form of polyandric relation to the mother, although this is not always the case. One of the most well known examples is that of Trobriand "virgin birth". The matrilineal Trobriand Islanders recognize the importance of sex in reproduction but do not believe the male makes a contribution to the constitution of the child, who therefore remains attached to their mother's lineage alone. The mother's non-resident husbands are not recognized as fathers, although the mother's co-resident brothers are, since they are part of the mother's lineage.

According to inscriptions describing the reforms of the Sumerian king Urukagina of Lagash (ca. 2300 BC), the earlier custom of polyandry in his country was abolished, on pain of the woman taking multiple husbands being stoned upon which her crime is written.

An extreme gender imbalance has been suggested as a justification for polyandry. For example, the selective abortion of female fetuses in India has led to a significant margin in sex ratio and, it has been suggested, results in related men "sharing" a wife.

Polyandry in Tibet was a common practice and continues to a lesser extent today. A survey of 753 Tibetan families by Tibet University in 1988 found that 13% practiced polyandry. Polyandry in India still exists among minorities, and also in Bhutan, and the northern parts of Nepal. Polyandry has been practised in several parts of India, such as Rajasthan, Ladakh and Zanskar, in the Jaunsar-Bawar region in Uttarakhand, among the Toda of South India.

It also occurs or has occurred in Nigeria, the Nymba, and some pre-contact Polynesian societies, though probably only among higher caste women. It is also encountered in some regions of Yunnan and Sichuan regions of China, among the Mosuo people in China (who also practice polygyny as well), and in some sub-Saharan African such as the Maasai people in Kenya and northern Tanzania and American indigenous communities. The Guanches, the first known inhabitants of the Canary Islands, practiced polyandry until their disappearance. The Zo'e tribe in the state of Pará on the Cuminapanema River, Brazil, also practice polyandry.







There is at least one reference to polyandry in the ancient Hindu epic "Mahabharata". Draupadi married the five Pandava brothers, as this is what she chose in a previous life. This ancient text remains largely neutral to the concept of polyandry, accepting this as her way of life. However, in the same epic, when questioned by Kunti to give an example of polyandry, Yudhishthira cites Gautam-clan Jatila (married to seven Saptarishis) and Hiranyaksha's sister Pracheti (married to ten brothers), thereby implying a more open attitude toward polyandry in Vedic society.

The Hebrew Bible contains no examples of women married to more than one man, but its description of adultery clearly implies that polyandry is unacceptable and the practice is unknown in Jewish tradition. In addition, the children from other than the first husband are considered illegitimate, unless he has already divorced her or died (i.e., a mamzer), being a product of an adulterous relationship.

Most Christian denominations in the Western world strongly advocate monogamous marriage, and a passage from the Pauline epistles () can be interpreted as forbidding polyandry.

Joseph Smith and Brigham Young, and other early Latter-day Saints, practiced polygynous marriages. The practice was officially ended with the 1890 Manifesto. Polyandrous marriages did exist, albeit in significantly less numbers, in early LDS history.
Although Islamic marital law allows men to have up to four wives, polyandry is prohibited in Islam. 

Polyandrous marriages were practiced in pre-Islamic Arabian cultures, but were outlawed during the rise of Islam. Nikah Ijtimah was a pagan tradition of polyandry in older Arab regions which was condemned and abolished during the rise of Islam.

Polyandrous behavior is quite widespread in the animal kingdom. It is prominent in many species of insects and fish (for example pipefish; see Polyandry in fish). It is also found in other animals such as birds (for example dunnocks), whales, and in some mammals such as the house mouse.

Among the whales, polyandrous behavior has been noted among the bowhead, harbour porpoise ("Phocoena phocoena"), and humpback whales.

Among the relevant insect species are the honeybees, the red flour beetle, the species of spider" Stegodyphus lineatus", the crickets "Gryllus bimaculatus", and the fruit fly "Drosophila pseudoobscura".

Polyandry also occurs in some primates such as marmosets, and in the marsupial genus' "Antechinus".

Types of mating, marriage and lifestyle:



</doc>
<doc id="24475" url="https://en.wikipedia.org/wiki?curid=24475" title="Polygamy">
Polygamy

Polygamy (from Late Greek , "polygamía", "state of marriage to many spouses") is the practice of marrying multiple spouses. When a man is married to more than one wife at a time, sociologists call this polygyny. When a woman is married to more than one husband at a time, it is called polyandry. If a marriage includes multiple husbands and wives, it can be called a group marriage.

In contrast, monogamy is marriage consisting of only two parties. Like "monogamy", the term "polygamy" is often used in a "de facto" sense, applied regardless of whether the state recognizes the relationship. In sociobiology and zoology, researchers use "polygamy" in a broad sense to mean any form of multiple mating.

Worldwide, different societies variously encourage, accept or outlaw polygamy. Of societies which allow or tolerate polygamy, in the vast majority of cases the form accepted is polygyny. According to the Ethnographic Atlas (1998), of 1,231 societies noted, 588 had frequent polygyny, 453 had occasional polygyny, 186 were monogamous and 4 had polyandry; although more recent research suggests polyandry may be more common than previously thought.<ref name="Starkweather/Hames 2012"></ref> From a religious point of view, "The bible shows over 36 named men who had more than one wife. "In cultures which practice polygamy, its prevalence among that population is often connected to class and socioeconomic status.

From a legal point of view, in many countries, although marriage is legally monogamous (a person can only have one spouse, and bigamy is illegal), adultery is not illegal, leading to a situation of "de facto" polygamy being allowed, although without legal recognition for non-official "spouses".

According to scientific studies, the human mating system is considered to be moderately polygynous, based on both surveys of world populations, and on characteristics of human reproductive physiology.

Polygamy exists in three specific forms:

Polygamy is also common among some animals, such as the common fruit-fly, "Drosophila melanogaster".

Polygyny, the practice wherein a man has more than one wife at the same time, is by far the most common form of polygamy. Polygyny is legally accepted in many Muslim majority countries and some countries with a sizeable Muslim minority; it is also accepted in some secular countries to varying degrees.

Polygyny is more widespread in Africa than in any other continent, especially in West Africa, and some scholars see the slave trade's impact on the male-to-female sex ratio as a key factor in the emergence and fortification of polygynous practices in regions of Africa.

Anthropologist Jack Goody's comparative study of marriage around the world utilizing the Ethnographic Atlas demonstrated an historical correlation between the practice of extensive shifting horticulture and polygamy in the majority of sub-Saharan African societies. Drawing on the work of Ester Boserup, Goody notes that the sexual division of labour varies between the male-dominated intensive plough-agriculture common in Eurasia and the extensive shifting horticulture found in sub-Saharan Africa. In some of the sparsely populated regions where shifting cultivation takes place in Africa, women do much of the work. This favours polygamous marriages in which men sought to monopolize the production of women "who are valued both as workers and as child bearers". Goody, however, observes that the correlation is imperfect and varied, and also discusses more traditionally male-dominated though relatively extensive farming systems such as those that exist in much of West Africa, especially in the West African savanna, where polygyny is desired more for the creation of male offspring, whose labor is valued.

Anthropologists Douglas R. White and Michael L. Burton discuss and support Jack Goody's observation regarding African male farming systems in "Causes of Polygyny: Ecology, Economy, Kinship, and Warfare" where these authors note: "Goody (1973) argues against the female contributions hypothesis. He notes Dorjahn's (1959) comparison of East and West Africa, showing higher female agricultural contributions in East Africa and higher polygyny rates in West Africa, especially the West African savanna, where one finds especially high male agricultural contributions. Goody says, "The reasons behind polygyny are sexual and reproductive rather than economic and productive" (1973:189), arguing that men marry polygynously to maximize their fertility and to obtain large households containing many young dependent males."

Polygynous marriages fall into two types: "sororal polygyny", in which the co-wives are sisters, and "non-sororal", where the co-wives are not related. Polygyny offers husbands the benefit of allowing them to have more children, may provide them with a larger number of productive workers (where workers are family), and allows them to establish politically useful ties with a greater number of kin groups. Senior wives can benefit as well when the addition of junior wives to the family lightens their workload. Wives', especially senior wives', status in a community can increase through the addition of other wives, who add to the family's prosperity or symbolize conspicuous consumption (much as a large house, domestic help, or expensive vacations operate in a western country). For such reasons, senior wives sometimes work hard or contribute from their own resources to enable their husbands to accumulate the bride price for an extra wife.

Polygyny may also result from the practice of levirate marriage. In such cases, the deceased man's heir may inherit his assets and wife; or, more usually, his brothers may marry the widow. This provides support for the widow and her children (usually also members of the brothers' kin group) and maintains the tie between the husbands' and wives' kin groups. The sororate resembles the levirate, in that a widower must marry the sister of his dead wife. The family of the late wife, in other words, must provide a replacement for her, thus maintaining the marriage alliance. Both levirate and sororate may result in a man having multiple wives.

In monogamous societies, wealthy and powerful men established enduring relationships with, and established separate household for, multiple female partners, aside from their legitimate wives; a practice accepted in Imperial China up until the Qing Dynasty of 1636-1912. This constitutes a form of "de facto" polygyny referred to as concubinage.

Marriage is the moment at which a new household is formed, but different arrangements may occur depending upon the type of marriage and some polygamous marriages do not result in the formation of a single household. In many polygynous marriages the husband's wives may live in separate households They can thus be described as a "series of linked nuclear families with a 'father' in common".

Polyandry, the practice of a woman having more than one husband at the one time, is much less prevalent than polygyny and is now illegal in virtually every country in the world. It takes place only in remote communities.

Polyandry is believed to be more likely in take place societies with scarce environmental resources, as it is believed to limit human population growth and enhance child survival. It is a rare form of marriage that exists not only among poor families, but also the elite. For example, in the Himalayan Mountains polyandry is related to the scarcity of land; the marriage of all brothers in a family to the same wife allows family land to remain intact and undivided. If every brother married separately and had children, family land would be split into unsustainable small plots. In Europe, this outcome was avoided through the social practice of impartible inheritance, under which most siblings would be disinherited.

"Fraternal polyandry" was traditionally practiced among nomadic Tibetans in Nepal, parts of China and part of northern India, in which two or more brothers would marry the same woman. It is most common in societies marked by high male mortality. It is associated with "partible paternity", the cultural belief that a child can have more than one father.

"Non-fraternal polyandry" occurs when the wives' husbands are unrelated, as among the Nayar tribe of India, where girls undergo a ritual marriage before puberty, and the first husband is acknowledged as the father of all her children. However, the woman may never cohabit with that man, taking multiple lovers instead; these men must acknowledge the paternity of their children (and hence demonstrate that no caste prohibitions have been breached) by paying the midwife. The women remain in their maternal home, living with their brothers, and property is passed matrilineally. A similar form of matrilineal, de facto polyandry can be found in the institution of walking marriage among the Mosuo tribe of China.

Serial monogamy refers to remarriage after divorce or death of a spouse from a monogamous marriage, i.e. multiple marriages but only one legal spouse at a time (a series of monogamous relationships).

According to Danish scholar Miriam K. Zeitzen, anthropologists treat serial monogamy, in which divorce and remarriage occur, as a form of polygamy as it also can establish a series of households that may continue to be tied by shared paternity and shared income. As such, they are similar to the household formations created through divorce and serial monogamy.

Serial monogamy creates a new kind of relative, the "ex-". The "ex-wife", for example, can remain an active part of her "ex-husband's" life, as they may be tied together by legally or informally mandated economic support, which can last for years, including by alimony, child support, and joint custody. Bob Simpson notes that in the British case, it creates an "extended family", that is, a number of households tied together in this way, including mobile children, noting that Britons may have ex‑wives or ex‑brothers‑in‑law, but not an "ex‑child". According to him, these "unclear families" do not fit the mold of the monogamous nuclear family.

Group marriage is a marriage wherein the family unit consists of more than two partners, any of whom share parental responsibility for any children arising from the marriage. Group marriage is a form of non-monogamy and polyamory.

The Rig Veda mentions that during the Vedic period, a man could have more than one wife. The practice is attested in epics like Ramayana and Mahabharata. The Dharmashastras permit a man to marry women of lower castes provided that the first wife was of equal caste. Despite its existence, it was most usually practiced by men of higher castes and higher status. Common people were only allowed a second marriage if the first wife could not bear a son.

According to Vishnu Smriti, the number of wives is linked to the caste system:
This linkage of the number of permitted wives to the caste system is also supported by Baudhayana Dharmasutra and Paraskara Grihyasutra.

The Apastamba Dharmasutra and Manusmriti allow a second wife if the first one is unable to discharge her religious duties or is unable to bear a son.

For a Brahmana, only one wife could rank as the chief consort who performed the religious rites ("dharma-patni") along with the husband. The chief consort had to be of an equal caste. If a man married several women from the same caste, then eldest wife is the chief consort. Hindu kings commonly had more than one wife and are regularly attributed four wives by the scriptures. They were: Mahisi who was the chief consort, Parivrkti who had no son, Vaivata who is considered the favorite wife and the Palagali who was the daughter of the last of the court officials.

The other practice though not well documented is polyandry, where a woman marries more than one man. Draupadi in the epic Mahabharat had five husbands: the Pandavas.

Traditional Hindu law allowed polygamy if the first wife could not bear a son.

The Hindu Marriage Act was enacted in 1955 by the Indian Parliament and made polygamy illegal for everyone in India except for Muslims. Prior to 1955, polygamy was permitted for Hindus. Marriage laws in India are dependent upon the religion of the parties in question.

In Buddhism, marriage is not a sacrament. It is purely a secular affair and the monks do not participate in it, though in some sects priests and monks do marry. Hence it receives no religious sanction. Forms of marriage consequently vary from country to country. It is said in the Parabhava Sutta that "a man who is not satisfied with one woman and seeks out other women is on the path to decline". Other fragments in the Buddhist scripture can be found that seem to treat polygamy unfavorably, leading some authors to conclude that Buddhism generally does not approve of it or alternatively that it is a tolerated, but subordinate marital model.

Until 2010, polygyny was legally recognized in Thailand. In Myanmar, polygyny is outlawed since 2015. In Sri Lanka, polyandry was practiced (though not widespread) until recent times. When the Buddhist texts were translated into Chinese, the concubines of others were added to the list of inappropriate partners. Polyandry in Tibet as well was common traditionally, as was polygyny, and having several wives or husbands was never regarded as having sex with inappropriate partners. Most typically, fraternal polyandry is practiced, but sometimes father and son have a common wife, which is a unique family structure in the world. Other forms of marriage are also present, like group marriage and monogamous marriage. Polyandry (especially fraternal polyandry) is also common among Buddhists in Bhutan, Ladakh, and other parts of the Indian subcontinent.

Some pre-Christian Celtic pagans were known to practice polygamy, although the Celtic peoples wavered between it, monogamy and polyandry depending on the time period and area. In some areas this continued even after Christianization began, for instance the Brehon Laws of Gaelic Ireland explicitly allowed for polygamy, especially amongst the noble class. Some modern Celtic pagan religions accept the practice of polygamy to varying degrees, though how widespread the practice is within these religions is unknown.

The Torah contains a few specific regulations that apply to polygamy, such as Exodus 21:10: "If he take another wife for himself; her food, her clothing, and her duty of marriage, shall he not diminish". , states that a man must award the inheritance due to a first-born son to the son who was actually born first, even if he hates that son's mother and likes another wife more; and states that the king shall not have too many wives.

The Torah may distinguish concubines and "sub-standard" wives with the prefix "to" (e.g., lit. "took to wives"). Despite these nuances to the biblical perspective on polygamy, many important figures had more than one wife, such as in the instances of Esau (Gen 26:34; 28:6-9), Jacob (Gen 29:15-28), Elkanah (1 Samuel 1:1-8), David (1 Samuel 25:39-44; 2 Samuel 3:2-5; 5:13-16), and Solomon (1 Kings 11:1-3).

Multiple marriage was considered a realistic alternative in the case of famine, widowhood, or female infertility like in the practice of levirate marriage, wherein a man was required to marry and support his deceased brother's widow, as mandated by . Despite its prevalence in the Hebrew Bible, scholars do not believe that polygyny was commonly practiced in the biblical era because it required a significant amount of wealth. Michael Coogan, in contrast, states that "Polygyny continued to be practiced well into the biblical period, and it is attested among Jews as late as the second century CE".

The monogamy of the Roman Empire was the cause of two explanatory notes in the writings of Josephus describing how the polygamous marriages of Herod the Great were permitted under Jewish custom.
The Rabbinical era that began with the destruction of the second temple in Jerusalem in 70 CE saw a continuation of some degree of legal acceptance for polygamy. In the Babylonian Talmud (BT), Kiddushin 7a, its states, "Raba said: 'Be thou betrothed to half of me,' she is betrothed: 'half of thee be betrothed to me,' she is not betrothed." The BT during a discussion of Levirate marriage in Yevamot 65a appears to repeat the precedent found in Exodus 21:10: "Raba said: a man may marry wives in addition to the first wife; provided only that he possesses the means to maintain them". The Jewish Codices began a process of restricting polygamy in Judaism.

Maimonides, in his Mishneh Torah maintained that polygamous unions were permissible from a legal point of view, which was contrary to his personal opinion. The Mishneh Torah, while maintaining the right to multiple spouses, and the requirement to provide fully for each as indicated in previously cited sources, went further: "He may not, however, compel his wives to live in the same courtyard. Instead, each one is entitled to her own household".

The only case of a polygamous rabbi recorded in the Talmud[null 5] provides an excellent illustration: Rabbi Tarfon married 300 women. Why? Because there was a famine in the land. But Rabbi Tarfon had plenty of food, since he was a "kohen" and received the priestly tithes. The wife of a "kohen" is also permitted to eat those tithes. Those 300 women were very happy that the Torah permitted polygamy.

The Shulchan Aruch, builds on all of the previous works by adding further nuances: "...but in any event, our sages have advised well not to marry more than four wives, in order that he can meet their conjugal needs at least once a month. And in a place where it is customary to marry only one wife, he is not permitted to take another wife on top of his present wife." As can be seen, while the tradition of the Rabbinic period began with providing legal definition for the practice of polygamy (although this does not indicate the frequency with which polygamy in fact occurred) that corresponded to precedents in the tanakh, by the time of the Codices the Rabbis had greatly reduced or eliminated sanction of the practice.

Most notable in the Rabbinic period on the issue of polygamy, though more specifically for Ashkenazi Jews, was the synod of Rabbeinu Gershom. About 1000 CE he called a synod which decided the following particulars: (1) prohibition of polygamy; (2) necessity of obtaining the consent of both parties to a divorce; (3) modification of the rules concerning those who became apostates under compulsion; (4) prohibition against opening correspondence addressed to another.

In the modern day, polygamy is almost nonexistent in Rabbinic Judaism. Ashkenazi Jews have continued to follow Rabbenu Gershom's ban since the 11th century. Some Mizrahi Jewish communities (particularly Yemenite Jews and Persian Jews) discontinued polygyny more recently, after they immigrated to countries where it was forbidden or illegal. Israel prohibits polygamy by law. In practice, however, the law is loosely enforced, primarily to avoid interference with Bedouin culture, where polygyny is practiced. Pre-existing polygynous unions among Jews from Arab countries (or other countries where the practice was not prohibited by their tradition and was not illegal) are not subject to this Israeli law. But Mizrahi Jews are not permitted to enter into new polygamous marriages in Israel. However polygamy may still occur in non-European Jewish communities that exist in countries where it is not forbidden, such as Jewish communities in Yemen and the Arab world.

Among Karaite Jews, who do not adhere to Rabbinic interpretations of the Torah, polygamy is almost non-existent today. Like other Jews, Karaites interpret to mean that a man can only take a second wife if his first wife gives her consent (Keter Torah on Leviticus, pp. 96–97) and Karaites interpret Exodus 21:10 to mean that a man can only take a second wife if he is capable of maintaining the same level of marital duties due to his first wife; the marital duties are 1) food, 2) clothing, and 3) sexual gratification. Because of these two biblical limitations and because most countries outlaw it, polygamy is considered highly impractical, and there are only a few known cases of it among Karaite Jews today.

Israel has made polygamy illegal. Provisions were instituted to allow for existing polygamous families immigrating from countries where the practice was legal. Furthermore, former chief rabbi Ovadia Yosef has come out in favor of legalizing polygamy and the practice of pilegesh (concubine) by the Israeli government.

Tzvi Zohar, a professor from the Bar-Ilan University, recently suggested that based on the opinions of leading halachic authorities, the concept of concubines may serve as a practical Halachic justification for premarital or non-marital cohabitation.

Polygamy is not forbidden in the Old Testament. Although the New Testament is largely silent on polygamy, some point to Jesus's repetition of the earlier scriptures, noting that a man and a wife "shall become one flesh". However, some look to Paul's writings to the Corinthians: "Do you not know that he who is joined to a prostitute becomes one body with her? For, as it is written, 'The two will become one flesh. Supporters of polygamy claim this indicates that the term refers to a physical, rather than spiritual, union.

Some Christian theologians argue that in and referring to Jesus explicitly states a man should have only one wife:
The Bible states in the New Testament that polygamy should not be practiced by certain church leaders. 1 Timothy states that certain Church leaders should have but one wife: "A "bishop" then must be blameless, the husband of one wife, vigilant, sober, of good behavior, given to hospitality, apt to teach" (chapter 3, verse 2; see also verse 12 regarding deacons having only one wife). Similar counsel is repeated in the first chapter of the Epistle to Titus.

Periodically, Christian reform movements that have aimed at rebuilding Christian doctrine based on the Bible alone ("sola scriptura") have at least temporarily accepted polygyny as a Biblical practice. For example, during the Protestant Reformation, in a document referred to simply as ""Der Beichtrat"" (or ""The Confessional Advice"" ), Martin Luther granted the Landgrave Philip of Hesse, who, for many years, had been living "constantly in a state of adultery and fornication", a dispensation to take a second wife. The double marriage was to be done in secret, however, to avoid public scandal. Some fifteen years earlier, in a letter to the Saxon Chancellor Gregor Brück, Luther stated that he could not "forbid a person to marry several wives, for it does not contradict Scripture." (""Ego sane fateor, me non posse prohibere, si quis plures velit uxores ducere, nec repugnat sacris literis."")

In Sub-Saharan Africa, there has often been a tension between the Christian insistence on monogamy and traditional polygamy. For instance, Mswati III, the Christian king of Swaziland, has 15 wives. In some instances in recent times there have been moves for accommodation; in other instances, churches have resisted such moves strongly. African Independent Churches have sometimes referred to those parts of the Old Testament that describe polygamy in defending the practice.

The Roman Catholic Church condemns polygamy; the "Catechism of the Catholic Church" lists it in paragraph 2387 under the head "Other offenses against the dignity of marriage" and states that it "is not in accord with the moral law." Also in paragraph 1645 under the head "The Goods and Requirements of Conjugal Love" states "The unity of marriage, distinctly recognized by our Lord, is made clear in the equal personal dignity which must be accorded to husband and wife in mutual and unreserved affection. Polygamy is contrary to conjugal love which is undivided and exclusive."

Saint Augustine saw a conflict with Old Testament polygamy. He refrained from judging the patriarchs, but did not deduce from their practice the ongoing acceptability of polygyny. On the contrary, he argued that the polygamy of the Fathers, which was tolerated by the Creator because of fertility, was a diversion from His original plan for human marriage. Augustine wrote: "That the good purpose of marriage, however, is better promoted by one husband with one wife, than by a husband with several wives, is shown plainly enough by the very first union of a married pair, which was made by the Divine Being Himself."

Augustine taught that the reason patriarchs had many wives was not because of fornication, but because they wanted more children. He supported his premise by showing that their marriages, in which husband was the head, were arranged according to the rules of good management: those who are "in command" ("quae principantur") in their society were always singular, while "subordinates" ("subiecta") were multiple. He gave two examples of such relationships: "dominus-servus" - master-servant (in older translation: "slave") and "God-soul". The Bible often equates worshiping multiple gods, i.e. idolatry to fornication. Augustine relates to that: "On this account there is no True God of souls, save One: but one soul by means of many false gods may commit fornication, but not be made fruitful."

As tribal populations grew, fertility was no longer a valid justification of polygamy: it "was lawful among the ancient fathers: whether it be lawful now also, I would not hastily pronounce (utrum et nunc fas sit, non temere dixerim). For there is not now necessity of begetting children, as there then was, when, even when wives bear children, it was allowed, in order to a more numerous posterity, to marry other wives in addition, which now is certainly not lawful."

Augustine saw marriage as a covenant between one man and one woman, which may not be broken. It was the Creator who established monogamy: "Therefore, the first natural bond of human society is man and wife." Such marriage was confirmed by the Saviour in the Gospel of Matthew (Mat 19:9) and by His presence at the wedding in Cana (John 2:2). In the Church—the City of God—marriage is a sacrament and may not and cannot be dissolved as long as the spouses live: "But a marriage once for all entered upon in the City of our God, where, even from the first union of the two, the man and the woman, marriage bears a certain sacramental character, can in no way be dissolved but by the death of one of them." In chapter 7, Augustine pointed out that the Roman Empire forbad polygamy, even if the reason of fertility would support it: "For it is in a man's power to put away a wife that is barren, and marry one of whom to have children. And yet it is not allowed; and now indeed in our times, and after the usage of Rome (nostris quidem iam temporibus ac more Romano), neither to marry in addition, so as to have more than one wife living." Further on he notices that the Church's attitude goes much further than the secular law regarding monogamy: It forbids remarrying, considering such to be a form of fornication: "And yet, save in the City of our God, in His Holy Mount, the case is not such with the wife. But, that the laws of the Gentiles are otherwise, who is there that knows not."

In modern times a minority of Roman Catholic theologians have argued that polygamy, though not ideal, can be a legitimate form of Christian marriage in certain regions, in particular Africa. The Roman Catholic Church teaches in its Catechism that 
polygamy is not in accord with the moral law. [Conjugal] communion is radically contradicted by polygamy; this, in fact, directly negates the plan of God that was revealed from the beginning, because it is contrary to the equal personal dignity of men and women who in matrimony give themselves with a love that is total and therefore unique and exclusive.

The illegality of polygamy in certain areas creates, according to certain Bible passages, additional arguments against it. Paul the Apostle writes "submit to the authorities, not only because of possible punishment but also because of conscience" (Romans 13:5), for "the authorities that exist have been established by God." (Romans 13:1) St Peter concurs when he says to "submit yourselves for the Lord's sake to every authority instituted among men: whether to the king, as the supreme authority, or to governors, who are sent by him to punish those who do wrong and to commend those who do right." (1 Peter 2:13,14) Pro-polygamists argue that, as long as polygamists currently do not obtain legal marriage licenses nor seek "common law marriage status" for additional spouses, no enforced laws are being broken any more than when monogamous couples similarly co-habitate without a marriage license.

In accordance with a revelation to Joseph Smith, the practice of plural marriage—the marriage of one man to two or more women—was instituted among members of The Church of Jesus Christ of Latter-day Saints in the early 1840s Despite Smith's revelation, the 1835 edition of the 101st Section of the Doctrine and Covenants, written after the doctrine of plural marriage began to be practiced, publicly condemned polygamy. This scripture was used by John Taylor in 1850 to quash Mormon polygamy rumors in Liverpool, England. Polygamy was made illegal in the state of Illinois during the 1839–44 Nauvoo era when several top Mormon leaders, including Smith, Brigham Young and Heber C. Kimball took multiple wives. Mormon elders who publicly taught that all men were commanded to enter plural marriage were subject to harsh discipline. On 7 June 1844 the "Nauvoo Expositor" criticized Smith for plural marriage.

After Joseph Smith was killed by a mob on 27 June 1844, the main body of Latter Day Saints left Nauvoo and followed Brigham Young to Utah where the practice of plural marriage continued. In 1852, Brigham Young, the second president of the LDS Church, publicly acknowledged the practice of plural marriage through a sermon he gave. Additional sermons by top Mormon leaders on the virtues of polygamy followed. Controversy followed when polygamy became a social cause, writers began to publish works condemning polygamy. The key plank of the Republican Party's 1856 platform was "to prohibit in the territories those twin relics of barbarism, polygamy and slavery". In 1862, Congress issued the Morrill Anti-Bigamy Act which clarified that the practice of polygamy was illegal in all US territories. The LDS Church believed that their religiously based practice of plural marriage was protected by the United States Constitution, however, the unanimous 1878 Supreme Court decision "Reynolds v. United States" declared that polygamy was not protected by the Constitution, based on the longstanding legal principle that "laws are made for the government of actions, and while they cannot interfere with mere religious belief and opinions, they may with practices."

Increasingly harsh anti-polygamy legislation in the US led some Mormons to emigrate to Canada and Mexico. In 1890, LDS Church president Wilford Woodruff issued a public declaration (the Manifesto) announcing that the LDS Church had discontinued new plural marriages. Anti-Mormon sentiment waned, as did opposition to statehood for Utah. The Smoot Hearings in 1904, which documented that the LDS Church was still practicing polygamy spurred the LDS Church to issue a Second Manifesto again claiming that it had ceased performing new plural marriages. By 1910 the LDS Church excommunicated those who entered into, or performed, new plural marriages. Even so, many plural husbands and wives continued to cohabit until their deaths in the 1940s and 1950s.

Enforcement of the 1890 Manifesto caused various splinter groups to leave the LDS Church in order to continue the practice of plural marriage. Polygamy among these groups persists today in Utah and neighboring states as well as in the spin-off colonies. Polygamist churches of Mormon origin are often referred to as "Mormon fundamentalist" even though they are not a part of the LDS Church. Such fundamentalists often use a purported 1886 revelation to John Taylor as the basis for their authority to continue the practice of plural marriage. "The Salt Lake Tribune" stated in 2005 there were as many as 37,000 fundamentalists with less than half of them living in polygamous households.

On 13 December 2013, US Federal Judge Clark Waddoups ruled in "Brown v. Buhman" that the portions of Utah's anti-polygamy laws which prohibit multiple cohabitation were unconstitutional, but also allowed Utah to maintain its ban on multiple marriage licenses. Unlawful cohabitation, where prosecutors did not need to prove that a marriage ceremony had taken place (only that a couple had lived together), had been the primary tool used to prosecute polygamy in Utah since the 1882 Edmunds Act.

The Council of Friends (also known as the Woolley Group and the Priesthood Council) was one of the original expressions of Mormon fundamentalism, having its origins in the teachings of Lorin C. Woolley, a dairy farmer excommunicated from the LDS Church in 1924. Several Mormon fundamentalist groups claim lineage through the Council of Friends, including but not limited to, the Fundamentalist Church of Jesus Christ of Latter Day Saints (FLDS Church), the Apostolic United Brethren, the Centennial Park group, the Latter Day Church of Christ, and the Righteous Branch of the Church of Jesus Christ of Latter-day Saints.

The Community of Christ, known as the Reorganized Church of Jesus Christ of Latter Day Saints (RLDS Church) prior to 2001, has never sanctioned polygamy since its foundation in 1860. Joseph Smith III, the first Prophet-President of the RLDS Church following the Reorganized of the church, was an ardent opponent of the practice of plural marriage throughout his life. For most of his career, Smith denied that his father had been involved in the practice and insisted that it had originated with Brigham Young. Smith served many missions to the western United States where he met with and interviewed associates and women claiming to be widows of his father, who attempted to present him with evidence to the contrary. Smith typically responded to such accusations by saying that he was "not positive nor sure that was innocent", and that if, indeed, the elder Smith had been involved, it was still a false practice. However, many members of the Community of Christ, and some of the groups that were formerly associated with it are not convinced that Joseph Smith practiced plural marriage, and feel that the evidence that he did is flawed.

In Islamic marital jurisprudence, under reasonable and warranted conditions, a Muslim man may have more than one wife at the same time, up to a total of four. Muslim women are not permitted to have more than one husband at the same time under any circumstances.

Based on verse 30:21 of Quran the ideal relationship is the comfort that a couple find in each other's embrace:
The polygyny that is allowed in the Koran is for special situations. There are strict requirements to marrying more than one woman, as the man must treat them equally financially and in terms of support given to each wife, according to Islamic law. However, Islam advises monogamy for a man if he fears he can't deal justly with his wives. This is based on verse 4:3 of Quran which says:

Muslim women are not allowed to marry more than one husband at once. However, in the case of a divorce or their husbands' death they can remarry after the completion of Iddah, as divorce is legal in Islamic law. A non-Muslim woman who flees from her non-Muslim husband and accepts Islam has the option to remarry without divorce from her previous husband, as her marriage with non-Muslim husband is Islamically dissolved on her fleeing. A non-Muslim woman captured during war by Muslims, can also remarry, as her marriage with her non-Muslim husband is Islamically dissolved at capture by Muslim soldiers. This permission is given to such women in verse 4:24 of Quran. The verse also emphasizes on transparency, mutual agreement and financial compensation as prerequisites for matrimonial relationship as opposed to prostitution; it says:
Muhammad was monogamously married to Khadija, his first wife, for 25 years, until she died. After her death, he married multiple women, mostly widows, for social and political reasons. He had a total of nine wives, but not all at the same time, depending on the sources in his lifetime. The Qur'an does not give preference in marrying more than one wife. One reason cited for polygyny is that it allows a man to give financial protection to multiple women, who might otherwise not have any support (e.g. widows). However, the wife can set a condition, in then marriage contract, that the husband cannot marry another woman during their marriage. In such a case, the husband cannot marry another woman as long as he is married to his wife. According to traditional Islamic law, each of those wives keeps their property and assets separate; and are paid mahar and maintenance separately by their husband. Usually the wives have little to no contact with each other and lead separate, individual lives in their own houses, and sometimes in different cities, though they all share the same husband.

In most Muslim-majority countries, polygyny is legal with Kuwait being the only one where no restrictions are imposed on it. The practice is illegal in Muslim-majority Turkey, Tunisia, Albania, Kosovo and Central Asian countries.

Countries that allow polygyny typically also require a man to obtain permission from his previous wives before marrying another, and require the man to prove that he can financially support multiple wives. In Malaysia and Morocco, a man must justify taking an additional wife at a court hearing before he is allowed to do so. In Sudan, the government encouraged polygyny in 2001 to increase the population.

In 2000, the United Nations Human Rights Committee reported that polygamy violates the International Covenant on Civil and Political Rights (ICCPR), citing concerns that the lack of "equality of treatment with regard to the right to marry" meant that polygamy, restricted to polygyny in practice, violates the dignity of women and should be outlawed. Specifically, the reports to UN Committees have noted violations of the ICCPR due to these inequalities and reports to the General Assembly of the UN have recommended it be outlawed. Many Muslim states are not signatories of the International Covenant on Civil and Political Rights (ICCPR), including Saudi Arabia, United Arab Emirates, Qatar, Malaysia, Brunei, Oman, and South Sudan; therefore the UN treaty doesn't apply to these countries.

Bigamy is illegal in the United Kingdom. "De facto" polygamy (having multiple partners at the same time) is not a criminal offence, provided the person does not register more than one marriage at the same time. In the UK, adultery is not a criminal offense (it is only a ground for divorce). In a written answer to the House of Commons, "In Great Britain, polygamy is only recognized as valid in law in circumstances where the marriage ceremony has been performed in a country whose laws permit polygamy and the parties to the marriage were domiciled there at the time. In addition, immigration rules have generally prevented the formation of polygamous households in this country since 1988."

The 2010 Government in the UK decided that Universal Credit (UC), which replaces means-tested benefits and tax credits for working-age people and will not be completely introduced until 2021, will not recognize polygamous marriages. A House of Commons Briefing Paper states "Treating second and subsequent partners in polygamous relationships as separate claimants could in some situations mean that polygamous households receive more under Universal Credit than they do under the current rules for means-tested benefits and tax credits. This is because, as explained above, the amounts which may be paid in respect of additional spouses are lower than those which generally apply to single claimants." There is currently no official statistics data on cohabiting polygamous couples who have arranged marriage in religious ceremonies.

In October 2017 there was media attention in the UK concerning website over a dating website offering Muslim men an opportunity to seek second or third wives.The website had 100 000 users of which 25 000 were in the UK. Website founder Azad Chaiwala created the website when he was seeking a second wife for himself.

Polygamy is currently illegal in the United States. Federal legislation to outlaw the practice was endorsed as constitutional in 1878, despite the religious objections of The Church of Jesus Christ of Latter-day Saints (Mormons), by the Supreme Court, in "Reynolds v. United States".

On 13 December 2013, a federal judge, spurred by the American Civil Liberties Union and other groups, struck down the parts of Utah's bigamy law that criminalized cohabitation, while also acknowledging that the state may still enforce bans on having multiple marriage licenses.

Individualist feminism and advocates such as Wendy McElroy and journalist Jillian Keenan support the freedom for adults to voluntarily enter polygamous marriages.

Authors such as Alyssa Rower and Samantha Slark argue that there is a case for legalizing polygamy on the basis of regulation and monitoring of the practice, legally protecting the polygamous partners and allowing them to join mainstream society instead of forcing them to hide from it when any public situation arises.

In an October 2004 op-ed for "USA Today", George Washington University law professor Jonathan Turley argued that, as a simple matter of equal treatment under law, polygamy ought to be legal. Acknowledging that underage girls are sometimes coerced into polygamous marriages, Turley replied that "banning polygamy is no more a solution to child abuse than banning marriage would be a solution to spousal abuse".

Stanley Kurtz, a Conservative fellow at the Hudson Institute, rejects the decriminalization and legalization of polygamy. He stated:
In January 2015, Pastor Neil Patrick Carrick of Detroit, Michigan, brought a case ("Carrick v. Snyder") against the State of Michigan that the state's ban of polygamy violates the Free Exercise and Equal Protection Clause of the U.S. Constitution.

Canada has taken a strong stand against polygamy, and the Department of Justice of Canada has argued that polygyny is a violation of International Human Rights Law, as a form of gender discrimination. In Canada, the federal Criminal Code applies throughout the country. It extends the definition of polygamy to having any kind of conjugal union with more than one person at the same time. Also anyone who assists, celebrates, or is a part to a rite, ceremony, or contract that sanctions a polygamist relationship is guilty of polygamy. Polygamy is an offence punishable by up to five years in prison. In 2017, two Canadian religious leaders have been found guilty of practising polygamy by the Supreme Court of British Columbia. Both of them are former bishops of the Mormon denomination of the Fundamentalist Church of Jesus Christ of Latter-Day Saints (FLDS).

Polygamous marriages are not recognized in Russia. The Family Code of Russia states that a marriage can only be contracted between a man and a woman, neither of whom is married to someone else. Furthermore, Russia does not recognize polygamous marriages that had been contracted in other countries. However, "de facto" polygamy or multiple cohabitation in and of itself is not a crime. Due to the shortage of men in Russia's population, it is not uncommon for men to father children with multiple women, and sometimes that results in households that are openly "de facto" polygamous.





</doc>
<doc id="24478" url="https://en.wikipedia.org/wiki?curid=24478" title="Postscript">
Postscript

A postscript (P.S.) is an afterthought, thought of occurring after the letter has been written and signed. The term comes from the Latin "post scriptum", an expression meaning "written after" (which may be interpreted in the sense of "that which comes after the writing").
A postscript may be a sentence, a paragraph, or occasionally many paragraphs added, often hastily and incidentally, after the signature of a letter or (sometimes) the main body of an essay or book. In a book or essay, a more carefully composed addition (e.g., for a second edition) is called an afterword. The word "postscript" has, poetically, been used to refer to any sort of addendum to some main work, even if it is not attached to a main work, as in Søren Kierkegaard's book titled "Concluding Unscientific Postscript." 

Sometimes, when additional points are made after the first postscript, abbreviations such as PSS ("post-super-scriptum"), PPS ("postquam-post-scriptum" or "post-post-scriptum") and PPPS ("post-post-post-scriptum"), and so on, "ad infinitum" are used, though only PPS has somewhat common usage.



</doc>
<doc id="24479" url="https://en.wikipedia.org/wiki?curid=24479" title="Penectomy">
Penectomy

Penectomy is penis removal through surgery, generally for medical or personal reasons.

Cancer, for example, sometimes necessitates removal of part or all of the penis. The amount of penis removed depends on the severity of the cancer. Some men have only the tip of their penis removed. For others with more advanced cancer, the entire penis must be removed.

In rare instances, botched circumcisions have also resulted in full or partial penectomies, as with David Reimer.

Fournier gangrene can also be a reason for penectomy and/or orchiectomy.

Because of the rarity of cancers which require the partial or total removal of the penis, support from people who have had the penis removed can be difficult to find locally. Website support networks are available. For instance, the American Cancer Society's Cancer Survivors Network website provides information for finding support networks. Phalloplasty is also an option for surgical reconstruction of a penis.

Sexual support therapists and specialists are available nationally in the United States and can be accessed through the specialist cancer services. Many surgeons or hospitals will also provide this information post operatively. Local government health services departments may be able to provide advice, names, and contact numbers.

Genital surgical procedures for trans women undergoing genital reconstruction surgery do not usually involve the complete removal of the penis. Instead, part or all of the glans is usually kept and reshaped as a clitoris, while the skin of the penile shaft may also be inverted to form the vagina (some more recently developed procedures, such as that used by Dr. Suporn Watanyusakul use the scrotum to form the vaginal walls, and the skin of the penile shaft to form the labia majora). When procedures such as this are not possible, other procedures such as colovaginoplasty are used which may involve the removal of the penis. Some trans women have undergone penectomies, however this is much rarer.

Issues related to the removal of the penis appear in psychology, for example in the condition known as castration anxiety, which happens as a result of a man having anxiety as to whether he may at some point become castrated.

People who are third gender will sometimes want an emasculation by choosing to have their penis, testicles, or both removed.
Male members in the sect of skoptsy (Russian: скопцы, "castrated") were required to become castrated, either only the testicles ("lesser seal") or also the penis ("greater seal").



</doc>
<doc id="24481" url="https://en.wikipedia.org/wiki?curid=24481" title="Provirus">
Provirus

A provirus is a virus genome that is integrated into the DNA of a host cell. In the case of bacterial viruses (bacteriophages), proviruses are often referred to as prophages.

This state can be a stage of virus replication, or a state that persists over longer periods of time as either inactive viral infections or an endogenous viral element. In inactive viral infections the virus will not replicate itself but through replication of its host cell. This state can last over many host cell generations.

Endogenous retroviruses are always in the state of a provirus. When a (nonendogenous) retrovirus invades a cell, the RNA of the retrovirus is reverse-transcribed into DNA by reverse transcriptase, then inserted into the host genome by an integrase.

A provirus does not directly make new DNA copies of itself while integrated into a host genome in this way. Instead, it is passively replicated along with the host genome and passed on to the original cell's offspring; all descendants of the infected cell will also bear proviruses in their genomes. This is known as lysogenic viral reproduction. Integration can result in a latent infection or a productive infection. In a productive infection, the provirus is transcribed into messenger RNA which directly produces new virus, which in turn will infect other cells via the lytic cycle. A latent infection results when the provirus is transcriptionally silent rather than active.

A latent infection may become productive in response to changes in the host's environmental conditions or health; the provirus may be activated and begin transcription of its viral genome. This can result in the destruction of its host cell because the cell's protein synthesis machinery is hijacked to produce more viruses.

Proviruses may account for approximately 8% of the human genome in the form of inherited endogenous retroviruses.

A provirus not only refers to a retrovirus but is also used to describe other viruses that can integrate into the host chromosomes, another example being adeno-associated virus. 
Not only eukaryotic viruses integrate into the genomes of their hosts; many bacterial and archaeal viruses also employ this strategy of propagation. All families of bacterial viruses with circular (single-stranded or double-stranded) DNA genomes or replicating their genomes through a circular intermediate (e.g., tailed dsDNA viruses) have temperate members.



</doc>
<doc id="24484" url="https://en.wikipedia.org/wiki?curid=24484" title="Parade">
Parade

A parade (also called march or marchpast) is a procession of people, usually organized along a street, often in costume, and often accompanied by marching bands, floats or sometimes large balloons. Parades are held for a wide range of reasons, but are usually celebrations of some kind. In Britain the term parade is usually reserved for either military parades or other occasions where participants march in formation; for celebratory occasions the word procession is more usual. In the Canadian Forces the term also has several less formal connotations.

Protest demonstrations can also take the form of a parade, but such cases are usually referred to as a march instead.

The parade float got its name because the first floats were decorated barges that were towed along canals with ropes held by parade marchers on the shore. Floats were occasionally propelled from within by concealed oarsmen, but the practice was abandoned because of the high incidence of drowning when the lightweight and unstable frames capsized. Strikingly, among the first uses of grounded floats — towed by horses — was a ceremony in memory of recently drowned parade oarsmen. Today, parade floats are traditionally pulled by motor vehicles or are powered themselves.

A grand marshal is an honorary title given to an individual by the organizing committee of a periodic parade event. Similar to guests of honor at fan conventions, a grand marshal is selected on any basis relevant to the theme or nature of the parade, and often leads the parade from the front. Multiple grand marshals may often be designated for an iteration of the parade, and may or may not be in actual attendance due to circumstances (including death).

Some parades, such as pride parades, may select a "Community Grand Marshal" or other designations alongside a "Grand Marshal" to lead the front or other parts of the parade.

Since the advent of such technology, it became possible for aircraft and boats to parade. A flypast is an aerial parade of anything from one to dozens of aircraft, both in commercial context at airshows and also to mark, e.g., national days or significant anniversaries. They are particularly common in the United Kingdom, where they are often associated with Royal occasions. Similarly, for ships, there may be a sail-past of, e.g., tall ships (as was seen during Trafalgar 200) or other sailing vessels as during the celebrations of the 60th anniversary of World War II.


At the end of hostilities in Europe in 1944-45, "victory parades" were a common feature throughout the recently liberated territories. For example, on 3 September 1944, the personnel of the 2nd Canadian Infantry Division marched six abreast to the music of massed regimental pipe and drum bands through the streets of Dieppe, France to commemorate the liberation of the city from German occupation, as well as commemorate the loss of over 900 soldiers from that formation during the Dieppe Raid two years earlier. On the Moscow Victory Parade of 1945 held in Moscow, Soviet Union in June 1945, the Red Army commemorated Victory in Europe with a parade and the ceremonial destruction of captured Wehrmacht and Waffen-SS standards.





</doc>
<doc id="24485" url="https://en.wikipedia.org/wiki?curid=24485" title="Priority queue">
Priority queue

In computer science, a priority queue is an abstract data type which is like a regular queue or stack data structure, but where additionally each element has a "priority" associated with it. In a priority queue, an element with high priority is served before an element with low priority. If two elements have the same priority, they are served according to their order in the queue.

While priority queues are often implemented with heaps, they are conceptually distinct from heaps. A priority queue is an abstract concept like "a list" or "a map"; just as a list can be implemented with a linked list or an array, a priority queue can be implemented with a heap or a variety of other methods such as an unordered array.

A priority queue must at least support the following operations:


In addition, "peek" (in this context often called "find-max" or "find-min"), which returns the highest-priority element but does not modify the queue, is very frequently implemented, and nearly always executes in "O"(1) time. This operation and its "O"(1) performance is crucial to many applications of priority queues.

More advanced implementations may support more complicated operations, such as "pull_lowest_priority_element", inspecting the first few highest- or lowest-priority elements, clearing the queue, clearing subsets of the queue, performing a batch insert, merging two or more queues into one, incrementing priority of any element, etc.

One can imagine a priority queue as a modified queue, but when one would get the next element off the queue, the highest-priority element is retrieved first.

Stacks and queues may be modeled as particular kinds of priority queues. As a reminder, here is how stacks and queues behave:


In a stack, the priority of each inserted element is monotonically increasing; thus, the last element inserted is always the first retrieved. In a queue, the priority of each inserted element is monotonically decreasing; thus, the first element inserted is always the first retrieved.

There are a variety of simple, usually inefficient, ways to implement a priority queue. They provide an analogy to help one understand what a priority queue is. For instance, one can keep all the elements in an unsorted list. Whenever the highest-priority element is requested, search through all elements for the one with the highest priority. (In big "O" notation: "O"(1) insertion time, "O"("n") pull time due to search.)

To improve performance, priority queues typically use a heap as their backbone, giving "O"(log "n") performance for inserts and removals, and "O"("n") to build initially. Variants of the basic heap data structure such as pairing heaps or Fibonacci heaps can provide better bounds for some operations.

Alternatively, when a self-balancing binary search tree is used, insertion and removal also take "O"(log "n") time, although building trees from existing sequences of elements takes "O"("n" log "n") time; this is typical where one might already have access to these data structures, such as with third-party or standard libraries.

From a computational-complexity standpoint, priority queues are congruent to sorting algorithms. The section on the equivalence of priority queues and sorting algorithms, below, describes how efficient sorting algorithms can create efficient priority queues.

There are several specialized heap data structures that either supply additional operations or outperform heap-based implementations for specific types of keys, specifically integer keys.


For applications that do many "peek" operations for every "extract-min" operation, the time complexity for peek actions can be reduced to "O"(1) in all tree and heap implementations by caching the highest priority element after every insertion and removal. For insertion, this adds at most a constant cost, since the newly inserted element is compared only to the previously cached minimum element. For deletion, this at most adds an additional "peek" cost, which is typically cheaper than the deletion cost, so overall time complexity is not significantly impacted.

Monotone priority queues are specialized queues that are optimized for the case where no item is ever inserted that has a lower priority (in the case of min-heap) than any item previously extracted. This restriction is met by several practical applications of priority queues.

The semantics of priority queues naturally suggest a sorting method: insert all the elements to be sorted into a priority queue, and sequentially remove them; they will come out in sorted order. This is actually the procedure used by several sorting algorithms, once the layer of abstraction provided by the priority queue is removed. This sorting method is equivalent to the following sorting algorithms:

A sorting algorithm can also be used to implement a priority queue. Specifically, Thorup says:
We present a general deterministic linear space reduction from priority queues to sorting implying that if we can sort up to "n" keys in "S"("n") time per key, then there is a priority queue supporting "delete" and "insert" in "O"("S"("n")) time and "find-min" in constant time.
That is, if there is a sorting algorithm which can sort in "O"("S") time per key, where "S" is some function of "n" and word size, then one can use the given procedure to create a priority queue where pulling the highest-priority element is "O"(1) time, and inserting new elements (and deleting elements) is "O"("S") time. For example, if one has an "O"("n" log log "n") sort algorithm, one can create a priority queue with "O"(1) pulling and "O"(log log "n") insertion.

A priority queue is often considered to be a "container data structure".

The Standard Template Library (STL), and the C++ 1998 standard, specifies codice_1 as one of the STL container adaptor class templates. However, it does not specify how two elements with same priority should be served, and indeed, common implementations will not return them according to their order in the queue. It implements a max-priority-queue, and has three parameters: a comparison object for sorting such as a function object (defaults to less<T> if unspecified), the underlying container for storing the data structures (defaults to std::vector<T>), and two iterators to the beginning and end of a sequence. Unlike actual STL containers, it does not allow iteration of its elements (it strictly adheres to its abstract data type definition). STL also has utility functions for manipulating another random-access container as a binary max-heap. The Boost libraries also have an implementation in the library heap.

Python's heapq module implements a binary min-heap on top of a list.

Java's library contains a class, which implements a min-priority-queue.

Go's library contains a container/heap module, which implements a min-heap on top of any compatible data structure.

The Standard PHP Library extension contains the class SplPriorityQueue.

Apple's Core Foundation framework contains a CFBinaryHeap structure, which implements a min-heap.

Priority queuing can be used to manage limited resources such as bandwidth on a transmission line from a network router. In the event of outgoing traffic queuing due to insufficient bandwidth, all other queues can be halted to send the traffic from the highest priority queue upon arrival. This ensures that the prioritized traffic (such as real-time traffic, e.g. an RTP stream of a VoIP connection) is forwarded with the least delay and the least likelihood of being rejected due to a queue reaching its maximum capacity. All other traffic can be handled when the highest priority queue is empty. Another approach used is to send disproportionately more traffic from higher priority queues.

Many modern protocols for local area networks also include the concept of priority queues at the media access control (MAC) sub-layer to ensure that high-priority applications (such as VoIP or IPTV) experience lower latency than other applications which can be served with best effort service. Examples include IEEE 802.11e (an amendment to IEEE 802.11 which provides quality of service) and ITU-T G.hn (a standard for high-speed local area network using existing home wiring (power lines, phone lines and coaxial cables).

Usually a limitation (policer) is set to limit the bandwidth that traffic from the highest priority queue can take, in order to prevent high priority packets from choking off all other traffic. This limit is usually never reached due to high level control instances such as the Cisco Callmanager, which can be programmed to inhibit calls which would exceed the programmed bandwidth limit.

Another use of a priority queue is to manage the events in a discrete event simulation. The events are added to the queue with their simulation time used as the priority. The execution of the simulation proceeds by repeatedly pulling the top of the queue and executing the event thereon.

"See also": Scheduling (computing), queueing theory

When the graph is stored in the form of adjacency list or matrix, priority queue can be used to extract minimum efficiently when implementing Dijkstra's algorithm, although one also needs the ability to alter the priority of a particular vertex in the priority queue efficiently.

Huffman coding requires one to repeatedly obtain the two lowest-frequency trees. A priority queue is one method of doing this.

Best-first search algorithms, like the A* search algorithm, find the shortest path between two vertices or nodes of a weighted graph, trying out the most promising routes first. A priority queue (also known as the "fringe") is used to keep track of unexplored routes; the one for which the estimate (a lower bound in the case of A*) of the total path length is smallest is given highest priority. If memory limitations make best-first search impractical, variants like the SMA* algorithm can be used instead, with a double-ended priority queue to allow removal of low-priority items.

The Real-time Optimally Adapting Meshes (ROAM) algorithm computes a dynamically changing triangulation of a terrain. It works by splitting triangles where more detail is needed and merging them where less detail is needed. The algorithm assigns each triangle in the terrain a priority, usually related to the error decrease if that triangle would be split. The algorithm uses two priority queues, one for triangles that can be split and another for triangles that can be merged. In each step the triangle from the split queue with the highest priority is split, or the triangle from the merge queue with the lowest priority is merged with its neighbours.

Using min heap priority queue in Prim's algorithm to find the minimum spanning tree of a connected and undirected graph, one can achieve a good running time. This min heap priority queue uses the min heap data structure which supports operations such as "insert", "minimum", "extract-min", "decrease-key". In this implementation, the weight of the edges is used to decide the priority of the vertices. Lower the weight, higher the priority and higher the weight, lower the priority.




</doc>
<doc id="24487" url="https://en.wikipedia.org/wiki?curid=24487" title="Pāramitā">
Pāramitā

Pāramitā (Sanskrit, Pali) or pāramī (Pāli) is "perfection" or "completeness". While, technically, "pāramī" and "pāramitā" are both Pāli terms, Pali literature makes far greater reference to "pāramī".

Donald S. Lopez, Jr. describes the etymology of the term:
Theravada teachings on the "pāramīs" can be found in late canonical books and post-canonical commentaries.

In the Pāli Canon, the "Buddhavaṃsa" lists the ten perfections ("dasa pāramiyo") as:


Two of the above virtues, mettā and "upekkhā", also are brahmavihāras.

The Theravādin teachings on the pāramīs can be found in canonical books ("Jataka tales", "Apadāna", "Buddhavaṃsa", "Cariyāpiṭaka") and post-canonical commentaries written to supplement the Pāli Canon at a later time, and thus might not be an original part of the Theravādin teachings. The oldest parts of the "Sutta Piṭaka" (for example, "Majjhima Nikāya", "Digha Nikāya", "Saṃyutta Nikāya" and the "Aṅguttara Nikāya") do not have any mention of the pāramīs as a category (though they are all mentioned individually).

Some scholars even refer to the teachings of the pāramīs as a semi-Mahāyāna teaching added to the scriptures at a later time in order to appeal to the interests and needs of the lay community and to popularize their religion. However, these views rely on the early scholarly presumption of Mahāyāna originating with religious devotion and appeal to laity. More recently, scholars have started to open up early Mahāyāna literature, which is very ascetic and expounds the ideal of the monk's life in the forest. Therefore, the practice of the pāramitās in Buddhism is close to the ideals of the ascetic tradition of the śramaṇa.

Bodhi (2005) maintains that, in the earliest Buddhist texts (which he identifies as the first four "nikāyas"), those seeking the extinction of suffering ("nibbana") pursued the noble eightfold path. As time went on, a backstory was provided for the multi-life development of the Buddha; as a result, the ten perfections were identified as part of the path for the bodhisattva (Pāli: "bodhisatta"). Over subsequent centuries, the "pāramīs" were seen as being significant for aspirants to both Buddhahood and arahantship. Thus, Bodhi (2005) summarizes:

In Mahāyāna Buddhism, the Prajñapāramitā sūtras, the "Lotus Sutra" and a large number of other texts list the six perfections:


Note that this list is also mentioned by the Theravāda commentator Dhammapala, who says it is equivalent to the above list of ten.

In the "Ten Stages Sutra", four more pāramitās are listed:

According to the perspective of Tibetan Buddhism, Mahāyāna practitioners have the choice of two practice paths: the path of perfection (Sanskrit: "pāramitāyāna") or the path of tantra (Sanskrit: "tantrayāna"), which is the Vajrayāna.

Traleg Kyabgon Rinpoche renders "pāramitā" into English as "transcendent action" and then frames and qualifies it:
The pure illusory body is said to be endowed with the six perfections (Sanskrit: "ṣatpāramitā").

The first four perfections are skillful means practice while the last two are wisdom practice. These contain all the methods and skills required for eliminating delusion and fulfilling other's needs. Also, leading from happy to happier states.





</doc>
<doc id="24489" url="https://en.wikipedia.org/wiki?curid=24489" title="Outline of physics">
Outline of physics

The following outline is provided as an overview of and topical guide to physics:

Physics – natural science that involves the study of matter and its motion through spacetime, along with related concepts such as energy and force. More broadly, it is the general analysis of nature, conducted in order to understand how the universe behaves.

Physics can be described as all of the following:


History of physics – history of the physical science that studies matter and its motion through space-time, and related concepts such as energy and force

Physics – branch of science that studies matter and its motion through space and time, along with related concepts such as energy and force. Physics is one of the "fundamental sciences" because the other natural sciences (like biology, geology etc.) deal with systems that seem to obey the laws of physics. According to physics, the physical laws of matter, energy and the fundamental forces of nature govern the interactions between particles and physical entities (such as planets, molecules, atoms or the subatomic particles). Some of the basic pursuits of physics, which include some of the most prominent developments in modern science in the last millennium, include:

Gravity, light, physical system, physical observation, physical quantity, physical state, physical unit, physical theory, physical experiment

Theoretical concepts
Mass–energy equivalence, particle, physical field, physical interaction, physical law, fundamental force, physical constant, wave

Physics
This is a list of the primary theories in physics, major subtopics, and concepts.

Index of physics articles





</doc>
<doc id="24490" url="https://en.wikipedia.org/wiki?curid=24490" title="Outline of parapsychology">
Outline of parapsychology

Parapsychology is a field of research that studies a number of ostensible paranormal phenomena, including telepathy, precognition, clairvoyance, psychokinesis, near-death experiences, reincarnation, and apparitional experiences.









</doc>
<doc id="24493" url="https://en.wikipedia.org/wiki?curid=24493" title="Outline of public affairs">
Outline of public affairs

The following outline is provided as an overview of and topical guide to public affairs:

Public affairs – catch-all term that includes public policy as well as public administration, both of which are closely related to and draw upon the fields of political science and economics.








</doc>
<doc id="24494" url="https://en.wikipedia.org/wiki?curid=24494" title="Index of painting-related articles">
Index of painting-related articles

Below is a list of topics in painting.
























</doc>
<doc id="24497" url="https://en.wikipedia.org/wiki?curid=24497" title="Prakrit">
Prakrit

The Prakrits (; ; ; ) are any of several Middle Indo-Aryan languages formerly spoken in India. Texts written in these languages date from the 3rd century BC to the 8th century AD or later.

The Ardhamagadhi (or simply Magadhi) Prakrit, which was used extensively to write the scriptures of Jainism, is often considered to be the definitive form of Prakrit, while others are considered variants of it. Prakrit grammarians would give the full grammar of Ardhamagadhi first, and then define the other grammars with relation to it. For this reason, courses teaching 'Prakrit' are often regarded as teaching Ardhamagadhi. Pali, the Prakrit used in Theravada Buddhism, tends to be treated as a special exception from the variants of the Ardhamagadhi language, as Classical Sanskrit grammars do not consider it as a Prakrit "per se", presumably for sectarian rather than linguistic reasons. Other Prakrits, such as Paiśācī, are reported in old historical sources but are not attested.

Some modern scholars follow this classification by including all Middle Indo-Aryan languages under the rubric of 'Prakrits', while others emphasise the independent development of these languages, often separated from the history of Sanskrit by wide divisions of caste, religion, and geography. While Prakrits were originally seen as 'lower' forms of language, the influence they had on Sanskrit - allowing it to be more easily used by the common people - as well as the converse influence of Sanskrit on the Prakrits, gave Prakrits progressively higher cultural cachet.

The word 'Prakrit' itself has a flexible definition, being defined sometimes as 'original, natural, artless, normal, ordinary, usual' or 'vernacular,' in contrast to the literary and religious orthodoxy of Sanskrit. Alternatively, Prakrit can be taken to mean 'derived from an original,' which means evolved in a natural way. Prakrit is foremost a native term, designating 'vernaculars' as opposed to Sanskrit.

The Prakrits became literary languages, generally patronised by ancient kings of the Hindu kshatriya class, though were still regarded as illegitimate by the orthodoxy. The earliest extant use of Prakrit is in the Edicts of Ashoka on Buddhism (r. 268–232 BCE). It appears as a very significant liturgical language in the form of the Pāli Canon of Theravada Buddhists and the Agamas of the Jains, beside countless later grammars, lyrics, plays and epics.

Each variety of Prakrit gradually became associated with a particular patron dynasty, and indeed particular religions, as well as with numerous regional literatures. Every one is today considered to constitute a distinct literary tradition within the history of the subcontinent.

According to the dictionary of Monier Monier-Williams (1819–1899), the most frequent meanings of the term ', from which the word "prakrit" is derived, are "original, natural, normal" and the term is derived from ', "making or placing before or at first, the original or natural form or condition of anything, original or primary substance". In linguistic terms, this is used in contrast with "", "refined".

Dramatic Prakrits were those that were devised specifically for use in dramas and other literature. Whenever dialogue was written in a Prakrit, the reader would also be provided with a Sanskrit translation. None of these Prakrits came into being as vernaculars, but some ended up being used as such when Sanskrit fell out of favor.

The phrase "Dramatic Prakrits" often refers to three most prominent of them: Shauraseni, Magadhi Prakrit, and Maharashtri Prakrit. However, there were a slew of other less commonly used Prakrits that also fall into this category. These include Pracya, Bahliki, Daksinatya, Sakari, Candali, Sabari, Abhiri, Dramili, and Odri. There was a strict structure to the use of these different Prakrits in dramas. Characters each spoke a different Prakrit based on their role and background; for example, Dramili was the language of "forest-dwellers", Sauraseni was spoken by "the heroine and her female friends", and Avanti was spoken by "cheats and rogues".

Maharashtri Prakrit, the ancestor of modern Marathi, is a particularly interesting case. Maharashtri was often used for poetry and as such, diverged from proper Sanskrit grammar mainly to fit the language to the meter of different styles of poetry. The new grammar stuck, which led to the unique flexibility of vowels lengths – amongst other anomalies – in Marathi.

Unusual Prakrits appear in the margins of the Prakritic world: Elu in Sri Lanka and Gāndhārī Prakrit in Gandhara and Central Asia both have unusual phonological and grammatical changes not found in other Prakrits.


In 1955, government of Bihar established at Vaishali, the Research Institute of Prakrit Jainology and Ahimsa with the aim to promote research work in Prakrit.




</doc>
<doc id="24498" url="https://en.wikipedia.org/wiki?curid=24498" title="Palestrina (disambiguation)">
Palestrina (disambiguation)

Palestrina may refer to:


</doc>
<doc id="24501" url="https://en.wikipedia.org/wiki?curid=24501" title="Progressive music">
Progressive music

Progressive music is music that subverts genre and results in the expansion of stylistic boundaries. The word comes from the basic concept of "progress", which refers to development and growth by accumulation, and is often deployed for numerous music genres such as progressive country, progressive folk, progressive jazz, and (most significantly) progressive rock.

Music that is deemed "progressive" usually synthesizes influences from various cultural domains, such as European art music, Celtic folk, West Indian, or African. As an art theory, the progressive approach falls between formalism and eclecticism. "Formalism" refers to a preoccupation with established external compositional systems, structural unity, and the autonomy of individual art works. Like formalism, "eclecticism" connotates a predilection toward style synthesis or integration. However, contrary to formalist tendencies, eclecticism foregrounds discontinuities between historical and contemporary styles and electronic media, sometimes referring simultaneously to vastly different musical genres, idioms, and cultural codes. In marketing, "progressive" is used to distinguish a product from "commercial" pop music. Progressive music is rooted in the idea of a cultural alternative and may also be associated with auteur-stars and concept albums, considered traditional structures of the music industry.

Jazz began to take itself seriously as swing gave way to bebop in the 1940s, but could not maintain its listening audience. Following the economic boom of the mid 1960s, record labels began investing in artists whose ambitions paralleled these earlier attempts in jazz, offering performers limited control over their own content and marketing. This resulted in a brief period in which creative authenticity among musical artists and consumer marketing coincided with each other, a situation that fell into abeyance between the late 1970s and the birth of Internet stars. Beginning in 1967, pop music would be divided by a "progressive pop" and "mass/chart pop" demographic that subsequently gave rise to the "progressive rock" movement, which crystallized the progressive music of earlier artists into a recognizable genre.

Progressive jazz is a catch-all term for more complex forms of jazz that superseded the swing era, ranging from the cerebral explorations of Lennie Tristano and Dave Brubeck to the dissonant arrangements of big bands such as those led by Boyd Raeburn and Stan Kenton. The term was popularized by the bandleader Stan Kenton during the late 1940s. The defining feature is music that is complex or experimental. It originated in the 1940s, drawing inspiration from modernist composers such as Igor Stravinsky and Paul Hindemith. Proponents of the label have made comparisons between the music and modern art, for example the title of the composition "Dalvatore Sally" by Boyd Raeburn. Critics were initially wary of the idiom. The term has been used to include bebop, although that application was assertively dismissed by Dizzy Gillespie. Gillespie wrote in his autobiography; "They tried to make Stan Kenton a 'white hope,' called modern jazz and my music 'progressive,' then tried to tell me I played 'progressive' music. I said, 'You're full of shit!' 'Stan Kenton? There ain't nothing in my music that's cold, cold like his."

Progressive big band is a style of big band music that was made for listening rather than dancing, with denser, more modernistic arrangements. The online music guide AllMusic states that, along with Kenton, musicians like Gil Evans, Toshiko Akiyoshi, Cal Massey, Frank Foster, Carla Bley, George Gruntz, David Amram, Sun Ra, and Duke Ellington were major proponents of the style.

"Progressive rock" is almost synonymous with "art rock"; the latter is more likely to have experimental or avant-garde influences. Although a unidirectional English "progressive" style emerged in the late 1960s, by 1967, progressive rock had come to constitute a diversity of loosely associated style codes. With the arrival of a "progressive" label, the music was dubbed "progressive pop" before it was called "progressive rock". "Progressive" referred to the wide range of attempts to break with the standard pop music formula. A number of additional factors contributed to the label—lyrics were more poetic, technology was harnessed for new sounds, music approached the condition of "art", some harmonic language was imported from jazz and 19th-century classical music, the album format overtook singles, and the studio, rather than the stage, became the focus of musical activity, which often involved creating music for listening, not dancing.

During the mid 1960s, pop music made repeated forays into new sounds, styles, and techniques that inspired public discourse among its listeners. The word "progressive" was frequently used, and it was thought that every song and single was to be a "progression" from the last. In 1966, the degree of social and artistic dialogue among rock musicians dramatically increased for bands such as the Beach Boys, the Beatles, and the Byrds who fused elements of composed (cultivated) music with the oral (vernacular) musical traditions of rock. Rock music started to take itself seriously, paralleling earlier attempts in jazz (as swing gave way to bop, a move which did not succeed with audiences). In this period, the popular song began signaling a new possible means of expression that went beyond the three-minute love song, leading to an intersection between the "underground" and the "establishment" for listening publics.

The music was developed immediately following a brief period in the mid 1960s where creative authenticity among musical artists and consumer marketing coincided with each other. Before the progressive pop of the late 1960s, performers were typically unable to decide on the artistic content of their music. Assisted by the mid 1960s economic boom, record labels began investing in artists, giving them freedom to experiment, and offering them limited control over their content and marketing. The growing student market serviced record labels with the word "progressive", being adopted as a marketing term to differentiate their product from "commercial" pop. Music critic Simon Reynolds writes that beginning with 1967, a divide would exist between "progressive" pop and "mass/chart" pop, a separation which was "also, broadly, one between boys and girls, middle-class and working-class." Before progressive/art rock became the most commercially successful British sound of the early 1970s, the 1960s psychedelic movement brought together art and commercialism, broaching the question of what it meant to be an artist in a mass medium. Progressive musicians thought that artistic status depended on personal autonomy, and so the strategy of "progressive" rock groups was to present themselves as performers and composers "above" normal pop practice.

Proto-prog (short for proto-progressive) is the first wave of British progressive rock musicians who branched from psychedelia or the advanced music that slightly predates the full-fledged prog era. The musicians that approached this genre harnessed modern classical and other genres usually outside of traditional rock influences, longer and more complicated compositions, interconnected songs as medley, and studio composition. Progressive rock itself evolved from psychedelic/acid rock music, specifically a strain of classical/symphonic rock led by the Nice, Procol Harum, and the Moody Blues. Critics assumed King Crimson's album "In the Court of the Crimson King" (1969) to be the logical extension and development of late 1960s proto-progressive rock exemplified by the Moody Blues, Procol Harum, Pink Floyd, and the Beatles. According to Macan, the album may be the most influential to progressive rock for crystallizing the music of earlier "proto-progressive bands [...] into a distinctive, immediately recognizable style". He distinguishes 1970s "classic" prog from late 1960s proto-prog by the conscious rejection of psychedelic rock elements, which proto-progressive bands continued to incorporate.

"Post-progressive" is a term invented to distinguish a type of rock music from the persistent "progressive rock" style associated with the 1970s. In the mid to late 1970s, progressive music was denigrated for its assumed pretentiousness, specifically the likes of Yes, Genesis, and Emerson, Lake & Palmer. According to musicologist John Covach, "by the early 1980s, progressive rock was thought to be all but dead as a style, an idea reinforced by the fact that some of the principal progressive groups has developed a more commercial sound. [...] What went out of the music of these now ex-progressive groups [...] was any significant evocation of art music." In the opinion of King Crimson's Robert Fripp, "progressive" music was an attitude, not a style. He believed that genuinely "progressive" music pushes stylistic and conceptual boundaries outwards through the appropriation of procedures from classical music or jazz, and that once "progressive rock" ceased to cover new ground – becoming a set of conventions to be repeated and imitated – the genre's premise had ceased to be "progressive". 

A direct reaction to prog came in the form of the punk movement, which rejected classical traditions, virtuosity, and textural complexity. Post-punk, which author Doyle Green characterizes "as a kind of progressive punk", was played by bands like Talking Heads, Pere Ubu, Public Image Ltd, and Joy Division. It differs from punk rock by balancing punk's energy and skepticism with a re-engagement with an art school consciousness, Dadaist experimentalism, and atmospheric, ambient soundscapes. It was also majorly influenced from world music, especially African and Asian traditions. In the same period, new wave music was more sophisticated in production terms than some contemporaneous progressive music, but was largely perceived as simplistic, and thus had little overt appeal to art music or art-music practice. Musicologist Bill Martin writes; "the [Talking] Heads created a kind of new-wave music that was the perfect synthesis of punk urgency and attitude and progressive-rock sophistication and creativity. A good deal of the more interesting rock since that time is clearly 'post-Talking Heads' music, but this means that it is post-progressive rock as well."

AllMusic defines "progressive electronic" as a subgenre of new age music which "thrives in more unfamiliar territory. The styles that emerge are often dictated by the technology itself. Rather than sampling or synthesizing acoustic sounds to electronically replicate them, these composers tend to mutate the original timbres, sometimes to an unrecognizable state. True artists in the genre also create their own sounds." Reynolds posits that "the truly progressive edge in electronic music involves doing things that can't be physically achieved by human beings manipulating instruments in real-time."

In house music, a desire to define precise stylistic strands and taste markets saw the interposition of prefixes like "progressive", "tribal", and "intelligent". According to disc jockey and producer Carl Craig, the term "progressive" was used in Detroit in the early 1980s in reference to Italian disco. The music was dubbed "progressive" because it drew upon the influence of Giorgio Moroder's Euro disco rather than the disco inspired by the symphonic Philadelphia sound. In this context, Reynolds criticizes terms like "progressive" and "intelligent", arguing that "when an underground scene starts talking this talk, it's usually a sign that it's gearing up the media game as a prequel to buying into traditional music industry structure of auteur-stars, concept albums, and long-term careers. Above all, it's a sign of impending musical debility, creeping self-importance, and the hemorrhaging away of fun."

In the mid 1990s, progressive electronica artists were spearheaded by the Lowercase movement, a reductive approach towards new digital technologies. By 1993, progressive house and trance music had emerged in dance clubs. "Progressive house" was an English style of house distinguished by long tracks, big riffs, mild dub inflections, and multitiered percussion. According to Reynolds, the "'progressive' seemed to signify not just its anti-cheese, nongirly credentials, but its severing of house's roots from gay black disco." Reynolds also identifies links between progressive rock and other electronic music genres, and that "many post-rave genres bear an uncanny resemblance to progressive rock: conceptualism, auteur-geniuses, producers making music to impress other producers, [and] showboating virtuosity reborn as the 'science' of programming finesse."

Citations
Sources


</doc>
<doc id="24503" url="https://en.wikipedia.org/wiki?curid=24503" title="Pyotr Ilyich Tchaikovsky">
Pyotr Ilyich Tchaikovsky

Pyotr Ilyich Tchaikovsky ( ; , ; 25 April/7 May 1840 – 25 October/6 November 1893), often anglicized as Peter Ilich Tchaikovsky, was a Russian composer of the romantic period, some of whose works are among the most popular music in the classical repertoire. He was the first Russian composer whose music made a lasting impression internationally, bolstered by his appearances as a guest conductor in Europe and the United States. He was honored in 1884 by Emperor Alexander III, and awarded a lifetime pension.

Although musically precocious, Tchaikovsky was educated for a career as a civil servant. There was scant opportunity for a musical career in Russia at that time and no system of public music education. When an opportunity for such an education arose, he entered the nascent Saint Petersburg Conservatory, from which he graduated in 1865. The formal Western-oriented teaching he received there set him apart from composers of the contemporary nationalist movement embodied by the Russian composers of The Five, with whom his professional relationship was mixed. Tchaikovsky's training set him on a path to reconcile what he had learned with the native musical practices to which he had been exposed from childhood. From this reconciliation he forged a personal but unmistakably Russian style—a task that did not prove easy. The principles that governed melody, harmony and other fundamentals of Russian music ran completely counter to those that governed Western European music; this seemed to defeat the potential for using Russian music in large-scale Western composition or for forming a composite style, and it caused personal antipathies that dented Tchaikovsky's self-confidence. Russian culture exhibited a split personality, with its native and adopted elements having drifted apart increasingly since the time of Peter the Great. This resulted in uncertainty among the intelligentsia about the country's national identity—an ambiguity mirrored in Tchaikovsky's career.

Despite his many popular successes, Tchaikovsky's life was punctuated by personal crises and depression. Contributory factors included his early separation from his mother for boarding school followed by his mother's early death, the death of his close friend and colleague Nikolai Rubinstein, and the collapse of the one enduring relationship of his adult life, which was his 13-year association with the wealthy widow Nadezhda von Meck who was his patron even though they never actually met each other. His homosexuality, which he kept private, has traditionally also been considered a major factor, though some musicologists now downplay its importance. Tchaikovsky's sudden death at the age of 53 is generally ascribed to cholera; there is an ongoing debate as to whether cholera was indeed the cause of death, and whether his death was accidental or self-inflicted.

While his music has remained popular among audiences, critical opinions were initially mixed. Some Russians did not feel it was sufficiently representative of native musical values and expressed suspicion that Europeans accepted the music for its Western elements. In an apparent reinforcement of the latter claim, some Europeans lauded Tchaikovsky for offering music more substantive than base exoticism and said he transcended stereotypes of Russian classical music. Others dismissed Tchaikovsky's music as "lacking in elevated thought," according to longtime "New York Times" music critic Harold C. Schonberg, and derided its formal workings as deficient because they did not stringently follow Western principles.

Pyotr Ilyich Tchaikovsky was born in Votkinsk, a small town in Vyatka Governorate (present-day Udmurtia) in the Russian Empire, into a family with a long line of military service. His father, Ilya Petrovich Tchaikovsky, had served as a lieutenant colonel and engineer in the Department of Mines, and would manage the Kamsko-Votkinsk Ironworks. His grandfather, Pyotr Fedorovich Tchaikovsky (né Petro Fedorovych Chaika), served first as a physician's assistant in the army and later as city governor of Glazov in Vyatka. His great-grandfather, a Ukrainian Cossack named Fyodor Chaika (accordingly, 'Tchaikovsky' is the derivative of the Ukrainian family name 'Chaika' - 'seagull' in Ukrainian), distinguished himself under Peter the Great at the Battle of Poltava in 1709. Tchaikovsky's mother, Alexandra Andreyevna ( d'Assier), was the second of Ilya's three wives, 18 years her husband's junior and French on her father's side. Both Ilya and Alexandra were trained in the arts, including music—a necessity as a posting to a remote area of Russia also meant a need for entertainment, whether in private or at social gatherings. Of his six siblings, Tchaikovsky was close to his sister Alexandra and twin brothers Anatoly and Modest. Alexandra's marriage to Lev Davydov would produce seven children and lend Tchaikovsky the only real family life he would know as an adult, especially during his years of wandering. One of those children, Vladimir Davydov, whom the composer would nickname 'Bob', would become very close to him.

In 1844, the family hired Fanny Dürbach, a 22-year-old French governess. Four-and-a-half-year-old Tchaikovsky was initially thought too young to study alongside his older brother Nikolai and a niece of the family. His insistence convinced Dürbach otherwise. By the age of six, he had become fluent in French and German. Tchaikovsky also became attached to the young woman; her affection for him was reportedly a counter to his mother's coldness and emotional distance from him, though others assert that the mother doted on her son. Dürbach saved much of Tchaikovsky's work from this period, including his earliest known compositions, and became a source of several childhood anecdotes.

Tchaikovsky began piano lessons at age five. Precocious, within three years he had become as adept at reading sheet music as his teacher. His parents, initially supportive, hired a tutor, bought an orchestrion (a form of barrel organ that could imitate elaborate orchestral effects), and encouraged his piano study for both aesthetic and practical reasons. However, they decided in 1850 to send Tchaikovsky to the Imperial School of Jurisprudence in Saint Petersburg. They had both graduated from institutes in Saint Petersburg and the School of Jurisprudence, which mainly served the lesser nobility, and thought that this education would prepare Tchaikovsky for a career as a civil servant. Regardless of talent, the only musical careers available in Russia at that time—except for the affluent aristocracy—were as a teacher in an academy or as an instrumentalist in one of the Imperial Theaters. Both were considered on the lowest rank of the social ladder, with individuals in them enjoying no more rights than peasants. His father's income was also growing increasingly uncertain, so both parents may have wanted Tchaikovsky to become independent as soon as possible. As the minimum age for acceptance was 12 and Tchaikovsky was only 10 at the time, he was required to spend two years boarding at the Imperial School of Jurisprudence's preparatory school, from his family. Once those two years had passed, Tchaikovsky transferred to the Imperial School of Jurisprudence to begin a seven-year course of studies.

Tchaikovsky's early separation from his mother caused an emotional trauma that lasted the rest of his life and was intensified by her death from cholera in 1854, when he was fourteen. The loss of his mother also prompted Tchaikovsky to make his first serious attempt at composition, a waltz in her memory. Tchaikovsky's father, who had also contracted cholera but recovered fully, sent him back to school immediately in the hope that classwork would occupy the boy's mind. Isolated, Tchaikovsky compensated with friendships with fellow students that became lifelong; these included Aleksey Apukhtin and Vladimir Gerard. Music, while not an official priority at school, also bridged the gap between Tchaikovsky and his peers. They regularly attended the opera and Tchaikovsky would improvise at the school's harmonium on themes he and his friends had sung during choir practice. "We were amused," Vladimir Gerard later remembered, "but not imbued with any expectations of his future glory." Tchaikovsky also continued his piano studies through Franz Becker, an instrument manufacturer who made occasional visits to the school; however, the results, according to musicologist David Brown, were "negligible".

In 1855, Tchaikovsky's father funded private lessons with Rudolph Kündinger and questioned him about a musical career for his son. While impressed with the boy's talent, Kündinger said he saw nothing to suggest a future composer or performer. He later admitted that his assessment was also based on his own negative experiences as a musician in Russia and his unwillingness for Tchaikovsky to be treated likewise. Tchaikovsky was told to finish his course and then try for a post in the Ministry of Justice.

On 10 June 1859, the 19-year-old Tchaikovsky graduated as a titular counselor, a low rung on the civil service ladder. Appointed to the Ministry of Justice, he became a junior assistant within six months and a senior assistant two months after that. He remained a senior assistant for the rest of his three-year civil service career.

Meanwhile, the Russian Musical Society (RMS) was founded in 1859 by the Grand Duchess Elena Pavlovna (a German-born aunt of Tsar Alexander II) and her protégé, pianist and composer Anton Rubinstein. Previous tsars and the aristocracy had focused almost exclusively on importing European talent. The aim of the RMS was to fulfill Alexander II's wish to foster native talent. It hosted a regular season of public concerts (previously held only during the six weeks of Lent when the Imperial Theaters were closed) and provided basic professional training in music. In 1861, Tchaikovsky attended RMS classes in music theory taught by Nikolai Zaremba at the Mikhailovsky Palace (now the Russian Museum). These classes were a precursor to the Saint Petersburg Conservatory, which opened in 1862. Tchaikovsky enrolled at the Conservatory as part of its premiere class. He studied harmony and counterpoint with Zaremba and instrumentation and composition with Rubinstein.

The Conservatory benefited Tchaikovsky in two ways. It transformed him into a musical professional, with tools to help him thrive as a composer, and the in-depth exposure to European principles and musical forms gave him a sense that his art was not exclusively Russian or Western. This mindset became important in Tchaikovsky's reconciliation of Russian and European influences in his compositional style. He believed and attempted to show that both these aspects were "intertwined and mutually dependent". His efforts became both an inspiration and a starting point for other Russian composers to build their own individual styles.
Rubinstein was impressed by Tchaikovsky's musical talent on the whole and cited him as "a composer of genius" in his autobiography. He was less pleased with the more progressive tendencies of some of Tchaikovsky's student work. Nor did he change his opinion as Tchaikovsky's reputation grew. He and Zaremba clashed with Tchaikovsky when he submitted his First Symphony for performance by the RMS in Saint Petersburg. Rubinstein and Zaremba refused to consider the work unless substantial changes were made. Tchaikovsky complied but they still refused to perform the symphony. Tchaikovsky, distressed that he had been treated as though he were still their student, withdrew the symphony. It was given its first complete performance, minus the changes Rubinstein and Zaremba had requested, in Moscow in February 1868.

Once Tchaikovsky graduated in 1865, Rubinstein's brother Nikolai offered him the post of Professor of Music Theory at the soon-to-open Moscow Conservatory. While the salary for his professorship was only 50 rubles a month, the offer itself boosted Tchaikovsky's morale and he accepted the post eagerly. He was further heartened by news of the first public performance of one of his works, his "Characteristic Dances", conducted by Johann Strauss II at a concert in Pavlovsk Park on 11 September 1865 (Tchaikovsky later included this work, re-titled, "Dances of the Hay Maidens", in his opera "The Voyevoda").

From 1867 to 1878, Tchaikovsky combined his professorial duties with music criticism while continuing to compose. This activity exposed him to a range of contemporary music and afforded him the opportunity to travel abroad. In his reviews, he praised Beethoven, considered Brahms overrated and, despite his admiration, took Schumann to task for poor orchestration. He appreciated the staging of Wagner's "Der Ring des Nibelungen" at its inaugural performance in Bayreuth, Germany, but not the music, calling "Das Rheingold" "unlikely nonsense, through which, from time to time, sparkle unusually beautiful and astonishing details." A recurring theme he addressed was the poor state of Russian opera.

In 1856, while Tchaikovsky was still at the School of Jurisprudence and Anton Rubinstein lobbied aristocrats to form the RMS, critic Vladimir Stasov and an 18-year-old pianist, Mily Balakirev, met and agreed upon a nationalist agenda for Russian music, one that would take the operas of Mikhail Glinka as a model and incorporate elements from folk music, reject traditional Western practices and use exotic harmonic devices such as the whole tone and octatonic scales. They saw Western-style conservatories as unnecessary and antipathetic to fostering native talent. Eventually, Balakirev, César Cui, Modest Mussorgsky, Nikolai Rimsky-Korsakov and Alexander Borodin became known as the "moguchaya kuchka", translated into English as the Mighty Handful or The Five. Rubinstein criticized their emphasis on amateur efforts in musical composition; Balakirev and later Mussorgsky attacked Rubinstein for his musical conservatism and his belief in professional music training. Tchaikovsky and his fellow conservatory students were caught in the middle.

While ambivalent about much of The Five's music, Tchaikovsky remained on friendly terms with most of its members. In 1869, he and Balakirev worked together on what became Tchaikovsky's first recognized masterpiece, the fantasy-overture "Romeo and Juliet", a work which The Five wholeheartedly embraced. The group also welcomed his Second Symphony, subtitled the "Little Russian". Despite their support, Tchaikovsky made considerable efforts to ensure his musical independence from the group as well as from the conservative faction at the Saint Petersburg Conservatory.

The infrequency of Tchaikovsky's musical successes, won with tremendous effort, exacerbated his lifelong sensitivity to criticism. Nikolai Rubinstein's private fits of rage critiquing his music, most famously attacking the First Piano Concerto, did not help matters. His popularity grew, however, as several first-rate artists became willing to perform his compositions. Hans von Bülow premiered the First Piano Concerto and championed other Tchaikovsky works both as pianist and conductor. Other artists included Adele Aus der Ohe, Max Erdmannsdörfer, Eduard Nápravník and Sergei Taneyev.

Another factor that helped Tchaikovsky's music become popular was a shift in attitude among Russian audiences. Whereas they had previously been satisfied with flashy virtuoso performances of technically demanding but musically lightweight compositions, they gradually began listening with increasing appreciation of the music itself. Tchaikovsky's works were performed frequently, with few delays between their composition and first performances; the publication from 1867 onward of his songs and great piano music for the home market also helped boost the composer's popularity.

During the late 1860s, Tchaikovsky began to compose operas. His first, "The Voyevoda", based on a play by Alexander Ostrovsky, premiered in 1869. The composer became dissatisfied with it, however, and, having re-used parts of it in later works, destroyed the manuscript. "Undina" followed in 1870. Only excerpts were performed and it, too, was destroyed. Between these projects, Tchaikovsky started to compose an opera called "Mandragora", to a libretto by Sergei Rachinskii; the only music he completed was a short chorus of Flowers and Insects.

The first Tchaikovsky opera to survive intact, "The Oprichnik", premiered in 1874. During its composition, he lost Ostrovsky's part-finished libretto. Tchaikovsky, too embarrassed to ask for another copy, decided to write the libretto himself, modelling his dramatic technique on that of Eugène Scribe. Cui wrote a "characteristically savage press attack" on the opera. Mussorgsky, writing to Vladimir Stasov, disapproved of the opera as pandering to the public. Nevertheless, "The Oprichnik" continues to be performed from time to time in Russia.

The last of the early operas, "Vakula the Smith" (Op.14), was composed in the second half of 1874. The libretto, based on Gogol's "Christmas Eve", was to have been set to music by Alexander Serov. With Serov's death, the libretto was opened to a competition with a guarantee that the winning entry would be premiered by the Imperial Mariinsky Theatre. Tchaikovsky was declared the winner, but at the 1876 premiere, the opera enjoyed only a lukewarm reception. After Tchaikovsky's death, Rimsky-Korsakov wrote the opera "Christmas Eve", based on the same story.

Other works of this period include the "Variations on a Rococo Theme" for cello and orchestra, the Third and Fourth Symphonies, the ballet "Swan Lake", and the opera "Eugene Onegin".

Discussion of Tchaikovsky's personal life, especially his sexuality, has perhaps been the most extensive of any composer in the 19th century and certainly of any Russian composer of his time. It has also at times caused considerable confusion, from Soviet efforts to expunge all references to same-sex attraction and portray him as a heterosexual, to efforts at armchair analysis by Western biographers. Biographers have generally agreed that Tchaikovsky was homosexual. He sought the company of other men in his circle for extended periods, "associating openly and establishing professional connections with them." His first love was reportedly Sergey Kireyev, a younger fellow student at the Imperial School of Jurisprudence. According to Modest Tchaikovsky, this was Pyotr Ilyich's "strongest, longest and purest love". The degree to which the composer might have felt comfortable with his sexual nature has, however, remained open to debate. It is still unknown whether Tchaikovsky, according to musicologist and biographer David Brown, "felt tainted within himself, defiled by something from which he finally realized he could never escape" or whether, according to Alexander Poznansky, he experienced "no unbearable guilt" over his sexual nature and "eventually came to see his sexual peculiarities as an insurmountable and even natural part of his personality ... without experiencing any serious psychological damage." Relevant portions of his brother Modest's autobiography, where he tells of the composer's sexual orientation, have been published, as have letters previously suppressed by Soviet censors in which Tchaikovsky openly writes of it. Such censorship has persisted in the current Russian government, resulting in many officials, including the current culture minister Vladimir Medinsky, to outright deny his homosexuality.

Tchaikovsky lived as a bachelor for most of his life. In 1868 he met Belgian soprano Désirée Artôt. They became infatuated with each other and were engaged to be married but due to Artôt's refusal to give up the stage or settle in Russia, the relationship ended. Tchaikovsky later claimed she was the only woman he ever loved. In 1877, at the age of 37, he wed a former student, Antonina Miliukova. The marriage was a disaster. Mismatched psychologically and sexually, the couple lived together for only two and a half months before Tchaikovsky left, overwrought emotionally and suffering from an acute writer's block. Tchaikovsky's family remained supportive of him during this crisis and throughout his life. He was also aided by Nadezhda von Meck, the widow of a railway magnate, who had begun contact with him not long before the marriage. As well as an important friend and emotional support, she became his patroness for the next 13 years, which allowed him to focus exclusively on composition. Tchaikovsky's marital debacle may have forced him to face the full truth about his sexuality; he never blamed Antonina for the failure of their marriage.

Tchaikovsky remained abroad for a year after the disintegration of his marriage. During this time, he completed "Eugene Onegin", orchestrated his Fourth Symphony, and composed the Violin Concerto. He returned briefly to the Moscow Conservatory in the autumn of 1879. For the next few years, assured of a regular income from von Meck, he traveled incessantly throughout Europe and rural Russia, mainly alone, and avoided social contact whenever possible. During this time, Tchaikovsky's foreign reputation grew and a positive reassessment of his music also took place in Russia, thanks in part to Russian novelist Fyodor Dostoyevsky's call for "universal unity" with the West at the unveiling of the Pushkin Monument in Moscow in 1880. Before Dostoyevsky's speech, Tchaikovsky's music had been considered "overly dependent on the West." As Dostoyevsky's message spread throughout Russia, this stigma toward Tchaikovsky's music evaporated. An unprecedented acclaim for him even drew a cult following among the young intelligentsia of Saint Petersburg, including Alexandre Benois, Léon Bakst and Sergei Diaghilev.

Two musical works from this period stand out. With the Cathedral of Christ the Saviour nearing completion in Moscow in 1880, the 25th anniversary of the coronation of Alexander II in 1881, and the 1882 Moscow Arts and Industry Exhibition in the planning stage, Nikolai Rubinstein suggested that Tchaikovsky compose a grand commemorative piece. Tchaikovsky agreed and finished it within six weeks. He wrote to Nadezhda von Meck that this piece, the "1812 Overture", would be "very loud and noisy, but I wrote it with no warm feeling of love, and therefore there will probably be no artistic merits in it." He also warned conductor Eduard Nápravník that "I shan't be at all surprised and offended if you find that it is in a style unsuitable for symphony concerts." Nevertheless, the overture became, for many, "the piece by Tchaikovsky they know best.", particularly well-known for the use of cannon in the scores. 

On 23 March 1881, Nikolai Rubinstein died in Paris. That December, Tchaikovsky started work on his Piano Trio in A minor, "dedicated to the memory of a great artist." First performed privately at the Moscow Conservatory on the first anniversary of Rubinstein's death, the piece became extremely popular during the composer's lifetime; in November 1893, it would become Tchaikovsky's own elegy at memorial concerts in Moscow and St. Petersburg.

In 1884, Tchaikovsky began to shed his unsociability and restlessness. That March, Tsar Alexander III conferred upon him the Order of St. Vladimir (fourth class), which included a title of hereditary nobility and a personal audience with the Tsar. This was seen as a seal of official approval which advanced Tchaikovsky's social standing and might have been cemented in the composer's mind by the success of his Orchestral Suite No. 3 at its January 1885 premiere in Saint Petersburg. In 1885, Alexander III requested a new production of "Eugene Onegin" at the Bolshoi Kamenny Theatre in Saint Petersburg. By having the opera staged there and not at the Mariinsky Theatre, he served notice that Tchaikovsky's music was replacing Italian opera as the official imperial art. In addition, thanks to Ivan Vsevolozhsky, Director of the Imperial Theaters and a patron of the composer, Tchaikovsky was awarded a lifetime annual pension of 3,000 rubles from the Tsar. This made him the premier court composer, in practice if not in actual title.

Despite Tchaikovsky's disdain for public life, he now participated in it as part of his increasing celebrity and out of a duty he felt to promote Russian music. He helped support his former pupil Sergei Taneyev, who was now director of Moscow Conservatory, by attending student examinations and negotiating the sometimes sensitive relations among various members of the staff. He served as director of the Moscow branch of the Russian Musical Society during the 1889–1890 season. In this post, he invited many international celebrities to conduct, including Johannes Brahms, Antonín Dvořák and Jules Massenet.

During this period, Tchaikovsky also began promoting Russian music as a conductor, In January 1887, he substituted, on short notice, at the Bolshoi Theater in Moscow for performances of his opera "Cherevichki". Within a year, he was in considerable demand throughout Europe and Russia. These appearances helped him overcome life-long stage fright and boosted his self-assurance. In 1888, Tchaikovsky led the premiere of his Fifth Symphony in Saint Petersburg, repeating the work a week later with the first performance of his tone poem "Hamlet". Although critics proved hostile, with César Cui calling the symphony "routine" and "meretricious", both works were received with extreme enthusiasm by audiences and Tchaikovsky, undeterred, continued to conduct the symphony in Russia and Europe. Conducting brought him to the United States in 1891, where he led the New York Music Society's orchestra in his "Festival Coronation March" at the inaugural concert of Carnegie Hall.

In November 1887, Tchaikovsky arrived at Saint Petersburg in time to hear several of the Russian Symphony Concerts, devoted exclusively to the music of Russian composers. One included the first complete performance of his revised First Symphony; another featured the final version of Third Symphony of Nikolai Rimsky-Korsakov, with whose circle Tchaikovsky was already in touch. Rimsky-Korsakov, with Alexander Glazunov, Anatoly Lyadov and several other nationalistically minded composers and musicians, had formed a group known as the Belyayev circle, named after a merchant and amateur musician who became an influential music patron and publisher. Tchaikovsky spent much time in this circle, becoming far more at ease with them than he had been with the 'Five' and increasingly confident in showcasing his music alongside theirs. This relationship lasted until Tchaikovsky's death.

In 1892, Tchaikovsky was voted a member of the Académie des Beaux-Arts in France, only the second Russian subject to be so honored (the first was sculptor Mark Antokolski). The following year, the University of Cambridge in England awarded Tchaikovsky an honorary Doctor of Music degree.

On 16/28 October 1893, Tchaikovsky conducted the premiere of his Sixth Symphony, the "Pathétique" in Saint Petersburg. Nine days later, Tchaikovsky died there, aged 53. He was interred in Tikhvin Cemetery at the Alexander Nevsky Monastery, near the graves of fellow-composers Alexander Borodin, Mikhail Glinka, and Modest Mussorgsky; later, Rimsky-Korsakov and Balakirev were also buried nearby.

While Tchaikovsky's death has traditionally been attributed to cholera from drinking unboiled water at a local restaurant, as one story accounts, many writers have theorized that his death was a suicide. Opinion has been summarized as follows: "The polemics over [Tchaikovsky's] death have reached an impasse ... Rumors attached to the famous die hard ... As for illness, problems of evidence offer little hope of satisfactory resolution: the state of diagnosis; the confusion of witnesses; disregard of long-term effects of smoking and alcohol. We do not know how Tchaikovsky died. We may never find out ..." 

Tchaikovsky wrote many works that are popular with the classical music public, including his "Romeo and Juliet", the "1812 Overture", his three ballets ("The Nutcracker", "Swan Lake", "The Sleeping Beauty") and "Marche Slav". These, along with his First Piano Concerto and his Violin Concerto, the last three of his six numbered symphonies and his operas "The Queen of Spades" and "Eugene Onegin", are among his most familiar works. Almost as popular are the "Manfred" Symphony, "Francesca da Rimini", the "Capriccio Italien" and the Serenade for Strings.

Tchaikovsky displayed an unusually wide stylistic and emotional range, from salon works of innocuous charm to symphonies of tremendous depth, power, and grandeur. Some of his works, such as the "Variations on a Rococo Theme", employ a poised "Classical" form reminiscent of 18th-century composers such as Mozart (his favorite composer). Other compositions, such as his "Little Russian" symphony and his opera "Vakula the Smith", flirt with musical practices more akin to those of the Five, especially in their use of folk song. Other works, such as Tchaikovsky's last three symphonies, employ a personal musical idiom that facilitated intense emotional expression.

Tchaikovsky first visited Ukraine in 1864, staying in Trostianets where he wrote his first orchestral work, "The Storm" overture. Over the next 28 years, he visited over 15 places in Ukraine, where he stayed a few months at the time. Among his most favorite places was Kamianka, Cherkasy Oblast, where his sister Alexandra lived with her family. He wrote of Kamianka: "I found a feeling of peace in my soul, which I couldn't find in Moscow and St Petersburg". Tchaikovsky wrote more than 30 compositions while in Ukraine. He also visited Ukrainian composer Mykola Lysenko and attended his "Taras Bulba" opera performance in 1890 in the Kiev Opera House. Tchaikovsky was one of the founders of the Kiev Music Conservatory, which was later renamed after him. He also performed in concerts as a conductor in Kiev, Odessa, and Kharkiv.

American music critic and journalist Harold C. Schonberg wrote of Tchaikovsky's "sweet, inexhaustible, supersensuous fund of melody," a feature that has ensured his music's continued success with audiences. Tchaikovsky's complete range of melodic styles was as wide as that of his compositions. Sometimes he used Western-style melodies, sometimes original melodies written in the style of Russian folk song; sometimes he used actual folk songs. According to "The New Grove", Tchaikovsky's melodic gift could also become his worst enemy in two ways. The first challenge arose from his ethnic heritage. Unlike Western themes, the melodies that Russian composers wrote tended to be self-contained; they functioned with a mindset of stasis and repetition rather than one of progress and ongoing development. On a technical level, it made modulating to a new key to introduce a contrasting second theme exceedingly difficult, as this was literally a foreign concept that did not exist in Russian music. The second way melody worked against Tchaikovsky was a challenge that he shared with the majority of Romantic-age composers. They did not write in the regular, symmetrical melodic shapes that worked well with sonata form, such as those favored by Classical composers such as Haydn, Mozart or Beethoven, but were complete and independent in themselves. This completeness hindered their use as structural elements in combination with one another. This challenge was why the Romantics "were never natural symphonists." All a composer like Tchaikovsky could do with them was to essentially repeat them, even when he modified them to generate tension, maintain interest and satisfy listeners.

Harmony could be a potential trap for Tchaikovsky, according to Brown, since Russian creativity tended to focus on inertia and self-enclosed tableaux, while Western harmony worked against this to propel the music onward and, on a larger scale, shape it. Modulation, the shifting from one key to another, was a driving principle in both harmony and sonata form, the primary Western large-scale musical structure since the middle of the 18th century. Modulation maintained harmonic interest over an extended time-scale, provided a clear contrast between musical themes and showed how those themes were related to each other. One point in Tchaikovsky's favor was "a flair for harmony" that "astonished" Rudolph Kündinger, Tchaikovsky's music tutor during his time at the School of Jurisprudence. Added to what he learned at the Saint Petersburg Conservatory studies, this talent allowed Tchaikovsky to employ a varied range of harmony in his music, from the Western harmonic and textural practices of his first two string quartets to the use of the whole tone scale in the center of the finale of the Second Symphony, a practice more typically used by The Five.

Rhythmically, Tchaikovsky sometimes experimented with unusual meters. More often, he used a firm, regular meter, a practice that served him well in dance music. At times, his rhythms became pronounced enough to become the main expressive agent of the music. They also became a means, found typically in Russian folk music, of simulating movement or progression in large-scale symphonic movements—a "synthetic propulsion," as Brown phrases it, which substituted for the momentum that would be created in strict sonata form by the interaction of melodic or motivic elements. This interaction generally does not take place in Russian music. (For more on this, please see Repetition below.)

Tchaikovsky struggled with sonata form. Its principle of organic growth through the interplay of musical themes was alien to Russian practice. According to Brown and musicologists Hans Keller and Daniel Zhitomirsky, Tchaikovsky found his solution to large-scale structure while composing the Fourth Symphony. He essentially sidestepped thematic interaction and kept sonata form only as an "outline," as Zhitomirsky phrases it. Within this outline, the focus centered on periodic alternation and juxtaposition. Tchaikovsky placed blocks of dissimilar tonal and thematic material alongside one another, with what Keller calls "new and violent contrasts" between musical themes, keys, and harmonies. This process, according to Brown and Keller, builds momentum and adds intense drama. While the result, Warrack charges, is still "an ingenious episodic treatment of two tunes rather than a symphonic development of them" in the Germanic sense, Brown counters that it took the listener of the period "through a succession of often highly charged sections which "added up" to a radically new kind of symphonic experience" (italics Brown), one that functioned not on the basis of summation, as Austro-German symphonies did, but on one of accumulation.

Partly due to the melodic and structural intricacies involved in this accumulation and partly due to the composer's nature, Tchaikovsky's music became intensely expressive. This intensity was entirely new to Russian music and prompted some Russians to place Tchaikovsky's name alongside that of Dostoyevsky. German musicologist Hermann Kretzschmar credits Tchaikovsky in his later symphonies with offering "full images of life, developed freely, sometimes even dramatically, around psychological contrasts ... This music has the mark of the truly lived and felt experience." Botstein, in elaborating on this comment, suggests that listening to Tchaikovsky's music "became a psychological mirror connected to everyday experience, one that reflected on the dynamic nature of the listener’s own emotional self." This active engagement with the music "opened for the listener a vista of emotional and psychological tension and an extremity of feeling that possessed relevance because it seemed reminiscent of one’s own 'truly lived and felt experience' or one’s search for intensity in a deeply personal sense."

As mentioned above, repetition was a natural part of Tchaikovsky's music, just as it is an integral part of Russian music. His use of sequences within melodies (repeating a tune at a higher or lower pitch in the same voice) could go on for extreme length. The problem with repetition is that, over a period of time, the melody being repeated remains static, even when there is a surface level of rhythmic activity added to it. Tchaikovsky kept the musical conversation flowing by treating melody, tonality, rhythm and sound color as one integrated unit, rather than as separate elements. By making subtle but noticeable changes in the rhythm or phrasing of a tune, modulating to another key, changing the melody itself or varying the instruments playing it, Tchaikovsky could keep a listener's interest from flagging. By extending the number of repetitions, he could increase the musical and dramatic tension of a passage, building "into an emotional experience of almost unbearable intensity," as Brown phrases it, controlling when the peak and release of that tension would take place. Musicologist Martin Cooper calls this practice a subtle form of unifying a piece of music and adds that Tchaikovsky brought it to a high point of refinement. (For more on this practice, see the next section.)

Like other late Romantic composers, Tchaikovsky relied heavily on orchestration for musical effects. Tchaikovsky, however, became noted for the "sensual opulence" and "voluptuous timbrel virtuosity" of his scoring. Like Glinka, Tchaikovsky tended toward bright primary colors and sharply delineated contrasts of texture. However, beginning with the Third Symphony, Tchaikovsky experimented with an increased range of timbres Tchaikovsky's scoring was noted and admired by some of his peers. Rimsky-Korsakov regularly referred his students at the Saint Petersburg Conservatory to it and called it "devoid of all striving after effect, [to] give a healthy, beautiful sonority." This sonority, musicologist Richard Taruskin points out, is essentially Germanic in effect. Tchaikovsky's expert use of having two or more instruments play a melody simultaneously (a practice called doubling) and his ear for uncanny combinations of instruments resulted in "a generalized orchestral sonority in which the individual timbres of the instruments, being thoroughly mixed, would vanish."

In works like the "Serenade for Strings" and the "Variations on a Rococo Theme", Tchaikovsky showed he was highly gifted at writing in a style of 18th-century European pastiche. In the ballet "The Sleeping Beauty" and the opera "The Queen of Spades", Tchaikovsky graduated from imitation to full-scale evocation. This practice, which Alexandre Benois calls "passé-ism," lends an air of timelessness and immediacy, making the past seem as though it were the present. On a practical level, Tchaikovsky was drawn to past styles because he felt he might find the solution to certain structural problems within them. His Rococo pastiches also may have offered escape into a musical world purer than his own, into which he felt himself irresistibly drawn. (In this sense, Tchaikovsky operated in the opposite manner to Igor Stravinsky, who turned to Neoclassicism partly as a form of compositional self-discovery.) Tchaikovsky's attraction to ballet might have allowed a similar refuge into a fairy-tale world, where he could freely write dance music within a tradition of French elegance.

Of Tchaikovsky's Western contemporaries, Robert Schumann stands out as an influence in formal structure, harmonic practices and piano writing, according to Brown and musicologist Roland John Wiley. As mentioned earlier, Asafyev comments that Schumann left his mark on Tchaikovsky not just as a formal influence but also as an example of musical dramaturgy and self-expression. Leon Botstein claims the music of Franz Liszt and Richard Wagner also left their imprints on Tchaikovsky's orchestral style. The late-Romantic trend for writing orchestral suites, begun by Franz Lachner, Jules Massenet, and Joachim Raff after the rediscovery of Bach's works in that genre, may have influenced Tchaikovsky to try his own hand at them. His teacher Anton Rubinstein's opera "The Demon" became a model for the final tableau of "Eugene Onegin". So did Léo Delibes' ballets "Coppélia" and "Sylvia" for "The Sleeping Beauty" and Georges Bizet's opera "Carmen" (a work Tchaikovsky admired tremendously) for "The Queen of Spades". Otherwise, it was to composers of the past that Tchaikovsky turned—Beethoven, whose music he respected; Mozart, whose music he loved; Glinka, whose opera "A Life for the Tsar" made an indelible impression on him as a child and whose scoring he studied assiduously; and Adolphe Adam, whose ballet "Giselle" was a favorite of his from his student days and whose score he consulted while working on "The Sleeping Beauty". Beethoven's string quartets may have influenced Tchaikovsky's attempts in that medium. Other composers whose work interested Tchaikovsky included Hector Berlioz, Felix Mendelssohn, Giacomo Meyerbeer, and Henry Litolff.

Maes maintains that, regardless of what he was writing, Tchaikovsky's main concern was how his music impacted his listeners on an aesthetic level, at specific moments in the piece and on a cumulative level once the music had finished. What his listeners experienced on an emotional or visceral level became an end in itself. Tchaikovsky's focus on pleasing his audience might be considered closer to that of Mendelssohn or Mozart. Considering that he lived and worked in what was probably the last 19th-century feudal nation, the statement is not actually that surprising. And yet, even when writing so-called 'programme music, for example his Romeo and Juliet fantasy overture, he cast it in sonata form. His use of stylized 18th-century melodies and patriotic themes was geared toward the values of Russian aristocracy. He was aided in this by Ivan Vsevolozhsky, who commissioned "The Sleeping Beauty" from Tchaikovsky and the libretto for "The Queen of Spades" from Modest with their use of 18th century settings stipulated firmly. Tchaikovsky also used the polonaise frequently, the dance being a musical code for the Romanov dynasty and a symbol of Russian patriotism. Using it in the finale of a work could assure its success with Russian listeners.

Tchaikovsky's relationship with collaborators was mixed. Like Nikolai Rubinstein with the First Piano Concerto, virtuoso and pedagogue Leopold Auer rejected the Violin Concerto initially but changed his mind; he played it to great public success and taught it to his students, who included Jascha Heifetz and Nathan Milstein. Wilhelm Fitzenhagen "intervened considerably in shaping what he considered 'his' piece," the "Variations on a Rococo Theme", according to music critic Michael Steinberg. Tchaikovsky was angered by Fitzenhagen's license but did nothing; the Rococo Variations were published with the cellist's amendments. His collaboration on the three ballets went better and in Marius Petipa, who worked with him on the last two, he might have found an advocate. When "The Sleeping Beauty" was seen by its dancers as needlessly complicated, Petipa convinced them to put in the extra effort. Tchaikovsky compromised to make his music as practical as possible for the dancers and was accorded more creative freedom than ballet composers were usually accorded at the time. He responded with scores that minimized the rhythmic subtleties normally present in his work but were inventive and rich in melody, with more refined and imaginative orchestration than in the average ballet score.

Critical reception to Tchaikovsky's music has also varied but also improved over time. Even after 1880, some inside Russia held it suspect for not being nationalistic enough and thought Western European critics lauded it for exactly that reason. There might have been a grain of truth in the latter, according to musicologist and conductor Leon Botstein, as German critics especially wrote of the "indeterminacy of [Tchaikovsky's] artistic character ... being truly at home in the non-Russian." Of the foreign critics who did not care for his music, Eduard Hanslick lambasted the Violin Concerto as a musical composition "whose stink one can hear" and William Forster Abtrop wrote of the Fifth Symphony, "The furious peroration sounds like nothing so much as a horde of demons struggling in a torrent of brandy, the music growing drunker and drunker. Pandemonium, delerium tremens, raving, and above all, noise worse confounded!"

The division between Russian and Western critics remained through much of the 20th century but for a different reason. According to Brown and Wiley, the prevailing view of Western critics was that the same qualities in Tchaikovsky's music that appealed to audiences—its strong emotions, directness and eloquence and colorful orchestration—added up to compositional shallowness. The music's use in popular and film music, Brown says, lowered its esteem in their eyes still further. There was also the fact, pointed out earlier, that Tchaikovsky's music demanded active engagement from the listener and, as Botstein phrases it, "spoke to the listener’s imaginative interior life, regardless of nationality." Conservative critics, he adds, may have felt threatened by the "violence and 'hysteria' " they detected and felt such emotive displays "attacked the boundaries of conventional aesthetic appreciation—the cultured reception of art as an act of formalist discernment—and the polite engagement of art as an act of amusement."

There has also been the fact that the composer did not follow sonata form strictly, relying instead on juxtaposing blocks of tonalities and thematic groups. Maes states this point has been seen at times as a weakness rather than a sign of originality. Even with what Schonberg termed "a professional reevaluaton" of Tchaikovsky's work, the practice of faulting Tchaikovsky for not following in the steps of the Viennese masters has not gone away entirely, while his intent of writing music that would please his audiences is also sometimes taken to task. In a 1992 article, "New York Times" critic Allan Kozinn writes, "It is Tchaikovsky's flexibility, after all, that has given us a sense of his variability... Tchaikovsky was capable of turning out music—entertaining and widely beloved though it is—that seems superficial, manipulative and trivial when regarded in the context of the whole literature. The First Piano Concerto is a case in point. It makes a joyful noise, it swims in pretty tunes and its dramatic rhetoric allows (or even requires) a soloist to make a grand, swashbuckling impression. But it is entirely hollow."

In the 21st century, however, critics are reacting more positively to Tchaikovsky's tunefulness, originality, and craftsmanship. "Tchaikovsky is being viewed again as a composer of the first rank, writing music of depth, innovation and influence," according to cultural historian and author Joseph Horowitz. Important in this reevaluation is a shift in attitude away from the disdain for overt emotionalism that marked half of the 20th century. "We have acquired a different view of Romantic 'excess,'" Horowitz says. "Tchaikovsky is today more admired than deplored for his emotional frankness; if his music seems harried and insecure, so are we all."

Horowitz maintains that, while the standing of Tchaikovsky's music has fluctuated among critics, for the public, "it never went out of style, and his most popular works have yielded iconic sound-bytes , such as the love theme from "Romeo and Juliet"." Along with those tunes, Botstein adds, "Tchaikovsky appealed to audiences outside of Russia with an immediacy and directness that were startling even for music, an art form often associated with emotion." Tchaikovsky's melodies, stated with eloquence and matched by his inventive use of harmony and orchestration, have always ensured audience appeal. His popularity is considered secure, with his following in many countries, including Great Britain and the United States, second only to that of Beethoven. His music has also been used frequently in popular music and film.

According to Wiley, Tchaikovsky was a pioneer in several ways. "Thanks in large part to Nadezhda von Meck", Wiley writes, "he became the first full-time professional Russian composer." This, Wiley adds, allowed him the time and freedom to consolidate the Western compositional practices he had learned at the Saint Petersburg Conservatory with Russian folk song and other native musical elements to fulfill his own expressive goals and forge an original, deeply personal style. He made an impact in not only absolute works such as the symphony but also program music and, as Wiley phrases it, "transformed Liszt's and Berlioz's achievements ... into matters of Shakespearean elevation and psychological import." Wiley and Holden both note that Tchaikovsky did all this without a native school of composition upon which to fall back. They point out that only Glinka had preceded him in combining Russian and Western practices and his teachers in Saint Petersburg had been thoroughly Germanic in their musical outlook. He was, they write, for all intents and purposes alone in his artistic quest.
Maes and Taruskin write that Tchaikovsky believed that his professionalism in combining skill and high standards in his musical works separated him from his contemporaries in The Five. Maes adds that, like them, he wanted to produce music that reflected Russian national character but which did so to the highest European standards of quality. Tchaikovsky, according to Maes, came along at a time when the nation itself was deeply divided as to what that character truly was. Like his country, Maes writes, it took him time to discover how to express his Russianness in a way that was true to himself and what he had learned. Because of his professionalism, Maes says, he worked hard at this goal and succeeded. The composer's friend, music critic Hermann Laroche, wrote of "The Sleeping Beauty" that the score contained "an element deeper and more general than color, in the internal structure of the music, above all in the foundation of the element of melody. This basic element is undoubtedly Russian."

Tchaikovsky was inspired to reach beyond Russia with his music, according to Maes and Taruskin. His exposure to Western music, they write, encouraged him to think it belonged to not just Russia but also the world at large. Volkov adds that this mindset made him think seriously about Russia's place in European musical culture—the first Russian composer to do so. It steeled him to become the first Russian composer to acquaint foreign audiences personally with his own works, Warrack writes, as well as those of other Russian composers. In his biography of Tchaikovsky, Anthony Holden recalls the dearth of Russian classical music before Tchaikovsky's birth, then places the composer's achievements into historical perspective: "Twenty years after Tchaikovsky's death, in 1913, Igor Stravinsky's "The Rite of Spring" erupted onto the musical scene, signalling Russia's arrival into 20th-century music. Between these two very different worlds, Tchaikovsky's music became the sole bridge."

The following recording was made in Moscow in January 1890, by Julius Block on behalf of Thomas Edison.

According to musicologist Leonid Sabaneyev, Tchaikovsky was not comfortable with being recorded for posterity and tried to shy away from it. On an apparently separate visit from the one related above, Block asked the composer to play something on a piano or at least say something. Tchaikovsky refused. He told Block, "I am a bad pianist and my voice is raspy. Why should one eternalize it?"




</doc>
<doc id="24505" url="https://en.wikipedia.org/wiki?curid=24505" title="Phospholipid">
Phospholipid

Phospholipids are a class of lipids that are a major component of all cell membranes. They can form lipid bilayers because of their amphiphilic characteristic. The structure of the phospholipid molecule generally consists of two hydrophobic fatty acid "tails" and a hydrophilic "head" consisting of a phosphate group. The two components are joined together by a glycerol molecule. The phosphate groups can be modified with simple organic molecules such as choline, ethanolamine or serine.

The first phospholipid identified in 1847 as such in biological tissues was lecithin, or phosphatidylcholine, in the egg yolk of chickens by the French chemist and pharmacist, Theodore Nicolas Gobley. Biological membranes in eukaryotes also contain another class of lipid, sterol, interspersed among the phospholipids and together they provide membrane fluidity and mechanical strength. Purified phospholipids are produced commercially and have found applications in nanotechnology and materials science.

An amphiphile (from the Greek αμφις, amphis: both and φιλíα, philia: love, friendship) is a term describing a chemical compound possessing both hydrophilic (water-loving, polar) and lipophilic (fat-loving, non-polar) properties. The phospholipid head contains a negatively charged phosphate group and glycerol; it is hydrophilic. The phospholipid tails usually consist of 2 long fatty acid chains; they are hydrophobic and avoid interactions with water. When placed in aqueous solutions, phospholipids are driven by hydrophobic interactions that result in the fatty acid tails aggregating to minimize interactions with water molecules. These specific properties allow phospholipids to play an important role in the phospholipid bilayer. In biological systems, the phospholipids often occur with other molecules (e.g., proteins, glycolipids, sterols) in a bilayer such as a cell membrane. Lipid bilayers occur when hydrophobic tails line up against one another, forming a membrane of hydrophilic heads on both sides facing the water.

Such movement can be described by the fluid mosaic model, that describes the membrane as a mosaic of lipid molecules that act as a solvent for all the substances and proteins within it, so proteins and lipid molecules are then free to diffuse laterally through the lipid matrix and migrate over the membrane. Sterols contribute to membrane fluidity by hindering the packing together of phospholipids. However, this model has now been superseded, as through the study of lipid polymorphism it is now known that the behaviour of lipids under physiological (and other) conditions is not simple.


"See Sphingolipid"


Phospholipids have been widely used to prepare liposomal, ethosomal and other nanoformulations of topical, oral and parenteral drugs for differing reasons like improved bio-availability, reduced toxicity and increased permeability across membranes. Liposomes are often composed of phosphatidylcholine-enriched phospholipids and may also contain mixed phospholipid chains with surfactant properties. The ethosomal formulation of ketoconazole using phospholipids is a promising option for transdermal delivery in fungal infections.

Computational simulations of phospholipids are often performed using molecular dynamics with force fields such as GROMOS, CHARMM, or AMBER.

Phospholipids are optically highly birefringent, i.e. their refractive index is different along their axis as opposed to perpendicular to it. Measurement of birefringence can be achieved using cross polarisers in a microscope to obtain an image of e.g. vesicle walls or using techniques such as dual polarisation interferometry to quantify lipid order or disruption in supported bilayers.

There are no simple methods available for analysis of phospholipids since the close range of polarity between different phospholipid species makes detection difficult. Oil chemists often use spectroscopy to determine total Phosphorus abundance and then calculate approximate mass of phospholipids based on molecular weight of expected fatty acid species. Modern lipid profiling employs more absolute methods of analysis, with nuclear magnetic resonance spectroscopy (NMR), particularly P-NMR, while HPLC-ELSD provides relative values.

Phospholipid synthesis occurs in the cytosole adjacent to ER membrane that is studded with proteins that act in synthesis (GPAT and LPAAT acyl transferases, phosphatase and choline phosphotransferase) and allocation (flippase and floppase). Eventually a vesicle will bud off from the ER containing phospholipids destined for the cytoplasmic cellular membrane on its exterior leaflet and phospholipids destined for the exoplasmic cellular membrane on its inner leaflet.

Common sources of industrially produced phospholipids are soya, rapeseed, sunflower, chicken eggs, bovine milk, fish eggs etc. Each source has a unique profile of individual phospholipid species and consequently differing applications in food, nutrition, pharmaceuticals, cosmetics and drug delivery.

The key companies operating in the global Phospholipid market include Avanti Polar Lipids, Lipoid GmbH, VAV Life Sciences Pvt. Ltd. and NOF Corporation.

Some types of phospholipid can be split to produce products that function as second messengers in signal transduction. Examples include phosphatidylinositol (4,5)-bisphosphate (PIP), that can be split by the enzyme Phospholipase C into inositol triphosphate (IP) and diacylglycerol (DAG), which both carry out the functions of the G type of G protein in response to various stimuli and intervene in various processes from long term depression in neurons to leukocyte signal pathways started by chemokine receptors.

Phospholipids also intervene in prostaglandin signal pathways as the raw material used by lipase enzymes to produce the prostaglandin precursors. In plants they serve as the raw material to produce Jasmonic acid, a plant hormone similar in structure to prostaglandins that mediates defensive responses against pathogens.

Phospholipids can act as emulsifiers, enabling oils to form a colloid with water. Phospholipids are one of the components of lecithin which is found in egg-yolks, as well as being extracted from soy beans, and is used as a food additive in many products, and can be purchased as a dietary supplement. Lysolecithins are typically used for water-oil emulsions like margarine, due to their higher HLB ratio.




</doc>
<doc id="24507" url="https://en.wikipedia.org/wiki?curid=24507" title="Pierre Trudeau">
Pierre Trudeau

Joseph Philippe Pierre Yves Elliott Trudeau (; ; October 18, 1919 – September 28, 2000), often referred to by the initials PET, was a Canadian statesman who served as the 15th Prime Minister of Canada (1968–1979 and 1980–1984). He is the third longest-serving Prime Minister in Canadian history (behind William Lyon Mackenzie King and John A. Macdonald), having served for 15 years, 164 days.

Trudeau rose to prominence as a lawyer, intellectual, and activist in Quebec politics. In the 1960s he entered federal politics by joining the Liberal Party of Canada. He was appointed as Lester B. Pearson's Parliamentary Secretary and later became his Minister of Justice. Trudeau became a media sensation, inspiring "Trudeaumania", and took charge of the Liberals in 1968. From the late 1960s until the mid-1980s, his personality dominated the political scene to an extent never before seen in Canadian political life. Despite his personal motto, "Reason before passion", his personality and political career aroused polarizing reactions throughout Canada.

Admirers praise what they consider to be the force of Trudeau's intellect and his political acumen, maintaining national unity over the Quebec sovereignty movement, suppressing a Quebec terrorist crisis, fostering a pan-Canadian identity, and in achieving sweeping institutional reform, including the implementation of official bilingualism, patriation of the Constitution, and the establishment of the Charter of Rights and Freedoms. Critics accuse him of arrogance, of economic mismanagement, and of unduly centralizing Canadian decision-making to the detriment of the culture of Quebec and the economy of the Prairies. He retired from politics in 1984, and John Turner succeeded him.

His eldest son, Justin Trudeau, became the 23rd and current Prime Minister as a result of the 2015 federal election and is the first prime minister of Canada to be a descendant of a former prime minister.

The Trudeau family can be traced to Marcillac-Lanville in France in the 16th century and to a Robert Truteau (1544–1589). In 1659 the first Trudeau to arrive in Canada was Étienne Trudeau or Truteau (1641–1712), a carpenter and home builder from La Rochelle.

Pierre Trudeau was born at home at 5779 Durocher Avenue, Outremont, Montreal, on October 18, 1919, to Charles-Émile "Charley" Trudeau, a French-Canadian businessman and lawyer, and Grace Elliott, who was of mixed Scottish and French-Canadian descent. He had an older sister named Suzette and a younger brother named Charles Jr.; he remained close to both siblings for his entire life. The family had become quite wealthy by the time Trudeau was in his teens, as his father sold his prosperous gas station business to Imperial Oil. Trudeau attended the prestigious Collège Jean-de-Brébeuf (a private French Jesuit school), where he supported Quebec nationalism. Trudeau's father died when Pierre was 15 years old. This death hit him and the family very hard emotionally. Trudeau remained very close to his mother for the rest of her life.

According to long-time friend and colleague Marc Lalonde, the clerically influenced dictatorships of António de Oliveira Salazar in Portugal (the Estado Novo), Francisco Franco in Spain (the Spanish State), and Marshal Philippe Pétain in Vichy France were seen as political role models by many youngsters educated at elite Jesuit schools in Quebec. Lalonde asserts that Trudeau's later intellectual development as an "intellectual rebel, anti-establishment fighter on behalf of unions and promoter of religious freedom" came from his experiences after leaving Quebec to study in the United States, France and England, and to travel to dozens of countries. His international experiences allowed him to break from Jesuit influence and study French progressive Catholic philosophers such as Jacques Maritain and Emmanuel Mounier as well as John Locke and David Hume.

Trudeau earned his law degree at the Université de Montréal in 1943. During his studies, he was conscripted into the Canadian Army as part of the National Resources Mobilization Act. When conscripted, he decided to join the Canadian Officers' Training Corps, and he then served with the other conscripts in Canada, since they were not assigned to overseas military service until after the Conscription Crisis of 1944 after the Invasion of Normandy that June. Before this, all Canadians serving overseas were volunteers, and not conscripts.

Trudeau said he was willing to fight during World War II, but he believed that to do so would be to turn his back on the population of Quebec that he believed had been betrayed by the government of William Lyon Mackenzie King. Trudeau reflected on his opposition to conscription and his doubts about the war in his "Memoirs" (1993): "So there was a war? Tough ... if you were a French Canadian in Montreal in the early 1940s, you did not automatically believe that this was a just war ... we tended to think of this war as a settling of scores among the superpowers."

In an Outremont by-election in 1942 he campaigned for the anticonscription candidate Jean Drapeau (later the Mayor of Montreal). After the war Trudeau continued his studies, first taking a master's degree in political economy at Harvard University's Graduate School of Public Administration (now the John F. Kennedy School of Government). He then studied in Paris, France in 1947 at the Institut d'Études Politiques de Paris. Finally, he enrolled for a doctorate at the London School of Economics, but did not finish his dissertation.

Trudeau was interested in Marxist ideas in the 1940s and his Harvard dissertation was on the topic of Communism and Christianity. Thanks to the great intellectual migration away from Europe's fascism, Harvard had become a major intellectual centre in which he profoundly changed. Despite this, Trudeau found himself an outsider – a French Catholic living for the first time outside of Quebec in the predominantly Protestant American Harvard University. This isolation deepened finally into despair, and led to Trudeau's decision to continue his Harvard studies abroad.

In 1947 Trudeau travelled to Paris to continue his dissertation work. Over a five-week period he attended many lectures and became a follower of personalism after being influenced most notably by Emmanuel Mounier. He also was influenced by Nikolai Berdyaev, particularly his book "Slavery and Freedom". Max and Monique Nemni argue that Berdyaev's book influenced Trudeau's rejection of nationalism and separatism. The Harvard dissertation remained unfinished when Trudeau entered a doctoral program to study under the socialist economist Harold Laski at the London School of Economics. This cemented Trudeau's belief that Keynesian economics and social science were essential to the creation of the "good life" in democratic society.

From the late 1940s through the mid-1960s, Trudeau was primarily based in Montreal and was seen by many as an intellectual. In 1949 he was an active supporter of workers in the Asbestos Strike. In 1956 he edited an important book on the subject, "La grève de l'amiante", which argued that the strike was a seminal event in Quebec's history, marking the beginning of resistance to the conservative, Francophone clerical establishment and Anglophone business class that had long ruled the province. Throughout the 1950s, Trudeau, as the co-founder and editor of "Cité Libre", a dissident journal that helped provide the intellectual basis for the Quiet Revolution, was a leading figure in the opposition to the repressive rule of Premier of Quebec Maurice Duplessis.

From 1949 to 1951 Trudeau worked briefly in Ottawa, in the Privy Council Office of the Liberal Prime Minister Louis St. Laurent as an economic policy advisor. He wrote in his memoirs that he found this period very useful later on, when he entered politics, and that senior civil servant Norman Robertson tried unsuccessfully to persuade him to stay on.

His progressive values and his close ties with Co-operative Commonwealth Federation (CCF) intellectuals (including F. R. Scott, Eugene Forsey, Michael Kelway Oliver and Charles Taylor) led to his support of and membership in that federal democratic socialist party throughout the 1950s. Despite these connections, when Trudeau entered federal politics in the 1960s he decided to join the Liberal Party of Canada rather than the CCF's successor, the New Democratic Party (NDP). Trudeau felt the federal NDP could not achieve power, expressed doubts about the feasibility of the centralizing policies of the party, and felt that the party leadership tended toward a ""deux nations"" approach he could not support.

In his memoirs, published in 1993, Trudeau wrote that during the 1950s he wanted to teach at the Université de Montréal, but was blacklisted three times from doing so by Maurice Duplessis, then-Premier of Quebec. He was offered a position at Queen's University teaching political science by James Corry, who later became principal of Queen's, but turned it down because he preferred to teach in Quebec. During the 1950s he was blacklisted by the United States and prevented from entering that country because of a visit to a conference in Moscow, and because he subscribed to a number of left-wing publications. Trudeau later appealed the ban and it was rescinded.

An associate professor of law at the Université de Montréal from 1961 to 1965, Trudeau's views evolved towards a liberal position in favour of individual rights counter to the state and made him an opponent of Quebec nationalism. He admired the labour unions, which were tied to the Cooperative Commonwealth Federation (CCF), and tried to infuse his Liberal party with some of their reformist zeal. By the late 1950s Trudeau began to reject social democratic and labour parties, arguing that they should put their narrow goals aside and join forces with Liberals to fight for democracy first. In economic theory he was influenced by professors Joseph Schumpeter and John Kenneth Galbraith while he was at Harvard. Trudeau criticized the Liberal Party of Lester Pearson when it supported arming Bomarc missiles in Canada with nuclear warheads. Nevertheless, he was persuaded to join the party in 1965, together with his friends Gérard Pelletier and Jean Marchand. These "three wise men" ran successfully for the Liberals in the 1965 election. Trudeau himself was elected in the safe Liberal riding of Mount Royal, in western Montreal. He would hold this seat until his retirement from politics in 1984, winning each election with large majorities.

Upon arrival in Ottawa, Trudeau was appointed as Prime Minister Lester Pearson's parliamentary secretary, and spent much of the next year travelling abroad, representing Canada at international meetings and bodies, including the United Nations. In 1967 he was appointed to Pearson's cabinet as Minister of Justice.

As Minister of Justice, Trudeau was responsible for introducing the landmark Criminal Law Amendment Act, an omnibus bill whose provisions included, among other things, the decriminalization of homosexual acts between consenting adults, the legalization of contraception, abortion and lotteries, new gun ownership restrictions as well as the authorization of breathalyzer tests on suspected drunk drivers. Trudeau famously defended the segment of the bill decriminalizing homosexual acts by telling reporters that "there's no place for the state in the bedrooms of the nation", adding that "what's done in private between adults doesn't concern the Criminal Code". Trudeau paraphrased the term from Martin O'Malley's editorial piece in "The Globe and Mail" on December 12, 1967. Trudeau also liberalized divorce laws, and clashed with Quebec Premier Daniel Johnson, Sr. during constitutional negotiations.
At the end of Canada's centennial year in 1967, Prime Minister Pearson announced his intention to step down, and Trudeau entered the race for the Liberal leadership. His energetic campaign attracted massive media attention and mobilized many young people, who saw Trudeau as a symbol of generational change. Going into the leadership convention, Trudeau was the front-runner and a clear favourite with the Canadian public. However, many Liberals still had reservations given that he joined the Liberal Party in 1965 and that his views, particularly those on divorce, abortion, and homosexuality, were seen as radical and opposed by a substantial segment of the party. During the convention, prominent Cabinet Minister Judy LaMarsh was caught on television profanely stating that Trudeau wasn't a Liberal.

Nevertheless, at the April 1968 Liberal leadership convention, Trudeau was elected as the leader on the fourth ballot, with the support of 51% of the delegates. He defeated several prominent and long-serving Liberals including Paul Martin Sr., Robert Winters and Paul Hellyer. As the new leader of the governing Liberals, Trudeau was sworn in as Prime Minister two weeks later on April 20.

Trudeau soon called an election, for June 25. His election campaign benefited from an unprecedented wave of personal popularity called "Trudeaumania", which saw Trudeau mobbed by throngs of youths. Trudeau's main national opponents were PC leader Robert Stanfield and NDP leader Tommy Douglas, both popular figures who had been Premiers, respectively, of Nova Scotia and Saskatchewan. As a candidate Trudeau espoused participatory democracy as a means of making Canada a "Just Society". He defended vigorously the newly implemented universal health care and regional development programmes, as well as the recent reforms found in the Omnibus bill.

On the eve of the election, during the annual Saint-Jean-Baptiste Day parade in Montreal, rioting Quebec sovereignists threw rocks and bottles at the grandstand where Trudeau was seated, chanting "Trudeau au poteau!" (Trudeau – to the stake!). Rejecting the pleas of his aides that he take cover, Trudeau stayed in his seat, facing the rioters, without any sign of fear. The image of the defiant Prime Minister impressed the public, and he handily won the election the next day.

Trudeau's first government implemented many procedural reforms to make Parliament and the Liberal caucus meetings run more efficiently, significantly expanded the size and role of the Prime Minister's office, and substantially expanded the welfare state, with the establishment of new programmes.

Trudeau's first major legislative push was implementing the majority of recommendations of Pearson's Royal Commission on Bilingualism and Biculturalism via the Official Languages Act, which made French and English the co-equal official languages of the federal government. More controversial than the declaration (which was backed by the NDP and, with some opposition in caucus, the PCs) was the implementation of the Act's principles: between 1966 and 1976, the francophone proportion of the civil service and military doubled, causing alarm in some sections of anglophone Canada that they were being disadvantaged.

Trudeau's Cabinet fulfilled Part IV of the Royal Commission on Bilingualism and Biculturalism's report by announcing a "Multiculturalism Policy" on October 8, 1971. This statement recognized that while Canada was a country of two official languages, it recognized a plurality of cultures – "a multicultural policy within a bilingual framework". This annoyed public opinion in Quebec, which believed that it challenged Quebec's claim of Canada as a country of two nations.

The first major policy failure of Trudeau's first term was the 1969 White Paper on Indians, which was promoted by new Department of Indian and Northern Affairs minister Jean Chrétien as part of Trudeau's push for classical liberal participatory democracy. The statement proposed the general assimilation of First Nations into the Canadian body politic through the elimination of the Indian Act and Indian status, the parcelling of reserve land to private owners, and the elimination of the Department of Indian and Northern Affairs. The White Paper prompted the first major national mobilization of Indian and Aboriginal activists against the federal government's proposal, leading to Trudeau setting aside the legislation.

Trudeau's first serious test came during the October Crisis of 1970, when a Marxist group, the "Front de libération du Québec" (FLQ) kidnapped British Trade Consul James Cross at his residence on October 5. Five days later Quebec Labour Minister Pierre Laporte was also kidnapped. Trudeau, with the acquiescence of Premier of Quebec Robert Bourassa, responded by invoking the War Measures Act which gave the government sweeping powers of arrest and detention without trial. Trudeau presented a determined public stance during the crisis, answering the question of how far he would go to stop the violence by saying "Just watch me". Laporte was found dead on October 17 in the trunk of a car. The cause of his death is still debated. Five of the FLQ members were flown to Cuba in 1970 as part of a deal in exchange for James Cross' life, although they eventually returned to Canada years later, where they served time in prison.

Although this response is still controversial and was opposed at the time as excessive by parliamentarians like Tommy Douglas and David Lewis, it was met with only limited objections from the public.

After consultations with the provincial premiers, Trudeau agreed to attend a conference called by British Columbia Premier W. A. C. Bennett to attempt to finally patriate the Canadian constitution. Negotiations with the provinces by Minister of Justice John Turner created a draft agreement, known as the Victoria Charter, that entrenched a charter of rights, bilingualism, and a guarantee of a veto of constitutional amendments for Ontario and Quebec, as well as regional vetoes for Western Canada and Atlantic Canada, within the new constitution. The agreement was acceptable to the nine predominantly-English speaking provinces, while Quebec's Premier Robert Bourassa requested two weeks to consult with his cabinet. After a strong backlash of popular opinion against the agreement in Quebec, Bourassa stated Quebec would not accept it.

In foreign affairs, Trudeau kept Canada firmly in the North Atlantic Treaty Organization (NATO), but often pursued an independent path in international relations. He established Canadian diplomatic relations with the People's Republic of China, before the United States did, and went on an official visit to Beijing. He was known as a friend of Fidel Castro, the leader of Cuba.

Trudeau was the first world leader to meet John Lennon and his wife Yoko Ono on their "tour for world peace". Lennon said, after talking with Trudeau for 50 minutes, that Trudeau was "a beautiful person" and that "if all politicians were like Pierre Trudeau, there would be world peace".

In the federal election of 1972, the Liberals won a minority government, with the New Democratic Party led by David Lewis holding the balance of power.

Requiring NDP support to continue, the government would move to the political left, including the creation of Petro-Canada.

In May 1974 the House of Commons passed a motion of no confidence in the Trudeau government, defeating its budget bill after Trudeau intentionally antagonized Stanfield and Lewis. The election of 1974 focused mainly on the current economic recession. Stanfield proposed the immediate introduction of wage and price controls to help end the increasing inflation Canada was currently facing. Trudeau mocked the proposal, saying to a newspaper reporter that it was the equivalent of a magician saying "Zap! You're frozen", and instead promoted a variety of small tax cuts to curb inflation. A campaign tour featuring Trudeau's wife and infant sons was popular, and NDP supporters scared of wage controls moved toward the Liberals.

The Liberals were re-elected with a majority government with 141 of the 264 seats, prompting Stanfield's retirement. The Liberals won no seats in Alberta, though, where Peter Lougheed was a vociferous opponent of Trudeau's 1974 budget.

While popular with the electorate, Trudeau's promised minor reforms had little effect on the growing rate of inflation, and he struggled with conflicting advice on the crisis. In September 1975 the popular Finance Minister John Turner resigned over a perceived lack of support in countervailing measures. In October 1975, in an embarrassing about-face, Trudeau and new Finance Minister Donald Macdonald introduced wage and price controls by passing the Anti-Inflation Act. The breadth of the legislation, which touched on many powers traditionally considered the purview of the provinces, prompted a Supreme Court reference that only upheld the legislation as an emergency requiring Federal intervention under the British North America Act. During the annual 1975 Christmas interview with CTV, Trudeau discussed the economy, citing market failures and stating that more state intervention would be necessary. However, the academic wording and hypothetical solutions posed during the complex discussion led much of the public to believe he had declared capitalism itself a failure, creating a lasting distrust among increasingly neoliberal business leaders.

Trudeau continued his attempts at increasing Canada's international profile, including joining the G7 group of major economic powers in 1976 at the behest of U.S. President Gerald Ford. On July 14, 1976, after long and emotional debate, Bill C-84 was passed by the House of Commons by a vote of 130 to 124, abolishing the death penalty completely and instituting a life sentence without parole for 25 years for first-degree murder.

Trudeau faced increasing challenges in Quebec, starting with bitter relations with Bourassa and his Liberal government in Quebec. After a rise in the polls after the rejection of the Victoria Charter, the Quebec Liberals had taken a more confrontational approach with the Federal government on the constitution, French language laws, and the language of air traffic control in Quebec. Trudeau responded with increasing anger at what he saw as nationalist provocations against the Federal government's bilingualism and constitutional initiatives, at times expressing his personal contempt for Bourassa.

Partially in an attempt to shore up his support, Bourassa called a surprise election in 1976 that resulted in René Lévesque and the Parti Québécois (PQ) winning a majority government. The PQ had chiefly campaigned on a "good government" platform, but promised a referendum on independence to be held within their first mandate. Trudeau and Lévesque had been personal rivals, with Trudeau's intellectualism contrasting with Lévesque's more working-class image. While Trudeau claimed to welcome the "clarity" provided by the PQ victory, the unexpected rise of the sovereignist movement became, in his view, his biggest challenge.

As the PQ began to take power, Trudeau faced the prolonged failure of his marriage, which was covered in lurid detail on a day-by-day basis by the English language press. Trudeau's reserve was seen as dignified by contemporaries and his poll numbers actually rose during the height of coverage, but aides felt the personal tensions left him uncharacteristically emotional and prone to outbursts.

In 1976, Trudeau, succumbing to pressure from the Communist Chinese, issued an order barring Taiwan from participating as China in the 1976 Montreal Olympics, although technically it was a matter for the IOC. His action strained relations with the United States – from President Ford, future President Carter and the press – and subjected Canada to international condemnation and shame.

As the 1970s wore on, growing public exhaustion towards Trudeau's personality and the country's constitutional debates caused his poll numbers to fall rapidly in the late 1970s. At the 1978 G7 summit, he discussed strategies for the upcoming election with West German Chancellor Helmut Schmidt, who advised him to announce several spending cuts to quell criticism of the large deficits his government was running.

After a series of defeats in by-elections in 1978, Trudeau waited as long as he could to call a statutory general election in 1979. He finally did so in the 1979, only two months from the five-year limit provided under the British North America Act.

In the election of 1979, Trudeau and the Liberals faced declining poll numbers and the Joe Clark–led Progressive Conservatives focusing on "pocketbook" issues. Trudeau and his advisors, to contrast with the mild-mannered Clark, based their campaign on Trudeau's decisive personality and his grasp of the Constitution file, despite the general public's apparent wariness of both. The traditional Liberal rally at Maple Leaf Gardens saw Trudeau stressing the importance of major constitutional reform to general ennui, and his campaign "photo-ops" were typically surrounded by picket lines and protesters. Though polls portended disaster, Clark's struggles justifying his party's populist platform and a strong Trudeau performance in the election debate helped bring the Liberals to the point of contention.

Though winning the popular vote by four points, the Liberal vote was concentrated in Quebec and faltered in industrial Ontario, allowing the PCs to win the seat-count handily and form a minority government. Trudeau soon announced his intention to resign as Liberal Party leader and favoured Donald Macdonald to be his successor.

However, before a leadership convention could be held, with Trudeau's blessing and Allan MacEachen's maneuvering in the house, the Liberals supported an NDP subamendment to Clark's budget stating that the House had no confidence in the budget. In Canada, as in most other countries with a Westminster system, budget votes are indirectly considered to be votes of confidence in the government, and their failure automatically brings down the government. Liberal and NDP votes and Social Credit abstentions led to the subamendment passing 139-133, thereby toppling Clark's government and triggering a new election for a House less than a year old. The Liberal caucus, along with friends and advisers persuaded Trudeau to stay on as leader and fight the election, with Trudeau's main impetus being the upcoming referendum on Quebec sovereignty.

Trudeau and the Liberals engaged in a new strategy for the February 1980 election: facetiously called the "low bridge", it involved dramatically underplaying Trudeau's role and avoiding media appearances, to the point of refusing a televised debate. On election day Ontario returned to the Liberal fold, and Trudeau and the Liberals defeated Clark and won a majority government.

The Liberal victory in 1980 highlighted a sharp geographical divide in the country: the party had won no seats west of Manitoba. Trudeau, in an attempt to represent Western interests, offered to form a coalition government with Ed Broadbent's NDP, which had won 22 seats in the west, but was rebuffed by Broadbent out of fear the party would have no influence in a majority government. Trudeau then took the unusual step of appointing Liberal Senators from Western provinces to Cabinet, in the 22nd Canadian Ministry.

The first challenge Trudeau faced upon re-election was the referendum on Quebec sovereignty, called by the Parti Québécois government of René Lévesque, the date of which (May 20, 1980) was announced when Parliament re-opened after the election. Trudeau immediately initiated federal involvement in the referendum, reversing the Clark government's policy of leaving the issue to the Quebec Liberals and Claude Ryan. He appointed Jean Chrétien as the nominal spokesman for the federal government, helping to push the "Non" cause to working-class voters who tuned out the intellectual Ryan and Trudeau. Unlike Ryan and the Liberals, he refused to acknowledge the legitimacy of the referendum question, and noted that the "association" required consent from the other provinces.

As the campaign began to pick up steam, and the Quebec Liberals struggled in the legislative debate, Trudeau and Lévesque became heavily involved, with Lévesque mocking Trudeau's English middle name and aristocratic upbringing. Trudeau dramatically intervened in the best-received speech of his career a week before the referendum, extolling the virtues of federalism, mocking the unclear nature of the referendum, and dramatically pointing out that his name was neither French nor English, but a "Canadian" name. Trudeau noted that English Canada would have to listen to the various issues prompted by the referendum, and he promised a new constitutional agreement should it decide to stay in Canada. The "No" side (that is, "No" to sovereignty) ended up receiving nearly 60% of the vote. Trudeau stated that night that he "had never been so proud to be a Quebecer and a Canadian".

Trudeau had attempted patriation of the constitution earlier in his tenure, most notably with the Victoria Charter, but ran into the combined force of provincial premiers on the issues of an amending formula, a court-enforced Charter of Rights, and a further devolution of powers to the provinces. After the victory in the Quebec referendum, Chrétien was immediately tasked with creating a constitutional settlement.

After chairing a series of increasingly acrimonious conferences with first ministers on the issue, Trudeau announced the intention of the federal government to proceed with a request to the British parliament to patriate the constitution, with additions to be approved by a referendum without input from provincial governments. Trudeau was backed by the NDP, Ontario Premier Bill Davis, and New Brunswick Premier Richard Hatfield and was opposed by the remaining premiers and PC leader Joe Clark. After numerous provincial governments challenged the legality of the decision using their reference power, conflicting decisions prompted a Supreme Court decision that stated unilateral patriation was legal, but was in contravention of a constitutional convention that the provinces be consulted and have general agreement to the changes.

After the court decision, which prompted some reservations in the British parliament of accepting a unilateral request, Trudeau agreed to meet with the premiers one more time before proceeding. At the meeting, Trudeau reached an agreement with nine of the premiers on patriating the constitution and implementing the Canadian Charter of Rights and Freedoms, with the caveat that provincial legislatures would have the ability to use a notwithstanding clause to protect some laws from judicial oversight. The notable exception was Lévesque, who, Trudeau believed, would never have signed an agreement. The objection of the Quebec government to the new constitution became a source of continued acrimony between the federal and Quebec governments, and would forever stain Trudeau's reputation amongst nationalists in the province.

The Canada Act, which included the Constitution Act, 1982 and the Charter of Rights and Freedoms, was proclaimed by Queen Elizabeth II, as Queen of Canada, on April 17, 1982.

A series of difficult budgets by long-time loyalist Allan MacEachen in the early 1980s did not improve Trudeau's economic reputation. However, after tough bargaining on both sides, Trudeau did reach a revenue-sharing agreement on energy with Alberta Premier Peter Lougheed in 1982. Amongst the policies introduced by Trudeau's last term in office were an expansion in government support for Canada’s poorest citizens and the introduction of the National Energy Program (NEP), which created a firestorm of protest in the Western provinces and increased what many termed "Western alienation".

Trudeau's approval ratings slipped after the bounce from the 1982 patriation. Meanwhile, the Progressive Conservatives raced out to a substantial lead in opinion polls under new leader Brian Mulroney. By the beginning of 1984, it was obvious that the Liberals would be heavily defeated if Trudeau remained in office. On February 29, after what he described as a "long walk in the snow", Trudeau announced he would not lead the Liberals into the next election. He formally retired on June 30, ending his 15-year tenure as Prime Minister. Trudeau was succeeded as Liberal leader and Prime Minister by former cabinet minister John Turner, who had been out of politics for almost a decade.

Before handing power to Turner, Trudeau advised Governor General Jeanne Sauvé to appoint over 200 Liberals to patronage positions. He and Turner then crafted a legal agreement calling for Turner to advise an additional 70 patronage appointments. The sheer volume of appointments, combined with questions about the appointees' qualifications, led to condemnation from across the political spectrum. However, an apparent rebound in the polls prompted Turner to call an election for September 1984, almost a year before it was due. Turner's appointment deal with Trudeau came back to haunt the Liberals at the English-language debate, when Mulroney demanded that Turner apologize for not advising that the appointments be canceled—advice that Sauvé would have been required to follow by convention. Turner claimed that "I had no option" but to let the appointments stand, prompting Mulroney to tell him, "You had an option, sir--to say 'no'--and you chose to say 'yes' to the old attitudes and the old stories of the Liberal Party." The Liberals never recovered from this. Combined with anger in Quebec at being left out of patriation, the Liberals were heavily defeated at the 1984 election, losing 95 seats in what was then the worst defeat of a sitting government at the federal level.

Trudeau joined the Montreal law firm Heenan Blaikie as counsel and settled in the historic Maison Cormier in Montreal following his retirement from politics. Though he rarely gave speeches or spoke to the press, his interventions into public debate had a significant impact when they occurred. Trudeau wrote and spoke out against both the Meech Lake Accord and Charlottetown Accord proposals to amend the Canadian constitution, arguing that they would weaken federalism and the Charter of Rights if implemented. His opposition to both Accords was considered one of the major factors leading to the defeat of the two proposals.

He also continued to speak against the Parti Québécois and the sovereignty movement with less effect.

Trudeau also remained active in international affairs, visiting foreign leaders and participating in international associations such as the Club of Rome. He met with Soviet leader Mikhail Gorbachev and other leaders in 1985; shortly afterwards Gorbachev met President Ronald Reagan to discuss easing world tensions.

He published his memoirs in 1993; the book sold hundreds of thousands of copies in several editions, and became one of the most successful Canadian books ever published.

In the last years of his life, he was afflicted with Parkinson's disease and prostate cancer, and became less active, although he continued to work at his law practice until a few months before his death at the age of 80. He was devastated by the death of his youngest son, Michel Trudeau, who was killed in an avalanche on November 13, 1998.

Pierre Elliott Trudeau died on September 28, 2000, and was buried in the Trudeau family crypt, St-Rémi-de-Napierville Cemetery, Saint-Rémi, Quebec. His body lay in state in the Hall of Honour in Parliament Hill's Centre Block to allow Canadians to pay their last respects. Several world politicians, including former US President Jimmy Carter and Fidel Castro, attended the funeral. His son Justin delivered the eulogy during the state funeral which led to widespread speculation in the media that a career in politics was in his future. Eventually, Justin did enter politics, was elected to the House of Commons in late 2008, became the leader of the federal Liberal Party in April 2013, and was elected Prime Minister of Canada on October 19, 2015—the first time a father and his son have become prime ministers in Canada.

Trudeau was a Roman Catholic and attended church throughout his life. While mostly private about his beliefs, he made it clear that he was a believer, stating, in an interview with the "United Church Observer" in 1971: "I believe in life after death, I believe in God and I'm a Christian." Trudeau maintained, however, that he preferred to impose constraints on himself rather than have them imposed from the outside. In this sense, he believed he was more like a Protestant than a Catholic of the era in which he was schooled.

Michael W. Higgins, a former President of St. Thomas University, has researched Trudeau's spirituality and finds that it incorporated elements of three Catholic traditions. The first of these was the Jesuits who provided his education up to the college level. Trudeau frequently displayed the logic and love of argument consistent with that tradition. A second great spiritual influence in Trudeau's life was Dominican. According to Michel Gorges, Rector of the Dominican University College, Trudeau "considered himself a lay Dominican". He studied philosophy under Dominican Father Louis-Marie Régis and remained close to him throughout his life, regarding Régis as "spiritual director and friend". Another skein in Trudeau's spirituality was a contemplative aspect acquired from his association with the Benedictine tradition. According to Higgins, Trudeau was convinced of the centrality of meditation in a life fully lived. He took retreats at Saint-Benoît-du-Lac, Quebec and regularly attended Hours and the Eucharist at Montreal's Benedictine community.

Although never publicly theological in the way of Margaret Thatcher or Tony Blair, nor evangelical, in the way of Jimmy Carter or George W. Bush, Trudeau's spirituality, according to Michael W. Higgins, "suffused, anchored, and directed his inner life. In no small part, it defined him."

Described as a "swinging young bachelor" when he became prime minister in 1968, Trudeau also dated Hollywood star Barbra Streisand in 1969 and 1970. They had a serious romantic relationship, although (contrary to one published report) there was no express marriage proposal.

On March 4, 1971, while Prime Minister, he quietly married Margaret Sinclair at St. Stephen's Roman Catholic parish church in North Vancouver.

Contrary to his publicized exploits, Trudeau was an intense intellectual with robust work habits and little time for family or fun. As a result, Margaret felt trapped and bored in the marriage, feelings that were exacerbated by her retroactively diagnosed bipolar depression.

The couple had three sons: the first two, 23rd and current Prime Minister Justin (born 1971), and Alexandre (born 1973), were both born on Christmas Day, albeit two years apart. Their third son, Michel (1975–1998), died in an avalanche while skiing in Kokanee Glacier Provincial Park. They separated in 1977, and were finally divorced in 1984.

When his divorce was finalized in 1984, Trudeau became the first Canadian Prime Minister to become a single parent as the result of divorce. In 1984, Trudeau was romantically involved with Margot Kidder (a Canadian actress famous for her role as Lois Lane in "Superman: The Movie" and its sequels) in the last months of his prime-ministership and after leaving office.

In 1991, Trudeau became a father again, with Deborah Margaret Ryland Coyne, to his only daughter, Sarah.

Coyne later stood for the leadership of the federal Liberal Party in 2013, the same election that Trudeau's son Justin won, at which she came in fifth.

Trudeau began practising the Japanese martial art judo sometime in the mid-1950s when he was in his mid-thirties, and by the end of the decade he was ranked "ikkyū" (brown belt). Later, when he travelled to Japan as Prime Minister, he was promoted to "shodan" (first-degree black belt) by the Kodokan, and then promoted to "nidan" (second-degree black belt) by Masao Takahashi in Ottawa before leaving office. Trudeau began the night of his famous "walk in the snow" before announcing his retirement in 1984 by going to judo with his sons.

Trudeau remains well regarded by many Canadians. However, the passage of time has only slightly softened the strong antipathy he inspired among his opponents. Trudeau's strong personality, contempt for his opponents and distaste for compromise on many issues have made him, as historian Michael Bliss puts it, "one of the most admired and most disliked of all Canadian prime ministers". "He haunts us still", biographers Christina McCall and Stephen Clarkson wrote in 1990. Trudeau's electoral successes were matched in the 20th century only by those of Mackenzie King.

Trudeau's most enduring legacy may lie in his contribution to Canadian nationalism, and of pride in Canada in and for itself rather than as a derivative of the British Commonwealth. His role in this effort, and his related battles with Quebec on behalf of Canadian unity, cemented his political position when in office despite the controversies he faced—and remain the most remembered aspect of his tenure afterwards.

Some consider Trudeau's economic policies to have been a weak point. Inflation and unemployment marred much of his tenure as prime minister. When Trudeau took office in 1968 Canada had a debt of $18 billion (24% of GDP) which was largely left over from World War II, when he left office in 1984, that debt stood at $200 billion (46% of GDP), an increase of 83% in real terms. However, these trends were present in most western countries at the time, including the United States.

Many politicians still use the term "taking a walk in the snow", the line Trudeau used to describe his decision to leave office in 1984. Other popular Trudeauisms frequently used are "just watch me", the "Trudeau Salute", and "Fuddle Duddle".

Maclean's 1997 and 2011 scholarly surveys ranked him twice as the fifth best Canadian prime minister.

One of Trudeau's most enduring legacies is the 1982 patriation of the Canadian constitution, including a domestic amending formula and the Charter of Rights and Freedoms. It is seen as advancing civil rights and liberties and has become a cornerstone of Canadian values for most Canadians. It also represented the final step in Trudeau's liberal vision of a fully independent Canada based on fundamental human rights and the protection of individual freedoms as well as those of linguistic and cultural minorities. Court challenges based on the Charter of Rights have been used to advance the cause of women's equality, re-establish French school boards in provinces such as Alberta and Saskatchewan, and to mandate the adoption of same-sex marriage all across Canada. Section 35 of the Constitution Act, 1982, has clarified issues of aboriginal and equality rights, including establishing the previously denied aboriginal rights of Métis. Section 15, dealing with equality rights, has been used to remedy societal discrimination against minority groups. The coupling of the direct and indirect influences of the charter has meant that it has grown to influence every aspect of Canadian life and the override (notwithstanding clause) of the charter has been infrequently used.

Canadian conservatives claim the constitution has resulted in too much judicial activism on the part of the courts in Canada. It is also heavily criticized by Quebec nationalists, who resent that the 1982 amendments to the constitution were never ratified by any Quebec government and the constitution does not recognize a constitutional veto for Quebec.

Bilingualism is one of Trudeau's most lasting accomplishments, having been fully integrated into the Federal government's services, documents, and broadcasting (not, however, in provincial governments, except for Ontario, New Brunswick, and Manitoba). While official bilingualism has settled some of the grievances Francophones had towards the federal government, many Francophones had hoped that Canadians would be able to function in the official language of their choice no matter where in the country they were.

However, Trudeau's ambitions in this arena have been overstated: Trudeau once said that he regretted the use of the term "bilingualism", because it appeared to demand that all Canadians speak two languages. In fact, Trudeau's vision was to see Canada as a bilingual confederation in which "all" cultures would have a place. In this way, his conception broadened beyond simply the relationship of Quebec to Canada.

On October 8, 1971, Pierre Trudeau introduced the Multiculturalism Policy in the House of Commons. It was the first of its kind in the world, and was then emulated in several provinces, such as Alberta, Saskatchewan, Manitoba, and other countries most notably Australia, which has had a similar history and immigration pattern. Beyond the specifics of the policy itself, this action signalled an openness to the world and coincided with a more open immigration policy that had been brought in by Trudeau's predecessor Lester B. Pearson.

In the last years of his tenure he ensured both the National Gallery of Canada and the Canadian Museum of Civilization had proper homes in the national capital region. The Trudeau government also implemented programs which mandated Canadian content in film, and broadcasting, and gave substantial subsidies to develop the Canadian media and cultural industries. Though the policies remain controversial, Canadian media industries have become stronger since Trudeau's arrival.

Trudeau's posthumous reputation in the Western Provinces is notably less favourable than in the rest of English-speaking Canada, and he is sometimes regarded as the "father of Western alienation". To many westerners, Trudeau's policies seemed to favour other parts of the country, especially Ontario and Quebec, at their expense. Outstanding among such policies was the National Energy Program, which was seen as unfairly depriving western provinces of the full economic benefit from their oil and gas resources, in order to pay for nationwide social programs, and make regional transfer payments to poorer parts of the country. Sentiments of this kind were especially strong in oil-rich Alberta where unemployment rose from 4% to 10% following passage of the NEP. Estimates have placed Alberta's losses between $50 billion and $100 billion because of the NEP.

More particularly, two incidents involving Trudeau are remembered as having fostered Western alienation, and as emblematic of it. During a visit to Saskatoon, Saskatchewan on July 17, 1969, Trudeau met with a group of farmers who were protesting the Canadian Wheat Board. The widely remembered perception is that Trudeau dismissed the protesters' concerns with "Why should "I" sell your wheat?" – however, he had asked the question rhetorically and then proceeded to answer it himself. Years later, on a train trip through Salmon Arm, British Columbia, he "gave the finger" to a group of protesters through the carriage window – less widely remembered is that the protesters were shouting anti-French slogans at the train.

Trudeau's legacy in Quebec is mixed. Many credit his actions during the October Crisis as crucial in terminating the Front de libération du Québec (FLQ) as a force in Quebec, and ensuring that the campaign for Quebec separatism took a democratic and peaceful route. However, his imposition of the War Measures Act—which received majority support at the time—is remembered by some in Quebec and elsewhere as an attack on democracy. Trudeau is also credited by many for the defeat of the 1980 Quebec referendum.

At the federal level, Trudeau faced almost no strong political opposition in Quebec during his time as Prime Minister. For instance, his Liberal party captured 74 out of 75 Quebec seats in the 1980 federal election. Provincially, though, Québécois twice elected the pro-sovereignty Parti Québécois. Moreover, there were not at that time any pro-sovereignty federal parties such as the Bloc Québécois. Since the signing of the Constitutional Act of Canada in 1982 and until 2015, the Liberal Party of Canada had not succeeded in winning a majority of seats in Quebec. He was disliked by the Quebecois nationalists.

Trudeau was a strong advocate for a federalist model of government in Canada, developing and promoting his ideas in response and contrast to strengthening Quebec nationalist movements, for instance the social and political atmosphere created during Maurice Duplessis' time in power.

Federalism in this context can be defined as "a particular way of sharing political power among different peoples within a state...Those who believe in federalism hold that different peoples do not need states of their own in order to enjoy self-determination. Peoples ... may agree to share a single state while retaining substantial degrees of self-government over matters essential to their identity as peoples".

As a social democrat, Trudeau sought to combine and harmonize his theories on social democracy with those of federalism so that both could find effective expression in Canada. He noted the ostensible conflict between socialism, with its usually strong centralist government model, and federalism, which expounded a division and cooperation of power by both federal and provincial levels of government. In particular, Trudeau stated the following about socialists:

Trudeau pointed out that in sociological terms, Canada is inherently a federalist society, forming unique regional identities and priorities, and therefore a federalist model of spending and jurisdictional powers is most appropriate. He argues, "in the age of the mass society, it is no small advantage to foster the creation of quasi-sovereign communities at the provincial level, where power is that much less remote from the people."

Trudeau's idealistic plans for a cooperative Canadian federalist state were resisted and hindered as a result of his narrowness on ideas of identity and socio-cultural pluralism: "While the idea of a 'nation' in the sociological sense is acknowledged by Trudeau, he considers the allegiance which it generates—emotive and particularistic—to be contrary to the idea of cohesion between humans, and as such creating fertile ground for the internal fragmentation of states and a permanent state of conflict".

This position garnered significant criticism for Trudeau, in particular from Quebec and First Nations peoples on the basis that his theories denied their rights to nationhood. First Nations communities raised particular concerns with the proposed 1969 White Paper, developed under Trudeau by Jean Chrétien.

Trudeau chose the following jurists to be appointed as justices of the Supreme Court of Canada by the Governor General:


<br>

The following honours were bestowed upon him by the Governor General, or by Queen Elizabeth II herself:

Other honours include:

Pierre Trudeau received several Honorary Degrees in recognition of His political career, These Include





Trudeau was appointed a Companion of the Order of Canada on June 24, 1985. His citation reads:
Lawyer, professor, author and defender of human rights this statesman served as Prime Minister of Canada for fifteen years. Lending substance to the phrase "the style is the man," he has imparted, both in his and on the world stage, his quintessentially personal philosophy of modern politics.
Through hours of archival footage and interviews with Trudeau himself, the documentary "Memoirs" details the story of a man who used intelligence and charisma to bring together a country that was very nearly torn apart. Trudeau's life is depicted in two CBC Television mini-series. The first one, "Trudeau" (with Colm Feore in the title role), depicts his years as Prime Minister. "Trudeau II: Maverick in the Making" with Stéphane Demers as the young Pierre, and Tobie Pelletier as him in later years) portrays his earlier life.

The 1999 documentary film "" explores the impact of Trudeau's vision of Canadian bilingualism through interviews with eight young Canadians. He was the co-subject along with René Lévesque in the Donald Brittain-directed documentary mini-series "The Champions".



Books

News media

Other online sources

Editorial cartoons & humour

Archival videos of Trudeau


</doc>
<doc id="24508" url="https://en.wikipedia.org/wiki?curid=24508" title="Pencil">
Pencil

A pencil is a writing implement or art medium constructed of a narrow, solid pigment core inside a protective casing which prevents the core from being broken and/or from leaving marks on the user’s hand during use.

Pencils create marks by physical abrasion, leaving behind a trail of solid core material that adheres to a sheet of paper or other surface. They are distinct from pens, which instead disperse a trail of liquid or gel ink that stains the light colour of the paper by absorption.

Most pencil cores are made of graphite mixed with a clay binder which leaves grey or black marks that can be easily erased. Graphite pencils are used for both writing and drawing and result in durable markings: though writing is easily removable with an eraser, it is otherwise resistant to moisture, most chemicals, ultraviolet radiation, and natural aging. Other types of pencil core are less widely used, such as charcoal pencils, which are mainly used by artists for drawing and sketching. Coloured pencils are sometimes used by teachers or editors to correct submitted texts, but are typically regarded as art supplies, especially those with waxy core binders that tend to smear on paper instead of erasing. Grease pencils have a softer, crayon-like waxy core that can leave marks on smooth surfaces such as glass or porcelain.

The most common type of pencil casing is of thin wood, usually hexagonal in section but sometimes cylindrical, permanently bonded to the core. Similar permanent casings may be constructed of other materials such as plastic or paper. To use the pencil, the casing must be carved or peeled off to expose the working end of the core as a sharp point. Mechanical pencils have more elaborate casings which are not permanently bonded to the core. Instead, the casing supports a separate, mobile piece of pigment core that can be extended or retracted through the casing tip as needed; these pencil casings can be re-loaded with a new core (usually graphite) when necessary.

"Pencil", from Old French "pincel", from Latin a "little tail" (see "penis"; "pincellus" is Latin from the post-classical period) originally referred to an artist's fine brush of camel hair, also used for writing before modern lead or chalk pencils.

Though the archetypal pencil was an artist's brush, the stylus, a thin metal stick used for scratching in papyrus or wax tablets, was used extensively by the Romans and for palm-leaf manuscripts.

As a technique for drawing, the closest predecesor to the pencil was Silverpoint until in 1565 (some sources say as early as 1500), a large deposit of graphite was discovered on the approach to Grey Knotts from the hamlet of Seathwaite in Borrowdale parish, Cumbria, England. This particular deposit of graphite was extremely pure and solid, and it could easily be sawn into sticks. It remains the only large-scale deposit of graphite ever found in this solid form. Chemistry was in its infancy and the substance was thought to be a form of lead. Consequently, it was called "plumbago" (Latin for "lead ore"). Because the pencil core is still referred to as "lead", or a "lead", many people have the misconception that the graphite in the pencil is lead, and the black core of pencils is still referred to as "lead", even though it never contained the element lead. The words for pencil in German ("bleistift"), Irish ("peann luaidhe"), Arabic (قلم رصاص "qalam raṣāṣ"), and some other languages literally mean "lead pen".

The value of graphite would soon be realised to be enormous, mainly because it could be used to line the moulds for cannonballs; the mines were taken over by the Crown and were guarded. When sufficient stores of graphite had been accumulated, the mines were flooded to prevent theft until more was required.

The usefulness of graphite for pencils was discovered as well, but graphite for pencils had to be smuggled. The news of the usefulness of these early pencils spread far and wide, attracting the attention of artists all over the known world. Because graphite is soft, it requires some form of encasement. Graphite sticks were initially wrapped in string or sheepskin for stability. England would enjoy a monopoly on the production of pencils until a method of reconstituting the graphite powder was found in 1662 in Italy. However, the distinctively square English pencils continued to be made with sticks cut from natural graphite into the 1860s. The town of Keswick, near the original findings of block graphite, still manufactures pencils, the factory also being the location of the Cumberland Pencil Museum. The meaning of "graphite writing implement" apparently evolved late in the 16th century.

Around 1560, an Italian couple named Simonio and Lyndiana Bernacotti made what are likely the first blueprints for the modern, wood-encased carpentry pencil. Their version was a flat, oval, more compact type of pencil. Their concept involved the hollowing out of a stick of juniper wood. Shortly thereafter, a superior technique was discovered: two wooden halves were carved, a graphite stick inserted, and the halves then glued together—essentially the same method in use to this day.

The first attempt to manufacture graphite sticks from powdered graphite was in Nuremberg, Germany, in 1662. It used a mixture of graphite, sulphur, and antimony.

English and German pencils were not available to the French during the Napoleonic Wars; France, under naval blockade imposed by Great Britain, was unable to import the pure graphite sticks from the British Grey Knotts mines – the only known source in the world. France was also unable to import the inferior German graphite pencil substitute. It took the efforts of an officer in Napoleon's army to change this. In 1795, Nicolas-Jacques Conté discovered a method of mixing powdered graphite with clay and forming the mixture into rods that were then fired in a kiln. By varying the ratio of graphite to clay, the hardness of the graphite rod could also be varied. This method of manufacture, which had been earlier discovered by the Austrian Joseph Hardtmuth, the founder of the Koh-I-Noor in 1790, remains in use. In 1802, the production of graphite leads from graphite and clay was patented by the Koh-I-Noor company in Vienna.

In England, pencils continued to be made from whole sawn graphite. Henry Bessemer's first successful invention (1838) was a method of compressing graphite powder into solid graphite thus allowing the waste from sawing to be reused.

American colonists imported pencils from Europe until after the American Revolution. Benjamin Franklin advertised pencils for sale in his "Pennsylvania Gazette" in 1729, and George Washington used a three-inch pencil when he surveyed the Ohio Territory in 1762. It is said that William Munroe, a cabinetmaker in Concord, Massachusetts, made the first American wood pencils in 1812. This was not the only pencil-making occurring in Concord. According to Henry Petroski, transcendentalist philosopher Henry David Thoreau discovered how to make a good pencil out of inferior graphite using clay as the binder; this invention was prompted by his father's pencil factory in Concord, which employed graphite found in New Hampshire in 1821 by Charles Dunbar.

Munroe's method of making pencils was painstakingly slow, and in the neighbouring town of Acton, a pencil mill owner named Ebenezer Wood set out to automate the process at his own pencil mill located at Nashoba Brook. He used the first circular saw in pencil production. He constructed the first of the hexagon- and octagon-shaped wooden casings. Ebenezer did not patent his invention and shared his techniques with anyone. One of those was Eberhard Faber of New York, who became the leader in pencil production.

Joseph Dixon, an inventor and entrepreneur involved with the Tantiusques granite mine in Sturbridge, Massachusetts, developed a means to mass-produce pencils. By 1870, The Joseph Dixon Crucible Company was the world’s largest dealer and consumer of graphite and later became the contemporary Dixon Ticonderoga pencil and art supplies company.

By the end of the 19th century, over 240,000 pencils were used each day in the US. The favoured timber for pencils was Red Cedar as it was aromatic and did not splinter when sharpened. In the early 20th century supplies of Red Cedar were dwindling so that pencil manufacturers were forced to recycle the wood from cedar fences and barns to maintain supply.

One effect of this was that "during World War II rotary pencil sharpeners were outlawed in Britain because they wasted so much scarce lead and wood, and pencils had to be sharpened in the more conservative manner – with knives."

It was soon discovered that Incense cedar, when dyed and perfumed to resemble Red Cedar, was a suitable alternative and most pencils today are made from this timber which is grown in managed forests. Over 14 billion pencils are manufactured worldwide annually. Less popular alternatives to cedar include basswood and alder.

In Southeast Asia the wood Jelutong may be used to create pencils (though the use of this rainforest species is controversial). Environmentalists prefer the use of Pulai – another wood native to the region and used in pencil manufacturing.

On 30 March 1858, Hymen Lipman received the first patent for attaching an eraser to the end of a pencil. In 1862, Lipman sold his patent to Joseph Reckendorfer for $100,000, who went on to sue pencil manufacturer Faber-Castell for infringement. In 1875, the Supreme Court of the US ruled against Reckendorfer declaring the patent invalid.

Historian Henry Petroski notes that while ever more efficient means of mass production of pencils has driven the replacement cost of a pencil down, before this people would continue to use even the stub of a pencil. For those who "did not feel comfortable using a stub, pencil extenders were sold. These devices function something like a "porte-crayon"...the pencil stub can be inserted into the end of a shaft...Extenders were especially common among engineers and draftsmen, whose favorite pencils were priced dearly. The use of an extender also has the advantage that the pencil does not appreciably change its heft as it wears down." Artists currently use extenders to maximize the use of their colored pencils.









A standard, #2, hexagonal pencil is long.


There are also pencils which use mechanical methods to push lead through a hole at the end. These can be divided into two groups: propelling pencils use an internal mechanism to push the lead out from an internal compartment, while clutch pencils merely hold the lead in place (the lead is extended by releasing it and allowing some external force, usually gravity, to pull it out of the body). The erasers (sometimes replaced by a sharpener on pencils with larger lead sizes) are also removable (and thus replaceable), and usually cover a place to store replacement leads. Mechanical pencils are popular for their longevity and the fact that they may never need sharpening. Lead types are based on grade and size; with standard sizes being , , , , , , , , and (ISO 9175-1)—the size is available, but is not considered a standard ISO size.

Pioneered by Taiwanese stationery manufacturer Bensia Pioneer Industrial Corporation in the early 1970s, the product is also known as Bensia Pencils, stackable pencils or non-sharpening pencils. It is a type of pencil where many short pencil tips are housed in a cartridge-style plastic holder. A blunt tip is removed by pulling it from the writing end of the body and re-inserting it into the open-ended bottom of the body, thereby pushing a new tip to the top.

Invented by Harold Grossman for Empire Pencil Company in 1967 and subsequently improved upon by Arthur D. Little for Empire from 1969 through the early 1970s; the plastic pencil was commercialised by Empire as the "EPCON" Pencil. These pencils were co-extruded, extruding a plasticised graphite mix within a wood-composite core.

Residual graphite from a pencil stick is not poisonous, and graphite is harmless if consumed.

Although lead has not been used for writing since antiquity, lead poisoning from pencils was not uncommon. Until the middle of the 20th century the paint used for the outer coating could contain high concentrations of lead, and this could be ingested when the pencil was sucked or chewed.

The lead of the pencil is a mix of finely ground graphite and clay powders. Before the two substances are mixed, they are separately cleaned of foreign matter and dried in a manner that creates large square cakes. Once the cakes have fully dried, the graphite and the clay squares are mixed together using water. The amount of clay content added to the graphite depends on the intended pencil hardness (lower proportions of clay makes the core softer), and the amount of time spent on grinding the mixture determines the quality of the lead. The mixture is then shaped into long spaghetti-like strings, straightened, dried, cut, and then tempered in a kiln. The resulting strings are dipped in oil or molten wax, which seeps into the tiny holes of the material and allows for the smooth writing ability of the pencil. A juniper or incense-cedar plank with several long parallel grooves is cut to fashion a "slat," and the graphite/clay strings are inserted into the grooves. Another grooved plank is glued on top, and the whole assembly is then cut into individual pencils, which are then varnished or painted. Many pencils feature an eraser on the top and so the process is usually still considered incomplete at this point. Each pencil has a shoulder cut on one end of the pencil to allow for a metal ferrule to be secured onto the wood. A rubber plug is then inserted into the ferrule for a functioning eraser on the end of the pencil.

Graphite pencils are made of a mixture of clay and graphite and their darkness varies from light grey to black: the more clay the harder the pencil. There is a wide range of grades available, mainly for artists who are interested in creating a full range of tones from light grey to black. Engineers prefer harder pencils which allow for a greater control in the shape of the lead.

Manufacturers distinguish their pencils by grading them, but there is no common standard. Two pencils of the same grade but different manufacturers will not necessarily make a mark of identical tone nor have the same hardness.

Most manufacturers, and almost all in Europe, designate their pencils with the letters "H" (commonly interpreted as "hardness") to "B" (commonly "blackness"), as well as "F" (usually taken to mean "fineness", although F pencils are no more fine or more easily sharpened than any other grade. Also known as "firm" in Japan). The standard writing pencil is graded "HB". This designation might have been first used in the early 20th century by Brookman, an English pencil maker. It used "B" for black and "H" for hard; a pencil's grade was described by a sequence or successive Hs or Bs such as "BB" and "BBB" for successively softer leads, and "HH" and "HHH" for successively harder ones. The Koh-i-Noor Hardtmuth pencil manufacturers claim to have first used the HB designations, with "H" standing for Hardtmuth, "B" for the company's location of Budějovice, and "F" for Franz Hardtmuth, who was responsible for technological improvements in pencil manufacture.

As of 2009, a set of pencils ranging from a very hard, light-marking pencil to a very soft, black-marking pencil usually ranges from hardest to softest as follows:

Koh-i-noor offers twenty grades from 10H to 8B for its 1500 series; Mitsubishi Pencil offers twenty-two grades from 10H to 10B for its Hi-uni range; Derwent produces twenty grades from 9H to 9B for its graphic pencils and Staedtler produces sixteen from 6H to 8B for its Mars Lumograph pencils.

Numbers as designation were first used by Conté and later by John Thoreau, father of Henry David Thoreau, in the 19th century.
Although Conté/Thoreau's equivalence table is widely accepted, not all manufacturers follow it; for example, Faber-Castell uses a different equivalence table in its "Grip 2001" pencils: 1 = 2B, 2 = B, 2½ = HB, 3 = H, 4 = 2H.

Graded pencils can be used for a rapid test that provides relative ratings for a series of coated panels but can't be used to compare the pencil hardness of different coatings. This test defines a "pencil hardness" of a coating as the grade of the hardest pencil that does not permanently mark the coating when pressed firmly against it at a 45 degree angle. For standardized measurements, there are Mohs hardness testing pencils on the market. 

The majority of pencils made in the US are painted yellow. According to Henry Petroski, this tradition began in 1890 when the L. & C. Hardtmuth Company of Austria-Hungary introduced their Koh-I-Noor brand, named after the famous diamond. It was intended to be the world's best and most expensive pencil, and at a time when most pencils were either painted in dark colours or not at all, the Koh-I-Noor was yellow.
As well as simply being distinctive, the colour may have been inspired by the Austro-Hungarian flag; it was also suggestive of the Orient at a time when the best-quality graphite came from Siberia. Other companies then copied the yellow colour so that their pencils would be associated with this high-quality brand, and chose brand names with explicit Oriental references, such as Mikado (renamed Mirado) and Mongol.

Not all countries use yellow pencils. German and Brazilian pencils, for example, are often green, blue or black, based on the trademark colours of Faber-Castell, a major German stationery company which has plants in those countries. In southern European countries, pencils tend to be dark red or black with yellow lines, while in Australia, they are red with black bands at one end. In India, the most common pencil colour scheme was dark red with black lines, and pencils with a large number of colour schemes are produced by various companies.

Pencils are commonly round, hexagonal, or sometimes triangular in section. Carpenters' pencils are typically oval or rectangular, so they cannot easily roll away during work.


The following table lists the prominent manufacturers of wood-cased (including wood-free) pencils around the world.




</doc>
<doc id="24509" url="https://en.wikipedia.org/wiki?curid=24509" title="Pierre Curie">
Pierre Curie

Pierre Curie (; ; 15 May 1859 – 19 April 1906) was a French physicist, a pioneer in crystallography, magnetism, piezoelectricity and radioactivity. In 1903 he received the Nobel Prize in Physics with his wife, Marie Skłodowska-Curie, and Henri Becquerel, "in recognition of the extraordinary services they have rendered by their joint researches on the radiation phenomena discovered by Professor Henri Becquerel".

Born in Paris on 15 May 1859, Pierre Curie was the son of Eugene Curie (28 August 1827 – 25 February 1910), a doctor of Alsatian Protestant origin, and Sophie-Claire Depouilly Curie (15 January 1832 – 27 September 1897). He was educated by his father and in his early teens showed a strong aptitude for mathematics and geometry. When he was 16, he earned his math degree. By the age of 18 he had completed the equivalent of a higher degree, but did not proceed immediately to a doctorate due to lack of money. Instead he worked as a laboratory instructor. When Pierre Curie was preparing his bachelor of science degree, he worked in the laboratory of Jean-Gustave Bourbouze in the Faculty of Science.

In 1880, Pierre and his older brother Jacques (1856–1941) demonstrated that an electric potential was generated when crystals were compressed, i.e. piezoelectricity. To provide accurate measurements needed for their work, Pierre Curie created a highly sensitive instrument called the Curie scale. He used weights, microscopic meter readers, and pneumatic dampeners to create the scale. Also, to aid their work, they invented the piezoelectric quartz electrometer. Shortly afterwards, in 1881, they demonstrated the reverse effect: that crystals could be made to deform when subject to an electric field. Almost all digital electronic circuits now rely on this in the form of crystal oscillators.

Pierre Curie was introduced to Maria Skłodowska by their friend, physicist Józef Wierusz-Kowalski. Curie took her into his laboratory as his student. His admiration for her grew when he realized that she would not inhibit his research. He began to regard Sklodowska as his muse. She refused his initial proposal, but finally agreed to marry him on 26 July 1895.

Prior to his famous doctoral studies on magnetism, he designed and perfected an extremely sensitive torsion balance for measuring magnetic coefficients. Variations on this equipment were commonly used by future workers in that area. Pierre Curie studied ferromagnetism, paramagnetism, and diamagnetism for his doctoral thesis, and discovered the effect of temperature on paramagnetism which is now known as Curie's law. The material constant in Curie's law is known as the Curie constant. He also discovered that ferromagnetic substances exhibited a critical temperature transition, above which the substances lost their ferromagnetic behavior. This is now known as the Curie temperature. The Curie temperature is used to study plate tectonics, treat hypothermia, measure caffeine, and to understand extraterrestrial magnetic fields.

Pierre Curie formulated what is now known as the "Curie Dissymmetry Principle": a physical effect cannot have a dissymmetry absent from its efficient cause. For example, a random mixture of sand in zero gravity has no dissymmetry (it is isotropic). Introduce a gravitational field, and there is a dissymmetry because of the direction of the field. Then the sand grains can 'self-sort' with the density increasing with depth. But this new arrangement, with the directional arrangement of sand grains, actually reflects the dissymmetry of the gravitational field that causes the separation.

Curie worked with his wife in isolating polonium and radium. They were the first to use the term "radioactivity", and were pioneers in its study. Their work, including Marie Curie's celebrated doctoral work, made use of a sensitive piezoelectric electrometer constructed by Pierre and his brother Jacques Curie. Pierre Curie's 1898 publication with his wife Mme. Curie and also with M. G. Bémont for their discovery of radium and polonium was honored by a Citation for Chemical Breakthrough Award from the Division of History of Chemistry of the American Chemical Society presented to the ESPCI ParisTech (officially the École supérieure de physique et de chimie industrielles de la Ville de Paris) in 2015.

Curie and one of his students, Albert Laborde, made the first discovery of nuclear energy, by identifying the continuous emission of heat from radium particles. Curie also investigated the radiation emissions of radioactive substances, and through the use of magnetic fields was able to show that some of the emissions were positively charged, some were negative and some were neutral. These correspond to alpha, beta and gamma radiation.

The curie is a unit of radioactivity (3.7 × 10 decays per second or 37 gigabecquerels) originally named in honor of Curie by the Radiology Congress in 1910, after his death. Subsequently, there has been some controversy over whether the naming was in honor of Pierre, Marie, or both.

In the late nineteenth century, Pierre Curie was investigating the mysteries of ordinary magnetism when he became aware of the spiritualist experiments of other European scientists, such as Charles Richet and Camille Flammarion. Pierre Curie initially thought systematic investigation into the paranormal could help with some unanswered questions about magnetism. He wrote to his fiancée Marie: "I must admit that those spiritual phenomena intensely interest me. I think in them are questions that deal with physics." Pierre Curie's notebooks from this period show he read many books on spiritualism. He did not attend séances such as those of Eusapia Palladino in Paris in 1905–6 as a mere spectator, and his goal certainly was not to communicate with spirits. He saw the séances as scientific experiments, tried to monitor different parameters, and took detailed notes of every observation. Despite studying spiritualism, Curie was an atheist.

Pierre and Marie Curie's daughter, Irène, and their son-in-law, Frédéric Joliot-Curie, were also physicists involved in the study of radioactivity, and each received Nobel prizes for their work as well. The Curies' other daughter, Ève, wrote a noted biography of her mother. She was the only member of the Curie family to not become a physicist. Ève married Henry Richardson Labouisse, Jr., who received a Nobel Peace Prize on behalf of Unicef in 1965. Pierre and Marie Curie's granddaughter, Hélène Langevin-Joliot, is a professor of nuclear physics at the University of Paris, and their grandson, Pierre Joliot, who was named after Pierre Curie, is a noted biochemist.

Pierre Curie died in a street accident in Paris on 19 April 1906. Crossing the busy Rue Dauphine in the rain at the Quai de Conti, he slipped and fell under a heavy horse-drawn cart. He died instantly when one of the wheels ran over his head, fracturing his skull. Statements made by his father and lab assistant imply that Curie's characteristic absent-minded preoccupation with his thoughts contributed to his death.

Both the Curies experienced radium burns, both accidentally and voluntarily, and were exposed to extensive doses of radiation while conducting their research. They experienced radiation sickness and Marie Curie died of aplastic anemia in 1934. Even now, all their papers from the 1890s, even her cookbooks, are too dangerous to touch. Their laboratory books are kept in special lead boxes and people who want to see them have to wear protective clothing. Had Pierre Curie not been killed as he was, it is likely that he would have eventually died of the effects of radiation, as did his wife, their daughter, Irène, and her husband, Frédéric Joliot.

In April 1995, Pierre and Marie Curie were moved from their original resting place, a family cemetery, and enshrined in the crypt of the Panthéon in Paris. Marie Curie was the first woman to be honored in this way "for her own merits".




</doc>
<doc id="24510" url="https://en.wikipedia.org/wiki?curid=24510" title="Pushdown automaton">
Pushdown automaton

In the theory of computation, a branch of theoretical computer science, a pushdown automaton (PDA) is 
a type of automaton that employs a stack.

Pushdown automata are used in theories about what can be computed by machines. They are more capable than finite-state machines but less capable than Turing machines.
Deterministic pushdown automata can recognize all deterministic context-free languages while nondeterministic ones can recognize all context-free languages, with the former often used in parser design.

The term "pushdown" refers to the fact that the stack can be regarded as being "pushed down" like a tray dispenser at a cafeteria, since the operations never work on elements other than the top element. A stack automaton, by contrast, does allow access to and operations on deeper elements. Stack automata can recognize a strictly larger set of languages than pushdown automata.
A nested stack automaton allows full access, and also allows stacked values to be entire sub-stacks rather than just single finite symbols.

A finite state machine just looks at the input signal and the current state: it has no stack to work with. It chooses a new state, the result of following the transition. A pushdown automaton (PDA) differs from a finite state machine in two ways:

A pushdown automaton reads a given input string from left to right. In each step, it chooses a transition by indexing a table by input symbol, current state, and the symbol at the top of the stack. A pushdown automaton can also manipulate the stack, as part of performing a transition. The manipulation can be to push a particular symbol to the top of the stack, or to pop off the top of the stack. The automaton can alternatively ignore the stack, and leave it as it is. 

Put together: Given an input symbol, current state, and stack symbol, the automaton can follow a transition to another state, and optionally manipulate (push or pop) the stack.

If, in every situation, at most one such transition action is possible, then the automaton is called a deterministic pushdown automaton (DPDA). In general, if several actions are possible, then the automaton is called a general, or nondeterministic, PDA. A given input string may drive a nondeterministic pushdown automaton to one of several configuration sequences; if one of them leads to an accepting configuration after reading the complete input string, the latter is said to belong to the "language accepted by the automaton".
We use standard formal language notation: formula_1 denotes the set of strings over alphabet formula_2 and formula_3 denotes the empty string.

A PDA is formally defined as a 7-tuple:

formula_4
where


An element formula_13 is a transition of formula_14. It has the intended meaning that formula_14, in state formula_16, on the input formula_17 and with formula_18 as topmost stack symbol, may read formula_19, change the state to formula_20, pop formula_21, replacing it by pushing formula_22. The formula_23 component of the transition relation is used to formalize that the PDA can either read a letter from the input, or proceed leaving the input untouched.
In many texts the transition relation is replaced by an (equivalent) formalization, where


Here formula_27 contains all possible actions in state formula_28 with formula_21 on the stack, while reading formula_19 on the input. One writes for example formula_31 precisely when formula_32 because formula_33. Note that "finite" in this definition is essential.

In order to formalize the semantics of the pushdown automaton a description of the current situation is introduced. Any 3-tuple formula_34 is called an instantaneous description (ID) of formula_14, which includes the current state, the part of the input tape that has not been read, and the contents of the stack (topmost symbol written first). The transition relation formula_8 defines the step-relation formula_37 of formula_14 on instantaneous descriptions. For instruction formula_13 there exists a step formula_40, for every formula_41 and every formula_42.

In general pushdown automata are nondeterministic meaning that in a given instantaneous description formula_43 there may be several possible steps. Any of these steps can be chosen in a computation.
With the above definition in each step always a single symbol (top of the stack) is popped, replacing it with as many symbols as necessary. As a consequence no step is defined when the stack is empty.

Computations of the pushdown automaton are sequences of steps. The computation starts in the initial state formula_44 with the initial stack symbol formula_45 on the stack, and a string formula_46 on the input tape, thus with initial description formula_47. 
There are two modes of accepting. The pushdown automaton either accepts by final state, which means after reading its input the automaton reaches an accepting state (in formula_48), or it accepts by empty stack (formula_3), which means after reading its input the automaton empties its stack. The first acceptance mode uses the internal memory (state), the second the external memory (stack).

Formally one defines

Here formula_55 represents the reflexive and transitive closure of the step relation formula_56 meaning any number of consecutive steps (zero, one or more).

For each single pushdown automaton these two languages need to have no relation: they may be equal but usually this is not the case. A specification of the automaton should also include the intended mode of acceptance. Taken over all pushdown automata both acceptance conditions define the same family of languages.

Theorem. For each pushdown automaton formula_14 one may construct a pushdown automaton formula_58 such that formula_59, and vice versa, for each pushdown automaton formula_14 one may construct a pushdown automaton formula_58 such that formula_62

The following is the formal description of the PDA which recognizes the language formula_63 by final state:
formula_64, where


The transition relation formula_8 consists of the following six instructions:

In words, the first two instructions say that in state any time the symbol is read, one is pushed onto the stack. Pushing symbol on top of another is formalized as replacing top by (and similarly for pushing symbol on top of a ).

The third and fourth instructions say that, at any moment the automaton may move from state to state .

The fifth instruction says that in state , for each symbol read, one is popped.

Finally, the sixth instruction says that the machine may move from state to accepting state only when the stack consists of a single .

There seems to be no generally used representation for PDA. Here we have depicted the instruction formula_77 by an edge from state to state labelled by formula_78 (read ; replace by formula_79).

The following illustrates how the above PDA computes on different input strings. The subscript from the step symbol formula_80 is here omitted.

Every context-free grammar can be transformed into an equivalent nondeterministic pushdown automaton. The derivation process of the grammar is simulated in a leftmost way. Where the grammar rewrites a nonterminal, the PDA takes the topmost nonterminal from its stack and replaces it by the right-hand part of a grammatical rule ("expand"). Where the grammar generates a terminal symbol, the PDA reads a symbol from input when it is the topmost symbol on the stack ("match"). In a sense the stack of the PDA contains the unprocessed data of the grammar, corresponding to a pre-order traversal of a derivation tree.

Technically, given a context-free grammar, the PDA has a single state, 1, and its transition relation is constructed as follows.


The PDA accepts by empty stack. Its initial stack symbol is the grammar's start symbol.

For a context-free grammar in Greibach normal form, defining (1,γ) ∈ δ(1,"a","A") for each grammar rule "A" → "a"γ also yields an equivalent nondeterministic pushdown automaton.

The converse, finding a grammar for a given PDA, is not that easy. The trick is to code two states of the PDA into the nonterminals of the grammar.

Theorem. For each pushdown automaton formula_14 one may construct a context-free grammar formula_86 such that formula_87.

The language of strings accepted by a deterministic pushdown automaton is called a deterministic context-free language. Not all context-free languages are deterministic. As a consequence, the DPDA is a strictly weaker variant of the PDA 

A finite automaton with access to two stacks is a more powerful device, equivalent in power to a Turing machine. A linear bounded automaton is a device which is more powerful than a pushdown automaton but less so than a Turing machine.

A GPDA is a PDA which writes an entire string of some known length to the stack or removes an entire string from the stack in one step.

A GPDA is formally defined as a 6-tuple: 
where formula_89, and are defined the same way as a PDA.
is the transition function.

Computation rules for a GPDA are the same as a PDA except that the formula_92's and formula_93's are now strings instead of symbols.

GPDA's and PDA's are equivalent in that if a language is recognized by a PDA, it is also recognized by a GPDA and vice versa.

One can formulate an analytic proof for the equivalence of GPDA's and PDA's using the following simulation:

Let formula_94 be a transition of the GPDA

where formula_95.

Construct the following transitions for the PDA:

As a generalization of pushdown automata, Ginsburg, Greibach, and Harrison (1967) investigated stack automata, which may additionally step left or right in the input string (surrounded by special endmarker symbols to prevent slipping out), and step up or down in the stack in read-only mode. 
A stack automaton is called "nonerasing" if it never pops from the stack. The class of languages accepted by nondeterministic, nonerasing stack automata is "NSPACE"("n"), which is a superset of the context-sensitive languages. The class of languages accepted by deterministic, nonerasing stack automata is "DSPACE"("n"⋅log("n")).

An alternating pushdown automaton (APDA) is a pushdown automaton with a state set


States in formula_99 and formula_100 are called "existential" resp. "universal". In an existential state an APDA nondeterministically chooses the next state and accepts if "at least one" of the resulting computations accepts. In a universal state APDA moves to all next states and accepts if "all" the resulting computations accept.

The model was introduced by Chandra, Kozen and Stockmeyer. Ladner, Lipton and Stockmeyer proved that this model is equivalent to EXPTIME i.e. a language is accepted by some APDA iff it can be decided by an exponential-time algorithm.

Aizikowitz and Kaminski introduced "synchronized alternating pushdown automata" (SAPDA) that are equivalent to conjunctive grammars in the same way as nondeterministic PDA are equivalent to context-free grammars.





</doc>
<doc id="24511" url="https://en.wikipedia.org/wiki?curid=24511" title="Protein primary structure">
Protein primary structure

Protein primary structure is the linear sequence of amino acids in a peptide or protein. By convention, the primary structure of a protein is reported starting from the amino-terminal (N) end to the carboxyl-terminal (C) end. Protein biosynthesis is most commonly performed by ribosomes in cells. Peptides can also be synthesized in the laboratory. Protein primary structures can be directly sequenced, or inferred from DNA sequences.

Amino acids are polymerised via peptide bonds to form a long backbone, with the different amino acid side chains protruding along it. In biological systems, proteins are produced during translation by a cell's ribosomes. Some organisms can also make short peptides by non-ribosomal peptide synthesis, which often use amino acids other than the standard 20, and may be cyclised, modified and cross-linked.

Peptides can be synthesised chemically via a range of laboratory methods. Chemical methods typically synthesise peptides in the opposite order to biological protein synthesis (starting at the C-terminus).

Protein sequence is typically notated as a string of letters, listing the amino acids starting at the amino-terminal end through to the carboxyl-terminal end. Either a three letter code or single letter code can be used to represent the 20 naturally occurring amino acids, as well as mixtures or ambiguous amino acids (similar to nucleic acid notation).

Peptides can be directly sequenced, or inferred from DNA sequences. Large sequence databases now exist that collate known protein sequences.

In general, polypeptides are unbranched polymers, so their primary structure can often be specified by the sequence of amino acids along their backbone. However, proteins can become cross-linked, most commonly by disulfide bonds, and the primary structure also requires specifying the cross-linking atoms, e.g., specifying the cysteines involved in the protein's disulfide bonds. Other crosslinks include desmosine.

The chiral centers of a polypeptide chain can undergo racemization. Although it does not change the sequence, it does affect the chemical properties of the sequence. In particular, the -amino acids normally found in proteins can spontaneously isomerize at the formula_1 atom to form -amino acids, which cannot be cleaved by most proteases. Additionally, proline can form stable trans-isomers at the peptide bond.

Finally, the protein can undergo a variety of posttranslational modifications, which are briefly summarized here.

The N-terminal amino group of a polypeptide can be modified covalently, e.g.,




The C-terminal carboxylate group of a polypeptide can also be modified, e.g.,


Finally, the peptide side chains can also be modified covalently, e.g.,












Most of the polypeptide modifications listed above occur "post-translationally", i.e., after the protein has been synthesized on the ribosome, typically occurring in the endoplasmic reticulum, a subcellular organelle of the eukaryotic cell.

Many other chemical reactions (e.g., cyanylation) have been applied to proteins by chemists, although they are not found in biological systems.

In addition to those listed above, the most important modification of primary structure is peptide cleavage (by chemical hydrolysis or by proteases). Proteins are often synthesized in an inactive precursor form; typically, an N-terminal or C-terminal segment blocks the active site of the protein, inhibiting its function. The protein is activated by cleaving off the inhibitory peptide.

Some proteins even have the power to cleave themselves. Typically, the hydroxyl group of a serine (rarely, threonine) or the thiol group of a cysteine residue will attack the carbonyl carbon of the preceding peptide bond, forming a tetrahedrally bonded intermediate [classified as a hydroxyoxazolidine (Ser/Thr) or hydroxythiazolidine (Cys) intermediate]. This intermediate tends to revert to the amide form, expelling the attacking group, since the amide form is usually favored by free energy, (presumably due to the strong resonance stabilization of the peptide group). However, additional molecular interactions may render the amide form less stable; the amino group is expelled instead, resulting in an ester (Ser/Thr) or thioester (Cys) bond in place of the peptide bond. This chemical reaction is called an N-O acyl shift.

The ester/thioester bond can be resolved in several ways:


The proposal that proteins were linear chains of α-amino acids was made nearly simultaneously by two scientists at the same conference in 1902, the 74th meeting of the Society of German Scientists and Physicians, held in Karlsbad. Franz Hofmeister made the proposal in the morning, based on his observations of the biuret reaction in proteins. Hofmeister was followed a few hours later by Emil Fischer, who had amassed a wealth of chemical details supporting the peptide-bond model. For completeness, the proposal that proteins contained amide linkages was made as early as 1882 by the French chemist E. Grimaux.

Despite these data and later evidence that proteolytically digested proteins yielded only oligopeptides, the idea that proteins were linear, unbranched polymers of amino acids was not accepted immediately. Some well-respected scientists such as William Astbury doubted that covalent bonds were strong enough to hold such long molecules together; they feared that thermal agitations would shake such long molecules asunder. Hermann Staudinger faced similar prejudices in the 1920s when he argued that rubber was composed of macromolecules.

Thus, several alternative hypotheses arose. The colloidal protein hypothesis stated that proteins were colloidal assemblies of smaller molecules. This hypothesis was disproved in the 1920s by ultracentrifugation measurements by Theodor Svedberg that showed that proteins had a well-defined, reproducible molecular weight and by electrophoretic measurements by Arne Tiselius that indicated that proteins were single molecules. A second hypothesis, the cyclol hypothesis advanced by Dorothy Wrinch, proposed that the linear polypeptide underwent a chemical cyclol rearrangement C=O + HN formula_8 C(OH)-N that crosslinked its backbone amide groups, forming a two-dimensional "fabric". Other primary structures of proteins were proposed by various researchers, such as the diketopiperazine model of Emil Abderhalden and the pyrrol/piperidine model of Troensegaard in 1942. Although never given much credence, these alternative models were finally disproved when Frederick Sanger successfully sequenced insulin and by the crystallographic determination of myoglobin and hemoglobin by Max Perutz and John Kendrew.

Any linear-chain heteropolymer can be said to have a "primary structure" by analogy to the usage of the term for proteins, but this usage is rare compared to the extremely common usage in reference to proteins. In RNA, which also has extensive secondary structure, the linear chain of bases is generally just referred to as the "sequence" as it is in DNA (which usually forms a linear double helix with little secondary structure). Other biological polymers such as polysaccharides can also be considered to have a primary structure, although the usage is not standard.

The primary structure of a biological polymer to a large extent determines the three-dimensional shape (tertiary structure). Protein sequence can be used to predict local features, such as segments of secondary structure, or trans-membrane regions. However, the complexity of protein folding currently prohibits predicting the tertiary structure of a protein from its sequence alone. Knowing the structure of a similar homologous sequence (for example a member of the same protein family) allows highly accurate prediction of the tertiary structure by homology modeling. If the full-length protein sequence is available, it is possible to estimate its general biophysical properties, such as its isoelectric point.

Sequence families are often determined by sequence clustering, and structural genomics projects aim to produce a set of representative structures to cover the sequence space of possible non-redundant sequences.



</doc>
<doc id="24512" url="https://en.wikipedia.org/wiki?curid=24512" title="Peter principle">
Peter principle

The Peter principle is a concept in management developed by Laurence J. Peter, which observes that people in a hierarchy tend to rise to their "level of incompetence". In other words, an employee is promoted based on their success in previous jobs until they reach a level at which they are no longer competent, as skills in one job do not necessarily translate to another. The concept was elucidated in the 1969 book The Peter Principle by Peter and Raymond Hull.

"The Peter Principle" was published by William Morrow and Company in 1969. Peter and Hull intended the book to be satire, but it became popular as it was seen to make a serious point about the shortcomings of how people are promoted within hierarchical organizations. Hull wrote the text, based on Peter's research.

The Peter principle has been the subject of much later commentary and research.

The Peter principle states that a person who is competent at their job will earn promotion to a more senior position which requires different skills. If the promoted person lacks the skills required for their new role, then they will be incompetent at their new level, and so they will not be promoted again. But if they are competent at their new role, then they will be promoted again, and they will continue to be promoted until they eventually reach a level at which they are incompetent. Being incompetent, they do not qualify to be promoted again, and so remain stuck at that final level for the rest of their career (termed "Final Placement" or "Peter's Plateau"). This outcome is inevitable, given enough time and assuming that there are enough positions in the hierarchy to promote competent employees to. The "Peter Principle" is therefore expressed as: ""In a hierarchy every employee tends to rise to his level of incompetence."" This leads to Peter's Corollary: "In time, every post tends to be occupied by an employee who is incompetent to carry out its duties." Hull calls the study of how hierarchies work "hierarchiology."

Laurence J. Peter had conducted the research that led to the formulation of the Peter principle well before publishing his findings. He worked with Raymond Hull on a book that elucidated his observations about hierarchies. The principle is named for Peter because although Hull actually wrote the book, it is a summary of Peter's research. "The Peter Principle" was published by William Morrow and Company in 1969.

In chapters 1 and 2, Peter and Hull give various examples of the Peter principle in action. A competent mechanic may make an incompetent foreman; a competent school teacher may make a competent assistant principal, but then go on to be an incompetent principal, and therefore will not be considered for promotion to assistant superintendent; a military officer may be promoted all the way up through the ranks to general and still be competent at that rank, but then make an incompetent field marshal. In each case, the higher position required skills which were not required at the level immediately below. The mechanic only had to know how to fix cars, but as a foreman he needed to be able to manage the other mechanics and deal with customers. The teacher was competent at educating children, and as assistant principal he was good at dealing with parents and other teachers, but as principal he was poor at maintaining good relations with the school board and the superintendent. The general was capable at dealing with ordinary soldiers, but as a field marshal he did not know how to liaise with politicians and the field marshals of his country's allies. They conclude that "this could happen to every employee in every hierarchy."

In chapter 3, Peter and Hull discuss apparent exceptions to this principle and then debunk them. One of these illusory exceptions in when someone who is incompetent is still promoted anyway. This is known as ""percussive sublimation"" (i.e. being "kicked upstairs"). But it is only a pseudo-promotion: a move from one unproductive position to another; whereas a true promotion is a move from a position of competence (either to a position of competence or to a position of incompetence). This improves staff morale, as other employees believe that they too can be promoted again. Another pseudo-promotion is the ""lateral arabesque"", when a person is moved out of the way and given a longer job title.

Competence is measured by the employer, not by the customers or anyone else outside the hierarchy. "Competence, like truth, beauty and contact lenses, is in the eye of the beholder." So people who "appear" to outsiders to be incompetent because they follow the rules of the organisation to an extent which actually impedes their effectiveness are still deemed competent by their immediate superior, because ""internal consistency is valued more highly than efficient service"". This is dubbed "Peter's Inversion," because the "means" have become more important than the "ends". Those who value the means more than the ends are called "Peter's Inverts."

While incompetence is merely a barrier to further promotion, "super-incompetence" is grounds for dismissal. So is super-competence. The competence of employees at a given level can be represented by a bell curve: the majority are either competent or incompetent and so can remain employed, but a small number of outliers are either super-incompetent or super-competent, and in both cases "they tend to disrupt the hierarchy." They are therefore expelled in order to preserve the hierarchy; a process called ""hierarchical exfoliation"". One example of a super-competent employee is a teacher of children with special needs who was so effective at educating them that after a year they exceeded all expectations at reading and arithmetic, but the teacher was still fired because he had neglected to devote enough time to bead-stringing and finger-painting.

Chapters 4 and 5 deal with the methods of achieving promotion: "Push" and "Pull." Push means the employee's own efforts, such as working hard and taking self-improvement courses. This is usually not very effective, because of the Seniority Factor: the next level up is often fully occupied, blocking the path to promotion. (Murdering one's immediate superiors can be an effective way of overcoming this obstacle, but as this is such a rare phenomenon it does not really affect Peter and Hull's assessment of Push.) Pull is far more effective, and refers to accelerated promotion brought about by the efforts of an employee's mentors or patrons. It is better to have as many patrons as possible, because each additional patron produces a multiplying effect on their combined effectiveness, as patrons reinforce their positive opinion of the employee by discussing him with each other ("Hull's Theorem").

Chapter 6 explains why "good followers do not become good leaders." In chapter 7, Peter and Hull describe the effect of the Peter Principle in politics and government. Chapter 8, entitled "Hints and Foreshadowings", discusses the work of earlier writers on the subject of incompetence, such as Sigmund Freud, Karl Marx and Alexander Pope.

Chapter 9 explains that once employees have reached their level of incompetence, they always lack insight into their situation. Most don't realise that they are incompetent, but those who do recognise their own incompetence still never realise that it is because they have been promoted, and so they futilely search for other explanations instead. In this chapter, Peter and Hull go on to explain why aptitude tests don't work and are actually counter-productive. Finally, they describe "Summit Competence": when someone reaches the highest level in their organisation and yet is still competent at that level. This is only because there were not enough ranks in the hierarchy, or because they did not have time to reach a level of incompetence. Such people often seek a level of incompetence in another hierarchy. For example, Socrates was an outstanding teacher but a terrible defence attorney. This is known as "Compulsive Incompetence."

Chapter 10 explains why trying to assist an incompetent employee by promoting another employee to act as his assistant doesn't work. ""Incompetence plus incompetence equals incompetence.""

Chapters 11 and 12 describe the various medical and psychological manifestations of stress which may result when someone reaches his level of incompetence, as well as other symptoms such as certain characteristic habits of speech or behaviour.

Chapter 13 considers whether it is possible for an employee who has reached his level of incompetence to be happy and healthy once he gets there. The answer is no, if he realises his true situation, and yes if he does not. Those who realise that they are incompetent usually think (mistakenly) that this is only because they are not working hard enough, and so they work themselves harder until they burn out or damage their health. So facing the sordid truth is not recommended. Those who have not realised that they are at their level of incompetence remain happy and healthy because they substitute irrelevant duties for the proper duties of their post, and excel at those instead. Peter and Hull describe six different Substitution techniques.

In chapter 14 various ways of avoiding promotion to the final level are described. Attempting to refuse an offered promotion is ill-advised, and is only practicable if the employee is not married and has no-one else to answer to. Generally it is better to avoid being considered for promotion in the first place, by pretending to be incompetent while one is actually still employed at a level of competence. This is "Creative Incompetence," and several examples of successful techniques are given. It works best if the chosen field of incompetence does not actually impair one's work.

The concluding chapter applies Peter's Principle to the entire human species at an evolutionary level, and asks whether humanity can survive in the long run, or will become extinct upon reaching its level of incompetence as technology advances.

Other commenters made observations similar to the Peter principle long before Peter's research. Gotthold Ephraim Lessing's 1763 play "Minna von Barnhelm" features an army sergeant who shuns the opportunity to move up in the ranks, saying "I am a good sergeant; I might easily make a bad captain, and certainly an even worse general. One knows from experience." Similarly, Carl von Clausewitz (1780–1831) wrote that "there is nothing more common than to hear of men losing their energy on being raised to a higher position, to which they do not feel themselves equal." Closely echoing Peter, Spanish philosopher José Ortega y Gasset (1883–1955) wrote, "All public employees should be demoted to their immediately lower level, as they have been promoted until turning incompetent."

A number of scholars have engaged in research interpreting the Peter principle and its effects. In 2000, Edward Lazear explored two possible explanations for the phenomenon. First is the idea that employees work harder to gain a promotion, and then slack off once it is achieved. The other is that it is a statistical process: workers who are promoted have passed a particular benchmark of productivity based on factors that cannot necessarily be replicated in their new role, leading to a Peter principle situation. Lazear concluded that the former explanation only occurs under particular compensation structures, whereas the latter always holds up.

Alessandro Pluchino, Andrea Rapisarda, and Cesare Garofalo used an agent-based modelling approach to simulate the promotion of employees in a system where the Peter principle is assumed to be true. They found that the best way to improve efficiency in an enterprise is to promote people randomly, or to shortlist the best and the worst performer in a given group, from which the person to be promoted is then selected randomly. For this work, they won the 2010 edition of the parody Ig Nobel Prize in management science.

In 2018, professors Alan Benson, Danielle Li, and Kelly Shue analyzed sales workers' performance and promotion practices at 214 American businesses to test the veracity of the Peter principle. They found that these companies tended to promote employees to management position based on their performance in their previous position, rather than based on managerial potential. Consistent with the Peter principle, the researchers found that high performing sales employees were likelier to be promoted, and that they were likelier to perform poorly as managers, leading to considerable costs to the businesses.

Companies and organizations shaped their policies to contend with the Peter principle. Lazear stated that some companies expect that productivity will "regress to the mean" following promotion in their hiring and promotion practices. Other companies have adopted the Cravath System, in which employees who do not advance are periodically fired. The Cravath System was developed at the law firm Cravath, Swaine & Moore, which made a practice of hiring chiefly recent law graduates, promoting internally and firing employees who do not perform at the required level. Brian Christian and Tom Griffiths have suggested the additive increase/multiplicative decrease algorithm as a solution to the Peter principle less severe than firing employees who fail to advance. They propose a dynamic hierarchy in which employees are regularly either promoted or reassigned to a lower level, so that any worker who is promoted to their point of failure is soon moved to an area where they are productive.




</doc>
<doc id="24513" url="https://en.wikipedia.org/wiki?curid=24513" title="Platonic realism">
Platonic realism

Platonic realism is a philosophical term usually used to refer to the idea of realism regarding the existence of universals or abstract objects after the Greek philosopher Plato (c. 427–c. 347 BC), a student of Socrates. As universals were considered by Plato to be ideal forms, this stance is ambiguously also called Platonic idealism. This should not be confused with idealism as presented by philosophers such as George Berkeley: as Platonic abstractions are not spatial, temporal, or mental, they are not compatible with the later idealism's emphasis on mental existence. Plato's Forms include numbers and geometrical figures, making them a theory of mathematical realism; they also include the Form of the Good, making them in addition a theory of ethical realism.

Plato expounded his own articulation of realism regarding the existence of universals in his dialogue "The Republic" and elsewhere, notably in the "Phaedo", the "Phaedrus", the "Meno" and the "Parmenides".

In Platonic realism, "universals" do not exist in the way that ordinary physical objects exist, even though Plato metaphorically referred to such objects in order to explain his concepts. More modern versions of the theory seek to avoid applying potentially misleading descriptions to universals. Instead, such versions maintain that it is meaningless (or a category mistake) to apply the categories of space and time to "universals".

Regardless of their description, Platonic realism holds that "universals" do exist in a broad, abstract sense, although not at any spatial or temporal distance from people's bodies. Thus, people cannot see or otherwise come into sensory contact with universals, but in order to conceive of universals, one must be able to conceive of these abstract forms.

Theories of universals, including Platonic realism, are challenged to satisfy certain constraints on theories of "universals".

Platonic realism satisfies one of those constraints, in that it is a theory of what general terms refer to. "Forms" are ideal in supplying meaning to referents for general terms. That is, to understand terms such as "wikt:Applehood" and "redness", Platonic realism says that they refer to forms. Indeed, Platonism gets much of its plausibility because mentioning "redness", for example, could be assumed to be referring to something that is apart from space and time, but which has lots of specific instances.

Some contemporary linguistic philosophers construe "Platonism" to mean the proposition that universals exist independently of particulars (a universal is anything that can be predicated of a particular). Similarly, a form of modern Platonism is found in the predominant philosophy of mathematics, especially regarding the foundations of mathematics. The Platonic interpretation of this philosophy includes the thesis that mathematics is discovered rather than created.

Plato's interpretation of universals is linked to his "Theory of Forms" in which he uses both the terms ("eidos": "form") and ("idea": "characteristic") to describe his theory. Forms are mind independent abstract objects or paradigms (παραδείγματα: "patterns in nature") of which particular objects and the properties and relations present in them are copies. Form is inherent in the particulars and these are said to "participate in" the form. Classically "idea" has been translated (or transliterated) as "idea," but secondary literature now typically employs the term "form" (or occasionally "kind," usually in discussion of Plato's "Sophist" and "Statesman") to avoid confusion with the English word connoting "thought".

Platonic form can be illustrated by contrasting a material triangle with an ideal triangle. The Platonic form is the ideal triangle — a figure with perfectly drawn lines whose angles add to 180 degrees. Any form of triangle that we experience will be an imperfect representation of the ideal triangle. Regardless of how precise your measuring and drawing tools you will never be able to recreate this perfect shape. Even drawn to the point where our senses cannot perceive a defect, in its essence the shape will still be imperfect; forever unable to match the ideal triangle.

Some versions of Platonic realism, like that of Proclus, regard Plato's forms as thoughts in the mind of God. Most consider forms not to be mental entities at all.

In Platonic realism, forms are related to "particulars" (instances of objects and properties) in that a particular is regarded as a copy of its form. For example, a particular apple is said to be a copy of the form of "applehood" and the apple's redness is an instance of the form of "Redness". "Participation" is another relationship between forms and particulars. Particulars are said to "participate" in the forms, and the forms are said to "inhere" in the particulars.

According to Plato, there are some forms that are not instantiated at all, but, he contends, that does not imply that the forms "could not" be instantiated. Forms are capable of being instantiated by many different particulars, which would result in the forms' having many copies, or inhering many particulars.

Two main criticisms with Platonic realism relate to inherence and difficulty of creating concepts without sense perception. Despite these criticisms, realism has strong defenders. Its popularity through the centuries has been variable.

Critics claim that the terms "instantiation" and "copy" are not further defined and that "participation" and "inherence" are similarly mysterious and unenlightening.
They question what it means to say that the form of applehood "inheres" a particular apple or that the apple is a "copy" of the form of applehood. To the critic, it seems that the forms, not being spatial, cannot have a shape, so it cannot be that the apple "is the same shape as" the form. Likewise, the critic claims it is unclear what it means to say that an apple "participates" in "applehood".

Arguments refuting the inherence criticism, however, claim that a form of something spatial can lack a concrete (spatial) location and yet have "in abstracto" spatial qualities. An apple, then, can have the same shape as its form. Such arguments typically claim that the relationship between a particular and its form is very intelligible and easily grasped; that people unproblematically apply Platonic theory in everyday life; and that the inherence criticism is only created by the artificial demand to explain the normal understanding of inherence as if it were highly problematic. That is, the supporting argument claims that the criticism is with the mere illusion of a problem and thus could render suspect any philosophical concept.

A criticism of forms relates to the origin of concepts without the benefit of sense-perception. For example, to think of redness in general, according to Plato, is to think of the form of redness. Critics, however, question how one can have the concept of a form existing in a special realm of the universe, apart from space and time, since such a concept cannot come from sense-perception. Although one can see an apple and its redness, the critic argues, those things merely participate in, or are copies of, the forms. Thus, they claim, to conceive of a particular apple and its redness is not to conceive of "applehood" or "redness-in-general", so they question the source of the concept.

Plato's doctrine of recollection, however, addresses such criticism by saying that souls are "born" with the concepts of the forms, and just have to be "reminded" of those concepts from back before birth, when the souls were in close contact with the forms in the Platonic heaven. Plato is thus known as one of the very first rationalists, believing as he did that humans are born with a fund of "a priori" knowledge, to which they have access through a process of reason or intellection — a process that critics find to be rather mysterious.

A more modern response to this criticism of "concepts without sense-perception" is the claim that the universality of its qualities is an unavoidable given because one only experiences an object by means of general concepts. So, since the critic already grasps the relation between the abstract and the concrete, he is invited to stop thinking that it implies a contradiction. The response reconciles Platonism with empiricism by contending that an abstract (i.e., not concrete) object is "real" and knowable by its instantiation. Since the critic has, after all, naturally understood the abstract, the response suggests merely to abandon prejudice and accept it.





</doc>
<doc id="24514" url="https://en.wikipedia.org/wiki?curid=24514" title="Psychosis">
Psychosis

Psychosis is an abnormal condition of the mind that results in difficulties determining what is real and what is not. Symptoms may include false beliefs and seeing or hearing things that others do not see or hear. Other symptoms may include incoherent speech and behavior that is inappropriate for the situation. There may also be sleep problems, social withdrawal, lack of motivation, and difficulties carrying out daily activities.
Psychosis has many different causes. These include mental illness, such as schizophrenia or bipolar disorder, sleep deprivation, some medical conditions, certain medications, and drugs such as alcohol or cannabis. One type, known as postpartum psychosis, can occur after childbirth. The neurotransmitter dopamine is believed to play a role. Acute psychosis is considered primary if it results from a psychiatric condition and secondary if it is caused by a medical condition. The diagnosis of a mental illness requires excluding other potential causes. Testing may be done to check for central nervous system diseases, toxins, or other health problems as a cause.
Treatment may include antipsychotic medication, counselling, and social support. Early treatment appears to improve outcomes. Medications appear to have a moderate effect. Outcomes depend on the underlying cause. In the United States about 3% of people develop psychosis at some point in time. The condition has been described since at least the 4th century BC by Hippocrates and possibly as early as 1,500 BC in the Egyptian Ebers Papyrus.

A hallucination is defined as sensory perception in the absence of external stimuli. Hallucinations are different from illusions, or perceptual distortions, which are the misperception of external stimuli. Hallucinations may occur in any of the senses and take on almost any form, which may include simple sensations (such as lights, colors, tastes, and smells) to experiences such as seeing and interacting with fully formed animals and people, hearing voices, and having complex tactile sensations. Hallucinations are generally characterized as being vivid, and uncontrollable.

Auditory hallucinations, particularly experiences of hearing voices, are the most common and often prominent feature of psychosis. 
Up to 15% of the general population may experience auditory hallucinations. The prevalence in schizophrenia is generally put around 70%, but may go as high as 98%. During the early 20th century auditory hallucinations were second to visual hallucinations in frequency, but they are now the most common manifestation of schizophrenia, although rates vary throughout cultures and regions. Auditory hallucinations are most commonly intelligible voices. When voices are present, the average number has been estimated at three. Content, like frequency, differs significantly, especially across cultures and demographics. People who experience auditory hallucinations can frequently identify the loudness, location of origin, and may settle on identities for voices. Western cultures are associated with auditory experiences concerning religious content, frequently related to sin. Hallucinations may command a person to do something, which may be dangerous when combined with delusions.

Extracampine hallucinations are auditory hallucinations originating from a particular body part (e.g. a voice coming from a person's knee).

Visual hallucinations occur in roughly a third of people with schizophrenia, although rates as high as 55% are reported. Content frequently involves animate objects, although perceptual abnormalities such as changes in lighting, shading, streaks, or lines may be seen. Visual abnormalities may conflict with proprioceptive information, and visions may include experiences such as the ground tilting. Lilliputian hallucinations are less common in schizophrenia, and occur more frequently in various types of encephalopathy (e.g. Peduncular hallucinosis).

A visceral hallucination, also called a cenesthetic hallucination, is characterized by visceral sensations in the absence of stimuli. Cenesthetic hallucinations may include sensations of burning, or re-arrangement of internal organs.

Psychosis may involve delusional beliefs. Delusions are strong beliefs against the reality, or held despite contradictory evidence. Delusions are necessarily incongruent with societal norms, and some beliefs may constitute a delusion in certain cultures where they impact functioning, while they may be a perfectly normal belief in others. Delusional thinking is relatively common in the general population with around a quarter of people believing they have special power, and a third believing in telepathy. The distinguishing feature between delusional thinking and full blown delusions is the degree with which they impact functioning. Multiple themes are common in delusions, although cultural norms are highly influential (e.g. religious content differing significantly across countries). The most common type of delusion are persecutory delusions, where a person believes that an individual, organization or group is attempting to harm them. Other delusions include delusions of reference (beliefs that a particular stimulus has a special meaning that is directed at the holder of belief), grandiose delusions (delusions that a person has a special power or importance), thought broadcasting (the belief that ones thoughts are audible) and thought insertion (the belief that one's thoughts are not one's own). The DSM-5 characterizes certain delusions as "bizarre" if they are clearly implausible, or are incompatible within the cultural context. The concept of bizarre delusions has been criticized as excessively subjective.

Historically, Karl Jaspers has classified psychotic delusions into "primary" and "secondary" types. Primary delusions are defined as arising suddenly and not being comprehensible in terms of normal mental processes, whereas secondary delusions are typically understood as being influenced by the person's background or current situation (e.g., ethnicity; also religious, superstitious, or political beliefs).

Disorganization is split into disorganized speech or thinking, and grossly disorganized motor behavior. Disorganized speech, also called formal thought disorder, is disorganization of thinking that is inferred from speech. Characteristics of disorganized speech include rapidly switching topics, called derailment or loose association; switching to topics that are unrelated, called tangental thinking; incomprehensible speech, called word salad or incoherence. Disorganized motor behavior includes repetitive, odd, or sometimes purposeless movement. Disorganized motor behavior rarely includes catatonia, and although it was a historically prominent symptom, it is rarely seen today. Whether this is due to historically used treatments or the lack thereof is unknown.

Catatonia describes a profoundly agitated state in which the experience of reality is generally considered impaired. There are two primary manifestations of catatonic behavior. The classic presentation is a person who does not move or interact with the world in any way while awake. This type of catatonia presents with waxy flexibility. Waxy flexibility is when someone physically moves part of a catatonic person's body and the person stays in the position even if it is bizarre and otherwise nonfunctional (such as moving a person's arm straight up in the air and the arm staying there).

The other type of catatonia is more of an outward presentation of the profoundly agitated state described above. It involves excessive and purposeless motor behaviour, as well as extreme mental preoccupation that prevents an intact experience of reality. An example is someone walking very fast in circles to the exclusion of anything else with a level of mental preoccupation (meaning not focused on anything relevant to the situation) that was not typical of the person prior to the symptom onset. In both types of catatonia there is generally no reaction to anything that happens outside of them. It is important to distinguish catatonic agitation from severe bipolar mania, although someone could have both.

Negative symptoms include reduced emotional expression, decreased motivation, and reduced spontaneous speech.

Brief hallucinations are not uncommon in those without any psychiatric disease. Causes or triggers include:

There is evidence that trauma during childhood increases the risk of developing psychosis. About 65% of those who experience psychosis have experienced childhood trauma (abuse and neglect). The relationship appears to be a causal and cumulative one, meaning the more adverse childhood events experienced the greater the likelihood of developing psychosis of one kind or another.

From a diagnostic standpoint, organic disorders were believed to be caused by physical illness affecting the brain (that is, psychiatric disorders secondary to other conditions), while functional disorders were considered disorders of the functioning of the mind in the absence of physical disorders (that is, primary psychological or psychiatric disorders). Subtle physical abnormalities have been found in illnesses traditionally considered functional, such as schizophrenia. The DSM-IV-TR avoids the functional/organic distinction, and instead lists traditional psychotic illnesses, psychosis due to general medical conditions, and substance-induced psychosis.

Primary psychiatric causes of psychosis include the following:

Psychotic symptoms may also be seen in:

Stress is known to contribute to and trigger psychotic states. A history of psychologically traumatic events, and the recent experience of a stressful event, can both contribute to the development of psychosis. Short-lived psychosis triggered by stress is known as brief reactive psychosis, and patients may spontaneously recover normal functioning within two weeks. In some rare cases, individuals may remain in a state of full-blown psychosis for many years, or perhaps have attenuated psychotic symptoms (such as low intensity hallucinations) present at most times.

Neuroticism is an independent predictor of the development of psychosis.

Subtypes of psychosis include:

Cycloid psychosis is a psychosis that progresses from normal to full-blown, usually between a few hours to days, not related to drug intake or brain injury. The cycloid psychosis has a long history in European psychiatry diagnosis. The term "cycloid psychosis" was first used by Karl Kleist 1926. Despite the significant clinical relevance, this diagnosis is neglected both in literature as in nosology. The cycloid psychosis has attracted much interest in the international literature of the past 50 years, but the number of scientific studies have greatly decreased over the past 15 years, possibly partly explained by the misconception that the diagnosis has been incorporated in current diagnostic classification systems. The cycloid psychosis is therefore only partially described in the diagnostic classification systems used. Cycloid psychosis is nevertheless its own specific disease that is distinct from both the manic-depressive disorder, and from schizophrenia, and this despite the fact that the cycloid psychosis can include both bipolar (basic mood shifts) as well as schizophrenic symptoms. The disease is an acute, usually self-limiting, functionally psychotic state, with a very diverse clinical picture that almost consistently is characterized by the existence of some degree of confusion or distressing perplexity, but above all, of the multifaceted and diverse expressions the disease takes. The main features of the disease is thus that the onset is acute, the multifaceted picture of symptoms and typically reverses to a normal state and that the long-term prognosis is good. In addition, diagnostic criteria include at least four of the following symptoms:

Cycloid psychosis occurs in people of generally 15–50 years of age.

A very large number of medical conditions can cause psychosis, sometimes called "secondary psychosis". Examples include:

Various psychoactive substances (both legal and illegal) have been implicated in causing, exacerbating, or precipitating psychotic states or disorders in users, with varying levels of evidence. This may be upon intoxication, for a more prolonged period after use, or upon withdrawal. Individuals who have a substance induced psychosis tend to have a greater awareness of their psychosis and tend to have higher levels of suicidal thinking compared to individuals who have a primary psychotic illness. Drugs commonly alleged to induce psychotic symptoms include alcohol, cannabis, cocaine, amphetamines, cathinones, psychedelic drugs (such as LSD and psilocybin), κ-opioid receptor agonists (such as enadoline and salvinorin A) and NMDA receptor antagonists (such as phencyclidine and ketamine).

Approximately three percent of people who are suffering from alcoholism experience psychosis during acute intoxication or withdrawal. Alcohol related psychosis may manifest itself through a kindling mechanism. The mechanism of alcohol-related psychosis is due to the long-term effects of alcohol resulting in distortions to neuronal membranes, gene expression, as well as thiamin deficiency. It is possible in some cases that alcohol abuse via a kindling mechanism can cause the development of a chronic substance induced psychotic disorder, i.e. schizophrenia. The effects of an alcohol-related psychosis include an increased risk of depression and suicide as well as causing psychosocial impairments.

According to some studies, the more often cannabis is used the more likely a person is to develop a psychotic illness, with frequent use being correlated with twice the risk of psychosis and schizophrenia. While cannabis use is accepted as a contributory cause of schizophrenia by some, it remains controversial, with pre-existing vulnerability to psychosis emerging as the key factor that influences the link between cannabis use and psychosis. Some studies indicate that the effects of two active compounds in cannabis, tetrahydrocannabinol (THC) and cannabidiol (CBD), have opposite effects with respect to psychosis. While THC can induce psychotic symptoms in healthy individuals, CBD may reduce the symptoms caused by cannabis.

Cannabis use has increased dramatically over the past few decades whereas the rate of psychosis has not increased. Together, these findings suggest that cannabis use may hasten the onset of psychosis in those who may already be predisposed to psychosis. High-potency cannabis use indeed seems to accelerate the onset of psychosis in predisposed patients. A 2012 study concluded that cannabis plays an important role in the development of psychosis in vulnerable individuals, and that cannabis use in early adolescence should be discouraged.

Methamphetamine induces a psychosis in 26–46 percent of heavy users. Some of these people develop a long-lasting psychosis that can persist for longer than six months. Those who have had a short-lived psychosis from methamphetamine can have a relapse of the methamphetamine psychosis years later after a stress event such as severe insomnia or a period of heavy alcohol abuse despite not relapsing back to methamphetamine. Individuals who have long history of methamphetamine abuse and who have experienced psychosis in the past from methamphetamine abuse are highly likely to rapidly relapse back into a methamphetamine psychosis within a week or so of going back onto methamphetamine.

Administration, or sometimes withdrawal, of a large number of medications may provoke psychotic symptoms. Drugs that can induce psychosis experimentally or in a significant proportion of people include amphetamine and other sympathomimetics, dopamine agonists, ketamine, corticosteroids (often with mood changes in addition), and some anticonvulsants such as vigabatrin. Stimulants that may cause this include lisdexamfetamine.

Meditation may induce psychological side effects, including depersonalization, derealization and psychotic symptoms like hallucinations as well as mood disturbances.

The first brain image of an individual with psychosis was completed as far back as 1935 using a technique called pneumoencephalography (a painful and now obsolete procedure where cerebrospinal fluid is drained from around the brain and replaced with air to allow the structure of the brain to show up more clearly on an X-ray picture).

Both first episode psychosis, and high risk status is associated with reductions in grey matter volume. First episode psychotic and high risk populations are associated with similar but distinct abnormalities in GMV. Reductions in the right middle temporal gyrus, right superior temporal gyrus, right parahippocampus, right hippocampus, right middle frontal gyrus, and left anterior cingulate cortex are observed in high risk populations. Reductions in first episode psychosis span a region from the right STG to the right insula, left insula, and cerebellum, and are more severe in the right ACC, right STG, insula and cerebellum. Another meta analysis reported similar reductions in temporal, medial frontal, and insular regions, but also reported increased GMV in the right lingual gyrus and left precentral gyrus. The Kraeplinian dichotomy is made questionable by grey matter abnormalities in bipolar and schizophrenia; schizophrenia is distinguishable from bipolar in that regions of grey matter reduction are generally larger in magnitude, although adjusting for gender differences reduces the difference to the left dorsomedial prefrontal cortex, and right dorsolateral prefrontal cortex.

During attentional tasks, first episode psychosis is associated with hypoactivation in the right middle frontal gyrus, a region generally described as encompassing the dorsolateral prefrontal cortex (dlPFC). In congruence with studies on grey matter volume, hypoactivity in the right insula, and right inferior parietal lobe is also reported. With the exceptions of reduced deactivation of the inferior frontal gyrus during cognitive tasks(i.e. hyperactivation), highly consistent and replicable hypoactivity in the right insula, dACC, and precuneus, as well as hyperactivity in the right basal ganglia and thalamus is observed. Decreased grey matter volume in conjunction with hypoactivity is observed in the dorsal ACC, right anterior/middle insula, and left middle insula. Decreased grey matter volume and hyperactivity is reported in the ventral ACC(i.e. the pgACC and sgACC), and more posterior regions of the insula.

Studies during acute experience of hallucinations demonstrate increased activity in primary or secondary sensory cortices. As auditory hallucinations are most common in psychosis, most robust evidence exists for increased activity in the left middle temporal gyrus, left superior temporal gyrus, and left inferior frontal gyrus (i.e. Broca's area). Activity in the ventral striatum, hippocampus, and ACC are related to the lucidity of hallucinations, and indicate that activation or involvement of emotional circuitry are key to the impact of abnormal activity in sensory cortices. Together, these findings indicate abnormal processing of internally generated sensory experiences, coupled with abnormal emotional processing, results in hallucinations. One proposed model involves a failure of feedforward networks from sensory cortices to the inferior frontal cortex, which normal cancel out sensory cortex activity during internally generated speech. The resulting disruption in expected and perceived speech is thought to produce lucid hallucinatory experiences.

The two factor model of delusions posits that dysfunction in both belief formation systems and belief evaluation systems are necessary for delusions. Dysfunction in evaluations systems localized to the right lateral prefrontal cortex, regardless of delusion content, is supported by neuroimaging studies and is congruent with its role in conflict monitoring in healthy persons. Abnormal activation and reduced volume is seen in people with delusions, as well as in disorders associated with delusions such as frontotemporal dementia, psychosis and Lewy body dementia. Furthermore, lesions to this region are associated with "jumping to conclusions", damage to this region is associated with post-stroke delusions, and hypometabolism this region associated with caudate strokes presenting with delusions.

The aberrant salience model suggests that delusions are a result of people assigning excessive importance to irrelevant stimuli. In support of this hypothesis, regions normally associated with the salience network demonstrate reduced grey matter in people with delusions, and the neurotransmitter dopamine, which is widely implicated in salience processing, is also widely implicated in psychotic disorders.

Specific regions have been associated with specific types of delusions. The volume of the hippocampus and parahippocampus is related to paranoid delusions in Alzheimer's disease, and has been reported to be abnormal post mortem in one person with delusions. Capragas delusions have been associated with occipito-temporal damage, and may be related to failure to elicit normal emotions or memories in response to faces.

Psychosis is associated with ventral striatal hypoactivity during reward anticipation and feedback. Hypoactivity in the left ventral striatum is correlated with the severity of negative symptoms. While anhedonia is a commonly reported symptom in psychosis, hedonic experiences are actually intact in most people with schizophrenia. The impairment that may present itself as anhedonia probably actually lies in the inability to identify goals, and to identify and engage in the behaviors necessary to achieve goals. Studies support a deficiency in the neural representation of goals and goal directed behavior by demonstrating that receipt (not anticipation) of reward is associated with robust response in the ventral striatum; reinforcement learning is intact when contingencies are implicit, but not when they require explicit processing; reward prediction errors (during functional neuroimaging studies), particularly positive PEs are abnormal; ACC response, taken as an indicator of effort allocation, does not increase with reward or reward probability increase, and is associated with negative symptoms; deficits in dlPFC activity and failure to improve performance on cognitive tasks when offered monetary incentives are present; and dopamine mediated functions are abnormal.

Psychosis has been traditionally linked to the neurotransmitter dopamine. In particular, the dopamine hypothesis of psychosis has been influential and states that psychosis results from an overactivity of dopamine function in the brain, particularly in the mesolimbic pathway. The two major sources of evidence given to support this theory are that dopamine receptor D2 blocking drugs (i.e., antipsychotics) tend to reduce the intensity of psychotic symptoms, and that drugs that accentuate dopamine release, or inhibit its reuptake (such as amphetamines and cocaine) can trigger psychosis in some people (see amphetamine psychosis).

NMDA receptor dysfunction has been proposed as a mechanism in psychosis. This theory is reinforced by the fact that dissociative NMDA receptor antagonists such as ketamine, PCP and dextromethorphan (at large overdoses) induce a psychotic state. The symptoms of dissociative intoxication are also considered to mirror the symptoms of schizophrenia, including negative psychotic symptoms. NMDA receptor antagonism, in addition to producing symptoms reminiscent of psychosis, mimics the neurophysiological aspects, such as reduction in the amplitude of P50, P300, and MMN evoked potentials. Hierarchical Bayesian neurocomputational models of sensory feedback, in agreement with neuroimaging literature, link NMDA receptor hypofunction to delusional or hallucinatory symptoms via proposing a failure of NMDA mediated top down predictions to adequately cancel out enhanced bottom up AMPA mediated predictions errors. Excessive prediction errors in response to stimuli that would normally not produce such as response is thought to confer excessive salience to otherwise mundane events. Dsyfunction higher up in the hierarchy, where representation is more abstract, could result in delusions. The common finding of reduced GAD67 expression in psychotic disorders may explain enhanced AMPA mediated signaling, caused by reduced GABAergic inhibition.

The connection between dopamine and psychosis is generally believed complex. While dopamine receptor D2 suppresses adenylate cyclase activity, the D1 receptor increases it. If D2-blocking drugs are administered the blocked dopamine spills over to the D1 receptors. The increased adenylate cyclase activity affects genetic expression in the nerve cell, which takes time. Hence antipsychotic drugs take a week or two to reduce the symptoms of psychosis. Moreover, newer and equally effective antipsychotic drugs actually block slightly less dopamine in the brain than older drugs whilst also blocking 5-HT2A receptors, suggesting the 'dopamine hypothesis' may be oversimplified. Soyka and colleagues found no evidence of dopaminergic dysfunction in people with alcohol-induced psychosis and Zoldan et al. reported moderately successful use of ondansetron, a 5-HT receptor antagonist, in the treatment of levodopa psychosis in Parkinson's disease patients.

A review found an association between a first-episode of psychosis and prediabetes.

Prolonged or high dose use of psychostimulants can alter normal functioning, making it similar to the manic phase of bipolar disorder. NMDA antagonists replicate some of the so-called "negative" symptoms like thought disorder in subanesthetic doses (doses insufficient to induce anesthesia), and catatonia in high doses. Psychostimulants, especially in one already prone to psychotic thinking, can cause some "positive" symptoms, such as delusional beliefs, particularly those persecutory in nature.

To make a diagnosis of a mental illness in someone with psychosis other potential causes must be excluded. An initial assessment includes a comprehensive history and physical examination by a health care provider. Tests may be done to exclude substance use, medication, toxins, surgical complications, or other medical illnesses. A person with psychosis is referred to as psychotic.

Delirium should be ruled out, which can be distinguished by visual hallucinations, acute onset and fluctuating level of consciousness, indicating other underlying factors, including medical illnesses. Excluding medical illnesses associated with psychosis is performed by using blood tests to measure:

Other investigations include:

Because psychosis may be precipitated or exacerbated by common classes of medications, medication-induced psychosis should be ruled out, particularly for first-episode psychosis. Both substance- and medication-induced psychosis can be excluded to a high level of certainty, using toxicology screening.

Because some dietary supplements may also induce psychosis or mania, but cannot be ruled out with laboratory tests, a psychotic individual's family, partner, or friends should be asked whether the patient is currently taking any dietary supplements.

Common mistakes made when diagnosing people who are psychotic include:


Only after relevant and known causes of psychosis are excluded, a mental health clinician may make a psychiatric differential diagnosis using a person's family history, incorporating information from the person with psychosis, and information from family, friends, or significant others.

Types of psychosis in psychiatric disorders may be established by formal rating scales. The Brief Psychiatric Rating Scale (BPRS) assesses the level of 18 symptom constructs of psychosis such as hostility, suspicion, hallucination, and grandiosity. It is based on the clinician's interview with the patient and observations of the patient's behavior over the previous 2–3 days. The patient's family can also answer questions on the behavior report. During the initial assessment and the follow-up, both positive and negative symptoms of psychosis can be assessed using the 30 item Positive and Negative Symptom Scale (PANSS).

The DSM-5 characterizes disorders as psychotic or on the schizophrenia spectrum if they involve hallucinations, delusions, disorganized thinking, grossly disorganized motor behavior, or negative symptoms. The DSM-5 does not include psychosis as a definition in the glossary, although it defines "psychotic features", as well as "psychoticism" with respect to personality disorder. The ICD-10 has no specific definition of psychosis.

Factor analysis of symptoms generally regarded as psychosis frequently yields a five factor solution, albeit five factors that are distinct from the five domains defined by the DSM-5 to encompass psychotic or schizophrenia spectrum disorders. The five factors are frequently labeled as hallucinations, delusions, disorganization, excitement, and emotional distress. The DSM-5 emphasizes a psychotic spectrum, wherein the low end is characterized by schizoid personality disorder, and the high end is characterized by schizophrenia.

The evidence for the effectiveness of early interventions to prevent psychosis appeared inconclusive. Whilst early intervention in those with a psychotic episode might improve short term outcomes, little benefit was seen from these measures after five years. However, there is evidence that cognitive behavioral therapy (CBT) may reduce the risk of becoming psychotic in those at high risk, and in 2014 the UK National Institute for Health and Care Excellence (NICE) recommended preventive CBT for people at risk of psychosis.

The treatment of psychosis depends on the specific diagnosis (such as schizophrenia, bipolar disorder or substance intoxication). The first-line treatment for many psychotic disorders is antipsychotic medication, which can reduce the positive symptoms of psychosis in about 7 to 14 days.

The choice of which antipsychotic to use is based on benefits, risks, and costs. It is debatable whether, as a class, typical or atypical antipsychotics are better. Tentative evidence supports that amisulpride, olanzapine, risperidone and clozapine may be more effective for positive symptoms but result in more side effects. Typical antipsychotics have equal drop-out and symptom relapse rates to atypicals when used at low to moderate dosages. There is a good response in 40–50%, a partial response in 30–40%, and treatment resistance (failure of symptoms to respond satisfactorily after six weeks to two or three different antipsychotics) in 20% of people. Clozapine is an effective treatment for those who respond poorly to other drugs ("treatment-resistant" or "refractory" schizophrenia), but it has the potentially serious side effect of agranulocytosis (lowered white blood cell count) in less than 4% of people.

Most people on antipsychotics get side effects. People on typical antipsychotics tend to have a higher rate of extrapyramidal side effects while some atypicals are associated with considerable weight gain, diabetes and risk of metabolic syndrome; this is most pronounced with olanzapine, while risperidone and quetiapine are also associated with weight gain. Risperidone has a similar rate of extrapyramidal symptoms to haloperidol.

Psychological treatments such as acceptance and commitment therapy (ACT) are possibly useful in the treatment of psychosis, helping people to focus more on what they can do in terms of valued life directions despite challenging symptomology.

Early intervention in psychosis is based on the observation that identifying and treating someone in the early stages of a psychosis can improve their longer term outcome. This approach advocates the use of an intensive multi-disciplinary approach during what is known as the critical period, where intervention is the most effective, and prevents the long term morbidity associated with chronic psychotic illness.

The word "psychosis" was introduced to the psychiatric literature in 1841 by Karl Friedrich Canstatt in his work "Handbuch der Medizinischen Klinik". He used it as a shorthand for 'psychic neurosis'. At that time neurosis meant any disease of the nervous system, and Canstatt was thus referring to what was considered a psychological manifestation of brain disease. Ernst von Feuchtersleben is also widely credited as introducing the term in 1845, as an alternative to insanity and mania.

The term stems from Modern Latin "psychosis", "a giving soul or life to, animating, quickening" and that from Ancient Greek ψυχή ("psyche"), "soul" and the suffix -ωσις ("-osis"), in this case "abnormal condition".

The word was also used to distinguish a condition considered a disorder of the mind, as opposed to "neurosis", which was considered a disorder of the nervous system. The psychoses thus became the modern equivalent of the old notion of madness, and hence there was much debate on whether there was only one (unitary) or many forms of the new disease. One type of broad usage would later be narrowed down by Koch in 1891 to the 'psychopathic inferiorities'—later renamed abnormal personalities by Schneider.

The division of the major psychoses into manic depressive illness (now called bipolar disorder) and dementia praecox (now called schizophrenia) was made by Emil Kraepelin, who attempted to create a synthesis of the various mental disorders identified by 19th century psychiatrists, by grouping diseases together based on classification of common symptoms. Kraepelin used the term 'manic depressive insanity' to describe the whole spectrum of mood disorders, in a far wider sense than it is usually used today.

In Kraepelin's classification this would include 'unipolar' clinical depression, as well as bipolar disorder and other mood disorders such as cyclothymia. These are characterised by problems with mood control and the psychotic episodes appear associated with disturbances in mood, and patients often have periods of normal functioning between psychotic episodes even without medication. Schizophrenia is characterized by psychotic episodes that appear unrelated to disturbances in mood, and most non-medicated patients show signs of disturbance between psychotic episodes.

Early civilizations considered madness a supernaturally inflicted phenomenon. Archaeologists have unearthed skulls with clearly visible drillings, some datable back to 5000 BC suggesting that trepanning was a common treatment for psychosis in ancient times. Written record of supernatural causes and resultant treatments can be traced back to the New Testament. Mark 5:8–13 describes a man displaying what would today be described as psychotic symptoms. Christ cured this "demonic madness" by casting out the demons and hurling them into a herd of swine. Exorcism is still utilized in some religious circles as a treatment for psychosis presumed to be demonic possession. A research study of out-patients in psychiatric clinics found that 30 percent of religious patients attributed the cause of their psychotic symptoms to evil spirits. Many of these patients underwent exorcistic healing rituals that, though largely regarded as positive experiences by the patients, had no effect on symptomology. Results did, however, show a significant worsening of psychotic symptoms associated with exclusion of medical treatment for coercive forms of exorcism.

The medical teachings of the fourth-century philosopher and physician Hippocrates of Cos proposed a natural, rather than supernatural, cause of human illness. In Hippocrates' work, the Hippocratic corpus, a holistic explanation for health and disease was developed to include madness and other "diseases of the mind." Hippocrates writes:

Hippocrates espoused a theory of humoralism wherein disease is resultant of a shifting balance in bodily fluids including blood, phlegm, black bile, and yellow bile. According to humoralism, each fluid or "humour" has temperamental or behavioral correlates. In the case of psychosis, symptoms are thought to be caused by an excess of both blood and yellow bile. Thus, the proposed surgical intervention for psychotic or manic behavior was bloodletting.

18th century physician, educator, and widely considered "founder of American psychiatry", Benjamin Rush, also prescribed bloodletting as a first-line treatment for psychosis. Although not a proponent of humoralism, Rush believed that active purging and bloodletting were efficacious corrections for disruptions in the circulatory system, a complication he believed was the primary cause of "insanity". Although Rush's treatment modalities are now considered antiquated and brutish, his contributions to psychiatry, namely the biological underpinnings of psychiatric phenomenon including psychosis, have been invaluable to the field. In honor of such contributions, Benjamin Rush's image is in the official seal of the American Psychiatric Association.

Early 20th century treatments for severe and persisting psychosis were characterized by an emphasis on shocking the nervous system. Such therapies include insulin shock therapy, cardiazol shock therapy, and electroconvulsive therapy. Despite considerable risk, shock therapy was considered highly efficacious in the treatment of psychosis including schizophrenia. The acceptance of high-risk treatments led to more invasive medical interventions including psychosurgery.

In 1888, Swiss psychiatrist Gottlieb Burckhardt performed the first medically sanctioned psychosurgery in which the cerebral cortex was excised. Although some patients showed improvement of symptoms and became more subdued, one patient died and several developed aphasia or seizure disorders. Burckhardt would go on to publish his clinical outcomes in a scholarly paper. This procedure was met with criticism from the medical community and his academic and surgical endeavors were largely ignored. In the late 1930s, Egas Moniz conceived the leucotomy (AKA prefrontal lobotomy) in which the fibers connecting the frontal lobes to the rest of the brain were severed. Moniz’s primary inspiration stemmed from a demonstration by neuroscientists John Fulton and Carlyle’s 1935 experiment in which two chimpanzees were given leucotomies and pre and post surgical behavior was compared. Prior to the leucotomy, the chimps engaged in typical behavior including throwing feces and fighting. After the procedure, both chimps were pacified and less violent. During the Q&A, Moniz asked if such a procedure could be extended to human subjects, a question that Fulton admitted was quite startling. Moniz would go on to extend the controversial practice to humans suffering from various psychotic disorders, an endeavor for which he received a Nobel Prize in 1949. Between the late 1930s and early 1970s, the leucotomy was a widely accepted practice, often performed in non-sterile environments such as small outpatient clinics and patient homes. Psychosurgery remained standard practice until the discovery of antipsychotic pharmacology in the 1950s.

The first clinical trial of antipsychotics (also commonly known as neuroleptics) for the treatment of psychosis took place in 1952. Chlorpromazine (brand name: Thorazine) passed clinical trials and became the first antipsychotic medication approved for the treatment of both acute and chronic psychosis. Although the mechanism of action was not discovered until 1963, the administration of chlorpromazine marked the advent of the dopamine antagonist, or first generation antipsychotic. While clinical trials showed a high response rate for both acute psychosis and disorders with psychotic features, the side-effects were particularly harsh, which included high rates of often irreversible Parkinsonian symptoms such as tardive dyskinesia. With the advent of atypical antipsychotics (also known as second generation antipsychotics) came a dopamine antagonist with a comparable response rate but a far different, though still extensive, side-effect profile that included a lower risk of Parkinsonian symptoms but a higher risk of cardiovascular disease. Atypical antipsychotics remain the first-line treatment for psychosis associated with various psychiatric and neurological disorders including schizophrenia, bipolar disorder, major depressive disorder, anxiety disorders, dementia, and some autism spectrum disorders.

It is now known that dopamine is the primary neurotransmitter implicated in psychotic symptomology. Thus, blocking dopamine receptors (namely, the dopamine D2 receptors) and decreasing dopaminergic activity continues to be an effective but highly unrefined pharmacologic goal of antipsychotics. Recent pharmacological research suggests that the decrease in dopaminergic activity does not eradicate psychotic delusions or hallucinations, but rather attenuates the reward mechanisms involved in the development of delusional thinking; that is, connecting or finding meaningful relationships between unrelated stimuli or ideas. The author of this research paper acknowledges the importance of future investigation:
Freud´s former student Wilhelm Reich explored independent insights into the physical effects of neurotic and traumatic upbringing, and published his holistic psychoanalytic treatment with a schizophrenic. With his incorporation of breathwork and insight with the patient, a young woman, she achieved sufficient self-management skills to end the therapy.

Psychiatrist David Healy has criticised pharmaceutical companies for promoting simplified biological theories of mental illness that seem to imply the primacy of pharmaceutical treatments while ignoring social and developmental factors that are known important influences in the aetiology of psychosis.





</doc>
<doc id="24515" url="https://en.wikipedia.org/wiki?curid=24515" title="Paranoia">
Paranoia

Paranoia is an instinct or thought process believed to be heavily influenced by anxiety or fear, often to the point of delusion and irrationality. Paranoid thinking typically includes persecutory, or beliefs of conspiracy concerning a perceived threat towards oneself (e.g. the American colloquial phrase,""Everyone is out to get me""). Paranoia is distinct from phobias, which also involve irrational fear, but usually no blame. Making false accusations and the general distrust of others also frequently accompany paranoia. For example, an incident most people would view as an accident or coincidence, a paranoid person might believe was intentional. Paranoia is a central symptom of psychosis. It is also a matter of personal tolerance for the individual that might be in conflict with psychiatric diagnoses.

A popular symptom of paranoia is the attribution bias. These individuals typically have a biased perception of reality, often exhibiting more hostile beliefs. A paranoid person may view someone else's accidental behavior as though it is with intent or threatening.

An investigation of a non-clinical paranoid population found that feeling powerless and depressed, isolating oneself, and relinquishing activities are characteristics that could be associated with those exhibiting more frequent paranoia.
Some scientists have created different subtypes for the various symptoms of paranoia including erotic, persecutory, litigious, and exalted.

Due to the suspicious and troublesome personality traits of paranoia, it is unlikely that someone with paranoia will thrive in interpersonal relationships. Most commonly paranoid individuals tend to be of a single status. According to some research there is a hierarchy for paranoia. The least common types of paranoia at the very top of the hierarchy would be those involving more serious threats. Social anxiety is at the bottom of this hierarchy as the most frequently exhibited level of paranoia.

Social circumstances appear to be highly influential on paranoid beliefs. Based on data collected by means of a mental health survey distributed to residents of Ciudad Juárez, Chihuahua (in Mexico) and El Paso, Texas (in the United States), paranoid beliefs seem to be associated with feelings of powerlessness and victimization, enhanced by social situations. Potential causes of these effects included a sense of believing in external control, and mistrust which can be strengthened by lower socioeconomic status. Those living in a lower socioeconomic status may feel less in control of their own lives. In addition, this study explains that females have the tendency to believe in external control at a higher rate than males, potentially making females more susceptible to mistrust and the effects of socioeconomic status on paranoia.

Emanuel Messinger reports that surveys have revealed that those exhibiting paranoia can evolve from parental relationships and dis-trustworthy environments. These environments could include being very disciplinary, stringent, and unstable. It was even noted that, "indulging and pampering (thereby impressing the child that he is something special and warrants special privileges)," can be contributing backgrounds. Experiences likely to enhance or manifest the symptoms of paranoia include increased rates of disappointment, stress, and a hopeless state of mind.

Discrimination has also been reported as a potential predictor of paranoid delusions. Such reports that paranoia seemed to appear more in older patients who had experienced higher levels of discrimination throughout their lives. In addition to this it has been noted that immigrants are quite susceptible to forms of psychosis. This could be due to the aforementioned effects of discriminatory events and humiliation.

While many more mood-based symptoms, grandiosity and guilt, may underlie functional paranoia.

Colbi (1981) defined "paranoid cognition" in terms of "persecutory delusions and false beliefs whose propositional content clusters around ideas of being harassed, threatened, harmed, subjugated, persecuted, accused, mistreated, wronged, tormented, disparaged, vilified, and so on, by malevolent others, either specific individuals or groups" (p. 518).
Three components of paranoid cognition have been identified by Robins & Post: "a) suspicions without enough basis that others are exploiting, harming, or deceiving them; b) preoccupation with unjustified doubts about the loyalty, or trustworthiness, of friends or associates; c) reluctance to confide in others because of unwarranted fear that the information will be used maliciously against them" (1997, p. 3).

Paranoid cognition has been conceptualized by clinical psychology almost exclusively in terms of psychodynamic constructs and dispositional variables. From this point of view, paranoid cognition is a manifestation of an intra-psychic conflict or disturbance. For instance, Colby (1981) suggested that the biases of blaming others for one’s problems serve to alleviate the distress produced by the feeling of being humiliated, and helps to repudiate the belief that the self is to blame for such incompetence. This intra-psychic perspective emphasize that the cause of paranoid cognitions are inside the head of the people (social perceiver), and dismiss the fact that paranoid cognition may be related with the social context in which such cognitions are embedded. This point is extremely relevant because when origins of distrust and suspicion (two components of paranoid cognition) are studied many researchers have accentuated the importance of social interaction, particularly when social interaction has gone awry. Even more, a model of trust development pointed out that trust increases or decreases as a function of the cumulative history of interaction between two or more persons.

Another relevant difference can be discerned among "pathological and non-pathological forms of trust and distrust". According to Deutsch, the main difference is that non-pathological forms are flexible and responsive to changing circumstances. Pathological forms reflect exaggerated perceptual biases and judgmental predispositions that can arise and perpetuate them, are reflexively caused errors similar to a self-fulfilling prophecy.

It has been suggested that a "hierarchy" of paranoia exists, extending from mild social evaluative concerns, through ideas of social reference, to persecutory beliefs concerning mild, moderate, and severe threats.

A paranoid reaction may be caused from a decline in brain circulation as a result of high blood pressure or hardening of the arterial walls.

Drug-induced paranoia, associated with amphetamines, methamphetamine and similar stimulants has much in common with schizophrenic paranoia; the relationship has been under investigation since 2012. Drug-induced paranoia has a better prognosis than schizophrenic paranoia once the drug has been removed. For further information, see Stimulant psychosis and Substance-induced psychosis.

During the 1960s with the increase of the use of marijuana, LSD, and other drugs in places like California, paranoia began to set in with many individuals. Some thought that because there was illegality in the use of the drugs--the fear of police catching the individuals was bothering them. Professionals felt, however, that those drugs brought on paranoia. 

Based on data obtained by the Dutch NEMISIS project in 2005, there was an association between impaired hearing and the onset of symptoms of psychosis, which was based on a five-year follow up. Some older studies have actually declared that a state of paranoia can be produced in patients that were under a hypnotic state of deafness. This idea however generated much skepticism during its time.

In the DSM-IV-TR, paranoia is diagnosed in the form of:

According to clinical psychologist P. J. McKenna, "As a noun, paranoia denotes a disorder which has been argued in and out of existence, and whose clinical features, course, boundaries, and virtually every other aspect of which is controversial. Employed as an adjective, paranoid has become attached to a diverse set of presentations, from paranoid schizophrenia, through paranoid depression, to paranoid personality—not to mention a motley collection of paranoid 'psychoses', 'reactions', and 'states'—and this is to restrict discussion to functional disorders. Even when abbreviated down to the prefix para-, the term crops up causing trouble as the contentious but stubbornly persistent concept of paraphrenia".

At least 50% of the diagnosed cases of schizophrenia experience delusions of reference and delusions of persecution. Paranoia perceptions and behavior may be part of many mental illnesses, such as depression and dementia, but they are more prevalent in three mental disorders: paranoid schizophrenia, delusional disorder (persecutory type), and paranoid personality disorder.

The word "paranoia" comes from the Greek παράνοια ("paranoia"), "madness", and that from παρά ("para"), "beside, by" and νόος ("noos"), "mind". However, the history of the term is not the history of the concept, let alone the mental condition currently considered as a disease. Indeed, the historiography of ‘paranoia’ is complicated and must be approached with care. The term was used to describe a mental illness in which a delusional belief is the sole or most prominent feature. In this definition, the belief does not have to be persecutory to be classified as paranoid, so any number of delusional beliefs can be classified as paranoia. For example, a person who has the sole delusional belief that he is an important religious figure would be classified by Kraepelin as having 'pure paranoia'.

According to Michael Phelan, Padraig Wright, and Julian Stern (2000), paranoia and paraphrenia are debated entities that were detached from dementia praecox by Kraepelin, who explained paranoia as a continuous systematized delusion arising much later in life with no presence of either hallucinations or a deteriorating course, paraphrenia as an identical syndrome to paranoia but with hallucinations. Even at the present time, a delusion need not be suspicious or fearful to be classified as paranoid. A person might be diagnosed with paranoid schizophrenia without delusions of persecution, simply because their delusions refer mainly to themselves.

It has generally been agreed upon that individuals with paranoid delusions will have the tendency to take action based on their beliefs. More research is needed on the particular types of actions that are pursued based on paranoid delusions. Some researchers have made attempts to distinguish the different variations of actions brought on as a result of delusions. Wessely et al. (1993) did just this by studying individuals with delusions of which more than half had reportedly taken action or behaved as a result of these delusions. However, the overall actions were not of a violent nature in most of the informants. The authors note that other studies such as one by Taylor (1985), have shown that violent behaviors were more common in certain types of paranoid individuals, mainly those considered to be offensive such as prisoners.

Other researchers have found associations between childhood abusive behaviors and the appearance of violent behaviors in psychotic individuals. This could be a result of their inability to cope with aggression as well as other people, especially when constantly attending to potential threats in their environment. The attention to threat itself has been proposed as one of the major contributors of violent actions in paranoid people, although there has been much deliberation about this as well. Other studies have shown that there may only be certain types of delusions that promote any violent behaviors, persecutory delusions seem to be one of these.

Having resentful emotions towards others and the inability to be able to understand what other people are feeling seem to have an association with violence in paranoid individuals. This was based on a study of paranoid schizophrenics' (one of the common mental disorders that exhibit paranoid symptoms) theories of mind capabilities in relation to empathy. The results of this study revealed specifically that although the violent patients were more successful at the higher level theory of mind tasks, they were not as able to interpret others' emotions or claims.

Social psychological research has proposed a mild form of paranoid cognition, "paranoid social cognition", that has its origins in social determinants more than intra-psychic conflict. This perspective states that in milder forms, paranoid cognitions may be very common among normal individuals. For instance, it is not strange that people may exhibit in their daily life, self-centered thought such as they are being talked about, suspiciousness about other’ intentions, and assumptions of ill or hostility (i.e. people may feel as if everything is going against them). According to Kramer, (1998) these milder forms of paranoid cognition may be considered as an adaptive response to cope with or make sense of disturbing and threatening social environment.

Paranoid cognition captures the idea that dysphoric self-consciousness may be related with the position that people occupies within a social system. This self-consciousness conduces to a hypervigilant and ruminative mode to process social information that finally will stimulate a variety of paranoid-like forms of social misperception and misjudgment. This model identifies four components that are essential to understanding paranoid social cognition: situational antecedents, dysphoric self-consciousness, hypervigilance and rumination, and judgmental biases.

Perceived social distinctiveness, perceived evaluative scrutiny and uncertainty about the social standing.

Refers to an aversive form of heightened public self-consciousness characterized by the feelings that one is under intensive evaluative scrutiny. Becoming self-tormenting will increase the odds of interpreting others' behaviors in a self-referential way.

Self-consciousness was characterized as an aversive psychological state. According to this model, people experiencing self-consciousness will be highly motivated to reduce it, trying to make sense of what they are experiencing. These attempts promote hyper vigilance and rumination in a circular relationship: more hyper vigilance generates more rumination, whereupon more rumination generates more hyper vigilance. Hyper vigilance can be thought of as a way to appraise threatening social information, but in contrast to adaptive vigilance, hyper vigilance will produce elevated levels of arousal, fear, anxiety, and threat perception. Rumination is another possible response to threatening social information. Rumination can be related to the paranoid social cognition because it can increase negative thinking about negative events, and evoke a pessimistic explanatory style.

Three main judgmental consequences have been identified:





</doc>
<doc id="24516" url="https://en.wikipedia.org/wiki?curid=24516" title="Polybius">
Polybius

Polybius (; , "Polýbios";  –  BC) was a Greek historian of the Hellenistic period noted for his work which covered the period of 264–146 BC in detail. The work describes the rise of the Roman Republic to the status of dominance in the ancient Mediterranean world and includes his eyewitness account of the Sack of Carthage in 146 BC. 

Polybius is important for his analysis of the mixed constitution or the separation of powers in government, which was influential on Montesquieu's "The Spirit of the Laws" and the framers of the United States Constitution.

Polybius was born around 200 BC in Megalopolis, Arcadia, when it was an active member of the Achaean League. His father, Lycortas, was a prominent, land-owning politician and member of the governing class who became "strategos" (commanding general) of the Achaean League. Consequently, Polybius was able to observe first hand the political and military affairs of Megalopolis. He developed an interest in horse riding and hunting, diversions that later commended him to his Roman captors. 

In 182 BC, he was given quite an honor when he was chosen to carry the funeral urn of Philopoemen, one of the most eminent Achaean politicians of his generation. In either 169 BC or 170 BC, Polybius was elected hipparchus (cavalry officer), an event which often presaged election to the annual "strategia" (chief generalship). His early political career was devoted largely towards maintaining the independence of Megalopolis.

Polybius’ father, Lycortas, was a prominent advocate of neutrality during the Roman war against Perseus of Macedon. Lycortas attracted the suspicion of the Romans, and Polybius subsequently was one of the 1,000 Achaean nobles who were transported to Rome as hostages in 167 BC, and was detained there for 17 years. In Rome, by virtue of his high culture, Polybius was admitted to the most distinguished houses, in particular to that of Lucius Aemilius Paullus Macedonicus, the conqueror in the Third Macedonian War, who entrusted Polybius with the education of his sons, Fabius and Scipio Aemilianus (who had been adopted by the eldest son of Scipio Africanus). Polybius remained on cordial terms with his former pupil Scipio Aemilianus and was among the members of the Scipionic Circle. 

When Scipio defeated the Carthaginians in the Third Punic War, Polybius remained his counsellor. The Achaean hostages were released in 150 BC, and Polybius was granted leave to return home, but the next year he went on campaign with Scipio Aemilianus to Africa, and was present at the Sack of Carthage in 146, which he later described. Following the destruction of Carthage, Polybius likely journeyed along the Atlantic coast of Africa, as well as Spain.

After the destruction of Corinth in the same year, Polybius returned to Greece, making use of his Roman connections to lighten the conditions there. Polybius was charged with the difficult task of organizing the new form of government in the Greek cities, and in this office he gained great recognition.

In the succeeding years, Polybius resided in Rome, completing his historical work while occasionally undertaking long journeys through the Mediterranean countries in the furtherance of his history, in particular with the aim of obtaining firsthand knowledge of historical sites. He apparently interviewed veterans to clarify details of the events he was recording and was similarly given access to archival material. Little is known of Polybius' later life; he most likely accompanied Scipio to Spain, acting as his military advisor during the Numantine War. 

He later wrote about this war in a lost monograph. Polybius probably returned to Greece later in his life, as evidenced by the many existent inscriptions and statues of him there. The last event mentioned in his "Histories" seems to be the construction of the Via Domitia in southern France in 118 BC, which suggests the writings of Pseudo-Lucian may have some grounding in fact when they state, "[Polybius] fell from his horse while riding up from the country, fell ill as a result and died at the age of eighty-two".

Polybius’ "Histories" cover the period from 264 BC to 146 BC. Its main focus is the period from 220 BC to 167 BC, describing Rome's efforts in subduing its arch-enemy, Carthage, and thereby becoming the dominant Mediterranean force. Books I through V of "The Histories" are the introduction for the years during his lifetime, describing the politics in leading Mediterranean states, including ancient Greece and Egypt, and culminating in their ultimate "συμπλοκή" or interconnectedness. In Book VI, Polybius describes the political, military, and moral institutions that allowed the Romans to succeed. He describes the First and Second Punic Wars. Polybius concludes the Romans are the pre-eminent power because they have customs and institutions which promote a deep desire for noble acts, a love of virtue, piety towards parents and elders, and a fear of the gods ("deisidaimonia"). 

He also chronicled the conflicts between Hannibal and Publius Cornelius Scipio Africanus such as the Battle of Ticinus, the Battle of the Trebia, the Siege of Saguntum, the Battle of Lilybaeum, and the Battle of Rhone Crossing. In Book XII, Polybius discusses the worth of Timaeus’ account of the same period of history. He asserts Timaeus' point of view is inaccurate, invalid, and biased in favor of Rome. Therefore, Polybius's "Histories" is also useful in analyzing the different Hellenistic versions of history and of use as a credible illustration of actual events during the Hellenistic period.

In the seventh volume of his "Histories", Polybius defines the historian's job as the analysis of documentation, the review of relevant geographical information, and political experience. Polybius held that historians should only chronicle events whose participants the historian was able to interview, and was among the first to champion the notion of factual integrity in historical writing. In Polybius' time, the profession of a historian required political experience (which aided in differentiating between fact and fiction) and familiarity with the geography surrounding one's subject matter to supply an accurate version of events. 

Polybius himself exemplified these principles as he was well travelled and possessed political and military experience. He did not neglect written sources that proved essential material for his histories of the period from 264 BC to 220 BC. When addressing events after 220 BC, he examined the writings of Greek and Roman historians to acquire credible sources of information, but rarely did he name those sources.

Polybius wrote several works, the majority of which are lost. His earliest work was a biography of the Greek statesman Philopoemen; this work was later used as a source by Plutarch when composing his Parallel Lives, however the original Polybian text is lost. In addition, Polybius wrote an extensive treatise entitled "Tactics", which may have detailed Roman and Greek military tactics. Small parts of this work may survive in his major "Histories", but the work itself is lost, as well. Another missing work was a historical monograph on the events of the Numantine War. The largest Polybian work was, of course, his "Histories", of which only the first five books survive entirely intact, along with a large portion of the sixth book and fragments of the rest. Along with Cato the Elder (234–149 BC), he can be considered one of the founding fathers of Roman historiography.

Livy made reference to and uses Polybius' "Histories" as source material in his own narrative. Polybius was among the first historians to attempt to present history as a sequence of causes and effects, based upon a careful examination and criticism of tradition. He narrated his history based upon first-hand knowledge. "The Histories" capture the varied elements of the story of human behavior: nationalism, xenophobia, duplicitous politics, war, brutality, loyalty, valour, intelligence, reason, and resourcefulness.

Aside from the narrative of the historical events, Polybius also included three books of digressions. Book 34 was entirely devoted to questions of geography and included some trenchant criticisms of Eratosthenes, whom he accused of passing on popular preconceptions or "laodogmatika". Book 12 was a disquisition on the writing of history, citing extensive passages of lost historians, such as Callisthenes and Theopompus. Most influential was Book 6, which describes Roman political, military, and moral institutions, which he considered key to Rome's success; it presented Rome as having a mixed constitution in which monarchical, aristocratic, and popular elements existed in stable equilibrium. This enabled Rome to escape, for the time being, the cycle of eternal revolutions ("anacyclosis"). While Polybius was not the first to advance this view, his account provides the most cogent illustration of the ideal for later political theorists.

A key theme of "The Histories" is the good statesman as virtuous and composed. The character of the Polybian statesman is exemplified in that of Philip II. His beliefs about Philip's character led Polybius to reject historian Theopompus' description of Philip's private, drunken debauchery. For Polybius, it was inconceivable that such an able and effective statesman could have had an immoral and unrestrained private life as described by Theopompus.
In recounting the Roman Republic, Polybius stated that "the Senate stands in awe of the multitude, and cannot neglect the feelings of the people".

Other important themes running through "The Histories" are the role of Fortune in the affairs of nations, his insistence that history should be demonstratory, or "apodeiktike", providing lessons for statesmen, and that historians should be "men of action" ("pragmatikoi").

Polybius is considered by some to be the successor of Thucydides in terms of objectivity and critical reasoning, and the forefather of scholarly, painstaking historical research in the modern scientific sense. According to this view, his work sets forth the course of history's occurrences with clearness, penetration, sound judgment, and, among the circumstances affecting the outcomes, he lays especial emphasis on geographical conditions. Modern historians are especially impressed with the manner in which Polybius used his sources, particularly documentary evidence as well as his citation and quotation of sources. Furthermore, there is some admiration of Polybius's meditation on the nature of historiography in Book 12. His work belongs, therefore, amongst the greatest productions of ancient historical writing. The writer of the "Oxford Companion to Classical Literature" (1937) praises him for his "earnest devotion to truth" and his systematic pursuit of causation.

It has long been acknowledged that Polybius's writings are prone to a certain hagiographic tone when writing of his friends, such as Scipio, and subject to a vindictive tone when detailing the exploits of his enemies, such as Callicrates, the Achaean statesman responsible for his Roman exile.

As a hostage in Rome, then as client to the Scipios, and after 146 BC, a collaborator with Roman rule, Polybius was probably in no position to freely express any negative opinions of Rome. Peter Green advises that Polybius was chronicling Roman history for a Greek audience, to justify what he believed to be the inevitability of Roman rule. Nonetheless, Green considers Polybius's "Histories" the best source for the era they cover. For Ronald Mellor, Polybius was a loyal partisan of Scipio, intent on vilifying his patron's opponents. Adrian Goldsworthy, while using Polybius as a source for Scipio's generalship, notes Polybius' underlying and overt bias in Scipio's favour. H. Ormerod considers that Polybius cannot be regarded as an 'altogether unprejudiced witness' in relation to his "betes noires"; the Aetolians, the Carthaginians, and the Cretans. Other historians perceive considerable negative bias in Polybius' account of Crete; on the other hand, Hansen notes that the same work, along with passages from Strabo and Scylax, proved a reliable guide in the eventual rediscovery of the lost city of Kydonia.

Polybius was responsible for a useful tool in telegraphy that allowed letters to be easily signaled using a numerical system (mentioned in Hist. X.45.6 ff.). This idea also lends itself to cryptographic manipulation and steganography.

This was known as the "Polybius square", where the letters of the alphabet were arranged left to right, top to bottom in a 5 x 5 square, (when used with the modern 26 letter alphabet, the letters "I" and "J" are combined). Five numbers were then aligned on the outside top of the square, and five numbers on the left side of the square vertically. Usually these numbers were arranged 1 through 5. By cross-referencing the two numbers along the grid of the square, a letter could be deduced.

In "The Histories", he specifies how this cypher could be used in fire signals, where long-range messages could be sent by means of torches raised and lowered to signify the column and row of each letter. This was a great leap forward from previous fire signaling, which could send prearranged codes only (such as, 'if we light the fire, it means that the enemy has arrived').

Other writings of scientific interest include detailed discussions of the machines Archimedes created for the defense of Syracuse against the Romans, where he praises the 'old man' and his engineering in the highest terms, and an analysis of the usefulness of astronomy to generals (both in the "Histories").

Polybius was considered a poor stylist by Dionysius of Halicarnassus, writing of Polybius' history that "no one has the endurance to reach [its] end". Nevertheless, clearly he was widely read by Romans and Greeks alike. He is quoted extensively by Strabo writing in the 1st century BC and Athenaeus in the 3rd century AD. 

His emphasis on explaining causes of events, rather than just recounting events, influenced the historian Sempronius Asellio. Polybius is mentioned by Cicero and mined for information by Diodorus, Livy, Plutarch and Arrian. Much of the text that survives today from the later books of "The Histories" was preserved in Byzantine anthologies.
His works reappeared in the West first in Renaissance Florence. Polybius gained a following in Italy, and although poor Latin translations hampered proper scholarship on his works, they contributed to the city's historical and political discourse. Niccolò Machiavelli in his "Discourses on Livy" evinces familiarity with Polybius. Vernacular translations in French, German, Italian and English first appeared during the 16th century. Consequently, in the late 16th century, Polybius's works found a greater reading audience among the learned public. Study of the correspondence of such men as Isaac Casaubon, Jacques Auguste de Thou, William Camden, and Paolo Sarpi reveals a growing interest in Polybius' works and thought during the period. Despite the existence of both printed editions in the vernacular and increased scholarly interest, however, Polybius remained an "historian's historian", not much read by the public at large. 

Printings of his work in the vernacular remained few in number — seven in French, five in English, and five in Italian.
Polybius' political analysis has influenced republican thinkers from Cicero to Charles de Montesquieu to the Founding Fathers of the United States. John Adams, for example, considered him one of the most important teachers of constitutional theory. Since the Age of Enlightenment, Polybius has in general held appeal to those interested in Hellenistic Greece and early Republican Rome, while his political and military writings have lost influence in academia. More recently, thorough work on the Greek text of Polybius, and his historical technique, has increased the academic understanding and appreciation of him as a historian.

According to Edward Tufte, he was also a major source for Charles Joseph Minard's figurative map of Hannibal's overland journey into Italy during the Second Punic War.

In his "Meditations On Hunting", Spanish philosopher José Ortega y Gasset calls Polybius "one of the few great minds that the turbid human species has managed to produce", and says the damage to the "Histories" is "without question one of the gravest losses that we have suffered in our Greco-Roman heritage".

The Italian version of his name, Polibio, was used as a male first name - for example, the composer Polibio Fumagalli - though it never became very common.

The University of Pennsylvania has an intellectual society, the Polybian Society, which is named in his honor and serves as a non-partisan forum for discussing societal issues and policy.








</doc>
<doc id="24517" url="https://en.wikipedia.org/wiki?curid=24517" title="Plutarch">
Plutarch

Plutarch (; , "Ploútarkhos", ; c. CE 46 – CE 120), later named, upon becoming a Roman citizen, Lucius Mestrius Plutarchus, () was a Greek biographer and essayist, known primarily for his "Parallel Lives" and "Moralia".
He is classified as a Middle Platonist. Plutarch's surviving works were written in Greek, but intended for both Greek and Roman readers.

Plutarch was born to a prominent family in the small town of Chaeronea, about 80 km (50 miles) east of Delphi, in the Greek region of Boeotia. His family was wealthy. The name of Plutarch's father has not been preserved, but based on the common Greek custom of repeating a name in alternate generations, it was probably Nikarchus (). The name of Plutarch's grandfather was Lamprias, as he attested in "Moralia" and in his "Life of Antony".

His brothers, Timon and Lamprias, are frequently mentioned in his essays and dialogues, which speak of Timon in particular in the most affectionate terms. Rualdus, in his 1624 work "Life of Plutarchus", recovered the name of Plutarch's wife, Timoxena, from internal evidence afforded by his writings. A letter is still extant, addressed by Plutarch to his wife, bidding her not to grieve too much at the death of their two-year-old daughter, who was named Timoxena after her mother. He hinted at a belief in reincarnation in that letter of consolation.

The exact number of his sons is not certain, although two of them, Autobulus and the second Plutarch, are often mentioned. Plutarch's treatise "De animae procreatione in Timaeo" is dedicated to them, and the marriage of his son Autobulus is the occasion of one of the dinner parties recorded in the "Table Talk". Another person, Soklarus, is spoken of in terms which seem to imply that he was Plutarch's son, but this is nowhere definitely stated. His treatise on marriage questions, addressed to Eurydice and Pollianus, seems to speak of her as having been recently an inmate of his house, but without any clear evidence on whether she was his daughter or not.

Plutarch studied mathematics and philosophy at the Academy of Athens under Ammonius from 66 to 67.

At some point, Plutarch took Roman citizenship. As evidenced by his new name, Lucius Mestrius Plutarchus, his sponsor for citizenship was Lucius Mestrius Florus, a Roman of consular status whom Plutarch also used as a historical source for his "Life of Otho".

He lived most of his life at Chaeronea, and was initiated into the mysteries of the Greek god Apollo. For many years Plutarch served as one of the two priests at the temple of Apollo at Delphi, the site of the famous Delphic Oracle, twenty miles from his home. By his writings and lectures Plutarch became a celebrity in the Roman Empire, yet he continued to reside where he was born, and actively participated in local affairs, even serving as mayor. At his country estate, guests from all over the empire congregated for serious conversation, presided over by Plutarch in his marble chair. Many of these dialogues were recorded and published, and the 78 essays and other works which have survived are now known collectively as the "Moralia". 

In addition to his duties as a priest of the Delphic temple, Plutarch was also a magistrate at Chaeronea and he represented his home on various missions to foreign countries during his early adult years. Plutarch held the office of archon in his native municipality, probably only an annual one which he likely served more than once. He busied himself with all the little matters of the town and undertook the humblest of duties.

The "Suda", a medieval Greek encyclopedia, states that Emperor Trajan made Plutarch procurator of Illyria. However, most historians consider this unlikely, since Illyria was not a procuratorial province, and Plutarch probably did not speak Illyrian.

According to the 8th/9th-century historian George Syncellus, late in Plutarch's life, Emperor Hadrian appointed him nominal procurator of Achaea – which entitled him to wear the vestments and ornaments of a consul.

Plutarch spent the last thirty years of his life serving as a priest in Delphi. He thus connected part of his literary work with the sanctuary of Apollo, the processes of oracle-giving and the personalities who lived or traveled there. One of his most important works is the "Why Pythia does not give oracles in verse" (Moralia 11) ( "Περὶ τοῦ μὴ χρᾶν ἔμμετρα νῦν τὴν Πυθίαν"). Even more important is the dialogue "On the E in Delphi" ("Περὶ τοῦ Εἶ τοῦ ἐν Δελφοῖς"), which features Ammonius, a Platonic philosopher and teacher of Plutarch, and Lambrias, Plutarch's brother. According to Ammonius, the letter E written on the temple of Apollo in Delphi originated from the following fact: the wise men of antiquity, whose maxims were also written on the walls of the vestibule of the temple, were not seven but actually five: Chilon, Solon, Thales, Bias and Pittakos. However, the tyrants Cleobulos and Periandros used their political power in order to be incorporated in the list. Thus, the E, which corresponds to number 5, constituted an acknowledgment that the Delphic maxims actually originated from the five real wise men. 
The portrait of a philosopher exhibited at the exit of the Archaeological Museum of Delphi, dating to the 2nd century AD, had been in the past identified with Plutarch. The man, although bearded, is depicted at a relatively young age. His hair and beard are rendered in coarse volumes and thin incisions. The gaze is deep, due to the heavy eyelids and the incised pupils. The portrait is no longer thought to represent Plutarch. 
Next to this portrait stands a fragmentary hermaic stele, bearing a portrait probably of the author from Chaeronea and priest in Delphi. Its inscription, however, reads: Δελφοὶ Χαιρωνεῦσιν ὁμοῦ Πλούταρχον ἔθηκαν | τοῖς Ἀμφικτυόνων δόγμασι πειθόμενοι. (Syll.3 843=CID 4, no. 151) The citizens of Delphi and Chaeronea dedicated this to Plutarch together, following the precepts of the Amphictyony.

Plutarch's first biographical works were the Lives of the Roman Emperors from Augustus to Vitellius. Of these, only the Lives of Galba and Otho survive. The Lives of Tiberius and Nero are extant only as fragments, provided by Damascius (Life of Tiberius, cf. his Life of Isidore) and Plutarch himself (Life of Nero, cf. Galba 2.1), respectively. These early emperors’ biographies were probably published under the Flavian dynasty or during the reign of Nerva (AD 96–98).

There is reason to believe that the two Lives still extant, those of Galba and Otho, "ought to be considered as a single work." Therefore, they do not form a part of the Plutarchian canon of single biographies – as represented by the Life of Aratus of Sicyon and the Life of Artaxerxes II (the biographies of Hesiod, Pindar, Crates and Daiphantus were lost). Unlike in these biographies, in "Galba-Otho" the individual characters of the persons portrayed are not depicted for their own sake but instead serve as an illustration of an abstract principle; namely the adherence or non-adherence to Plutarch’s morally founded ideal of governing as a Princeps (cf. Galba 1.3; Moralia 328D–E).

Arguing from the perspective of Platonic political philosophy (cf. Republic 375E, 410D-E, 411E-412A, 442B-C), in "Galba-Otho" Plutarch reveals the constitutional principles of the Principate in the time of the civil war after Nero's death. While morally questioning the behavior of the autocrats, he also gives an impression of their tragic destinies, ruthlessly competing for the throne and finally destroying each other. "The Caesars' house in Rome, the Palatium, received in a shorter space of time no less than four Emperors", Plutarch writes, "passing, as it were, across the stage, and one making room for another to enter" (Galba 1).

"Galba-Otho" was handed down through different channels. It can be found in the appendix to Plutarch's "Parallel Lives" as well as in various Moralia manuscripts, most prominently in Maximus Planudes' edition where Galba and Otho appear as "Opera" XXV and XXVI. Thus it seems reasonable to maintain that "Galba-Otho" was from early on considered as an illustration of a moral-ethical approach, possibly even by Plutarch himself.

Plutarch's best-known work is the "Parallel Lives", a series of biographies of famous Greeks and Romans, arranged in pairs to illuminate their common moral virtues and vices. The surviving "Lives" contain 23 pairs, each with one Greek "Life" and one Roman "Life", as well as four unpaired single "Lives".

As is explained in the opening paragraph of his "Life of Alexander", Plutarch was not concerned with history so much as the influence of character, good or bad, on the lives and destinies of men. Whereas sometimes he barely touched on epoch-making events, he devoted much space to charming anecdote and incidental triviality, reasoning that this often said far more for his subjects than even their most famous accomplishments. He sought to provide rounded portraits, likening his craft to that of a painter; indeed, he went to tremendous lengths (often leading to tenuous comparisons) to draw parallels between physical appearance and moral character. In many ways, he must be counted amongst the earliest moral philosophers.

Some of the "Lives", such as those of Heracles, Philip II of Macedon, Epaminondas and Scipio Africanus, no longer exist; many of the remaining "Lives" are truncated, contain obvious lacunae or have been tampered with by later writers. Extant "Lives" include those on Solon, Themistocles, Aristides, Agesilaus II, Pericles, Alcibiades, Nicias, Demosthenes, Pelopidas, Philopoemen, Timoleon, Dion of Syracuse, Eumenes, Alexander the Great, Pyrrhus of Epirus, Romulus, Numa Pompilius, Coriolanus, Theseus, Aemilius Paullus, Tiberius Gracchus, Gaius Gracchus, Gaius Marius, Sulla, Sertorius, Lucullus, Pompey, Julius Caesar, Cicero, Cato the Elder, Mark Antony, and Marcus Junius Brutus.

Since Spartans wrote no history prior to the Hellenistic period, and since their only extant literature is fragments of 7th-century lyrics, Plutarch's five Spartan lives and "Sayings of Spartans" and "Sayings of Spartan Women", rooted in sources that have since disappeared, are one of the richest sources for historians of Lacedaemonia. But while they are important, they are also controversial. Plutarch lived centuries after the Sparta he writes about (and a full millennium separates him from the earliest events he records) and even though he visited Sparta, many of the ancient customs he reports had been long abandoned, so he never actually saw what he wrote. Plutarch's sources themselves can be problematic. As the historians Sarah Pomeroy, Stanley Burstein, Walter Donlan, and Jennifer Tolbert Roberts have written, "Plutarch was influenced by histories written after the decline of Sparta and marked by nostalgia for a happier past, real or imagined." Turning to Plutarch himself, they write, "the admiration writers like Plutarch and Xenophon felt for Spartan society led them to exaggerate its monolithic nature, minimizing departures from ideals of equality and obscuring patterns of historical change." Thus the Spartan egalitarianism and superhuman immunity to pain that have seized the popular imagination are likely myths, and their main architect is Plutarch. While flawed, Plutarch is nonetheless indispensable as one of the only ancient sources of information on Spartan life. Pomeroy et al. conclude that Plutarch's works on Sparta, while they must be treated with skepticism, remain valuable for their "large quantities of information" and these historians concede that "Plutarch's writings on Sparta, more than those of any other ancient author, have shaped later views of Sparta", despite their potential to misinform.

Plutarch's "Life of Alexander", written as a parallel to that of Julius Caesar, is one of only five extant tertiary sources on the Macedonian conqueror Alexander the Great. It includes anecdotes and descriptions of events that appear in no other source, just as Plutarch's portrait of Numa Pompilius, the putative second king of Rome, holds much that is unique on the early Roman calendar.

Plutarch devotes a great deal of space to Alexander's drive and desire, and strives to determine how much of it was presaged in his youth. He also draws extensively on the work of Lysippus, Alexander's favourite sculptor, to provide what is probably the fullest and most accurate description of the conqueror's physical appearance. When it comes to his character, Plutarch emphasizes his unusual degree of self-control. As the narrative progresses, however, the subject incurs less admiration from his biographer and the deeds that it recounts become less savoury. The murder of Cleitus the Black, which Alexander instantly and deeply regretted, is commonly cited to this end.

Much, too, is made of Alexander's scorn for luxury: "He desired not pleasure or wealth, but only excellence and glory." This is mostly true, for Alexander's tastes grew more extravagant as he grew older only in the last year of his life and only as a means of approaching the image of a ruler his Persian subjects were better accustomed to — thus making it easier for him to succeed in uniting the Greek and Persian worlds together, according to the plan he had announced in his famous Speech given in Opis in 324 BC.

Together with Suetonius's "The Twelve Caesars", and Caesar's own works "de Bello Gallico" and "de Bello Civili", this "Life" is the main account of Julius Caesar's feats by ancient historians. Plutarch starts by telling the audacity of Caesar and his refusal to dismiss Cinna's daughter, Cornelia. Other important parts are these containing his military deeds, accounts of battles and Caesar's capacity of inspiring the soldiers.

However, this "Life" shows few differences between Suetonius' work and Caesar's own works (see "De Bello Gallico" and "De Bello Civili"). Sometimes, Plutarch quotes directly from the "De Bello Gallico" and even tells us of the moments when Caesar was dictating his works.

In the final part of this "Life", Plutarch counts Caesar's assassination, and several details. The book ends on telling the destiny of his murderers, and says that Caesar's "great guardian-genius" avenged him after life.

Plutarch's "Life of Pyrrhus" is a key text because it is the main historical account on Roman history for the period from 293 to 264 BC, for which neither Dionysius nor Livy have surviving texts.

Plutarch stretches and occasionally fabricates the similarities between famous Greeks and Romans in order to be able to write their biographies as parallel. The lives of Nicias and Crassus, for example, have little in common except that "both were rich and both suffered great military defeats at the ends of their lives".

In his "Life of Pompey", Plutarch praises Pompey's trustworthy character and tactful behaviour in order to conjure a moral judgement that opposes most historical accounts. Plutarch delivers anecdotes with moral points, rather than in-depth comparative analyses of the causes of the fall of the Achaemenid Empire and the Roman Republic, and tends on occasion to fit facts to hypotheses.

On the other hand, he generally sets out his moral anecdotes in chronological order (unlike, say, his Roman contemporary Suetonius) and is rarely narrow-minded and unrealistic, almost always prepared to acknowledge the complexity of the human condition where moralising cannot explain it.

The remainder of Plutarch's surviving work is collected under the title of the "Moralia" (loosely translated as "Customs and Mores"). It is an eclectic collection of seventy-eight essays and transcribed speeches, including "On Fraternal Affection"—a discourse on honour and affection of siblings toward each other, "On the Fortune or the Virtue of Alexander the Great"—an important adjunct to his Life of the great king, "On the Worship of Isis and Osiris" (a crucial source of information on Egyptian religious rites), along with more philosophical treatises, such as "On the Decline of the Oracles", "On the Delays of the Divine Vengeance", "On Peace of Mind" and lighter fare, such as "Odysseus and Gryllus", a humorous dialogue between Homer's Odysseus and one of Circe's enchanted pigs. The "Moralia" was composed first, while writing the Lives occupied much of the last two decades of Plutarch's own life.

Book IV of the "Moralia" contains the "Roman and Greek Questions" (Αἰτίαι Ῥωμαϊκαί and Αἰτίαι Ἑλλήνων). The customs of Romans and Greeks are illuminated in little essays that pose questions such as 'Why were patricians not permitted to live on the Capitoline?' (no. 91) and then suggests answers to them.

In "On the Malice of Herodotus" Plutarch criticizes the historian Herodotus for all manner of prejudice and misrepresentation. It has been called the "first instance in literature of the slashing review." The 19th-century English historian George Grote considered this essay a serious attack upon the works of Herodotus, and speaks of the "honourable frankness which Plutarch calls his malignity." Plutarch makes some palpable hits, catching Herodotus out in various errors, but it is also probable that it was merely a rhetorical exercise, in which Plutarch plays devil's advocate to see what could be said against so favourite and well-known a writer. According to Plutarch scholar R. H. Barrow, Herodotus’ real failing in Plutarch’s eyes was to advance any criticism at all of those states that saved Greece from Persia. “Plutarch”, he concluded, “is fanatically biased in favor of the Greek cities; they can do no wrong.”

"Symposiacs" (Συμποσιακά); "Convivium Septem Sapientium".

The Romans loved the "Lives", and enough copies were written out over the centuries so that a copy of most of the lives has survived to the present day. An ancient list of works attributed to Plutarch, the 'Catalogue of Lamprias' contains 227 works, of which 78 have come down to us. The lost works of Plutarch are determined by references in his own texts to them and from other authors' references over time. There are traces of twelve more Lives that are now lost.

Plutarch's general procedure for the "Lives" was to write the life of a prominent Greek, then cast about for a suitable Roman parallel, and end with a brief comparison of the Greek and Roman lives. Currently, only 19 of the parallel lives end with a comparison, while possibly they all did at one time. Also missing are many of his "Lives" which appear in a list of his writings, those of Hercules, the first pair of "Parallel Lives", Scipio Africanus and Epaminondas, and the companions to the four solo biographies. Even the lives of such important figures as Augustus, Claudius and Nero have not been found and may be lost forever.

Other lost works include "Whether One Who Suspends Judgment on Everything Is Condemned to Inaction", "On Pyrrho’s Ten Modes", and "On the Difference between the Pyrrhonians and the Academics".

Plutarch was a Platonist, but was open to the influence of the Peripatetics, and in some details even to Stoicism despite his criticism of their principles. He rejected only Epicureanism absolutely. He attached little importance to theoretical questions and doubted the possibility of ever solving them. He was more interested in moral and religious questions.

In opposition to Stoic materialism and Epicurean atheism he cherished a pure idea of God that was more in accordance with Plato. He adopted a second principle ("Dyad") in order to explain the phenomenal world. This principle he sought, however, not in any indeterminate matter but in the evil world-soul which has from the beginning been bound up with matter, but in the creation was filled with reason and arranged by it. Thus it was transformed into the divine soul of the world, but continued to operate as the source of all evil. He elevated God above the finite world, and thus daemons became for him agents of God's influence on the world. He strongly defends freedom of the will, and the immortality of the soul.

Platonic-Peripatetic ethics were upheld by Plutarch against the opposing theories of the Stoics and Epicureans. The most characteristic feature of Plutarch's ethics is, however, its close connection with religion. However pure Plutarch's idea of God is, and however vivid his description of the vice and corruption which superstition causes, his warm religious feelings and his distrust of human powers of knowledge led him to believe that God comes to our aid by direct revelations, which we perceive the more clearly the more completely that we refrain in "enthusiasm" from all action; this made it possible for him to justify popular belief in divination in the way which had long been usual among the Stoics.

His attitude to popular religion was similar. The gods of different peoples are merely different names for one and the same divine Being and the powers that serve it. The myths contain philosophical truths which can be interpreted allegorically. Thus Plutarch sought to combine the philosophical and religious conception of things and to remain as close as possible to tradition.

Plutarch's writings had an enormous influence on English and French literature. Shakespeare paraphrased parts of Thomas North's translation of selected "Lives "in his plays, and occasionally quoted from them verbatim.

Ralph Waldo Emerson and the Transcendentalists were greatly influenced by the "Moralia" and in his glowing introduction to the five-volume, 19th-century edition, he called the "Lives" "a bible for heroes". He also opined that it was impossible to "read Plutarch without a tingling of the blood; and I accept the saying of the Chinese Mencius: 'A sage is the instructor of a hundred ages. When the manners of Loo are heard of, the stupid become intelligent, and the wavering, determined.'"

Montaigne's "Essays" draw extensively on Plutarch's "Moralia" and are consciously modelled on the Greek's easygoing and discursive inquiries into science, manners, customs and beliefs. "Essays" contains more than 400 references to Plutarch and his works.

James Boswell quoted Plutarch on writing lives, rather than biographies, in the introduction to his own "Life of Samuel Johnson". Other admirers included Ben Jonson, John Dryden, Alexander Hamilton, John Milton, Louis L'amour, and Francis Bacon, as well as such disparate figures as Cotton Mather and Robert Browning.

Plutarch's influence declined in the 19th and 20th centuries, but it remains embedded in the popular ideas of Greek and Roman history. One of his most famous quotes was one that he included in one of his earliest works. "The world of man is best captured through the lives of the men who created history."

There are translations, from the original Greek, in Latin, English, French, German, Italian, Polish and Hebrew.

“One advantage to a modern reader who is not well acquainted with Greek is, that being but a moderate stylist, Plutarch is almost as good in a translation as in the original.”

Jacques Amyot's translations brought Plutarch's works to Western Europe. He went to Italy and studied the Vatican text of Plutarch, from which he published a French translation of the "Lives" in 1559 and "Moralia" in 1572, which were widely read by educated Europe. Amyot's translations had as deep an impression in England as France, because Thomas North later published his English translation of the "Lives" in 1579 based on Amyot’s French translation instead of the original Greek.

Plutarch's "Lives" were translated into English, from Amyot's version, by Sir Thomas North in 1579. The complete "Moralia" was first translated into English from the original Greek by Philemon Holland in 1603.

In 1683, John Dryden began a life of Plutarch and oversaw a translation of the "Lives" by several hands and based on the original Greek. This translation has been reworked and revised several times, most recently in the 19th century by the English poet and classicist Arthur Hugh Clough (first published in 1859). One contemporary publisher of this version is Modern Library. Another is Encyclopædia Britannica in association with the University of Chicago, , copyright 1952, Library of Congress catalogue card number 55-10323.

In 1770, English brothers John and William Langhorne published "Plutarch's Lives from the original Greek, with notes critical and historical, and a new life of Plutarch" in 6 volumes and dedicated to Lord Folkestone. Their translation was re-edited by Archdeacon Wrangham in the year 1819.

From 1901 to 1912, an American classicist, Bernadotte Perrin, produced a new translation of the "Lives" for the Loeb Classical Library. The "Moralia" is also included in the Loeb series, translated by various authors.

Penguin Classics began a series of translations by various scholars in 1958 with "The Fall of the Roman Republic", which contained six Lives and was translated by Rex Warner. Penguin continues to revise the volumes.

Note: just main translations from the second half of 15th century.


There are multiple translations of "Parallel Lives" into Latin, most notably the one titled "Pour le Dauphin" (French for "for the Prince") written by a scribe in the court of Louis XV of France and a 1470 Ulrich Han translation.

In 1519, Hieronymus Emser translated "De capienda ex inimicis utilitate" (wie ym eyner seinen veyndt nutz machen kan, Leipzig).

The biographies were translated by Gottlob Benedict von Schirach (1743–1804) and printed in Vienna by Franz Haas, 1776–80.

Plutarch's "Lives and Moralia" were translated into German by Johann Friedrich Salomon Kaltwasser:


Following some Hebrew translations of selections from Plutarch's "Parallel Lives" published in the 1920s and the 1940s, a complete translation was published in three volumes by the Bialik Institute in 1954, 1971 and 1973. The first volume, "Roman Lives", first published in 1954, presents the translations of Joseph G. Liebes to the biographies of Coriolanus, Fabius Maximus, Tiberius Gracchus and Gaius Gracchus, Cato the Elder and Cato the Younger, Gaius Marius, Sulla, Sertorius, Lucullus, Pompey, Crassus, Cicero, Julius Caesar, Brutus and Mark Anthony.

The second volume, "Greek Lives", first published in 1971 presents A. A. Halevy's translations of the biographies of Lycurgus, Aristides, Cimon, Pericles, Nicias, Lysander, Agesilaus, Pelopidas, Dion, Timoleon, Demosthenes, Alexander the Great, Eumenes and Phocion. Three more biographies presented in this volume, those of Solon, Themistocles and Alcibiades were translated by M. H. Ben-Shamai.

The third volume, "Greek and Roman Lives", published in 1973, presented the remaining biographies and parallels as translated by Halevy. Included are the biographies of Demetrius, Pyrrhus, Agis and Cleomenes, Aratus and Artaxerxes, Philopoemen, Camillus, Marcellus, Flamininus, Aemilius Paulus, Galba and Otho, Theseus, Romulus, Numa Pompilius and Poplicola. It completes the translation of the known remaining biographies. In the introduction to the third volume Halevy explains that originally the Bialik Institute intended to publish only a selection of biographies, leaving out mythological figures and biographies that had no parallels. Thus, to match the first volume in scope the second volume followed the same path and the third volume was required.

Some editions of the "Moralia" include several works now known to have been falsely attributed to Plutarch. Among these are the "Lives of the Ten Orators", a series of biographies of the Attic orators based on Caecilius of Calacte; "On the Opinions of the Philosophers", "On Fate", and "On Music". These works are all attributed to a single, unknown author, referred to as "Pseudo-Plutarch". Pseudo-Plutarch lived sometime between the third and fourth centuries A.D. Despite being falsely attributed, the works are still considered to possess historical value.







</doc>
<doc id="24518" url="https://en.wikipedia.org/wiki?curid=24518" title="Peter Sellers">
Peter Sellers

Peter Sellers, CBE (born Richard Henry Sellers; 8 September 1925 – 24 July 1980) was an English film actor, comedian and singer. He performed in the BBC Radio comedy series "The Goon Show", featured on a number of hit comic songs and became known to a worldwide audience through his many film characterisations, among them Chief Inspector Clouseau in "The Pink Panther" series of films.

Born in Portsmouth, Sellers made his stage debut at the Kings Theatre, Southsea, when he was two weeks old. He began accompanying his parents in a variety act that toured the provincial theatres. He first worked as a drummer and toured around England as a member of the Entertainments National Service Association (ENSA). He developed his mimicry and improvisational skills during a spell in Ralph Reader's wartime Gang Show entertainment troupe, which toured Britain and the Far East. After the war, Sellers made his radio debut in "ShowTime", and eventually became a regular performer on various BBC radio shows. During the early 1950s, Sellers, along with Spike Milligan, Harry Secombe and Michael Bentine, took part in the successful radio series "The Goon Show", which ended in 1960.

Sellers began his film career during the 1950s. Although the bulk of his work was comedic, often parodying characters of authority such as military officers or policemen, he also performed in other film genres and roles. Films demonstrating his artistic range include "I'm All Right Jack" (1959), Stanley Kubrick's "Lolita" (1962) and "Dr. Strangelove" (1964), "What's New, Pussycat?" (1965), "Casino Royale" (1967), "The Party" (1968), "Being There" (1979) and five films of the "Pink Panther" series (1963–78). Sellers's versatility enabled him to portray a wide range of comic characters using different accents and guises, and he would often assume multiple roles within the same film, frequently with contrasting temperaments and styles. Satire and black humour were major features of many of his films, and his performances had a strong influence on a number of later comedians. Sellers was nominated three times for an Academy Award, twice for the Academy Award for Best Actor for his performances in "Dr. Strangelove" and "Being There", and once for the Academy Award for Best Live Action Short Film for "The Running Jumping & Standing Still Film" (1959). He won the BAFTA Award for Best Actor in a Leading Role twice, for "I'm All Right Jack" and for the original Pink Panther film, "The Pink Panther" (1963) and was nominated as Best Actor three times. In 1980 he won the Golden Globe Award for Best Actor – Motion Picture Musical or Comedy for his role in "Being There", and was previously nominated three times in the same category. Turner Classic Movies calls Sellers "one of the most accomplished comic actors of the late 20th century."

In his personal life, Sellers struggled with depression and insecurities. An enigmatic figure, he often claimed to have no identity outside the roles that he played. His behaviour was often erratic and compulsive, and he frequently clashed with his directors and co-stars, especially in the mid-1970s when his physical and mental health, together with his alcohol and drug problems, were at their worst. Sellers was married four times, and had three children from his first two marriages. He died as a result of a heart attack in 1980, aged 54. English filmmakers the Boulting brothers described Sellers as "the greatest comic genius this country has produced since Charles Chaplin."

Sellers was born on 8 September 1925, in Southsea, a suburb of Portsmouth. His parents were Yorkshire-born William "Bill" Sellers (1900–62) and Agnes Doreen "Peg" (née Marks, 1892–1967). Both were variety entertainers; Peg was in the Ray Sisters troupe. Although christened Richard Henry, his parents called him Peter, after his elder stillborn brother. Sellers remained an only child. Peg Sellers was related to the pugilist Daniel Mendoza (1764–1836), whom Sellers greatly revered, and whose engraving later hung in his office. At one time Sellers planned to use Mendoza's image for his production company's logo.

Sellers was two weeks old when he was carried on stage by Dick Henderson, the headline act at the Kings Theatre in Southsea: the crowd sang "For He's a Jolly Good Fellow", which caused the infant to cry. The family constantly toured, causing much upheaval and unhappiness in the young Sellers' life.

Sellers maintained a very close relationship with his mother, which his friend Spike Milligan later considered unhealthy for a grown man. Sellers's agent, Dennis Selinger, recalled his first meeting with Peg and Peter Sellers, noting that "Sellers was an immensely shy young man, inclined to be dominated by his mother, but without resentment or objection". As an only child though, he spent much time alone.

In 1935 the Sellers family moved to North London and settled in Muswell Hill. Although Bill Sellers was Protestant and Peg was Jewish, Sellers attended the North London Roman Catholic school St. Aloysius College, run by the Brothers of Our Lady of Mercy. The family was not rich, but Peg insisted on an expensive private schooling for her son. According to biographer Peter Evans, Sellers was fascinated, puzzled, and worried by religion from a young age, particularly Catholicism, while Roger Lewis believed that soon after entering Catholic school, Sellers "discovered he was a Jew—he was someone on the outside of the mysteries of faith". Later in his life, Sellers observed that while his father's faith was according to the Church of England, his mother was Jewish, "and Jews take the faith of their mother." According to Milligan, Sellers held a guilt complex about being Jewish and recalls that Sellers was once reduced to tears when he presented him with a candlestick from a synagogue for Christmas, believing the gesture to be an anti-Jewish slur.

Sellers became a top student at the school, excelling in drawing in particular. However, he was prone to laziness, but his natural talents shielded him from criticism by his teachers. Sellers recalled that a teacher scolded the other boys for not studying, saying: "The Jewish boy knows his catechism better than the rest of you!"

Accompanying his family on the variety show circuit, Sellers learned stagecraft, but received conflicting encouragement from his parents and developed mixed feelings about show business. His father doubted Sellers's abilities in the entertainment field, even suggesting that his son's talents were only enough to become a road sweeper, while Sellers's mother encouraged him continuously.

While at St Aloysius College, Sellers began to develop his improvisational skills. He and his closest friend at the time, Bryan Connon, both enjoyed listening to early radio comedy shows. Connon remembers that "Peter got endless pleasure imitating the people in "Monday Night at Eight". He had a gift for improvising dialogue. Sketches, too. I'd be the 'straight man', the 'feed', ... I'd cue Peter and he'd do all the radio personalities and chuck in a few voices of his own invention as well."

With the outbreak of the Second World War in 1939, St Aloysius College was evacuated to Cambridgeshire. Because his mother did not allow Sellers to go, his formal education ended at fourteen. Early in 1940, the family moved to the north Devon town of Ilfracombe, where Sellers's maternal uncle managed the Victoria Palace Theatre; Sellers got his first job at the theatre, aged fifteen, starting as a caretaker. He was steadily promoted, becoming a box office clerk, usher, assistant stage manager and lighting operator. He was also offered some small acting parts. Working backstage gave him a chance to study actors such as Paul Scofield. He became close friends with Derek Altman, and together they launched Sellers's first stage act under the name "Altman and Sellers", consisting of playing ukuleles, singing, and telling jokes.

During his backstage theatre job, Sellers began practising on a set of drums that belonged to the band Joe Daniels and his Hot Shots. Daniels noticed his efforts and gave him practical instructions. The instrument greatly suited Sellers's temperament and artistic skills. Spike Milligan later noted that Sellers was very proficient on the drums and might have remained a jazz drummer, had he lacked his skills in mimicry and improvisation.

As the war progressed, Sellers continued to develop his drumming skills, and played with a series of touring bands, including those of Oscar Rabin, Henry Hall and Waldini, as well as his father's quartet, before he left and joined a band from Blackpool. Sellers became a member of the Entertainments National Service Association (ENSA), which provided entertainment for British forces and factory workers during the war. Sellers also performed comedy routines at these concerts, including impersonations of George Formby, with Sellers accompanying his own singing on ukulele.

In September 1943, he joined the Royal Air Force, although it is unclear whether he volunteered or was conscripted; his mother unsuccessfully tried to have him deferred on medical grounds. Sellers wanted to become a pilot, but his poor eyesight restricted him to ground staff duties. He found these duties dull, so auditioned for Squadron Leader Ralph Reader's RAF "Gang Show" entertainment troupe: Reader accepted him and Sellers toured the UK before the troupe was transferred to India. His tour also included Ceylon and Burma, although the duration of his stay in Asia is unknown, and Sellers may have exaggerated its length. He also served in Germany and France after the war.
According to David Lodge, who became friends with Sellers, he was "one of the best performers ever" on the drums and developed a fine ability to impersonate military officers during this period.

In 1946, Sellers made his final show with ENSA starring in the pantomime "Jack and the Beanstalk" at the Théâtre Marigny in Paris. He was posted back to England shortly afterwards to work at the Air Ministry, and demobilised later that year. On resuming his theatrical career, Sellers could get only sporadic work. He was fired after one performance of a comedy routine in Peterborough; the headline act, Welsh vocalist Dorothy Squires, however, persuaded the management to reinstate him. Sellers also continued his drumming and was billed on his appearance at The Hippodrome in Aldershot as "Britain's answer to Gene Krupa". In March 1948 Sellers gained a six-week run at the Windmill Theatre in London, which predominantly staged revue acts: he provided the comedy turns in between the nude shows on offer.

Sellers wrote to the BBC in 1948, and was subsequently auditioned. As a result, he made his television debut on 18 March 1948 in "New To You". His act, largely based on impressions, was well received, and he returned the following week. Frustrated with the slow pace of his career, Sellers telephoned BBC radio producer Roy Speer, pretending to be Kenneth Horne, star of the radio show "Much-Binding-in-the-Marsh". Speer called Sellers a "cheeky young sod" for his efforts, but gave him an audition. This led to his brief appearance on 1 July 1948 on "ShowTime" and subsequently to work on "Ray's a Laugh" with comedian Ted Ray. In October 1948, Sellers was a regular radio performer, appearing in "Starlight Hour", "The Gang Show", "Henry Hall's Guest Night" and "It's Fine To Be Young".

By the end of 1948, the BBC Third Programme began to broadcast the comedy series "Third Division", which starred, among others, Harry Secombe, Michael Bentine and Sellers. One evening, Sellers and Bentine visited the Hackney Empire, where Secombe was performing, and Bentine introduced Sellers to Spike Milligan. The four would meet up at Grafton's public house near Victoria, owned by Jimmy Grafton, who was also a BBC script writer. The four comedians dubbed him "KOGVOS" (Keeper of Goons and Voice of Sanity) Grafton later edited some of the first "Goon Shows".

In 1949, Sellers started to date Anne Howe, an Australian actress who lived in London. Sellers proposed to her in April 1950 and the couple were married in London on 15 September 1951; their son, Michael, was born on 2 April 1954, and their daughter, Sarah, followed in 1958.

Sellers's introduction to film work came in 1950, where he dubbed the voice of Alfonso Bedoya in "The Black Rose". He continued to work with Bentine, Milligan, and Secombe. On 3 February 1951, he made a trial tape entitled "The Goons", and sent it to the BBC producer Pat Dixon, who eventually accepted it. The first "Goon Show" was broadcast on 28 May 1951. Against their wishes, they appeared under the name "Crazy People". Sellers appeared in "The Goons" until the last programme of the ten-series run, broadcast on 28 January 1960. Sellers played four main characters—Major Bloodnok, Hercules Grytpype-Thynne, Bluebottle and Henry Crun—and seventeen minor ones.

Starting with 370,000 listeners, the show eventually reached up to seven million people in Britain, and was described by one newspaper as "probably the most influential comedy show of all time". For Sellers, the BBC considers it had the effect of launching his career "on the road to stardom".

In 1951 the Goons made their feature film debut in "Penny Points to Paradise". Sellers and Milligan then penned the script to "Let's Go Crazy", the earliest film to showcase Sellers's ability to portray a series of different characters within the same film, and he made another appearance opposite his Goons co-stars in the 1952 flop, "Down Among the Z Men". In 1954, Sellers was cast opposite Sid James, Tony Hancock, Raymond Huntley, Donald Pleasence and Eric Sykes in the British Lion Film Corporation comedy production, "Orders Are Orders". John Grierson believes that this was Sellers's breakthrough role on screen and credits this film with launching the film careers of both Sellers and Hancock.

Sellers pursued a film career and took a number of small roles such as a police inspector in "John and Julie" (1955). He accepted a larger part in the 1955 Alexander Mackendrick-directed Ealing comedy "The Ladykillers" in which he starred opposite Alec Guinness, Herbert Lom and Cecil Parker as Harry Robinson, the Teddy Boy; biographer Peter Evans considers this Sellers's first good role. "The Ladykillers" was a success in both Britain and the US, and the film was nominated for an Academy Award for Best Original Screenplay.

The following year Sellers appeared in a further three television series based on "The Goons", which aired on Britain's new ITV channel. The series were "The Idiot Weekly, Price 2d", "A Show Called Fred" and "Son of Fred". In 1957 film producer Michael Relph became impressed with Sellers's portrayal of an elderly character in "Idiot Weekly", and cast the 32-year-old actor as a 68-year-old projectionist in Basil Dearden's "The Smallest Show on Earth", supporting Bill Travers, Virginia McKenna and Margaret Rutherford. The film was a commercial success and is now thought of as a minor classic of British screen comedy in the post-war era. Following this, Sellers provided the growling voice of Winston Churchill to the BAFTA award winning film "The Man Who Never Was". Later in 1957 Sellers portrayed a television star with a talent for disguises in Mario Zampi's offbeat black comedy "The Naked Truth", opposite Terry-Thomas, Peggy Mount, Shirley Eaton and Dennis Price.

Sellers's difficulties in getting his film career to take off, and increasing problems in his personal life, prompted him to seek periodic consultations with astrologer Maurice Woodruff, who held considerable sway over his later career. After a chance meeting with a North American Indian spirit guide in the 1950s, Sellers became convinced that the music hall comedian Dan Leno, who died in 1904, haunted him and guided his career and life-decisions. Sellers was a member of the Grand Order of Water Rats, the same exclusive theatrical fraternity founded by Leno in 1890.

In 1958 Sellers starred with David Tomlinson, Wilfrid Hyde-White, David Lodge and Lionel Jeffries as a chief petty officer in Val Guest's "Up the Creek". Guest later claimed that he had written and directed the film as a vehicle for Sellers, and thus had started Sellers's film career. To practice his voice, Sellers purchased a reel-to-reel tape recorder. The film received critical acclaim in the United States and Roger Lewis viewed it as an important practice ground for Sellers. Next, Sellers featured with Terry-Thomas as one of a pair of comic villains in George Pal's "tom thumb" (1958), a musical fantasy film, opposite Russ Tamblyn, Jessie Matthews and Peter Butterworth. Terry-Thomas later said that "my part was perfect, but Peter's was bloody awful. He wasn't difficult about it, but he knew it". The performance was a major landmark in Sellers's career and became his first contact with the Hollywood film industry.

Sellers released his first studio album in 1958 called "The Best of Sellers"; a collection of sketches and comic songs, which were undertaken in a variety of comic characters. Produced by George Martin and released on Parlophone, the album reached number three in the UK Albums Chart; The same year, Sellers made his first film with John and Roy Boulting in "Carlton-Browne of the F.O.", a comedy in which he played a supporting role for the film's lead, Terry-Thomas. Before the release of that film, the Boultings, along with Sellers and Thomas in the cast, started filming "I'm All Right Jack", which became the highest grossing film at the British box office in 1960. In preparation for his role as Fred Kite, Sellers watched footage of union officials. The role earned him a BAFTA, and the critic for "The Manchester Guardian" believed it was Sellers's best screen performance to date.

In between "Carlton-Browne of the F.O." and "I'm All Right Jack", Sellers starred in "The Mouse That Roared", a film in which Jean Seberg also appeared, and was directed by Jack Arnold. He played three leading and distinct roles: the elderly Grand Duchess, the ambitious Prime Minister and the innocent and clumsy farm boy selected to lead an invasion of the United States. The film received universal and high praise by critics.

After completing "I'm All Right Jack", Sellers returned to record a new series of "The Goon Show". Over the course of two weekends, he took his 16mm cine-camera to Totteridge Lane in London and filmed himself, Spike Milligan, Mario Fabrizi, Leo McKern and Richard Lester. Originally intended as a private film, the eleven-minute short film "The Running Jumping & Standing Still Film" was screened at the 1959 Edinburgh and San Francisco film festivals. It won the award for best fiction short in the latter festival, and received an Academy Award nomination for Best Short Subject (Live Action). In 1959 Sellers released his second album, "Songs For Swinging Sellers", which—like his first record—reached number three in the UK Albums Chart. Sellers's last film of the fifties was "The Battle of the Sexes"; a comedy directed by Charles Crichton.

In 1960 Sellers portrayed an Indian doctor, Dr Ahmed el Kabir in Anthony Asquith's romantic comedy "The Millionairess", a film based on a George Bernard Shaw play of the same name. Sellers was not interested in accepting the role until he learned that Sophia Loren was to be his co-star. When asked about Loren, he explained to reporters "I don't normally act with romantic, glamorous women ... she's a lot different from Harry Secombe." Sellers and Loren developed a close relationship during filming, culminating in Sellers declaring his love for her in front of his wife. Sellers also woke his son at night to ask: "Do you think I should divorce your mummy?" Roger Lewis observed that Sellers immersed himself completely in the characters he enacted during productions, that "he'd play a role as an Indian doctor, and for the next six months, he'd be an Indian in his real [daily] life." The film inspired the George Martin-produced novelty hit single "Goodness Gracious Me", with Sellers and Loren, which reached number four in the UK Singles Chart in November 1960. A follow-up single by the duo, "Bangers and Mash", reached number 22 in the UK chart. The songs were included on an album released by the couple, "Peter & Sophia", which reached number five in the UK Albums Chart.

In 1961 Sellers made his directorial debut with "Mr. Topaze", in which he also starred. The film was based on the Marcel Pagnol play "Topaze". Sellers portrayed an ex-schoolmaster in a small French town who turns to a life of crime to obtain wealth. The film and Sellers's directorial abilities received an unenthusiastic response from the public and critics alike, and Sellers rarely referred to it again. The same year he starred in the Sidney Gilliat-directed "Only Two Can Play", a film based on the novel "That Uncertain Feeling" by Kingsley Amis. He was nominated for the Best British Actor award at the 16th British Academy Film Awards for his role as John Lewis, a frustrated Welsh librarian whose affections swing between the glamorous Liz (Mai Zetterling), and his long-suffering wife Jean (Virginia Maskell).

In 1962 Sellers played a retired British army general in John Guillermin's "Waltz of the Toreadors", based on the play of the same name. The film was widely criticised for its slapstick cinematic adaption, and director Guillermin himself considered the film an "amateurish" effort. However, Sellers won the San Sebastián International Film Festival Award for Best Actor and a BAFTA award nomination for his performance, and it was well received by the critics. Stanley Kubrick asked Sellers to play the role of Clare Quilty in the 1962 film "Lolita", opposite James Mason and Shelley Winters. Kubrick had seen Sellers in "The Battle of the Sexes" and listened to the album "The Best of Sellers", and was impressed by the range of characters he could portray. Sellers was apprehensive about accepting the role, doubting his ability to successfully portray the part of a flamboyant American television playwright who was according to Sellers "a fantastic nightmare, part homosexual, part drug addict, part sadist". Kubrick encouraged Sellers to improvise and stated that he would often reach a "state of comic ecstasy". Kubrick had American jazz producer Norman Granz record portions of the script for Sellers to listen to, so he could study the voice and develop confidence, granting Sellers a free artistic licence. Sellers later claimed that his relationship with Kubrick became one of the most rewarding of his career. Writing in "The Sunday Times", Dilys Powell noted that Sellers gave "a firework performance, funny, malicious, only once for a few seconds overreaching itself, and in the murder scene which is both prologue and epilogue achieving the macabre in comedy". Towards the end of 1962, Sellers appeared in "The Dock Brief", a legal satire directed by James Hill and co-starring Richard Attenborough.

Sellers's behaviour towards his family worsened in 1962; according to his son Michael, Sellers asked him and his sister Sarah "who we love more, our mother or him. Sarah, to keep the peace, said, 'I love you both equally'. I said, 'No, I love my mum.'" This prompted Sellers to throw both children out, saying that he never wanted to see them again. At the end of 1962, his marriage to Anne broke down. In 1963, Sellers starred as gang leader "Pearly Gates" in Cliff Owen's "The Wrong Arm of the Law", followed by his portrayal of a vicar in "Heavens Above!"

After his father's death in October 1962, Sellers decided to leave England and was approached by director Blake Edwards who offered him the role of Inspector Clouseau in "The Pink Panther", after Peter Ustinov had backed out of the film. Edwards later recalled his feelings as "desperately unhappy and ready to kill, but as fate would have it, I got Mr. Sellers instead of Mr. Ustinov—thank God!" Sellers accepted a fee of £90,000 (£ in pounds) for five weeks' work on location in Rome and Cortina. The film starred David Niven in the principal role, with two other actors—Capucine and Claudia Cardinale—having more prominent roles than Sellers. However, Sellers's performance is regarded as being on par with that of Charlie Chaplin and Buster Keaton, according to biographer Peter Evans. Although the Clouseau character was in the script, Sellers created the personality, devising the costume, accent, make-up, moustache and trench coat.

"The Pink Panther" was released in the UK in January 1964 and received a mixed reception from the critics, although Penelope Gilliatt, writing in "The Observer", remarked that Sellers had a "flawless sense of mistiming" in a performance that was "one of the most delicate studies in accident-proneness since the silents". Despite the views of the critics, the film was one of the top ten grossing films of the year. The role earned Sellers a nomination for the Golden Globe Award for Best Actor – Motion Picture Musical or Comedy at the 22nd Golden Globe Awards, and for a Best British Actor award at the 18th British Academy Film Awards.

In 1963, Stanley Kubrick cast Sellers to appear in "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb" alongside George C. Scott, Sterling Hayden, Keenan Wynn and Slim Pickens. Sellers and Kubrick got on famously during the film's production and had the greatest of respect for each other, also sharing a love of photography. The director asked Sellers to play three roles: US President Merkin Muffley, Dr. Strangelove and Group Captain Lionel Mandrake of the RAF. Sellers was initially hesitant about taking on these divergent characters, but Kubrick prevailed. According to some accounts, Sellers was also invited to play the part of General Buck Turgidson, but turned it down because it was too physically demanding. Kubrick later commented that the idea of having Sellers in so many of the film's key roles was that "everywhere you turn there is some version of Peter Sellers holding the fate of the world in his hands". Sellers was especially anxious about successfully enacting the role of Kong and accurately affecting a Texan accent. Kubrick requested screenwriter Terry Southern to record in his natural accent a tape of Kong's lines. After practising with Southern's recording, Sellers got sufficient control of the accent, and started shooting the scenes in the aeroplane. After the first day's shooting, Sellers sprained his ankle while leaving a restaurant and could no longer work in the cramped cockpit set. Kubrick then re-cast Slim Pickens as Kong. The three roles Sellers undertook were distinct, "variegated, complex and refined", and critic Alexander Walker considered that these roles "showed his genius at full stretch". Sellers played Muffley as a bland, placid intellectual in the mould of Adlai Stevenson; he played Mandrake as an unflappable Englishman; and Dr. Strangelove, a character influenced by pre-war German cinema, as a wheelchair-bound fanatic. The critic for "The Times" wrote that the film includes, "three remarkable performances from Mr. Peter Sellers, masterly as the President, diverting as a revue-sketch ex-Nazi US Scientist ... and acceptable as an RAF officer, although the critic from "The Guardian" thought his portrayal of the RAF officer alone was, "worth the price of an admission ticket". For his performance in all three roles, Sellers was nominated for an Academy Award for Best Actor at the 37th Academy Awards, and the Best British Actor award at the 18th British Academy Film Awards.

Between November 1963 and February 1964, Sellers began filming "A Shot in the Dark", an adaptation of a French play, "L'Idiote" by Marcel Achard. Sellers found the part and the director, Anatole Litvak, uninspiring; the producers brought in Blake Edwards to replace Litvak. Together with writer William Peter Blatty, they turned the script into a Clouseau comedy, also adding Herbert Lom as Commissioner Dreyfus and Burt Kwouk as Cato. During filming, Sellers's relationship with Edwards became strained; the two would often stop speaking to each other during filming, communicating only by the passing of notes. Sellers's personality was described by others as difficult and demanding, and he often clashed with fellow actors and directors. Upon its release in late June 1964, Bosley Crowther noted the "joyously free and facile way" in which Sellers had developed his comedy technique.

Towards the end of filming, in early February 1964, Sellers met Britt Ekland, a Swedish actress who had arrived in London to film "Guns at Batasi". On 19 February 1964, just ten days after their first meeting, the couple married. Sellers soon showed signs of insecurity and paranoia; he would become highly anxious and jealous, for example, when Ekland starred opposite attractive men. Shortly after the wedding, Sellers started filming on location in Twentynine Palms, California for Billy Wilder's "Kiss Me, Stupid", opposite Dean Martin and Kim Novak. The relationship between Wilder and Sellers became strained; both had different approaches to work and often clashed as a result. On the night of 5 April 1964, prior to having sex with Ekland, Sellers inhaled amyl nitrites (poppers) as a sexual stimulant in his search for "the ultimate orgasm", and suffered a series of eight heart attacks over the course of three hours as a result. His illness forced him to withdraw from the filming of "Kiss Me, Stupid" and he was replaced by Ray Walston. Wilder was unsympathetic about the heart attacks, saying that "you have to have a heart before you can have an attack".

After some time recovering, Sellers returned to filming in October 1964, playing King of the Individualists alongside Ekland in "A Carol for Another Christmas", a feature-length United Nations special broadcast in the United States on the ABC channel on 28 December 1964. Sellers had been concerned that his heart attacks might have caused brain damage and that he would be unable to remember his lines, but he was reassured that his memory and abilities were unimpaired after the experience of filming. Sellers followed this with the role of the perverted Austrian psychoanalyst Doctor Fritz Fassbender in Clive Donner's "What's New Pussycat?", appearing alongside Peter O'Toole, Romy Schneider, Capucine, Paula Prentiss and Ursula Andress. The film was the first screenwriting and acting credit for Woody Allen, and featured Sellers in a love triangle. Because of Sellers's poor health, producer Charles K. Feldman insured him at a cost of $360,000 ($ in dollars).

Sellers became a close friend of Antony Armstrong-Jones, 1st Earl of Snowdon, a photographer who was then married to Princess Margaret. Snowdon shared a love of women, photography, fine wine and fast cars with Sellers; both were also prone to bouts of depression. They spent many weekends together with their wives and went on several holidays on board Sellers's yacht "Bobo" in Sardinia. On 20 January 1965, Sellers and Ekland announced the birth of a daughter, Victoria. They moved to Rome in May to film "After the Fox", an Anglo-Italian production in which they were both to appear. The film was directed by Vittorio De Sica, whose English Sellers struggled to understand. Sellers attempted to have De Sica fired, causing tensions on the set. Sellers also became unhappy with his wife's performance, straining their relationship and triggering open arguments during one of which Sellers threw a chair at Ekland. Despite these conflicts, the script was praised for its wit.

Following the commercial success of "What's New Pussycat?", Charles Feldman again brought together Sellers and Woody Allen for his next project, "Casino Royale", which also starred Orson Welles; Sellers signed a $1 million contract for the film ($ in dollars). Seven screenwriters worked on the project, and filming was chaotic. To make matters worse, according to Ekland, Sellers was "so insecure, he won't trust anyone". A poor working relationship quickly developed between Sellers and Welles: Sellers eventually demanded that the two should not share the same set. Sellers left the film before his part was complete. A further agent's part was then written for Terence Cooper, to cover Sellers's departure.

Shortly after leaving "Casino Royale", Sellers was appointed a Commander of the Most Excellent Order of the British Empire (CBE) in honour of his career achievements. The day before the investiture at Buckingham Palace, Sellers and Ekland argued, with Ekland scratching his face in the process; Sellers had a make-up artist cover the marks. Ekland later reported that although the couple argued, Sellers never hit her. During his next film, "The Bobo", which again co-starred Ekland, the couple's marital problems worsened. Three weeks into production in Italy, Sellers told director Robert Parrish to fire his wife, saying "I'm not coming back after lunch if that bitch is on the set". Ekland later stated that the marriage was "an atrocious sham" at this stage. In the midst of filming "The Bobo", Sellers's mother had a heart attack; Parrish asked Sellers if he wanted to visit her in hospital, but Sellers remained on set. She died within days, without Sellers having seen her. He was deeply affected by her death and remorseful at not having returned to London to see her. Ekland served him with divorce papers shortly afterwards. The divorce was finalised on 18 December 1968, and Sellers's friend Spike Milligan sent Ekland a congratulatory telegram. Upon its release in September 1967, "The Bobo" was poorly received.

Sellers's first film appearance of 1968 was a reunion with Blake Edwards for the fish-out-of-water comedy "The Party", in which he starred alongside Claudine Longet and Denny Miller. He appears as Hrundi V. Bakshi, a bungling Indian actor who accidentally receives an invitation to a lavish Hollywood dinner party. His character, according to Sellers's biographer Peter Evans, was "clearly an amalgam of Clouseau and the doctor in "The Millionairess"". Roger Lewis notes that like a number of Sellers's characters, he is played in a sympathetic and dignified manner. He followed it later that year with Hy Averback's "I Love You, Alice B. Toklas", playing an attorney who abandons his lifestyle to become a hippie. Roger Ebert of the "Chicago Sun-Times" gave the film three stars, remarking that Sellers was "back doing what he does best", although he also said that in Sellers's previous films he had "been at his worst recently".

In 1969 Sellers starred opposite Ringo Starr in the Joseph McGrath-directed film "The Magic Christian". Sellers portrayed Sir Guy Grand, an eccentric billionaire who plays elaborate practical jokes on people. The critic Irv Slifkin remarked that the film was a reflection of the cynicism of Peter Sellers, describing the film as a "proto-Pythonesque adaption of Terry Southern's semi-free-form short novel", and "one of the strangest films to be shown at a gala premiere for Britain's royal family". The film, a satire on human nature, was in general viewed negatively by critics. Roger Greenspun of "The New York Times" believed that the film was of variable quality and summarised it as a "brutal satire".

After a cameo appearance in "A Day at the Beach" (1970), and a serious role later in 1970 as an ageing businessman who seduces Sinéad Cusack in "Hoffman", Sellers starred in Roy Boulting's "There's a Girl in My Soup" opposite Goldie Hawn. According to "The Times", the film was a major commercial success and became the seventh most popular film at the British box office in 1970. Andrew Spicer, writing for the British Film Institute's Screenonline, considers that although Sellers favoured playing romantic roles, he "was always more successful in parts that sent up his own vanities and pretensions, as with the TV presenter and narcissistic lothario he played in "There's a Girl in My Soup"". The film was seen as a small revival of his career. However, Sellers's next films, including Rodney Amateau's "Where Does It Hurt?" (1972) and Peter Medak's "Ghost in the Noonday Sun" (1974), were again poorly received, and his acting was viewed as frenetic rather than funny. Despite these setbacks, Sellers won the Best Actor award at the 1973 Tehran Film Festival for his tragi-comedic role as a street performer in Anthony Simmons's "The Optimists of Nine Elms". Fellow comedian and friend Spike Milligan believed that the early 1970s were for Sellers "a period of indifference, and it would appear at one time that his career might have come to a conclusion". This was echoed by Sellers's biographer, Peter Evans, who notes that out of nine films in the period, three were never released and five had flopped, while only "There's a Girl in My Soup" had been a success. In his private life, he had been seeing the twenty-three-year-old model Miranda Quarry. The couple married on 24 August 1970, despite Sellers's private doubts—expressed to his agent, Dennis Selinger—about his decision to re-marry.

On 20 April 1972, Sellers reunited with Milligan and Harry Secombe to record "The Last Goon Show of All", which was broadcast on 5 October. In May 1973, with his third marriage failing, Sellers went to the theatre to watch Liza Minnelli perform. He became entranced with Minnelli and the couple became engaged three days later, despite Minnelli's current betrothal to Desi Arnaz, Jr., and Sellers still being married. Their relationship lasted a month before breaking up. By 1974, Sellers's friends were concerned that he was having a nervous breakdown. Directors John and Roy Boulting considered that Sellers was "a deeply troubled man, distrustful, self-absorbed, ultimately self-destructive. He was the complete contradiction." Sellers was shy and insecure when out of character. When he was invited to appear on Michael Parkinson's eponymous chat show in 1974, he withdrew the day before, explaining to Parkinson that "I just can't walk on as myself". When he was told he could come on as someone else, he appeared dressed as a member of the Gestapo. After a few lines in keeping with his assumed character, he stepped out of the role and settled down and, according to Parkinson himself, "was brilliant, giving the audience an astonishing display of his virtuosity". In 1974, Sellers again claimed to have communicated with the long-dead music hall comic Dan Leno, who advised him to return to the role of Clouseau.

In 1974, Sellers portrayed a "sexually voracious" Queen Victoria in Joseph McGrath's comedic biographical film of the Scottish poet William McGonagall, "The Great McGonagall", starring opposite Milligan and Julia Foster. However, the film was a critical failure, and Sellers's career and life reached an all-time low. As a result, by 1974 he agreed to accept salaries of £100,000 and 10 per cent of the gross to appear in TV productions and advertisements, well below the £1 million he had once commanded per film. In 1973, he appeared in a Benson & Hedges cinema commercial; in 1975, he appeared in a series of advertisements for Trans World Airlines, in which he played several eccentric characters, including Thrifty McTravel, Jeremy "Piggy" Peak Thyme and an Italian singer, Vito. Biographer Michael Starr asserts that Sellers showed enthusiasm towards these roles, although the airline campaign failed commercially.

A turning point in Sellers's flailing career came in 1974, when he teamed up with Blake Edwards to make "The Return of the Pink Panther", starring alongside Christopher Plummer, Herbert Lom and Catherine Schell. The film was shot on a budget of £3 million and earned $33 million at the box office upon release in May 1975, reinvigorating Sellers's career as an A-list film star and restoring his millionaire status. The film earned Sellers a nomination for the Best Actor – Musical or Comedy award at the 33rd Golden Globe Awards. In 1976, he followed it with "The Pink Panther Strikes Again". During the filming from February to June 1976, the already fraught relationship between Sellers and Blake Edwards had seriously deteriorated. Edwards says of the actor's mental state at the time of "The Pink Panther Strikes Again", "If you went to an asylum and you described the first inmate you saw, that's what Peter had become. He was certifiable." With declining physical health, Sellers could at times be unbearable on set. His behaviour was regarded as unprofessional and childish, and he frequently threw tantrums, often threatening to abandon projects. Peter Evans mentioned that Sellers was a "volatile and perplexing character [who] left a trail of misery in his private life". He also noted that Sellers had a "compulsive personality and [was] an eccentric hypochondriac" who became addicted to various medicines aside from his recreational drug habits during this period. His difficult behaviour during productions was widely reported and made it more difficult for Sellers to get employment in the industry at a time when he most needed the work. Despite Sellers's deep personal problems, "The Pink Panther Strikes Again" was well received critically. Vincent Canby of "The New York Times" said of Sellers in the film, "There is, too, something most winningly seedy about Mr. Sellers' Clouseau, a fellow who, when he attempts to tear off his clothes in the heat of passion, gets tangled up in his necktie, and who, when he masquerades—for reasons never gone into—as Quasimodo, overinflates his hump with helium." Sellers's performance earned him a further nomination at the 34th Golden Globe Awards.

In March 1976 Sellers began dating actress Lynne Frederick, whom he married on 18 February 1977. Biographer Roger Lewis documents that of all of Sellers's wives, Frederick was the most poorly treated; Julian Upton likened it to a boxing match between a heavyweight and a featherweight, a relationship that "oscillated from ardour to hatred, reconciliation and remorse." Peter Evans claims that Milligan detested his friend's choice of partner and believed she was to blame for his increasing alcohol and cocaine dependency. On 20 March 1977, Sellers suffered a second major heart attack during a flight from Paris to London; he was subsequently fitted with a pacemaker. Sellers returned from his illness to undertake "Revenge of the Pink Panther"; although it was a commercial success, the critics were tiring of Inspector Clouseau. Julian Upton expressed the view that the strain behind the scenes began to manifest itself in the sluggish pace of the film, describing it as a "laboured, stunt-heavy hotchpotch of half-baked ideas and rehashed gags". Sellers too had become tired of the role, saying after production, "I've honestly had enough of Clouseau—I've got nothing more to give". Steven Bach, the senior vice-president and head of worldwide productions for United Artists, who worked with Sellers on "Revenge of the Pink Panther", considered that Sellers was "deeply unbalanced, if not committable: that was the source of his genius and his truly quite terrifying aspects as manipulator and hysteric." He refused to seek professional help for his mental issues. Sellers would claim that he had no personality and was almost unnoticeable, which meant that he "needed a strongly defined character to play." He would make similar references throughout his life: when he appeared on "The Muppet Show" in 1978, a guest appearance that earned him an Emmy nomination for Outstanding Continuing or Single Performance by a Supporting Actor in Variety or Music, he chose not to appear as himself, instead appearing in a variety of costumes and accents. When Kermit the Frog told Sellers he could relax and be himself, Sellers replied:

In 1979, Sellers starred alongside Lynne Frederick, Lionel Jeffries and Elke Sommer in Richard Quine's "The Prisoner of Zenda". He portrayed three roles, including King Rudolf IV and King Rudolf V—rulers of the fictional small nation of Ruritania—and Syd Frewin, Rudolf V's half-brother. Upon its release in May 1979, the film was well received; Janet Maslin of "The New York Times" observed how Sellers divided "his energies between a serious character and a funny one, but that it was his serious performance which was more impressive". However, Philip French, for "The Observer", was unimpressed by the film, describing it as "a mess of porridge" and stating that "Sellers reveals that he cannot draw the line between the sincere and the sentimental".

Later in 1979, Sellers starred opposite Shirley MacLaine, Melvyn Douglas and Jack Warden in the black comedy "Being There" as Chance, a mindless, emotionless gardener addicted to watching TV. In a BBC interview in 1971, Sellers had said that more than anything else, he wanted to play the role, and successfully persuaded the author of the book, Jerzy Kosinski, to allow him and director Hal Ashby to make the film, provided he could write the script. During filming, to remain in character, Sellers refused most interview requests and kept his distance from the other actors. Sellers considered Chance's walking and voice the character's most important attributes, and in preparing for the role he worked alone with a tape recorder or with his wife, and then with Ashby, to perfect the clear enunciation and flat delivery needed to reveal "the childlike mind behind the words". Sellers described his experience of working on the film as "so humbling, so powerful", and co-star Shirley MacLaine found Sellers "a dream" to work with. Sellers's performance was universally lauded by critics and is considered by critic Danny Smith to be the "crowning triumph of Peter Sellers's remarkable career". Critic Frank Rich wrote that the acting skill required for this sort of role, with a "schismatic personality that Peter had to convey with strenuous vocal and gestural technique ... A lesser actor would have made the character's mental dysfunction flamboyant and drastic ... [His] intelligence was always deeper, his onscreen confidence greater, his technique much more finely honed": in achieving this, Sellers "makes the film's fantastic premise credible". The film earned Sellers a Best Actor award at the 51st National Board of Review Awards; the London Critics Circle Film Awards Special Achievement Award, the Best Actor award at the 45th New York Film Critics Circle Awards; and the Best Actor – Musical or Comedy award at the 37th Golden Globe Awards. Additionally, Sellers was nominated for the Best Actor award at the 52nd Academy Awards and the Best Actor in a Leading Role award at the 34th British Academy Film Awards.

In March 1980 Sellers asked his fifteen-year-old daughter Victoria what she thought about "Being There": she reported later that, "I said yes, I thought it was great. But then I said, 'You looked like a little fat old man'. ... he went mad. He threw his drink over me and told me to get the next plane home." His other daughter Sarah told Sellers her thoughts about the incident and he sent her a telegram that read "After what happened this morning with Victoria, I shall be happy if I never hear from you again. I won't tell you what I think of you. It must be obvious. Goodbye, Your Father."

Sellers's last film was "The Fiendish Plot of Dr. Fu Manchu", a comedic re-imagining of the eponymous adventure novels by Sax Rohmer; Sellers played both police inspector Nayland Smith and Fu Manchu, alongside Helen Mirren and David Tomlinson. The production of the film was troublesome before filming started, with two directors—Richard Quine and John Avildsen—fired before the script had been completed. Sellers also expressed dissatisfaction with his own portrayal of Manchu with his ill-health often causing delays. Arguments between Sellers and director Piers Haggard led to Haggard's firing at Sellers's instigation and Sellers took over direction, using his long-time friend David Lodge to direct some sequences. Tom Shales of "The Washington Post" described the film as "an indefensibly inept comedy", adding that "it is hard to name another good actor who ever made so many bad movies as Sellers, a comedian of great gifts but ferociously faulty judgment. "Manchu" will take its rightful place alongside such colossally ill-advised washouts as "Tell Me Where It Hurts", "The Bobo" and "The Prisoner of Zenda"".

Sellers's final performances were a series of advertisements for Barclays Bank. Filmed in April 1980 in Ireland, he played Monty Casino, a Jewish con-man. Four adverts were scheduled, but only three were filmed as Sellers collapsed in Dublin, again with heart problems. After two days in care—and against the advice of his doctors—he travelled to the Cannes Film Festival, where "Being There" was in competition. Sellers was again ill in Cannes, returning to his residence in Gstaad to work on the script for his next project, "Romance of the Pink Panther". He agreed to undergo an angiogram at the Cedars-Sinai Medical Center in Los Angeles, to see if he was able to undergo open-heart surgery. Spike Milligan later considered that Sellers's heart condition had lasted fifteen years and had "made life difficult for him and had a debilitating effect on his personality." Sellers's fourth marriage to Frederick collapsed soon after.

Sellers had recently started to rebuild his relationship with his son Michael after the failure of the latter's marriage. Michael later said that "it marked the beginning of an all-too-brief closeness between us." Sellers admitted to his son that "he hated so many things he had done," including leaving his first wife, Anne, and his infatuation with Sophia Loren. In lighter moments, Sellers had joked that his epitaph should read "Star of stage, screen and alimony."

On 21 July 1980 Sellers arrived in London from Geneva. He checked into the Dorchester hotel, before visiting Golders Green Crematorium for the first time to see the location of his parents' ashes. He had plans to attend a reunion dinner with his "Goon Show" partners Spike Milligan and Harry Secombe, scheduled for the evening of 22 July. On the day of the dinner, Sellers took lunch in his hotel suite and shortly afterwards collapsed from a heart attack. He was taken to the Middlesex Hospital, London, and died just after midnight on 24 July 1980, aged 54.

Following Sellers's death, fellow actor Richard Attenborough said that Sellers "had the genius comparable to Chaplin", while the Boulting brothers considered Sellers as "a man of enormous gifts; and these gifts he gave to the world. For them, he is assured of a place in the history of art as entertainment." Burt Kwouk, who appeared as Cato in the "Pink Panther" films, stated that "Peter was a well-loved actor in Britain ... the day he died, it seemed that the whole country came to a stop. Everywhere you went, the fact that Peter had died seemed like an umbrella over everything". Director Blake Edwards thought that "Peter was brilliant. He had an enormous facility for finding really unusual, unique facets of the character he was playing". Sellers's friend and "Goon Show" colleague Spike Milligan was too upset to speak to the press at the time of Sellers's death, while fellow Goon Harry Secombe said "I'm shattered. Peter was such a tremendous artist. He had so much talent, it just oozed out of him"; in dark humour, referring to the missed dinner the Goons had planned, he added, "Anything to avoid paying for dinner". Secombe later declared to journalists "Bluebottle is deaded now". Milligan later said that "it's hard to say this, but he died at the right time." The "Daily Mail" described Sellers as "the greatest comic talent of his generation as well as a womanising drug-taker who married four times in a fruitless search for happiness", a "flawed genius" who, once he latched on to a comic idea, "loved nothing more than to carry it to extremes".

A private funeral service was held at Golders Green Crematorium on 26 July, conducted by Sellers's old friend, Canon John Hester. Sellers's final joke was the playing of "In the Mood" by Glenn Miller, a tune he hated. His body was cremated and his ashes were interred at the Crematorium. After her death in 1994, the ashes of his widow Lynne Frederick were co-interred with his. A memorial service was held at St Martin-in-the-Fields on 8 September 1980—what would have been Sellers's 55th birthday. Close friend Lord Snowdon read the twenty-third Psalm, Harry Secombe sang "Bread of Heaven" and the eulogy was read by David Niven.

Although Sellers was reportedly in the process of excluding Frederick from his will a week before he died, she inherited almost his entire estate worth an estimated £4.5 million (£ million in pounds) while his children received £800 each (£ in pounds). Spike Milligan appealed to her on behalf of Sellers's three children, but she refused to increase the amount. Sellers's only son, Michael, died of a heart attack at 52 during surgery on 24 July 2006, twenty-six years to the day after his father's death.

After his death, Metro-Goldwyn-Mayer tried to continue with "Romance of the Pink Panther" and offered the role of Clouseau to Dudley Moore, who turned it down. The studio subsequently returned to Blake Edwards, who was adamant not to recast the character, feeling certain that no one could adequately replace Sellers. In 1982 Edwards released "Trail of the Pink Panther", which was composed entirely of deleted scenes from his past three "Panther" films. Frederick sued, claiming the use of the clips was a breach of contract; the court awarded her $1million ($ million today), plus 3.15 per cent of the film's profits and 1.36 per cent of its gross revenue.

Vincent Canby of "The New York Times" said of the Pink Panther films, "I'm not sure why Mr. Sellers and Mr. Lom are such a hilarious team, though it may be because each is a fine comic actor with a special talent for portraying the sort of all-consuming, epic self-absorption that makes slapstick farce initially acceptable—instead of alarming—and finally so funny." Film critic Elvis Mitchell has said that Sellers was one of the few comic geniuses who was able to truly hide behind his characters, giving the audience no sense of what he was really like in real life. A feature of the characterisations undertaken by Sellers is that, regardless of how clumsy or idiotic they are, he ensured that they always retain their dignity. On his playing of Clouseau, Sellers said: "I set out to play Clouseau with great dignity because I feel that he thinks he is probably one of the greatest detectives in the world. The original script makes him out to be a complete idiot. I thought a forgivable vanity would humanise him and make him kind of touching." His biographer, Ed Sikov, notes that because of this retained dignity, Sellers is "the master of playing men who have no idea how ridiculous they are." Social historian Sam Wasson notes the complexity in Sellers's performances in the "Pink Panther" films, which has the effect of alienating Clouseau from his environment. Wesson considers that "As 'low' and 'high' comedy rolled into one, it's the performative counterpoint to Edwardian sophisticated naturalism". This combination of "high" and "low", exemplified by Clouseau's attempting to retain dignity after a fall, means that within the film Clouseau was "the sole representative of humanity". Film critic Dilys Powell also saw the inherent dignity in the parts and wrote that Sellers had a "balance between character and absurdity". Richard Attenborough also thought that because of his sympathy, Sellers could "inject into his characterisations the frailty and substance of a human being".

Author Aaron Sultanik observed that in Sellers's early films, such as "I'm All Right Jack", he displays "deft, technical interpretations [that] pinpoint the mechanical nature of his comic characterization", which "... reduces each of his characters to a series of gross, awkward tics". Academic Cynthia Baron observed that Sellers's external characterisations led to doubt with reviewers as to whether Sellers's work was "true" acting. Critic Tom Milne saw a change over Sellers's career and thought that his "comic genius as a character actor was ... stifled by his elevation to leading man" and his later films suffered as a result. Sultanik agreed, commenting that Sellers's "exceptional vocal and physical technique" was under-used during his career in the US.

Academics Maria Pramaggiore and Tom Wallis remarked that Sellers fits the mould of a technical actor because he displays a mastery of physical characterisation, such as accent or physical trait. Writer and playwright John Mortimer saw the process for himself when Sellers was about to undertake filming on Mortimer's "The Dock Brief" and could not decide how to play the character of the barrister. By chance he ordered cockles for lunch and the smell brought back a memory of the seaside town of Morecambe: this gave him "the idea of a faded North Country accent and the suggestion of a scrappy moustache". So important was the voice as the starting point for character development, that Sellers would walk around London with a reel-to-reel tape recorder, recording voices to study at home.

"New York" magazine stated that all of the films starring Sellers as Clouseau showcased his "comedic brilliance". Sellers's friend and "Goon Show" colleague Spike Milligan said that Sellers "had one of the most glittering comic talents of his age", while John and Roy Boulting noted that he was "the greatest comic genius this country has produced since Charles Chaplin". Irv Slifkin said that the most prominent albeit ever-changing face in comedies of the sixties was Sellers who "changed like a chameleon throughout the era, dazzling audiences". In a 2005 poll to find "The Comedian's Comedian", Sellers was voted 14 in the list of the top 20 greatest comedians by fellow comics and comedy insiders. Sellers and "The Goon Show" were a strong influence on the Monty Python performers, as well as on Peter Cook, who described Sellers as "the best comic actor in the world". The British actor Stephen Mangan stated that Sellers was a large influence, as did the comedians Alan Carr and Rob Brydon. The comic performer Sacha Baron Cohen referred to Sellers as "the most seminal force in shaping [his] early ideas on comedy". Cohen was considered for the role of Sellers in the biographical film "The Life and Death of Peter Sellers". The three members of Spinal Tap—Michael McKean, Christopher Guest and Harry Shearer—have also cited Sellers as being an influence on them, as has the American talk-show host Conan O'Brien. David Schwimmer is another whose approach was influenced by Sellers: "he could do anything, from Dr Strangelove to Inspector Clouseau. He was just amazing." Eddie Izzard notes that the Goons "influenced a new generation of comedians who came to be known as 'alternative'"—including himself, while the media historian Graham McCann states "the anarchic spirit of the Goon Show ... would inspire, directly or indirectly and to varying extents, ... "The Hitchhiker's Guide to the Galaxy", "The Young Ones", "Vic Reeves Big Night Out", "The League of Gentlemen" [and] "Brass Eye"."

The stage play "Being Sellers" premiered in Australia in 1998, three years after the release of the biography by Roger Lewis, "The Life and Death of Peter Sellers". In 2004, the book was turned into an HBO film with the same title, starring Geoffrey Rush. The play later transferred to New York in December 2010. "The Belfast Telegraph" notes how the film captured Sellers's "life of drugs, drink, fast cars and lots and lots of beautiful women". Although the film was widely praised by critics, both Lord Snowdon and Britt Ekland were highly critical of the film and the enactment of Sellers; Ekland believed that the film left the audiences with the wrong idea of her former husband, saying "the film leaves you with the impression that Peter Sellers was essentially a likeable man when in reality he was a monster. He may have been a brilliant actor, but as a human being he had no saving graces at all". Snowdon disagreed with Ekland's verdict, and with the film, and stated that Sellers "had a light touch, a sense of humour, I can't bear to see him portrayed as somebody who was apparently without either ... The man on the screen is charmless, humourless and boring—the one thing you could never say about Peter."

Selected works, based on award nominations



</doc>
<doc id="24519" url="https://en.wikipedia.org/wiki?curid=24519" title="Project Runeberg">
Project Runeberg

Project Runeberg (Swedish, "Projekt Runeberg") is a digital cultural archive initiative that publishes free electronic versions of books significant to the culture and history of the Nordic countries. Patterned after Project Gutenberg, it was founded by Lars Aronsson and colleagues at Linköping University and began archiving Nordic-language literature in December 1992. As of 2015 it had accomplished digitization to provide graphical facsimiles of old works such as the "Nordisk familjebok", and had accomplished, in whole or in part, the text extractions and copyediting of these as well as esteemed Latin works and English translations from Nordic authors, and sheet music and other texts of cultural interest.

Project Runeberg is a digital cultural archive initiative patterned after the English-language cultural initiative, Project Gutenberg; it was founded by Lars Aronsson and colleagues at Linköping University, especially within the university group Lysator (see below), with the aim of publishing free electronic versions of books significant to the culture and history of the Nordic countries. The Project began archiving its first Nordic-language literature pieces (parts of the "Fänrik Ståls Sägner", of Nordic dictionaries and of a Bible from 1917) in December 1992.

In its naming, a moniker similar to "Gutenberg" was desired. The Project was thereby given the name of Finland's national poet Johan Ludvig Runeberg, and so contained a further allusion based on the meanings of its component parts—"Rune" (letter in Runic script) and "berg" (mountain)—so that in most Nordic languages it can be translated loosely as "mountain of letters".

The Project began archiving Nordic-language literature in December 1992. As of 2015 it had accomplished digitization to provide graphical facsimiles of old works such as the "Nordisk familjebok", and had accomplished, in whole or in part, the text extractions and copyediting of these as well as esteemed Latin works and English translations from Nordic authors—e.g., Carl August Hagberg's interpretations of Shakespeare's plays—and sheet music and other texts of cultural interest.

By 2001, technology—image scanning and optical character recognition techniques—had improved enough to allow full digitization and text extraction of important target texts, e.g., of both print editions of the Nordisk familjebok (45,000 pages). Project Runeberg is hosted by an academic computer group, Lysator, at Linköping University, in Linköping in southern Sweden.



</doc>
<doc id="24521" url="https://en.wikipedia.org/wiki?curid=24521" title="Pico (text editor)">
Pico (text editor)

Pico (Pine composer) is a text editor for Unix and Unix-based computer systems. It is integrated with the Pine e-mail client, which was designed by the Office of Computing and Communications at the University of Washington.

From the Pine FAQ: "Pine's message composition editor is also available as a separate stand-alone program, called PICO. PICO is a very simple and easy-to-use text editor offering paragraph justification, cut/paste, and a spelling checker...".

Pico does not support working with several files simultaneously and cannot perform a find and replace across multiple files. It also cannot copy text from one file to another (though it is possible to read text into the editor from a file in its working directory). Pico does support search and replace operations.

By comparison, some popular Unix text editors such as vi and Emacs provide a wider range of features than Pico; including regular expression search and replace, and working with multiple files at the same time. By comparison, Pico's simplicity makes it suitable for beginners.

A clone of Pico called nano, which is part of the GNU Project, was developed because Pico's earlier license had unclear redistribution terms. Newer versions of Pico as part of Alpine are released under the Apache License.

Pico features a number of commands for editing. Arrow keys move the cursor a character at the time in the direction of the movement. Inserting a character is done by pressing the corresponding character key in the keyboard, while giving commands (such as save, spell check, justify, search, etc.) is done using a control key.

The CTRL-T command is used to spell check. The speller is defined from the command line using the -s option. When a person writes files in different languages, the speller can be set to be a script that interacts with the user to select the language to be spelled.

The CTRL-J command is used to left justify text. Text is flowed in each line of a paragraph up to a limit set with the -r option in the command line. If no limit is given in the command line, then a default value of 72 characters per line is used. This limit is used to wrap lines during composition, as well as to justify text. The CTRL-J command justifies the text in the paragraph that the cursor is placed on. The command CTRL-W CTRL-U is used to justify the full file. In case that justification is not done correctly, or by mistake, it can be undone by pressing the CTRL-U command immediately after justification has been done.

The CTRL-W command is used to search for text. Search is done case insensitively, The search and replace command is not available by default, but must be enabled through the -b option in the command line.

Moving inside the editor can be done using the keyboard by using the arrow keys. Keys such as Page Up, or Page Down, scroll the text up or down (towards the beginning or end of the file, respectively). The commands CTRL-W CTRL-Y, and CTRL-W CTRL-V move the cursor to the beginning or end of the file respectively, while the commands CTRL-A and CTRL-E move the cursor to the beginning and the end of the line that the cursor is located on.

The following command line options allow users to configure Pico before editing a file. This information can be obtained by starting Pico with the -h command. When Pico is invoked from Pine or Alpine some of the options below can be configured from their Setup Configuration Screen by either enabling a specific feature, or configuring a variable. Below is indicated the way to configure Pico from the command line, as well as how to configure it from Alpine. Possible starting arguments for the Pico editor are:

"All arguments may be followed by a file name to edit."

The options -dcs, -kcs and -syscs are not available in the Windows version of Pico. However, the Windows version of Pico also has four options (-cnf, -cnb, -crf, -crb) that are not available in unix versions of Pico; each option is defined as follows: -cnf for Color for Normal Foreground, -cnb for Color for Normal Background, -crf for Color for Reverse Foreground and -crf for Color for Reverse Background. Their possible values are black, red, green, yellow, blue, magenta, cyan, and white or a 3 digit number, such as 009, 064, or 137.




</doc>
<doc id="24522" url="https://en.wikipedia.org/wiki?curid=24522" title="Power law">
Power law

In statistics, a power law is a functional relationship between two quantities, where a relative change in one quantity results in a proportional relative change in the other quantity, independent of the initial size of those quantities: one quantity varies as a power of another. For instance, considering the area of a square in terms of the length of its side, if the length is doubled, the area is multiplied by a factor of four.

The distributions of a wide variety of physical, biological, and man-made phenomena approximately follow a power law over a wide range of magnitudes: these include the sizes of craters on the moon and of solar flares, the foraging pattern of various species, the sizes of activity patterns of neuronal populations, the frequencies of words in most languages, frequencies of family names, the species richness in clades of organisms, the sizes of power outages, criminal charges per convict, volcanic eruptions, human judgements of stimulus intensity and many other quantities. Few empirical distributions fit a power law for all their values, but rather follow a power law in the tail.
Acoustic attenuation follows frequency power-laws within wide frequency bands for many complex media. Allometric scaling laws for relationships between biological variables are among the best known power-law functions in nature.

One attribute of power laws is their scale invariance. Given a relation formula_1, scaling the argument formula_2 by a constant factor formula_3 causes only a proportionate scaling of the function itself. That is,

where formula_5 denotes direct proportionality. That is, scaling by a constant formula_3 simply multiplies the original power-law relation by the constant formula_7. Thus, it follows that all power laws with a particular scaling exponent are equivalent up to constant factors, since each is simply a scaled version of the others. This behavior is what produces the linear relationship when logarithms are taken of both formula_8 and formula_2, and the straight-line on the log–log plot is often called the "signature" of a power law. With real data, such straightness is a necessary, but not sufficient, condition for the data following a power-law relation. In fact, there are many ways to generate finite amounts of data that mimic this signature behavior, but, in their asymptotic limit, are not true power laws (e.g., if the generating process of some data follows a Log-normal distribution). Thus, accurately fitting and validating power-law models is an active area of research in statistics; see below.

A power-law formula_10 has a well-defined mean over formula_11 only if formula_12, and it has a finite variance only if formula_13; most identified power laws in nature have exponents such that the mean is well-defined but the variance is not, implying they are capable of black swan behavior. This can be seen in the following thought experiment: imagine a room with your friends and estimate the average monthly income in the room. Now imagine the world's richest person entering the room, with a monthly income of about 1 billion US$. What happens to the average income in the room? Income is distributed according to a power-law known as the Pareto distribution (for example, the net worth of Americans is distributed according to a power law with an exponent of 2).

On the one hand, this makes it incorrect to apply traditional statistics that are based on variance and standard deviation (such as regression analysis). On the other hand, this also allows for cost-efficient interventions. For example, given that car exhaust is distributed according to a power-law among cars (very few cars contribute to most contamination) it would be sufficient to eliminate those very few cars from the road to reduce total exhaust substantially.

The median does exist, however: for a power law "x", with exponent , it takes the value 2"x", where "x" is the minimum value for which the power law holds

The equivalence of power laws with a particular scaling exponent can have a deeper origin in the dynamical processes that generate the power-law relation. In physics, for example, phase transitions in thermodynamic systems are associated with the emergence of power-law distributions of certain quantities, whose exponents are referred to as the critical exponents of the system. Diverse systems with the same critical exponents—that is, which display identical scaling behaviour as they approach criticality—can be shown, via renormalization group theory, to share the same fundamental dynamics. For instance, the behavior of water and CO at their boiling points fall in the same universality class because they have identical critical exponents. In fact, almost all material phase transitions are described by a small set of universality classes. Similar observations have been made, though not as comprehensively, for various self-organized critical systems, where the critical point of the system is an attractor. Formally, this sharing of dynamics is referred to as universality, and systems with precisely the same critical exponents are said to belong to the same universality class.

Scientific interest in power-law relations stems partly from the ease with which certain general classes of mechanisms generate them. The demonstration of a power-law relation in some data can point to specific kinds of mechanisms that might underlie the natural phenomenon in question, and can indicate a deep connection with other, seemingly unrelated systems; see also universality above. The ubiquity of power-law relations in physics is partly due to dimensional constraints, while in complex systems, power laws are often thought to be signatures of hierarchy or of specific stochastic processes. A few notable examples of power laws are Pareto's law of income distribution, structural self-similarity of fractals, and scaling laws in biological systems. Research on the origins of power-law relations, and efforts to observe and validate them in the real world, is an active topic of research in many fields of science, including physics, computer science, linguistics, geophysics, neuroscience, sociology, economics and more.

However, much of the recent interest in power laws comes from the study of probability distributions: The distributions of a wide variety of quantities seem to follow the power-law form, at least in their upper tail (large events). The behavior of these large events connects these quantities to the study of theory of large deviations (also called extreme value theory), which considers the frequency of extremely rare events like stock market crashes and large natural disasters. It is primarily in the study of statistical distributions that the name "power law" is used.

In empirical contexts, an approximation to a power-law formula_14 often includes a deviation term formula_15, which can represent uncertainty in the observed values (perhaps measurement or sampling errors) or provide a simple way for observations to deviate from the power-law function (perhaps for stochastic reasons):

Mathematically, a strict power law cannot be a probability distribution, but a distribution that is a truncated power function is possible: formula_17 for formula_18 where the exponent formula_19 (Greek letter alpha, not to be confused with scaling factor formula_20 used above) is greater than 1 (otherwise the tail has infinite area), the minimum value formula_21 is needed otherwise the distribution has infinite area as "x" approaches 0, and the constant "C" is a scaling factor to ensure that the total area is 1, as required by a probability distribution. More often one uses an asymptotic power law – one that is only true in the limit; see power-law probability distributions below for details. Typically the exponent falls in the range formula_22, though not always.

More than a hundred power-law distributions have been identified in physics (e.g. sandpile avalanches), biology (e.g. species extinction and body mass), and the social sciences (e.g. city sizes and income). Among them are:

A broken power law is a piecewise function, consisting of two or more power laws, combined with a threshold. For example, with two power laws:

A power law with an exponential cutoff is simply a power law multiplied by an exponential function:

In a looser sense, a power-law probability distribution is a distribution whose density function (or mass function in the discrete case) has the form, for large values of formula_2,

where formula_30, and formula_31 is a slowly varying function, which is any function that satisfies formula_32 for any positive factor formula_33. This property of formula_31 follows directly from the requirement that formula_35 be asymptotically scale invariant; thus, the form of formula_31 only controls the shape and finite extent of the lower tail. For instance, if formula_31 is the constant function, then we have a power law that holds for all values of formula_2. In many cases, it is convenient to assume a lower bound formula_39 from which the law holds. Combining these two cases, and where formula_2 is a continuous variable, the power law has the form

where the pre-factor to formula_42 is the normalizing constant. We can now consider several properties of this distribution. For instance, its moments are given by

which is only well defined for formula_44. That is, all moments formula_45 diverge: when formula_46, the average and all higher-order moments are infinite; when formula_47, the mean exists, but the variance and higher-order moments are infinite, etc. For finite-size samples drawn from such distribution, this behavior implies that the central moment estimators (like the mean and the variance) for diverging moments will never converge – as more data is accumulated, they continue to grow. These power-law probability distributions are also called Pareto-type distributions, distributions with Pareto tails, or distributions with regularly varying tails.

A modification, which does not satisfy the general form above, with an exponential cutoff, is

In this distribution, the exponential decay term formula_49 eventually overwhelms the power-law behavior at very large values of formula_2. This distribution does not scale and is thus not asymptotically as a power law; however, it does approximately scale over a finite region before the cutoff. (Note that the pure form above is a subset of this family, with formula_51.) This distribution is a common alternative to the asymptotic power-law distribution because it naturally captures finite-size effects.

The Tweedie distributions are a family of statistical models characterized by closure under additive and reproductive convolution as well as under scale transformation. Consequently, these models all express a power-law relationship between the variance and the mean. These models have a fundamental role as foci of mathematical convergence similar to the role that the normal distribution has as a focus in the central limit theorem. This convergence effect explains why the variance-to-mean power law manifests so widely in natural processes, as with Taylor's law in ecology and with fluctuation scaling in physics. It can also be shown that this variance-to-mean power law, when demonstrated by the method of expanding bins, implies the presence of 1/"f" noise and that 1/"f" noise can arise as a consequence of this Tweedie convergence effect.

Although more sophisticated and robust methods have been proposed, the most frequently used graphical methods of identifying power-law probability distributions using random samples are Pareto quantile-quantile plots (or Pareto Q-Q plots), mean residual life plots and log–log plots. Another, more robust graphical method uses bundles of residual quantile functions. (Please keep in mind that power-law distributions are also called Pareto-type distributions.) It is assumed here that a random sample is obtained from a probability distribution, and that we want to know if the tail of the distribution follows a power law (in other words, we want to know if the distribution has a "Pareto tail"). Here, the random sample is called "the data".

Pareto Q-Q plots compare the quantiles of the log-transformed data to the corresponding quantiles of an exponential distribution with mean 1 (or to the quantiles of a standard Pareto distribution) by plotting the former versus the latter. If the resultant scatterplot suggests that the plotted points " asymptotically converge" to a straight line, then a power-law distribution should be suspected. A limitation of Pareto Q-Q plots is that they behave poorly when the tail index formula_19 (also called Pareto index) is close to 0, because Pareto Q-Q plots are not designed to identify distributions with slowly varying tails.

On the other hand, in its version for identifying power-law probability distributions, the mean residual life plot consists of first log-transforming the data, and then plotting the average of those log-transformed data that are higher than the "i"-th order statistic versus the "i"-th order statistic, for "i" = 1, ..., "n", where n is the size of the random sample. If the resultant scatterplot suggests that the plotted points tend to "stabilize" about a horizontal straight line, then a power-law distribution should be suspected. Since the mean residual life plot is very sensitive to outliers (it is not robust), it usually produces plots that are difficult to interpret; for this reason, such plots are usually called Hill horror plots 

Log–log plots are an alternative way of graphically examining the tail of a distribution using a random sample. Caution has to be exercised however as a log-log plot is necessary but insufficient evidence for a power law relationship, as many non power-law distributions will appear as straight lines on a log-log plot. This method consists of plotting the logarithm of an estimator of the probability that a particular number of the distribution occurs versus the logarithm of that particular number. Usually, this estimator is the proportion of times that the number occurs in the data set. If the points in the plot tend to "converge" to a straight line for large numbers in the x axis, then the researcher concludes that the distribution has a power-law tail. Examples of the application of these types of plot have been published. A disadvantage of these plots is that, in order for them to provide reliable results, they require huge amounts of data. In addition, they are appropriate only for discrete (or grouped) data.

Another graphical method for the identification of power-law probability distributions using random samples has been proposed. This methodology consists of plotting a "bundle for the log-transformed sample". Originally proposed as a tool to explore the existence of moments and the moment generation function using random samples, the bundle methodology is based on residual quantile functions (RQFs), also called residual percentile functions, which provide a full characterization of the tail behavior of many well-known probability distributions, including power-law distributions, distributions with other types of heavy tails, and even non-heavy-tailed distributions. Bundle plots do not have the disadvantages of Pareto Q-Q plots, mean residual life plots and log–log plots mentioned above (they are robust to outliers, allow visually identifying power laws with small values of formula_19, and do not demand the collection of much data). In addition, other types of tail behavior can be identified using bundle plots.

In general, power-law distributions are plotted on doubly logarithmic axes, which emphasizes the upper tail region. The most convenient way to do this is via the (complementary) cumulative distribution (cdf), formula_54,

Note that the cdf is also a power-law function, but with a smaller scaling exponent. For data, an equivalent form of the cdf is the rank-frequency approach, in which we first sort the formula_56 observed values in ascending order, and plot them against the vector formula_57.

Although it can be convenient to log-bin the data, or otherwise smooth the probability density (mass) function directly, these methods introduce an implicit bias in the representation of the data, and thus should be avoided. The cdf, on the other hand, is more robust to (but not without) such biases in the data and preserves the linear signature on doubly logarithmic axes. Though a cdf representation is favored over that of the pdf while fitting a power law to the data with the linear least square method, it is not devoid of mathematical inaccuracy. Thus, while estimating exponents of a power law distribution, maximum likelihood estimator is recommended.

There are many ways of estimating the value of the scaling exponent for a power-law tail, however not all of them yield unbiased and consistent answers. Some of the most reliable techniques are often based on the method of maximum likelihood. Alternative methods are often based on making a linear regression on either the log–log probability, the log–log cumulative distribution function, or on log-binned data, but these approaches should be avoided as they can all lead to highly biased estimates of the scaling exponent.

For real-valued, independent and identically distributed data, we fit a power-law distribution of the form

to the data formula_59, where the coefficient formula_42 is included to ensure that the distribution is normalized. Given a choice for formula_61, the log likelihood function becomes:

The maximum of this likelihood is found by differentiating with respect to parameter formula_19, setting the result equal to zero. Upon rearrangement, this yields the estimator equation:

where formula_65 are the formula_56 data points formula_67. This estimator exhibits a small finite sample-size bias of order formula_68, which is small when "n" > 100. Further, the standard error of the estimate is formula_69. This estimator is equivalent to the popular Hill estimator from quantitative finance and extreme value theory.

For a set of "n" integer-valued data points formula_65, again where each formula_71, the maximum likelihood exponent is the solution to the transcendental equation

where formula_73 is the incomplete zeta function. The uncertainty in this estimate follows the same formula as for the continuous equation. However, the two equations for formula_74 are not equivalent, and the continuous version should not be applied to discrete data, nor vice versa.

Further, both of these estimators require the choice of formula_61. For functions with a non-trivial formula_31 function, choosing formula_61 too small produces a significant bias in formula_78, while choosing it too large increases the uncertainty in formula_74, and reduces the statistical power of our model. In general, the best choice of formula_61 depends strongly on the particular form of the lower tail, represented by formula_31 above.

More about these methods, and the conditions under which they can be used, can be found in . Further, this comprehensive review article provides usable code (Matlab, Python, R and C++) for estimation and testing routines for power-law distributions.

Another method for the estimation of the power-law exponent, which does not assume independent and identically distributed (iid) data, uses the minimization of the Kolmogorov–Smirnov statistic, formula_82, between the cumulative distribution functions of the data and the power law:

with

where formula_85 and formula_86 denote the cdfs of the data and the power law with exponent formula_19, respectively. As this method does not assume iid data, it provides an alternative way to determine the power-law exponent for data sets in which the temporal correlation can not be ignored.

This criterion can be applied for the estimation of power-law exponent in the case of scale free distributions and provides a more convergent estimate than the maximum likelihood method. It has been applied to study probability distributions of fracture apertures. In some contexts the probability distribution is described, not by the cumulative distribution function, by the cumulative frequency of a property "X", defined as the number of elements per meter (or area unit, second etc.) for which "X" > "x" applies, where "x" is a variable real number. As an example, the cumulative distribution of the fracture aperture, "X", for a sample of "N" elements is defined as 'the number of fractures per meter having aperture greater than "x" . Use of cumulative frequency has some advantages, e.g. it allows one to put on the same diagram data gathered from sample lines of different lengths at different scales (e.g. from outcrop and from microscope).

The following function estimates the exponent in R, plotting the log–log data and the fitted line.
Although power-law relations are attractive for many theoretical reasons, demonstrating that data does indeed follow a power-law relation requires more than simply fitting a particular model to the data. This is important for understanding the mechanism that gives rise to the distribution: superficially similar distributions may arise for significantly different reasons, and different models yield different predictions, such as extrapolation.

For example, log-normal distributions are often mistaken for power-law distributions: a data set drawn from a lognormal distribution will be approximately linear for large values (corresponding to the upper tail of the lognormal being close to a power law), but for small values the lognormal will drop off significantly (bowing down), corresponding to the lower tail of the lognormal being small (there are very few small values, rather than many small values in a power law).

For example, Gibrat's law about proportional growth processes produce distributions that are lognormal, although their log–log plots look linear over a limited range. An explanation of this is that although the logarithm of the lognormal density function is quadratic in , yielding a "bowed" shape in a log–log plot, if the quadratic term is small relative to the linear term then the result can appear almost linear, and the lognormal behavior is only visible when the quadratic term dominates, which may require significantly more data. Therefore, a log–log plot that is slightly "bowed" downwards can reflect a log-normal distribution – not a power law.

In general, many alternative functional forms can appear to follow a power-law form for some extent. Stumpf proposed plotting the empirical cumulative distribution function in the log-log domain and claimed that a candidate power-law should cover at least two orders of magnitude. Also, researchers usually have to face the problem of deciding whether or not a real-world probability distribution follows a power law. As a solution to this problem, Diaz proposed a graphical methodology based on random samples that allow visually discerning between different types of tail behavior. This methodology uses bundles of residual quantile functions, also called percentile residual life functions, which characterize many different types of distribution tails, including both heavy and non-heavy tails. However, Stumpf claimed the need for both a statistical and a theoretical background in order to support a power-law in the underlying mechanism driving the data generating process.

One method to validate a power-law relation tests many orthogonal predictions of a particular generative mechanism against data. Simply fitting a power-law relation to a particular kind of data is not considered a rational approach. As such, the validation of power-law claims remains a very active field of research in many areas of modern science.



Notes
Bibliography




</doc>
<doc id="24527" url="https://en.wikipedia.org/wiki?curid=24527" title="Punt">
Punt

Punt or punting may refer to:





</doc>
<doc id="24530" url="https://en.wikipedia.org/wiki?curid=24530" title="PH">
PH

In chemistry, pH () is a logarithmic scale used to specify the acidity or basicity of an aqueous solution. It is approximately the negative of the base 10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions. More precisely it is the negative of the base 10 logarithm of the activity of the hydrogen ion. Solutions with a pH less than 7 are acidic and solutions with a pH greater than 7 are basic. Pure water is neutral, at pH 7 (25 °C), being neither an acid nor a base. Contrary to popular belief, the pH value can be less than 0 or greater than 14 for very strong acids and bases respectively.

Measurements of pH are important in agronomy, medicine, chemistry, water treatment, and many other applications.

The pH scale is traceable to a set of standard solutions whose pH is established by international agreement.
Primary pH standard values are determined using a concentration cell with transference, by measuring the potential difference between a hydrogen electrode and a standard electrode such as the silver chloride electrode.
The pH of aqueous solutions can be measured with a glass electrode and a pH meter, or an indicator.

There are three current theories used to describe Acid–base reactions: Arrhenius, Bronsted-Lowry and Lewis when determining pH.

The concept of pH was first introduced by the Danish chemist Søren Peder Lauritz Sørensen at the Carlsberg Laboratory in 1909 and revised to the modern pH in 1924 to accommodate definitions and measurements in terms of electrochemical cells. In the first papers, the notation had the "H" as a subscript to the lowercase "p", as so: p.

The exact meaning of the "p" in "pH" is disputed, but according to the Carlsberg Foundation, pH stands for "power of hydrogen". It has also been suggested that the "p" stands for the German "Potenz" (meaning "power"), others refer to French "puissance" (also meaning "power", based on the fact that the Carlsberg Laboratory was French-speaking). 
Another suggestion is that the "p" stands for the Latin terms "pondus hydrogenii" (quantity of hydrogen), "potentia hydrogenii" (capacity of hydrogen), or potential hydrogen. It is also suggested that Sørensen used the letters "p" and "q" (commonly paired letters in mathematics) simply to label the test solution (p) and the reference solution (q). Currently in chemistry, the p stands for "decimal cologarithm of", and is also used in the term p"K", used for acid dissociation constants.

Bacteriologist Alice C. Evans, famed for her work's influence on dairying and food safety, credited William Mansfield Clark and colleagues (of whom she was one) with developing pH measuring methods in the 1910s, which had a wide influence on laboratory and industrial use thereafter. In her memoir, she does not mention how much, or how little, Clark and colleagues knew about Sørensen's work a few years prior. She said: 
In these studies [of bacterial metabolism] Dr. Clark's attention was directed to the effect of acid on the growth of bacteria. He found that it is the intensity of the acid in terms of hydrogen-ion concentration that affects their growth. But existing methods of measuring acidity determined the quantity, not the intensity, of the acid. Next, with his collaborators, Dr. Clark developed accurate methods for measuring hydrogen-ion concentration. These methods replaced the inaccurate titration method of determining acid content in use in biologic laboratories throughout the world. Also they were found to be applicable in many industrial and other processes in which they came into wide usage.

The first electronic method for measuring pH was invented by Arnold Orville Beckman, a professor at California Institute of Technology in 1934. It was in response to local citrus grower Sunkist that wanted a better method for quickly testing the pH of lemons they were picking from their nearby orchards.

pH is defined as the decimal logarithm of the reciprocal of the hydrogen ion activity, "a"+, in a solution.

For example, a solution with a hydrogen ion activity of (at that level essentially the number of moles of hydrogen ions per liter of solution) has a pH of . For a commonplace example based on the facts that the masses of a mole of water, a mole of hydrogen ions, and a mole of hydroxide ions are respectively 18 g, 1 g, and 17 g, a quantity of 10 moles of pure (pH 7) water, or 180 tonnes (18×10 g), contains close to 1 g of dissociated hydrogen ions (or rather 19 g of HO hydronium ions) and 17 g of hydroxide ions.

Note that pH depends on temperature. For instance at 0 °C the pH of pure water is 7.47. At 25 °C it's 7.00, and at 100 °C it's 6.14.

This definition was adopted because ion-selective electrodes, which are used to measure pH, respond to activity. Ideally, electrode potential, "E", follows the Nernst equation, which, for the hydrogen ion can be written as

where "E" is a measured potential, "E" is the standard electrode potential, "R" is the gas constant, "T" is the temperature in kelvins, "F" is the Faraday constant. For H number of electrons transferred is one. It follows that electrode potential is proportional to pH when pH is defined in terms of activity. Precise measurement of pH is presented in International Standard ISO 31-8 as follows: A galvanic cell is set up to measure the electromotive force (e.m.f.) between a reference electrode and an electrode sensitive to the hydrogen ion activity when they are both immersed in the same aqueous solution. The reference electrode may be a silver chloride electrode or a calomel electrode. The hydrogen-ion selective electrode is a standard hydrogen electrode.

Firstly, the cell is filled with a solution of known hydrogen ion activity and the emf, "E", is measured. Then the emf, "E", of the same cell containing the solution of unknown pH is measured.

The difference between the two measured emf values is proportional to pH. This method of calibration avoids the need to know the standard electrode potential. The proportionality constant, 1/"z" is ideally equal to formula_4 the "Nernstian slope".

To apply this process in practice, a glass electrode is used rather than the cumbersome hydrogen electrode. A combined glass electrode has an in-built reference electrode. It is calibrated against buffer solutions of known hydrogen ion activity. IUPAC has proposed the use of a set of buffer solutions of known H activity. Two or more buffer solutions are used in order to accommodate the fact that the "slope" may differ slightly from ideal. To implement this approach to calibration, the electrode is first immersed in a standard solution and the reading on a pH meter is adjusted to be equal to the standard buffer's value. The reading from a second standard buffer solution is then adjusted, using the "slope" control, to be equal to the pH for that solution. Further details, are given in the IUPAC recommendations. When more than two buffer solutions are used the electrode is calibrated by fitting observed pH values to a straight line with respect to standard buffer values. Commercial standard buffer solutions usually come with information on the value at 25 °C and a correction factor to be applied for other temperatures.

The pH scale is logarithmic and therefore pH is a dimensionless quantity.

This was the original definition of Sørensen, which was superseded in favor of pH in 1909. However, it is possible to measure the concentration of hydrogen ions directly, if the electrode is calibrated in terms of hydrogen ion concentrations. One way to do this, which has been used extensively, is to titrate a solution of known concentration of a strong acid with a solution of known concentration of strong alkaline in the presence of a relatively high concentration of background electrolyte. Since the concentrations of acid and alkaline are known, it is easy to calculate the concentration of hydrogen ions so that the measured potential can be correlated with concentrations. The calibration is usually carried out using a Gran plot. The calibration yields a value for the standard electrode potential, "E", and a slope factor, "f", so that the Nernst equation in the form
can be used to derive hydrogen ion concentrations from experimental measurements of "E". The slope factor, "f", is usually slightly less than one. A slope factor of less than 0.95 indicates that the electrode is not functioning correctly. The presence of background electrolyte ensures that the hydrogen ion activity coefficient is effectively constant during the titration. As it is constant, its value can be set to one by defining the standard state as being the solution containing the background electrolyte. Thus, the effect of using this procedure is to make activity equal to the numerical value of concentration.

The glass electrode (and other ion selective electrodes) should be calibrated in a medium similar to the one being investigated. For instance, if one wishes to measure the pH of a seawater sample, the electrode should be calibrated in a solution resembling seawater in its chemical composition, as detailed below.

The difference between p[H] and pH is quite small. It has been stated that pH = p[H] + 0.04. It is common practice to use the term "pH" for both types of measurement.

Indicators may be used to measure pH, by making use of the fact that their color changes with pH. Visual comparison of the color of a test solution with a standard color chart provides a means to measure pH accurate to the nearest whole number. More precise measurements are possible if the color is measured spectrophotometrically, using a colorimeter or spectrophotometer.
Universal indicator consists of a mixture of indicators such that there is a continuous color change from about pH 2 to pH 10. Universal indicator paper is made from absorbent paper that has been impregnated with universal indicator. Another method of measuring pH is using an electronic pH meter.

pOH is sometimes used as a measure of the concentration of hydroxide ions. OH. pOH values are derived from pH measurements. The concentration of hydroxide ions in water is related to the concentration of hydrogen ions by

where "K" is the self-ionisation constant of water. Taking logarithms

So, at room temperature, pOH ≈ 14 − pH. However this relationship is not strictly valid in other circumstances, such as in measurements of soil alkalinity.

Measurement of pH below about 2.5 (ca. 0.003 mol dm acid) and above about 10.5 (ca. 0.0003 mol dm alkaline) requires special procedures because, when using the glass electrode, the Nernst law breaks down under those conditions. Various factors contribute to this. It cannot be assumed that liquid junction potentials are independent of pH. Also, extreme pH implies that the solution is concentrated, so electrode potentials are affected by ionic strength variation. At high pH the glass electrode may be affected by "alkaline error", because the electrode becomes sensitive to the concentration of cations such as Na and K in the solution. Specially constructed electrodes are available which partly overcome these problems.

Runoff from mines or mine tailings can produce some very low pH values.

Hydrogen ion concentrations (activities) can be measured in non-aqueous solvents. pH values based on these measurements belong to a different scale from aqueous pH values, because activities relate to different standard states. Hydrogen ion activity, "a", can be defined as:
where "μ" is the chemical potential of the hydrogen ion, formula_9 is its chemical potential in the chosen standard state, "R" is the gas constant and "T" is the thermodynamic temperature. Therefore, pH values on the different scales cannot be compared directly due to different solvated proton ions such as lyonium ions, requiring an intersolvent scale which involves the transfer activity coefficient of hydronium/lyonium ion.

pH is an example of an acidity function. Other acidity functions can be defined. For example, the Hammett acidity function, "H", has been developed in connection with superacids.

The concept of "unified pH scale" has been developed on the basis of the absolute chemical potential of the proton. This model uses the Lewis acid–base definition. This scale applies to liquids, gases and even solids.
In 2010, a new "unified absolute pH scale" has been proposed that would allow various pH ranges across different solutions to use a common proton reference standard.

Pure water is neutral. When an acid is dissolved in water, the pH will be less than 7 (25 °C). When a base, or alkali, is dissolved in water, the pH will be greater than 7. A solution of a strong acid, such as hydrochloric acid, at concentration 1 mol dm has a pH of 0. A solution of a strong alkali, such as sodium hydroxide, at concentration 1 mol dm, has a pH of 14. Thus, measured pH values will lie mostly in the range 0 to 14, though negative pH values and values above 14 are entirely possible. Since pH is a logarithmic scale, a difference of one pH unit is equivalent to a tenfold difference in hydrogen ion concentration. <br>
The pH of neutrality is not exactly 7 (25 °C), although this is a good approximation in most cases. Neutrality is defined as the condition where [H] = [OH] (or the activities are equal). Since self-ionization of water holds the product of these concentration [H]×[OH] = K, it can be seen that at neutrality [H] = [OH] = , or pH = pK/2. pK is approximately 14 but depends on ionic strength and temperature, and so the pH of neutrality does also. Pure water and a solution of NaCl in pure water are both neutral, since dissociation of water produces equal numbers of both ions. However the pH of the neutral NaCl solution will be slightly different from that of neutral pure water because the hydrogen and hydroxide ions' activity is dependent on ionic strength, so K varies with ionic strength.

If pure water is exposed to air it becomes mildly acidic. This is because water absorbs carbon dioxide from the air, which is then slowly converted into bicarbonate and hydrogen ions (essentially creating carbonic acid).

The United States Department of Agriculture Natural Resources Conservation Service, formerly Soil Conservation Service classifies soil pH ranges as follows:

pH-dependent plant pigments that can be used as pH indicators occur in many plants, including hibiscus, red cabbage (anthocyanin) and red wine. The juice of citrus fruits is acidic mainly because it contains citric acid. Other carboxylic acids occur in many living systems. For example, lactic acid is produced by muscle activity. The state of protonation of phosphate derivatives, such as ATP, is pH-dependent. The functioning of the oxygen-transport enzyme hemoglobin is affected by pH in a process known as the Root effect.

The pH of seawater is typically limited to a range between 7.5 and 8.4. It plays an important role in the ocean's carbon cycle, and there is evidence of ongoing ocean acidification caused by carbon dioxide emissions. However, pH measurement is complicated by the chemical properties of seawater, and several distinct pH scales exist in chemical oceanography.

As part of its operational definition of the pH scale, the IUPAC defines a series of buffer solutions across a range of pH values (often denoted with NBS or NIST designation). These solutions have a relatively low ionic strength (≈0.1) compared to that of seawater (≈0.7), and, as a consequence, are not recommended for use in characterizing the pH of seawater, since the ionic strength differences cause changes in electrode potential. To resolve this problem, an alternative series of buffers based on artificial seawater was developed. This new series resolves the problem of ionic strength differences between samples and the buffers, and the new pH scale is referred to as the 'total scale', often denoted as pH. The total scale was defined using a medium containing sulfate ions. These ions experience protonation, H + SO HSO, such that the total scale includes the effect of both protons (free hydrogen ions) and hydrogen sulfate ions:

An alternative scale, the 'free scale', often denoted 'pH', omits this consideration and focuses solely on [H], in principle making it a simpler representation of hydrogen ion concentration. Only [H] can be determined, therefore [H] must be estimated using the [SO] and the stability constant of HSO, :

However, it is difficult to estimate K in seawater, limiting the utility of the otherwise more straightforward free scale.

Another scale, known as the 'seawater scale', often denoted 'pH', takes account of a further protonation relationship between hydrogen ions and fluoride ions, H + F ⇌ HF. Resulting in the following expression for [H]:

However, the advantage of considering this additional complexity is dependent upon the abundance of fluoride in the medium. In seawater, for instance, sulfate ions occur at much greater concentrations (>400 times) than those of fluoride. As a consequence, for most practical purposes, the difference between the total and seawater scales is very small.

The following three equations summarise the three scales of pH:

In practical terms, the three seawater pH scales differ in their values by up to 0.12 pH units, differences that are much larger than the accuracy of pH measurements typically required, in particular, in relation to the ocean's carbonate system. Since it omits consideration of sulfate and fluoride ions, the free scale is significantly different from both the total and seawater scales. Because of the relative unimportance of the fluoride ion, the total and seawater scales differ only very slightly.

The pH of different cellular compartments, body fluids, and organs is usually tightly regulated in a process called acid-base homeostasis. The most common disorder in acid-base homeostasis is acidosis, which means an acid overload in the body, generally defined by pH falling below 7.35. Alkalosis is the opposite condition, with blood pH being excessively high.

The pH of blood is usually slightly basic with a value of pH 7.365. This value is often referred to as physiological pH in biology and medicine. Plaque can create a local acidic environment that can result in tooth decay by demineralization. Enzymes and other proteins have an optimum pH range and can become inactivated or denatured outside this range.
The calculation of the pH of a solution containing acids and/or bases is an example of a chemical speciation calculation, that is, a mathematical procedure for calculating the concentrations of all chemical species that are present in the solution. The complexity of the procedure depends on the nature of the solution. For strong acids and bases no calculations are necessary except in extreme situations. The pH of a solution containing a weak acid requires the solution of a quadratic equation. The pH of a solution containing a weak base may require the solution of a cubic equation. The general case requires the solution of a set of non-linear simultaneous equations.

A complicating factor is that water itself is a weak acid and a weak base (see amphoterism). It dissociates according to the equilibrium
with a dissociation constant, defined as
where [H] stands for the concentration of the aqueous hydronium ion and [OH] represents the concentration of the hydroxide ion. This equilibrium needs to be taken into account at high pH and when the solute concentration is extremely low.

Strong acids and bases are compounds that, for practical purposes, are completely dissociated in water. Under normal circumstances this means that the concentration of hydrogen ions in acidic solution can be taken to be equal to the concentration of the acid. The pH is then equal to minus the logarithm of the concentration value. Hydrochloric acid (HCl) is an example of a strong acid. The pH of a 0.01M solution of HCl is equal to −log(0.01), that is, pH = 2. Sodium hydroxide, NaOH, is an example of a strong base. The p[OH] value of a 0.01M solution of NaOH is equal to −log(0.01), that is, p[OH] = 2. From the definition of p[OH] above, this means that the pH is equal to about 12. For solutions of sodium hydroxide at higher concentrations the self-ionization equilibrium must be taken into account.

Self-ionization must also be considered when concentrations are extremely low. Consider, for example, a solution of hydrochloric acid at a concentration of 5×10M. The simple procedure given above would suggest that it has a pH of 7.3. This is clearly wrong as an acid solution should have a pH of less than 7. Treating the system as a mixture of hydrochloric acid and the amphoteric substance water, a pH of 6.89 results.

A weak acid or the conjugate acid of a weak base can be treated using the same formalism.
First, an acid dissociation constant is defined as follows. Electrical charges are omitted from subsequent equations for the sake of generality
and its value is assumed to have been determined by experiment. This being so, there are three unknown concentrations, [HA], [H] and [A] to determine by calculation. Two additional equations are needed. One way to provide them is to apply the law of mass conservation in terms of the two "reagents" H and A.
C stands for analytical concentration. In some texts, one mass balance equation is replaced by an equation of charge balance. This is satisfactory for simple cases like this one, but is more difficult to apply to more complicated cases as those below. Together with the equation defining K, there are now three equations in three unknowns. When an acid is dissolved in water C = C = C, the concentration of the acid, so [A] = [H]. After some further algebraic manipulation an equation in the hydrogen ion concentration may be obtained.
Solution of this quadratic equation gives the hydrogen ion concentration and hence p[H] or, more loosely, pH. This procedure is illustrated in an ICE table which can also be used to calculate the pH when some additional (strong) acid or alkaline has been added to the system, that is, when C ≠ C.

For example, what is the pH of a 0.01M solution of benzoic acid, pK = 4.19?

For alkaline solutions an additional term is added to the mass-balance equation for hydrogen. Since addition of hydroxide reduces the hydrogen ion concentration, and the hydroxide ion concentration is constrained by the self-ionization equilibrium to be equal to formula_19
In this case the resulting equation in [H] is a cubic equation.

Some systems, such as with polyprotic acids, are amenable to spreadsheet calculations. With three or more reagents or when many complexes are formed with general formulae such as ABH,the following general method can be used to calculate the pH of a solution. For example, with three reagents, each equilibrium is characterized by an equilibrium constant, β.
Next, write down the mass-balance equations for each reagent:
Note that there are no approximations involved in these equations, except that each stability constant is defined as a quotient of concentrations, not activities. Much more complicated expressions are required if activities are to be used.

There are 3 non-linear simultaneous equations in the three unknowns, [A], [B] and [H]. Because the equations are non-linear, and because concentrations may range over many powers of 10, the solution of these equations is not straightforward. However, many computer programs are available which can be used to perform these calculations. There may be more than three reagents. The calculation of hydrogen ion concentrations, using this formalism, is a key element in the determination of equilibrium constants by potentiometric titration.




</doc>
<doc id="24533" url="https://en.wikipedia.org/wiki?curid=24533" title="Pastel">
Pastel

A pastel (, ) is an art medium in the form of a stick, consisting of pure powdered pigment and a binder. The pigments used in pastels are the same as those used to produce all colored art media, including oil paints; the binder is of a neutral hue and low saturation. The color effect of pastels is closer to the natural dry pigments than that of any other process.

Pastels have been used by artists since the Renaissance, and gained considerable popularity in the 18th century, when a number of notable artists made pastel their primary medium.

An artwork made using pastels is called a pastel (or a pastel drawing or pastel painting). "Pastel" used as a verb means to produce an artwork with pastels; as an adjective it means pale in color.

Pastel sticks or crayons consist of pure powdered pigment combined with a binder. The exact composition and characteristics of an individual pastel stick depends on the type of pastel and the type and amount of binder used. It also varies by individual manufacturer.

Dry pastels have historically used binders such as gum arabic and gum tragacanth. Methyl cellulose was introduced as a binder in the twentieth century. Often a chalk or gypsum component is present. They are available in varying degrees of hardness, the softer varieties being wrapped in paper. Some pastel brands use pumice in the binder to abrade the paper and create more tooth.

Dry pastel media can be subdivided as follows:
In addition, pastels using a different approach to manufacture have been developed:

There has been some debate within art societies as to what exactly counts as a pastel. The Pastel Society within the UK (the oldest pastel society) states the following are acceptable media for its exhibitions: ""Pastels, including Oil pastel, Charcoal, Pencil, Conté, Sanguine, or any dry media"." The emphasis appears to be on "dry media" but the debate continues.

In order to create hard and soft pastels, pigments are ground into a paste with water and a gum binder and then rolled or pressed into sticks. The name "pastel" comes from Medieval Latin "pastellum", woad paste, from Late Latin "pastellus", paste. The French word "pastel" first appeared in 1662.

Most brands produce gradations of a color, the original pigment of which tends to be dark, from pure pigment to near-white by mixing in differing quantities of chalk. This mixing of pigments with chalks is the origin of the word "pastel" in reference to "pale color" as it is commonly used in cosmetic and fashion venues.

A pastel is made by letting the sticks move over an abrasive ground, leaving color on the grain of the paper, sandboard, canvas etc. When fully covered with pastel, the work is called a pastel "painting"; when not, a pastel "sketch" or "drawing". Pastel paintings, being made with a medium that has the highest pigment concentration of all, reflect light without darkening refraction, allowing for very saturated colors.

Pastel supports need to provide a "tooth" for the pastel to adhere and hold the pigment in place. Supports include:


Pastels can be used to produce a permanent work of art if the artist meets appropriate archival considerations. This means:


For these reasons, some pastelists avoid the use of a fixative except in cases where the pastel has been overworked so much that the surface will no longer hold any more pastel. The fixative will restore the "tooth" and more pastel can be applied on top. It is the tooth of the painting surface that holds the pastels, not a fixative. Abrasive supports avoid or minimize the need to apply further fixative in this way. SpectraFix, a modern casein fixative available premixed in a pump misting bottle or as concentrate to be mixed with alcohol, is not toxic and does not darken or dull pastel colors. However, SpectraFix takes some practice to use because it's applied with a pump misting bottle instead of an aerosol spray can. It is easy to use too much SpectraFix and leave puddles of liquid that may dissolve passages of color; also it takes a little longer to dry than conventional spray fixatives between light layers.

Glassine (paper) is used by artists to protect artwork which is being stored or transported. Some good quality books of pastel papers also include glassine to separate pages.

Pastel techniques can be challenging since the medium is mixed and blended directly on the working surface, and unlike paint, colors cannot be tested on a palette before applying to the surface. Pastel errors cannot be covered the way a paint error can be painted out. Experimentation with the pastel medium on a small scale in order to learn various techniques gives the user a better command over a larger composition.

Pastels have some techniques in common with painting, such as blending, masking, building up layers of color, adding accents and highlighting, and shading. Some techniques are characteristic of both pastels and sketching mediums such as charcoal and lead, for example, hatching and crosshatching, and gradation. Other techniques are particular to the pastel medium.


Pastels are a dry medium and produce a great deal of dust, which can cause respiratory irritation. More seriously, pastels use the same pigments as artists' paints, many of which are toxic. For example, exposure to cadmium pigments, which are common and popular bright yellows, oranges, and reds, can lead to cadmium poisoning. Pastel artists, who use the pigments without a strong painting binder, are especially susceptible to such poisoning. For this reason, many modern pastels are made using substitutions for cadmium, chromium, and other toxic pigments, while retaining the traditional pigment names.

The manufacture of pastels originated in the 15th century. The pastel medium was mentioned by Leonardo da Vinci, who learned of it from the French artist Jean Perréal after that artist's arrival in Milan in 1499. Pastel was sometimes used as a medium for preparatory studies by 16th-century artists, notably Federico Barocci. The first French artist to specialize in pastel portraits was Joseph Vivien.

During the 18th century the medium became fashionable for portrait painting, sometimes in a mixed technique with gouache. Pastel was an important medium for artists such as Jean-Baptiste Perronneau, Maurice Quentin de La Tour (who never painted in oils), and Rosalba Carriera. The pastel still life paintings and portraits of Jean-Baptiste-Siméon Chardin are much admired, as are the works of the Swiss-French artist Jean-Étienne Liotard. In 18th-century England the outstanding practitioner was John Russell. In Colonial America, John Singleton Copley used pastel occasionally for portraits.

In France, pastel briefly became unpopular during and after the Revolution, as the medium was identified with the frivolity of the Ancien Régime. By the mid-19th century, French artists such as Eugène Delacroix and especially Jean-François Millet were again making significant use of pastel. Their countryman Édouard Manet painted a number of portraits in pastel on canvas, an unconventional ground for the medium. Edgar Degas was an innovator in pastel technique, and used it with an almost expressionist vigor after about 1885, when it became his primary medium. Odilon Redon produced a large body of works in pastel.

James Abbott McNeill Whistler produced a quantity of pastels around 1880, including a body of work relating to Venice, and this probably contributed to a growing enthusiasm for the medium in the United States. In particular, he demonstrated how few strokes were required to evoke a place or an atmosphere. Mary Cassatt, an American artist active in France, introduced the Impressionists and pastel to her friends in Philadelphia and Washington.

According to the Metropolitan Museum of Art's "Time Line of Art History: Nineteenth Century American Drawings": 

On the East Coast of the United States, the Society of Painters in Pastel was founded in 1883 by William Merritt Chase, Robert Blum, and others. The Pastellists, led by Leon Dabo, was organized in New York in late 1910 and included among its ranks Everett Shinn and Arthur Bowen Davies. On the American West Coast the influential artist and teacher Pedro Joseph de Lemos, who served as Chief Administrator of the San Francisco Art Institute and Director of the Stanford University Museum and Art Gallery, popularized pastels in regional exhibitions. Beginning in 1919 de Lemos published a series of articles on “painting” with pastels, which included such notable innovations as allowing the intensity of light on the subject to determine the distinct color of laid paper and the use of special optics for making “night sketches” in both urban and rural settings. His night scenes, which were often called “dreamscapes” in the press, were influenced by French Symbolism, and especially Odilon Redon.

Pastels have been favored by many modern artists because of the medium's broad range of bright colors. Modern notable artists who have worked extensively in pastels include Fernando Botero, Francesco Clemente, Daniel Greene, Wolf Kahn, and R. B. Kitaj.





</doc>
<doc id="24537" url="https://en.wikipedia.org/wiki?curid=24537" title="Pen pal">
Pen pal

Pen pals (or penpals, pen-pals, penfriends or pen friends) are people who regularly write to each other, particularly via postal mail.

A penpal relationship is often used to practice reading and writing in a foreign language, to improve literacy, to learn more about other countries and life-styles, and to make friendships. As with any friendships in life, some people remain penpals for only a short time, while others continue to exchange letters and presents for life. Some penpals eventually arrange to meet face to face; sometimes leading to serious relationships, or even marriage.

Penpals come in all ages, nationalities, cultures, languages and interests. Pals may seek new penfriends based on their own age group, a specific occupation, hobby, or select someone totally different from them to gain knowledge about the world around them.

A modern variation on the traditional penpal arrangement is to have a keypal / epal and exchange email addresses as well as or instead of paper letters. This has the advantage of saving money and being more immediate, allowing many messages to be exchanged in a short period of time. The disadvantage is that the communication can be very ephemeral if the email messages are not routinely saved. Many people prefer to receive paper letters, gaining the satisfaction of seeing their name carefully printed on a thick envelope in the letterbox. Using postal mail, it is possible to trade coupons, swap slips, postcards, stamps and anything else light and flat enough to fit inside an envelope, often called "tuck-ins". Many penpallers like to trade sheets of stickers, notecards and stationery sets.

While the expansion of the Internet has reduced the number of traditional penpals, penpal clubs can nowadays be found on the Internet, in magazine columns, newspapers, and sometimes through clubs or special interest groups. Some people are looking for romantic interests, while others just want to find friends. It seems, on the internet, that the term "pen pals" defines those looking to correspond with others that live in a different place, where pen pals originated via postal mail correspondences and has evolved to mean something more. Penpals also make and pass around friendship books, slams and crams. Another term used to describe pen pal is the word penfriend.

In recent years, pen pal correspondence with prison inmates has gained acceptance on the Internet.

Many penpals meet each other through organizations that bring people together for this purpose.

Organizations can be split into three main categories: free, partial subscription, and subscription-based clubs. Free clubs are usually funded by advertising and profiles are not reviewed, whereas subscription-based clubs will usually not contain any advertising and will have an administrator approving profiles to the database.

While the traditional snail mail pen pal relationship has fallen into a decline due to modern technology closing the world's communication gap, prison pen pal services have combined technology with traditional letter writing. These sites allow prisoners to place pen pal ads online; however, inmates in the United States and most of the world are not permitted to access the Internet. Therefore the pen pal relationships with inmates are still conducted via postal mail. Other pen pal organizations have survived by embracing the technology of the Internet.

The Australian author Geraldine Brooks wrote a memoir entitled "Foreign Correspondence" (1997) about her childhood, which was enriched by her exchanges of letters with other children in Australia and overseas and her travels as an adult in search of the people they had become.

In the 1970s, syndicated children's television program "Big Blue Marble" often invited viewers to write to them for their own pen pal.

On another children's TV show, "Pee-wee's Playhouse", Pee-wee Herman would often receive pen pal letters.

At the 1964/1965 World's Fair in New York, the Parker Pen pavilion provided a computer pen pal matching service. This service was officially terminated by Parker Pen in 1967. This service did not work in conjunction with any other pen friend clubs. The computer system and database used for this service were not sold, taken over, or continued in any way.

In the "Peanuts" comic strip from the 1960s and 1970s, Charlie Brown tries to write to a pen pal using a fountain pen but after several literally "botched" attempts, Charlie switches to using a pencil and referring to his penpal as his "pencil-pal", with his first letter to his "pencil-pal" explaining the reason for the name change.

The 1983 Bollywood film "Romance" is about two people, Amar (from India) and Sonia (from the UK) who fall in love after becoming pen pals. The 1999 Bollywood film "Sirf Tum" has a similar storyline.

The 1998 film "You've Got Mail" is a romantic comedy about two people in a pen pal Email courtship who are unaware that they are also business rivals.

The 2004 action-drama film "Out of Reach" is about a pen pal relationship between a Vietnam veteran and a 13-year-old orphaned girl from Poland. When the letters suddenly stop coming, he heads to Poland to find out the reason.

The 2009 claymation film "Mary and Max" is about the pen pal relationship between an American man and an Australian girl.

The 2012 novel "Penpal" takes a dark twist to this innocent idea by the plot revolving around the protagonist being stalked ever since sending his letter.

The 2015 musical album "Gold" is a collaboration between musicians Jetty Rae and Heath McNease under the moniker "Pen Pals".



</doc>
